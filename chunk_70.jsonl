{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the source to file open the rendered result in a viewer.", "response": "def view(self, filename=None, directory=None, cleanup=False):\n        \"\"\"Save the source to file, open the rendered result in a viewer.\n\n        Args:\n            filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``)\n            directory: (Sub)directory for source saving and rendering.\n            cleanup (bool): Delete the source file after rendering.\n        Returns:\n            The (possibly relative) path of the rendered file.\n        Raises:\n            graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n            subprocess.CalledProcessError: If the exit status is non-zero.\n            RuntimeError: If opening the viewer is not supported.\n\n        Short-cut method for calling :meth:`.render` with ``view=True``.\n        \"\"\"\n        return self.render(filename=filename, directory=directory, view=True,\n                           cleanup=cleanup)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the right viewer based on file format and platform.", "response": "def _view(self, filepath, format):\n        \"\"\"Start the right viewer based on file format and platform.\"\"\"\n        methodnames = [\n            '_view_%s_%s' % (format, backend.PLATFORM),\n            '_view_%s' % backend.PLATFORM,\n        ]\n        for name in methodnames:\n            view_method = getattr(self, name, None)\n            if view_method is not None:\n                break\n        else:\n            raise RuntimeError('%r has no built-in viewer support for %r '\n                'on %r platform' % (self.__class__, format, backend.PLATFORM))\n        view_method(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an instance of the class from the given file.", "response": "def from_file(cls, filename, directory=None,\n                  format=None, engine=None, encoding=File._encoding):\n        \"\"\"Return an instance with the source string read from the given file.\n\n        Args:\n            filename: Filename for loading/saving the source.\n            directory: (Sub)directory for source loading/saving and rendering.\n            format: Rendering output format (``'pdf'``, ``'png'``, ...).\n            engine: Layout command used (``'dot'``, ``'neato'``, ...).\n            encoding: Encoding for loading/saving the source.\n        \"\"\"\n        filepath = os.path.join(directory or '', filename)\n        if encoding is None:\n            encoding = locale.getpreferredencoding()\n        with io.open(filepath, encoding=encoding) as fd:\n            source = fd.read()\n        return cls(source, filename, directory, format, engine, encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn DOT identifier from string if needed.", "response": "def quote(identifier,\n          html=HTML_STRING.match, valid_id=ID.match, dot_keywords=KEYWORDS):\n    \"\"\"Return DOT identifier from string, quote if needed.\n\n    >>> quote('')\n    '\"\"'\n\n    >>> quote('spam')\n    'spam'\n\n    >>> quote('spam spam')\n    '\"spam spam\"'\n\n    >>> quote('-4.2')\n    '-4.2'\n\n    >>> quote('.42')\n    '.42'\n\n    >>> quote('<<b>spam</b>>')\n    '<<b>spam</b>>'\n\n    >>> quote(nohtml('<>'))\n    '\"<>\"'\n    \"\"\"\n    if html(identifier) and not isinstance(identifier, NoHtml):\n        pass\n    elif not valid_id(identifier) or identifier.lower() in dot_keywords:\n        return '\"%s\"' % identifier.replace('\"', '\\\\\"')\n    return identifier"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quote_edge(identifier):\n    node, _, rest = identifier.partition(':')\n    parts = [quote(node)]\n    if rest:\n        port, _, compass = rest.partition(':')\n        parts.append(quote(port))\n        if compass:\n            parts.append(compass)\n    return ':'.join(parts)", "response": "Return DOT edge statement node_id from string quote if needed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn assembled DOT a_list string.", "response": "def a_list(label=None, kwargs=None, attributes=None):\n    \"\"\"Return assembled DOT a_list string.\n\n    >>> a_list('spam', {'spam': None, 'ham': 'ham ham', 'eggs': ''})\n    'label=spam eggs=\"\" ham=\"ham ham\"'\n    \"\"\"\n    result = ['label=%s' % quote(label)] if label is not None else []\n    if kwargs:\n        items = ['%s=%s' % (quote(k), quote(v))\n            for k, v in tools.mapping_items(kwargs) if v is not None]\n        result.extend(items)\n    if attributes:\n        if hasattr(attributes, 'items'):\n            attributes = tools.mapping_items(attributes)\n        items = ['%s=%s' % (quote(k), quote(v))\n            for k, v in attributes if v is not None]\n        result.extend(items)\n    return ' '.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attr_list(label=None, kwargs=None, attributes=None):\n    content = a_list(label, kwargs, attributes)\n    if not content:\n        return ''\n    return ' [%s]' % content", "response": "Return assembled DOT attribute list string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning copy of s that will not treat leading and trailing HTML string in quoting.", "response": "def nohtml(s):\n    \"\"\"Return copy of ``s`` that will not treat ``'<...>'`` as DOT HTML string in quoting.\n\n    Args:\n        s: String in which leading ``'<'`` and trailing ``'>'`` should be treated as literal.\n    Raises:\n        TypeError: If ``s`` is not a ``str`` on Python 3, or a ``str``/``unicode`` on Python 2.\n\n    >>> quote('<>-*-<>')\n    '<>-*-<>'\n\n    >>> quote(nohtml('<>-*-<>'))\n    '\"<>-*-<>\"'\n    \"\"\"\n    try:\n        subcls = NOHTML[type(s)]\n    except KeyError:\n        raise TypeError('%r does not have one of the required types: %r' %\n                        (s, list(NOHTML)))\n    return subcls(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets content to an empty body clear graph node and edge attributes.", "response": "def clear(self, keep_attrs=False):\n        \"\"\"Reset content to an empty body, clear graph/node/egde_attr mappings.\n\n        Args:\n            keep_attrs (bool): preserve graph/node/egde_attr mappings\n        \"\"\"\n        if not keep_attrs:\n            for a in (self.graph_attr, self.node_attr, self.edge_attr):\n                a.clear()\n        del self.body[:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a node. Args: name: Unique identifier for the node inside the source. label: Caption to be displayed (defaults to the node ``name``). attrs: Any additional node attributes (must be strings).", "response": "def node(self, name, label=None, _attributes=None, **attrs):\n        \"\"\"Create a node.\n\n        Args:\n            name: Unique identifier for the node inside the source.\n            label: Caption to be displayed (defaults to the node ``name``).\n            attrs: Any additional node attributes (must be strings).\n        \"\"\"\n        name = self._quote(name)\n        attr_list = self._attr_list(label, attrs, _attributes)\n        line = self._node % (name, attr_list)\n        self.body.append(line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an edge between two nodes.", "response": "def edge(self, tail_name, head_name, label=None, _attributes=None, **attrs):\n        \"\"\"Create an edge between two nodes.\n\n        Args:\n            tail_name: Start node identifier.\n            head_name: End node identifier.\n            label: Caption to be displayed near the edge.\n            attrs: Any additional edge attributes (must be strings).\n        \"\"\"\n        tail_name = self._quote_edge(tail_name)\n        head_name = self._quote_edge(head_name)\n        attr_list = self._attr_list(label, attrs, _attributes)\n        line = self._edge % (tail_name, head_name, attr_list)\n        self.body.append(line)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef edges(self, tail_head_iter):\n        edge = self._edge_plain\n        quote = self._quote_edge\n        lines = (edge % (quote(t), quote(h)) for t, h in tail_head_iter)\n        self.body.extend(lines)", "response": "Create a bunch of edges."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attr(self, kw=None, _attributes=None, **attrs):\n        if kw is not None and kw.lower() not in ('graph', 'node', 'edge'):\n            raise ValueError('attr statement must target graph, node, or edge: '\n                '%r' % kw)\n        if attrs or _attributes:\n            if kw is None:\n                a_list = self._a_list(None, attrs, _attributes)\n                line = self._attr_plain % a_list\n            else:\n                attr_list = self._attr_list(None, attrs, _attributes)\n                line = self._attr % (kw, attr_list)\n            self.body.append(line)", "response": "Add a general or graph / node / edge attribute statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the current content of the given graph argument as subgraph \\ or return a new context manager.", "response": "def subgraph(self, graph=None, name=None, comment=None,\n                 graph_attr=None, node_attr=None, edge_attr=None, body=None):\n        \"\"\"Add the current content of the given sole ``graph`` argument as subgraph \\\n           or return a context manager returning a new graph instance created \\\n           with the given (``name``, ``comment``, etc.) arguments whose content is \\\n           added as subgraph when leaving the context manager's ``with``-block.\n\n        Args:\n            graph: An instance of the same kind (:class:`.Graph`, :class:`.Digraph`)\n                   as the current graph (sole argument in non-with-block use).\n            name: Subgraph name (``with``-block use).\n            comment: Subgraph comment (``with``-block use).\n            graph_attr: Subgraph-level attribute-value mapping (``with``-block use).\n            node_attr: Node-level attribute-value mapping (``with``-block use).\n            edge_attr: Edge-level attribute-value mapping (``with``-block use).\n            body: Verbatim lines to add to the subgraph ``body`` (``with``-block use).\n\n        See the :ref:`usage examples in the User Guide <subgraphs>`.\n\n        .. note::\n            If the ``name`` of the subgraph begins with ``'cluster'`` (all lowercase)\n            the layout engine will treat it as a special cluster subgraph.\n        \"\"\"\n        if graph is None:\n            kwargs = {'name': name, 'comment': comment,\n                      'graph_attr': graph_attr, 'node_attr': node_attr,\n                      'edge_attr': edge_attr, 'body': body}\n            return SubgraphContext(self, kwargs)\n\n        args = [name, comment, graph_attr, node_attr, edge_attr, body]\n        if not all(a is None for a in args):\n            raise ValueError('graph must be sole argument of subgraph()')\n        if graph.directed != self.directed:\n            raise ValueError('%r cannot add subgraph of different kind: %r '\n                % (self, graph))\n        lines = ['\\t' + line for line in graph.__iter__(subgraph=True)]\n        self.body.extend(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach(object, name):\n    def decorator(func):\n        setattr(object, name, func)\n        return func\n    return decorator", "response": "Return a decorator doing setattr with its argument."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mkdirs(filename, mode=0o777):\n    dirname = os.path.dirname(filename)\n    if not dirname:\n        return\n    _compat.makedirs(dirname, mode=mode, exist_ok=True)", "response": "Recursively create directories up to the path of filename as needed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mapping_items(mapping, _iteritems=_compat.iteritems):\n    if type(mapping) is dict:\n        return iter(sorted(_iteritems(mapping)))\n    return _iteritems(mapping)", "response": "Return an iterator over the mapping items sorted if it s a plain dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_url(host, path):\n        host += \"/\" if not host.endswith(\"/\") else \"\"\n        path = path.lstrip(\"/\")\n\n        return parse.urljoin(host, path)", "response": "Builds a valid URL from a host and path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_adapted_session(adapter):\n        session = requests.Session()\n        session.mount(\"http://\", adapter)\n        session.mount(\"https://\", adapter)\n        return session", "response": "Returns a new requests. Session object that is used to access the resource names of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the set of required headers for the given service and endpoint.", "response": "def get_required_headers(service, endpoint):\n        \"\"\"\n        :param Service service:\n            The service being called\n        :param Endpoint endpoint:\n            The endpoint being called\n        :return:\n            Headers required by the ``service`` and the ``endpoint`` being called\n        :rtype:\n            dict\n        \"\"\"\n        headers = {}\n        headers.update(service.required_headers)\n        headers.update(endpoint.required_headers)\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_formatted_path(self, **kwargs):\n        self._validate_path_placeholders(self.path_placeholders, kwargs)\n\n        return self.path.format(**kwargs)", "response": "Formats this endpoint s path with the supplied keyword arguments and returns the fully - formatted path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of formattable placeholders from this endpoint s path.", "response": "def path_placeholders(self):\n        \"\"\"\n        The formattable placeholders from this endpoint's path, in the order they appear.\n\n        Example:\n\n            >>> endpoint = Endpoint(path='/api/{foo}/{bar}')\n            >>> endpoint.path_placeholders\n            ['foo', 'bar']\n        \"\"\"\n\n        parser = string.Formatter()\n        return [placeholder_name for _, placeholder_name, _, _ in parser.parse(self.path) if placeholder_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of all of the default parameters and values for this endpoint.", "response": "def get_merged_params(self, supplied_params=None):\n        \"\"\"\n        Merge this endpoint's default parameters with the supplied parameters\n\n        :param dict supplied_params:\n            A dictionary of query parameter, value pairs\n        :return:\n            A dictionary of this endpoint's default parameters, merged with the supplied parameters.\n            Any default parameters which have a value supplied are overridden.\n        :rtype:\n            dict\n        :raises apiron.exceptions.UnfulfilledParameterException:\n            When a required parameter for this endpoint is not a default param and is not supplied by the caller\n        \"\"\"\n        supplied_params = supplied_params or {}\n\n        empty_params = {\n            param: supplied_params[param] for param in supplied_params if supplied_params[param] in (None, \"\")\n        }\n        if empty_params:\n            warnings.warn(\n                \"The {path} endpoint \"\n                \"was called with empty parameters: {empty_params}\".format(path=self.path, empty_params=empty_params),\n                RuntimeWarning,\n                stacklevel=5,\n            )\n\n        unfulfilled_params = {\n            param for param in self.required_params if param not in supplied_params and param not in self.default_params\n        }\n\n        if unfulfilled_params:\n            raise UnfulfilledParameterException(self.path, unfulfilled_params)\n\n        merged_params = self.default_params.copy()\n        merged_params.update(supplied_params)\n        return merged_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts JSON data from the response.", "response": "def format_response(self, response):\n        \"\"\"\n        Extracts JSON data from the response\n\n        :param requests.Response response:\n            The original response from :mod:`requests`\n        :return:\n            The response's JSON content\n        :rtype:\n            :class:`dict` if ``preserve_order`` is ``False``\n        :rtype:\n            :class:`collections.OrderedDict` if ``preserve_order`` is ``True``\n        \"\"\"\n\n        return response.json(object_pairs_hook=collections.OrderedDict if self.preserve_order else None)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pre_build_check():\n    if os.environ.get('CASS_DRIVER_NO_PRE_BUILD_CHECK'):\n        return True\n\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n        from distutils.dist import Distribution\n\n        # base build_ext just to emulate compiler option setup\n        be = build_ext(Distribution())\n        be.initialize_options()\n        be.finalize_options()\n\n        # First, make sure we have a Python include directory\n        have_python_include = any(os.path.isfile(os.path.join(p, 'Python.h')) for p in be.include_dirs)\n        if not have_python_include:\n            sys.stderr.write(\"Did not find 'Python.h' in %s.\\n\" % (be.include_dirs,))\n            return False\n\n        compiler = new_compiler(compiler=be.compiler)\n        customize_compiler(compiler)\n\n        try:\n            # We must be able to initialize the compiler if it has that method\n            if hasattr(compiler, \"initialize\"):\n                compiler.initialize()\n        except:\n            return False\n\n        executables = []\n        if compiler.compiler_type in ('unix', 'cygwin'):\n            executables = [compiler.executables[exe][0] for exe in ('compiler_so', 'linker_so')]\n        elif compiler.compiler_type == 'nt':\n            executables = [getattr(compiler, exe) for exe in ('cc', 'linker')]\n\n        if executables:\n            from distutils.spawn import find_executable\n            for exe in executables:\n                if not find_executable(exe):\n                    sys.stderr.write(\"Failed to find %s for compiler type %s.\\n\" % (exe, compiler.compiler_type))\n                    return False\n\n    except Exception as exc:\n        sys.stderr.write('%s\\n' % str(exc))\n        sys.stderr.write(\"Failed pre-build check. Attempting anyway.\\n\")\n\n    # if we are unable to positively id the compiler type, or one of these assumptions fails,\n    # just proceed as we would have without the check\n    return True", "response": "Try to verify build tools and return True if all checks are successful."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the result was applied to the LWT tree.", "response": "def check_applied(result):\n    \"\"\"\n    Raises LWTException if it looks like a failed LWT request. A LWTException\n    won't be raised in the special case in which there are several failed LWT\n    in a  :class:`~cqlengine.query.BatchQuery`.\n    \"\"\"\n    try:\n        applied = result.was_applied\n    except Exception:\n        applied = True  # result was not LWT form\n    if not applied:\n        raise LWTException(result.one())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a callback to be executed after the batch executes.", "response": "def add_callback(self, fn, *args, **kwargs):\n        \"\"\"Add a function and arguments to be passed to it to be executed after the batch executes.\n\n        A batch can support multiple callbacks.\n\n        Note, that if the batch does not execute, the callbacks are not executed.\n        A callback, thus, is an \"on batch success\" handler.\n\n        :param fn: Callable object\n        :type fn: callable\n        :param \\*args: Positional arguments to be passed to the callback at the time of execution\n        :param \\*\\*kwargs: Named arguments to be passed to the callback at the time of execution\n        \"\"\"\n        if not callable(fn):\n            raise ValueError(\"Value for argument 'fn' is {0} and is not a callable object.\".format(type(fn)))\n        self._callbacks.append((fn, args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a select clause based on the given filter args", "response": "def _select_query(self):\n        \"\"\"\n        Returns a select clause based on the given filter args\n        \"\"\"\n        if self._where:\n            self._validate_select_where()\n        return SelectStatement(\n            self.column_family_name,\n            fields=self._select_fields(),\n            where=self._where,\n            order_by=self._order,\n            limit=self._limit,\n            allow_filtering=self._allow_filtering,\n            distinct_fields=self._distinct_fields,\n            fetch_size=self._fetch_size\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fill_result_cache(self):\n\n        idx = 0\n        try:\n            while True:\n                idx += 1000\n                self._fill_result_cache_to_idx(idx)\n        except StopIteration:\n            pass\n\n        self._count = len(self._result_cache)", "response": "Fill the result cache with all results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the batch object for this query.", "response": "def batch(self, batch_obj):\n        \"\"\"\n        Set a batch object to run the query on.\n\n        Note: running a select query with a batch object will raise an exception\n        \"\"\"\n        if self._connection:\n            raise CQLEngineException(\"Cannot specify the connection on model in batch mode.\")\n\n        if batch_obj is not None and not isinstance(batch_obj, BatchQuery):\n            raise CQLEngineException('batch_obj must be a BatchQuery instance or None')\n        clone = copy.deepcopy(self)\n        clone._batch = batch_obj\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the consistency level for the operation.", "response": "def consistency(self, consistency):\n        \"\"\"\n        Sets the consistency level for the operation. See :class:`.ConsistencyLevel`.\n\n        .. code-block:: python\n\n            for user in User.objects(id=3).consistency(CL.ONE):\n                print(user)\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._consistency = consistency\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iff(self, *args, **kwargs):\n        if len([x for x in kwargs.values() if x is None]):\n            raise CQLEngineException(\"None values on iff are not allowed\")\n\n        clone = copy.deepcopy(self)\n        for operator in args:\n            if not isinstance(operator, ConditionalClause):\n                raise QueryException('{0} is not a valid query operator'.format(operator))\n            clone._conditional.append(operator)\n\n        for arg, val in kwargs.items():\n            if isinstance(val, Token):\n                raise QueryException(\"Token() values are not valid in conditionals\")\n\n            col_name, col_op = self._parse_filter_arg(arg)\n            try:\n                column = self.model._get_column(col_name)\n            except KeyError:\n                raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n\n            if isinstance(val, BaseQueryFunction):\n                query_val = val\n            else:\n                query_val = column.to_database(val)\n\n            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n            operator = operator_class()\n            clone._conditional.append(WhereClause(column.db_field_name, operator, query_val))\n\n        return clone", "response": "Adds IF statements to the queryset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, *args, **kwargs):\n        if args or kwargs:\n            return self.filter(*args, **kwargs).get()\n\n        self._execute_query()\n\n        # Check that the resultset only contains one element, avoiding sending a COUNT query\n        try:\n            self[1]\n            raise self.model.MultipleObjectsReturned('Multiple objects found')\n        except IndexError:\n            pass\n\n        try:\n            obj = self[0]\n        except IndexError:\n            raise self.model.DoesNotExist\n\n        return obj", "response": "Returns a single instance matching this query optionally with additional filter kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of rows matched by this query.", "response": "def count(self):\n        \"\"\"\n        Returns the number of rows matched by this query.\n\n        *Note: This function executes a SELECT COUNT() and has a performance cost on large datasets*\n        \"\"\"\n        if self._batch:\n            raise CQLEngineException(\"Only inserts, updates, and deletes are available in batch mode\")\n\n        if self._count is None:\n            query = self._select_query()\n            query.count = True\n            result = self._execute(query)\n            count_row = result.one().popitem()\n            self._count = count_row[1]\n        return self._count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef distinct(self, distinct_fields=None):\n\n        clone = copy.deepcopy(self)\n        if distinct_fields:\n            clone._distinct_fields = distinct_fields\n        else:\n            clone._distinct_fields = [x.column_name for x in self.model._partition_keys.values()]\n\n        return clone", "response": "Returns the DISTINCT rows matched by this query."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the number of rows that are fetched at a time.", "response": "def fetch_size(self, v):\n        \"\"\"\n        Sets the number of rows that are fetched at a time.\n\n        *Note that driver's default fetch size is 5000.*\n\n        .. code-block:: python\n\n            for user in User.objects().fetch_size(500):\n                print(user)\n        \"\"\"\n\n        if not isinstance(v, six.integer_types):\n            raise TypeError\n        if v == self._fetch_size:\n            return self\n\n        if v < 1:\n            raise QueryException(\"fetch size less than 1 is not allowed\")\n\n        clone = copy.deepcopy(self)\n        clone._fetch_size = v\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new object with the _allow_filtering property set to True.", "response": "def allow_filtering(self):\n        \"\"\"\n        Enables the (usually) unwise practive of querying on a clustering key without also defining a partition key\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._allow_filtering = True\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self):\n        # validate where clause\n        partition_keys = set(x.db_field_name for x in self.model._partition_keys.values())\n        if partition_keys - set(c.field for c in self._where):\n            raise QueryException(\"The partition key must be defined on delete queries\")\n\n        dq = DeleteStatement(\n            self.column_family_name,\n            where=self._where,\n            timestamp=self._timestamp,\n            conditionals=self._conditional,\n            if_exists=self._if_exists\n        )\n        self._execute(dq)", "response": "Deletes the contents of a query\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new instance of the current object with the specified timeout.", "response": "def timeout(self, timeout):\n        \"\"\"\n        :param timeout: Timeout for the query (in seconds)\n        :type timeout: float or None\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._timeout = timeout\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new instance of the Model class with the given keyspace and connection.", "response": "def using(self, keyspace=None, connection=None):\n        \"\"\"\n        Change the context on-the-fly of the Model class (keyspace, connection)\n        \"\"\"\n\n        if connection and self._batch:\n            raise CQLEngineException(\"Cannot specify a connection on model in batch mode.\")\n\n        clone = copy.deepcopy(self)\n        if keyspace:\n            from cassandra.cqlengine.models import _clone_model_class\n            clone.model = _clone_model_class(self.model, {'__keyspace__': keyspace})\n\n        if connection:\n            clone._connection = connection\n\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that a select statement will not create a valid select statement.", "response": "def _validate_select_where(self):\n        \"\"\" Checks that a filterset will not create invalid select statement \"\"\"\n        # check that there's either a =, a IN or a CONTAINS (collection)\n        # relationship with a primary key or indexed field. We also allow\n        # custom indexes to be queried with any operator (a difference\n        # between a secondary index)\n        equal_ops = [self.model._get_column_by_db_name(w.field) \\\n                     for w in self._where if not isinstance(w.value, Token)\n                     and (isinstance(w.operator, EqualsOperator)\n                          or self.model._get_column_by_db_name(w.field).custom_index)]\n        token_comparison = any([w for w in self._where if isinstance(w.value, Token)])\n        if not any(w.primary_key or w.has_index for w in equal_ops) and not token_comparison and not self._allow_filtering:\n            raise QueryException(\n                ('Where clauses require either  =, a IN or a CONTAINS '\n                 '(collection) comparison with either a primary key or '\n                 'indexed field. You might want to consider setting '\n                 'custom_index on fields that you manage index outside '\n                 'cqlengine.'))\n\n        if not self._allow_filtering:\n            # if the query is not on an indexed field\n            if not any(w.has_index for w in equal_ops):\n                if not any([w.partition_key for w in equal_ops]) and not token_comparison:\n                    raise QueryException(\n                        ('Filtering on a clustering key without a partition '\n                         'key is not allowed unless allow_filtering() is '\n                         'called on the queryset. You might want to consider '\n                         'setting custom_index on fields that you manage '\n                         'index outside cqlengine.'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_result_constructor(self):\n        if not self._values_list:  # we want models\n            return self.model._construct_instance\n        elif self._flat_values_list:  # the user has requested flattened list (1 value per row)\n            key = self._only_fields[0]\n            return lambda row: row[key]\n        else:\n            return lambda row: [row[f] for f in self._only_fields]", "response": "Returns a function that will be used to instantiate query results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef values_list(self, *fields, **kwargs):\n        flat = kwargs.pop('flat', False)\n        if kwargs:\n            raise TypeError('Unexpected keyword arguments to values_list: %s'\n                            % (kwargs.keys(),))\n        if flat and len(fields) > 1:\n            raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n        clone = self.only(fields)\n        clone._values_list = True\n        clone._flat_values_list = flat\n        return clone", "response": "Returns a new QuerySet with only the specified fields and values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the ttl for modified data.", "response": "def ttl(self, ttl):\n        \"\"\"\n        Sets the ttl (in seconds) for modified data.\n\n        *Note that running a select query with a ttl value will raise an exception*\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._ttl = ttl\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timestamp(self, timestamp):\n        clone = copy.deepcopy(self)\n        clone._timestamp = timestamp\n        return clone", "response": "Sets the timestamp of the record."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef if_not_exists(self):\n        if self.model._has_counter:\n            raise IfNotExistsWithCounterColumn('if_not_exists cannot be used with tables containing counter columns')\n        clone = copy.deepcopy(self)\n        clone._if_not_exists = True\n        return clone", "response": "Return a new object with the _if_not_exists attribute set to True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new object with the existence of the object.", "response": "def if_exists(self):\n        \"\"\"\n        Check the existence of an object before an update or delete.\n\n        If the update or delete isn't applied, a LWTException is raised.\n        \"\"\"\n        if self.model._has_counter:\n            raise IfExistsWithCounterColumn('if_exists cannot be used with tables containing counter columns')\n        clone = copy.deepcopy(self)\n        clone._if_exists = True\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the values of the column with the given values.", "response": "def update(self, **values):\n        \"\"\"\n        Performs an update on the row selected by the queryset. Include values to update in the\n        update like so:\n\n        .. code-block:: python\n\n            Model.objects(key=n).update(value='x')\n\n        Passing in updates for columns which are not part of the model will raise a ValidationError.\n\n        Per column validation will be performed, but instance level validation will not\n        (i.e., `Model.validate` is not called).  This is sometimes referred to as a blind update.\n\n        For example:\n\n        .. code-block:: python\n\n            class User(Model):\n                id = Integer(primary_key=True)\n                name = Text()\n\n            setup([\"localhost\"], \"test\")\n            sync_table(User)\n\n            u = User.create(id=1, name=\"jon\")\n\n            User.objects(id=1).update(name=\"Steve\")\n\n            # sets name to null\n            User.objects(id=1).update(name=None)\n\n\n        Also supported is blindly adding and removing elements from container columns,\n        without loading a model instance from Cassandra.\n\n        Using the syntax `.update(column_name={x, y, z})` will overwrite the contents of the container, like updating a\n        non container column. However, adding `__<operation>` to the end of the keyword arg, makes the update call add\n        or remove items from the collection, without overwriting then entire column.\n\n        Given the model below, here are the operations that can be performed on the different container columns:\n\n        .. code-block:: python\n\n            class Row(Model):\n                row_id      = columns.Integer(primary_key=True)\n                set_column  = columns.Set(Integer)\n                list_column = columns.List(Integer)\n                map_column  = columns.Map(Integer, Integer)\n\n        :class:`~cqlengine.columns.Set`\n\n        - `add`: adds the elements of the given set to the column\n        - `remove`: removes the elements of the given set to the column\n\n\n        .. code-block:: python\n\n            # add elements to a set\n            Row.objects(row_id=5).update(set_column__add={6})\n\n            # remove elements to a set\n            Row.objects(row_id=5).update(set_column__remove={4})\n\n        :class:`~cqlengine.columns.List`\n\n        - `append`: appends the elements of the given list to the end of the column\n        - `prepend`: prepends the elements of the given list to the beginning of the column\n\n        .. code-block:: python\n\n            # append items to a list\n            Row.objects(row_id=5).update(list_column__append=[6, 7])\n\n            # prepend items to a list\n            Row.objects(row_id=5).update(list_column__prepend=[1, 2])\n\n\n        :class:`~cqlengine.columns.Map`\n\n        - `update`: adds the given keys/values to the columns, creating new entries if they didn't exist, and overwriting old ones if they did\n\n        .. code-block:: python\n\n            # add items to a map\n            Row.objects(row_id=5).update(map_column__update={1: 2, 3: 4})\n\n            # remove items from a map\n            Row.objects(row_id=5).update(map_column__remove={1, 2})\n        \"\"\"\n        if not values:\n            return\n\n        nulled_columns = set()\n        updated_columns = set()\n        us = UpdateStatement(self.column_family_name, where=self._where, ttl=self._ttl,\n                             timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, val in values.items():\n            col_name, col_op = self._parse_filter_arg(name)\n            col = self.model._columns.get(col_name)\n            # check for nonexistant columns\n            if col is None:\n                raise ValidationError(\"{0}.{1} has no column named: {2}\".format(self.__module__, self.model.__name__, col_name))\n            # check for primary key update attempts\n            if col.is_primary_key:\n                raise ValidationError(\"Cannot apply update to primary key '{0}' for {1}.{2}\".format(col_name, self.__module__, self.model.__name__))\n\n            if col_op == 'remove' and isinstance(col, columns.Map):\n                if not isinstance(val, set):\n                    raise ValidationError(\n                        \"Cannot apply update operation '{0}' on column '{1}' with value '{2}'. A set is required.\".format(col_op, col_name, val))\n                val = {v: None for v in val}\n            else:\n                # we should not provide default values in this use case.\n                val = col.validate(val)\n\n            if val is None:\n                nulled_columns.add(col_name)\n                continue\n\n            us.add_update(col, val, operation=col_op)\n            updated_columns.add(col_name)\n\n        if us.assignments:\n            self._execute(us)\n\n        if nulled_columns:\n            delete_conditional = [condition for condition in self._conditional\n                                  if condition.field not in updated_columns] if self._conditional else None\n            ds = DeleteStatement(self.column_family_name, fields=nulled_columns,\n                                 where=self._where, conditionals=delete_conditional, if_exists=self._if_exists)\n            self._execute(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a delete query to remove columns that have changed to null.", "response": "def _delete_null_columns(self, conditionals=None):\n        \"\"\"\n        executes a delete query to remove columns that have changed to null\n        \"\"\"\n        ds = DeleteStatement(self.column_family_name, conditionals=conditionals, if_exists=self._if_exists)\n        deleted_fields = False\n        static_only = True\n        for _, v in self.instance._values.items():\n            col = v.column\n            if v.deleted:\n                ds.add_field(col.db_field_name)\n                deleted_fields = True\n                static_only &= col.static\n            elif isinstance(col, columns.Map):\n                uc = MapDeleteClause(col.db_field_name, v.value, v.previous_value)\n                if uc.get_context_size() > 0:\n                    ds.add_field(uc)\n                    deleted_fields = True\n                    static_only |= col.static\n\n        if deleted_fields:\n            keys = self.model._partition_keys if static_only else self.model._primary_keys\n            for name, col in keys.items():\n                ds.add_where(col, EqualsOperator(), getattr(self.instance, name))\n            self._execute(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate a row in the database.", "response": "def update(self):\n        \"\"\"\n        updates a row.\n        This is a blind update call.\n        All validation and cleaning needs to happen\n        prior to calling this.\n        \"\"\"\n        if self.instance is None:\n            raise CQLEngineException(\"DML Query intance attribute is None\")\n        assert type(self.instance) == self.model\n        null_clustering_key = False if len(self.instance._clustering_keys) == 0 else True\n        static_changed_only = True\n        statement = UpdateStatement(self.column_family_name, ttl=self._ttl, timestamp=self._timestamp,\n                                    conditionals=self._conditional, if_exists=self._if_exists)\n        for name, col in self.instance._clustering_keys.items():\n            null_clustering_key = null_clustering_key and col._val_is_null(getattr(self.instance, name, None))\n\n        updated_columns = set()\n        # get defined fields and their column names\n        for name, col in self.model._columns.items():\n            # if clustering key is null, don't include non static columns\n            if null_clustering_key and not col.static and not col.partition_key:\n                continue\n            if not col.is_primary_key:\n                val = getattr(self.instance, name, None)\n                val_mgr = self.instance._values[name]\n\n                if val is None:\n                    continue\n\n                if not val_mgr.changed and not isinstance(col, columns.Counter):\n                    continue\n\n                static_changed_only = static_changed_only and col.static\n                statement.add_update(col, val, previous=val_mgr.previous_value)\n                updated_columns.add(col.db_field_name)\n\n        if statement.assignments:\n            for name, col in self.model._primary_keys.items():\n                # only include clustering key if clustering key is not null, and non static columns are changed to avoid cql error\n                if (null_clustering_key or static_changed_only) and (not col.partition_key):\n                    continue\n                statement.add_where(col, EqualsOperator(), getattr(self.instance, name))\n            self._execute(statement)\n\n        if not null_clustering_key:\n            # remove conditions on fields that have been updated\n            delete_conditionals = [condition for condition in self._conditional\n                                   if condition.field not in updated_columns] if self._conditional else None\n            self._delete_null_columns(delete_conditionals)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating or updates a row.", "response": "def save(self):\n        \"\"\"\n        Creates / updates a row.\n        This is a blind insert call.\n        All validation and cleaning needs to happen\n        prior to calling this.\n        \"\"\"\n        if self.instance is None:\n            raise CQLEngineException(\"DML Query intance attribute is None\")\n        assert type(self.instance) == self.model\n\n        nulled_fields = set()\n        if self.instance._has_counter or self.instance._can_update():\n            if self.instance._has_counter:\n                warn(\"'create' and 'save' actions on Counters are deprecated. It will be disallowed in 4.0. \"\n                    \"Use the 'update' mechanism instead.\", DeprecationWarning)\n            return self.update()\n        else:\n            insert = InsertStatement(self.column_family_name, ttl=self._ttl, timestamp=self._timestamp, if_not_exists=self._if_not_exists)\n            static_save_only = False if len(self.instance._clustering_keys) == 0 else True\n            for name, col in self.instance._clustering_keys.items():\n                static_save_only = static_save_only and col._val_is_null(getattr(self.instance, name, None))\n            for name, col in self.instance._columns.items():\n                if static_save_only and not col.static and not col.partition_key:\n                    continue\n                val = getattr(self.instance, name, None)\n                if col._val_is_null(val):\n                    if self.instance._values[name].changed:\n                        nulled_fields.add(col.db_field_name)\n                    continue\n                if col.has_default and not self.instance._values[name].changed:\n                    # Ensure default columns included in a save() are marked as explicit, to get them *persisted* properly\n                    self.instance._values[name].explicit = True\n                insert.add_assignment(col, getattr(self.instance, name, None))\n\n        # skip query execution if it's empty\n        # caused by pointless update queries\n        if not insert.is_empty:\n            self._execute(insert)\n        # delete any nulled columns\n        if not static_save_only:\n            self._delete_null_columns()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting one instance from the database.", "response": "def delete(self):\n        \"\"\" Deletes one instance \"\"\"\n        if self.instance is None:\n            raise CQLEngineException(\"DML Query instance attribute is None\")\n\n        ds = DeleteStatement(self.column_family_name, timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, col in self.model._primary_keys.items():\n            val = getattr(self.instance, name)\n            if val is None and not col.partition_key:\n                continue\n            ds.add_where(col, EqualsOperator(), val)\n        self._execute(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the metrics stats name.", "response": "def set_stats_name(self, stats_name):\n        \"\"\"\n        Set the metrics stats name.\n        The stats_name is a string used to access the metris through scales: scales.getStats()[<stats_name>]\n        Default is 'cassandra-<num>'.\n        \"\"\"\n\n        if self.stats_name == stats_name:\n            return\n\n        if stats_name in scales._Stats.stats:\n            raise ValueError('\"{0}\" already exists in stats.'.format(stats_name))\n\n        stats = scales._Stats.stats[self.stats_name]\n        del scales._Stats.stats[self.stats_name]\n        self.stats_name = stats_name\n        scales._Stats.stats[self.stats_name] = stats"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a namedtuple for each row in the rows.", "response": "def named_tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a `namedtuple <https://docs.python.org/2/library/collections.html#collections.namedtuple>`_.\n    This is the default row factory.\n\n    Example::\n\n        >>> from cassandra.query import named_tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = named_tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> user = rows[0]\n\n        >>> # you can access field by their name:\n        >>> print \"name: %s, age: %d\" % (user.name, user.age)\n        name: Bob, age: 42\n\n        >>> # or you can access fields by their position (like a tuple)\n        >>> name, age = user\n        >>> print \"name: %s, age: %d\" % (name, age)\n        name: Bob, age: 42\n        >>> name = user[0]\n        >>> age = user[1]\n        >>> print \"name: %s, age: %d\" % (name, age)\n        name: Bob, age: 42\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    clean_column_names = map(_clean_column_name, colnames)\n    try:\n        Row = namedtuple('Row', clean_column_names)\n    except SyntaxError:\n        warnings.warn(\n            \"Failed creating namedtuple for a result because there were too \"\n            \"many columns. This is due to a Python limitation that affects \"\n            \"namedtuple in Python 3.0-3.6 (see issue18896). The row will be \"\n            \"created with {substitute_factory_name}, which lacks some namedtuple \"\n            \"features and is slower. To avoid slower performance accessing \"\n            \"values on row objects, Upgrade to Python 3.7, or use a different \"\n            \"row factory. (column names: {colnames})\".format(\n                substitute_factory_name=pseudo_namedtuple_factory.__name__,\n                colnames=colnames\n            )\n        )\n        return pseudo_namedtuple_factory(colnames, rows)\n    except Exception:\n        clean_column_names = list(map(_clean_column_name, colnames))  # create list because py3 map object will be consumed by first attempt\n        log.warning(\"Failed creating named tuple for results with column names %s (cleaned: %s) \"\n                    \"(see Python 'namedtuple' documentation for details on name rules). \"\n                    \"Results will be returned with positional names. \"\n                    \"Avoid this by choosing different names, using SELECT \\\"<col name>\\\" AS aliases, \"\n                    \"or specifying a different row_factory on your Session\" %\n                    (colnames, clean_column_names))\n        Row = namedtuple('Row', _sanitize_identifiers(clean_column_names))\n\n    return [Row(*row) for row in rows]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbinds a sequence of values to the prepared statement parameters and returns this instance.", "response": "def bind(self, values):\n        \"\"\"\n        Binds a sequence of values for the prepared statement parameters\n        and returns this instance.  Note that `values` *must* be:\n\n        * a sequence, even if you are only binding one value, or\n        * a dict that relates 1-to-1 between dict keys and columns\n\n        .. versionchanged:: 2.6.0\n\n            :data:`~.UNSET_VALUE` was introduced. These can be bound as positional parameters\n            in a sequence, or by name in a dict. Additionally, when using protocol v4+:\n\n            * short sequences will be extended to match bind parameters with UNSET_VALUE\n            * names may be omitted from a dict with UNSET_VALUE implied.\n\n        .. versionchanged:: 3.0.0\n\n            method will not throw if extra keys are present in bound dict (PYTHON-178)\n        \"\"\"\n        if values is None:\n            values = ()\n        proto_version = self.prepared_statement.protocol_version\n        col_meta = self.prepared_statement.column_metadata\n\n        # special case for binding dicts\n        if isinstance(values, dict):\n            values_dict = values\n            values = []\n\n            # sort values accordingly\n            for col in col_meta:\n                try:\n                    values.append(values_dict[col.name])\n                except KeyError:\n                    if proto_version >= 4:\n                        values.append(UNSET_VALUE)\n                    else:\n                        raise KeyError(\n                            'Column name `%s` not found in bound dict.' %\n                            (col.name))\n\n        value_len = len(values)\n        col_meta_len = len(col_meta)\n\n        if value_len > col_meta_len:\n            raise ValueError(\n                \"Too many arguments provided to bind() (got %d, expected %d)\" %\n                (len(values), len(col_meta)))\n\n        # this is fail-fast for clarity pre-v4. When v4 can be assumed,\n        # the error will be better reported when UNSET_VALUE is implicitly added.\n        if proto_version < 4 and self.prepared_statement.routing_key_indexes and \\\n           value_len < len(self.prepared_statement.routing_key_indexes):\n            raise ValueError(\n                \"Too few arguments provided to bind() (got %d, required %d for routing key)\" %\n                (value_len, len(self.prepared_statement.routing_key_indexes)))\n\n        self.raw_values = values\n        self.values = []\n        for value, col_spec in zip(values, col_meta):\n            if value is None:\n                self.values.append(None)\n            elif value is UNSET_VALUE:\n                if proto_version >= 4:\n                    self._append_unset_value()\n                else:\n                    raise ValueError(\"Attempt to bind UNSET_VALUE while using unsuitable protocol version (%d < 4)\" % proto_version)\n            else:\n                try:\n                    self.values.append(col_spec.type.serialize(value, proto_version))\n                except (TypeError, struct.error) as exc:\n                    actual_type = type(value)\n                    message = ('Received an argument of invalid type for column \"%s\". '\n                               'Expected: %s, Got: %s; (%s)' % (col_spec.name, col_spec.type, actual_type, exc))\n                    raise TypeError(message)\n\n        if proto_version >= 4:\n            diff = col_meta_len - len(self.values)\n            if diff:\n                for _ in range(diff):\n                    self._append_unset_value()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self):\n        del self._statements_and_parameters[:]\n        self.keyspace = None\n        self.routing_key = None\n        if self.custom_payload:\n            self.custom_payload.clear()", "response": "Clears the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, statement, parameters=None):\n        if isinstance(statement, six.string_types):\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                statement = bind_params(statement, parameters, encoder)\n            self._add_statement_and_params(False, statement, ())\n        elif isinstance(statement, PreparedStatement):\n            query_id = statement.query_id\n            bound_statement = statement.bind(() if parameters is None else parameters)\n            self._update_state(bound_statement)\n            self._add_statement_and_params(True, query_id, bound_statement.values)\n        elif isinstance(statement, BoundStatement):\n            if parameters:\n                raise ValueError(\n                    \"Parameters cannot be passed with a BoundStatement \"\n                    \"to BatchStatement.add()\")\n            self._update_state(statement)\n            self._add_statement_and_params(True, statement.prepared_statement.query_id, statement.values)\n        else:\n            # it must be a SimpleStatement\n            query_string = statement.query_string\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                query_string = bind_params(query_string, parameters, encoder)\n            self._update_state(statement)\n            self._add_statement_and_params(False, query_string, ())\n        return self", "response": "Adds a statement and optional sequence of parameters to the batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_all(self, statements, parameters):\n        for statement, value in zip(statements, parameters):\n            self.add(statement, value)", "response": "Adds a sequence of statements and a matching sequence\n        of parameters to the batch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the actual tracing details from Cassandra and populates the trace attributes of this instance with the actual tracing details.", "response": "def populate(self, max_wait=2.0, wait_for_complete=True, query_cl=None):\n        \"\"\"\n        Retrieves the actual tracing details from Cassandra and populates the\n        attributes of this instance.  Because tracing details are stored\n        asynchronously by Cassandra, this may need to retry the session\n        detail fetch.  If the trace is still not available after `max_wait`\n        seconds, :exc:`.TraceUnavailable` will be raised; if `max_wait` is\n        :const:`None`, this will retry forever.\n\n        `wait_for_complete=False` bypasses the wait for duration to be populated.\n        This can be used to query events from partial sessions.\n\n        `query_cl` specifies a consistency level to use for polling the trace tables,\n        if it should be different than the session default.\n        \"\"\"\n        attempt = 0\n        start = time.time()\n        while True:\n            time_spent = time.time() - start\n            if max_wait is not None and time_spent >= max_wait:\n                raise TraceUnavailable(\n                    \"Trace information was not available within %f seconds. Consider raising Session.max_trace_wait.\" % (max_wait,))\n\n            log.debug(\"Attempting to fetch trace info for trace ID: %s\", self.trace_id)\n            session_results = self._execute(\n                SimpleStatement(self._SELECT_SESSIONS_FORMAT, consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n\n            # PYTHON-730: There is race condition that the duration mutation is written before started_at the for fast queries\n            is_complete = session_results and session_results[0].duration is not None and session_results[0].started_at is not None\n            if not session_results or (wait_for_complete and not is_complete):\n                time.sleep(self._BASE_RETRY_SLEEP * (2 ** attempt))\n                attempt += 1\n                continue\n            if is_complete:\n                log.debug(\"Fetched trace info for trace ID: %s\", self.trace_id)\n            else:\n                log.debug(\"Fetching parital trace info for trace ID: %s\", self.trace_id)\n\n            session_row = session_results[0]\n            self.request_type = session_row.request\n            self.duration = timedelta(microseconds=session_row.duration) if is_complete else None\n            self.started_at = session_row.started_at\n            self.coordinator = session_row.coordinator\n            self.parameters = session_row.parameters\n            # since C* 2.2\n            self.client = getattr(session_row, 'client', None)\n\n            log.debug(\"Attempting to fetch trace events for trace ID: %s\", self.trace_id)\n            time_spent = time.time() - start\n            event_results = self._execute(\n                SimpleStatement(self._SELECT_EVENTS_FORMAT, consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n            log.debug(\"Fetched trace events for trace ID: %s\", self.trace_id)\n            self.events = tuple(TraceEvent(r.activity, r.event_id, r.source, r.source_elapsed, r.thread)\n                                for r in event_results)\n            break"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef type_name(cls):\n        if cls.__type_name__:\n            type_name = cls.__type_name__.lower()\n        else:\n            camelcase = re.compile(r'([a-z])([A-Z])')\n            ccase = lambda s: camelcase.sub(lambda v: '{0}_{1}'.format(v.group(1), v.group(2)), s)\n\n            type_name = ccase(cls.__name__)\n            # trim to less than 48 characters or cassandra will complain\n            type_name = type_name[-48:]\n            type_name = type_name.lower()\n            type_name = re.sub(r'^_+', '', type_name)\n            cls.__type_name__ = type_name\n\n        return type_name", "response": "Returns the type name of the current class name if it s not yet defined."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate(self):\n        for name, field in self._fields.items():\n            v = getattr(self, name)\n            if v is None and not self._values[name].explicit and field.has_default:\n                v = field.get_default()\n            val = field.validate(v)\n            setattr(self, name, val)", "response": "Validate the values of the related objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nindicate whether or not this value has changed.", "response": "def changed(self):\n        \"\"\"\n        Indicates whether or not this value has changed.\n\n        :rtype: boolean\n\n        \"\"\"\n        if self.explicit:\n            return self.value != self.previous_value\n\n        if isinstance(self.column, BaseContainerColumn):\n            default_value = self.column.get_default()\n            if self.column._val_is_null(default_value):\n                return not self.column._val_is_null(self.value) and self.value != self.previous_value\n            elif self.previous_value is None:\n                return self.value != default_value\n\n            return self.value != self.previous_value\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_column_def(self):\n        static = \"static\" if self.static else \"\"\n        return '{0} {1} {2}'.format(self.cql, self.db_type, static)", "response": "Returns a column definition for CQL table definition\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate(self, value):\n        value = super(Boolean, self).validate(value)\n\n        if value is not None:\n            value = bool(value)\n\n        return value", "response": "Always returns a Python boolean."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef uuid_from_time(time_arg, node=None, clock_seq=None):\n    if hasattr(time_arg, 'utctimetuple'):\n        seconds = int(calendar.timegm(time_arg.utctimetuple()))\n        microseconds = (seconds * 1e6) + time_arg.time().microsecond\n    else:\n        microseconds = int(time_arg * 1e6)\n\n    # 0x01b21dd213814000 is the number of 100-ns intervals between the\n    # UUID epoch 1582-10-15 00:00:00 and the Unix epoch 1970-01-01 00:00:00.\n    intervals = int(microseconds * 10) + 0x01b21dd213814000\n\n    time_low = intervals & 0xffffffff\n    time_mid = (intervals >> 32) & 0xffff\n    time_hi_version = (intervals >> 48) & 0x0fff\n\n    if clock_seq is None:\n        clock_seq = random.getrandbits(14)\n    else:\n        if clock_seq > 0x3fff:\n            raise ValueError('clock_seq is out of range (need a 14-bit value)')\n\n    clock_seq_low = clock_seq & 0xff\n    clock_seq_hi_variant = 0x80 | ((clock_seq >> 8) & 0x3f)\n\n    if node is None:\n        node = random.getrandbits(48)\n\n    return uuid.UUID(fields=(time_low, time_mid, time_hi_version,\n                             clock_seq_hi_variant, clock_seq_low, node), version=1)", "response": "Converts a datetime or timestamp to a type 1 UUID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_context(keyspaces, connections):\n\n    if keyspaces:\n        if not isinstance(keyspaces, (list, tuple)):\n            raise ValueError('keyspaces must be a list or a tuple.')\n\n    if connections:\n        if not isinstance(connections, (list, tuple)):\n            raise ValueError('connections must be a list or a tuple.')\n\n    keyspaces = keyspaces if keyspaces else [None]\n    connections = connections if connections else [None]\n\n    return product(connections, keyspaces)", "response": "Return all the execution contexts"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a keyspace with SimpleStrategy for replica placement.", "response": "def create_keyspace_simple(name, replication_factor, durable_writes=True, connections=None):\n    \"\"\"\n    Creates a keyspace with SimpleStrategy for replica placement\n\n    If the keyspace already exists, it will not be modified.\n\n    **This function should be used with caution, especially in production environments.\n    Take care to execute schema modifications in a single context (i.e. not concurrently with other clients).**\n\n    *There are plans to guard schema-modifying functions with an environment-driven conditional.*\n\n    :param str name: name of keyspace to create\n    :param int replication_factor: keyspace replication factor, used with :attr:`~.SimpleStrategy`\n    :param bool durable_writes: Write log is bypassed if set to False\n    :param list connections: List of connection names\n    \"\"\"\n    _create_keyspace(name, durable_writes, 'SimpleStrategy',\n                     {'replication_factor': replication_factor}, connections=connections)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_keyspace_network_topology(name, dc_replication_map, durable_writes=True, connections=None):\n    _create_keyspace(name, durable_writes, 'NetworkTopologyStrategy', dc_replication_map, connections=connections)", "response": "Create a keyspace with NetworkTopologyStrategy for replica placement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndrop a keyspace if it exists.", "response": "def drop_keyspace(name, connections=None):\n    \"\"\"\n    Drops a keyspace, if it exists.\n\n    *There are plans to guard schema-modifying functions with an environment-driven conditional.*\n\n    **This function should be used with caution, especially in production environments.\n    Take care to execute schema modifications in a single context (i.e. not concurrently with other clients).**\n\n    :param str name: name of keyspace to drop\n    :param list connections: List of connection names\n    \"\"\"\n    if not _allow_schema_modification():\n        return\n\n    if connections:\n        if not isinstance(connections, (list, tuple)):\n            raise ValueError('Connections must be a list or a tuple.')\n\n    def _drop_keyspace(name, connection=None):\n        cluster = get_cluster(connection)\n        if name in cluster.metadata.keyspaces:\n            execute(\"DROP KEYSPACE {0}\".format(metadata.protect_name(name)), connection=connection)\n\n    if connections:\n        for connection in connections:\n            _drop_keyspace(name, connection)\n    else:\n        _drop_keyspace(name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_index_name_by_column(table, column_name):\n    protected_name = metadata.protect_name(column_name)\n    possible_index_values = [protected_name, \"values(%s)\" % protected_name]\n    for index_metadata in table.indexes.values():\n        options = dict(index_metadata.index_options)\n        if options.get('target') in possible_index_values:\n            return index_metadata.name", "response": "Find the index name for a given table and column name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sync_table(model, keyspaces=None, connections=None):\n\n    context = _get_context(keyspaces, connections)\n    for connection, keyspace in context:\n        with query.ContextQuery(model, keyspace=keyspace) as m:\n            _sync_table(m, connection=connection)", "response": "Synchronizes the table with the given model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninspect the type_model and creates / updates the corresponding type. Note that the attributes removed from the type_model are not deleted on the database (this operation is not supported). They become effectively ignored by (will not show up on) the type_model. **This function should be used with caution, especially in production environments. Take care to execute schema modifications in a single context (i.e. not concurrently with other clients).** *There are plans to guard schema-modifying functions with an environment-driven conditional.*", "response": "def sync_type(ks_name, type_model, connection=None):\n    \"\"\"\n    Inspects the type_model and creates / updates the corresponding type.\n\n    Note that the attributes removed from the type_model are not deleted on the database (this operation is not supported).\n    They become effectively ignored by (will not show up on) the type_model.\n\n    **This function should be used with caution, especially in production environments.\n    Take care to execute schema modifications in a single context (i.e. not concurrently with other clients).**\n\n    *There are plans to guard schema-modifying functions with an environment-driven conditional.*\n    \"\"\"\n    if not _allow_schema_modification():\n        return\n\n    if not issubclass(type_model, UserType):\n        raise CQLEngineException(\"Types must be derived from base UserType.\")\n\n    _sync_type(ks_name, type_model, connection=connection)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_options(model, connection=None):\n    ks_name = model._get_keyspace()\n    msg = format_log_context(\"Checking %s for option differences\", keyspace=ks_name, connection=connection)\n    log.debug(msg, model)\n    model_options = model.__options__ or {}\n\n    table_meta = _get_table_metadata(model, connection=connection)\n    # go to CQL string first to normalize meta from different versions\n    existing_option_strings = set(table_meta._make_option_strings(table_meta.options))\n    existing_options = _options_map_from_strings(existing_option_strings)\n    model_option_strings = metadata.TableMetadataV3._make_option_strings(model_options)\n    model_options = _options_map_from_strings(model_option_strings)\n\n    update_options = {}\n    for name, value in model_options.items():\n        try:\n            existing_value = existing_options[name]\n        except KeyError:\n            msg = format_log_context(\"Invalid table option: '%s'; known options: %s\", keyspace=ks_name, connection=connection)\n            raise KeyError(msg % (name, existing_options.keys()))\n        if isinstance(existing_value, six.string_types):\n            if value != existing_value:\n                update_options[name] = value\n        else:\n            try:\n                for k, v in value.items():\n                    if existing_value[k] != v:\n                        update_options[name] = value\n                        break\n            except KeyError:\n                update_options[name] = value\n\n    if update_options:\n        options = ' AND '.join(metadata.TableMetadataV3._make_option_strings(update_options))\n        query = \"ALTER TABLE {0} WITH {1}\".format(model.column_family_name(), options)\n        execute(query, connection=connection)\n        return True\n\n    return False", "response": "Updates the table options for the given model if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef drop_table(model, keyspaces=None, connections=None):\n\n    context = _get_context(keyspaces, connections)\n    for connection, keyspace in context:\n        with query.ContextQuery(model, keyspace=keyspace) as m:\n            _drop_table(m, connection=connection)", "response": "Drops the table indicated by the model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dataReceived(self, data):\n        self.connection._iobuf.write(data)\n        self.connection.handle_read()", "response": "This is the callback function that is called when data has been received on the connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connectionMade(self):\n        try:\n            # Non SSL connection\n            self.connection = self.transport.connector.factory.conn\n        except AttributeError:\n            # SSL connection\n            self.connection = self.transport.connector.factory.wrappedFactory.conn\n\n        self.connection.client_connection_made(self.transport)", "response": "Callback function that is called when a connection has succeeded."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clientConnectionFailed(self, connector, reason):\n        log.debug(\"Connect failed: %s\", reason)\n        self.conn.defunct(reason.value)", "response": "Called when the connection attempt fails."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_connection(self):\n        if self.ssl_options:\n\n            if not _HAS_SSL:\n                raise ImportError(\n                    str(e) +\n                    ', pyOpenSSL must be installed to enable SSL support with the Twisted event loop'\n                )\n\n            self.connector = reactor.connectSSL(\n                host=self.endpoint.address, port=self.port,\n                factory=TwistedConnectionClientFactory(self),\n                contextFactory=_SSLContextFactory(self.ssl_options, self._check_hostname, self.endpoint.address),\n                timeout=self.connect_timeout)\n        else:\n            self.connector = reactor.connectTCP(\n                host=self.endpoint.address, port=self.port,\n                factory=TwistedConnectionClientFactory(self),\n                timeout=self.connect_timeout)", "response": "This method adds a connection to the Twisted connection pool."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef client_connection_made(self, transport):\n        with self.lock:\n            self.is_closed = False\n        self.transport = transport\n        self._send_options_message()", "response": "Called by twisted when a connection attempt has been made."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisconnecting and error - out all requests.", "response": "def close(self):\n        \"\"\"\n        Disconnect and error-out all requests.\n        \"\"\"\n        with self.lock:\n            if self.is_closed:\n                return\n            self.is_closed = True\n\n        log.debug(\"Closing connection (%s) to %s\", id(self), self.endpoint)\n        reactor.callFromThread(self.connector.disconnect)\n        log.debug(\"Closed socket to %s\", self.endpoint)\n\n        if not self.is_defunct:\n            self.error_all_requests(\n                ConnectionShutdown(\"Connection to %s was closed\" % self.endpoint))\n            # don't leave in-progress operations hanging\n            self.connected_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the next timestamp that should be used if now is greater than last.", "response": "def _next_timestamp(self, now, last):\n        \"\"\"\n        Returns the timestamp that should be used if ``now`` is the current\n        time and ``last`` is the last timestamp returned by this object.\n        Intended for internal and testing use only; to generate timestamps,\n        call an instantiated ``MonotonicTimestampGenerator`` object.\n\n        :param int now: an integer to be used as the current time, typically\n            representing the current time in microseconds since the UNIX epoch\n        :param int last: an integer representing the last timestamp returned by\n            this object\n        \"\"\"\n        if now > last:\n            self.last = now\n            return now\n        else:\n            self._maybe_warn(now=now)\n            self.last = last + 1\n            return self.last"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct an instance of the class cls from the values dict", "response": "def _construct_instance(cls, values):\n        \"\"\"\n        method used to construct instances from query results\n        this is where polymorphic deserialization occurs\n        \"\"\"\n        # we're going to take the values, which is from the DB as a dict\n        # and translate that into our local fields\n        # the db_map is a db_field -> model field map\n        if cls._db_map:\n            values = dict((cls._db_map.get(k, k), v) for k, v in values.items())\n\n        if cls._is_polymorphic:\n            disc_key = values.get(cls._discriminator_column_name)\n\n            if disc_key is None:\n                raise PolymorphicModelException('discriminator value was not found in values')\n\n            poly_base = cls if cls._is_polymorphic_base else cls._polymorphic_base\n\n            klass = poly_base._get_model_by_discriminator_value(disc_key)\n            if klass is None:\n                poly_base._discover_polymorphic_submodels()\n                klass = poly_base._get_model_by_discriminator_value(disc_key)\n                if klass is None:\n                    raise PolymorphicModelException(\n                        'unrecognized discriminator column {0} for class {1}'.format(disc_key, poly_base.__name__)\n                    )\n\n            if not issubclass(klass, cls):\n                raise PolymorphicModelException(\n                    '{0} is not a subclass of {1}'.format(klass.__name__, cls.__name__)\n                )\n\n            values = dict((k, v) for k, v in values.items() if k in klass._columns.keys())\n\n        else:\n            klass = cls\n\n        instance = klass(**values)\n        instance._set_persisted(force=True)\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the column mapped by db_field name", "response": "def _get_column_by_db_name(cls, name):\n        \"\"\"\n        Returns the column, mapped by db_field name\n        \"\"\"\n        return cls._columns.get(cls._db_map.get(name, name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the column family name of the class if it s not yet defined.", "response": "def column_family_name(cls, include_keyspace=True):\n        \"\"\"\n        Returns the column family name if it's been defined\n        otherwise, it creates it from the module and class name\n        \"\"\"\n        cf_name = protect_name(cls._raw_column_family_name())\n        if include_keyspace:\n            keyspace = cls._get_keyspace()\n            if not keyspace:\n                raise CQLEngineException(\"Model keyspace is not set and no default is available. Set model keyspace or setup connection before attempting to generate a query.\")\n            return '{0}.{1}'.format(protect_name(keyspace), cf_name)\n\n        return cf_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self):\n        for name, col in self._columns.items():\n            v = getattr(self, name)\n            if v is None and not self._values[name].explicit and col.has_default:\n                v = col.get_default()\n            val = col.validate(v)\n            self._set_column_value(name, val)", "response": "Validate the values of the related items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _as_dict(self):\n        values = self._dynamic_columns or {}\n        for name, col in self._columns.items():\n            values[name] = col.to_database(getattr(self, name, None))\n        return values", "response": "Returns a dictionary of column names to cleaned values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instance of this model in the database.", "response": "def create(cls, **kwargs):\n        \"\"\"\n        Create an instance of this model in the database.\n\n        Takes the model column values as keyword arguments. Setting a value to\n        `None` is equivalent to running a CQL `DELETE` on that column.\n\n        Returns the instance.\n        \"\"\"\n        extra_columns = set(kwargs.keys()) - set(cls._columns.keys())\n        if extra_columns:\n            raise ValidationError(\"Incorrect columns passed: {0}\".format(extra_columns))\n        return cls.objects.create(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self):\n\n        # handle polymorphic models\n        if self._is_polymorphic:\n            if self._is_polymorphic_base:\n                raise PolymorphicModelException('cannot save polymorphic base model')\n            else:\n                setattr(self, self._discriminator_column_name, self.__discriminator_value__)\n\n        self.validate()\n        self.__dmlquery__(self.__class__, self,\n                          batch=self._batch,\n                          ttl=self._ttl,\n                          timestamp=self._timestamp,\n                          consistency=self.__consistency__,\n                          if_not_exists=self._if_not_exists,\n                          conditional=self._conditional,\n                          timeout=self._timeout,\n                          if_exists=self._if_exists).save()\n\n        self._set_persisted()\n\n        self._timestamp = None\n\n        return self", "response": "Save an object to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **values):\n        for column_id, v in values.items():\n            col = self._columns.get(column_id)\n\n            # check for nonexistant columns\n            if col is None:\n                raise ValidationError(\n                    \"{0}.{1} has no column named: {2}\".format(\n                        self.__module__, self.__class__.__name__, column_id))\n\n            # check for primary key update attempts\n            if col.is_primary_key:\n                current_value = getattr(self, column_id)\n                if v != current_value:\n                    raise ValidationError(\n                        \"Cannot apply update to primary key '{0}' for {1}.{2}\".format(\n                            column_id, self.__module__, self.__class__.__name__))\n\n            setattr(self, column_id, v)\n\n        # handle polymorphic models\n        if self._is_polymorphic:\n            if self._is_polymorphic_base:\n                raise PolymorphicModelException('cannot update polymorphic base model')\n            else:\n                setattr(self, self._discriminator_column_name, self.__discriminator_value__)\n\n        self.validate()\n        self.__dmlquery__(self.__class__, self,\n                          batch=self._batch,\n                          ttl=self._ttl,\n                          timestamp=self._timestamp,\n                          consistency=self.__consistency__,\n                          conditional=self._conditional,\n                          timeout=self._timeout,\n                          if_exists=self._if_exists).update()\n\n        self._set_persisted()\n\n        self._timestamp = None\n\n        return self", "response": "Updates the model instance with the given values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self):\n        self.__dmlquery__(self.__class__, self,\n                          batch=self._batch,\n                          timestamp=self._timestamp,\n                          consistency=self.__consistency__,\n                          timeout=self._timeout,\n                          conditional=self._conditional,\n                          if_exists=self._if_exists).delete()", "response": "Deletes the object from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of the columns that have been changed since instantiation or save", "response": "def get_changed_columns(self):\n        \"\"\"\n        Returns a list of the columns that have been updated since instantiation or save\n        \"\"\"\n        return [k for k, v in self._values.items() if v.changed]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_lower_supported(cls, previous_version):\n        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version", "response": "Return the lower supported protocol version. Beta versions are omitted."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the query context with this clauses values", "response": "def update_context(self, ctx):\n        \"\"\" updates the query context with this clauses values \"\"\"\n        assert isinstance(ctx, dict)\n        ctx[str(self.context_id)] = self.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _analyze(self):\n        if self.value is None or self.value == self.previous:\n            pass\n        elif self._operation == \"add\":\n            self._additions = self.value\n        elif self._operation == \"remove\":\n            self._removals = self.value\n        elif self.previous is None:\n            self._assignments = self.value\n        else:\n            # partial update time\n            self._additions = (self.value - self.previous) or None\n            self._removals = (self.previous - self.value) or None\n        self._analyzed = True", "response": "checks out the updates to be performed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _analyze(self):\n        if self.value is None or self.value == self.previous:\n            pass\n\n        elif self._operation == \"append\":\n            self._append = self.value\n\n        elif self._operation == \"prepend\":\n            self._prepend = self.value\n\n        elif self.previous is None:\n            self._assignments = self.value\n\n        elif len(self.value) < len(self.previous):\n            # if elements have been removed,\n            # rewrite the whole list\n            self._assignments = self.value\n\n        elif len(self.previous) == 0:\n            # if we're updating from an empty\n            # list, do a complete insert\n            self._assignments = self.value\n        else:\n\n            # the max start idx we want to compare\n            search_space = len(self.value) - max(0, len(self.previous) - 1)\n\n            # the size of the sub lists we want to look at\n            search_size = len(self.previous)\n\n            for i in range(search_space):\n                # slice boundary\n                j = i + search_size\n                sub = self.value[i:j]\n                idx_cmp = lambda idx: self.previous[idx] == sub[idx]\n                if idx_cmp(0) and idx_cmp(-1) and self.previous == sub:\n                    self._prepend = self.value[:i] or None\n                    self._append = self.value[j:] or None\n                    break\n\n            # if both append and prepend are still None after looking\n            # at both lists, an insert statement will be created\n            if self._prepend is self._append is None:\n                self._assignments = self.value\n\n        self._analyzed = True", "response": "Does the actual analysis of the internal state of the internal state of the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the context dict for this statement", "response": "def get_context(self):\n        \"\"\"\n        returns the context dict for this statement\n        :rtype: dict\n        \"\"\"\n        ctx = {}\n        for clause in self.where_clauses or []:\n            clause.update_context(ctx)\n        return ctx"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a conditional clause to the current statement.", "response": "def add_conditional_clause(self, clause):\n        \"\"\"\n        Adds a iff clause to this statement\n\n        :param clause: The clause that will be added to the iff statement\n        :type clause: ConditionalClause\n        \"\"\"\n        clause.set_context_id(self.context_counter)\n        self.context_counter += clause.get_context_size()\n        self.conditionals.append(clause)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timestamp_normalized(self):\n        if not self.timestamp:\n            return None\n\n        if isinstance(self.timestamp, six.integer_types):\n            return self.timestamp\n\n        if isinstance(self.timestamp, timedelta):\n            tmp = datetime.now() + self.timestamp\n        else:\n            tmp = self.timestamp\n\n        return int(time.mktime(tmp.timetuple()) * 1e+6 + tmp.microsecond)", "response": "Return the timestamp normalized to the nearest 6 - digit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the next available request ID.", "response": "def get_request_id(self):\n        \"\"\"\n        This must be called while self.lock is held.\n        \"\"\"\n        try:\n            return self.request_ids.popleft()\n        except IndexError:\n            new_request_id = self.highest_request_id + 1\n            # in_flight checks should guarantee this\n            assert new_request_id <= self.max_request_id\n            self.highest_request_id = new_request_id\n            return self.highest_request_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting for responses from the specified messages to be sent.", "response": "def wait_for_responses(self, *msgs, **kwargs):\n        \"\"\"\n        Returns a list of (success, response) tuples.  If success\n        is False, response will be an Exception.  Otherwise, response\n        will be the normal query response.\n\n        If fail_on_error was left as True and one of the requests\n        failed, the corresponding Exception will be raised.\n        \"\"\"\n        if self.is_closed or self.is_defunct:\n            raise ConnectionShutdown(\"Connection %s is already closed\" % (self, ))\n        timeout = kwargs.get('timeout')\n        fail_on_error = kwargs.get('fail_on_error', True)\n        waiter = ResponseWaiter(self, len(msgs), fail_on_error)\n\n        # busy wait for sufficient space on the connection\n        messages_sent = 0\n        while True:\n            needed = len(msgs) - messages_sent\n            with self.lock:\n                available = min(needed, self.max_request_id - self.in_flight + 1)\n                request_ids = [self.get_request_id() for _ in range(available)]\n                self.in_flight += available\n\n            for i, request_id in enumerate(request_ids):\n                self.send_msg(msgs[messages_sent + i],\n                              request_id,\n                              partial(waiter.got_response, index=messages_sent + i))\n            messages_sent += available\n\n            if messages_sent == len(msgs):\n                break\n            else:\n                if timeout is not None:\n                    timeout -= 0.01\n                    if timeout <= 0.0:\n                        raise OperationTimedOut()\n                time.sleep(0.01)\n\n        try:\n            return waiter.deliver(timeout)\n        except OperationTimedOut:\n            raise\n        except Exception as exc:\n            self.defunct(exc)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a callback for a given event type.", "response": "def register_watcher(self, event_type, callback, register_timeout=None):\n        \"\"\"\n        Register a callback for a given event type.\n        \"\"\"\n        self._push_watchers[event_type].add(callback)\n        self.wait_for_response(\n            RegisterMessage(event_list=[event_type]),\n            timeout=register_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister multiple callback or event types expressed as a dict.", "response": "def register_watchers(self, type_callback_dict, register_timeout=None):\n        \"\"\"\n        Register multiple callback/event type pairs, expressed as a dict.\n        \"\"\"\n        for event_type, callback in type_callback_dict.items():\n            self._push_watchers[event_type].add(callback)\n        self.wait_for_response(\n            RegisterMessage(event_list=type_callback_dict.keys()),\n            timeout=register_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the keyspace for this connection asynchronously.", "response": "def set_keyspace_async(self, keyspace, callback):\n        \"\"\"\n        Use this in order to avoid deadlocking the event loop thread.\n        When the operation completes, `callback` will be called with\n        two arguments: this connection and an Exception if an error\n        occurred, otherwise :const:`None`.\n\n        This method will always increment :attr:`.in_flight` attribute, even if\n        it doesn't need to make a request, just to maintain an\n        \":attr:`.in_flight` is incremented\" invariant.\n        \"\"\"\n        # Here we increment in_flight unconditionally, whether we need to issue\n        # a request or not. This is bad, but allows callers -- specifically\n        # _set_keyspace_for_all_conns -- to assume that we increment\n        # self.in_flight during this call. This allows the passed callback to\n        # safely call HostConnection{Pool,}.return_connection on this\n        # Connection.\n        #\n        # We use a busy wait on the lock here because:\n        # - we'll only spin if the connection is at max capacity, which is very\n        #   unlikely for a set_keyspace call\n        # - it allows us to avoid signaling a condition every time a request completes\n        while True:\n            with self.lock:\n                if self.in_flight < self.max_request_id:\n                    self.in_flight += 1\n                    break\n            time.sleep(0.001)\n\n        if not keyspace or keyspace == self.keyspace:\n            callback(self, None)\n            return\n\n        query = QueryMessage(query='USE \"%s\"' % (keyspace,),\n                             consistency_level=ConsistencyLevel.ONE)\n\n        def process_result(result):\n            if isinstance(result, ResultMessage):\n                self.keyspace = keyspace\n                callback(self, None)\n            elif isinstance(result, InvalidRequestException):\n                callback(self, result.to_exception())\n            else:\n                callback(self, self.defunct(ConnectionException(\n                    \"Problem while setting keyspace: %r\" % (result,), self.endpoint)))\n\n        # We've incremented self.in_flight above, so we \"have permission\" to\n        # acquire a new request id\n        request_id = self.get_request_id()\n\n        self.send_msg(query, request_id, process_result)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deliver(self, timeout=None):\n        self.event.wait(timeout)\n        if self.error:\n            raise self.error\n        elif not self.event.is_set():\n            raise OperationTimedOut()\n        else:\n            return self.responses", "response": "Deliver the response from the server to the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a sequence of statements and parameters in a new order.", "response": "def execute_concurrent(session, statements_and_parameters, concurrency=100, raise_on_first_error=True, results_generator=False):\n    \"\"\"\n    Executes a sequence of (statement, parameters) tuples concurrently.  Each\n    ``parameters`` item must be a sequence or :const:`None`.\n\n    The `concurrency` parameter controls how many statements will be executed\n    concurrently.  When :attr:`.Cluster.protocol_version` is set to 1 or 2,\n    it is recommended that this be kept below 100 times the number of\n    core connections per host times the number of connected hosts (see\n    :meth:`.Cluster.set_core_connections_per_host`).  If that amount is exceeded,\n    the event loop thread may attempt to block on new connection creation,\n    substantially impacting throughput.  If :attr:`~.Cluster.protocol_version`\n    is 3 or higher, you can safely experiment with higher levels of concurrency.\n\n    If `raise_on_first_error` is left as :const:`True`, execution will stop\n    after the first failed statement and the corresponding exception will be\n    raised.\n\n    `results_generator` controls how the results are returned.\n\n    * If :const:`False`, the results are returned only after all requests have completed.\n    * If :const:`True`, a generator expression is returned. Using a generator results in a constrained\n      memory footprint when the results set will be large -- results are yielded\n      as they return instead of materializing the entire list at once. The trade for lower memory\n      footprint is marginal CPU overhead (more thread coordination and sorting out-of-order results\n      on-the-fly).\n\n    A sequence of ``ExecutionResult(success, result_or_exc)`` namedtuples is returned\n    in the same order that the statements were passed in.  If ``success`` is :const:`False`,\n    there was an error executing the statement, and ``result_or_exc`` will be\n    an :class:`Exception`.  If ``success`` is :const:`True`, ``result_or_exc``\n    will be the query result.\n\n    Example usage::\n\n        select_statement = session.prepare(\"SELECT * FROM users WHERE id=?\")\n\n        statements_and_params = []\n        for user_id in user_ids:\n            params = (user_id, )\n            statements_and_params.append((select_statement, params))\n\n        results = execute_concurrent(\n            session, statements_and_params, raise_on_first_error=False)\n\n        for (success, result) in results:\n            if not success:\n                handle_error(result)  # result will be an Exception\n            else:\n                process_user(result[0])  # result will be a list of rows\n\n    Note: in the case that `generators` are used, it is important to ensure the consumers do not\n    block or attempt further synchronous requests, because no further IO will be processed until\n    the consumer returns. This may also produce a deadlock in the IO event thread.\n    \"\"\"\n    if concurrency <= 0:\n        raise ValueError(\"concurrency must be greater than 0\")\n\n    if not statements_and_parameters:\n        return []\n\n    executor = ConcurrentExecutorGenResults(session, statements_and_parameters) if results_generator else ConcurrentExecutorListResults(session, statements_and_parameters)\n    return executor.execute(concurrency, raise_on_first_error)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute_concurrent_with_args(session, statement, parameters, *args, **kwargs):\n    return execute_concurrent(session, zip(cycle((statement,)), parameters), *args, **kwargs)", "response": "Like : meth : ~cassandra. concurrent. execute_concurrent but takes a single\n    statement and a sequence of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representation of the type for this column.", "response": "def _cql_from_cass_type(cass_type):\n    \"\"\"\n    A string representation of the type for this column, such as \"varchar\"\n    or \"map<string, int>\".\n    \"\"\"\n    if issubclass(cass_type, types.ReversedType):\n        return cass_type.subtypes[0].cql_parameterized_type()\n    else:\n        return cass_type.cql_parameterized_type()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef group_keys_by_replica(session, keyspace, table, keys):\n    cluster = session.cluster\n\n    partition_keys = cluster.metadata.keyspaces[keyspace].tables[table].partition_key\n\n    serializers = list(types._cqltypes[partition_key.cql_type] for partition_key in partition_keys)\n    keys_per_host = defaultdict(list)\n    distance = cluster._default_load_balancing_policy.distance\n\n    for key in keys:\n        serialized_key = [serializer.serialize(pk, cluster.protocol_version)\n                          for serializer, pk in zip(serializers, key)]\n        if len(serialized_key) == 1:\n            routing_key = serialized_key[0]\n        else:\n            routing_key = b\"\".join(struct.pack(\">H%dsB\" % len(p), len(p), p, 0) for p in serialized_key)\n        all_replicas = cluster.metadata.get_replicas(keyspace, routing_key)\n        # First check if there are local replicas\n        valid_replicas = [host for host in all_replicas if\n                          host.is_up and distance(host) == HostDistance.LOCAL]\n        if not valid_replicas:\n            valid_replicas = [host for host in all_replicas if host.is_up]\n\n        if valid_replicas:\n            keys_per_host[random.choice(valid_replicas)].append(key)\n        else:\n            # We will group under this statement all the keys for which\n            # we haven't found a valid replica\n            keys_per_host[NO_VALID_REPLICA].append(key)\n\n    return dict(keys_per_host)", "response": "Returns a dict with the keys grouped by replica."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rebuild_token_map(self, partitioner, token_map):\n        self.partitioner = partitioner\n        if partitioner.endswith('RandomPartitioner'):\n            token_class = MD5Token\n        elif partitioner.endswith('Murmur3Partitioner'):\n            token_class = Murmur3Token\n        elif partitioner.endswith('ByteOrderedPartitioner'):\n            token_class = BytesToken\n        else:\n            self.token_map = None\n            return\n\n        token_to_host_owner = {}\n        ring = []\n        for host, token_strings in six.iteritems(token_map):\n            for token_string in token_strings:\n                token = token_class.from_string(token_string)\n                ring.append(token)\n                token_to_host_owner[token] = host\n\n        all_tokens = sorted(ring)\n        self.token_map = TokenMap(\n            token_class, token_to_host_owner, all_tokens, self)", "response": "Rebuild our view of the topology from fresh rows from the system topology tables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_replicas(self, keyspace, key):\n        t = self.token_map\n        if not t:\n            return []\n        try:\n            return t.get_replicas(keyspace, t.token_class.from_key(key))\n        except NoMurmur3:\n            return []", "response": "Returns a list of host instances that are replicas for a given keyspace and partition key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds or returns a Host object to the cache if it is not already present.", "response": "def add_or_return_host(self, host):\n        \"\"\"\n        Returns a tuple (host, new), where ``host`` is a Host\n        instance, and ``new`` is a bool indicating whether\n        the host was newly added.\n        \"\"\"\n        with self._hosts_lock:\n            try:\n                return self._hosts[host.endpoint], False\n            except KeyError:\n                self._hosts[host.endpoint] = host\n                return host, True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a host from the metadata for a specific endpoint.", "response": "def get_host(self, endpoint_or_address):\n        \"\"\"\n        Find a host in the metadata for a specific endpoint. If a string inet address is passed,\n        iterate all hosts to match the :attr:`~.pool.Host.broadcast_rpc_address` attribute.\n        \"\"\"\n        if not isinstance(endpoint_or_address, EndPoint):\n            return self._get_host_by_address(endpoint_or_address)\n\n        return self._hosts.get(endpoint_or_address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_for_schema(self):\n        if self.options_map:\n            return dict((str(key), str(value)) for key, value in self.options_map.items())\n        return \"{'class': '%s'}\" % (self.name, )", "response": "Returns a string version of these replication options which are\n            suitable for use in a CREATE KEYSPACE statement."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_for_schema(self):\n        ret = \"{'class': 'NetworkTopologyStrategy'\"\n        for dc, repl_factor in sorted(self.dc_replication_factors.items()):\n            ret += \", '%s': '%d'\" % (dc, repl_factor)\n        return ret + \"}\"", "response": "Returns a string version of these replication options which are suitable for use in a CREATE KEYSPACE statement."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_as_string(self):\n        cql = \"\\n\\n\".join([self.as_cql_query() + ';'] +\n                          self.user_type_strings() +\n                          [f.export_as_string() for f in self.functions.values()] +\n                          [a.export_as_string() for a in self.aggregates.values()] +\n                          [t.export_as_string() for t in self.tables.values()])\n        if self._exc_info:\n            import traceback\n            ret = \"/*\\nWarning: Keyspace %s is incomplete because of an error processing metadata.\\n\" % \\\n                  (self.name)\n            for line in traceback.format_exception(*self._exc_info):\n                ret += line\n            ret += \"\\nApproximate structure, for reference:\\n(this should not be used to reproduce this schema)\\n\\n%s\\n*/\" % cql\n            return ret\n        if self.virtual:\n            return (\"/*\\nWarning: Keyspace {ks} is a virtual keyspace and cannot be recreated with CQL.\\n\"\n                    \"Structure, for reference:*/\\n\"\n                    \"{cql}\\n\"\n                    \"\").format(ks=self.name, cql=cql)\n        return cql", "response": "Returns a CQL query string that can be used to recreate the entire keyspace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a CQL query string that can be used to recreate just this keyspace and not including user - defined types and tables.", "response": "def as_cql_query(self):\n        \"\"\"\n        Returns a CQL query string that can be used to recreate just this keyspace,\n        not including user-defined types and tables.\n        \"\"\"\n        if self.virtual:\n            return \"// VIRTUAL KEYSPACE {}\".format(protect_name(self.name))\n        ret = \"CREATE KEYSPACE %s WITH replication = %s \" % (\n            protect_name(self.name),\n            self.replication_strategy.export_for_schema())\n        return ret + (' AND durable_writes = %s' % (\"true\" if self.durable_writes else \"false\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_cql_query(self, formatted=False):\n        ret = \"CREATE TYPE %s.%s (%s\" % (\n            protect_name(self.keyspace),\n            protect_name(self.name),\n            \"\\n\" if formatted else \"\")\n\n        if formatted:\n            field_join = \",\\n\"\n            padding = \"    \"\n        else:\n            field_join = \", \"\n            padding = \"\"\n\n        fields = []\n        for field_name, field_type in zip(self.field_names, self.field_types):\n            fields.append(\"%s %s\" % (protect_name(field_name), field_type))\n\n        ret += field_join.join(\"%s%s\" % (padding, field) for field in fields)\n        ret += \"\\n)\" if formatted else \")\"\n        return ret", "response": "Returns a CQL query that can be used to recreate this type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a CQL query that can be used to recreate this aggregate.", "response": "def as_cql_query(self, formatted=False):\n        \"\"\"\n        Returns a CQL query that can be used to recreate this aggregate.\n        If `formatted` is set to :const:`True`, extra whitespace will\n        be added to make the query more readable.\n        \"\"\"\n        sep = '\\n    ' if formatted else ' '\n        keyspace = protect_name(self.keyspace)\n        name = protect_name(self.name)\n        type_list = ', '.join(self.argument_types)\n        state_func = protect_name(self.state_func)\n        state_type = self.state_type\n\n        ret = \"CREATE AGGREGATE %(keyspace)s.%(name)s(%(type_list)s)%(sep)s\" \\\n              \"SFUNC %(state_func)s%(sep)s\" \\\n              \"STYPE %(state_type)s\" % locals()\n\n        ret += ''.join((sep, 'FINALFUNC ', protect_name(self.final_func))) if self.final_func else ''\n        ret += ''.join((sep, 'INITCOND ', self.initial_condition)) if self.initial_condition is not None else ''\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a CQL query that can be used to recreate this function.", "response": "def as_cql_query(self, formatted=False):\n        \"\"\"\n        Returns a CQL query that can be used to recreate this function.\n        If `formatted` is set to :const:`True`, extra whitespace will\n        be added to make the query more readable.\n        \"\"\"\n        sep = '\\n    ' if formatted else ' '\n        keyspace = protect_name(self.keyspace)\n        name = protect_name(self.name)\n        arg_list = ', '.join([\"%s %s\" % (protect_name(n), t)\n                             for n, t in zip(self.argument_names, self.argument_types)])\n        typ = self.return_type\n        lang = self.language\n        body = self.body\n        on_null = \"CALLED\" if self.called_on_null_input else \"RETURNS NULL\"\n\n        return \"CREATE FUNCTION %(keyspace)s.%(name)s(%(arg_list)s)%(sep)s\" \\\n               \"%(on_null)s ON NULL INPUT%(sep)s\" \\\n               \"RETURNS %(typ)s%(sep)s\" \\\n               \"LANGUAGE %(lang)s%(sep)s\" \\\n               \"AS $$%(body)s$$\" % locals()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_cql_compatible(self):\n        if self.virtual:\n            return False\n        comparator = getattr(self, 'comparator', None)\n        if comparator:\n            # no compact storage with more than one column beyond PK if there\n            # are clustering columns\n            incompatible = (self.is_compact_storage and\n                            len(self.columns) > len(self.primary_key) + 1 and\n                            len(self.clustering_key) >= 1)\n\n            return not incompatible\n        return True", "response": "A boolean indicating if this table can be represented as CQL in export\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_as_string(self):\n        if self._exc_info:\n            import traceback\n            ret = \"/*\\nWarning: Table %s.%s is incomplete because of an error processing metadata.\\n\" % \\\n                  (self.keyspace_name, self.name)\n            for line in traceback.format_exception(*self._exc_info):\n                ret += line\n            ret += \"\\nApproximate structure, for reference:\\n(this should not be used to reproduce this schema)\\n\\n%s\\n*/\" % self._all_as_cql()\n        elif not self.is_cql_compatible:\n            # If we can't produce this table with CQL, comment inline\n            ret = \"/*\\nWarning: Table %s.%s omitted because it has constructs not compatible with CQL (was created via legacy API).\\n\" % \\\n                  (self.keyspace_name, self.name)\n            ret += \"\\nApproximate structure, for reference:\\n(this should not be used to reproduce this schema)\\n\\n%s\\n*/\" % self._all_as_cql()\n        elif self.virtual:\n            ret = ('/*\\nWarning: Table {ks}.{tab} is a virtual table and cannot be recreated with CQL.\\n'\n                   'Structure, for reference:\\n'\n                   '{cql}\\n*/').format(ks=self.keyspace_name, tab=self.name, cql=self._all_as_cql())\n\n        else:\n            ret = self._all_as_cql()\n\n        return ret", "response": "Returns a string that can be used to recreate this table with all indexes on it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a CQL query that can be used to recreate this table.", "response": "def as_cql_query(self, formatted=False):\n        \"\"\"\n        Returns a CQL query that can be used to recreate this table (index\n        creations are not included).  If `formatted` is set to :const:`True`,\n        extra whitespace will be added to make the query human readable.\n        \"\"\"\n        ret = \"%s TABLE %s.%s (%s\" % (\n            ('VIRTUAL' if self.virtual else 'CREATE'),\n            protect_name(self.keyspace_name),\n            protect_name(self.name),\n            \"\\n\" if formatted else \"\")\n\n        if formatted:\n            column_join = \",\\n\"\n            padding = \"    \"\n        else:\n            column_join = \", \"\n            padding = \"\"\n\n        columns = []\n        for col in self.columns.values():\n            columns.append(\"%s %s%s\" % (protect_name(col.name), col.cql_type, ' static' if col.is_static else ''))\n\n        if len(self.partition_key) == 1 and not self.clustering_key:\n            columns[0] += \" PRIMARY KEY\"\n\n        ret += column_join.join(\"%s%s\" % (padding, col) for col in columns)\n\n        # primary key\n        if len(self.partition_key) > 1 or self.clustering_key:\n            ret += \"%s%sPRIMARY KEY (\" % (column_join, padding)\n\n            if len(self.partition_key) > 1:\n                ret += \"(%s)\" % \", \".join(protect_name(col.name) for col in self.partition_key)\n            else:\n                ret += protect_name(self.partition_key[0].name)\n\n            if self.clustering_key:\n                ret += \", %s\" % \", \".join(protect_name(col.name) for col in self.clustering_key)\n\n            ret += \")\"\n\n        # properties\n        ret += \"%s) WITH \" % (\"\\n\" if formatted else \"\")\n        ret += self._property_string(formatted, self.clustering_key, self.options, self.is_compact_storage)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_cql_query(self):\n        options = dict(self.index_options)\n        index_target = options.pop(\"target\")\n        if self.kind != \"CUSTOM\":\n            return \"CREATE INDEX %s ON %s.%s (%s)\" % (\n                protect_name(self.name),\n                protect_name(self.keyspace_name),\n                protect_name(self.table_name),\n                index_target)\n        else:\n            class_name = options.pop(\"class_name\")\n            ret = \"CREATE CUSTOM INDEX %s ON %s.%s (%s) USING '%s'\" % (\n                protect_name(self.name),\n                protect_name(self.keyspace_name),\n                protect_name(self.table_name),\n                index_target,\n                class_name)\n            if options:\n                # PYTHON-1008: `ret` will always be a unicode\n                opts_cql_encoded = _encoder.cql_encode_all_types(options, as_text_type=True)\n                ret += \" WITH OPTIONS = %s\" % opts_cql_encoded\n            return ret", "response": "Returns a CQL query that can be used to recreate this index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_replicas(self, keyspace, token):\n        tokens_to_hosts = self.tokens_to_hosts_by_ks.get(keyspace, None)\n        if tokens_to_hosts is None:\n            self.rebuild_keyspace(keyspace, build_if_absent=True)\n            tokens_to_hosts = self.tokens_to_hosts_by_ks.get(keyspace, None)\n\n        if tokens_to_hosts:\n            # The values in self.ring correspond to the end of the\n            # token range up to and including the value listed.\n            point = bisect_left(self.ring, token)\n            if point == len(self.ring):\n                return tokens_to_hosts[self.ring[0]]\n            else:\n                return tokens_to_hosts[self.ring[point]]\n        return []", "response": "Get a set of Host instances representing all of the nodes in a given keyspace for a given token."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_string(cls, token_string):\n        # unhexlify works fine with unicode input in everythin but pypy3, where it Raises \"TypeError: 'str' does not support the buffer interface\"\n        if isinstance(token_string, six.text_type):\n            token_string = token_string.encode('ascii')\n        # The BOP stores a hex string\n        return cls(unhexlify(token_string))", "response": "Create a new BOP object from a string representation of the server s response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_results(self, success, result, expected_failures=tuple()):\n        if not success and isinstance(result, expected_failures):\n            return []\n        elif success:\n            return dict_factory(*result.results) if result else []\n        else:\n            raise result", "response": "Handles the results from the query and returns a dictionary containing the keyspaces that were found."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_table_options(self, row):\n        options = dict((o, row.get(o)) for o in self.recognized_table_options if o in row)\n\n        # the option name when creating tables is \"dclocal_read_repair_chance\",\n        # but the column name in system.schema_columnfamilies is\n        # \"local_read_repair_chance\".  We'll store this as dclocal_read_repair_chance,\n        # since that's probably what users are expecting (and we need it for the\n        # CREATE TABLE statement anyway).\n        if \"local_read_repair_chance\" in options:\n            val = options.pop(\"local_read_repair_chance\")\n            options[\"dclocal_read_repair_chance\"] = val\n\n        return options", "response": "Build the options dictionary for the mostly - non - schema table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_table_options(self, row):\n        return dict((o, row.get(o)) for o in self.recognized_table_options if o in row)", "response": "Build a dictionary of table options from a row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a CQL query that can be used to recreate this table.", "response": "def as_cql_query(self, formatted=False):\n        \"\"\"\n        Returns a CQL query that can be used to recreate this function.\n        If `formatted` is set to :const:`True`, extra whitespace will\n        be added to make the query more readable.\n        \"\"\"\n        sep = '\\n    ' if formatted else ' '\n        keyspace = protect_name(self.keyspace_name)\n        name = protect_name(self.name)\n\n        selected_cols = '*' if self.include_all_columns else ', '.join(protect_name(col.name) for col in self.columns.values())\n        base_table = protect_name(self.base_table_name)\n        where_clause = self.where_clause\n\n        part_key = ', '.join(protect_name(col.name) for col in self.partition_key)\n        if len(self.partition_key) > 1:\n            pk = \"((%s)\" % part_key\n        else:\n            pk = \"(%s\" % part_key\n        if self.clustering_key:\n            pk += \", %s\" % ', '.join(protect_name(col.name) for col in self.clustering_key)\n        pk += \")\"\n\n        properties = TableMetadataV3._property_string(formatted, self.clustering_key, self.options)\n\n        ret = (\"CREATE MATERIALIZED VIEW %(keyspace)s.%(name)s AS%(sep)s\"\n               \"SELECT %(selected_cols)s%(sep)s\"\n               \"FROM %(keyspace)s.%(base_table)s%(sep)s\"\n               \"WHERE %(where_clause)s%(sep)s\"\n               \"PRIMARY KEY %(pk)s%(sep)s\"\n               \"WITH %(properties)s\") % locals()\n\n        if self.extensions:\n            registry = _RegisteredExtensionType._extension_registry\n            for k in six.viewkeys(registry) & self.extensions:  # no viewkeys on OrderedMapSerializeKey\n                ext = registry[k]\n                cql = ext.after_table_cql(self, k, self.extensions[k])\n                if cql:\n                    ret += \"\\n\\n%s\" % (cql,)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup_casstype_simple(casstype):\n    shortname = trim_if_startswith(casstype, apache_cassandra_type_prefix)\n    try:\n        typeclass = _casstypes[shortname]\n    except KeyError:\n        typeclass = mkUnrecognizedType(casstype)\n    return typeclass", "response": "Given a Cassandra type name returns the class responsible for it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a Cassandra type as a string returns the corresponding _UnrecognizedType subclass.", "response": "def lookup_casstype(casstype):\n    \"\"\"\n    Given a Cassandra type as a string (possibly including parameters), hand\n    back the CassandraType class responsible for it. If a name is not\n    recognized, a custom _UnrecognizedType subclass will be created for it.\n\n    Example:\n\n        >>> lookup_casstype('org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.Int32Type)')\n        <class 'cassandra.cqltypes.MapType(UTF8Type, Int32Type)'>\n\n    \"\"\"\n    if isinstance(casstype, (CassandraType, CassandraTypeType)):\n        return casstype\n    try:\n        return parse_casstype_args(casstype)\n    except (ValueError, AssertionError, IndexError) as e:\n        raise ValueError(\"Don't know how to parse type string %r: %s\" % (casstype, e))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef service_timeouts(cls):\n        timer_manager = cls._timers\n        while True:\n            next_end = timer_manager.service_timeouts()\n            sleep_time = max(next_end - time.time(), 0) if next_end else 10000\n            cls._new_timer.wait(sleep_time)\n            cls._new_timer.clear()", "response": "This method waits for new timers to arrive and sets the new timers to the new ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_query_plan(self, working_keyspace=None, query=None):\n        child_qp = self._child_policy.make_query_plan(\n            working_keyspace=working_keyspace, query=query\n        )\n        for host in child_qp:\n            if self.predicate(host):\n                yield host", "response": "Returns a list of hosts that match the predicate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None", "response": "This method is called when a write operation times out from the coordinator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntranslating the given address to the AWS - resolved IP address.", "response": "def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninstalls or upgrade setuptools and EasyInstall", "response": "def main(version=DEFAULT_VERSION):\n    \"\"\"Install or upgrade setuptools and EasyInstall\"\"\"\n    options = _parse_args()\n    tarball = download_setuptools(download_base=options.download_base)\n    return _install(tarball, _build_install_args(options))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencoding a float into a string using repr to preserve precision", "response": "def cql_encode_float(self, val):\n        \"\"\"\n        Encode floats using repr to preserve precision\n        \"\"\"\n        if math.isinf(val):\n            return 'Infinity' if val > 0 else '-Infinity'\n        elif math.isnan(val):\n            return 'NaN'\n        else:\n            return repr(val)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a datetime object to a string timestamp with millisecond precision precision.", "response": "def cql_encode_datetime(self, val):\n        \"\"\"\n        Converts a :class:`datetime.datetime` object to a (string) integer timestamp\n        with millisecond precision.\n        \"\"\"\n        timestamp = calendar.timegm(val.utctimetuple())\n        return str(long(timestamp * 1e3 + getattr(val, 'microsecond', 0) / 1e3))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a sequence to a string of the form ( item1 item2... ).", "response": "def cql_encode_sequence(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``(item1, item2, ...)``.  This\n        is suitable for ``IN`` value lists.\n        \"\"\"\n        return '(%s)' % ', '.join(self.mapping.get(type(v), self.cql_encode_object)(v)\n                                     for v in val)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a dict into a string of the form { key1 key2... }.", "response": "def cql_encode_map_collection(self, val):\n        \"\"\"\n        Converts a dict into a string of the form ``{key1: val1, key2: val2, ...}``.\n        This is suitable for ``map`` type columns.\n        \"\"\"\n        return '{%s}' % ', '.join('%s: %s' % (\n            self.mapping.get(type(k), self.cql_encode_object)(k),\n            self.mapping.get(type(v), self.cql_encode_object)(v)\n        ) for k, v in six.iteritems(val))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cql_encode_all_types(self, val, as_text_type=False):\n        encoded = self.mapping.get(type(val), self.cql_encode_object)(val)\n        if as_text_type and not isinstance(encoded, six.text_type):\n            return encoded.decode('utf-8')\n        return encoded", "response": "Converts any type into a CQL string defaulting to cql_encode_object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a column parser to deserialize ResultMessages returns a suitable Cython - based protocol handler.", "response": "def cython_protocol_handler(colparser):\n    \"\"\"\n    Given a column parser to deserialize ResultMessages, return a suitable\n    Cython-based protocol handler.\n\n    There are three Cython-based protocol handlers:\n\n        - obj_parser.ListParser\n            decodes result messages into a list of tuples\n\n        - obj_parser.LazyParser\n            decodes result messages lazily by returning an iterator\n\n        - numpy_parser.NumPyParser\n            decodes result messages into NumPy arrays\n\n    The default is to use obj_parser.ListParser\n    \"\"\"\n    from cassandra.row_parser import make_recv_results_rows\n\n    class FastResultMessage(ResultMessage):\n        \"\"\"\n        Cython version of Result Message that has a faster implementation of\n        recv_results_row.\n        \"\"\"\n        # type_codes = ResultMessage.type_codes.copy()\n        code_to_type = dict((v, k) for k, v in ResultMessage.type_codes.items())\n        recv_results_rows = classmethod(make_recv_results_rows(colparser))\n\n    class CythonProtocolHandler(_ProtocolHandler):\n        \"\"\"\n        Use FastResultMessage to decode query result message messages.\n        \"\"\"\n\n        my_opcodes = _ProtocolHandler.message_types_by_opcode.copy()\n        my_opcodes[FastResultMessage.opcode] = FastResultMessage\n        message_types_by_opcode = my_opcodes\n\n        col_parser = colparser\n\n    return CythonProtocolHandler"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencoding a cassandra. protocol. _MessageType object into a byte string.", "response": "def encode_message(cls, msg, stream_id, protocol_version, compressor, allow_beta_protocol_version):\n        \"\"\"\n        Encodes a message using the specified frame parameters, and compressor\n\n        :param msg: the message, typically of cassandra.protocol._MessageType, generated by the driver\n        :param stream_id: protocol stream id for the frame header\n        :param protocol_version: version for the frame header, and used encoding contents\n        :param compressor: optional compression function to be used on the body\n        \"\"\"\n        flags = 0\n        body = io.BytesIO()\n        if msg.custom_payload:\n            if protocol_version < 4:\n                raise UnsupportedOperation(\"Custom key/value payloads can only be used with protocol version 4 or higher\")\n            flags |= CUSTOM_PAYLOAD_FLAG\n            write_bytesmap(body, msg.custom_payload)\n        msg.send_body(body, protocol_version)\n        body = body.getvalue()\n\n        if compressor and len(body) > 0:\n            body = compressor(body)\n            flags |= COMPRESSED_FLAG\n\n        if msg.tracing:\n            flags |= TRACING_FLAG\n\n        if allow_beta_protocol_version:\n            flags |= USE_BETA_FLAG\n\n        buff = io.BytesIO()\n        cls._write_header(buff, protocol_version, flags, stream_id, msg.opcode, len(body))\n        buff.write(body)\n\n        return buff.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_header(f, version, flags, stream_id, opcode, length):\n        pack = v3_header_pack if version >= 3 else header_pack\n        f.write(pack(version, flags, stream_id, opcode))\n        write_int(f, length)", "response": "Write a CQL protocol frame header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_message(cls, protocol_version, user_type_map, stream_id, flags, opcode, body,\n                       decompressor, result_metadata):\n        \"\"\"\n        Decodes a native protocol message body\n\n        :param protocol_version: version to use decoding contents\n        :param user_type_map: map[keyspace name] = map[type name] = custom type to instantiate when deserializing this type\n        :param stream_id: native protocol stream id from the frame header\n        :param flags: native protocol flags bitmap from the header\n        :param opcode: native protocol opcode from the header\n        :param body: frame body\n        :param decompressor: optional decompression function to inflate the body\n        :return: a message decoded from the body and frame attributes\n        \"\"\"\n        if flags & COMPRESSED_FLAG:\n            if decompressor is None:\n                raise RuntimeError(\"No de-compressor available for compressed frame!\")\n            body = decompressor(body)\n            flags ^= COMPRESSED_FLAG\n\n        body = io.BytesIO(body)\n        if flags & TRACING_FLAG:\n            trace_id = UUID(bytes=body.read(16))\n            flags ^= TRACING_FLAG\n        else:\n            trace_id = None\n\n        if flags & WARNING_FLAG:\n            warnings = read_stringlist(body)\n            flags ^= WARNING_FLAG\n        else:\n            warnings = None\n\n        if flags & CUSTOM_PAYLOAD_FLAG:\n            custom_payload = read_bytesmap(body)\n            flags ^= CUSTOM_PAYLOAD_FLAG\n        else:\n            custom_payload = None\n\n        flags &= USE_BETA_MASK # will only be set if we asserted it in connection estabishment\n\n        if flags:\n            log.warning(\"Unknown protocol flags set: %02x. May cause problems.\", flags)\n\n        msg_class = cls.message_types_by_opcode[opcode]\n        msg = msg_class.recv_body(body, protocol_version, user_type_map, result_metadata)\n        msg.stream_id = stream_id\n        msg.trace_id = trace_id\n        msg.custom_payload = custom_payload\n        msg.warnings = warnings\n\n        if msg.warnings:\n            for w in msg.warnings:\n                log.warning(\"Server warning: %s\", w)\n\n        return msg", "response": "Decodes a native protocol message body and returns a message object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_log_context(msg, connection=None, keyspace=None):\n    connection_info = connection or 'DEFAULT_CONNECTION'\n\n    if keyspace:\n        msg = '[Connection: {0}, Keyspace: {1}] {2}'.format(connection_info, keyspace, msg)\n    else:\n        msg = '[Connection: {0}] {1}'.format(connection_info, msg)\n    return msg", "response": "Format log message to add keyspace and connection context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_connection(name, hosts=None, consistency=None, lazy_connect=False,\n                        retry_connect=False, cluster_options=None, default=False,\n                        session=None):\n    \"\"\"\n    Add a connection to the connection registry. ``hosts`` and ``session`` are\n    mutually exclusive, and ``consistency``, ``lazy_connect``,\n    ``retry_connect``, and ``cluster_options`` only work with ``hosts``. Using\n    ``hosts`` will create a new :class:`cassandra.cluster.Cluster` and\n    :class:`cassandra.cluster.Session`.\n\n    :param list hosts: list of hosts, (``contact_points`` for :class:`cassandra.cluster.Cluster`).\n    :param int consistency: The default :class:`~.ConsistencyLevel` for the\n        registered connection's new session. Default is the same as\n        :attr:`.Session.default_consistency_level`. For use with ``hosts`` only;\n        will fail when used with ``session``.\n    :param bool lazy_connect: True if should not connect until first use. For\n        use with ``hosts`` only; will fail when used with ``session``.\n    :param bool retry_connect: True if we should retry to connect even if there\n        was a connection failure initially. For use with ``hosts`` only; will\n        fail when used with ``session``.\n    :param dict cluster_options: A dict of options to be used as keyword\n        arguments to :class:`cassandra.cluster.Cluster`. For use with ``hosts``\n        only; will fail when used with ``session``.\n    :param bool default: If True, set the new connection as the cqlengine\n        default\n    :param Session session: A :class:`cassandra.cluster.Session` to be used in\n        the created connection.\n    \"\"\"\n\n    if name in _connections:\n        log.warning(\"Registering connection '{0}' when it already exists.\".format(name))\n\n    if session is not None:\n        invalid_config_args = (hosts is not None or\n                               consistency is not None or\n                               lazy_connect is not False or\n                               retry_connect is not False or\n                               cluster_options is not None)\n        if invalid_config_args:\n            raise CQLEngineException(\n                \"Session configuration arguments and 'session' argument are mutually exclusive\"\n            )\n        conn = Connection.from_session(name, session=session)\n        conn.setup_session()\n    else:  # use hosts argument\n        if consistency is None:\n            consistency = ConsistencyLevel.LOCAL_ONE\n        conn = Connection(\n            name, hosts=hosts,\n            consistency=consistency, lazy_connect=lazy_connect,\n            retry_connect=retry_connect, cluster_options=cluster_options\n        )\n        conn.setup()\n\n    _connections[name] = conn\n\n    if default:\n        set_default_connection(name)\n\n    return conn", "response": "Register a new connection with the Cassandra cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfigure the default connection to localhost using the driver defaults AttributeNames", "response": "def default():\n    \"\"\"\n    Configures the default connection to localhost, using the driver defaults\n    (except for row_factory)\n    \"\"\"\n\n    try:\n        conn = get_connection()\n        if conn.session:\n            log.warning(\"configuring new default connection for cqlengine when one was already set\")\n    except:\n        pass\n\n    register_connection('default', hosts=None, default=True)\n\n    log.debug(\"cqlengine connection initialized with default session to localhost\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures the default connection for cassandra.", "response": "def set_session(s):\n    \"\"\"\n    Configures the default connection with a preexisting :class:`cassandra.cluster.Session`\n\n    Note: the mapper presently requires a Session :attr:`~.row_factory` set to ``dict_factory``.\n    This may be relaxed in the future\n    \"\"\"\n\n    try:\n        conn = get_connection()\n    except CQLEngineException:\n        # no default connection set; initalize one\n        register_connection('default', session=s, default=True)\n        conn = get_connection()\n\n    if conn.session:\n        log.warning(\"configuring new default connection for cqlengine when one was already set\")\n\n    if s.row_factory is not dict_factory:\n        raise CQLEngineException(\"Failed to initialize: 'Session.row_factory' must be 'dict_factory'.\")\n    conn.session = s\n    conn.cluster = s.cluster\n\n    # Set default keyspace from given session's keyspace\n    if conn.session.keyspace:\n        from cassandra.cqlengine import models\n        models.DEFAULT_KEYSPACE = conn.session.keyspace\n\n    conn.setup_session()\n\n    log.debug(\"cqlengine default connection initialized with %s\", s)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_in_executor(f):\n\n    @wraps(f)\n    def new_f(self, *args, **kwargs):\n\n        if self.is_shutdown:\n            return\n        try:\n            future = self.executor.submit(f, self, *args, **kwargs)\n            future.add_done_callback(_future_completed)\n        except Exception:\n            log.exception(\"Failed to submit task to executor\")\n\n    return new_f", "response": "A decorator to run the given method in the ThreadPoolExecutor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_user_type(self, keyspace, user_type, klass):\n        if self.protocol_version < 3:\n            log.warning(\"User Type serialization is only supported in native protocol version 3+ (%d in use). \"\n                        \"CQL encoding for simple statements will still work, but named tuples will \"\n                        \"be returned when reading type %s.%s.\", self.protocol_version, keyspace, user_type)\n\n        self._user_types[keyspace][user_type] = klass\n        for session in tuple(self.sessions):\n            session.user_type_registered(keyspace, user_type, klass)\n        UserType.evict_udt_class(keyspace, user_type)", "response": "Register a class to use to represent a particular user - defined type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_execution_profile(self, name, profile, pool_wait_timeout=5):\n        if not isinstance(profile, ExecutionProfile):\n            raise TypeError(\"profile must be an instance of ExecutionProfile\")\n        if self._config_mode == _ConfigMode.LEGACY:\n            raise ValueError(\"Cannot add execution profiles when legacy parameters are set explicitly.\")\n        if name in self.profile_manager.profiles:\n            raise ValueError(\"Profile {} already exists\".format(name))\n        contact_points_but_no_lbp = (\n            self._contact_points_explicit and not\n            profile._load_balancing_policy_explicit)\n        if contact_points_but_no_lbp:\n            log.warning(\n                'Tried to add an ExecutionProfile with name {name}. '\n                '{self} was explicitly configured with contact_points, but '\n                '{ep} was not explicitly configured with a '\n                'load_balancing_policy. In the next major version, trying to '\n                'add an ExecutionProfile without an explicitly configured LBP '\n                'to a cluster with explicitly configured contact_points will '\n                'raise an exception; please specify a load-balancing policy '\n                'in the ExecutionProfile.'\n                ''.format(name=repr(name), self=self, ep=profile))\n\n        self.profile_manager.profiles[name] = profile\n        profile.load_balancing_policy.populate(self, self.metadata.all_hosts())\n        # on_up after populate allows things like DCA LBP to choose default local dc\n        for host in filter(lambda h: h.is_up, self.metadata.all_hosts()):\n            profile.load_balancing_policy.on_up(host)\n        futures = set()\n        for session in tuple(self.sessions):\n            futures.update(session.update_created_pools())\n        _, not_done = wait_futures(futures, pool_wait_timeout)\n        if not_done:\n            raise OperationTimedOut(\"Failed to create all new connection pools in the %ss timeout.\")", "response": "Add an ExecutionProfile to the cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_min_requests_per_connection(self, host_distance, min_requests):\n        if self.protocol_version >= 3:\n            raise UnsupportedOperation(\n                \"Cluster.set_min_requests_per_connection() only has an effect \"\n                \"when using protocol_version 1 or 2.\")\n        if min_requests < 0 or min_requests > 126 or \\\n           min_requests >= self._max_requests_per_connection[host_distance]:\n            raise ValueError(\"min_requests must be 0-126 and less than the max_requests for this host_distance (%d)\" %\n                             (self._min_requests_per_connection[host_distance],))\n        self._min_requests_per_connection[host_distance] = min_requests", "response": "Sets a threshold for concurrent requests per connection below which\n        connections will be considered for disposal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting a threshold for concurrent requests per connection above which new connections will be created to a host.", "response": "def set_max_requests_per_connection(self, host_distance, max_requests):\n        \"\"\"\n        Sets a threshold for concurrent requests per connection, above which new\n        connections will be created to a host (up to max connections;\n        see :meth:`~Cluster.set_max_connections_per_host`).\n\n        Pertains to connection pool management in protocol versions {1,2}.\n        \"\"\"\n        if self.protocol_version >= 3:\n            raise UnsupportedOperation(\n                \"Cluster.set_max_requests_per_connection() only has an effect \"\n                \"when using protocol_version 1 or 2.\")\n        if max_requests < 1 or max_requests > 127 or \\\n           max_requests <= self._min_requests_per_connection[host_distance]:\n            raise ValueError(\"max_requests must be 1-127 and greater than the min_requests for this host_distance (%d)\" %\n                             (self._min_requests_per_connection[host_distance],))\n        self._max_requests_per_connection[host_distance] = max_requests"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the minimum number of connections per host for each host.", "response": "def set_core_connections_per_host(self, host_distance, core_connections):\n        \"\"\"\n        Sets the minimum number of connections per Session that will be opened\n        for each host with :class:`~.HostDistance` equal to `host_distance`.\n        The default is 2 for :attr:`~HostDistance.LOCAL` and 1 for\n        :attr:`~HostDistance.REMOTE`.\n\n        Protocol version 1 and 2 are limited in the number of concurrent\n        requests they can send per connection. The driver implements connection\n        pooling to support higher levels of concurrency.\n\n        If :attr:`~.Cluster.protocol_version` is set to 3 or higher, this\n        is not supported (there is always one connection per host, unless\n        the host is remote and :attr:`connect_to_remote_hosts` is :const:`False`)\n        and using this will result in an :exc:`~.UnsupporteOperation`.\n        \"\"\"\n        if self.protocol_version >= 3:\n            raise UnsupportedOperation(\n                \"Cluster.set_core_connections_per_host() only has an effect \"\n                \"when using protocol_version 1 or 2.\")\n        old = self._core_connections_per_host[host_distance]\n        self._core_connections_per_host[host_distance] = core_connections\n        if old < core_connections:\n            self._ensure_core_connections()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_max_connections_per_host(self, host_distance, max_connections):\n        if self.protocol_version >= 3:\n            raise UnsupportedOperation(\n                \"Cluster.set_max_connections_per_host() only has an effect \"\n                \"when using protocol_version 1 or 2.\")\n        self._max_connections_per_host[host_distance] = max_connections", "response": "Sets the maximum number of connections per Session for each host."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new connection object with proper configuration.", "response": "def connection_factory(self, endpoint, *args, **kwargs):\n        \"\"\"\n        Called to create a new connection with proper configuration.\n        Intended for internal use only.\n        \"\"\"\n        kwargs = self._make_connection_kwargs(endpoint, kwargs)\n        return self.connection_class.factory(endpoint, self.connect_timeout, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, keyspace=None, wait_for_all_pools=False):\n        with self._lock:\n            if self.is_shutdown:\n                raise DriverException(\"Cluster is already shut down\")\n\n            if not self._is_setup:\n                log.debug(\"Connecting to cluster, contact points: %s; protocol version: %s\",\n                          self.contact_points, self.protocol_version)\n                self.connection_class.initialize_reactor()\n                _register_cluster_shutdown(self)\n                for endpoint in self.endpoints_resolved:\n                    host, new = self.add_host(endpoint, signal=False)\n                    if new:\n                        host.set_up()\n                        for listener in self.listeners:\n                            listener.on_add(host)\n\n                self.profile_manager.populate(\n                    weakref.proxy(self), self.metadata.all_hosts())\n                self.load_balancing_policy.populate(\n                    weakref.proxy(self), self.metadata.all_hosts()\n                )\n\n                try:\n                    self.control_connection.connect()\n\n                    # we set all contact points up for connecting, but we won't infer state after this\n                    for endpoint in self.endpoints_resolved:\n                        h = self.metadata.get_host(endpoint)\n                        if h and self.profile_manager.distance(h) == HostDistance.IGNORED:\n                            h.is_up = None\n\n                    log.debug(\"Control connection created\")\n                except Exception:\n                    log.exception(\"Control connection failed to connect, \"\n                                  \"shutting down Cluster:\")\n                    self.shutdown()\n                    raise\n\n                self.profile_manager.check_supported()  # todo: rename this method\n\n                if self.idle_heartbeat_interval:\n                    self._idle_heartbeat = ConnectionHeartbeat(\n                        self.idle_heartbeat_interval,\n                        self.get_connection_holders,\n                        timeout=self.idle_heartbeat_timeout\n                    )\n                self._is_setup = True\n\n        session = self._new_session(keyspace)\n        if wait_for_all_pools:\n            wait_futures(session._initial_connect_futures)\n        return session", "response": "Connects to the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shutdown(self):\n        with self._lock:\n            if self.is_shutdown:\n                return\n            else:\n                self.is_shutdown = True\n\n        if self._idle_heartbeat:\n            self._idle_heartbeat.stop()\n\n        self.scheduler.shutdown()\n\n        self.control_connection.shutdown()\n\n        for session in tuple(self.sessions):\n            session.shutdown()\n\n        self.executor.shutdown()\n\n        _discard_cluster_shutdown(self)", "response": "Closes all sessions and connection associated with this Cluster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_up(self, host):\n        if self.is_shutdown:\n            return\n\n        log.debug(\"Waiting to acquire lock for handling up status of node %s\", host)\n        with host.lock:\n            if host._currently_handling_node_up:\n                log.debug(\"Another thread is already handling up status of node %s\", host)\n                return\n\n            if host.is_up:\n                log.debug(\"Host %s was already marked up\", host)\n                return\n\n            host._currently_handling_node_up = True\n        log.debug(\"Starting to handle up status of node %s\", host)\n\n        have_future = False\n        futures = set()\n        try:\n            log.info(\"Host %s may be up; will prepare queries and open connection pool\", host)\n\n            reconnector = host.get_and_set_reconnection_handler(None)\n            if reconnector:\n                log.debug(\"Now that host %s is up, cancelling the reconnection handler\", host)\n                reconnector.cancel()\n\n            if self.profile_manager.distance(host) != HostDistance.IGNORED:\n                self._prepare_all_queries(host)\n                log.debug(\"Done preparing all queries for host %s, \", host)\n\n            for session in tuple(self.sessions):\n                session.remove_pool(host)\n\n            log.debug(\"Signalling to load balancing policies that host %s is up\", host)\n            self.profile_manager.on_up(host)\n\n            log.debug(\"Signalling to control connection that host %s is up\", host)\n            self.control_connection.on_up(host)\n\n            log.debug(\"Attempting to open new connection pools for host %s\", host)\n            futures_lock = Lock()\n            futures_results = []\n            callback = partial(self._on_up_future_completed, host, futures, futures_results, futures_lock)\n            for session in tuple(self.sessions):\n                future = session.add_or_renew_pool(host, is_host_addition=False)\n                if future is not None:\n                    have_future = True\n                    future.add_done_callback(callback)\n                    futures.add(future)\n        except Exception:\n            log.exception(\"Unexpected failure handling node %s being marked up:\", host)\n            for future in futures:\n                future.cancel()\n\n            self._cleanup_failed_on_up_handling(host)\n\n            with host.lock:\n                host._currently_handling_node_up = False\n            raise\n        else:\n            if not have_future:\n                with host.lock:\n                    host.set_up()\n                    host._currently_handling_node_up = False\n\n        # for testing purposes\n        return futures", "response": "Called by the agent when a node is up."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_down(self, host, is_host_addition, expect_host_to_be_down=False):\n        if self.is_shutdown:\n            return\n\n        with host.lock:\n            was_up = host.is_up\n\n            # ignore down signals if we have open pools to the host\n            # this is to avoid closing pools when a control connection host became isolated\n            if self._discount_down_events and self.profile_manager.distance(host) != HostDistance.IGNORED:\n                connected = False\n                for session in tuple(self.sessions):\n                    pool_states = session.get_pool_state()\n                    pool_state = pool_states.get(host)\n                    if pool_state:\n                        connected |= pool_state['open_count'] > 0\n                if connected:\n                    return\n\n            host.set_down()\n            if (not was_up and not expect_host_to_be_down) or host.is_currently_reconnecting():\n                return\n\n        log.warning(\"Host %s has been marked down\", host)\n\n        self.profile_manager.on_down(host)\n        self.control_connection.on_down(host)\n        for session in tuple(self.sessions):\n            session.on_down(host)\n\n        for listener in self.listeners:\n            listener.on_down(host)\n\n        self._start_reconnector(host, is_host_addition)", "response": "Called by the profile manager when a host is marked down."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a host to the metadata.", "response": "def add_host(self, endpoint, datacenter=None, rack=None, signal=True, refresh_nodes=True):\n        \"\"\"\n        Called when adding initial contact points and when the control\n        connection subsequently discovers a new node.\n        Returns a Host instance, and a flag indicating whether it was new in\n        the metadata.\n        Intended for internal use only.\n        \"\"\"\n        host, new = self.metadata.add_or_return_host(Host(endpoint, self.conviction_policy_factory, datacenter, rack))\n        if new and signal:\n            log.info(\"New Cassandra host %r discovered\", host)\n            self.on_add(host, refresh_nodes)\n\n        return host, new"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when the control connection observes that a node has left the ring.", "response": "def remove_host(self, host):\n        \"\"\"\n        Called when the control connection observes that a node has left the\n        ring.  Intended for internal use only.\n        \"\"\"\n        if host and self.metadata.remove_host(host):\n            log.info(\"Cassandra host %s removed\", host)\n            self.on_remove(host)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ensure_core_connections(self):\n        for session in tuple(self.sessions):\n            for pool in tuple(session._pools.values()):\n                pool.ensure_core_connections()", "response": "Ensure that all connections in the core pool are open."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the control connection host metadata.", "response": "def get_control_connection_host(self):\n        \"\"\"\n        Returns the control connection host metadata.\n        \"\"\"\n        connection = self.control_connection._connection\n        endpoint = connection.endpoint if connection else None\n        return self.metadata.get_host(endpoint) if endpoint else None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refresh_schema_metadata(self, max_schema_agreement_wait=None):\n        if not self.control_connection.refresh_schema(schema_agreement_wait=max_schema_agreement_wait, force=True):\n            raise DriverException(\"Schema metadata was not refreshed. See log for details.\")", "response": "Refresh all schema metadata."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrefresh the metadata of a keyspace.", "response": "def refresh_keyspace_metadata(self, keyspace, max_schema_agreement_wait=None):\n        \"\"\"\n        Synchronously refresh keyspace metadata. This applies to keyspace-level information such as replication\n        and durability settings. It does not refresh tables, types, etc. contained in the keyspace.\n\n        See :meth:`~.Cluster.refresh_schema_metadata` for description of ``max_schema_agreement_wait`` behavior\n        \"\"\"\n        if not self.control_connection.refresh_schema(target_type=SchemaTargetType.KEYSPACE, keyspace=keyspace,\n                                                      schema_agreement_wait=max_schema_agreement_wait, force=True):\n            raise DriverException(\"Keyspace metadata was not refreshed. See log for details.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrefresh the table metadata.", "response": "def refresh_table_metadata(self, keyspace, table, max_schema_agreement_wait=None):\n        \"\"\"\n        Synchronously refresh table metadata. This applies to a table, and any triggers or indexes attached\n        to the table.\n\n        See :meth:`~.Cluster.refresh_schema_metadata` for description of ``max_schema_agreement_wait`` behavior\n        \"\"\"\n        if not self.control_connection.refresh_schema(target_type=SchemaTargetType.TABLE, keyspace=keyspace, table=table,\n                                                      schema_agreement_wait=max_schema_agreement_wait, force=True):\n            raise DriverException(\"Table metadata was not refreshed. See log for details.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef refresh_user_type_metadata(self, keyspace, user_type, max_schema_agreement_wait=None):\n        if not self.control_connection.refresh_schema(target_type=SchemaTargetType.TYPE, keyspace=keyspace, type=user_type,\n                                                      schema_agreement_wait=max_schema_agreement_wait, force=True):\n            raise DriverException(\"User Type metadata was not refreshed. See log for details.\")", "response": "Refresh user defined type metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrefreshing user defined function metadata.", "response": "def refresh_user_function_metadata(self, keyspace, function, max_schema_agreement_wait=None):\n        \"\"\"\n        Synchronously refresh user defined function metadata.\n\n        ``function`` is a :class:`cassandra.UserFunctionDescriptor`.\n\n        See :meth:`~.Cluster.refresh_schema_metadata` for description of ``max_schema_agreement_wait`` behavior\n        \"\"\"\n        if not self.control_connection.refresh_schema(target_type=SchemaTargetType.FUNCTION, keyspace=keyspace, function=function,\n                                                      schema_agreement_wait=max_schema_agreement_wait, force=True):\n            raise DriverException(\"User Function metadata was not refreshed. See log for details.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh_user_aggregate_metadata(self, keyspace, aggregate, max_schema_agreement_wait=None):\n        if not self.control_connection.refresh_schema(target_type=SchemaTargetType.AGGREGATE, keyspace=keyspace, aggregate=aggregate,\n                                                      schema_agreement_wait=max_schema_agreement_wait, force=True):\n            raise DriverException(\"User Aggregate metadata was not refreshed. See log for details.\")", "response": "Refresh user defined aggregate metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_meta_refresh_enabled(self, enabled):\n        warn(\"Cluster.set_meta_refresh_enabled is deprecated and will be removed in 4.0. Set \"\n             \"Cluster.schema_metadata_enabled and Cluster.token_metadata_enabled instead.\", DeprecationWarning)\n        self.schema_metadata_enabled = enabled\n        self.token_metadata_enabled = enabled", "response": "Deprecated. Use set_schema_metadata_enabled and set_token_metadata_enabled to enable or disable all metadata refresh queries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a query and synchronously wait for the response.", "response": "def execute(self, query, parameters=None, timeout=_NOT_SET, trace=False,\n                custom_payload=None, execution_profile=EXEC_PROFILE_DEFAULT,\n                paging_state=None, host=None):\n        \"\"\"\n        Execute the given query and synchronously wait for the response.\n\n        If an error is encountered while executing the query, an Exception\n        will be raised.\n\n        `query` may be a query string or an instance of :class:`cassandra.query.Statement`.\n\n        `parameters` may be a sequence or dict of parameters to bind.  If a\n        sequence is used, ``%s`` should be used the placeholder for each\n        argument.  If a dict is used, ``%(name)s`` style placeholders must\n        be used.\n\n        `timeout` should specify a floating-point timeout (in seconds) after\n        which an :exc:`.OperationTimedOut` exception will be raised if the query\n        has not completed.  If not set, the timeout defaults to\n        :attr:`~.Session.default_timeout`.  If set to :const:`None`, there is\n        no timeout. Please see :meth:`.ResponseFuture.result` for details on\n        the scope and effect of this timeout.\n\n        If `trace` is set to :const:`True`, the query will be sent with tracing enabled.\n        The trace details can be obtained using the returned :class:`.ResultSet` object.\n\n        `custom_payload` is a :ref:`custom_payload` dict to be passed to the server.\n        If `query` is a Statement with its own custom_payload. The message payload\n        will be a union of the two, with the values specified here taking precedence.\n\n        `execution_profile` is the execution profile to use for this request. It can be a key to a profile configured\n        via :meth:`Cluster.add_execution_profile` or an instance (from :meth:`Session.execution_profile_clone_update`,\n        for example\n\n        `paging_state` is an optional paging state, reused from a previous :class:`ResultSet`.\n\n        `host` is the :class:`pool.Host` that should handle the query. Using this is discouraged except in a few\n        cases, e.g., querying node-local tables and applying schema changes.\n        \"\"\"\n        return self.execute_async(query, parameters, trace, custom_payload,\n                                  timeout, execution_profile, paging_state, host).result()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a query and return a ResponseFuture object.", "response": "def execute_async(self, query, parameters=None, trace=False, custom_payload=None,\n                      timeout=_NOT_SET, execution_profile=EXEC_PROFILE_DEFAULT,\n                      paging_state=None, host=None):\n        \"\"\"\n        Execute the given query and return a :class:`~.ResponseFuture` object\n        which callbacks may be attached to for asynchronous response\n        delivery.  You may also call :meth:`~.ResponseFuture.result()`\n        on the :class:`.ResponseFuture` to synchronously block for results at\n        any time.\n\n        See :meth:`Session.execute` for parameter definitions.\n\n        Example usage::\n\n            >>> session = cluster.connect()\n            >>> future = session.execute_async(\"SELECT * FROM mycf\")\n\n            >>> def log_results(results):\n            ...     for row in results:\n            ...         log.info(\"Results: %s\", row)\n\n            >>> def log_error(exc):\n            >>>     log.error(\"Operation failed: %s\", exc)\n\n            >>> future.add_callbacks(log_results, log_error)\n\n        Async execution with blocking wait for results::\n\n            >>> future = session.execute_async(\"SELECT * FROM mycf\")\n            >>> # do other stuff...\n\n            >>> try:\n            ...     results = future.result()\n            ... except Exception:\n            ...     log.exception(\"Operation failed:\")\n\n        \"\"\"\n        future = self._create_response_future(\n            query, parameters, trace, custom_payload, timeout,\n            execution_profile, paging_state, host)\n        future._protocol_handler = self.client_protocol_handler\n        self._on_request(future)\n        future.send_request()\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_response_future(self, query, parameters, trace, custom_payload,\n                                timeout, execution_profile=EXEC_PROFILE_DEFAULT,\n                                paging_state=None, host=None):\n        \"\"\" Returns the ResponseFuture before calling send_request() on it \"\"\"\n\n        prepared_statement = None\n\n        if isinstance(query, six.string_types):\n            query = SimpleStatement(query)\n        elif isinstance(query, PreparedStatement):\n            query = query.bind(parameters)\n\n        if self.cluster._config_mode == _ConfigMode.LEGACY:\n            if execution_profile is not EXEC_PROFILE_DEFAULT:\n                raise ValueError(\"Cannot specify execution_profile while using legacy parameters.\")\n\n            if timeout is _NOT_SET:\n                timeout = self.default_timeout\n\n            cl = query.consistency_level if query.consistency_level is not None else self.default_consistency_level\n            serial_cl = query.serial_consistency_level if query.serial_consistency_level is not None else self.default_serial_consistency_level\n\n            retry_policy = query.retry_policy or self.cluster.default_retry_policy\n            row_factory = self.row_factory\n            load_balancing_policy = self.cluster.load_balancing_policy\n            spec_exec_policy = None\n        else:\n            execution_profile = self._maybe_get_execution_profile(execution_profile)\n\n            if timeout is _NOT_SET:\n                timeout = execution_profile.request_timeout\n\n            cl = query.consistency_level if query.consistency_level is not None else execution_profile.consistency_level\n            serial_cl = query.serial_consistency_level if query.serial_consistency_level is not None else execution_profile.serial_consistency_level\n\n            retry_policy = query.retry_policy or execution_profile.retry_policy\n            row_factory = execution_profile.row_factory\n            load_balancing_policy = execution_profile.load_balancing_policy\n            spec_exec_policy = execution_profile.speculative_execution_policy\n\n        fetch_size = query.fetch_size\n        if fetch_size is FETCH_SIZE_UNSET and self._protocol_version >= 2:\n            fetch_size = self.default_fetch_size\n        elif self._protocol_version == 1:\n            fetch_size = None\n\n        start_time = time.time()\n        if self._protocol_version >= 3 and self.use_client_timestamp:\n            timestamp = self.cluster.timestamp_generator()\n        else:\n            timestamp = None\n\n        if isinstance(query, SimpleStatement):\n            query_string = query.query_string\n            statement_keyspace = query.keyspace if ProtocolVersion.uses_keyspace_flag(self._protocol_version) else None\n            if parameters:\n                query_string = bind_params(query_string, parameters, self.encoder)\n            message = QueryMessage(\n                query_string, cl, serial_cl,\n                fetch_size, timestamp=timestamp,\n                keyspace=statement_keyspace)\n        elif isinstance(query, BoundStatement):\n            prepared_statement = query.prepared_statement\n            message = ExecuteMessage(\n                prepared_statement.query_id, query.values, cl,\n                serial_cl, fetch_size,\n                timestamp=timestamp, skip_meta=bool(prepared_statement.result_metadata),\n                result_metadata_id=prepared_statement.result_metadata_id)\n        elif isinstance(query, BatchStatement):\n            if self._protocol_version < 2:\n                raise UnsupportedOperation(\n                    \"BatchStatement execution is only supported with protocol version \"\n                    \"2 or higher (supported in Cassandra 2.0 and higher).  Consider \"\n                    \"setting Cluster.protocol_version to 2 to support this operation.\")\n            statement_keyspace = query.keyspace if ProtocolVersion.uses_keyspace_flag(self._protocol_version) else None\n            message = BatchMessage(\n                query.batch_type, query._statements_and_parameters, cl,\n                serial_cl, timestamp, statement_keyspace)\n\n        message.tracing = trace\n\n        message.update_custom_payload(query.custom_payload)\n        message.update_custom_payload(custom_payload)\n        message.allow_beta_protocol_version = self.cluster.allow_beta_protocol_version\n        message.paging_state = paging_state\n\n        spec_exec_plan = spec_exec_policy.new_plan(query.keyspace or self.keyspace, query) if query.is_idempotent and spec_exec_policy else None\n        return ResponseFuture(\n            self, message, query, timeout, metrics=self._metrics,\n            prepared_statement=prepared_statement, retry_policy=retry_policy, row_factory=row_factory,\n            load_balancer=load_balancing_policy, start_time=start_time, speculative_execution_plan=spec_exec_plan,\n            host=host)", "response": "Creates a ResponseFuture for the given query and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_execution_profile(self, name):\n        profiles = self.cluster.profile_manager.profiles\n        try:\n            return profiles[name]\n        except KeyError:\n            raise ValueError(\"Invalid execution_profile: '%s'; valid profiles are %s\" % (name, profiles.keys()))", "response": "Returns the execution profile associated with the provided name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a shallow clone of the execution profile with the given kwargs.", "response": "def execution_profile_clone_update(self, ep, **kwargs):\n        \"\"\"\n        Returns a clone of the ``ep`` profile.  ``kwargs`` can be specified to update attributes\n        of the returned profile.\n\n        This is a shallow clone, so any objects referenced by the profile are shared. This means Load Balancing Policy\n        is maintained by inclusion in the active profiles. It also means updating any other rich objects will be seen\n        by the active profile. In cases where this is not desirable, be sure to replace the instance instead of manipulating\n        the shared object.\n        \"\"\"\n        clone = copy(self._maybe_get_execution_profile(ep))\n        for attr, value in kwargs.items():\n            setattr(clone, attr, value)\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a callback to be invoked when any request is created.", "response": "def add_request_init_listener(self, fn, *args, **kwargs):\n        \"\"\"\n        Adds a callback with arguments to be called when any request is created.\n\n        It will be invoked as `fn(response_future, *args, **kwargs)` after each client request is created,\n        and before the request is sent\\*. This can be used to create extensions by adding result callbacks to the\n        response future.\n\n        \\* where `response_future` is the :class:`.ResponseFuture` for the request.\n\n        Note that the init callback is done on the client thread creating the request, so you may need to consider\n        synchronization if you have multiple threads. Any callbacks added to the response future will be executed\n        on the event loop thread, so the normal advice about minimizing cycles and avoiding blocking apply (see Note in\n        :meth:`.ResponseFuture.add_callbacks`.\n\n        See `this example <https://github.com/datastax/python-driver/blob/master/examples/request_init_listener.py>`_ in the\n        source tree for an example.\n        \"\"\"\n        self._request_init_callbacks.append((fn, args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_request_init_listener(self, fn, *args, **kwargs):\n        self._request_init_callbacks.remove((fn, args, kwargs))", "response": "Removes a callback and arguments from the list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing a query string returning a ResponseFuture instance.", "response": "def prepare(self, query, custom_payload=None, keyspace=None):\n        \"\"\"\n        Prepares a query string, returning a :class:`~cassandra.query.PreparedStatement`\n        instance which can be used as follows::\n\n            >>> session = cluster.connect(\"mykeyspace\")\n            >>> query = \"INSERT INTO users (id, name, age) VALUES (?, ?, ?)\"\n            >>> prepared = session.prepare(query)\n            >>> session.execute(prepared, (user.id, user.name, user.age))\n\n        Or you may bind values to the prepared statement ahead of time::\n\n            >>> prepared = session.prepare(query)\n            >>> bound_stmt = prepared.bind((user.id, user.name, user.age))\n            >>> session.execute(bound_stmt)\n\n        Of course, prepared statements may (and should) be reused::\n\n            >>> prepared = session.prepare(query)\n            >>> for user in users:\n            ...     bound = prepared.bind((user.id, user.name, user.age))\n            ...     session.execute(bound)\n\n        Alternatively, if :attr:`~.Cluster.protocol_version` is 5 or higher\n        (requires Cassandra 4.0+), the keyspace can be specified as a\n        parameter. This will allow you to avoid specifying the keyspace in the\n        query without specifying a keyspace in :meth:`~.Cluster.connect`. It\n        even will let you prepare and use statements against a keyspace other\n        than the one originally specified on connection:\n\n            >>> analyticskeyspace_prepared = session.prepare(\n            ...     \"INSERT INTO user_activity id, last_activity VALUES (?, ?)\",\n            ...     keyspace=\"analyticskeyspace\")  # note the different keyspace\n\n        **Important**: PreparedStatements should be prepared only once.\n        Preparing the same query more than once will likely affect performance.\n\n        `custom_payload` is a key value map to be passed along with the prepare\n        message. See :ref:`custom_payload`.\n        \"\"\"\n        message = PrepareMessage(query=query, keyspace=keyspace)\n        future = ResponseFuture(self, message, query=None, timeout=self.default_timeout)\n        try:\n            future.send_request()\n            query_id, bind_metadata, pk_indexes, result_metadata, result_metadata_id = future.result()\n        except Exception:\n            log.exception(\"Error preparing query:\")\n            raise\n\n        prepared_keyspace = keyspace if keyspace else None\n        prepared_statement = PreparedStatement.from_message(\n            query_id, bind_metadata, pk_indexes, self.cluster.metadata, query, self.keyspace,\n            self._protocol_version, result_metadata, result_metadata_id)\n        prepared_statement.custom_payload = future.custom_payload\n\n        self.cluster.add_prepared(query_id, prepared_statement)\n\n        if self.cluster.prepare_on_all_hosts:\n            host = future._current_host\n            try:\n                self.prepare_on_all_hosts(prepared_statement.query_string, host, prepared_keyspace)\n            except Exception:\n                log.exception(\"Error preparing query on all hosts:\")\n\n        return prepared_statement"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_on_all_hosts(self, query, excluded_host, keyspace=None):\n        futures = []\n        for host in tuple(self._pools.keys()):\n            if host != excluded_host and host.is_up:\n                future = ResponseFuture(self, PrepareMessage(query=query, keyspace=keyspace),\n                                            None, self.default_timeout)\n\n                # we don't care about errors preparing against specific hosts,\n                # since we can always prepare them as needed when the prepared\n                # statement is used.  Just log errors and continue on.\n                try:\n                    request_id = future._query(host)\n                except Exception:\n                    log.exception(\"Error preparing query for host %s:\", host)\n                    continue\n\n                if request_id is None:\n                    # the error has already been logged by ResponsFuture\n                    log.debug(\"Failed to prepare query for host %s: %r\",\n                              host, future._errors.get(host))\n                    continue\n\n                futures.append((host, future))\n\n        for host, future in futures:\n            try:\n                future.result()\n            except Exception:\n                log.exception(\"Error preparing query for host %s:\", host)", "response": "Prepare the given query on all hosts excluding excluded_host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose all connections and wait for all the active connection attempts to finish.", "response": "def shutdown(self):\n        \"\"\"\n        Close all connections.  ``Session`` instances should not be used\n        for any purpose after being shutdown.\n        \"\"\"\n        with self._lock:\n            if self.is_shutdown:\n                return\n            else:\n                self.is_shutdown = True\n\n        # PYTHON-673. If shutdown was called shortly after session init, avoid\n        # a race by cancelling any initial connection attempts haven't started,\n        # then blocking on any that have.\n        for future in self._initial_connect_futures:\n            future.cancel()\n        wait_futures(self._initial_connect_futures)\n\n        for pool in tuple(self._pools.values()):\n            pool.shutdown()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds or renew a new connection pool.", "response": "def add_or_renew_pool(self, host, is_host_addition):\n        \"\"\"\n        For internal use only.\n        \"\"\"\n        distance = self._profile_manager.distance(host)\n        if distance == HostDistance.IGNORED:\n            return None\n\n        def run_add_or_renew_pool():\n            try:\n                if self._protocol_version >= 3:\n                    new_pool = HostConnection(host, distance, self)\n                else:\n                    # TODO remove host pool again ???\n                    new_pool = HostConnectionPool(host, distance, self)\n            except AuthenticationFailed as auth_exc:\n                conn_exc = ConnectionException(str(auth_exc), host=host)\n                self.cluster.signal_connection_failure(host, conn_exc, is_host_addition)\n                return False\n            except Exception as conn_exc:\n                log.warning(\"Failed to create connection pool for new host %s:\",\n                            host, exc_info=conn_exc)\n                # the host itself will still be marked down, so we need to pass\n                # a special flag to make sure the reconnector is created\n                self.cluster.signal_connection_failure(\n                    host, conn_exc, is_host_addition, expect_host_to_be_down=True)\n                return False\n\n            previous = self._pools.get(host)\n            with self._lock:\n                while new_pool._keyspace != self.keyspace:\n                    self._lock.release()\n                    set_keyspace_event = Event()\n                    errors_returned = []\n\n                    def callback(pool, errors):\n                        errors_returned.extend(errors)\n                        set_keyspace_event.set()\n\n                    new_pool._set_keyspace_for_all_conns(self.keyspace, callback)\n                    set_keyspace_event.wait(self.cluster.connect_timeout)\n                    if not set_keyspace_event.is_set() or errors_returned:\n                        log.warning(\"Failed setting keyspace for pool after keyspace changed during connect: %s\", errors_returned)\n                        self.cluster.on_down(host, is_host_addition)\n                        new_pool.shutdown()\n                        self._lock.acquire()\n                        return False\n                    self._lock.acquire()\n                self._pools[host] = new_pool\n\n            log.debug(\"Added pool for host %s to session\", host)\n            if previous:\n                previous.shutdown()\n\n            return True\n\n        return self.submit(run_add_or_renew_pool)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the load balancer pools based on the current distance of the nodes.", "response": "def update_created_pools(self):\n        \"\"\"\n        When the set of live nodes change, the loadbalancer will change its\n        mind on host distances. It might change it on the node that came/left\n        but also on other nodes (for instance, if a node dies, another\n        previously ignored node may be now considered).\n\n        This method ensures that all hosts for which a pool should exist\n        have one, and hosts that shouldn't don't.\n\n        For internal use only.\n        \"\"\"\n        futures = set()\n        for host in self.cluster.metadata.all_hosts():\n            distance = self._profile_manager.distance(host)\n            pool = self._pools.get(host)\n            future = None\n            if not pool or pool.is_shutdown:\n                # we don't eagerly set is_up on previously ignored hosts. None is included here\n                # to allow us to attempt connections to hosts that have gone from ignored to something\n                # else.\n                if distance != HostDistance.IGNORED and host.is_up in (True, None):\n                    future = self.add_or_renew_pool(host, False)\n            elif distance != pool.host_distance:\n                # the distance has changed\n                if distance == HostDistance.IGNORED:\n                    future = self.remove_pool(host)\n                else:\n                    pool.host_distance = distance\n            if future:\n                futures.add(future)\n        return futures"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_down(self, host):\n        future = self.remove_pool(host)\n        if future:\n            future.add_done_callback(lambda f: self.update_created_pools())", "response": "Called by the parent Cluster instance when a node is marked down."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall by the parent Cluster instance when the user registers a new user - defined type.", "response": "def user_type_registered(self, keyspace, user_type, klass):\n        \"\"\"\n        Called by the parent Cluster instance when the user registers a new\n        mapping from a user-defined type to a class.  Intended for internal\n        use only.\n        \"\"\"\n        try:\n            ks_meta = self.cluster.metadata.keyspaces[keyspace]\n        except KeyError:\n            raise UserTypeDoesNotExist(\n                'Keyspace %s does not exist or has not been discovered by the driver' % (keyspace,))\n\n        try:\n            type_meta = ks_meta.user_types[user_type]\n        except KeyError:\n            raise UserTypeDoesNotExist(\n                'User type %s does not exist in keyspace %s' % (user_type, keyspace))\n\n        field_names = type_meta.field_names\n        if six.PY2:\n            # go from unicode to string to avoid decode errors from implicit\n            # decode when formatting non-ascii values\n            field_names = [fn.encode('utf-8') for fn in field_names]\n\n        def encode(val):\n            return '{ %s }' % ' , '.join('%s : %s' % (\n                field_name,\n                self.encoder.cql_encode_all_types(getattr(val, field_name, None))\n            ) for field_name in field_names)\n\n        self.encoder.mapping[klass] = encode"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef submit(self, fn, *args, **kwargs):\n        if not self.is_shutdown:\n            return self.cluster.executor.submit(fn, *args, **kwargs)", "response": "Submit a function to the cluster executor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset a new connection to the control.", "response": "def _set_new_connection(self, conn):\n        \"\"\"\n        Replace existing connection (if there is one) and close it.\n        \"\"\"\n        with self._lock:\n            old = self._connection\n            self._connection = conn\n\n        if old:\n            log.debug(\"[control connection] Closing old connection %r, replacing with %r\", old, conn)\n            old.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to connect to each host in the query plan until one succeeds or a new Connection will be raised.", "response": "def _reconnect_internal(self):\n        \"\"\"\n        Tries to connect to each host in the query plan until one succeeds\n        or every attempt fails. If successful, a new Connection will be\n        returned.  Otherwise, :exc:`NoHostAvailable` will be raised\n        with an \"errors\" arg that is a dict mapping host addresses\n        to the exception that was raised when an attempt was made to open\n        a connection to that host.\n        \"\"\"\n        errors = {}\n        lbp = (\n            self._cluster.load_balancing_policy\n            if self._cluster._config_mode == _ConfigMode.LEGACY else\n            self._cluster._default_load_balancing_policy\n        )\n\n        for host in lbp.make_query_plan():\n            try:\n                return self._try_connect(host)\n            except ConnectionException as exc:\n                errors[str(host.endpoint)] = exc\n                log.warning(\"[control connection] Error connecting to %s:\", host, exc_info=True)\n                self._cluster.signal_connection_failure(host, exc, is_host_addition=False)\n            except Exception as exc:\n                errors[str(host.endpoint)] = exc\n                log.warning(\"[control connection] Error connecting to %s:\", host, exc_info=True)\n            if self._is_shutdown:\n                raise DriverException(\"[control connection] Reconnection in progress during shutdown\")\n\n        raise NoHostAvailable(\"Unable to connect to any servers\", errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _try_connect(self, host):\n        log.debug(\"[control connection] Opening new connection to %s\", host)\n\n        while True:\n            try:\n                connection = self._cluster.connection_factory(host.endpoint, is_control_connection=True)\n                if self._is_shutdown:\n                    connection.close()\n                    raise DriverException(\"Reconnecting during shutdown\")\n                break\n            except ProtocolVersionUnsupported as e:\n                self._cluster.protocol_downgrade(host.endpoint, e.startup_version)\n\n        log.debug(\"[control connection] Established new connection %r, \"\n                  \"registering watchers and refreshing schema and topology\",\n                  connection)\n\n        # use weak references in both directions\n        # _clear_watcher will be called when this ControlConnection is about to be finalized\n        # _watch_callback will get the actual callback from the Connection and relay it to\n        # this object (after a dereferencing a weakref)\n        self_weakref = weakref.ref(self, partial(_clear_watcher, weakref.proxy(connection)))\n        try:\n            connection.register_watchers({\n                \"TOPOLOGY_CHANGE\": partial(_watch_callback, self_weakref, '_handle_topology_change'),\n                \"STATUS_CHANGE\": partial(_watch_callback, self_weakref, '_handle_status_change'),\n                \"SCHEMA_CHANGE\": partial(_watch_callback, self_weakref, '_handle_schema_change')\n            }, register_timeout=self._timeout)\n\n            sel_peers = self._SELECT_PEERS if self._token_meta_enabled else self._SELECT_PEERS_NO_TOKENS\n            sel_local = self._SELECT_LOCAL if self._token_meta_enabled else self._SELECT_LOCAL_NO_TOKENS\n            peers_query = QueryMessage(query=sel_peers, consistency_level=ConsistencyLevel.ONE)\n            local_query = QueryMessage(query=sel_local, consistency_level=ConsistencyLevel.ONE)\n            shared_results = connection.wait_for_responses(\n                peers_query, local_query, timeout=self._timeout)\n\n            self._refresh_node_list_and_token_map(connection, preloaded_results=shared_results)\n            self._refresh_schema(connection, preloaded_results=shared_results, schema_agreement_wait=-1)\n        except Exception:\n            connection.close()\n            raise\n\n        return connection", "response": "Attempts to connect to the specified host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_and_set_reconnection_handler(self, new_handler):\n        with self._reconnection_lock:\n            old = self._reconnection_handler\n            self._reconnection_handler = new_handler\n            return old", "response": "Get and set the _ControlReconnectionHandler for the current _ControlReconnectionHandler."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _address_from_row(self, row):\n        addr = None\n        if \"rpc_address\" in row:\n            addr = row.get(\"rpc_address\")  # peers and local\n        if \"native_transport_address\" in row:\n            addr = row.get(\"native_transport_address\")\n        if not addr or addr in [\"0.0.0.0\", \"::\"]:\n            addr = row.get(\"peer\")\n\n        return addr", "response": "Parse the broadcast rpc address from a row and return it untranslated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when the request timed out.", "response": "def _on_timeout(self, _attempts=0):\n        \"\"\"\n        Called when the request associated with this ResponseFuture times out.\n\n        This function may reschedule itself. The ``_attempts`` parameter tracks\n        the number of times this has happened. This parameter should only be\n        set in those cases, where ``_on_timeout`` reschedules itself.\n        \"\"\"\n        # PYTHON-853: for short timeouts, we sometimes race with our __init__\n        if self._connection is None and _attempts < 3:\n            self._timer = self.session.cluster.connection_class.create_timer(\n                0.01,\n                partial(self._on_timeout, _attempts=_attempts + 1)\n            )\n            return\n\n        if self._connection is not None:\n            try:\n                self._connection._requests.pop(self._req_id)\n            # This prevents the race condition of the\n            # event loop thread just receiving the waited message\n            # If it arrives after this, it will be ignored\n            except KeyError:\n                return\n\n            pool = self.session._pools.get(self._current_host)\n            if pool and not pool.is_shutdown:\n                with self._connection.lock:\n                    self._connection.request_ids.append(self._req_id)\n\n                pool.return_connection(self._connection)\n\n        errors = self._errors\n        if not errors:\n            if self.is_schema_agreed:\n                key = str(self._current_host.endpoint) if self._current_host else 'no host queried before timeout'\n                errors = {key: \"Client request timeout. See Session.execute[_async](timeout)\"}\n            else:\n                connection = self.session.cluster.control_connection._connection\n                host = str(connection.endpoint) if connection else 'unknown'\n                errors = {host: \"Request timed out while waiting for schema agreement. See Session.execute[_async](timeout) and Cluster.max_schema_agreement_wait.\"}\n\n        self._set_final_exception(OperationTimedOut(errors, self._current_host))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_fetching_next_page(self):\n        if not self._paging_state:\n            raise QueryExhausted()\n\n        self._make_query_plan()\n        self.message.paging_state = self._paging_state\n        self._event.clear()\n        self._final_result = _NOT_SET\n        self._final_exception = None\n        self._start_timer()\n        self.send_request()", "response": "Start fetching the next page of the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the response to our attempt to prepare a statement.", "response": "def _execute_after_prepare(self, host, connection, pool, response):\n        \"\"\"\n        Handle the response to our attempt to prepare a statement.\n        If it succeeded, run the original query again against the same host.\n        \"\"\"\n        if pool:\n            pool.return_connection(connection)\n\n        if self._final_exception:\n            return\n\n        if isinstance(response, ResultMessage):\n            if response.kind == RESULT_KIND_PREPARED:\n                if self.prepared_statement:\n                    # result metadata is the only thing that could have\n                    # changed from an alter\n                    (_, _, _,\n                     self.prepared_statement.result_metadata,\n                     new_metadata_id) = response.results\n                    if new_metadata_id is not None:\n                        self.prepared_statement.result_metadata_id = new_metadata_id\n\n                # use self._query to re-use the same host and\n                # at the same time properly borrow the connection\n                request_id = self._query(host)\n                if request_id is None:\n                    # this host errored out, move on to the next\n                    self.send_request()\n            else:\n                self._set_final_exception(ConnectionException(\n                    \"Got unexpected response when preparing statement \"\n                    \"on host %s: %s\" % (host, response)))\n        elif isinstance(response, ErrorMessage):\n            if hasattr(response, 'to_exception'):\n                self._set_final_exception(response.to_exception())\n            else:\n                self._set_final_exception(response)\n        elif isinstance(response, ConnectionException):\n            log.debug(\"Connection error when preparing statement on host %s: %s\",\n                      host, response)\n            # try again on a different host, preparing again if necessary\n            self._errors[host] = response\n            self.send_request()\n        else:\n            self._set_final_exception(ConnectionException(\n                \"Got unexpected response type when preparing \"\n                \"statement on host %s: %s\" % (host, response)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef result(self):\n        self._event.wait()\n        if self._final_result is not _NOT_SET:\n            return ResultSet(self, self._final_result)\n        else:\n            raise self._final_exception", "response": "Return the final result or raise an exception if errors were encountered."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_query_trace(self, max_wait=None, query_cl=ConsistencyLevel.LOCAL_ONE):\n        if self._final_result is _NOT_SET and self._final_exception is None:\n            raise TraceUnavailable(\n                \"Trace information was not available. The ResponseFuture is not done.\")\n\n        if self._query_traces:\n            return self._get_query_trace(len(self._query_traces) - 1, max_wait, query_cl)", "response": "Fetches and returns the last response from the Cassandra query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_query_traces(self, max_wait_per=None, query_cl=ConsistencyLevel.LOCAL_ONE):\n        if self._query_traces:\n            return [self._get_query_trace(i, max_wait_per, query_cl) for i in range(len(self._query_traces))]\n        return []", "response": "Fetches and returns the query traces for all query pages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_callback(self, fn, *args, **kwargs):\n        run_now = False\n        with self._callback_lock:\n            # Always add fn to self._callbacks, even when we're about to\n            # execute it, to prevent races with functions like\n            # start_fetching_next_page that reset _final_result\n            self._callbacks.append((fn, args, kwargs))\n            if self._final_result is not _NOT_SET:\n                run_now = True\n        if run_now:\n            fn(self._final_result, *args, **kwargs)\n        return self", "response": "Adds a callback function to be called when the final result arrives."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a callback function to be called when an exception occurs.", "response": "def add_errback(self, fn, *args, **kwargs):\n        \"\"\"\n        Like :meth:`.add_callback()`, but handles error cases.\n        An Exception instance will be passed as the first positional argument\n        to `fn`.\n        \"\"\"\n        run_now = False\n        with self._callback_lock:\n            # Always add fn to self._errbacks, even when we're about to execute\n            # it, to prevent races with functions like start_fetching_next_page\n            # that reset _final_exception\n            self._errbacks.append((fn, args, kwargs))\n            if self._final_exception:\n                run_now = True\n        if run_now:\n            fn(self._final_exception, *args, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_callbacks(self, callback, errback,\n                      callback_args=(), callback_kwargs=None,\n                      errback_args=(), errback_kwargs=None):\n        \"\"\"\n        A convenient combination of :meth:`.add_callback()` and\n        :meth:`.add_errback()`.\n\n        Example usage::\n\n            >>> session = cluster.connect()\n            >>> query = \"SELECT * FROM mycf\"\n            >>> future = session.execute_async(query)\n\n            >>> def log_results(results, level='debug'):\n            ...     for row in results:\n            ...         log.log(level, \"Result: %s\", row)\n\n            >>> def log_error(exc, query):\n            ...     log.error(\"Query '%s' failed: %s\", query, exc)\n\n            >>> future.add_callbacks(\n            ...     callback=log_results, callback_kwargs={'level': 'info'},\n            ...     errback=log_error, errback_args=(query,))\n\n        \"\"\"\n        self.add_callback(callback, *callback_args, **(callback_kwargs or {}))\n        self.add_errback(errback, *errback_args, **(errback_kwargs or {}))", "response": "Add callbacks to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef one(self):\n        row = None\n        if self._current_rows:\n            try:\n                row = self._current_rows[0]\n            except TypeError:  # generator object is not subscriptable, PYTHON-1026\n                row = next(iter(self._current_rows))\n\n        return row", "response": "Return a single row of the result set or None if there are no results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef was_applied(self):\n        if self.response_future.row_factory not in (named_tuple_factory, dict_factory, tuple_factory):\n            raise RuntimeError(\"Cannot determine LWT result with row factory %s\" % (self.response_future.row_factory,))\n\n        is_batch_statement = isinstance(self.response_future.query, BatchStatement)\n        if is_batch_statement and (not self.column_names or self.column_names[0] != \"[applied]\"):\n            raise RuntimeError(\"No LWT were present in the BatchStatement\")\n\n        if not is_batch_statement and len(self.current_rows) != 1:\n            raise RuntimeError(\"LWT result should have exactly one row. This has %d.\" % (len(self.current_rows)))\n\n        row = self.current_rows[0]\n        if isinstance(row, tuple):\n            return row[0]\n        else:\n            return row['[applied]']", "response": "Returns whether the transaction was applied."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays the files in the liststore", "response": "def display_files(self, pcs_files):\n        '''\u91cd\u65b0\u683c\u5f0f\u5316\u4e00\u4e0b\u6587\u4ef6\u5217\u8868, \u53bb\u9664\u4e0d\u9700\u8981\u7684\u4fe1\u606f\n\n        \u8fd9\u4e00\u64cd\u4f5c\u4e3b\u8981\u662f\u4e3a\u4e86\u4fbf\u4e8e\u63a5\u4e0b\u6765\u7684\u67e5\u627e\u5de5\u4f5c.\n        \u6587\u4ef6\u7684path\u90fd\u88ab\u63d0\u53d6\u51fa\u6765, \u7136\u540e\u653e\u5230\u4e86\u4e00\u4e2alist\u4e2d.\n        '''\n        tree_iters = []\n        for pcs_file in pcs_files:\n            path = pcs_file['path']\n            pixbuf, type_ = self.app.mime.get(path, pcs_file['isdir'],\n                                              icon_size=self.ICON_SIZE)\n            name = os.path.split(path)[NAME_COL]\n            tooltip = gutil.escape(name)\n            size = pcs_file.get('size', 0)\n            if pcs_file['isdir']:\n                human_size = '--'\n            else:\n                human_size = util.get_human_size(pcs_file['size'])[0]\n            mtime = pcs_file.get('server_mtime', 0)\n            human_mtime = time.ctime(mtime)\n            tree_iter = self.liststore.append([\n                pixbuf, name, path, tooltip, size, human_size,\n                pcs_file['isdir'], mtime, human_mtime, type_,\n                json.dumps(pcs_file)\n            ])\n            tree_iters.append(tree_iter)\n        cache_path = Config.get_cache_path(self.app.profile['username'])\n        gutil.async_call(gutil.update_liststore_image, self.liststore,\n                         tree_iters, PIXBUF_COL, pcs_files, cache_path,\n                         self.ICON_SIZE)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_drag_data_get(self, widget, context, data, info, time):\n        '''\u62d6\u653e\u5f00\u59cb'''\n        tree_paths = self.iconview.get_selected_items()\n        if not tree_paths:\n            return\n        filelist = []\n        for tree_path in tree_paths:\n            filelist.append({\n                'path': self.liststore[tree_path][PATH_COL],\n                'newname': self.liststore[tree_path][NAME_COL],\n            })\n        filelist_str = json.dumps(filelist)\n        if info == TargetInfo.PLAIN_TEXT:\n            data.set_text(filelist_str, -1)\n        # \u62d6\u62fd\u65f6\u65e0\u6cd5\u83b7\u53d6\u76ee\u6807\u8def\u5f84, \u6240\u4ee5\u4e0d\u80fd\u5b9e\u73b0\u62d6\u62fd\u4e0b\u8f7d\u529f\u80fd\n        elif info == TargetInfo.URI_LIST:\n            data.set_uris([])", "response": "Drag data get event handler"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlaunching an app with the given tree_path.", "response": "def launch_app(self, tree_path):\n        '''\u7528\u9ed8\u8ba4\u7684\u7a0b\u5e8f\u6253\u5f00\u8fd9\u4e2a\u6587\u4ef6\u94fe\u63a5.'''\n        file_type = self.liststore[tree_path][TYPE_COL]\n        app_infos = Gio.AppInfo.get_recommended_for_type(file_type)\n        if app_infos:\n            self.launch_app_with_app_info(app_infos[0])\n        else:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the BT task to the liststore.", "response": "def on_cloud_download_item_activated(self, menu_item):\n        '''\u521b\u5efa\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1, \u4e0b\u8f7d\u9009\u4e2d\u7684BT\u79cd\u5b50.'''\n        tree_paths = self.iconview.get_selected_items()\n        if not tree_paths:\n            return\n        self.app.cloud_page.add_cloud_bt_task(\n                self.liststore[tree_paths[0]][PATH_COL])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving to menu item.", "response": "def on_download_to_activated(self, menu_item):\n        '''\u4e0b\u8f7d\u6587\u4ef6/\u76ee\u5f55\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u5939\u91cc.'''\n        tree_paths = self.iconview.get_selected_items()\n        if not tree_paths:\n            return\n\n        dialog = Gtk.FileChooserDialog(_('Save to...'), self.app.window,\n                Gtk.FileChooserAction.SELECT_FOLDER,\n                (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL,\n                 Gtk.STOCK_OK, Gtk.ResponseType.OK))\n        response = dialog.run()\n        if response != Gtk.ResponseType.OK:\n            dialog.destroy()\n            return\n        dirname = dialog.get_filename()\n        dialog.destroy()\n\n        pcs_files = [self.get_pcs_file(p) for p in tree_paths]\n        self.app.blink_page(self.app.download_page)\n        self.app.download_page.add_tasks(pcs_files, dirname)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_drag_data_received(self, widget, context, x, y, data, info, time):\n        '''\u62d6\u653e\u7ed3\u675f'''\n        if not data:\n            return\n        bx, by = self.iconview.convert_widget_to_bin_window_coords(x, y)\n        selected = Gtk.TreeView.get_path_at_pos(self.iconview, bx, by)\n        if not selected:\n            return\n        tree_path = selected[0]\n        if tree_path is None:\n            return\n        target_path = self.liststore[tree_path][PATH_COL]\n        is_dir = self.liststore[tree_path][ISDIR_COL]\n        if not is_dir or info != TargetInfo.PLAIN_TEXT:\n            return\n        filelist_str = data.get_text()\n        filelist = json.loads(filelist_str)\n        for file_item in filelist:\n            if file_item['path'] == target_path:\n                self.app.toast(_('Error: Move folder to itself!'))\n                return\n        for file_item in filelist:\n            file_item['dest'] = target_path\n        gutil.async_call(pcs.move, self.app.cookie, self.app.tokens, filelist,\n                         callback=self.parent.reload)", "response": "Drag data received from the user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_path(self):\n        '''\u83b7\u53d6\u9009\u62e9\u7684\u8def\u5f84, \u5982\u679c\u6ca1\u6709\u9009\u62e9, \u5c31\u8fd4\u56de\u6839\u76ee\u5f55'''\n        model, tree_iter = self.selection.get_selected()\n        if not tree_iter:\n            return '/'\n        else:\n            return model[tree_iter][PATH_COL]", "response": "get_path - Get path of the current page"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_db(self):\n        '''\u8fd9\u4e2a\u4efb\u52a1\u6570\u636e\u5e93\u53ea\u5728\u7a0b\u5e8f\u5f00\u59cb\u65f6\u8bfb\u5165, \u5728\u7a0b\u5e8f\u5173\u95ed\u65f6\u5bfc\u51fa.\n\n        \u56e0\u4e3aGtk\u6ca1\u6709\u50cf\u5728Qt\u4e2d\u90a3\u4e48\u65b9\u4fbf\u7684\u4f7f\u7528SQLite, \u800c\u5fc5\u987b\u5c06\u6240\u6709\u6570\u636e\u8bfb\u5165\u4e00\u4e2a\n        liststore\u4e2d\u624d\u884c.\n        '''\n        cache_path = os.path.join(Config.CACHE_DIR,\n                                  self.app.profile['username'])\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path, exist_ok=True)\n        db = os.path.join(cache_path, TASK_FILE)\n        self.conn = sqlite3.connect(db)\n        self.cursor = self.conn.cursor()\n        sql = '''CREATE TABLE IF NOT EXISTS tasks (\n        name CHAR NOT NULL,\n        path CHAR NOT NULL,\n        fsid CHAR NOT NULL,\n        size INTEGER NOT NULL,\n        currsize INTEGER NOT NULL,\n        link CHAR,\n        isdir INTEGER,\n        savename CHAR NOT NULL,\n        savedir CHAR NOT NULL,\n        state INT NOT NULL,\n        statename CHAR NOT NULL,\n        humansize CHAR NOT NULL,\n        percent INT NOT NULL,\n        tooltip CHAR\n        )\n        '''\n        self.cursor.execute(sql)", "response": "\u8fd9\u4e2a\u4efb\u52a1\u6570\u636e\u5e93\u53ea\u5728\u7a0b\u5e8f\u5f00\u59cb\u65f6\u8bfb\u5165, \u5728\u7a0b\u5e8f\u5173\u95ed\u65f6\u5bfc\u51fa.\n\n        \u56e0\u4e3aGtk\u6ca1\u6709\u50cf\u5728Qt\u4e2d\u90a3\u4e48\u65b9\u4fbf\u7684\u4f7f\u7528SQLite, \u800c\u5fc5\u987b\u5c06\u6240\u6709\u6570\u636e\u8bfb\u5165\u4e00\u4e2a\n        liststore\u4e2d\u624d\u884c."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a task to the database", "response": "def add_task_db(self, task):\n        '''\u5411\u6570\u636e\u5e93\u4e2d\u5199\u5165\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u8bb0\u5f55'''\n        sql = 'INSERT INTO tasks VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?)'\n        req = self.cursor.execute(sql, task)\n        self.check_commit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_task_db(self, fs_id):\n        '''\u4ece\u6570\u636e\u5e93\u4e2d\u67e5\u8be2fsid\u7684\u4fe1\u606f.\n        \n        \u5982\u679c\u5b58\u5728\u7684\u8bdd, \u5c31\u8fd4\u56de\u8fd9\u6761\u8bb0\u5f55;\n        \u5982\u679c\u6ca1\u6709\u7684\u8bdd, \u5c31\u8fd4\u56deNone\n        '''\n        sql = 'SELECT * FROM tasks WHERE fsid=?'\n        req = self.cursor.execute(sql, [fs_id, ])\n        if req:\n            return req.fetchone()\n        else:\n            None", "response": "get the task db"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the database with the given row.", "response": "def update_task_db(self, row):\n        '''\u66f4\u65b0\u6570\u636e\u5e93\u4e2d\u7684\u4efb\u52a1\u4fe1\u606f'''\n        sql = '''UPDATE tasks SET \n        currsize=?, state=?, statename=?, humansize=?, percent=?\n        WHERE fsid=?\n        '''\n        self.cursor.execute(sql, [\n            row[CURRSIZE_COL], row[STATE_COL], row[STATENAME_COL],\n            row[HUMANSIZE_COL], row[PERCENT_COL], row[FSID_COL]\n        ])\n        self.check_commit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_task_db(self, fs_id):\n        '''\u5c06\u4efb\u52a1\u4ece\u6570\u636e\u5e93\u4e2d\u5220\u9664'''\n        sql = 'DELETE FROM tasks WHERE fsid=?'\n        self.cursor.execute(sql, [fs_id, ])\n        self.check_commit()", "response": "Removes the task db from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_row_by_fsid(self, fs_id):\n        '''\u786e\u8ba4\u5728Liststore\u4e2d\u662f\u5426\u5b58\u5728\u8fd9\u6761\u4efb\u52a1. \u5982\u679c\u5b58\u5728, \u8fd4\u56deTreeModelRow,\n        \u5426\u5219\u5c31\u8fd4\u56deNone'''\n        for row in self.liststore:\n            if row[FSID_COL] == fs_id:\n                return row\n        return None", "response": "Return TreeModelRow object by FSID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_tasks(self, pcs_files, dirname=''):\n        '''\u5efa\u7acb\u6279\u91cf\u4e0b\u8f7d\u4efb\u52a1, \u5305\u62ec\u76ee\u5f55'''\n        def on_list_dir(info, error=None):\n            path, pcs_files = info\n            if error or not pcs_files:\n                dialog = Gtk.MessageDialog(self.app.window,\n                        Gtk.DialogFlags.MODAL,\n                        Gtk.MessageType.ERROR, Gtk.ButtonsType.CLOSE,\n                        _('Failed to scan folder to download'))\n                dialog.format_secondary_text(\n                        _('Please download {0} again').format(path))\n                dialog.run()\n                dialog.destroy()\n                return\n            self.add_tasks(pcs_files, dirname)\n\n        self.check_first()\n        for pcs_file in pcs_files:\n            if pcs_file['isdir']:\n                gutil.async_call(pcs.list_dir_all, self.app.cookie,\n                                 self.app.tokens, pcs_file['path'],\n                                 callback=on_list_dir)\n            else:\n                self.add_task(pcs_file, dirname)\n        self.check_commit(force=True)", "response": "add_tasks - Add tasks to the queue"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_task(self, pcs_file, dirname=''):\n        '''\u52a0\u5165\u65b0\u7684\u4e0b\u8f7d\u4efb\u52a1'''\n        if pcs_file['isdir']:\n            return\n        fs_id = str(pcs_file['fs_id'])\n        row = self.get_row_by_fsid(fs_id)\n        if row:\n            self.app.toast(_('Task exists: {0}').format(\n                           pcs_file['server_filename']))\n            # \u5982\u679c\u6587\u4ef6\u5df2\u4e0b\u8f7d\u5b8c\u6210, \u5c31\u76f4\u63a5\u5c1d\u8bd5\u7528\u672c\u5730\u7a0b\u5e8f\u6253\u5f00\n            if row[STATE_COL] == State.FINISHED:\n                self.launch_app(fs_id)\n            return\n        if not dirname:\n            dirname = self.app.profile['save-dir']\n        save_dir = os.path.dirname(\n                os.path.join(dirname, pcs_file['path'][1:]))\n        save_name = pcs_file['server_filename']\n        human_size = util.get_human_size(pcs_file['size'])[0]\n        tooltip = gutil.escape(_('From {0}\\nTo {1}').format(pcs_file['path'],\n                                                            save_dir))\n        task = (\n            pcs_file['server_filename'],\n            pcs_file['path'],\n            fs_id,\n            pcs_file['size'],\n            0,\n            '',  # pcs['dlink' removed in new version.\n            pcs_file['isdir'],\n            save_name,\n            save_dir,\n            State.WAITING,\n            StateNames[State.WAITING],\n            human_size,\n            0,\n            tooltip,\n        )\n        self.liststore.append(task)\n        self.add_task_db(task)\n        self.scan_tasks()", "response": "add a new task to the liststore"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscanning tasks in the liststore.", "response": "def scan_tasks(self, ignore_shutdown=False):\n        '''\u626b\u63cf\u6240\u6709\u4e0b\u8f7d\u4efb\u52a1, \u5e76\u5728\u9700\u8981\u65f6\u542f\u52a8\u65b0\u7684\u4e0b\u8f7d.'''\n        for row in self.liststore:\n            if len(self.workers.keys()) >= self.app.profile['concurr-download']:\n                break\n            if row[STATE_COL] == State.WAITING:\n                self.start_worker(row)\n\n        if not self.shutdown_button.get_active() or ignore_shutdown:\n            return\n        # Shutdown system after all tasks have finished\n        for row in self.liststore:\n            if (row[STATE_COL] not in\n                    (State.PAUSED, State.FINISHED, State.CANCELED)):\n                return\n        self.shutdown.shutdown()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_task(self, row, scan=True):\n        '''\u542f\u52a8\u4e0b\u8f7d\u4efb\u52a1.\n\n        \u5c06\u4efb\u52a1\u72b6\u6001\u8bbe\u5b9a\u4e3aDownloading, \u5982\u679c\u6ca1\u6709\u8d85\u8fc7\u6700\u5927\u4efb\u52a1\u6570\u7684\u8bdd;\n        \u5426\u5219\u5c06\u5b83\u8bbe\u5b9a\u4e3aWaiting.\n        '''\n        if not row or row[STATE_COL] in RUNNING_STATES :\n            return\n        row[STATE_COL] = State.WAITING\n        row[STATENAME_COL] = StateNames[State.WAITING]\n        self.update_task_db(row)\n        if scan:\n            self.scan_tasks()", "response": "start a task from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npauses all tasks in the liststore", "response": "def pause_tasks(self):\n        '''\u6682\u505c\u6240\u6709\u4e0b\u8f7d\u4efb\u52a1'''\n        if self.first_run:\n            return\n        for row in self.liststore:\n            self.pause_task(row, scan=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef operate_selected_rows(self, operator):\n        '''\u5bf9\u9009\u4e2d\u7684\u6761\u76ee\u8fdb\u884c\u64cd\u4f5c.\n\n        operator  - \u5904\u7406\u51fd\u6570\n        '''\n        model, tree_paths = self.selection.get_selected_rows()\n        if not tree_paths:\n            return\n        fs_ids = []\n        for tree_path in tree_paths:\n            fs_ids.append(model[tree_path][FSID_COL])\n        for fs_id in fs_ids:\n            row = self.get_row_by_fsid(fs_id)\n            if not row:\n                return\n            operator(row, scan=False)\n        self.check_commit(force=True)\n        self.scan_tasks(ignore_shutdown=True)", "response": "This method operate selected rows."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload url and save it to liststore", "response": "def load_url(self):\n        '''\u8bfb\u53d6\u5206\u4eab\u6587\u4ef6\u5217\u8868'''\n        def on_load_url(filelist, error=None):\n            self.url_entry.props.secondary_icon_name = REFRESH_ICON\n            if timestamp != self.url_entry.timestamp:\n                logger.debug('SharePage.load_url, dirname not match, ignored')\n                return\n            if error or not filelist:\n                self.app.toast(\n                        _('Failed to get files, please reload this page'))\n                logger.warn('SharePage.load_url: %s, %s, %s' %\n                            (self.curr_url, filelist, error))\n                self.has_next = False\n                return\n            state = self.select_all_button.get_active()\n            tree_iters = []\n\n            # \u63d2\u5165.. \u70b9\u51fb\u540e\u8fd4\u56de\u4e0a\u4e2a\u76ee\u5f55\n            if filelist and self.dirname and self.dirname != '/':\n                parent_dirname = os.path.dirname(self.dirname)\n                pixbuf, type_ = self.app.mime.get(parent_dirname, True,\n                                                  icon_size=ICON_SIZE)\n                large_pixbuf, type_ = self.app.mime.get(parent_dirname, True,\n                        icon_size=LARGE_ICON_SIZE)\n                self.liststore.append([\n                    state,\n                    pixbuf,\n                    large_pixbuf,\n                    '..',\n                    parent_dirname,\n                    True,\n                    0,\n                    '0',\n                    0,\n                    '',\n                ])\n\n            for file_ in filelist:\n                isdir = file_['isdir'] == '1'\n                pixbuf, type_ = self.app.mime.get(file_['path'], isdir,\n                                                  icon_size=ICON_SIZE)\n                large_pixbuf, type_ = self.app.mime.get(file_['path'], isdir,\n                        icon_size=LARGE_ICON_SIZE)\n                size = int(file_.get('size', 0))\n                human_size = util.get_human_size(size)[0]\n                mtime = int(file_.get('server_mtime', 0))\n                human_mtime = time.ctime(mtime)\n                tree_iter = self.liststore.append([\n                    state,\n                    pixbuf,\n                    large_pixbuf,\n                    file_['server_filename'],\n                    file_['path'],\n                    isdir,\n                    size,\n                    human_size,\n                    mtime,\n                    human_mtime,\n                ])\n                tree_iters.append(tree_iter)\n            cache_path = Config.get_cache_path(self.app.profile['username'])\n            gutil.async_call(gutil.update_share_image, self.liststore,\n                             tree_iters, ICON_COL, LARGE_ICON_COL,\n                             filelist, cache_path,\n                             ICON_SIZE, LARGE_ICON_SIZE)\n\n        self.url_entry.props.secondary_icon_name = ABORT_ICON\n        if not self.uk or not self.shareid:\n            self.app.toast(_('Invalid link: {0}!').format(self.curr_url))\n            self.has_next = False\n            self.url_entry.props.secondary_icon_name = REFRESH_ICON\n            return\n        timestamp = time.time()\n        self.url_entry.timestamp = timestamp\n        gutil.async_call(pcs.list_share_files, self.app.cookie, self.app.tokens,\n                         self.uk, self.shareid, self.dirname, self.page,\n                         callback=on_load_url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to connect to the given dbus services. If successful it will return a callable dbus proxy and those arguments.", "response": "def _prepair(self):\n        '''Try to connect to the given dbus services. If successful it will\n        return a callable dbus proxy and those arguments.\n        '''\n        try:\n            sessionbus = dbus.SessionBus()\n            systembus  = dbus.SystemBus()\n        except:\n            return (None, None)\n        for dbus_props in self.DBUS_SHUTDOWN.values():\n            try:\n                if dbus_props['bus'] == SESSION_BUS:\n                    bus = sessionbus\n                else:\n                    bus = systembus\n                interface = bus.get_object(dbus_props['service'],\n                                           dbus_props['objectPath'])\n                proxy = interface.get_dbus_method(dbus_props['method'],\n                                                  dbus_props['interface'])\n                return (proxy, dbus_props['arguments'])\n            except dbus.exceptions.DBusException:\n                continue\n        return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the dbus proxy to start the shutdown.", "response": "def shutdown(self):\n        '''Call the dbus proxy to start the shutdown.'''\n        if self._proxy:\n            os.sync()\n            self._proxy(*self._args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_app_shutdown(self, app):\n        '''Dump profile content to disk'''\n\n        if self.filewatcher:\n            self.filewatcher.stop()\n        if self.profile:\n            self.upload_page.on_destroy()\n            self.download_page.on_destroy()", "response": "Dump profile content to disk"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_main_window_drag_data_received(self, window, drag_context, x, y,\n                                          data, info, time):\n        '''\u4ece\u5176\u5b83\u7a0b\u5e8f\u62d6\u653e\u76ee\u5f55/\u6587\u4ef6, \u4ee5\u4fbf\u4e0a\u4f20.\n\n        \u8fd9\u91cc, \u4f1a\u5f39\u51fa\u4e00\u4e2a\u9009\u62e9\u76ee\u6807\u6587\u4ef6\u5939\u7684\u5bf9\u8bdd\u6846\n        '''\n        if not self.profile:\n            return\n        if info == TargetInfo.URI_LIST:\n            uris = data.get_uris()\n            source_paths = util.uris_to_paths(uris)\n            if source_paths:\n                self.upload_page.upload_files(source_paths)", "response": "called when data is received from the main window"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the clipboard with the given text", "response": "def update_clipboard(self, text):\n        '''\u5c06\u6587\u672c\u590d\u5236\u5230\u7cfb\u7edf\u526a\u8d34\u677f\u91cc\u9762'''\n        clipboard = Gtk.Clipboard.get(Gdk.SELECTION_CLIPBOARD)\n        clipboard.set_text(text, -1)\n        self.toast(_('{0} copied to clipboard'.format(text)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the path of the current locale.", "response": "def set_path(self, path, is_user=False):\n        \"\"\"\n        :param bool is_user: this event was fired by user\n        \"\"\"\n        self.clear_buttons()\n        pathlist = util.rec_split_path(path)\n        for (abspath, name) in pathlist:\n            self.append_button(abspath, name)\n\n        if is_user:\n            self.add_view_history(path)\n\n        self.back_button.set_sensitive(self.can_back())\n        self.forward_button.set_sensitive(self.can_forward())\n\n        self.show_all()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_drag_data_received(self, drag_context, x, y, data, info, time):\n        '''\u4ece\u5176\u5b83\u7a0b\u5e8f\u62d6\u653e\u76ee\u5f55/\u6587\u4ef6, \u4ee5\u4fbf\u4e0a\u4f20.\n\n        \u8fd9\u91cc, \u4f1a\u76f4\u63a5\u628a\u6587\u4ef6\u4e0a\u4f20\u5230\u5f53\u524d\u76ee\u5f55(self.path).\n        \u62d6\u653e\u4e8b\u4ef6\u5df2\u7ecf\u88ab\u5904\u7406, \u6240\u4ee5\u4e0d\u4f1a\u89e6\u53d1self.app.window\u7684\u62d6\u653e\u52a8\u4f5c.\n        '''\n        if not self.app.profile:\n            return\n        if info == TargetInfo.URI_LIST:\n            uris = data.get_uris()\n            source_paths = util.uris_to_paths(uris)\n            if source_paths:\n                self.app.upload_page.upload_files(source_paths, self.path)", "response": "Handle data received from the drag_context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads all cloud page items.", "response": "def load(self):\n        '''\u83b7\u53d6\u5f53\u524d\u7684\u79bb\u7ebf\u4efb\u52a1\u5217\u8868'''\n        def on_list_task(info, error=None):\n            self.loading_spin.stop()\n            self.loading_spin.hide()\n            if not info:\n                self.app.toast(_('Network error, info is empty'))\n            if error or not info:\n                logger.error('CloudPage.load: %s, %s' % (info, error))\n                return\n            tasks = info['task_info']\n            for task in tasks:\n                self.liststore.append([\n                    task['task_id'],\n                    task['task_name'],\n                    task['save_path'],\n                    task['source_url'],\n                    0,\n                    0,\n                    int(task['status']),\n                    0,\n                    '0',\n                    gutil.escape(task['save_path'])\n                ])\n            self.scan_tasks()\n\n            nonlocal start\n            start = start + len(tasks)\n            if info['total'] > start:\n                gutil.async_call(pcs.cloud_list_task, self.app.cookie,\n                                 self.app.tokens, start, callback=on_list_task)\n\n        self.loading_spin.start()\n        self.loading_spin.show_all()\n        start = 0\n        gutil.async_call(pcs.cloud_list_task, self.app.cookie, self.app.tokens,\n                         start, callback=on_list_task)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_row_by_task_id(self, task_id):\n        '''\u8fd4\u56de\u8fd9\u4e2a\u4efb\u52a1\u7684TreeModelRow, \u5982\u679c\u4e0d\u5b58\u5728, \u5c31\u8fd4\u56deNone.'''\n        for row in self.liststore:\n            if row and row[TASKID_COL] == task_id:\n                return row\n        return None", "response": "Return the TreeModelRow object for the given task_id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscanning the liststore for tasks and update status of the task.", "response": "def scan_tasks(self):\n        '''\u5b9a\u671f\u83b7\u53d6\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1\u7684\u4fe1\u606f, \u6bd4\u598210\u79d2\u949f'''\n        def update_task_status(info, error=None):\n            if error or not info:\n                logger.error('CloudPage.scan_tasks: %s, %s' % (info, error))\n                return\n            tasks = info['task_info']\n            for row in self.liststore:\n                if not row or row[TASKID_COL] not in tasks:\n                    continue\n                task = tasks[row[TASKID_COL]]\n                row[SIZE_COL] = int(task['file_size'])\n                row[FINISHED_COL] = int(task['finished_size'])\n                row[STATUS_COL] = int(task['status'])\n                if row[SIZE_COL]:\n                    row[PERCENT_COL] = int(\n                            row[FINISHED_COL] / row[SIZE_COL] * 100)\n                size = util.get_human_size(row[SIZE_COL])[0]\n                finished_size = util.get_human_size(row[FINISHED_COL])[0]\n                if row[SIZE_COL] == row[FINISHED_COL]:\n                    row[HUMANSIZE_COL] = size\n                else:\n                    row[HUMANSIZE_COL] = '{0}/{1}'.format(finished_size, size)\n\n        task_ids = [row[TASKID_COL] for row in self.liststore]\n        if task_ids:\n            gutil.async_call(pcs.cloud_query_task, self.app.cookie,\n                             self.app.tokens, task_ids,\n                             callback=update_task_status)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_cloud_bt_task(self, source_url, save_path=None):\n        '''\u4ece\u670d\u52a1\u5668\u4e0a\u83b7\u53d6\u79cd\u5b50, \u5e76\u5efa\u7acb\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1\n\n        source_url - BT \u79cd\u5b50\u5728\u670d\u52a1\u5668\u4e0a\u7684\u7edd\u5bf9\u8def\u5f84, \u6216\u8005\u662f\u78c1\u94fe\u7684\u5730\u5740.\n        save_path  - \u8981\u4fdd\u5b58\u5230\u7684\u8def\u5f84, \u5982\u679c\u4e3aNone, \u5c31\u4f1a\u5f39\u51fa\u76ee\u5f55\u9009\u62e9\u7684\u5bf9\u8bdd\u6846\n        '''\n        def check_vcode(info, error=None):\n            if error or not info:\n                logger.error('CloudPage.check_vcode: %s, %s' % (info, error))\n                return\n            if info.get('error_code', -1) != 0:\n                logger.error('CloudPage.check_vcode: %s, %s' % (info, error))\n\n            if 'task_id' in info or info['error_code'] == 0:\n                self.reload()\n            elif info['error_code'] == -19:\n                vcode_dialog = VCodeDialog(self, self.app, info)\n                response = vcode_dialog.run()\n                vcode_input = vcode_dialog.get_vcode()\n                vcode_dialog.destroy()\n                if response != Gtk.ResponseType.OK:\n                    return\n                gutil.async_call(pcs.cloud_add_bt_task, self.app.cookie,\n                                 self.app.tokens, source_url, save_path,\n                                 selected_idx, file_sha1, info['vcode'],\n                                 vcode_input, callback=check_vcode)\n            else:\n                self.app.toast(_('Error: {0}').format(info['error_msg']))\n\n        self.check_first()\n\n        if not save_path:\n            folder_browser = FolderBrowserDialog(self, self.app, _('Save to..'))\n            response = folder_browser.run()\n            save_path = folder_browser.get_path()\n            folder_browser.destroy()\n            if response != Gtk.ResponseType.OK:\n                return\n        if not save_path:\n            return\n\n        bt_browser = BTBrowserDialog(self, self.app, _('Choose..'),\n                                     source_url, save_path)\n        response = bt_browser.run()\n        selected_idx, file_sha1 = bt_browser.get_selected()\n        bt_browser.destroy()\n        if response != Gtk.ResponseType.OK or not selected_idx:\n            return\n        gutil.async_call(pcs.cloud_add_bt_task, self.app.cookie,\n                         self.app.tokens, source_url, save_path, selected_idx,\n                         file_sha1, callback=check_vcode)\n        self.app.blink_page(self.app.cloud_page)", "response": "Add a new BT task to the cloud page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking first element of the list", "response": "def check_first():\n    '''\u8fd9\u91cc, \u8981\u521b\u5efa\u57fa\u672c\u7684\u76ee\u5f55\u7ed3\u6784'''\n    if not os.path.exists(CONF_DIR):\n        os.makedirs(CONF_DIR, exist_ok=True)\n    if not os.path.exists(CACHE_DIR):\n        os.makedirs(CACHE_DIR, exist_ok=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the configuration file.", "response": "def load_conf():\n    '''\u83b7\u53d6\u57fa\u672c\u8bbe\u5b9a\u4fe1\u606f, \u91cc\u9762\u5b58\u653e\u7740\u6240\u6709\u53ef\u7528\u7684profiles, \u4ee5\u53ca\u9ed8\u8ba4\u7684profile'''\n    if os.path.exists(_conf_file):\n        with open(_conf_file) as fh:\n            return json.load(fh)\n    else:\n        dump_conf(_base_conf)\n        return _base_conf"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the path to the cache file for the given profile name.", "response": "def get_cache_path(profile_name):\n    '''\u83b7\u53d6\u8fd9\u4e2a\u5e10\u6237\u7684\u7f13\u5b58\u76ee\u5f55, \u5982\u679c\u4e0d\u5b58\u5728, \u5c31\u521b\u5efa\u5b83'''\n    path = os.path.join(CACHE_DIR, profile_name, 'cache')\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading a new file to the server.", "response": "def upload(self):\n        '''\u4e00\u822c\u4e0a\u4f20\u6a21\u5f0f.\n\n        \u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u4e0a\u4f20, \u4e0d\u53ef\u4ee5\u4e2d\u65ad\u4e0a\u4f20\u8fc7\u7a0b, \u4f46\u56e0\u4e3a\u53ea\u7528\u5b83\u6765\u4e0a\u4f20\u5c0f\u7684\u6587\u4ef6, \u6240\u4ee5\n        \u6700\u7ec8\u7684\u5f71\u54cd\u4e0d\u4f1a\u5f88\u5927.'''\n        info = pcs.upload(self.cookie, self.row[SOURCEPATH_COL],\n                          self.row[PATH_COL], self.upload_mode)\n        if info:\n            self.emit('uploaded', self.row[FID_COL])\n        else:\n            self.emit('network-error', self.row[FID_COL])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef slice_upload(self):\n        '''\u5206\u7247\u4e0a\u4f20'''\n        self.is_slice_upload = True\n        fid = self.row[FID_COL]\n        slice_start = self.row[CURRSIZE_COL]\n        slice_end = self.row[CURRSIZE_COL]\n        file_size = os.path.getsize(self.row[SOURCEPATH_COL])\n        if file_size < slice_start:\n            self.emit('disk-error', fid)\n            return\n        elif file_size == slice_start and slice_start == self.row[SIZE_COL]:\n            self.emit('uploaded', fid)\n            return\n        fh = open(self.row[SOURCEPATH_COL], 'rb')\n        fh.seek(slice_start)\n        while self.row[STATE_COL] == State.UPLOADING:\n            if slice_end >= file_size:\n                self.emit('merge-files', self.row[FID_COL])\n                break\n            slice_start = slice_end\n            slice_end = min(slice_start + self.row[THRESHOLD_COL], file_size)\n            data = fh.read(slice_end - slice_start)\n            slice_end = slice_start + len(data)\n            info = pcs.slice_upload(self.cookie, data)\n            if info and 'md5' in info:\n                self.emit('slice-sent', fid, slice_end, info['md5'])\n            else:\n                self.emit('network-error', fid)\n                break\n        if not fh.closed:\n            fh.close()\n        return", "response": "slice_upload - Uploads a new item to the server"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a header string for the cookie", "response": "def header_output(self):\n        '''\u53ea\u8f93\u51facookie\u7684key-value\u5b57\u4e32.\n        \n        \u6bd4\u5982: HISTORY=21341; PHPSESSION=3289012u39jsdijf28; token=233129\n        '''\n        result = []\n        for key in self.keys():\n            result.append(key + '=' + self.get(key).value)\n        return '; '.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a human - readable version of the given size.", "response": "def get_human_size(size, use_giga=True):\n    '''\u5c06\u6587\u4ef6\u5927\u5c0f\u7531byte, \u8f6c\u4e3a\u4eba\u7c7b\u53ef\u8bfb\u7684\u5b57\u7b26\u4e32\n    size     -  \u6574\u6570, \u6587\u4ef6\u7684\u5927\u5c0f, \u4ee5byte\u4e3a\u5355\u4f4d\n    use_giga - \u5982\u679c\u8fd9\u4e2a\u9009\u9879\u4e3aFalse, \u90a3\u6700\u5927\u7684\u5355\u4f4d\u5c31\u662fMegaBytes, \u800c\u4e0d\u4f1a\u7528\u5230\n               GigaBytes, \u8fd9\u4e2a\u5728\u663e\u793a\u4e0b\u8f7d\u8fdb\u5ea6\u65f6\u5f88\u6709\u7528, \u56e0\u4e3a\u53ef\u4ee5\u52a8\u6001\u7684\u663e\u793a\u4e0b\u8f7d\n               \u72b6\u6001.\n    '''\n    size_kb = '{0:,}'.format(size)\n    if size < SIZE_K:\n        return ('{0} B'.format(size), size_kb)\n    if size < SIZE_M:\n        return ('{0:.1f} kB'.format(size / SIZE_K), size_kb)\n    if size < SIZE_G or not use_giga:\n        return ('{0:.1f} MB'.format(size / SIZE_M), size_kb)\n    if size < SIZE_T:\n        return ('{0:.1f} GB'.format(size / SIZE_G), size_kb)\n    return ('{0:.1f} TB'.format(size / SIZE_T), size_kb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_delta_days(from_sec, to_sec):\n    '''\u8ba1\u7b97\u4e24\u4e2a\u65f6\u95f4\u8282\u70b9\u4e4b\u95f4\u7684\u65e5\u671f'''\n    seconds = abs(to_sec - from_sec)\n    delta = datetime.timedelta(seconds=seconds)\n    return delta.days", "response": "Get the number of days between two seconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a human - readable time string.", "response": "def get_human_time(t):\n    '''\u5c06\u65f6\u95f4\u6807\u8bb0\u8f6c\u6362\u6210\u5b57\u7b26\u4e32'''\n    if isinstance(t, int):\n        # ignore micro seconds\n        if len(str(t)) == 13:\n            t = t // 1000\n        t = datetime.datetime.fromtimestamp(t)\n    return datetime.datetime.strftime(t, '%Y-%m-%d %H:%M:%S')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the most recent modification time", "response": "def get_recent_mtime(t):\n    '''\u83b7\u53d6\u66f4\u7cbe\u7b80\u7684\u65f6\u95f4.\n\n    \u5982\u679c\u662f\u5f53\u5929\u7684, \u5c31\u8fd4\u56de\u65f6\u95f4; \u5982\u679c\u662f\u5f53\u5e74\u7684, \u5c31\u8fd1\u56de\u6708\u4efd\u548c\u65e5\u671f; \u5426\u5219\u8fd4\u56de\u5b8c\u6574\u7684\u65f6\u95f4\n    '''\n    if isinstance(t, int):\n        # ignore micro seconds\n        if len(str(t)) == 13:\n            t = t // 1000\n        t = datetime.datetime.fromtimestamp(t)\n    now = datetime.datetime.now()\n    delta = now - t\n    if delta.days == 0:\n        return datetime.datetime.strftime(t, '%H:%M:%S')\n    elif now.year == t.year:\n        return datetime.datetime.strftime(t, '%b %d')\n    else:\n        return datetime.datetime.strftime(t, '%b %d %Y')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits path into a list of names and names.", "response": "def rec_split_path(path):\n    '''\u5c06\u4e00\u4e2a\u8def\u5f84\u8fdb\u884c\u5206\u9694, \u5206\u522b\u5f97\u5230\u6bcf\u7236\u6bcd\u7684\u7edd\u5bf9\u8def\u5f84\u53ca\u76ee\u5f55\u540d'''\n    if len(path) > 1 and path.endswith('/'):\n        path = path[:-1]\n    if '/' not in path:\n        return [path,]\n    result = []\n    while path != '/':\n        parent, name = os.path.split(path)\n        result.append((path, name))\n        path = parent\n    result.append(('/', '/'))\n    result.reverse()\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the item at index", "response": "def list_remove_by_index(l, index):\n    '''\u5c06list\u4e2d\u7684index\u4f4d\u7684\u6570\u636e\u5220\u9664'''\n    if index < 0 or index >= len(l):\n        raise ValueError('index out of range')\n    if index == (len(l) - 1):\n        l.pop()\n    elif index == 0:\n        l = l[1:]\n    else:\n        l = l[0:index] + l[index+1:]\n\n    return l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a list of URIs to a list of source paths.", "response": "def uris_to_paths(uris):\n    '''\u5c06\u4e00\u4e32URI\u5730\u5740\u8f6c\u4e3a\u7edd\u5bf9\u8def\u5f84, \u7528\u4e8e\u5904\u7406\u684c\u9762\u7a0b\u5e8f\u4e2d\u7684\u6587\u4ef6\u62d6\u653e'''\n    source_paths = []\n    for uri in uris:\n        source_path = uri_to_path(uri)\n        if source_path:\n            source_paths.append(source_path)\n    return source_paths"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef natsort(string):\n    '''\u6309\u7167\u8bed\u8a00\u91cc\u7684\u610f\u4e49\u5bf9\u5b57\u7b26\u4e32\u8fdb\u884c\u6392\u5e8f.\n\n    \u8fd9\u4e2a\u65b9\u6cd5\u7528\u4e8e\u66ff\u6362\u6309\u7167\u5b57\u7b26\u7f16\u7801\u987a\u5e8f\u5bf9\u5b57\u7b26\u4e32\u8fdb\u884c\u6392\u5e8f.\n    \u76f8\u5173\u94fe\u63a5:\n    http://stackoverflow.com/questions/2545532/python-analog-of-natsort-function-sort-a-list-using-a-natural-order-algorithm\n    http://www.codinghorror.com/blog/2007/12/sorting-for-humans-natural-sort-order.html\n    '''\n    return [int(s) if s.isdigit() else s for s in re.split('(\\d+)', string)]", "response": "\u6309\u7167\u8bed\u8a00\u91cc\u7684\u610f\u4e49\u5bf9\u5b57\u7b26\u4e32\u8fdb\u884c\u6392\u5e8f.\n\n    \u8fd9\u4e2a\u65b9\u6cd5\u7528\u4e8e\u66ff\u6362\u6309\u7167\u5b57\u7b26\u7f16\u7801\u987a\u5e8f\u5bf9\u5b57\u7b26\u4e32\u8fdb\u884c\u6392\u5e8f.\n    \u76f8\u5173\u94fe\u63a5:\n    http://stackoverflow.com/questions/2545532/python-analog-of-natsort-function-sort-a-list-using-a-natural-order-algorithm\n    http://www.codinghorror.com/blog/2007/12/sorting-for-humans-natural-sort-order.html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef RSA_encrypt(public_key, message):\n    '''\u7528RSA\u52a0\u5bc6\u5b57\u7b26\u4e32.\n\n    public_key - \u516c\u94a5\n    message    - \u8981\u52a0\u5bc6\u7684\u4fe1\u606f, \u4f7f\u7528UTF-8\u7f16\u7801\u7684\u5b57\u7b26\u4e32\n    @return    - \u4f7f\u7528base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\n    '''\n    # \u5982\u679c\u6ca1\u80fd\u6210\u529f\u5bfc\u5165RSA\u6a21\u5757, \u5c31\u76f4\u63a5\u8fd4\u56de\u7a7a\u767d\u5b57\u7b26\u4e32.\n    if not globals().get('RSA'):\n        return ''\n    rsakey = RSA.importKey(public_key)\n    rsakey = PKCS1_v1_5.new(rsakey)\n    encrypted = rsakey.encrypt(message.encode())\n    return base64.encodestring(encrypted).decode().replace('\\n', '')", "response": "\u7528RSA\u52a0\u5bc6\u5b57\u7b26\u4e32.\n\n    public_key - \u516c\u94a5\n    message    - \u8981\u52a0\u5bc6\u7684\u4fe1\u606f, \u4f7f\u7528UTF-8\u7f16\u7801\u7684\u5b57\u7b26\u4e32\n    @return    - \u4f7f\u7528base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json_loads_single(s):\n    '''\u5904\u7406\u4e0d\u6807\u51c6JSON\u7ed3\u6784\u5316\u6570\u636e'''\n    try:\n        return json.loads(s.replace(\"'\", '\"').replace('\\t', ''))\n    except (ValueError, UnicodeDecodeError):\n        logger.error(traceback.format_exc())\n        return None", "response": "JSON loads a single string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_pathname(filepath):\n    '''\u68c0\u67e5\u8def\u5f84\u4e2d\u662f\u5426\u5305\u542b\u7279\u6b8a\u5b57\u7b26.\n\n    \u767e\u5ea6\u7f51\u76d8\u5bf9\u8def\u5f84/\u6587\u4ef6\u540d\u7684\u8981\u6c42\u5f88\u4e25\u683c:\n      1. \u8def\u5f84\u957f\u5ea6\u9650\u5236\u4e3a1000\n      2. \u8def\u5f84\u4e2d\u4e0d\u80fd\u5305\u542b\u4ee5\u4e0b\u5b57\u7b26\uff1a\\\\ ? | \" > < : *\n      3. \u6587\u4ef6\u540d\u6216\u8def\u5f84\u540d\u5f00\u5934\u7ed3\u5c3e\u4e0d\u80fd\u662f\u201c.\u201d\u6216\u7a7a\u767d\u5b57\u7b26\uff0c\u7a7a\u767d\u5b57\u7b26\u5305\u62ec: \\r, \\n, \\t, \u7a7a\u683c, \\0, \\x0B\n\n    @return, \u8fd4\u56de\u7684\u72b6\u6001\u7801: 0 \u8868\u793a\u6b63\u5e38\n\n    '''\n    if filepath == '/':\n        return ValidatePathState.OK\n    if len(filepath) > 1000:\n        return ValidatePathState.LENGTH_ERROR\n    filter2 = '\\\\?|\"><:*'\n    for c in filter2:\n        if c in filepath:\n            return ValidatePathState.CHAR_ERROR2\n    paths = rec_split_path(filepath)\n    filter3 = '.\\r\\n\\t \\0\\x0b'\n    for path in paths:\n        if path[0] in filter3 or path[-1] in filter3:\n            return ValidatePathState.CHAR_ERROR3\n    return ValidatePathState.OK", "response": "\u68c0\u67e5\u8def\u5f84\u4e2d\u662f\u5426\u5305\u542b\u7279\u6b8a\u5b57\u7b26.\n\n    \u767e\u5ea6\u7f51\u76d8\u5bf9\u8def\u5f84/\u6587\u4ef6\u540d\u7684\u8981\u6c42\u5f88\u4e25\u683c:\n      1. \u8def\u5f84\u957f\u5ea6\u9650\u5236\u4e3a1000\n      2. \u8def\u5f84\u4e2d\u4e0d\u80fd\u5305\u542b\u4ee5\u4e0b\u5b57\u7b26\uff1a\\\\ ? | \" > < : *\n      3. \u6587\u4ef6\u540d\u6216\u8def\u5f84\u540d\u5f00\u5934\u7ed3\u5c3e\u4e0d\u80fd\u662f\u201c.\u201d\u6216\u7a7a\u767d\u5b57\u7b26\uff0c\u7a7a\u767d\u5b57\u7b26\u5305\u62ec: \\r, \\n, \\t, \u7a7a\u683c, \\0, \\x0B\n\n    @return, \u8fd4\u56de\u7684\u72b6\u6001\u7801: 0 \u8868\u793a\u6b63\u5e38"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_data(self):\n        '''\u5728\u8c03\u7528dialog.run()\u4e4b\u524d\u5148\u8c03\u7528\u8fd9\u4e2a\u51fd\u6570\u6765\u83b7\u53d6\u6570\u636e'''\n        def on_tasks_received(info, error=None):\n            if error or not info:\n                logger.error('BTBrowserDialog.on_tasks_received: %s, %s.' %\n                             (info, error))\n                return\n            if 'magnet_info' in info:\n                tasks = info['magnet_info']\n            elif 'torrent_info' in info:\n                tasks = info['torrent_info']['file_info']\n                self.file_sha1 = info['torrent_info']['sha1']\n            elif 'error_code' in info:\n                logger.error('BTBrowserDialog.on_tasks_received: %s, %s.' %\n                             (info, error))\n                self.app.toast(info.get('error_msg', ''))\n                return\n            else:\n                logger.error('BTBrowserDialog.on_tasks_received: %s, %s.' %\n                             (info, error))\n                self.app.toast(_('Unknown error occured: %s') % info)\n                return\n            for task in tasks:\n                size = int(task['size'])\n                human_size = util.get_human_size(size)[0]\n                select = (size > MIN_SIZE_TO_CHECK or \n                          task['file_name'].endswith(CHECK_EXT))\n                self.liststore.append([\n                    select,\n                    task['file_name'],\n                    size,\n                    human_size,\n                ])\n\n        if self.source_url.startswith('magnet'):\n            gutil.async_call(pcs.cloud_query_magnetinfo, self.app.cookie,\n                             self.app.tokens, self.source_url, self.save_path,\n                             callback=on_tasks_received)\n        else:\n            gutil.async_call(pcs.cloud_query_sinfo, self.app.cookie,\n                             self.app.tokens, self.source_url,\n                             callback=on_tasks_received)", "response": "Request data from the cloud."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_selected(self):\n        '''\u8fd4\u56de\u9009\u4e2d\u8981\u4e0b\u8f7d\u7684\u6587\u4ef6\u7684\u7f16\u53f7\u53casha1\u503c, \u4ece1\u5f00\u59cb\u8ba1\u6570.'''\n        selected_idx = []\n        for i, row in enumerate(self.liststore):\n            if row[CHECK_COL]:\n                selected_idx.append(i + 1)\n        return (selected_idx, self.file_sha1)", "response": "return a tuple of the selected entries and the file sha1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef async_call(func, *args, callback=None):\n    '''Call `func` in background thread, and then call `callback` in Gtk main thread.\n\n    If error occurs in `func`, error will keep the traceback and passed to\n    `callback` as second parameter. Always check `error` is not None.\n    '''\n    def do_call():\n        result = None\n        error = None\n\n        try:\n            result = func(*args)\n        except Exception:\n            error = traceback.format_exc()\n            logger.error(error)\n        if callback:\n            GLib.idle_add(callback, result, error)\n\n    thread = threading.Thread(target=do_call)\n    thread.daemon = True\n    thread.start()", "response": "Call func in background thread and then call callback in Gtk main thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xdg_open(uri):\n    '''\u4f7f\u7528\u684c\u9762\u73af\u5883\u4e2d\u9ed8\u8ba4\u7684\u7a0b\u5e8f\u6253\u5f00\u6307\u5b9a\u7684URI\n    \n    \u5f53\u7136, \u9664\u4e86URI\u683c\u5f0f\u4e4b\u5916, \u4e5f\u53ef\u4ee5\u662f\u8def\u5f84\u540d, \u6587\u4ef6\u540d, \u6bd4\u5982:\n    xdg_open('/etc/issue')\n    \u63a8\u8350\u4f7f\u7528Gio.app_info_xx() \u6765\u542f\u52a8\u4e00\u822c\u7a0b\u5e8f, \u800c\u7528xdg_open() \u6765\u6253\u5f00\u76ee\u5f55.\n    '''\n    try:\n        subprocess.call(['xdg-open', uri, ])\n    except FileNotFoundError:\n        logger.error(traceback.format_exc())", "response": "xdg - open \u6587\u4ef6"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the liststore with the image data.", "response": "def update_liststore_image(liststore, tree_iters, col, pcs_files, dir_name,\n                           icon_size=96):\n    '''\u4e0b\u8f7d\u6587\u4ef6\u7f29\u7565\u56fe, \u5e76\u5c06\u5b83\u663e\u793a\u5230liststore\u91cc.\n    \n    pcs_files - \u91cc\u9762\u5305\u542b\u4e86\u51e0\u4e2a\u5fc5\u8981\u7684\u5b57\u6bb5.\n    dir_name  - \u7f13\u5b58\u76ee\u5f55, \u4e0b\u8f7d\u5230\u7684\u56fe\u7247\u4f1a\u4fdd\u5b58\u8fd9\u4e2a\u76ee\u5f55\u91cc.\n    size      - \u6307\u5b9a\u56fe\u7247\u7684\u7f29\u653e\u5927\u5c0f, \u9ed8\u8ba4\u662f96px.\n    '''\n    def update_image(filepath, tree_iter):\n        try:\n            pix = GdkPixbuf.Pixbuf.new_from_file_at_size(filepath, icon_size,\n                                                         icon_size)\n            tree_path = liststore.get_path(tree_iter)\n            if tree_path is None:\n                return\n            liststore[tree_path][col] = pix\n        except GLib.GError:\n            logger.error(traceback.format_exc())\n\n    def dump_image(url, filepath):\n        req = net.urlopen(url)\n        if not req or not req.data:\n            logger.warn('update_liststore_image(), failed to request %s' % url)\n            return False\n        with open(filepath, 'wb') as fh:\n            fh.write(req.data)\n        return True\n\n    for tree_iter, pcs_file in zip(tree_iters, pcs_files):\n        if 'thumbs' not in pcs_file:\n            continue\n        if 'url1' in pcs_file['thumbs']:\n            key = 'url1'\n        elif 'url2' in pcs_file['thumbs']:\n            key = 'url2'\n        elif 'url3' in pcs_file['thumbs']:\n            key = 'url3'\n        else:\n            continue\n        fs_id = pcs_file['fs_id']\n        url = pcs_file['thumbs'][key]\n        filepath = os.path.join(dir_name, '{0}.jpg'.format(fs_id))\n        if os.path.exists(filepath) and os.path.getsize(filepath):\n            GLib.idle_add(update_image, filepath, tree_iter)\n        elif not url or len(url) < 10:\n            logger.warn('update_liststore_image(), failed to get url')\n        else:\n            status = dump_image(url, filepath)\n            if status:\n                GLib.idle_add(update_image, filepath, tree_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating image of a tree in liststore.", "response": "def update_share_image(liststore, tree_iters, col, large_col, pcs_files,\n                       dir_name, icon_size, large_icon_size):\n    '''\u4e0b\u8f7d\u6587\u4ef6\u7f29\u7565\u56fe, \u5e76\u5c06\u5b83\u663e\u793a\u5230liststore\u91cc.\n\n    \u9700\u8981\u540c\u65f6\u66f4\u65b0\u4e24\u5217\u91cc\u7684\u56fe\u7247, \u7528\u4e0d\u540c\u7684\u7f29\u653e\u5c3a\u5bf8.\n    pcs_files - \u91cc\u9762\u5305\u542b\u4e86\u51e0\u4e2a\u5fc5\u8981\u7684\u5b57\u6bb5.\n    dir_name  - \u7f13\u5b58\u76ee\u5f55, \u4e0b\u8f7d\u5230\u7684\u56fe\u7247\u4f1a\u4fdd\u5b58\u8fd9\u4e2a\u76ee\u5f55\u91cc.\n    '''\n    def update_image(filepath, tree_iter):\n        try:\n            tree_path = liststore.get_path(tree_iter)\n            if tree_path is None:\n                return\n            pix = GdkPixbuf.Pixbuf.new_from_file(filepath)\n            width = pix.get_width()\n            height = pix.get_height()\n            small_pix = pix.scale_simple(icon_size,\n                                         height * icon_size // width,\n                                         GdkPixbuf.InterpType.NEAREST)\n            liststore[tree_path][col] = small_pix\n            liststore[tree_path][large_col] = pix \n        except GLib.GError:\n            logger.error(traceback.format_exc())\n\n    def dump_image(url, filepath):\n        req = net.urlopen(url)\n        if not req or not req.data:\n            logger.warn('update_share_image:, failed to request %s' % url)\n            return False\n        with open(filepath, 'wb') as fh:\n            fh.write(req.data)\n        return True\n\n    for tree_iter, pcs_file in zip(tree_iters, pcs_files):\n        if 'thumbs' not in pcs_file:\n            continue\n        elif 'url2' in pcs_file['thumbs']:\n            key = 'url2'\n        elif 'url1' in pcs_file['thumbs']:\n            key = 'url1'\n        elif 'url3' in pcs_file['thumbs']:\n            key = 'url3'\n        else:\n            continue\n        fs_id = pcs_file['fs_id']\n        url = pcs_file['thumbs'][key]\n        filepath = os.path.join(dir_name, 'share-{0}.jpg'.format(fs_id))\n        if os.path.exists(filepath) and os.path.getsize(filepath):\n            GLib.idle_add(update_image, filepath, tree_iter)\n        elif not url or len(url) < 10:\n            logger.warn('update_share_image: failed to get url %s' % url)\n        else:\n            status = dump_image(url, filepath)\n            if status:\n                GLib.idle_add(update_image, filepath, tree_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_avatar(cookie, tokens, dir_name):\n    '''\u83b7\u53d6\u7528\u6237\u5934\u50cf\u4fe1\u606f'''\n    uk = pcs.get_user_uk(cookie, tokens)\n    if not uk:\n        return None\n    user_info = pcs.get_user_info(tokens, uk)\n    if not user_info:\n        return None\n    img_path = os.path.join(dir_name, 'avatar.jpg')\n    if (os.path.exists(img_path) and\n            time.time() - os.stat(img_path).st_mtime <= AVATAR_UPDATE_INTERVAL):\n        return (uk, user_info['uname'], img_path)\n    img_url = user_info['avatar_url']\n    if not img_url:\n        return None\n    req = net.urlopen(img_url)\n    if not req or not req.data:\n        logger.warn('gutil.update_avatar(), failed to request %s' % url)\n        return None\n    with open(img_path, 'wb') as fh:\n        fh.write(req.data)\n    return (uk, user_info['uname'], img_path)", "response": "get user avatar from cookie"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a profile from a file.", "response": "def load_profile(profile_name):\n    '''\u8bfb\u53d6\u7279\u5b9a\u5e10\u6237\u7684\u914d\u7f6e\u4fe1\u606f\n\n    \u6709\u65f6, dbus\u4f1a\u51fa\u73b0\u8fde\u63a5\u9519\u8bef, \u8fd9\u91cc\u4f1a\u8fdb\u884c\u91cd\u8bd5. \u4f46\u5982\u679c\u8d85\u8fc7\u6700\u5927\u5c1d\u8bd5\u6b21\u6570, \u5c31\n    \u4f1a\u5931\u6548, \u6b64\u65f6, profile['password'] \u662f\u4e00\u4e2a\u7a7a\u5b57\u7b26\u4e32, \u6240\u4ee5\u5728\u4e0b\u4e00\u6b65, \u5e94\u8be5\u53bb\n    \u68c0\u67e5\u4e00\u4e0bpassword\u662f\u5426\u6709\u6548, \u5982\u679c\u65e0\u6548, \u5e94\u8be5\u63d0\u9192\u7528\u6237.\n    '''\n    path = os.path.join(Config.CONF_DIR, profile_name)\n    if not os.path.exists(path):\n        return DEFAULT_PROFILE\n    with open(path) as fh:\n        profile = json.load(fh)\n\n    for key in DEFAULT_PROFILE:\n        if key not in profile:\n            profile[key] = DEFAULT_PROFILE[key]\n\n    global keyring_available\n    if keyring_available:\n        for i in range(RETRIES):\n            try:\n                profile['password'] = keyring.get_password(\n                        Config.DBUS_APP_NAME, profile['username'])\n                break\n            except (keyring.errors.InitError, dbus.exceptions.DBusException):\n                logger.error(traceback.format_exc())\n        else:\n            keyring_available = False\n    if not profile['password']:\n        profile['password'] = ''\n    return profile"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndumps a profile to a file.", "response": "def dump_profile(profile):\n    '''\u4fdd\u5b58\u5e10\u6237\u7684\u914d\u7f6e\u4fe1\u606f.\n\n    \u8fd9\u91cc\u4f1a\u68c0\u67e5\u7528\u6237\u662f\u5426\u613f\u610f\u4fdd\u5b58\u5bc6\u7801, \u5982\u679c\u9700\u8981\u4fdd\u5b58\u5bc6\u7801\u7684\u8bdd, \u5c31\u8c03\u7528keyring\u6765\u5b58\n    \u653e\u5bc6\u7801.\n    \u4f46\u5982\u679c\u5bc6\u7801\u4e3a\u7a7a, \u5c31\u4e0d\u518d\u5b58\u653e\u5b83\u4e86.\n    '''\n    profile = profile.copy()\n    path = os.path.join(Config.CONF_DIR, profile['username'])\n    if profile['remember-password'] and profile['password']:\n        for i in range(RETRIES):\n            try:\n                keyring.set_password(Config.DBUS_APP_NAME, profile['username'],\n                                     profile['password'])\n                break\n            except dbus.exceptions.DBusException:\n                logger.error(traceback.format_exc())\n    profile['password'] = ''\n    with open(path, 'w') as fh:\n        json.dump(profile, fh)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the mime type of the file.", "response": "def get_mime(self, path, isdir):\n        '''\u731c\u6d4b\u6587\u4ef6\u7c7b\u578b, \u6839\u636e\u5b83\u7684\u6587\u4ef6\u6269\u5c55\u540d'''\n        if isdir:\n            file_type = FOLDER\n        else:\n            file_type = mimetypes.guess_type(path)[0]\n            if not file_type:\n                file_type = UNKNOWN\n        return file_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, path, isdir, icon_size=ICON_SIZE):\n        '''\u53d6\u5f97\u4e00\u4e2a\u7f29\u7565\u56fe.\n        \n        path - \u6587\u4ef6\u7684\u8def\u5f84, \u53ef\u4ee5\u5305\u62ec\u7edd\u5bf9\u8def\u5f84, \u4e5f\u53ef\u4ee5\u662f\u6587\u4ef6\u540d.\n        isdir - \u662f\u5426\u4e3a\u4e00\u4e2a\u76ee\u5f55.\n        icon_size - \u56fe\u6807\u7684\u5927\u5c0f, \u5982\u679c\u662f\u663e\u793a\u5728IconView\u4e2d\u7684, 48\u5c31\u53ef\u4ee5;\n                    \u5982\u679c\u662f\u663e\u793a\u5728TreView\u7684\u8bdd, \u53ef\u4ee5\u7528Gtk.IconSize.MENU\n\n        @return \u4f1a\u8fd4\u56de\u4e00\u4e2aPixbuf\u4ee5\u8c61, \u548c\u8fd9\u4e2a\u6587\u4ef6\u7684\u7c7b\u578b(MIME)\n        '''\n        file_type = self.get_mime(path, isdir)\n        key = (file_type, icon_size)\n        if key in self._data:\n            return (self._data.get(key), file_type)\n\n        themed_icon = Gio.content_type_get_icon(file_type)\n        icon_names = themed_icon.to_string().split(' ')[2:]\n        icon_info = self.app.icon_theme.choose_icon(icon_names, icon_size,\n                Gtk.IconLookupFlags.GENERIC_FALLBACK)\n        if icon_info:\n            pixbuf = icon_info.load_icon()\n            self._data[key] = pixbuf\n            return (pixbuf, file_type)\n        else:\n            key = (UNKNOWN, icon_size)\n            pixbuf = self._data.get(key, None)\n            if not pixbuf:\n                pixbuf = self.get('/placeholder', isdir, icon_size)[0]\n            return (pixbuf, file_type)", "response": "Get a new image from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_task_db(self, task, force=True):\n        '''\u5411\u6570\u636e\u5e93\u4e2d\u5199\u5165\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u8bb0\u5f55, \u5e76\u8fd4\u56de\u5b83\u7684fid'''\n        sql = '''INSERT INTO upload (\n        name, source_path, path, size, curr_size, state, state_name,\n        human_size, percent, tooltip, threshold)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'''\n        req = self.cursor.execute(sql, task)\n        self.check_commit(force=force)\n        return req.lastrowid", "response": "Add a task to the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd slice db to database", "response": "def add_slice_db(self, fid, slice_end, md5):\n        '''\u5728\u6570\u636e\u5e93\u4e2d\u52a0\u5165\u4e0a\u4f20\u4efb\u52a1\u5206\u7247\u4fe1\u606f'''\n        sql = 'INSERT INTO slice VALUES(?, ?, ?)'\n        self.cursor.execute(sql, (fid, slice_end, md5))\n        self.check_commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_task_db(self, source_path):\n        '''\u4ece\u6570\u636e\u5e93\u4e2d\u67e5\u8be2source_path\u7684\u4fe1\u606f.\n        \n        \u5982\u679c\u5b58\u5728\u7684\u8bdd, \u5c31\u8fd4\u56de\u8fd9\u6761\u8bb0\u5f55;\n        \u5982\u679c\u6ca1\u6709\u7684\u8bdd, \u5c31\u8fd4\u56deNone\n        '''\n        sql = 'SELECT * FROM upload WHERE source_path=?'\n        req = self.cursor.execute(sql, [source_path, ])\n        if req:\n            return req.fetchone()\n        else:\n            None", "response": "get the task db from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_slice_db(self, fid):\n        '''\u4ece\u6570\u636e\u5e93\u4e2d\u53d6\u5f97fid\u7684\u6240\u6709\u5206\u7247.\n        \n        \u8fd4\u56de\u7684\u662f\u4e00\u4e2alist, \u91cc\u9762\u662f\u6309\u987a\u5e8f\u6392\u597d\u7684md5\u7684\u503c\n        '''\n        sql = 'SELECT md5 FROM slice WHERE fid=?'\n        req = self.cursor.execute(sql, [fid, ])\n        if req:\n            return [r[0] for r in req]\n        else:\n            return None", "response": "get the slice db"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_task_db(self, row, force=False):\n        '''\u66f4\u65b0\u6570\u636e\u5e93\u4e2d\u7684\u4efb\u52a1\u4fe1\u606f'''\n        sql = '''UPDATE upload SET \n        curr_size=?, state=?, state_name=?, human_size=?, percent=?\n        WHERE fid=?\n        '''\n        self.cursor.execute(sql, [\n            row[CURRSIZE_COL], row[STATE_COL], row[STATENAME_COL],\n            row[HUMANSIZE_COL], row[PERCENT_COL], row[FID_COL]\n        ])\n        self.check_commit(force=force)", "response": "Update the database with the given row."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_task_db(self, fid, force=False):\n        '''\u5c06\u4efb\u52a1\u4ece\u6570\u636e\u5e93\u4e2d\u5220\u9664'''\n        self.remove_slice_db(fid)\n        sql = 'DELETE FROM upload WHERE fid=?'\n        self.cursor.execute(sql, [fid, ])\n        self.check_commit(force=force)", "response": "Removes the task table from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_slice_db(self, fid):\n        '''\u5c06\u4e0a\u4f20\u4efb\u52a1\u7684\u5206\u7247\u4ece\u6570\u636e\u5e93\u4e2d\u5220\u9664'''\n        sql = 'DELETE FROM slice WHERE fid=?'\n        self.cursor.execute(sql, [fid, ])\n        self.check_commit()", "response": "Removes slice from database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_files(self, source_paths, dir_name=None):\n        '''\u6279\u91cf\u521b\u5efa\u4e0a\u4f20\u4efb\u52a1, \u4f1a\u626b\u63cf\u5b50\u76ee\u5f55\u5e76\u4f9d\u6b21\u4e0a\u4f20.\n\n        source_path - \u672c\u5730\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n        dir_name    - \u6587\u4ef6\u5728\u670d\u52a1\u5668\u4e0a\u7684\u7236\u76ee\u5f55, \u5982\u679c\u4e3aNone\u7684\u8bdd, \u4f1a\u5f39\u51fa\u4e00\u4e2a\n                      \u5bf9\u8bdd\u6846\u8ba9\u7528\u6237\u6765\u9009\u62e9\u4e00\u4e2a\u76ee\u5f55.\n        '''\n        def scan_folders(folder_path):\n            file_list = os.listdir(folder_path)\n            source_paths = [os.path.join(folder_path, f) for f in file_list]\n            self.upload_files(source_paths,\n                    os.path.join(dir_name, os.path.split(folder_path)[1]))\n\n        self.check_first()\n        if not dir_name:\n            folder_dialog = FolderBrowserDialog(self, self.app)\n            response = folder_dialog.run()\n            if response != Gtk.ResponseType.OK:\n                folder_dialog.destroy()\n                return\n            dir_name = folder_dialog.get_path()\n            folder_dialog.destroy()\n        invalid_paths = []\n        for source_path in source_paths:\n            if util.validate_pathname(source_path) != ValidatePathState.OK:\n                invalid_paths.append(source_path)\n                continue\n            if (os.path.split(source_path)[1].startswith('.') and\n                    not self.app.profile['upload-hidden-files']):\n                continue\n            if os.path.isfile(source_path):\n                self.upload_file(source_path, dir_name)\n            elif os.path.isdir(source_path):\n                scan_folders(source_path)\n\n        self.app.blink_page(self)\n        self.scan_tasks()\n\n        if not invalid_paths:\n            return\n        dialog = Gtk.Dialog(_('Invalid Filepath'), self.app.window,\n                            Gtk.DialogFlags.MODAL,\n                            (Gtk.STOCK_CLOSE, Gtk.ResponseType.OK))\n        dialog.set_default_size(640, 480)\n        dialog.set_border_width(10)\n        box = dialog.get_content_area()\n\n        scrolled_window = Gtk.ScrolledWindow()\n        box.pack_start(scrolled_window, True, True, 0)\n        text_buffer = Gtk.TextBuffer()\n        textview = Gtk.TextView.new_with_buffer(text_buffer)\n        scrolled_window.add(textview)\n        for invalid_path in invalid_paths:\n            text_buffer.insert_at_cursor(invalid_path)\n            text_buffer.insert_at_cursor('\\n')\n\n        infobar = Gtk.InfoBar()\n        infobar.set_message_type(Gtk.MessageType.ERROR)\n        box.pack_end(infobar, False, False, 0)\n        info_label= Gtk.Label()\n        infobar.get_content_area().pack_start(info_label, False, False, 0)\n        info_label.set_label(''.join([\n            '* ', ValidatePathStateText[1], '\\n',\n            '* ', ValidatePathStateText[2], '\\n',\n            '* ', ValidatePathStateText[3], '\\n',\n        ]))\n\n        box.show_all()\n        dialog.run()\n        dialog.destroy()", "response": "Upload files to a folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupload a file to the liststore", "response": "def upload_file(self, source_path, dir_name):\n        '''\u4e0a\u4f20\u4e00\u4e2a\u6587\u4ef6'''\n        row = self.get_task_db(source_path)\n        source_dir, filename = os.path.split(source_path)\n        \n        path = os.path.join(dir_name, filename)\n        size = os.path.getsize(source_path)\n        total_size = util.get_human_size(size)[0]\n        tooltip = gutil.escape(_('From {0}\\nTo {1}').format(source_path, path))\n        if size < 2 ** 27:           # 128M \n            threshold = 2 ** 17      # 128K\n        elif size < 2 ** 29:         # 512M\n            threshold =  2 ** 19     # 512K\n        elif size < 10 * (2 ** 30):  # 10G\n            threshold = math.ceil(size / 1000)\n        else:\n            self.app.toast(_('{0} is too large to upload (>10G).').format(path))\n            return\n        task = [\n            filename,\n            source_path,\n            path,\n            size,\n            0,\n            State.WAITING,\n            StateNames[State.WAITING],\n            '0 / {0}'.format(total_size),\n            0,\n            tooltip,\n            threshold,\n        ]\n        row_id = self.add_task_db(task, force=False)\n        task.insert(0, row_id)\n        self.liststore.append(task)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_task(self, row, scan=True):\n        '''\u5220\u9664\u4e0b\u8f7d\u4efb\u52a1'''\n        if row[STATE_COL] == State.UPLOADING:\n            self.remove_worker(row[FID_COL], stop=True)\n        self.remove_task_db(row[FID_COL])\n        tree_iter = row.iter\n        if tree_iter:\n            self.liststore.remove(tree_iter)\n        if scan:\n            self.scan_tasks()", "response": "Remove a task from the liststore"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef operate_selected_rows(self, operator):\n        '''\u5bf9\u9009\u4e2d\u7684\u6761\u76ee\u8fdb\u884c\u64cd\u4f5c.\n\n        operator  - \u5904\u7406\u51fd\u6570\n        '''\n        model, tree_paths = self.selection.get_selected_rows()\n        if not tree_paths:\n            return\n        fids = []\n        for tree_path in tree_paths:\n            fids.append(model[tree_path][FID_COL])\n        for fid in fids:\n            row = self.get_row_by_fid(fid)\n            if row:\n                operator(row)", "response": "operate on selected rows"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef urlopen_without_redirect(url, headers={}, data=None, retries=RETRIES):\n    '''\u8bf7\u6c42\u4e00\u4e2aURL, \u5e76\u8fd4\u56de\u4e00\u4e2aResponse\u5bf9\u8c61. \u4e0d\u5904\u7406\u91cd\u5b9a\u5411.\n\n    \u4f7f\u7528\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u8fd4\u56deURL\u91cd\u5b9a\u5411(Error 301/302)\u540e\u7684\u5730\u5740, \u4e5f\u53ef\u4ee5\u91cd\u5230URL\u4e2d\u8bf7\n    \u6c42\u7684\u6587\u4ef6\u7684\u5927\u5c0f, \u6216\u8005Header\u4e2d\u7684\u5176\u5b83\u8ba4\u8bc1\u4fe1\u606f.\n    '''\n    headers_merged = default_headers.copy()\n    for key in headers.keys():\n        headers_merged[key] = headers[key]\n\n    parse_result = urllib.parse.urlparse(url)\n    for i in range(retries):\n        try:\n            conn = http.client.HTTPConnection(parse_result.netloc)\n            if data:\n                conn.request('POST', url, body=data, headers=headers_merged)\n            else:\n                conn.request('GET', url, body=data, headers=headers_merged)\n            return conn.getresponse()\n        except OSError:\n            logger.error(traceback.format_exc())\n        except:\n            logger.error(traceback.format_exc())\n            #return None\n    return None", "response": "URL - > HttpResponse"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget BAIDUID - BAIDUID.", "response": "def get_BAIDUID():\n    '''\u83b7\u53d6\u4e00\u4e2acookie - BAIDUID.\n\n    \u8fd9\u91cc, \u6211\u4eec\u8bbf\u95ee\u767e\u5ea6\u9996\u9875, \u8fd4\u56de\u7684response header\u91cc\u9762\u6709\u6211\u4eec\u9700\u8981\u7684cookie\n    '''\n    url = ''.join([\n        const.PASSPORT_URL,\n        '?getapi&tpl=mn&apiver=v3',\n        '&tt=', util.timestamp(),\n        '&class=login&logintype=basicLogin',\n    ])\n    req = net.urlopen(url, headers={'Referer': ''})\n    if req:\n        return req.headers.get_all('Set-Cookie')\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a token from a cookie.", "response": "def get_token(cookie):\n    '''\u83b7\u53d6\u4e00\u4e2a\u9875\u9762\u8bbf\u95ee\u7684token, \u8fd9\u91cc\u9700\u8981\u4e4b\u524d\u5f97\u5230\u7684BAIDUID \u8fd9\u4e2acookie\u503c\n\n    \u8fd9\u4e2atoken\u7684\u6709\u6548\u671f\u8fd8\u4e0d\u786e\u5b9a.\n    \u8fd4\u56de\u7684\u6570\u636e\u5982\u4e0b:\n    {\"errInfo\":{\"no\": \"0\"},\n     \"data\": {\n         \"rememberedUserName\" : \"\",\n         \"codeString\" : \"\",\n         \"token\" : \"xxxxx\",\n         \"cookie\" : \"1\",\n         \"usernametype\":\"2\",\n         \"spLogin\" : \"rate\",\n         \"disable\":\"\",\n         \"loginrecord\":{ 'email':[ ], 'phone':[]}\n    }}\n    '''\n    url = ''.join([\n        const.PASSPORT_URL,\n        '?getapi&tpl=pp&apiver=v3',\n        '&tt=', util.timestamp(),\n        '&class=login&logintype=basicLogin',\n    ])\n    headers={\n        'Cookie': cookie.header_output(),\n        'Accept': const.ACCEPT_HTML,\n        'Cache-control': 'max-age=0',\n    }\n    req = net.urlopen(url, headers=headers)\n    if req:\n        cookie = req.headers.get_all('Set-Cookie')\n        content_obj = util.json_loads_single(req.data.decode())\n        if content_obj:\n            return cookie, content_obj['data']['token']\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting UBI from cookie.", "response": "def get_UBI(cookie, tokens):\n    '''\u68c0\u67e5\u767b\u5f55\u5386\u53f2, \u53ef\u4ee5\u83b7\u5f97\u4e00\u4e2aCookie - UBI.\n    \u8fd4\u56de\u7684\u4fe1\u606f\u7c7b\u4f3c\u4e8e: \n    {\"errInfo\":{ \"no\": \"0\" }, \"data\": {'displayname':['xxx@163.com']}}\n    '''\n    url = ''.join([\n        const.PASSPORT_URL,\n        '?loginhistory',\n        '&token=', tokens['token'],\n        '&tpl=pp&apiver=v3',\n        '&tt=', util.timestamp(),\n    ])\n    headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.REFERER,\n    }\n    req = net.urlopen(url, headers=headers)\n    if req:\n        return req.headers.get_all('Set-Cookie')\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_signin_vcode(cookie, codeString):\n    '''\u83b7\u53d6\u767b\u5f55\u65f6\u7684\u9a8c\u8bc1\u7801\u56fe\u7247.\n\n    codeString - \u8c03\u7528check_login()\u65f6\u8fd4\u56de\u7684codeString.\n    '''\n    url = ''.join([\n        const.PASSPORT_BASE,\n        'cgi-bin/genimage?',\n        codeString,\n    ])\n    headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.REFERER,\n    }\n    req = net.urlopen(url, headers=headers)\n    if req:\n        return req.data\n    else:\n        return None", "response": "Get the vcode from the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrefresh vcode from cookie", "response": "def refresh_signin_vcode(cookie, tokens, vcodetype):\n    '''\u5237\u65b0\u9a8c\u8bc1\u7801.\n\n    vcodetype - \u5728\u8c03\u7528check_login()\u65f6\u8fd4\u56de\u7684vcodetype.\n    '''\n    url = ''.join([\n        const.PASSPORT_BASE,\n        'v2/?reggetcodestr',\n        '&token=', tokens['token'],\n        '&tpl=pp&apiver=v3',\n        '&tt=', util.timestamp(),\n        '&fr=ligin',\n        '&vcodetype=', encoder.encode_uri(vcodetype),\n    ])\n    headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.REFERER,\n    }\n    logger.debug('refresh vcode url: %s' % url)\n    req = net.urlopen(url, headers=headers)\n    if req:\n        try:\n            data = req.data.decode('gbk')\n            logger.debug('refresh vcode: %s' % data)\n            return json.loads(data)\n        except ValueError:\n            logger.error(traceback.format_exc())\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_public_key(cookie, tokens):\n    '''\u83b7\u53d6RSA\u516c\u94a5, \u8fd9\u4e2a\u7528\u4e8e\u52a0\u5bc6\u7528\u6237\u7684\u5bc6\u7801\n    \n    \u8fd4\u56de\u7684\u6570\u636e\u5982\u4e0b:\n    {\"errno\":'0',\"msg\":'',\"pubkey\":'-----BEGIN PUBLIC KEY-----\\nMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDk\\/ufXg3IBW8+h5i8L8NoXUzcN\\nMeKrh4zEupGBkyrURIPUXKDFLWjrv4n2j3RpMZ8GQn\\/ETcfoIHGBoCUKJWcfcvmi\\nG+OkYeqT6zyJasF0OlKesKfz0fGogMtdCQ6Kqq7X2vrzBPL+4SNU2wgU31g\\/tVZl\\n3zy5qAsBFkC70vs5FQIDAQAB\\n-----END PUBLIC KEY-----\\n',\"key\":'lwCISJnvs7HRNCTxpX7vi25bV9YslF2J'}\n    '''\n    url = ''.join([\n        const.PASSPORT_BASE, 'v2/getpublickey',\n        '?token=', tokens['token'],\n        '&tpl=pp&apiver=v3&tt=', util.timestamp(),\n    ])\n    headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.REFERER,\n    }\n    req = net.urlopen(url, headers=headers)\n    if req:\n        data = req.data\n        return util.json_loads_single(req.data.decode())\n    return None", "response": "Get public key from cookie."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets bdstoken from cookie", "response": "def get_bdstoken(cookie):\n    '''\u4ece/disk/home\u9875\u9762\u83b7\u53d6bdstoken\u7b49token\u4fe1\u606f\n\n    \u8fd9\u4e9btoken\u5bf9\u4e8e\u4e4b\u540e\u7684\u8bf7\u6c42\u975e\u5e38\u91cd\u8981.\n    '''\n    url = const.PAN_REFERER\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()})\n    if req:\n        return parse_bdstoken(req.data.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a temporary file path.", "response": "def get_tmp_filepath(dir_name, save_name):\n    '''\u8fd4\u56de\u6700\u7ec8\u8def\u5f84\u540d\u53ca\u4e34\u65f6\u8def\u5f84\u540d'''\n    filepath = os.path.join(dir_name, save_name)\n    return filepath, filepath + '.part', filepath + '.bcloud-stat'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a request from the URL.", "response": "def get_req(self, start_size, end_size):\n        '''\u6253\u5f00socket'''\n        logger.debug('DownloadBatch.get_req: %s, %s' % (start_size, end_size))\n        opener = request.build_opener()\n        content_range = 'bytes={0}-{1}'.format(start_size, end_size)\n        opener.addheaders = [\n            ('Range', content_range),\n            ('User-Agent', const.USER_AGENT),\n            ('Referer', const.PAN_REFERER),\n        ]\n        for i in range(RETRIES):\n            try:\n                return opener.open(self.url, timeout=self.timeout)\n            except OSError:\n                logger.error(traceback.format_exc())\n                self.queue.put((self.id_, BATCH_ERROR), block=False)\n                return None\n            except:\n                self.queue.put((self.id_, BATCH_ERROR), block=False)\n                return None\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_uk(cookie, tokens):\n    '''\u83b7\u53d6\u7528\u6237\u7684uk'''\n    url = 'http://yun.baidu.com'\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()})\n    if req:\n        content = req.data.decode()\n        match = re.findall('/share/home\\?uk=(\\d+)\" target=', content)\n        if len(match) == 1:\n            return match[0]\n        else:\n            logger.warn('pcs.get_user_uk(), failed to parse uk, %s' % url)\n    return None", "response": "get UK from Yun. baidu. com"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting user info from Ploud.", "response": "def get_user_info(tokens, uk):\n    '''\u83b7\u53d6\u7528\u6237\u7684\u90e8\u5206\u4fe1\u606f.\n\n    \u6bd4\u5982\u5934\u50cf, \u7528\u6237\u540d, \u81ea\u6211\u4ecb\u7ecd, \u7c89\u4e1d\u6570\u7b49.\n    \u8fd9\u4e2a\u63a5\u53e3\u53ef\u7528\u4e8e\u67e5\u8be2\u4efb\u4f55\u7528\u6237\u7684\u4fe1\u606f, \u53ea\u8981\u77e5\u9053\u4ed6/\u5979\u7684uk.\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'pcloud/user/getinfo?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n        '&query_uk=', uk,\n        '&t=', util.timestamp(),\n    ])\n    req = net.urlopen(url)\n    if req:\n        info = json.loads(req.data.decode())\n        if info and info['errno'] == 0:\n            return info['user_info']\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget list of user s share", "response": "def list_share(cookie, tokens, uk, page=1):\n    '''\u83b7\u53d6\u7528\u6237\u5df2\u7ecf\u5171\u4eab\u7684\u6240\u6709\u6587\u4ef6\u7684\u4fe1\u606f\n\n    uk   - user key\n    page - \u9875\u6570, \u9ed8\u8ba4\u4e3a\u7b2c\u4e00\u9875.\n    num  - \u4e00\u6b21\u6027\u83b7\u53d6\u7684\u5171\u4eab\u6587\u4ef6\u7684\u6570\u91cf, \u9ed8\u8ba4\u4e3a100\u4e2a.\n    '''\n    num = 100\n    start = 100 * (page - 1)\n    url = ''.join([\n        const.PAN_URL,\n        'pcloud/feed/getsharelist?',\n        '&t=', util.timestamp(),\n        '&categor=0&auth_type=1&request_location=share_home',\n        '&start=', str(start),\n        '&limit=', str(num),\n        '&query_uk=', str(uk),\n        '&channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.SHARE_REFERER,\n    })\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_share_files(cookie, tokens, uk, shareid, dirname, page=1):\n    '''\u5217\u4e3e\u51fa\u7528\u6237\u5171\u4eab\u7684\u67d0\u4e00\u4e2a\u76ee\u5f55\u4e2d\u7684\u6587\u4ef6\u4fe1\u606f\n\n    \u8fd9\u4e2a\u5bf9\u6240\u6709\u7528\u6237\u90fd\u6709\u6548\n    uk       - user key\n    shareid - \u5171\u4eab\u6587\u4ef6\u7684ID\u503c\n    dirname  - \u5171\u4eab\u76ee\u5f55, \u5982\u679cdirname\u4e3aNone, \u8bf4\u660e\u8fd9\u6709\u53ef\u80fd\u662f\u4e00\u4e2a\u5355\u72ec\u5171\u4eab\u7684\u6587\u4ef6,\n               \u8fd9\u91cc, \u9700\u8981\u8c03\u7528list_share_single_file()\n    '''\n    if not dirname:\n        return list_share_single_file(cookie, tokens, uk, shareid)\n    url = ''.join([\n        const.PAN_URL,\n        'share/list?channel=chunlei&clienttype=0&web=1&num=50',\n        '&t=', util.timestamp(),\n        '&page=', str(page),\n        '&dir=', encoder.encode_uri_component(dirname),\n        '&t=', util.latency(),\n        '&shareid=', shareid,\n        '&order=time&desc=1',\n        '&uk=', uk,\n        '&_=', util.timestamp(),\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.SHARE_REFERER,\n    })\n    if req:\n        content = req.data\n        info = json.loads(content.decode())\n        if info['errno'] == 0:\n            return info['list']\n    return list_share_single_file(cookie, tokens, uk, shareid)", "response": "List all files in a user s share."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_share_single_file(cookie, tokens, uk, shareid):\n    '''\u83b7\u53d6\u5355\u72ec\u5171\u4eab\u51fa\u6765\u7684\u6587\u4ef6.\n\n    \u76ee\u524d\u652f\u6301\u7684\u94fe\u63a5\u683c\u5f0f\u6709:\n      * http://pan.baidu.com/wap/link?uk=202032639&shareid=420754&third=0\n      * http://pan.baidu.com/share/link?uk=202032639&shareid=420754\n    '''\n    def parse_share_page(content):\n        tree = html.fromstring(content)\n        script_sel = CSS('script')\n        scripts = script_sel(tree)\n        for script in scripts:\n            if (script.text and (script.text.find('viewsingle_param') > -1 or\n                script.text.find('mpan.viewlist_param') > -1)):\n                break\n        else:\n            logger.warn('pcs.parse_share_page: failed to get filelist, %s', url)\n            return None\n        start = script.text.find('viewsingle_param.list=JSON.parse(')\n        end = script.text.find(');mpan.viewsingle_param.username')\n        if start == -1 or end == -1:\n            start = script.text.find('listArr:JSON.parse(')\n            end = script.text.find('),rootPath:')\n            if start == -1 or end == -1:\n                return None\n            else:\n                json_str = script.text[start+19:end]\n        else:\n            json_str = script.text[start+33:end]\n        try:\n            return json.loads(json.loads(json_str))\n        except ValueError:\n            logger.warn(traceback.format_exc())\n            return None\n\n    url = ''.join([\n        const.PAN_URL, 'wap/link',\n        '?shareid=', shareid,\n        '&uk=', uk,\n        '&third=0',\n    ])\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Referer': const.SHARE_REFERER,\n    })\n    if req:\n        return parse_share_page(req.data.decode())\n    else:\n        return None", "response": "Get a list of files in a single share."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_share(cookie, tokens, fid_list):\n    '''\u5efa\u7acb\u65b0\u7684\u5206\u4eab.\n\n    fid_list - \u662f\u4e00\u4e2alist, \u91cc\u9762\u7684\u6bcf\u4e00\u6761\u90fd\u662f\u4e00\u4e2a\u6587\u4ef6\u7684fs_id\n    \u4e00\u6b21\u53ef\u4ee5\u5206\u4eab\u540c\u4e00\u4e2a\u76ee\u5f55\u4e0b\u7684\u591a\u4e2a\u6587\u4ef6/\u76ee\u5f55, \u5b83\u4eec\u4f1a\u4f1a\u6253\u5305\u4e3a\u4e00\u4e2a\u5206\u4eab\u94fe\u63a5,\n    \u8fd9\u4e2a\u5206\u4eab\u94fe\u63a5\u8fd8\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684shareid. \u6211\u4eec\u53ef\u4ee5\u7528uk\u4e0eshareid\u6765\u5728\u767e\u5ea6\u7f51\u76d8\u91cc\n    \u9762\u5b9a\u4f4d\u5230\u8fd9\u4e2a\u5206\u4eab\u5185\u5bb9.\n    @return - \u4f1a\u8fd4\u56de\u5206\u4eab\u94fe\u63a5\u548cshareid.\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'share/set?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    data = encoder.encode_uri(\n            'fid_list={0}&schannel=0&channel_list=[]'.format(fid_list))\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Content-type': const.CONTENT_FORM_UTF8,\n        }, data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Enable a new share."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_share(cookie, tokens, shareid_list):\n    '''\u53d6\u6d88\u5206\u4eab.\n\n    shareid_list \u662f\u4e00\u4e2alist, \u6bcf\u4e00\u9879\u90fd\u662f\u4e00\u4e2ashareid\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'share/cancel?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    data = 'shareid_list=' + encoder.encode_uri(json.dumps(shareid_list))\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Content-type': const.CONTENT_FORM_UTF8,\n        }, data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Disable a list of shareid_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the uk and shareid from a cookie.", "response": "def get_share_uk_and_shareid(cookie, url):\n    '''\u4ece\u5171\u4eab\u94fe\u63a5\u4e2d\u63d0\u793auk\u548cshareid.\n\n    \u5982\u679c\u5171\u4eab\u6587\u4ef6\u9700\u8981\u8f93\u5165\u5bc6\u7801, \u5c31\u4f1a\u5c06need_pwd\u8bbe\u4e3aTrue\n    \u5982\u679c\u6210\u529f, \u8fd4\u56de(need_pwd, uk, shareid)\n    \u5982\u679c\u5931\u8d25, \u5c31\u8fd4\u56deNone\n\n    \u76ee\u524d\u652f\u6301\u7684\u94fe\u63a5\u683c\u5f0f\u6709:\n      * http://pan.baidu.com/wap/link?uk=202032639&shareid=420754&third=0\n      * http://pan.baidu.com/share/link?uk=202032639&shareid=420754\n      * http://pan.baidu.com/s/1i3iQY48\n    '''\n    def parse_share_uk(content):\n        '''\u4ee3\u7801\u7247\u6bb5\u5982\u4e0b:\n\n        yunData.SHARE_ID = \"677200861\";\n        yunData.SHARE_UK = \"1295729848\";\n        '''\n        uk_reg = re.compile('yunData.SHARE_UK\\s*=\\s*\"(\\d+)\"')\n        shareid_reg = re.compile('yunData.SHARE_ID\\s*=\\s*\"(\\d+)\"')\n        uk_match = uk_reg.search(content)\n        shareid_match = shareid_reg.search(content)\n        if uk_match and shareid_match:\n            return False, uk_match.group(1), shareid_match.group(1)\n        else:\n            return None\n\n    def parse_uk_from_url(url):\n        uk_reg = re.compile('uk=(\\d+)')\n        uk_match = uk_reg.search(url)\n        shareid_reg = re.compile('shareid=(\\d+)')\n        shareid_match = shareid_reg.search(url)\n        if not uk_match or not shareid_match:\n            return '', ''\n        uk = uk_match.group(1)\n        shareid = shareid_match.group(1)\n        return uk, shareid\n\n    # \u8bc6\u522b\u52a0\u5bc6\u94fe\u63a5\n    req = net.urlopen_without_redirect(url, headers={\n        'Cookie': cookie.header_output(),\n    })\n    if req and req.headers.get('Location'):\n        init_url = req.headers.get('Location')\n        if init_url.find('share/init') > -1:\n            uk, shareid = parse_uk_from_url(init_url)\n            return True, uk, shareid\n\n    # \u5904\u7406\u77ed\u94fe\u63a5\n    if url.startswith('http://pan.baidu.com/s/'):\n        req = net.urlopen(url, headers={\n            'Cookie': cookie.header_output(),\n        })\n        if req:\n            return parse_share_uk(req.data.decode())\n    # \u5904\u7406\u6b63\u5e38\u94fe\u63a5\n    uk, shareid = parse_uk_from_url(url)\n    return False, uk, shareid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_share_dirname(url):\n    '''\u4eceurl\u4e2d\u63d0\u53d6\u51fa\u5f53\u524d\u7684\u76ee\u5f55'''\n    dirname_match = re.search('(dir|path)=([^&]+)',\n                              encoder.decode_uri_component(url))\n    if dirname_match:\n        return dirname_match.group(2)\n    else:\n        return None", "response": "Get the share directory name from url."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a URL to a share with a given dirname.", "response": "def get_share_url_with_dirname(uk, shareid, dirname):\n    '''\u5f97\u5230\u5171\u4eab\u76ee\u5f55\u7684\u94fe\u63a5'''\n    return ''.join([\n           const.PAN_URL, 'wap/link',\n           '?shareid=', shareid,\n           '&uk=', uk,\n           '&dir=', encoder.encode_uri_component(dirname),\n           '&third=0',\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef share_transfer(cookie, tokens, shareid, uk, filelist, dest, upload_mode):\n    '''\n    \u5c06\u5176\u4ed6\u7528\u6237\u7684\u6587\u4ef6\u4fdd\u5b58\u5230\u81ea\u5df1\u7f51\u76d8\u91cc.\n\n    uk - \u5176\u4ed6\u7528\u6237\u7684uk\n    filelist - \u8981\u8f6c\u79fb\u6587\u4ef6\u7684\u5217\u8868, \u662f\u7edd\u5bf9\u8def\u5f84\n    '''\n    ondup = const.UPLOAD_ONDUP[upload_mode]\n    url = ''.join([\n        const.PAN_URL,\n        'share/transfer?app_id=250528&channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n        '&from=', uk,\n        '&shareid=', shareid,\n        '&ondup=', ondup,\n        '&async=1',\n    ])\n    data = ''.join([\n        'path=', encoder.encode_uri_component(dest),\n        '&filelist=', encoder.encode_uri_component(json.dumps(filelist))\n    ])\n\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Content-Type': const.CONTENT_FORM_UTF8\n    }, data=data.encode())\n    if req:\n        content = req.data.decode()\n        return json.loads(content)\n    else:\n        return None", "response": "\u5c06\u5176\u4ed6\u7528\u6237\u7684\u6587\u4ef6\u4fdd\u5b58\u5230\u81ea\u5df1\u7f51\u76d8\u91cc.\n\n    uk - \u5176\u4ed6\u7528\u6237\u7684uk\n    filelist - \u8981\u8f6c\u79fb\u6587\u4ef6\u7684\u5217\u8868, \u662f\u7edd\u5bf9\u8def\u5f84"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrestore the trash of a file.", "response": "def restore_trash(cookie, tokens, fidlist):\n    '''\u4ece\u56de\u6536\u7ad9\u4e2d\u8fd8\u539f\u6587\u4ef6/\u76ee\u5f55.\n\n    fildlist - \u8981\u8fd8\u539f\u7684\u6587\u4ef6/\u76ee\u5f55\u5217\u8868, fs_id.\n    '''\n    url = ''.join([\n        const.PAN_API_URL,\n        'recycle/restore?channel=chunlei&clienttype=0&web=1',\n        '&t=', util.timestamp(),\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    data = 'fidlist=' + encoder.encode_uri_component(json.dumps(fidlist))\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        'Content-type': const.CONTENT_FORM_UTF8,\n        }, data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the trash of the current user.", "response": "def clear_trash(cookie, tokens):\n    '''\u6e05\u7a7a\u56de\u6536\u7ad9, \u5c06\u91cc\u9762\u7684\u6240\u6709\u6587\u4ef6\u90fd\u5220\u9664.'''\n    url = ''.join([\n        const.PAN_API_URL,\n        'recycle/clear?channel=chunlei&clienttype=0&web=1',\n        '&t=', util.timestamp(),\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    # \u4f7f\u7528POST\u65b9\u5f0f\u53d1\u9001\u547d\u4ee4, \u4f46data\u4e3a\u7a7a.\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.header_output(),\n        }, data=''.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_dir_all(cookie, tokens, path):\n    '''\u5f97\u5230\u4e00\u4e2a\u76ee\u5f55\u4e2d\u6240\u6709\u6587\u4ef6\u7684\u4fe1\u606f, \u5e76\u8fd4\u56de\u5b83\u7684\u6587\u4ef6\u5217\u8868'''\n    pcs_files = []\n    page = 1\n    while True:\n        content = list_dir(cookie, tokens, path, page)\n        if not content:\n            return (path, None)\n        if not content['list']:\n            return (path, pcs_files)\n        pcs_files.extend(content['list'])\n        page = page + 1", "response": "List all files in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the directory of a file.", "response": "def list_dir(cookie, tokens, path, page=1, num=100):\n    '''\u5f97\u5230\u4e00\u4e2a\u76ee\u5f55\u4e2d\u7684\u6240\u6709\u6587\u4ef6\u7684\u4fe1\u606f(\u6700\u591a100\u6761\u8bb0\u5f55).'''\n    timestamp = util.timestamp()\n    url = ''.join([\n        const.PAN_API_URL,\n        'list?channel=chunlei&clienttype=0&web=1',\n        '&num=', str(num),\n        '&t=', timestamp,\n        '&page=', str(page),\n        '&dir=', encoder.encode_uri_component(path),\n        '&t=', util.latency(),\n        '&order=time&desc=1',\n        '&_=', timestamp,\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    req = net.urlopen(url, headers={\n        'Content-type': const.CONTENT_FORM_UTF8,\n        'Cookie': cookie.sub_output('BAIDUID', 'BDUSS', 'PANWEB', 'cflag'),\n    })\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_category(cookie, tokens, category, page=1):\n    '''\u83b7\u53d6\u4e00\u4e2a\u5206\u7c7b\u4e2d\u7684\u6240\u6709\u6587\u4ef6\u4fe1\u606f, \u6bd4\u5982\u97f3\u4e50/\u56fe\u7247\n\n    \u76ee\u524d\u7684\u6709\u5206\u7c7b\u6709:\n      \u89c6\u9891 - 1\n      \u97f3\u4e50 - 2\n      \u56fe\u7247 - 3\n      \u6587\u6863 - 4\n      \u5e94\u7528 - 5\n      \u5176\u5b83 - 6\n      BT\u79cd\u5b50 - 7\n    '''\n    timestamp = util.timestamp()\n    url = ''.join([\n        const.PAN_API_URL,\n        'categorylist?channel=chunlei&clienttype=0&web=1',\n        '&category=', str(category),\n        '&pri=-1&num=100',\n        '&t=', timestamp,\n        '&page=', str(page),\n        '&order=time&desc=1',\n        '&_=', timestamp,\n        '&bdstoken=', cookie.get('STOKEN').value,\n    ])\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()})\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Get a category from a cookie."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stream_download(cookie, tokens, path):\n    '''\u4e0b\u8f7d\u6d41\u5a92\u4f53\u6587\u4ef6.\n\n    path - \u6d41\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84.\n    '''\n    url = ''.join([\n        const.PCS_URL_D,\n        'file?method=download',\n        '&path=', encoder.encode_uri_component(path),\n        '&app_id=250528',\n    ])\n    req = net.urlopen_without_redirect(url, headers=\n            {'Cookie': cookie.header_output()})\n    if req:\n        return req\n    else:\n        return None", "response": "Stream the file to the local filesystem."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a streaming playlist from a cookie.", "response": "def get_streaming_playlist(cookie, path, video_type='M3U8_AUTO_480'):\n    '''\u83b7\u53d6\u6d41\u5a92\u4f53(\u901a\u5e38\u662f\u89c6\u9891)\u7684\u64ad\u653e\u5217\u8868.\n\n    \u9ed8\u8ba4\u5f97\u5230\u7684\u662fm3u8\u683c\u5f0f\u7684\u64ad\u653e\u5217\u8868, \u56e0\u4e3a\u5b83\u6700\u901a\u7528.\n    path       - \u89c6\u9891\u7684\u7edd\u5bf9\u8def\u5f84\n    video_type - \u89c6\u9891\u683c\u5f0f, \u53ef\u4ee5\u6839\u636e\u7f51\u901f\u53ca\u7247\u6e90, \u9009\u62e9\u4e0d\u540c\u7684\u683c\u5f0f.\n    '''\n    url = ''.join([\n        const.PCS_URL,\n        'file?method=streaming',\n        '&path=', encoder.encode_uri_component(path),\n        '&type=', video_type,\n        '&app_id=250528',\n    ])\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()})\n    if req:\n        return req.data\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading a file to the BDUSS.", "response": "def upload(cookie, source_path, path, upload_mode):\n    '''\u4e0a\u4f20\u4e00\u4e2a\u6587\u4ef6.\n\n    \u8fd9\u4e2a\u662f\u4f7f\u7528\u7684\u7f51\u9875\u4e2d\u7684\u4e0a\u4f20\u63a5\u53e3.\n    upload_mode - const.UploadMode, \u5982\u679c\u6587\u4ef6\u5df2\u5728\u670d\u52a1\u5668\u4e0a\u5b58\u5728:\n      * overwrite, \u76f4\u63a5\u5c06\u5176\u91cd\u5199.\n      * newcopy, \u4fdd\u7559\u539f\u5148\u7684\u6587\u4ef6, \u5e76\u5728\u65b0\u4e0a\u4f20\u7684\u6587\u4ef6\u540d\u5c3e\u90e8\u52a0\u4e0a\u5f53\u524d\u65f6\u95f4\u6233.\n    '''\n    ondup = const.UPLOAD_ONDUP[upload_mode]\n    dir_name, file_name = os.path.split(path)\n    url = ''.join([\n        const.PCS_URL_C,\n        'file?method=upload&app_id=250528',\n        '&ondup=', ondup,\n        '&dir=', encoder.encode_uri_component(dir_name),\n        '&filename=', encoder.encode_uri_component(file_name),\n        '&', cookie.sub_output('BDUSS'),\n    ])\n    with open(source_path, 'rb') as fh:\n        data = fh.read()\n    fields = []\n    files = [('file', file_name, data)]\n    headers = {'Accept': const.ACCEPT_HTML, 'Origin': const.PAN_URL}\n    req = net.post_multipart(url, headers, fields, files)\n    if req:\n        return json.loads(req.data.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading a file to rapid.", "response": "def rapid_upload(cookie, tokens, source_path, path, upload_mode):\n    '''\u5feb\u901f\u4e0a\u4f20'''\n    ondup = const.UPLOAD_ONDUP[upload_mode]\n    content_length = os.path.getsize(source_path)\n    assert content_length > RAPIDUPLOAD_THRESHOLD, 'file size is not satisfied!'\n    dir_name, file_name = os.path.split(path)\n    content_md5 = hasher.md5(source_path)\n    slice_md5 = hasher.md5(source_path, 0, RAPIDUPLOAD_THRESHOLD)\n    url = ''.join([\n        const.PCS_URL_C,\n        'file?method=rapidupload&app_id=250528',\n        '&ondup=', ondup,\n        '&dir=', encoder.encode_uri_component(dir_name),\n        '&filename=', encoder.encode_uri_component(file_name),\n        '&content-length=', str(content_length),\n        '&content-md5=', content_md5,\n        '&slice-md5=', slice_md5,\n        '&path=', encoder.encode_uri_component(path),\n        '&', cookie.sub_output('BDUSS'),\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()})\n    if req:\n        return json.loads(req.data.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nslicing a file into a new node.", "response": "def slice_upload(cookie, data):\n    '''\u5206\u7247\u4e0a\u4f20\u4e00\u4e2a\u5927\u6587\u4ef6\n    \n    \u5206\u7247\u4e0a\u4f20\u5b8c\u6210\u540e, \u4f1a\u8fd4\u56de\u8fd9\u4e2a\u5206\u7247\u7684MD5, \u7528\u4e8e\u6700\u7ec8\u7684\u6587\u4ef6\u5408\u5e76.\n    \u5982\u679c\u4e0a\u4f20\u5931\u8d25, \u9700\u8981\u91cd\u65b0\u4e0a\u4f20.\n    \u4e0d\u9700\u8981\u6307\u5b9a\u4e0a\u4f20\u8def\u5f84, \u4e0a\u4f20\u540e\u7684\u6570\u636e\u4f1a\u88ab\u5b58\u50a8\u5728\u670d\u52a1\u5668\u7684\u4e34\u65f6\u76ee\u5f55\u91cc.\n    data - \u8fd9\u4e2a\u6587\u4ef6\u5206\u7247\u7684\u6570\u636e.\n    '''\n    url = ''.join([\n        const.PCS_URL_C,\n        'file?method=upload&type=tmpfile&app_id=250528',\n        '&', cookie.sub_output('BDUSS'),\n    ])\n    fields = []\n    files = [('file', ' ', data)]\n    headers = {'Accept': const.ACCEPT_HTML,'Origin': const.PAN_URL}\n    req = net.post_multipart(url, headers, fields, files)\n    if req:\n        return json.loads(req.data.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a superfile from a cookie.", "response": "def create_superfile(cookie, path, block_list):\n    '''\u5408\u5e76slice_upload()\u4e2d\u4ea7\u751f\u7684\u4e34\u65f6\u6587\u4ef6\n\n    path       - \u6587\u4ef6\u5728\u670d\u52a1\u5668\u4e0a\u7684\u7edd\u5bf9\u8def\u5f84\n    block_list - \u8fd9\u4e9b\u6587\u4ef6\u5206\u7247\u7684MD5\u5217\u8868\n    \u8fd4\u56de\u5b8c\u6574\u7684\u6587\u4ef6pcs\u4fe1\u606f.\n    '''\n    url = ''.join([\n        const.PCS_URL_C,\n        'file?method=createsuperfile&app_id=250528',\n        '&path=', encoder.encode_uri_component(path),\n        '&', cookie.sub_output('BDUSS'),\n    ])\n    param = {'block_list': block_list}\n    data = 'param=' + json.dumps(param)\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()},\n                      data=data.encode())\n    if req:\n        return json.loads(req.data.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_metas(cookie, tokens, filelist, dlink=True):\n    '''\u83b7\u53d6\u591a\u4e2a\u6587\u4ef6\u7684metadata.\n\n    filelist - \u4e00\u4e2alist, \u91cc\u9762\u662f\u6bcf\u4e2a\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84.\n               \u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32, \u53ea\u5305\u542b\u4e00\u4e2a\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84.\n    dlink    - \u662f\u5426\u5305\u542b\u4e0b\u8f7d\u94fe\u63a5, \u9ed8\u8ba4\u4e3aTrue, \u5305\u542b.\n\n    @return \u5305\u542b\u4e86\u6587\u4ef6\u7684\u4e0b\u8f7d\u94fe\u63a5dlink, \u901a\u8fc7\u5b83\u53ef\u4ee5\u5f97\u5230\u6700\u7ec8\u7684\u4e0b\u8f7d\u94fe\u63a5.\n    '''\n    if isinstance(filelist, str):\n        filelist = [filelist, ]\n    url = ''.join([\n        const.PAN_API_URL,\n        'filemetas?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    if dlink:\n        data = ('dlink=1&target=' +\n                encoder.encode_uri_component(json.dumps(filelist)))\n    else:\n        data = ('dlink=0&target=' +\n                encoder.encode_uri_component(json.dumps(filelist)))\n    req = net.urlopen(url, headers={\n        'Cookie': cookie.sub_output('BDUSS'),\n        'Content-type': const.CONTENT_FORM,\n        }, data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Get metadata from file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cloud_add_link_task(cookie, tokens, source_url, save_path,\n                        vcode='', vcode_input=''):\n    '''\u65b0\u5efa\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1.\n    \n    source_url - \u53ef\u4ee5\u662fhttp/https/ftp\u7b49\u4e00\u822c\u7684\u94fe\u63a5\n                 \u53ef\u4ee5\u662feMule\u8fd9\u6837\u7684\u94fe\u63a5\n    path       - \u8981\u4fdd\u5b58\u5230\u54ea\u4e2a\u76ee\u5f55, \u6bd4\u5982 /Music/, \u4ee5/\u5f00\u5934, \u4ee5/\u7ed3\u5c3e\u7684\u7edd\u5bf9\u8def\u5f84.\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'rest/2.0/services/cloud_dl?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    type_ = ''\n    if source_url.startswith('ed2k'):\n        type_ = '&type=3'\n    if not save_path.endswith('/'):\n        save_path = save_path + '/'\n    data = [\n        'method=add_task&app_id=250528',\n        '&source_url=', encoder.encode_uri_component(source_url),\n        '&save_path=', encoder.encode_uri_component(save_path),\n        '&type=', type_,\n    ]\n    if vcode:\n        data.append('&input=')\n        data.append(vcode_input)\n        data.append('&vcode=')\n        data.append(vcode)\n    data = ''.join(data)\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()},\n                      data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Add a link to a cloud page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cloud_add_bt_task(cookie, tokens, source_url, save_path, selected_idx,\n                      file_sha1='', vcode='', vcode_input=''):\n    '''\u65b0\u5efa\u4e00\u4e2aBT\u7c7b\u7684\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1, \u5305\u62ecmagent\u78c1\u94fe.\n\n    source_path  - BT\u79cd\u5b50\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\n    save_path    - \u4e0b\u8f7d\u7684\u6587\u4ef6\u8981\u5b58\u653e\u5230\u7684\u76ee\u5f55\n    selected_idx - BT\u79cd\u5b50\u4e2d, \u5305\u542b\u82e5\u5e72\u4e2a\u6587\u4ef6, \u8fd9\u91cc, \u6765\u6307\u5b9a\u8981\u4e0b\u8f7d\u54ea\u4e9b\u6587\u4ef6,\n                   \u4ece1\u5f00\u59cb\u8ba1\u6570.\n    file_sha1    - BT\u79cd\u5b50\u7684sha1\u503c, \u5982\u679c\u662fmagent\u7684\u8bdd, \u8fd9\u4e2asha1\u503c\u53ef\u4ee5\u4e3a\u7a7a\n    vcode        - \u9a8c\u8bc1\u7801\u7684vcode\n    vcode_input  - \u7528\u6237\u8f93\u5165\u7684\u56db\u4f4d\u9a8c\u8bc1\u7801\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'rest/2.0/services/cloud_dl?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    type_ = '2'\n    url_type = 'source_path'\n    if source_url.startswith('magnet:'):\n        type_ = '4'\n        url_type = 'source_url'\n    if not save_path.endswith('/'):\n        save_path = save_path + '/'\n    data = [\n        'method=add_task&app_id=250528',\n        '&file_sha1=', file_sha1,\n        '&save_path=', encoder.encode_uri_component(save_path),\n        '&selected_idx=', ','.join(str(i) for i in selected_idx),\n        '&task_from=1',\n        '&t=', util.timestamp(),\n        '&', url_type, '=', encoder.encode_uri_component(source_url),\n        '&type=', type_\n    ]\n    if vcode:\n        data.append('&input=')\n        data.append(vcode_input)\n        data.append('&vcode=')\n        data.append(vcode)\n    data = ''.join(data)\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()},\n                      data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Add a BT task to the cloud."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cloud_query_sinfo(cookie, tokens, source_path):\n    '''\u83b7\u53d6\u7f51\u76d8\u4e2d\u79cd\u5b50\u7684\u4fe1\u606f, \u6bd4\u5982\u91cc\u9762\u7684\u6587\u4ef6\u540d, \u6587\u4ef6\u5927\u5c0f\u7b49.\n\n    source_path - BT\u79cd\u5b50\u7684\u7edd\u5bf9\u8def\u5f84.\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'rest/2.0/services/cloud_dl?channel=chunlei&clienttype=0&web=1',\n        '&method=query_sinfo&app_id=250528',\n        '&bdstoken=', tokens['bdstoken'],\n        '&source_path=', encoder.encode_uri_component(source_path),\n        '&type=2',\n        '&t=', util.timestamp(),\n    ])\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()})\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None", "response": "Query Cloud DL for info."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries Cloud DL for magnetinfo.", "response": "def cloud_query_magnetinfo(cookie, tokens, source_url, save_path):\n    '''\u83b7\u53d6\u78c1\u94fe\u7684\u4fe1\u606f.\n    \n    \u5728\u65b0\u5efa\u78c1\u94fe\u4efb\u52a1\u65f6, \u8981\u5148\u83b7\u53d6\u8fd9\u4e2a\u78c1\u94fe\u7684\u4fe1\u606f, \u6bd4\u5982\u91cc\u9762\u5305\u542b\u54ea\u4e9b\u6587\u4ef6, \u6587\u4ef6\u7684\u540d\n    \u79f0\u4e0e\u5927\u5c0f\u7b49.\n\n    source_url - \u78c1\u94fe\u7684url, \u4ee5magent:\u5f00\u5934.\n    save_path  - \u4fdd\u5b58\u5230\u54ea\u4e2a\u76ee\u5f55\n    '''\n    url = ''.join([\n        const.PAN_URL,\n        'rest/2.0/services/cloud_dl?channel=chunlei&clienttype=0&web=1',\n        '&bdstoken=', tokens['bdstoken'],\n    ])\n    data = ''.join([\n        'method=query_magnetinfo&app_id=250528',\n        '&source_url=', encoder.encode_uri_component(source_url),\n        '&save_path=', encoder.encode_uri_component(save_path),\n        '&type=4',\n    ])\n    req = net.urlopen(url, headers={'Cookie': cookie.header_output()},\n                      data=data.encode())\n    if req:\n        content = req.data\n        return json.loads(content.decode())\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying Cast operator to a base on the given operator scope.", "response": "def apply_cast(scope, input_name, output_name, container, operator_name=None, to=None):\n    '''\n    :param to: enum defined in ONNX TensorProto.DataType, for example, TensorProto.FLOAT and TensorProto.INT64.\n    '''\n    name = _create_name_or_use_existing_one(scope, 'Cast', operator_name)\n    attrs = {'name': name}\n\n    d = onnx_proto.TensorProto.DataType.DESCRIPTOR\n    allowed_type_name_and_type_enum_pairs = {v.number: k for k, v in d.values_by_name.items()}\n    if to not in allowed_type_name_and_type_enum_pairs:\n        raise ValueError('Attribute \"to\" must be one of %s' % allowed_type_name_and_type_enum_pairs.keys())\n\n    if container.target_opset < 9:\n        if to in [onnx_proto.TensorProto.STRING, onnx_proto.TensorProto.COMPLEX64, onnx_proto.TensorProto.COMPLEX128]:\n            raise ValueError('Attribute \"to\" cannot correspond to a String or Complex TensorProto type.')\n\n        if container.target_opset < 6:\n            # Convert enum to string, for example, TensorProto.INT64 to 'INT64'\n            attrs['to'] = allowed_type_name_and_type_enum_pairs[to]\n            op_version = 1\n        else:\n            # Enum, for example, TensorProto.INT64\n            attrs['to'] = to\n            op_version = 6\n    else:\n        # Enum value, for example, TensorProto.INT64\n        # String casting is supported in opset 9\n        if to in [onnx_proto.TensorProto.COMPLEX64, onnx_proto.TensorProto.COMPLEX128]:\n            raise ValueError('Attribute \"to\" cannot correspond to a Complex TensorProto type.')\n        attrs['to'] = to\n        op_version = 9\n\n    container.add_node('Cast', input_name, output_name, op_version=op_version, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_gemm(scope, input_name, output_name, container, operator_name=None, alpha=1.0, beta=1.0,\n               transA=0, transB=0):\n    \"\"\"\n    Applies operator `gemm <https://github.com/onnx/onnx/blob/master/docs/Operators.md#gemm>`.\n    \"\"\"\n    name = _create_name_or_use_existing_one(scope, 'Gemm', operator_name)\n    attrs = {'alpha': alpha, 'beta': beta, 'transA': transA, 'transB': transB}\n    if container.target_opset < 5:\n        attrs['op_version'] = 1\n        attrs['broadcast'] = 1\n    elif container.target_opset < 7:\n        attrs['op_version'] = 6\n        attrs['broadcast'] = 1\n    else:\n        attrs['op_version'] = 7\n\n    container.add_node('Gemm', input_name, output_name, name=name, **attrs)", "response": "Applies operator Gemm to the given container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply_upsample(scope, input_name, output_name, container, operator_name=None, mode='nearest', scales=None):\n    '''\n    :param mode: nearest or linear\n    :param scales: an integer list of scaling-up rate of all input dimensions\n    '''\n    if container.target_opset < 10:\n        name = _create_name_or_use_existing_one(scope, 'Upsample', operator_name)\n        inputs = [input_name]\n        attrs = {'name': name}\n        if container.target_opset < 7:\n            if len(scales) != 4:\n                raise ValueError('Need to specify a 4-element list the the scales of N-, C-, H-, and W-axes')\n            attrs['height_scale'] = float(scales[2])\n            attrs['width_scale'] = float(scales[3])\n            attrs['mode'] = mode.upper()\n            op_version = 1\n        else:\n            attrs['mode'] = mode.lower()\n            if container.target_opset < 9:\n                attrs['scales'] = list(map(float, scales))\n                op_version = 7\n            else:\n                # scales moved from attribute to input in opset 9\n                scales_tensor_name = scope.get_unique_variable_name(name + '_scales')\n                container.add_initializer(scales_tensor_name, onnx_proto.TensorProto.FLOAT, [len(scales)], scales)\n                inputs = [input_name, scales_tensor_name]\n                op_version = 9\n\n        container.add_node('Upsample', inputs, output_name, op_version=op_version, **attrs)\n    else:\n        # TODO, we need verify this after onnx opset 10 release\n        name = _create_name_or_use_existing_one(scope, 'Resize', operator_name)\n        attrs = {'name': name}\n        attrs['mode'] = mode.lower()\n\n        scales_tensor_name = scope.get_unique_variable_name(name + '_scales')\n        container.add_initializer(scales_tensor_name, onnx_proto.TensorProto.FLOAT, [len(scales)], scales)\n        inputs = [input_name, scales_tensor_name]\n        op_version = 10\n\n        container.add_node('Resize', inputs, output_name, op_version=op_version, **attrs)", "response": "This operator applies an Upsample operator on the internal architectures."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_coreml_feature(feature_info, target_opset, batch_size=1):\n    '''\n    Encode type information from CoreML's FeatureType protobuf message in converter's type system.\n\n    Scalar types such as Int64FeatureType, DoubleFeatureType, and StringFeatureType in CoreML are interpreted as\n    [batch_size, 1]-tensor. Tensor-like types such as ArrayFeature in CoreML is viewed as tensors with a prepend\n    batch_size; for example, we use [batch_size, C, H, W] to denote [C, H, W]-array in CoreML.\n    :param feature_info: CoreML FeatureDescription (https://apple.github.io/coremltools/coremlspecification/sections/DataStructuresAndFeatureTypes.html#featuretype)\n    :param target_opset: the target ospet number in the converted model.\n    :param batch_size: default batch size prepend to scalars and tensors variables from CoreML\n    :return: one of our Int64Type, FloatType, StringType, Int64TensorType, FloatTensorType, or DictionaryType\n    '''\n    raw_type = feature_info.type\n    doc_string = feature_info.shortDescription\n    type_name = raw_type.WhichOneof('Type')\n\n    if type_name == 'int64Type':\n        return Int64Type(doc_string=doc_string)\n    elif type_name == 'doubleType':\n        return FloatType(doc_string=doc_string)\n    elif type_name == 'stringType':\n        return StringType(doc_string=doc_string)\n    elif type_name == 'imageType':\n        # Produce [C, H, W]-tensor, where C is the number of color channels, H the height, and W the width.\n        color_space = raw_type.imageType.colorSpace\n        shape = [batch_size]\n\n        if doc_string:\n            if doc_string[-1] not in ['.', '!', '?']:\n                doc_string += '. '\n            else:\n                doc_string += ' '\n\n        if color_space == 10:  # gray scale\n            shape.append(1)\n            doc_string += 'Image(s) in gray scale. If there are N images, it is a 4-D tensor with shape [N, 1, H, W]'\n        elif color_space == 20:  # RGB (20)\n            shape.append(3)\n            doc_string += 'Image(s) in RGB format. It is a [N, C, H, W]-tensor. The 1st/2nd/3rd slices along the ' \\\n                          'C-axis are red, green, and blue channels, respectively.'\n        elif color_space == 30:  # BGR (30)\n            shape.append(3)\n            doc_string += 'Image(s) in BGR format. It is a [N, C, H, W]-tensor. The 1st/2nd/3rd slices along the ' \\\n                          'C-axis are blue, green, and red channels, respectively.'\n        else:\n            raise ValueError('Unknown image format. Only gray-level, RGB, and BGR are supported')\n        shape.append(raw_type.imageType.height)\n        shape.append(raw_type.imageType.width)\n        color_space_map = {10: 'Gray8', 20: 'Rgb8', 30: 'Bgr8'}\n        return FloatTensorType(shape, color_space_map[color_space], doc_string=doc_string,\n                               denotation='IMAGE', channel_denotations=['DATA_BATCH', 'DATA_CHANNEL', 'DATA_FEATURE', 'DATA_FEATURE'])\n    elif type_name == 'multiArrayType':\n        element_type_id = raw_type.multiArrayType.dataType\n        shape = [d for d in raw_type.multiArrayType.shape]\n        if len(shape) == 1:\n            # [C]\n            shape = [batch_size, shape[0]]\n        elif len(shape) == 3:\n            # [C, H, W]\n            shape = [batch_size, shape[0], shape[1], shape[2]]\n        else:\n            shape = [batch_size, 1]  # Missing shape information. We will try inferring it.\n\n        if element_type_id in [65568, 65600]:\n            # CoreML FLOAT32 & DOUBLE\n            return FloatTensorType(shape, doc_string=doc_string)\n        elif element_type_id == 131104:\n            # CoreML INT32\n            return Int64TensorType(shape, doc_string=doc_string)\n        else:\n            raise ValueError('Invalid element type')\n    elif type_name == 'dictionaryType':\n        key_type = raw_type.dictionaryType.WhichOneof('KeyType')\n        if key_type == 'int64KeyType':\n            if target_opset < 7:\n                return DictionaryType(Int64TensorType([1]), FloatTensorType([1]), doc_string=doc_string)\n            else:\n                return DictionaryType(Int64TensorType([]), FloatTensorType([]), doc_string=doc_string)\n        elif key_type == 'stringKeyType':\n            if target_opset < 7:\n                return DictionaryType(StringTensorType([1]), FloatTensorType([1]), doc_string=doc_string)\n            else:\n                return DictionaryType(StringTensorType([]), FloatTensorType([]), doc_string=doc_string)\n        else:\n            raise ValueError('Unsupported key type: {}'.format(key_type))\n    else:\n        raise ValueError('Unsupported feature type: {}'.format(type_name))", "response": "Parses a CoreML feature into a Int64Type FloatType or DictionaryType."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a simple CoreML model containing only one operator and one input and one output.", "response": "def _parse_simple_model(topology, parent_scope, model, inputs, outputs):\n    '''\n    Parse a model containing only one operator (aka simple model).\n    Steps:\n        1. Create local scope for allocating local variables and operators\n        2. Create operator and then feed the model's inputs and outputs to the operator\n        3. Connect local variables and their corresponding parent variables\n    Note:\n        1. Notice that a CoreML operator can contain no input and output, so we directly use model's inputs (outputs).\n        2. Input and output names can be identical in CoreML, but they must be different for ONNX.\n    '''\n\n    # Create local scope for the considered model\n    scope = topology.declare_scope('single', [parent_scope] + parent_scope.parent_scopes)\n\n    # Create operator for the considered model\n    this_operator = scope.declare_local_operator(model.WhichOneof('Type'), model)\n\n    # Allocate inputs for the operator and then connect them with inputs from outside\n    for var in model.description.input:\n        # We assume that no duplicated raw name exists. Note that we set prepend=True because model inputs should\n        # not hide any intermediate variables.\n        variable = scope.declare_local_variable(\n            var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size),\n            prepend=True)\n        this_operator.inputs.append(variable)\n\n    # Connect local variables and variables passed into this scope. Our assumptions are described below.\n    # 1. Assume a variable with 'A' as its CoreML name is passed in. There must be at least one local variable gets a\n    #    raw name 'A'. That is, for each parent variable, at least one local duplicate is available.\n    # 2. It's possible to find multiple local variables associated with the same raw name. For example, raw name 'A' can\n    #    be associated with 'A' and 'A1' in ONNX. In this case, we connect the first one to parent input.\n    for parent_variable in inputs:\n        raw_name = parent_variable.raw_name\n        child_variable = scope.variables[scope.variable_name_mapping[raw_name][0]]\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(parent_variable)\n        operator.outputs.append(child_variable)\n\n    # Allocate outputs for the operator and then connect them with outputs from outside\n    for var in model.description.output:\n        # We assume that no duplicated output raw name exists.\n        variable = scope.declare_local_variable(\n            var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size))\n        this_operator.outputs.append(variable)\n\n    # Connect local variables and variables passed into this scope. Our assumptions are described below.\n    # 1. Assume a variable with 'A' as its CoreML name is passed in. There must be at least one local variable gets a\n    #    raw name 'A'. That is, for each parent variable, at least one local duplicate is available.\n    # 2. It's possible to find multiple local variables associated with the same raw name. For example, raw name 'A' can\n    #    be associated with 'A' and 'A1' in ONNX. In this case, we connect the last one to parent output.\n    for parent_variable in outputs:\n        raw_name = parent_variable.raw_name\n        child_variable = scope.variables[scope.variable_name_mapping[raw_name][-1]]\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(child_variable)\n        operator.outputs.append(parent_variable)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_pipeline_model(topology, parent_scope, model, inputs, outputs):\n    '''\n    Parse a pipeline including multiple sub-models.\n    Steps:\n        1. Create local scope for allocating local variables and operators\n        2. Sequentially parse the sub-models and create their inputs and outputs variables\n        3. Connect model's (not sub-model's) inputs and outputs with proper variables created when parsing sub-models\n        4. Link local variables and the corresponding parent variables (only model's inputs and outputs are considered)\n    Note:\n        1. A CoreML sub-model can use the same variable for its input and output.\n        2. Two CoreML variables may have the same name but different types.\n    '''\n\n    # Create local scope\n    scope = topology.declare_scope('pipeline', [parent_scope] + parent_scope.parent_scopes)\n\n    # Use the same name to denote sub-models\n    pipeline_type = model.WhichOneof('Type')\n    if pipeline_type == 'pipelineClassifier':\n        sub_models = model.pipelineClassifier.pipeline.models\n    elif pipeline_type == 'pipelineRegressor':\n        sub_models = model.pipelineRegressor.pipeline.models\n    elif pipeline_type == 'pipeline':\n        sub_models = model.pipeline.models\n    else:\n        raise ValueError('Unsupported CoreML pipeline type: {0}'.format(pipeline_type))\n\n    # Sequentially parse the sub-models\n    for sub_model in sub_models:\n        # Declare the sub-model's input and output in this scope. Those input and output variables will be passed into\n        # the sub-model's parsing function and connected with proper child variables.\n        sub_inputs = []\n        for var in sub_model.description.input:\n            variable = scope.get_local_variable_or_declare_one(\n                var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size))\n            sub_inputs.append(variable)\n        sub_outputs = []\n        for var in sub_model.description.output:\n            variable = scope.declare_local_variable(\n                var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size))\n            sub_outputs.append(variable)\n        _parse_model(topology, scope, sub_model, sub_inputs, sub_outputs)\n\n    # Declare the model's (not sub-model's) inputs and then link them with sub-model's inputs\n    for var in model.description.input:\n        # Find the first variable with the same raw name declared when parsing the sub-models\n        child_variable = scope.variables[scope.variable_name_mapping[var.name][0]]\n        # Create model's input variable. Note that we set prepend=True because model inputs should not hide any\n        # intermediate variables.\n        variable = scope.declare_local_variable(\n            var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size),\n            prepend=True)\n        # Feed the input to the sub-model's input. It's possible to add type conversion here by using a casting operator\n        # rather than identity, but we haven't see the need of doing so in practices.\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(variable)\n        operator.outputs.append(child_variable)\n    for parent_variable in inputs:\n        raw_name = parent_variable.raw_name\n        child_variable = scope.variables[scope.variable_name_mapping[raw_name][0]]\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(parent_variable)\n        operator.outputs.append(child_variable)\n\n    # Declare the model's (not sub-model's) inputs and then link them with sub-model's inputs\n    for var in model.description.output:\n        # Find the latest variable with the same raw name declared when parsing the sub-models\n        child_variable = scope.variables[scope.variable_name_mapping[var.name][-1]]\n        # Create model's output variable\n        variable = scope.declare_local_variable(\n            var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size))\n        # Connect the input and a sub-model's input. It's possible to add type conversion here by using a casting\n        # operator rather than identity, but we haven't see the need of doing so in practices.\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(child_variable)\n        operator.outputs.append(variable)\n    for parent_variable in outputs:\n        raw_name = parent_variable.raw_name\n        child_variable = scope.variables[scope.variable_name_mapping[raw_name][-1]]\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(child_variable)\n        operator.outputs.append(parent_variable)", "response": "Parses a CoreML pipeline and returns a CoreML pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a neural network model and return a CoreML model.", "response": "def _parse_neural_network_model(topology, parent_scope, model, inputs, outputs):\n    '''\n    Parse a neural network model.\n    Steps:\n        1. Create local scope for allocating local variables and operators\n        2. Sequentially parse the preprocessors and layers\n        3. Connect model's (neither layers' nor preprocessors') inputs and outputs with proper variables created when\n           parsing sub-models.\n        4. Link local variables and the corresponding parent variables (only model's inputs and outputs are considered)\n    Note:\n        1. A CoreML preprocessor/layer can use the same variable for its input and output.\n        2. Two CoreML variables may have the same name but different types.\n        3. Preprocessor sometime may not include any information about its input\n    '''\n\n    # Create local scope to which all subsequent variables and operators belongs\n    scope = topology.declare_scope('NeuralNetwork', [parent_scope] + parent_scope.parent_scopes)\n\n    network = None\n    network_type = model.WhichOneof('Type')\n    if network_type == 'neuralNetworkClassifier':\n        network = model.neuralNetworkClassifier\n    elif network_type == 'neuralNetworkRegressor':\n        network = model.neuralNetworkRegressor\n    elif network_type == 'neuralNetwork':\n        network = model.neuralNetwork\n    else:\n        raise ValueError('Unknown network type {}'.format(network_type))\n\n    for op in network.preprocessing:\n        operator = scope.declare_local_operator(op.WhichOneof('preprocessor') + 'Preprocessor', op)\n\n        # Infer the variable name to be processed if feature name is an empty string\n        name = op.featureName if op.featureName != '' else model.description.input[0].name\n\n        # Find out input variable\n        original = scope.get_local_variable_or_declare_one(name)\n        original.type = FloatTensorType()  # A newly-declared variable has no type, so we add it.\n        operator.inputs.append(original)\n\n        # Declare a variable for storing the processed result\n        processed = scope.declare_local_variable(name)\n        processed.type = FloatTensorType()  # A newly-declared variable has no type, so we add it\n        operator.outputs.append(processed)\n\n    for op in network.layers:\n        operator = scope.declare_local_operator(op.WhichOneof('layer'), op)\n\n        # Find out input variable and connect them with the operator\n        for name in op.input:\n            variable = scope.get_local_variable_or_declare_one(name)\n            # Although most neural network operators only accepts floats, we still need to handle the only exception,\n            # embedding layer. In the furture, we should create a Cast operator right inside embedding's converter.\n            if operator.type == 'embedding':\n                variable.type = Int64TensorType()\n            else:\n                variable.type = FloatTensorType()\n            operator.inputs.append(variable)\n\n        # Declare variables for catching the operator's outputs\n        for name in op.output:\n            variable = scope.declare_local_variable(name)\n            variable.type = FloatTensorType()  # A newly-declared variable has no type, so we add it\n            operator.outputs.append(variable)\n\n    sink_variables = scope.find_sink_variables()\n\n    # Declare the model's inputs and outputs. Then, connect them with proper variables computed by the main network\n    for var in model.description.input:\n        # Search for the first variable (declared when parsing network layers) associated with the considered raw name\n        child_variable = scope.variables[scope.variable_name_mapping[var.name][0]]\n\n        # Declare model input. To prevent intermediate variables form being hidden by model inputs, prepend is True.\n        variable = scope.declare_local_variable(\n            var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size),\n            prepend=True)\n\n        # A heuristic which forces the input of embedding to be integer tensor rather than float tensor.\n        # Ideally this should be done by adding a cast operator, but ONNX doesn't have float-to-int casting.\n        # If this variable is produced by another component in a CoreML pipeline, a bug may occur especially\n        # when the source component's output type is float tensor.\n        if isinstance(child_variable.type, Int64TensorType):\n            variable.type = Int64TensorType(variable.type.shape)\n\n        # Feed model input to the associated model input\n        operator_type = find_type_conversion(source_type=variable.type, target_type=child_variable.type)\n        operator = scope.declare_local_operator(operator_type)\n        operator.inputs.append(variable)\n        operator.outputs.append(child_variable)\n\n    # Connect local input variables with proper variables from parent scope\n    for parent_variable in inputs:\n        raw_name = parent_variable.raw_name\n        child_variable = scope.variables[scope.variable_name_mapping[raw_name][0]]\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(parent_variable)\n        operator.outputs.append(child_variable)\n\n    for var in model.description.output:\n        # CoreML's predicted label is not connected with any operator, so we handle it later as a special case.\n        special_variable_names = [model.description.predictedFeatureName, model.description.predictedProbabilitiesName]\n        if model.WhichOneof('Type') == 'neuralNetworkClassifier' and var.name in special_variable_names:\n            continue\n        # Search for the latest variable (declared when parsing network layers) associated with the considered raw name\n        child_variable = scope.variables[scope.variable_name_mapping[var.name][-1]]\n\n        # Create model output variable\n        variable = scope.declare_local_variable(\n            var.name, _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size))\n\n        # Feed result calculated by the network to the output variable\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(child_variable)\n        operator.outputs.append(variable)\n\n    # If predicted label exists, connect probability tensor and label by a special operator\n    if model.WhichOneof('Type') == 'neuralNetworkClassifier' and model.description.predictedFeatureName:\n        # Find out the description of predicted label and declare a label variable\n        label_variable = None\n        for var in model.description.output:\n            if var.name == model.description.predictedFeatureName:\n                label_type = _parse_coreml_feature(var, topology.target_opset, topology.default_batch_size)\n                label_variable = scope.declare_local_variable(var.name, label_type)\n                break\n        operator = scope.declare_local_operator('tensorToLabel', model)\n\n        probability_name = model.description.predictedProbabilitiesName\n        if probability_name in scope.variable_name_mapping:\n            # Find the latest probability variable\n            operator.inputs.append(scope.variables[scope.variable_name_mapping[probability_name][-1]])\n        else:\n            # If predicted probability tensor is missing in CoreML model, it defaults to the first sink of the network\n            operator.inputs.append(sink_variables[0])\n        operator.outputs.append(label_variable)\n\n    # Probability tensor is implicitly converted into a dictionary (i.e., map) in CoreML. We handle this case here.\n    if model.WhichOneof('Type') == 'neuralNetworkClassifier' and model.description.predictedProbabilitiesName:\n        operator = scope.declare_local_operator('tensorToProbabilityMap', model)\n\n        probability_name = model.description.predictedProbabilitiesName\n        if probability_name in scope.variable_name_mapping:\n            # Find the latest probability variable\n            operator.inputs.append(scope.variables[scope.variable_name_mapping[probability_name][-1]])\n        else:\n            # If predicted probability tensor is missing in CoreML model, it defaults to the first sink of the network\n            operator.inputs.append(sink_variables[0])\n\n        # Find out the description of predicted probabilities and declare a variable for probability map\n        for var in model.description.output:\n            if var.name == model.description.predictedProbabilitiesName:\n                probability_type = _parse_coreml_feature(var, topology.target_opset,\n                                                         topology.default_batch_size)\n                probability_variable = scope.declare_local_variable(var.name, probability_type)\n                operator.outputs.append(probability_variable)\n                break\n\n    # Connect local output variables with proper variables from parent scope\n    for parent_variable in outputs:\n        raw_name = parent_variable.raw_name\n        child_variable = scope.variables[scope.variable_name_mapping[raw_name][-1]]\n        operator = scope.declare_local_operator('identity')\n        operator.inputs.append(child_variable)\n        operator.outputs.append(parent_variable)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nseeing LSTM s conversion function for its output shapes.", "response": "def calculate_lstm_output_shapes(operator):\n    '''\n    See LSTM's conversion function for its output shapes.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, 3], output_count_range=[1, 3])\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input_shape = operator.inputs[0].type.shape\n\n    if len(input_shape) not in [2, 4]:\n        raise RuntimeError('Input must be a 2-D tensor')\n\n    params = operator.raw_operator.uniDirectionalLSTM\n\n    # The following line is more accurate but it may break some tests\n    # output_shape = ['None', params.outputVectorSize] if params.params.sequenceOutput else [1, params.outputVectorSize]\n    output_shape = ['None', params.outputVectorSize]\n    state_shape = [1, params.outputVectorSize]\n\n    # TODO: Changing input shapes of an operator is dangerous, this should be move to Topology's _fix_shapes function\n    if len(operator.inputs) > 1:\n        Y_h_in = operator.inputs[1]  # The initial hidden state of a single sequence\n        Y_h_in.type.shape = state_shape\n    if len(operator.inputs) > 2:\n        Y_c_in = operator.inputs[2]  # The initial cell state of a single sequence\n        Y_c_in.type.shape = state_shape\n\n    operator.outputs[0].type.shape = output_shape\n    if len(operator.outputs) > 1:\n        operator.outputs[1].type.shape = state_shape\n    if len(operator.outputs) > 2:\n        operator.outputs[2].type.shape = state_shape"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve parameters of a model.", "response": "def get_xgb_params(xgb_node):\n    \"\"\"\n    Retrieves parameters of a model.\n    \"\"\"\n    if hasattr(xgb_node, 'kwargs'):\n        # XGBoost >= 0.7\n        params = xgb_node.get_xgb_params()\n    else:\n        # XGBoost < 0.7\n        params = xgb_node.__dict__\n        \n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a TensorProto with specified arguments. If raw is False, this function will choose the corresponding proto field to store the values based on data_type. If raw is True, use \"raw_data\" proto field to store the values, and values should be of type bytes in this case.", "response": "def _make_tensor_fixed(name, data_type, dims, vals, raw=False):\n    '''\n    Make a TensorProto with specified arguments.  If raw is False, this\n    function will choose the corresponding proto field to store the\n    values based on data_type. If raw is True, use \"raw_data\" proto\n    field to store the values, and values should be of type bytes in\n    this case.\n    '''\n    tensor = TensorProto()\n    tensor.data_type = data_type\n    tensor.name = name\n\n    if (data_type == TensorProto.COMPLEX64 or\n            data_type == TensorProto.COMPLEX128):\n        vals = split_complex_to_pairs(vals)\n    if raw:\n        tensor.raw_data = vals\n    else:\n        field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[\n            mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[data_type]]\n        getattr(tensor, field).extend(vals)\n\n    tensor.dims.extend(dims)\n    return tensor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_embedding_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, 1] ---> [N, C]\n        2. [N, 1, 1, 1] ---> [N, C, 1, 1]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[Int64Type, Int64TensorType])\n\n    output = operator.outputs[0]\n\n    input_shape = operator.inputs[0].type.shape\n\n    if input_shape[1] != 1 or (len(input_shape) > 2 and (input_shape[2] != 1 or input_shape[3] != 1)):\n        raise RuntimeError('If input is a 4-D tensor, its shape must be [N, 1, 1, 1]')\n\n    params = operator.raw_operator.embedding\n    if len(input_shape) == 4:\n        output_shape = [input_shape[0], params.outputChannels, 1, 1]\n    elif len(input_shape) == 2:\n        output_shape = [input_shape[0], params.outputChannels]\n    else:\n        raise RuntimeError('Input must be a 2-D or a 4-D tensor')\n\n    output.type.shape = output_shape", "response": "This function calculates the output shapes of the embedding operator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nallowing input/output patterns are 1. Map ---> [1, C] C is the number of all allowed keys in the input dictionary.", "response": "def calculate_dictionary_vectorizer_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. Map ---> [1, C]\n\n    C is the number of all allowed keys in the input dictionary.\n    '''\n    # We assume all dictionaries' value types are float. It seems be reasonable to CoreML's\n    # model input, but the existence of other map types leads to some concerns.\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    # Two types are allowed. One is DictionaryType and the other one, SequenceType, means a sequence of dictionaries.\n    check_input_and_output_types(operator, good_input_types=[DictionaryType, SequenceType])\n\n    params = operator.raw_operator.dictVectorizer\n    string_key_vector = params.stringToIndex.vector\n    int64_key_vector = params.int64ToIndex.vector\n\n    if len(string_key_vector) > 0 and len(int64_key_vector) > 0:\n        raise RuntimeError('Only one key type can present at the same time')\n\n    doc_string = operator.outputs[0].type.doc_string\n    if len(string_key_vector) > 0:\n        operator.outputs[0].type = FloatTensorType([1, len(string_key_vector)], doc_string=doc_string)\n    elif len(int64_key_vector) > 0:\n        operator.outputs[1].type.shape = FloatTensorType([1, len(int64_key_vector)], doc_string=doc_string)\n    else:\n        raise ValueError('Key vector cannot be empty')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visualize_model(onnx_model, open_browser=True, dest=\"index.html\"):\n    graph = onnx_model.graph\n    model_info = \"Model produced by: \" + onnx_model.producer_name + \\\n        \" version(\" + onnx_model.producer_version + \")\"\n\n    html_str = \"\"\"\n    <!doctype html>\n    <meta charset=\"utf-8\">\n    <title>ONNX Visualization</title>\n    <script src=\"https://d3js.org/d3.v3.min.js\"></script>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n    <script src=\"dagre-d3.min.js\"></script>\n\n    <h2>[model_info]</h2>\n\n    <svg id=\"svg-canvas\" width=960 height=600></svg>\n\n    <script id=\"js\">\n    var g = new dagreD3.graphlib.Graph()\n    .setGraph({})\n    .setDefaultEdgeLabel(function() { return {}; });\n\n    [nodes_html]\n\n    g.nodes().forEach(function(v) {\n    var node = g.node(v);\n    // Round the corners of the nodes\n    node.rx = node.ry = 5;\n    });\n\n    [edges_html]\n\n    // Create the renderer\n    var render = new dagreD3.render();\n\n    // Set up an SVG group so that we can translate the final graph.\n    var svg = d3.select(\"svg\"),\n        svgGroup = svg.append(\"g\");\n\n    // Run the renderer. This is what draws the final graph.\n    render(d3.select(\"svg g\"), g);\n\n    // Center the graph\n    svgGroup.attr(\"transform\", \"translate(20, 20)\");\n    svg.attr(\"height\", g.graph().height + 40);\n    svg.attr(\"width\", g.graph().width + 40);\n    </script>\n    \"\"\"\n\n    html_str = html_str.replace(\"[nodes_html]\", \"\\n\".join(\n        get_nodes_builder(get_nodes(graph))))\n\n    html_str = html_str.replace(\"[edges_html]\", \"\\n\".join(\n        [get_set_edge(edge[0], edge[1]) for edge in get_edges(graph)]))\n\n    html_str = html_str.replace(\"[model_info]\", model_info)\n\n    Html_file = open(dest, \"w\")\n    Html_file.write(html_str)\n    Html_file.close()\n\n    pkgdir = sys.modules['onnxmltools'].__path__[0]\n    fullpath = os.path.join(pkgdir, \"utils\", \"styles.css\")\n    shutil.copy(fullpath, os.getcwd())\n    fullpath = os.path.join(pkgdir, \"utils\", \"dagre-d3.min.js\")\n    shutil.copy(fullpath, os.getcwd())\n\n    open_new_tab(\"file://\" + os.path.realpath(\"index.html\"))", "response": "Creates a graph visualization of an ONNX protobuf model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the ONNX cast op from the specified operator. :param origin_model:these :param oplist: :return:", "response": "def decast(origin_model, oplist):\n    \"\"\"\n    remove the ONNX cast op from the specified operator.\n    :param origin_model:these\n    :param oplist:\n    :return:\n    \"\"\"\n    graph = origin_model.graph\n    nodelist = list(graph.node)\n    del graph.node[:]\n\n    all_nodes = LinkedNode.build_from_onnx(nodelist,\n                                           [],\n                                           [i_.name for i_ in graph.input],\n                                           [o_.name for o_ in graph.output])\n\n    nodes = remove_cast(all_nodes, set(oplist))\n    for n_ in nodes:\n        graph.node.extend(n_.generate())\n\n    return origin_model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_convolution_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C, H', W']\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n\n    params = operator.raw_operator.convolution\n\n    input_shape = operator.inputs[0].type.shape\n    operator.outputs[0].type.shape = [0, 0, 0, 0]  # Initialize output shape. It will be modified below.\n    output_shape = operator.outputs[0].type.shape\n\n    # Adjust N-axis\n    output_shape[0] = input_shape[0]\n\n    # Adjust C-axis\n    output_shape[1] = params.outputChannels\n\n    # Set up default and non-default parameters\n    dilations = [1, 1]\n    if len(params.dilationFactor) > 0:\n        dilations = [params.dilationFactor[0], params.dilationFactor[1]]\n    kernel_shape = [3, 3]\n    if len(params.kernelSize) > 0:\n        kernel_shape = params.kernelSize\n    strides = [1, 1]\n    if len(params.stride) > 0:\n        strides = params.stride\n    specified_output_shape = [0, 0]  # Only used with convolution transpose\n    if params.isDeconvolution and len(params.outputShape) > 0:\n        specified_output_shape = list(int(i) for i in params.outputShape)\n    pad_mode = params.WhichOneof('ConvolutionPaddingType')\n    if pad_mode == 'valid' and len(params.valid.paddingAmounts.borderAmounts) > 0:\n        pad_amounts = params.valid.paddingAmounts.borderAmounts\n        pad_heads = [pad_amounts[0].startEdgeSize, pad_amounts[1].startEdgeSize]\n        pad_tails = [pad_amounts[0].endEdgeSize, pad_amounts[1].endEdgeSize]\n    else:\n        # Padding amounts are useless for same padding and valid padding uses [0, 0] by default.\n        pad_heads = [0, 0]\n        pad_tails = [0, 0]\n\n    # Adjust H- and W-axes\n    for i in range(2):\n        if params.isDeconvolution:\n            output_shape[i + 2] = calculate_convolution_transpose_1D_output_shape(\n                input_shape[i + 2], kernel_shape[i], dilations[i], strides[i],\n                pad_mode, pad_heads[i], pad_tails[i], specified_output_shape[i])\n        else:\n            output_shape[i + 2] = calculate_convolution_and_pooling_1D_output_shape(\n                input_shape[i + 2], kernel_shape[i], dilations[i], strides[i],\n                pad_mode, pad_heads[i], pad_tails[i])", "response": "Calculates the output shapes of the convolutional domain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_linear_classifier_output_shapes(operator):\n    '''\n    This operator maps an input feature vector into a scalar label if the number of outputs is one. If two outputs\n    appear in this operator's output list, we should further generate a map storing all classes' probabilities.\n\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, 1], A sequence of map\n\n    Note that the second case is not allowed as long as ZipMap only produces dictionary.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=[1, 2])\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType, Int64TensorType])\n    if len(operator.inputs[0].type.shape) != 2:\n        raise RuntimeError('Input must be a [N, C]-tensor')\n\n    N = operator.inputs[0].type.shape[0]\n\n    class_labels = operator.raw_operator.classes_\n    if all(isinstance(i, np.ndarray) for i in class_labels):\n        class_labels = np.concatenate(class_labels)\n    if all(isinstance(i, (six.string_types, six.text_type)) for i in class_labels):\n        operator.outputs[0].type = StringTensorType(shape=[N])\n        if len(class_labels) > 2 or operator.type != 'SklearnLinearSVC':\n            # For multi-class classifier, we produce a map for encoding the probabilities of all classes\n            if operator.target_opset < 7:\n                operator.outputs[1].type = DictionaryType(StringTensorType([1]), FloatTensorType([1]))\n            else:\n                operator.outputs[1].type = SequenceType(DictionaryType(StringTensorType([]), FloatTensorType([])), N)\n        else:\n            # For binary LinearSVC, we produce probability of the positive class\n            operator.outputs[1].type = FloatTensorType(shape=[N, 1])\n    elif all(isinstance(i, (numbers.Real, bool, np.bool_)) for i in class_labels):\n        operator.outputs[0].type = Int64TensorType(shape=[N])\n        if len(class_labels) > 2 or operator.type != 'SklearnLinearSVC':\n            # For multi-class classifier, we produce a map for encoding the probabilities of all classes\n            if operator.target_opset < 7:\n                operator.outputs[1].type = DictionaryType(Int64TensorType([1]), FloatTensorType([1]))\n            else:\n                operator.outputs[1].type = SequenceType(DictionaryType(Int64TensorType([]), FloatTensorType([])), N)\n        else:\n            # For binary LinearSVC, we produce probability of the positive class\n            operator.outputs[1].type = FloatTensorType(shape=[N, 1])\n    else:\n        raise ValueError('Unsupported or mixed label types')", "response": "This operator maps an input feature vector into a scalar label if the number of outputs is one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_linear_regressor_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, 1]\n\n    This operator produces a scalar prediction for every example in a batch. If the input batch size is N, the output\n    shape may be [N, 1].\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n\n    N = operator.inputs[0].type.shape[0]\n    op = operator.raw_operator\n    if hasattr(op, 'n_outputs_'):\n        nout = op.n_outputs_\n    else:\n        nout = 1\n    operator.outputs[0].type = FloatTensorType([N, nout])", "response": "This operator calculates the output shapes of the linear regressor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_upsample_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C, H', W']\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    scales = operator.raw_operator.upsample.scalingFactor\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    output_shape[2] *= scales[0]\n    output_shape[3] *= scales[1]\n\n    operator.outputs[0].type = FloatTensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)", "response": "This function calculates the output shapes of the last output of the last input pattern."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nevaluating a condition in a sequence of tokens.", "response": "def evaluate_condition(backend, condition):\n    \"\"\"\n    Evaluates a condition such as\n    ``StrictVersion(onnxruntime.__version__) <= StrictVersion('0.1.3')``\n    \"\"\"\n    if backend == \"onnxruntime\":\n        import onnxruntime\n        import onnx\n        return eval(condition)\n    else:\n        raise NotImplementedError(\"Not implemented for backend '{0}'\".format(backend))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntelling if a backend is enabled.", "response": "def is_backend_enabled(backend):\n    \"\"\"\n    Tells if a backend is enabled.\n    \"\"\"\n    if backend == \"onnxruntime\":\n        try:\n            import onnxruntime\n            return True\n        except ImportError:\n            return False\n    else:\n        raise NotImplementedError(\"Not implemented for backend '{0}'\".format(backend))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_converted_models(root=None):\n    if root is None:\n        root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"tests\"))\n        root = os.path.normpath(root)\n    if not os.path.exists(root):\n        raise FileNotFoundError(\"Unable to find '{0}'.\".format(root))\n    \n    founds = glob.iglob(\"{0}/**/*.model.onnx\".format(root), recursive=True)\n    keep = []\n    for found in founds:\n        onnx = found\n        basename = onnx[:-len(\".model.onnx\")]\n        data = basename + \".data.pkl\"\n        expected = basename + \".expected.pkl\"\n        res = dict(onnx=onnx, data=data, expected=expected)\n        ok = True\n        for k, v in res.items():\n            if not os.path.exists(v):\n                ok = False\n        if ok:\n            models = [basename + \".model.pkl\", basename + \".model.keras\"]\n            for model in models:\n                if os.path.exists(model):\n                    res['model'] = model\n                    break\n            if 'model' in res:\n                keep.append((basename, res))\n    keep.sort()\n    return [_[1] for _ in keep]", "response": "Searches for all converted models in folders tests and function\n    *dump_data_and_model*."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading all data and model files in a dictionary.", "response": "def load_data_and_model(items_as_dict, **context):\n    \"\"\"\n    Loads every file in a dictionary {key: filename}.\n    The extension is either *pkl* and *onnx* and determines\n    how it it loaded. If the value is not a string,\n    the function assumes it was already loaded.\n    \"\"\"\n    res = {}\n    for k, v in items_as_dict.items():\n        if isinstance(v, str):\n            if os.path.splitext(v)[-1] == \".pkl\":\n                with open(v, \"rb\") as f:\n                    try:\n                        bin = pickle.load(f)\n                    except ImportError as e:\n                        if '.model.' in v:\n                            continue\n                        else:\n                            raise ImportError(\"Unable to load '{0}' due to {1}\".format(v, e))\n                    res[k] = bin\n            elif os.path.splitext(v)[-1] == \".keras\":\n                import keras.models\n                res[k] = keras.models.load_model(v, custom_objects=context)\n            else:\n                res[k] = v\n        else:\n            res[k] = v\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_options(name):\n    opts = name.replace(\"\\\\\", \"/\").split(\"/\")[-1].split('.')[0].split('-')\n    if len(opts) == 1:\n        return {}\n    else:\n        res = {}\n        for opt in opts[1:]:\n            if opt in (\"SkipDim1\", \"OneOff\", \"NoProb\", \"Dec4\", \"Dec3\", 'Out0', 'Dec2', 'Reshape', 'Opp'):\n                res[opt] = True\n            else:\n                raise NameError(\"Unable to parse option '{}'\".format(opts[1:]))\n        return res", "response": "Extracts comparison options from a file name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare_outputs(expected, output, **kwargs):\n    SkipDim1 = kwargs.pop(\"SkipDim1\", False)\n    NoProb = kwargs.pop(\"NoProb\", False)\n    Dec4 = kwargs.pop(\"Dec4\", False)\n    Dec3 = kwargs.pop(\"Dec3\", False)\n    Dec2 = kwargs.pop(\"Dec2\", False)\n    Disc = kwargs.pop(\"Disc\", False)\n    Mism = kwargs.pop(\"Mism\", False)\n    Opp = kwargs.pop(\"Opp\", False)\n    if Opp and not NoProb:\n        raise ValueError(\"Opp is only available if NoProb is True\")\n\n    if Dec4:\n        kwargs[\"decimal\"] = min(kwargs[\"decimal\"], 4)\n    if Dec3:\n        kwargs[\"decimal\"] = min(kwargs[\"decimal\"], 3)\n    if Dec2:\n        kwargs[\"decimal\"] = min(kwargs[\"decimal\"], 2)\n        \n    if isinstance(expected, numpy.ndarray) and isinstance(output, numpy.ndarray):\n        if SkipDim1:\n            # Arrays like (2, 1, 2, 3) becomes (2, 2, 3) as one dimension is useless.\n            expected = expected.reshape(tuple([d for d in expected.shape if d > 1]))\n            output = output.reshape(tuple([d for d in expected.shape if d > 1]))\n        if NoProb:\n            # One vector is (N,) with scores, negative for class 0\n            # positive for class 1\n            # The other vector is (N, 2) score in two columns.\n            if len(output.shape) == 2 and output.shape[1] == 2 and len(expected.shape) == 1:\n                output = output[:, 1]                \n            elif len(output.shape) == 1 and len(expected.shape) == 1:\n                pass\n            elif len(expected.shape) == 1 and len(output.shape) == 2 and \\\n                    expected.shape[0] == output.shape[0] and output.shape[1] == 1:\n                output = output[:, 0]\n            elif expected.shape != output.shape:\n                raise NotImplementedError(\"No good shape: {0} != {1}\".format(expected.shape, output.shape))\n            if Opp:\n                output = -output\n        if len(expected.shape) == 1 and len(output.shape) == 2 and output.shape[1] == 1:\n            output = output.ravel()\n        if len(expected.shape) == 2 and len(output.shape) == 1 and expected.shape[1] == 1:\n            expected = expected.ravel()\n        if not numpy.issubdtype(expected.dtype, numpy.number):\n            try:\n                assert_array_equal(expected, output)\n            except Exception as e:\n                if Disc:\n                    # Bug to be fixed later.\n                    return ExpectedAssertionError(str(e))\n                else:\n                    return OnnxRuntimeAssertionError(str(e))\n        else:\n            try:\n                assert_array_almost_equal(expected, output, **kwargs)\n            except Exception as e:\n                expected_ = expected.ravel()\n                output_ = output.ravel()\n                if len(expected_) == len(output_):\n                    diff = numpy.abs(expected_ - output_).max()\n                elif Mism:\n                    return ExpectedAssertionError(\"dimension mismatch={0}, {1}\\n{2}\".format(expected.shape, output.shape, e))\n                else:\n                    return OnnxRuntimeAssertionError(\"dimension mismatch={0}, {1}\\n{2}\".format(expected.shape, output.shape, e))\n                if Disc:\n                    # Bug to be fixed later.\n                    return ExpectedAssertionError(\"max diff(expected, output)={0}\\n{1}\".format(diff, e))\n                else:\n                    return OnnxRuntimeAssertionError(\"max diff(expected, output)={0}\\n{1}\".format(diff, e))\n    else:\n        return OnnxRuntimeAssertionError(\"Unexpected types {0} != {1}\".format(type(expected), type(output)))\n    return None", "response": "Compares expected and output values and returns None if no error."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_sparkml_string_indexer_output_shapes(operator):\n    '''\n    This function just copy the input shape to the output because label encoder only alters input features' values, not\n    their shape.\n    '''\n    check_input_and_output_numbers(operator, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[Int64TensorType, StringTensorType])\n\n    input_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    operator.outputs[0].type = Int64TensorType(input_shape)", "response": "This function just copies the input shape to the output because label encoder only alters input features values not\n    their shape."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a scikit - learn model into an ONNX model.", "response": "def convert(model, name=None, initial_types=None, doc_string='', target_opset=None,\n            targeted_onnx=onnx.__version__, custom_conversion_functions=None, custom_shape_calculators=None):\n    \"\"\"\n    :param model: a libsvm model\n    :param initial_types: a python list. Each element is a tuple of a variable name and a type defined in data_types.py\n    :param name: The name of the graph (type: GraphProto) in the produced ONNX model (type: ModelProto)\n    :param doc_string: A string attached onto the produced ONNX model\n    :param target_opset: number, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3.\n    :param targeted_onnx: A string (for example, '1.1.2' and '1.2') used to specify the targeted ONNX version of the\n    produced model. If ONNXMLTools cannot find a compatible ONNX python package, an error may be thrown.\n    :param custom_conversion_functions: a dictionary for specifying the user customized conversion function\n    :param custom_shape_calculators: a dictionary for specifying the user customized shape calculator\n    :return: An ONNX model (type: ModelProto) which is equivalent to the input scikit-learn model    \n    \"\"\"\n    if initial_types is None:\n        raise ValueError('Initial types are required. See usage of convert(...) in \\\n                         onnxmltools.convert.libsvm.convert for details')\n\n    if name is None:\n        name = str(uuid4().hex)\n\n    # Parse scikit-learn model as our internal data structure (i.e., Topology)\n    topology = parse_libsvm(model, initial_types, custom_conversion_functions, \n                            custom_shape_calculators)\n\n    # Infer variable shapes\n    topology.compile()\n\n    # Convert our Topology object into ONNX. The outcome is an ONNX model.\n    onnx_model = convert_topology(topology, name, doc_string, target_opset, targeted_onnx)\n\n    return onnx_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compare_runtime(test, decimal=5, options=None, verbose=False, context=None):\n    if context is None:\n        context = {}\n    load = load_data_and_model(test, **context)\n\n    onx = test['onnx']\n    if options is None:\n        if isinstance(onx, str):\n            options = extract_options(onx)\n        else:\n            options = {}\n    elif options is None:\n        options = {}\n    elif not isinstance(options, dict):\n        raise TypeError(\"options must be a dictionary.\")\n\n    try:\n        import onnxruntime\n    except ImportError as e:\n        warnings.warn(\"Unable to import onnxruntime.\")\n        return\n\n    try:\n        sess = onnxruntime.InferenceSession(onx)\n    except ExpectedAssertionError as expe:\n        raise expe\n    except Exception as e:\n        if \"CannotLoad\" in options:\n            raise ExpectedAssertionError(\"Unable to load onnx '{0}' due to\\n{1}\".format(onx, e))\n        else:\n            if verbose:\n                import onnx\n                model = onnx.load(onx)\n                smodel = \"\\nJSON ONNX\\n\" + str(model)\n            else:\n                smodel = \"\"\n            raise OnnxRuntimeAssertionError(\"Unable to load onnx '{0}'\\nONNX\\n{1}\".format(onx, smodel))\n\n    input = load[\"data\"]\n    if isinstance(input, dict):\n        inputs = input\n    elif isinstance(input, (list, numpy.ndarray)):\n        inp = sess.get_inputs()\n        if len(inp) == len(input):\n            inputs = {i.name: v for i, v in zip(inp, input)}\n        elif len(inp) == 1:\n            inputs = {inp[0].name: input}\n        elif isinstance(input, numpy.ndarray):\n            shape = sum(i.shape[1] if len(i.shape) == 2 else i.shape[0] for i in inp)\n            if shape == input.shape[1]:\n                inputs = {n.name: input[:, i] for i, n in enumerate(inp)}\n            else:\n                raise OnnxRuntimeAssertionError(\"Wrong number of inputs onnx {0} != original shape {1}, onnx='{2}'\".format(len(inp), input.shape, onnx))\n        elif isinstance(input, list):\n            try:\n                array_input = numpy.array(input)\n            except Exception as e:\n                raise OnnxRuntimeAssertionError(\"Wrong number of inputs onnx {0} != original {1}, onnx='{2}'\".format(len(inp), len(input), onnx))\n            shape = sum(i.shape[1] for i in inp)\n            if shape == array_input.shape[1]:\n                inputs = {n.name: _create_column([row[i] for row in input], n.type) for i, n in enumerate(inp)}\n            else:\n                raise OnnxRuntimeAssertionError(\"Wrong number of inputs onnx {0} != original shape {1}, onnx='{2}'*\".format(len(inp), array_input.shape, onnx))\n        else:\n            raise OnnxRuntimeAssertionError(\"Wrong number of inputs onnx {0} != original {1}, onnx='{2}'\".format(len(inp), len(input), onnx))\n    else:\n        raise OnnxRuntimeAssertionError(\"Dict or list is expected, not {0}\".format(type(input)))\n\n    for k in inputs:\n        if isinstance(inputs[k], list):\n            inputs[k] = numpy.array(inputs[k])\n\n    OneOff = options.pop('OneOff', False)\n    if OneOff:\n        if len(inputs) == 1:\n            name, values = list(inputs.items())[0]\n            res = []\n            for input in values:\n                try:\n                    one = sess.run(None, {name: input})\n                except ExpectedAssertionError as expe:\n                    raise expe\n                except Exception as e:\n                    raise OnnxRuntimeAssertionError(\"Unable to run onnx '{0}' due to {1}\".format(onnx, e))\n                res.append(one)\n            output = _post_process_output(res)\n        else:\n            def to_array(vv):\n                if isinstance(vv, (numpy.ndarray, numpy.int64, numpy.float32)):\n                    return numpy.array([vv])\n                else:\n                    return numpy.array([vv], dtype=numpy.float32)\n            t = list(inputs.items())[0]\n            res = []\n            for i in range(0, len(t[1])):\n                iii = {k: to_array(v[i]) for k, v in inputs.items()}\n                try:\n                    one = sess.run(None, iii)\n                except ExpectedAssertionError as expe:\n                    raise expe\n                except Exception as e:\n                    raise OnnxRuntimeAssertionError(\"Unable to run onnx '{0}' due to {1}\".format(onx, e))\n                res.append(one)\n            output = _post_process_output(res)\n    else:\n        try:\n            output = sess.run(None, inputs)\n        except ExpectedAssertionError as expe:\n            raise expe\n        except RuntimeError as e:\n            if \"-Fail\" in onx:\n                raise ExpectedAssertionError(\"onnxruntime cannot compute the prediction for '{0}'\".format(onx))\n            else:\n                raise OnnxRuntimeAssertionError(\"onnxruntime cannot compute the prediction for '{0}' due to {1}\".format(onx, e))\n        except Exception as e:\n            raise OnnxRuntimeAssertionError(\"Unable to run onnx '{0}' due to {1}\".format(onnx, e))\n\n    output0 = output.copy()\n\n    try:\n        _compare_expected(load[\"expected\"], output, sess, onx, decimal=decimal, **options)\n    except ExpectedAssertionError as expe:\n        raise expe\n    except Exception as e:\n        if verbose:\n            import onnx\n            model = onnx.load(onx)\n            smodel = \"\\nJSON ONNX\\n\" + str(model)\n        else:\n            smodel = \"\"\n        raise OnnxRuntimeAssertionError(\"Model '{0}' has discrepencies.\\n{1}: {2}{3}\".format(onx, type(e), e, smodel))\n\n    return output0", "response": "This function compares the expected output with the ONNX model produced with module ONNX runtime."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _post_process_output(res):\n    if isinstance(res, list):\n        if len(res) == 0:\n            return res\n        elif len(res) == 1:\n            return _post_process_output(res[0])\n        elif isinstance(res[0], numpy.ndarray):\n            return numpy.array(res)\n        elif isinstance(res[0], dict):\n            import pandas\n            return pandas.DataFrame(res).values\n        else:\n            ls = [len(r) for r in res]\n            mi = min(ls)\n            if mi != max(ls):\n                raise NotImplementedError(\"Unable to postprocess various number of outputs in [{0}, {1}]\".format(min(ls), max(ls)))\n            if mi > 1:\n                output = []\n                for i in range(mi):\n                    output.append(_post_process_output([r[i] for r in res]))\n                return output\n            elif isinstance(res[0], list):\n                # list of lists\n                if isinstance(res[0][0], list):\n                    return numpy.array(res)\n                elif len(res[0]) == 1 and isinstance(res[0][0], dict):\n                    return _post_process_output([r[0] for r in res])\n                elif len(res) == 1:\n                    return res\n                else:\n                    if len(res[0]) != 1:\n                        raise NotImplementedError(\"Not conversion implemented for {0}\".format(res))\n                    st = [r[0] for r in res]\n                    return numpy.vstack(st)\n            else:\n                return res\n    else:\n        return res", "response": "Applies post processingings before running the comparison\n    such as changing type from list to arrays."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_column(values, dtype):\n    \"Creates a column from values with dtype\"\n    if str(dtype) == \"tensor(int64)\":\n        return numpy.array(values, dtype=numpy.int64)\n    elif str(dtype) == \"tensor(float)\":\n        return numpy.array(values, dtype=numpy.float32)\n    else:\n        raise OnnxRuntimeAssertionError(\"Unable to create one column from dtype '{0}'\".format(dtype))", "response": "Creates a column from values with dtype"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compare_expected(expected, output, sess, onnx, decimal=5, onnx_shape=None, **kwargs):\n    tested = 0\n    if isinstance(expected, list):\n        if isinstance(output, list):\n            onnx_shapes = [_.shape for _ in sess.get_outputs()]\n            if 'Out0' in kwargs:\n                expected = expected[:1]\n                output = output[:1]\n                del kwargs['Out0']\n            if 'Reshape' in kwargs:\n                del kwargs['Reshape']\n                output = numpy.hstack(output).ravel()\n                output = output.reshape((len(expected),\n                                         len(output.ravel()) // len(expected)))\n            if len(expected) != len(output):\n                raise OnnxRuntimeAssertionError(\"Unexpected number of outputs '{0}', expected={1}, got={2}\".format(onnx, len(expected), len(output)))\n            for exp, out, osh in zip(expected, output, onnx_shapes):\n                _compare_expected(exp, out, sess, onnx, decimal=decimal, onnx_shape=osh, **kwargs)\n                tested += 1\n        else:\n            raise OnnxRuntimeAssertionError(\"Type mismatch for '{0}', output type is {1}\".format(onnx, type(output)))\n    elif isinstance(expected, dict):\n        if not isinstance(output, dict):\n            raise OnnxRuntimeAssertionError(\"Type mismatch for '{0}'\".format(onnx))\n        for k, v in output.items():\n            if k not in expected:\n                continue\n            msg = compare_outputs(expected[k], v, decimal=decimal, **kwargs)\n            if msg:\n                raise OnnxRuntimeAssertionError(\"Unexpected output '{0}' in model '{1}'\\n{2}\".format(k, onnx, msg))\n            tested += 1\n    elif isinstance(expected, numpy.ndarray):\n        if isinstance(output, list):\n            if expected.shape[0] == len(output) and isinstance(output[0], dict):\n                import pandas\n                output = pandas.DataFrame(output)\n                output = output[list(sorted(output.columns))]\n                output = output.values\n        if isinstance(output, (dict, list)):\n            if len(output) != 1:\n                ex = str(output)\n                if len(ex) > 70:\n                    ex = ex[:70] + \"...\"\n                raise OnnxRuntimeAssertionError(\"More than one output when 1 is expected for onnx '{0}'\\n{1}\".format(onnx, ex))\n            output = output[-1]\n        if not isinstance(output, numpy.ndarray):\n            raise OnnxRuntimeAssertionError(\"output must be an array for onnx '{0}' not {1}\".format(onnx, type(output)))\n        if onnx_shape is not None:\n            if len(onnx_shape) == 2:\n                cols = onnx_shape[1]\n                ecols = output.shape[1] if len(output.shape) == 2 else 1\n                if cols != ecols:\n                    raise OnnxRuntimeAssertionError(\"Unexpected onnx shape {0} != {1} for onnx '{2}'\".format(\n                                onnx_shape, output.shape, onnx))\n        msg = compare_outputs(expected, output, decimal=decimal, **kwargs)\n        if isinstance(msg, ExpectedAssertionError):\n            raise msg\n        if msg:\n            raise OnnxRuntimeAssertionError(\"Unexpected output in model '{0}'\\n{1}\".format(onnx, msg))\n        tested += 1\n    else:\n        from scipy.sparse.csr import csr_matrix\n        if isinstance(expected, csr_matrix):\n            # DictVectorizer\n            one_array = numpy.array(output)\n            msg = compare_outputs(expected.todense(), one_array, decimal=decimal, **kwargs)\n            if msg:\n                raise OnnxRuntimeAssertionError(\"Unexpected output in model '{0}'\\n{1}\".format(onnx, msg))\n            tested += 1\n        else:\n            raise OnnxRuntimeAssertionError(\"Unexpected type for expected output ({1}) and onnx '{0}'\".format(onnx, type(expected)))\n    if tested ==0:\n        raise OnnxRuntimeAssertionError(\"No test for onnx '{0}'\".format(onnx))", "response": "Compare the expected output against the runtime outputs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_sparkml_one_hot_encoder_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C']\n        2. [N, 'None'] ---> [N, 'None']\n    '''\n    op = operator.raw_operator\n\n    # encoded_slot_sizes[i] is the number of output coordinates associated with the ith categorical feature.\n    encoded_slot_sizes = op.categorySizes\n\n    N = operator.inputs[0].type.shape[0]\n    # Calculate the output feature length by replacing the count of categorical\n    # features with their encoded widths\n    if operator.inputs[0].type.shape[1] != 'None':\n        C = operator.inputs[0].type.shape[1] - 1 + sum(encoded_slot_sizes)\n    else:\n        C = 'None'\n\n    operator.outputs[0].type = FloatTensorType([N, C])", "response": "This function calculates the output shapes of one - hot encoder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the output shapes of the split operator.", "response": "def calculate_split_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N', C]\n        2. [N, C, H, W] ---> [N', C, H, W]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=[1, None])\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n\n    divided = output_shape[1] / operator.raw_operator.split.nOutputs\n    if divided != int(divided):\n        raise RuntimeError('Variable dimension along C-axis must be divisible by partition number')\n\n    output_shape[1] = int(divided)\n\n    for i in range(len(operator.outputs)):\n        operator.outputs[i].type.shape = copy.deepcopy(output_shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nallows input/output patterns are 1. [N, C] ---> [N, C'] The number C' is the length of prediction vector. It can be a scalar (C'=1) or a vector (C'>1)", "response": "def calculate_traditional_regressor_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C']\n\n    The number C' is the length of prediction vector. It can be a scalar (C'=1) or a vector (C'>1)\n    '''\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType, Int64TensorType, FloatType, Int64Type])\n\n    if any(len(variable.type.shape) != 2 for variable in operator.inputs):\n        raise RuntimeError('Input(s) must be 2-D tensor(s)')\n\n    model_type = operator.raw_operator.WhichOneof('Type')\n    if model_type == 'glmRegressor':\n        glm = operator.raw_operator.glmRegressor\n        C = len(glm.weights)\n    elif model_type == 'treeEnsembleRegressor':\n        tree = operator.raw_operator.treeEnsembleRegressor.treeEnsemble\n        C = len(tree.basePredictionValue)\n    elif model_type == 'supportVectorRegressor':\n        C = 1\n    else:\n        raise ValueError('Model should be one of linear model, tree-based model, and support vector machine')\n\n    N = operator.inputs[0].type.shape[0]\n    operator.outputs[0].type = FloatTensorType([N, C], doc_string=operator.outputs[0].type.doc_string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the output shapes of the GRU operator.", "response": "def calculate_gru_output_shapes(operator):\n    '''\n    See GRU's conversion function for its output shapes.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, 2], output_count_range=[1, 2])\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input_shape = operator.inputs[0].type.shape\n\n    if len(input_shape) not in [2, 4]:\n        raise RuntimeError('Input must be a [N, C]- or [N, C, 1, 1]-tensor')\n\n    if operator.type == 'gru':\n        params = operator.raw_operator.gru\n    elif operator.type == 'simpleRecurrent':\n        params = operator.raw_operator.simpleRecurrent\n    else:\n        raise RuntimeError('Only GRU and SimpleRNN are supported')\n\n    # The following line is more accurate but it may break some tests\n    # output_shape = ['None', params.outputVectorSize] if params.params.sequenceOutput else [2, params.outputVectorSize]\n    output_shape = [input_shape[0] if params.sequenceOutput else 'None', params.outputVectorSize]  # 'None' should be 1\n    state_shape = [1, params.outputVectorSize]\n\n    # TODO: Changing input shapes of an operator is dangerous, this should be move to Topology's _fix_shapes function\n    if len(operator.inputs) > 1:\n        Y_h_in = operator.inputs[1]  # The initial hidden state of a single sequence\n        Y_h_in.type.shape = state_shape\n\n    operator.outputs[0].type.shape = output_shape\n    if len(operator.outputs) > 1:\n        operator.outputs[1].type.shape = state_shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the output shapes of the reduce operation.", "response": "def calculate_reduce_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, 1, H, W]\n        2. [N, C, H, W] ---> [N, C, 1, W]\n        3. [N, C, H, W] ---> [N, C, H, 1]\n        4. [N, C, H, W] ---> [N, C, 1, 1]\n        5. [N, C, H, W] ---> [N, 1, 1, 1]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    params = operator.raw_operator.reduce\n\n    from coremltools.proto.NeuralNetwork_pb2 import ReduceLayerParams as Params\n    # Adjust C-axis\n    if params.axis in [Params.CHW, Params.C]:\n        output_shape[1] = 1\n    # Adjust H-axis\n    if params.axis in [Params.CHW, Params.HW, Params.H]:\n        output_shape[2] = 1\n    # Adjust W-axis\n    if params.axis in [Params.CHW, Params.HW, Params.W]:\n        output_shape[3] = 1\n\n    operator.outputs[0].type.shape = output_shape"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_permute_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N', C', H', W']\n\n    Note that here [N', C', H', W'] means all possible permutations of [N, C, H, W]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType, Int64TensorType, StringTensorType],\n                                 good_output_types=[FloatTensorType, Int64TensorType, StringTensorType])\n\n    input = operator.inputs[0]\n    output = operator.outputs[0]\n\n    axes = [int(i) for i in operator.raw_operator.permute.axis]\n    input_shape = copy.deepcopy(input.type.shape)\n    output.type.shape = [input_shape[a] for a in axes]", "response": "This function calculates the output shapes of the permutation operator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nallowing input/output patterns are 1. [N, C, H, W] ---> [N, C', H', W'] Note that C*H*W should equal to C'*H'*W'.", "response": "def calculate_reshape_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C', H', W']\n\n    Note that C*H*W should equal to C'*H'*W'.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    params = operator.raw_operator.reshape\n\n    output_shape = list(int(i) for i in params.targetShape)\n\n    if len(output_shape) == 3:\n        output_shape = [operator.inputs[0].type.shape[0]] + output_shape\n\n    operator.outputs[0].type.shape = output_shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the output shapes of the padding term in the cluster.", "response": "def calculate_padding_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C, H', W']\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n\n    params = operator.raw_operator.padding\n    if len(params.paddingAmounts.borderAmounts) > 0:\n        output_shape[2] += params.paddingAmounts.borderAmounts[0].startEdgeSize\n        output_shape[2] += params.paddingAmounts.borderAmounts[0].endEdgeSize\n        output_shape[3] += params.paddingAmounts.borderAmounts[1].startEdgeSize\n        output_shape[3] += params.paddingAmounts.borderAmounts[1].endEdgeSize\n\n    operator.outputs[0].type.shape = output_shape"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculte_tensor_to_label_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, 1]\n\n    Note that N must be 1 currently because TensorToProbability doesn't support batch size larger than 1.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    N = operator.inputs[0].type.shape[0]\n    if operator.target_opset < 7:\n        output_shape = [1, 1]\n    else:\n        output_shape = [N, 1]\n\n    if type(operator.outputs[0].type) in [Int64Type, Int64TensorType]:\n        operator.outputs[0].type = Int64TensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)\n    elif type(operator.outputs[0].type) in [StringType, StringTensorType]:\n        operator.outputs[0].type = StringTensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)\n    else:\n        raise ValueError('Unsupported label type')", "response": "This function converts a TensorToProbability operator into a sequence of output shapes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimize_onnx(onnx_nodes, nchw_inputs=None, inputs=None, outputs=None):\n    node_list = LinkedNode.build_from_onnx(onnx_nodes,\n                                           nchw_inputs if nchw_inputs else [],\n                                           [] if inputs is None else [i_.name for i_ in inputs],\n                                           [] if outputs is None else [o_.name for o_ in outputs])\n    solution = _find_an_optimization(node_list)\n    while solution:\n        node_list = _apply_optimization(solution, node_list)\n        solution = _find_an_optimization(node_list)\n\n    return _build_onnx_model(node_list)", "response": "Optimize onnx model by several approaches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef optimize_onnx_model(origin_model, nchw_inputs=None):\n    graph = origin_model.graph\n    nodelist = list(graph.node)\n    del graph.node[:]\n\n    all_nodes = optimize_onnx(nodelist,\n                              inputs=graph.input,\n                              outputs=graph.output)\n    nodes = [n_ for n_ in all_nodes if not isinstance(n_, tuple)]\n    graph.node.extend(nodes)\n\n    alter_tensors = {n_[1]: n_[0] for n_ in all_nodes if isinstance(n_, tuple)}\n    update_tensor = lambda x: \\\n        helper.make_tensor(x.name, x.data_type, (x.dims[0], 1, 1),\n                           onnx.numpy_helper.to_array(x).flatten())\n    new_initializer = [init_ if init_.name not in alter_tensors else update_tensor(init_)\n                       for init_ in graph.initializer]\n    del graph.initializer[:]\n    graph.initializer.extend(new_initializer)\n\n    update_value_info = lambda x: \\\n        helper.make_tensor_value_info(x.name, x.type.tensor_type.elem_type,\n                                      (x.type.tensor_type.shape.dim[0].dim_value, 1, 1))\n    new_input = [in_ if in_.name not in alter_tensors else update_value_info(in_)\n                 for in_ in graph.input]\n    del graph.input[:]\n    graph.input.extend(new_input)\n    return origin_model", "response": "optimizes the graph of the origin model"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef in_single_path(self):\n        return len(self.successor) == 1 and not self.successor[0].in_or_out and \\\n               len(self.precedence) == 1 and len(self.precedence[0].successor) <= 1", "response": "Test if a node is linking to a single fan in or out node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests if a node is linking to a single fan in or out node and if it is not linking to any fan in or out node.", "response": "def in_single_path_and_inner(self):\n        \"\"\"\n        Test if a node is not linking to any fan in or out node.\n        \"\"\"\n        return len(self.successor) == 1 and self.successor[0] is not None and not self.successor[0].in_or_out and \\\n               len(self.precedence) == 1 and self.precedence[0] is not None and not self.successor[0].in_or_out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef in_simo_and_inner(self):\n        return len(self.successor) > 1 and self.successor[0] is not None and not self.successor[0].in_or_out and \\\n               len(self.precedence) == 1 and self.precedence[0] is not None and not self.successor[0].in_or_out", "response": "Test if a node is simo and inner node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in_miso_and_inner(self):\n        return len(self.successor) == 1 and self.successor[0] is not None and not self.successor[0].in_or_out and \\\n               len(self.precedence) > 1 and self.precedence[0] is not None and not self.successor[0].in_or_out", "response": "Test if a node is miso and inner node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the node which has n - input and 1 - output node.", "response": "def delete_node_nto1(node_list, begin, node, end):  # type: ([],LinkedNode, LinkedNode, LinkedNode)->[]\n        \"\"\"\n        delete the node which has n-input and 1-output\n        \"\"\"\n        if begin is None:\n            assert node is not None\n            begin = node.precedence\n        elif not isinstance(begin, list):\n            begin = [begin]\n\n        if end.in_or_out:\n            # if the end is output node, the output name will be kept to avoid the model output name updating.\n            for nb_ in begin:\n                nb_.out_redirect(node.single_input, node.single_output)\n        else:\n            for nb_ in begin:\n                target_var_name = node.single_input\n                assert target_var_name in nb_.output.values()  # since the output info never be updated, except the final.\n                end.in_redirect(node.single_output, target_var_name)\n\n        for nb_ in begin:\n            nb_.successor = [end if v_ == node else v_ for v_ in nb_.successor]\n        end.precedence = [v_ for v_ in end.precedence if v_ != node] + node.precedence\n\n        node_list.remove(node)\n        return node_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the node which has 1 - input and n - output and n - output.", "response": "def delete_node_1ton(node_list, begin, node, end):  # type: ([],LinkedNode, LinkedNode, LinkedNode)->[]\n        \"\"\"\n        delete the node which has 1-input and n-output\n        \"\"\"\n        if end is None:\n            assert end is not None\n            end = node.successor\n        elif not isinstance(end, list):\n            end = [end]\n\n        if any(e_.in_or_out for e_ in end):\n            # if the end is output node, the output name will be kept to avoid the model output name updating.\n            begin.out_redirect(node.single_input, node.single_output)\n        else:\n            for ne_ in end:\n                target_var_name = node.single_input\n                # since the output info never be updated, except the final.\n                assert target_var_name in begin.output.values()\n                ne_.in_redirect(node.single_output, target_var_name)\n\n        begin.successor = [v_ for v_ in begin.successor if v_ != node] + node.successor\n        for ne_ in end:\n            ne_.precedence = [begin if v_ == node else v_ for v_ in ne_.precedence]\n\n        node_list.remove(node)\n        return node_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_topology(topology, model_name, doc_string, target_opset, targeted_onnx, channel_first_inputs=None):\n    '''\n    This function is used to convert our Topology object defined in _parser.py into a ONNX model (type: ModelProto).\n    :param topology: The Topology object we are going to convert\n    :param model_name: GraphProto's name. Let \"model\" denote the returned model. The string \"model_name\" would be\n    assigned to \"model.graph.name.\"\n    :param doc_string: A string attached to the produced model\n    :param target_opset: number, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3.\n    :param targeted_onnx[deprecated]: A string, which specifies the targeted ONNX version of the produced model. Possible values\n    include '1.1.2', '1.2', and so on.\n    :return: a ONNX ModelProto\n    '''\n    if targeted_onnx is not None and StrictVersion(targeted_onnx) != StrictVersion(onnx.__version__):\n        warnings.warn(\n            'targeted_onnx is deprecated, please specify target_opset for the target model.\\n' +\n            '*** ONNX version conflict found. The installed version is %s while the targeted version is %s' % (\n                onnx.__version__, targeted_onnx))\n\n    opset_from_onnx_version = onnx.defs.onnx_opset_version()\n    if target_opset is None:\n        target_opset = opset_from_onnx_version\n    elif target_opset > opset_from_onnx_version:\n        raise RuntimeError(\"target_opset %d is higher than the number of the installed onnx package.\")\n\n    topology._initialize_graph_status_for_traversing()\n\n    container = ModelComponentContainer(target_opset)\n\n    # Put roots and leaves as ONNX's model into buffers. They will be added into ModelComponentContainer later.\n    tensor_inputs = {}\n    other_inputs = {}\n    tensor_outputs = {}\n    other_outputs = {}\n    for scope in topology.scopes:\n        for variable in scope.variables.values():\n            if variable.is_root:\n                if isinstance(variable.type, (TensorType, Int64Type, FloatType, StringType)):\n                    tensor_inputs[variable.raw_name] = variable\n                else:\n                    other_inputs[variable.raw_name] = variable\n            if variable.is_leaf:\n                if isinstance(variable.type, (TensorType, Int64Type, FloatType, StringType)):\n                    tensor_outputs[variable.raw_name] = variable\n                else:\n                    other_outputs[variable.raw_name] = variable\n\n    # Add roots the graph according to their order in the original model\n    invalid_name = []\n    nhwc_inputs = []\n    if channel_first_inputs is None:\n        channel_first_inputs = []\n    for name in topology.raw_model.input_names:\n        # Check input naming convention\n        input_name = name.replace('_', '').replace(\":\", \"\").replace(\"/\", \"\")\n        if input_name and (input_name[0].isdigit() or (not input_name.isalnum())):\n            invalid_name.append(name)\n        if name in tensor_inputs:\n            onnx_input = tensor_inputs[name]  # type: Variable\n            if name in channel_first_inputs or \\\n                    (name.endswith(':0') and name[:-2] in channel_first_inputs):\n                nhwc_inputs.append(onnx_input.full_name)\n                s = onnx_input.type.shape\n                onnx_input.type.shape = [s[0], s[3], s[1], s[2]]\n            container.add_input(onnx_input)\n\n    if invalid_name:\n        warnings.warn('Some input names are not compliant with ONNX naming convention: %s' % invalid_name)\n    for name in topology.raw_model.input_names:\n        if name in other_inputs:\n            container.add_input(other_inputs[name])\n\n    # Add leaves the graph according to their order in the original model\n    invalid_name = []\n    for name in topology.raw_model.output_names:\n        # Check output naming convention\n        output_name = name.replace('_', '').replace(\":\", \"\").replace(\"/\", \"\")\n        if output_name and (output_name[0].isdigit() or (not output_name.isalnum())):\n            invalid_name.append(name)\n        if name in tensor_outputs:\n            container.add_output(tensor_outputs[name])\n    if invalid_name:\n        warnings.warn('Some output names are not compliant with ONNX naming convention: %s' % invalid_name)\n    for name in topology.raw_model.output_names:\n        if name in other_outputs:\n            container.add_output(other_outputs[name])\n\n    # Traverse the graph from roots to leaves\n    for operator in topology.topological_operator_iterator():\n        scope = next(scope for scope in topology.scopes if scope.name == operator.scope)\n        if operator.type in topology.custom_conversion_functions:\n            topology.custom_conversion_functions[operator.type](scope, operator, container)\n        else:\n            # Convert the selected operator into some ONNX objects and save them into the container\n            registration.get_converter(operator.type)(scope, operator, container)\n\n    # When calling ModelComponentContainer's add_initializer(...), nothing is added into the input list.\n    # However, for ONNX target opset < 9, initializers should also be model's (GraphProto) inputs.\n    # Thus, we create ValueInfoProto objects from initializers (type: TensorProto) directly and\n    # then add them into model's input list.\n    extra_inputs = []  # ValueInfoProto list of the initializers\n    for tensor in container.initializers:\n        # Sometimes (especially when creating optional input values such as RNN's initial hidden state), an initializer\n        # is also one of the original model's input, so it has been added into the container's input list. If this is\n        # the case, we need to skip one iteration to avoid duplicated inputs.\n        if tensor.name in [value_info.name for value_info in container.inputs]:\n            continue\n\n        # Initializers are always tensors so we can just call make_tensor_value_info(...)\n        value_info = helper.make_tensor_value_info(tensor.name, tensor.data_type, tensor.dims)\n        extra_inputs.append(value_info)\n\n    # enable the ONNX optimizations\n    nodes = optimize_onnx(container.nodes, nhwc_inputs, container.inputs + extra_inputs, container.outputs)\n\n    # Create a graph from its main components\n    if container.target_opset < 9:\n        # Before ONNX opset 9, initializers need to be passed in with inputs\n        graph = helper.make_graph(nodes, model_name, container.inputs + extra_inputs,\n                                  container.outputs, container.initializers)\n    else:\n        # In ONNX opset 9 and above, initializers are included as operator\n        # inputs, and therefore do not need to be passed as extra_inputs\n        graph = helper.make_graph(nodes, model_name, container.inputs,\n                                  container.outputs, container.initializers)\n\n    # Add extra information related to the graph\n    graph.value_info.extend(container.value_info)\n\n    # Create model\n    onnx_model = helper.make_model(graph)\n\n    # Merge operator sets for the same domain, the largest version number would be kept\n    purified_operator_set = dict()\n    for op_domain, op_version in container.node_domain_version_pair_sets:\n        if op_domain not in purified_operator_set:\n            purified_operator_set[op_domain] = op_version\n        else:\n            purified_operator_set[op_domain] = max(purified_operator_set[op_domain], op_version)\n\n    # Fill operator sets\n    i = 0\n    for op_domain, op_version in purified_operator_set.items():\n        if i == 0 and len(onnx_model.opset_import) == 1:\n            # Overwrite the default operator set created by helper.make_model(...)\n            op_set = onnx_model.opset_import[0]\n        else:\n            # Just create one ONNX element in opset_import\n            op_set = onnx_model.opset_import.add()\n        op_set.domain = op_domain\n        op_set.version = op_version\n        i += 1\n        if container.target_opset < op_version:\n            raise RuntimeError(('The specified opset %d is too low to convert this model, ' +\n                               'which requires at least opset %d.') % (container.target_opset, op_version))\n        elif container.target_opset > op_version:\n            getLogger('onnxmltools').warning('The maximum opset needed by this model is only %d.' % op_version)\n\n    # Add extra information\n    add_metadata_props(onnx_model, topology.metadata_props, target_opset)\n    onnx_model.ir_version = onnx_proto.IR_VERSION\n    onnx_model.producer_name = utils.get_producer()\n    onnx_model.producer_version = utils.get_producer_version()\n    onnx_model.domain = utils.get_domain()\n    onnx_model.model_version = utils.get_model_version()\n    onnx_model.doc_string = doc_string\n\n    return onnx_model", "response": "This function converts a Topology object to a ONNX ModelProto."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the variable name of the given seed or create one if it is not present.", "response": "def get_onnx_variable_name(self, seed):\n        '''\n        Retrieve the variable ID of the given seed or create one if it is the first time of seeing this seed\n        '''\n        if seed in self.variable_name_mapping:\n            return self.variable_name_mapping[seed][-1]\n        else:\n            return self.get_unique_variable_name(seed)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind sink variables in this scope.", "response": "def find_sink_variables(self):\n        '''\n        Find sink variables in this scope\n        '''\n        # First we assume all variables are sinks\n        is_sink = {name: True for name in self.variables.keys()}\n        # Then, we remove those variables which are inputs of some operators\n        for operator in self.operators.values():\n            for variable in operator.inputs:\n                is_sink[variable.onnx_name] = False\n        return [variable for name, variable in self.variables.items() if is_sink[name]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the operator whose onnx_name is the input onnx_name.", "response": "def delete_local_operator(self, onnx_name):\n        '''\n        Remove the operator whose onnx_name is the input onnx_name\n        '''\n        if onnx_name not in self.onnx_operator_names or onnx_name not in self.operators:\n            raise RuntimeError('The operator to be removed not found')\n        self.onnx_operator_names.discard(onnx_name)\n        del self.operators[onnx_name]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_local_variable(self, onnx_name):\n        '''\n        Remove the variable whose onnx_name is the input onnx_name\n        '''\n        if onnx_name not in self.onnx_variable_names or onnx_name not in self.variables:\n            raise RuntimeError('The variable to be removed not found')\n        self.onnx_variable_names.discard(onnx_name)\n        raw_name = self.variables[onnx_name].raw_name\n        self.variable_name_mapping[raw_name].remove(onnx_name)\n        del self.variables[onnx_name]", "response": "Removes the local variable whose onnx_name is the input onnx_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_unique_name(seed, existing_names):\n        '''\n        Produce an unique string based on the seed\n        :param seed: a string\n        :param existing_names: a set containing strings which cannot be produced\n        :return: a string similar to the seed\n        '''\n        if seed == '':\n            raise ValueError('Name seed must be an non-empty string')\n\n        # Make the seed meet C-style naming convention\n        seed = re.sub('[^0-9a-zA-Z]', '_', seed)  # Only alphabets and numbers are allowed\n        if re.match('^[0-9]', seed):  # The first symbol cannot be a number\n            seed = '_' + seed\n\n        # If seed has never been seen, we return it as it is. Otherwise, we will append an number to make it unique.\n        if seed not in existing_names:\n            existing_names.add(seed)\n            return seed\n        else:\n            i = 1\n            while seed + str(i) in existing_names:\n                i += 1\n            new_name = seed + str(i)\n            existing_names.add(new_name)\n            return new_name", "response": "Generates a unique name based on the seed and the set of existing names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding root variables of the whole graph", "response": "def find_root_and_sink_variables(self):\n        '''\n        Find root variables of the whole graph\n        '''\n        # First we assume all variables are roots\n        is_root = {name: True for scope in self.scopes for name in scope.variables.keys()}\n        # Then, we remove those variables which are outputs of some operators\n        for operator in self.unordered_operator_iterator():\n            for variable in operator.outputs:\n                is_root[variable.onnx_name] = False\n        is_sink = {name: True for scope in self.scopes for name in scope.variables.keys()}\n        for operator in self.unordered_operator_iterator():\n            for variable in operator.inputs:\n                is_sink[variable.onnx_name] = False\n        return [variable for scope in self.scopes for name, variable in scope.variables.items()\n                if is_root[name] or is_sink[name]]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef topological_operator_iterator(self):\n        '''\n        This is an iterator of all operators in Topology object. Operators may be produced in a topological order.\n        If you want to simply go though all operators without considering their topological structure, please use\n        another function, unordered_operator_iterator.\n        '''\n        self._initialize_graph_status_for_traversing()\n        priorities = {'tensorToProbabilityMap': 2, 'tensorToLabel': 1}\n        while not all(operator.is_evaluated for scope in self.scopes for operator in scope.operators.values()):\n            is_evaluation_happened = False\n            for operator in sorted(self.unordered_operator_iterator(),\n                                   key=lambda op: priorities[op.type] if op.type in priorities else 0):\n                if all(variable.is_fed for variable in operator.inputs) and not operator.is_evaluated:\n                    # Check if over-writing problem occurs (i.e., multiple operators produce results on one variable).\n                    for variable in operator.outputs:\n                        # Throw an error if this variable has been treated as an output somewhere\n                        if variable.is_fed:\n                            raise RuntimeError('One variable can only be assigned once')\n                        # Mark this variable as filled\n                        variable.is_fed = True\n                    # Make this operator as handled\n                    operator.is_evaluated = True\n                    is_evaluation_happened = True\n                    # Send out an operator\n                    yield operator\n            # After scanning through the whole computational graph, at least one operator should be evaluated. If not,\n            # we need to terminate this procedure to avoid dead lock.\n            if not is_evaluation_happened:\n                break", "response": "This is an iterator of all operators produced in a topological order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrename an ONNX variable in the Topology.", "response": "def rename_variable(self, old_name, new_name):\n        '''\n        Replace the old ONNX variable name with a new ONNX variable name. There are several fields we need to edit.\n            a. Topology\n                1. scopes (the scope where the specified ONNX variable was declared)\n                2. variable_name_set\n            b. Scope\n                1. onnx_variable_names (a mirror of Topology's variable_name_set)\n                2. variable_name_mapping\n                3. variables\n\n        :param old_name: a string\n        :param new_name: a string\n        '''\n        # Search for the first variable that is named as old_name.\n        scope, onnx_name, variable = next((scope, onnx_name, variable) for scope in self.scopes\n                                          for onnx_name, variable in scope.variables.items() if onnx_name == old_name)\n\n        # Rename the variable we just found\n        variable.onnx_name = new_name\n\n        # Because the ONNX name of the targeted variable got changed, the (onnx_name, variable) pair in the associated\n        # scope's variable dictionary should be changed as well. We therefore create a new pair to replace the old pair.\n        scope.variables[new_name] = variable\n        del scope.variables[old_name]\n\n        # One original CoreML name may have several ONNX names recorded. To fix the record affected by renaming, we need\n        # to replace old_name with new_name in the record of the associated CoreML name (variable.raw_name). Note that\n        # derived_names contains all ONNX variable names derived from variable.raw_name.\n        derived_names = scope.variable_name_mapping[variable.raw_name]\n        for i in range(len(derived_names)):\n            # Find old_name in derived_names\n            if old_name != derived_names[i]:\n                continue\n            # Replace the recorded ONNX name with the new name\n            derived_names[i] = new_name\n            # Because ONNX names are unique so name replacement only happens once, we terminate the loop right after one\n            # name replacement.\n            break\n\n        # Finally, new_name takes the place of old_name in the set of all existing variable names\n        scope.onnx_variable_names.remove(old_name)\n        scope.onnx_variable_names.add(new_name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_structure(self):\n        '''\n        This function applies some rules to check if the parsed model is proper. Currently, it only checks if isolated\n        variable and isolated operator exists.\n        '''\n        # Collect all variable names and operator names\n        unused_variables = set()\n        unused_operators = set()\n        for variable in self.unordered_variable_iterator():\n            unused_variables.add(variable.full_name)\n        for operator in self.unordered_operator_iterator():\n            unused_operators.add(operator.full_name)\n\n        for operator in self.unordered_operator_iterator():\n            for variable in operator.inputs:\n                # A variable is used by an operator, so we remove the variable from the unused-variable list.\n                unused_variables.discard(variable.full_name)\n                # A operator has an input, so we remove the operator from the unused-operator list.\n                unused_operators.discard(operator.full_name)\n            for variable in operator.outputs:\n                # A variable is used by an operator, so we remove the variable from the unused-variable list.\n                unused_variables.discard(variable.full_name)\n                # A operator has an output, so we remove the operator from the unused-operator list.\n                unused_operators.discard(operator.full_name)\n\n        if len(unused_variables) > 0:\n            raise RuntimeError('Isolated variables exist: %s' % unused_variables)\n\n        if len(unused_operators) > 0:\n            raise RuntimeError('Isolated operators exist: %s' % unused_operators)", "response": "This function applies some rules to check if the parsed model is proper. Currently it only checks if variable and operator exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the status of all variables and operators for traversing the underline graph.", "response": "def _initialize_graph_status_for_traversing(self):\n        '''\n        Initialize the status of all variables and operators for traversing the underline graph\n        '''\n        # In the beginning, we set is_root and is_leaf true. For is_fed, we have two different behaviors depending on\n        # whether root_names is empty.\n        for variable in self.unordered_variable_iterator():\n            # If root_names is set, we only set those variable to be fed. Otherwise, all roots would be fed.\n            if self.root_names:\n                if variable.onnx_name in self.root_names:\n                    variable.is_fed = True\n                else:\n                    variable.is_fed = False\n            else:\n                variable.is_fed = True\n            variable.is_root = True\n            variable.is_leaf = True\n\n        # Then, we flip some flags by applying some simple rules so that only\n        #   1. all roots get is_root=True and is_fed=True\n        #   2. all leaves get is_leaf=True\n        for operator in self.unordered_operator_iterator():\n            operator.is_evaluated = False  # All operators are not processed in the beginning\n            for variable in operator.outputs:\n                # Output cannot be fed before graph traversing\n                variable.is_fed = False\n                # If the variable is an output of one operator, it must not be a root\n                variable.is_root = False\n            for variable in operator.inputs:\n                # If the variable is an input of one operator, it must not be a leaf\n                variable.is_leaf = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _infer_all_types(self):\n        '''\n        Infer all variables' shapes in the computational graph.\n        '''\n        self._initialize_graph_status_for_traversing()\n\n        # Deliver user-specified types to root variables\n        for raw_name, initial_type in self.initial_types:\n            # Check all variables declared using raw_name in the whole graph\n            for scope in self.scopes:\n                # Skip scopes without having the considered variable name\n                if raw_name not in scope.variable_name_mapping:\n                    continue\n                # Assign initial_type to all variables declared using raw_name\n                for onnx_name in scope.variable_name_mapping[raw_name]:\n                    variable = scope.variables[onnx_name]\n                    if variable.is_root:\n                        # Assign type to the root; existing type produced by parser may be overwritten\n                        variable.type = initial_type\n\n        # Traverse the graph from roots to leaves\n        for operator in self.topological_operator_iterator():\n            if operator.type in self.custom_shape_calculators:\n                self.custom_shape_calculators[operator.type](operator)\n            elif operator.type in self.custom_conversion_functions:\n                pass  # in Keras converter, the shape calculator can be optional.\n            else:\n                operator.infer_types()", "response": "Infer all variables in the computational graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve duplicate variables in the current binary graph.", "response": "def _resolve_duplicates(self):\n        '''\n        Merge variables connected by identity operator to reduce the number of redundant variables\n        '''\n        self._initialize_graph_status_for_traversing()\n\n        # Traverse the graph from roots to leaves\n        for operator in self.topological_operator_iterator():\n            if operator.type != 'identity':\n                continue\n\n            if any(variable.is_root for variable in operator.inputs) and \\\n                    any(variable.is_leaf for variable in operator.outputs):\n                continue\n\n            # Replace the output variable with the input variable everywhere\n            original = operator.inputs[0]\n            duplicate = operator.outputs[0]\n            for another_scope in self.scopes:\n                for another_operator in another_scope.operators.values():\n                    for i in range(len(another_operator.inputs)):\n                        if another_operator.inputs[i].onnx_name != duplicate.onnx_name:\n                            continue\n                        another_operator.inputs[i] = original\n\n            # When original variable's documentation string or denotation is empty but duplicate's is not, we\n            # copy that field to the original variable to avoid information loss.\n            if not original.type.doc_string and duplicate.type.doc_string:\n                original.type.doc_string = duplicate.type.doc_string\n\n            if isinstance(original.type, TensorType) and isinstance(duplicate.type, TensorType):\n                if not original.type.denotation and duplicate.type.denotation:\n                    original.type.denotation = duplicate.type.denotation\n                if not original.type.channel_denotations:\n                    original.type.channel_denotations = duplicate.type.channel_denotations\n                elif duplicate.type.channel_denotations:\n                    # Merge the channel denotations if available in both the original and the duplicate\n                    for i in range(len(original.type.channel_denotations)):\n                        if original.type.channel_denotations[i]:\n                            continue\n                        original.type.channel_denotations[i] = duplicate.type.channel_denotations[i]\n                # Sometime, shapes of duplicates are different. We try to replace the original variable's unknown dimensions\n                # as many as possible because we will get rid of the duplicate.\n                if len(original.type.shape) == len(duplicate.type.shape):\n                    for i in range(len(original.type.shape)):\n                        if original.type.shape[i] != 'None':\n                            continue\n                        original.type.shape[i] = duplicate.type.shape[i]\n\n            # Because we're iterating through the topology, we cannot delete any operator or variable. Otherwise,\n            # the traversing function may be broken. We will delete those abandoned ones later.\n            duplicate.is_abandoned = True\n            operator.is_abandoned = True\n\n        for scope in self.scopes:\n            # Find out who is going to be abandoned\n            abandoned_operator_names = set(onnx_name for onnx_name, operator in scope.operators.items()\n                                           if operator.is_abandoned)\n            abandoned_variable_names = set(onnx_name for onnx_name, variable in scope.variables.items()\n                                           if variable.is_abandoned)\n\n            # Remove abandoned operators\n            for name in abandoned_operator_names:\n                scope.delete_local_operator(name)\n\n            # Remove abandoned variables\n            for name in abandoned_variable_names:\n                scope.delete_local_variable(name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compile(self):\n        '''\n        This function aims at giving every operator enough information so that all operator conversions can happen\n        independently. We also want to check, fix, and simplify the network structure here.\n        '''\n        self._prune()\n        self._resolve_duplicates()\n        self._fix_shapes()\n        self._infer_all_types()\n        self._check_structure()", "response": "This function is used to compile the network structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nallowing input/output patterns are 1. [N, C, H, W] ---> [N', C, H, W]", "response": "def calculate_sequence_repeat_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N', C, H, W]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input_shape = operator.inputs[0].type.shape\n    if len(input_shape) not in [2, 4]:\n        raise RuntimeError('Input shape of CoreML SequenceRepeat must be either 2-D or 4-D but got %s' % input_shape)\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    if output_shape[0] != None:\n        output_shape[0] *= operator.raw_operator.sequenceRepeat.nRepetitions\n\n    operator.outputs[0].type = FloatTensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_array_feature_extractor_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C']\n\n    C' is the number of extracted features.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType, Int64TensorType, StringTensorType])\n\n    N = operator.inputs[0].type.shape[0]\n    extracted_feature_number = len(operator.raw_operator.arrayFeatureExtractor.extractIndex)\n\n    # Save doc_string before over-writing by us\n    doc_string = operator.outputs[0].type.doc_string\n    operator.outputs[0].type = copy.deepcopy(operator.inputs[0].type)\n    operator.outputs[0].type.shape = [N, extracted_feature_number]\n    # Assign correct doc_string to the output\n    operator.outputs[0].type.doc_string = doc_string", "response": "This function calculates the output shapes of the array feature extractor operator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_concat_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N_1, C, H, W], ..., [N_n, C, H, W] ---> [N_1 + ... + N_n, C, H, W]\n        2. [N, C_1, H, W], ..., [N, C_n, H, W] ---> [N, C_1 + ... + C_n, H, W]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, None], output_count_range=[1, 1])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    dims = []\n    for variable in operator.inputs:\n        if variable.type.shape[0] != 'None' and variable.type.shape[0] != output_shape[0]:\n            raise RuntimeError('Only dimensions along C-axis can be different')\n        if variable.type.shape[2] != 'None' and variable.type.shape[2] != output_shape[2]:\n            raise RuntimeError('Only dimensions along C-axis can be different')\n        if variable.type.shape[3] != 'None' and variable.type.shape[3] != output_shape[3]:\n            raise RuntimeError('Only dimensions along C-axis can be different')\n        dims.append(variable.type.shape[1])\n\n    output_shape[1] = 'None' if 'None' in dims else sum(dims)\n    operator.outputs[0].type.shape = output_shape", "response": "This function calculates the output shapes of the concat operator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the output shapes of CoreML Pooling.", "response": "def calculate_pooling_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C, H', W']\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input = operator.inputs[0]\n    input_shape = operator.inputs[0].type.shape\n\n    if len(input.type.shape) != 4:\n        raise RuntimeError('Input must be 4-D float tensor')\n\n    operator.outputs[0].type.shape = [0, 0, 0, 0]\n    output_shape = operator.outputs[0].type.shape\n\n    # Adjust N-axis\n    output_shape[0] = input_shape[0]\n\n    # Adjust C-axis\n    output_shape[1] = input_shape[1]\n\n    params = operator.raw_operator.pooling\n    # Set up default and non-default parameters. Notice that they are only set for H- and W-axes.\n    dilations = [1, 1]  # CoreML Pooling doesn't allow dilation, so we use [1, 1] which is equivalent to no dilation.\n    kernel_shape = [3, 3]\n    if len(params.kernelSize) > 0:\n        kernel_shape = params.kernelSize\n    strides = [1, 1]\n    if len(params.stride) > 0:\n        strides = params.stride\n    pad_mode = params.WhichOneof('PoolingPaddingType')\n    if pad_mode == 'valid' and len(params.valid.paddingAmounts.borderAmounts) > 0:\n        pad_amounts = params.valid.paddingAmounts.borderAmounts\n        pad_heads = [pad_amounts[0].startEdgeSize, pad_amounts[1].startEdgeSize]\n        pad_tails = [pad_amounts[0].endEdgeSize, pad_amounts[1].endEdgeSize]\n    elif pad_mode == 'includeLastPixel' and len(params.includeLastPixel.paddingAmounts) > 0:\n        pad_amounts = params.includeLastPixel.paddingAmounts\n        pad_heads = [pad_amounts[0], pad_amounts[1]]\n        pad_tails = [pad_amounts[0], pad_amounts[1]]\n    else:\n        # For same padding, padding amounts are not used\n        pad_heads = [0, 0]\n        pad_tails = [0, 0]\n\n    # Calculate output shape along H- and W-axes\n    for i in range(2):\n        output_shape[i + 2] = calculate_convolution_and_pooling_1D_output_shape(\n            input_shape[i + 2], kernel_shape[i], dilations[i], strides[i],\n            pad_mode, pad_heads[i], pad_tails[i], params.globalPooling)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_reorganize_data_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C * B * B , H / B, W / B]\n        2. [N, C, H, W] ---> [N, C / B / B , H * B, W * B]\n\n    Note taht B is the block size specified in this operator.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n\n    params = operator.raw_operator.reorganizeData\n\n    from coremltools.proto.NeuralNetwork_pb2 import ReorganizeDataLayerParams as Params\n\n    if params.mode == Params.DEPTH_TO_SPACE:\n        if output_shape[1] % (params.blockSize * params.blockSize) != 0:\n            raise RuntimeError('Channel number must be divisible by the square of block size')\n\n        output_shape = [output_shape[0], output_shape[1] / params.blockSize / params.blockSize,\n                        output_shape[2] * params.blockSize, output_shape[3] * params.blockSize]\n    elif params.mode == Params.SPACE_TO_DEPTH:\n        if output_shape[2] % params.blockSize != 0 or output_shape[3] % params.blockSize != 0:\n            raise RuntimeError('Height and weight must be divisible by block size')\n\n        output_shape = [output_shape[0], output_shape[1] * params.blockSize * params.blockSize,\n                        output_shape[2] / params.blockSize, output_shape[3] / params.blockSize]\n    else:\n        raise ValueError('Unsupport reorganization mode {0}'.format(params.mode))\n\n    operator.outputs[0].type = FloatTensorType([int(i) if i != 'None' else 'None' for i in output_shape],\n                                               doc_string=operator.outputs[0].type.doc_string)", "response": "This function calculates the output shapes of the data layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_batch_normalization_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C]\n        2. [N, C, H, W] ---> [N, C, H, W]\n\n    This operator just uses the operator input shape as its output shape.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input_shape = operator.inputs[0].type.shape\n    if len(input_shape) not in [2, 4]:\n        raise RuntimeError('Input must be a 2-D or a 4-D tensor')\n\n    operator.outputs[0].type.shape = copy.deepcopy(operator.inputs[0].type.shape)", "response": "This operator calculates the output shapes of the batch normalization operator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_libsvm_simple_model(scope, model, inputs):\n    '''\n    This function handles all non-pipeline models.\n\n    :param scope: Scope object\n    :param model: A libsvm object (e.g., OneHotEncoder and LogisticRegression)\n    :param inputs: A list of variables\n    :return: A list of output variables which will be passed to next stage\n    '''\n\n    if model.get_svm_type() in (0, 1):\n        label_variable = scope.declare_local_variable('label', FloatTensorType())\n        probability_map_variable = scope.declare_local_variable('probabilities', FloatTensorType())\n        this_operator = scope.declare_local_operator(\"LibSvmSVC\", model)\n        this_operator.inputs = inputs\n        this_operator.outputs.append(label_variable)\n        this_operator.outputs.append(probability_map_variable)\n    elif model.get_svm_type() in (4, 3): \n        # We assume that all scikit-learn operator can only produce a single float tensor.\n        variable = scope.declare_local_variable('variable', FloatTensorType())\n        this_operator = scope.declare_local_operator(\"LibSvmSVR\", model)\n        this_operator.inputs = inputs\n        this_operator.outputs.append(variable)\n    else:\n        raise ValueError(\"Unknown SVM type '{0}'\".format(model.get_svm_type()))\n    return this_operator.outputs", "response": "This function handles all libsvm models and returns a list of output variables which will be passed to next stage\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_tensor_to_probability_map(scope, operator, container):\n    '''\n    This converter tries to convert a special operator 'TensorToProbabilityMap' into a sequence of some ONNX operators.\n    Those operators are used to create a dictionary in which keys are class labels and values are the associated\n    probabilities. We assume that the elements in the given probability tensor are aligned with the class labels\n    specified in the CoreML model.\n\n    Notice that ONNX<1.2 doesn't support a CoreML classifier with a batch size larger than one because old ONNX ZipMap\n    is not able to produce a sequence of dictionaries. This issue has been fixed in ONNX-1.2.\n    '''\n    attrs = {'name': scope.get_unique_operator_name('ZipMap')}\n\n    model_type = operator.raw_operator.WhichOneof('Type')\n    if model_type == 'neuralNetworkClassifier':\n        model = operator.raw_operator.neuralNetworkClassifier\n        if model.WhichOneof('ClassLabels') == 'stringClassLabels':\n            attrs['classlabels_strings'] = list(s.encode('utf-8') for s in model.stringClassLabels.vector)\n        elif model.WhichOneof('ClassLabels') == 'int64ClassLabels':\n            attrs['classlabels_int64s'] = list(int(i) for i in model.int64ClassLabels.vector)\n        else:\n            raise ValueError('Unknown label type found')\n    elif model_type == 'pipelineClassifier':\n        model = operator.raw_operator.pipelineClassifier\n        if model.WhichOneof('ClassLabels') == 'stringClassLabels':\n            attrs['classlabels_strings'] = list(s.encode('utf-8') for s in model.stringClassLabels.vector)\n        elif model.WhichOneof('ClassLabels') == 'int64ClassLabels':\n            attrs['classlabels_int64s'] = list(int(i) for i in model.int64ClassLabels.vector)\n        else:\n            raise ValueError('Unknown label type found')\n    else:\n        raise TypeError('Only neural network classifiers and pipeline classifiers are supported')\n\n    input_shape = operator.inputs[0].type.shape\n    if len(operator.inputs[0].type.shape) != 2:\n        # Calculate the shape attribute of ONNX Reshape\n        if input_shape[0] != 'None':\n            N = input_shape[0]\n        else:\n            N = -1  # -1 means that this dimension is automatically determined in runtime and unknown in conversion time\n\n        if all(isinstance(i, numbers.Integral) for i in input_shape[1:]):\n            C = 1\n            for i in input_shape[1:]:\n                C *= int(i)\n        else:\n            C = -1  # -1 means that this dimension is automatically determined in runtime and unknown in conversion time\n\n        # ZipMap in ONNX only accepts [C] and [N, C] inputs. In cases of [N, C, 1, 1], we reshape the probability tensor\n        # into [N, C] before feeding it into ZipMap.\n        buffer_name = scope.get_unique_variable_name('buffer')\n        apply_reshape(scope, operator.inputs[0].full_name, buffer_name, container, desired_shape=[N, C])\n    else:\n        buffer_name = operator.inputs[0].full_name\n\n    container.add_node('ZipMap', buffer_name, operator.outputs[0].full_name,\n                       op_domain='ai.onnx.ml', **attrs)", "response": "This converter tries to convert a TensorToProbabilityMap operator into a sequence of ONNX operators."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees bidirectional LSTM's conversion function for its output shapes.", "response": "def calculate_bidirectional_lstm_output_shapes(operator):\n    '''\n    See bidirectional LSTM's conversion function for its output shapes.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, 5], output_count_range=[1, 5])\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input_shape = operator.inputs[0].type.shape\n\n    # LSTM accepts [N, C] and [N, C, 1, 1] inputs\n    if len(input_shape) not in [2, 4]:\n        raise RuntimeError('Input must be a 2-D or 4-D tensor')\n\n    params = operator.raw_operator.biDirectionalLSTM\n    # The following line is more accurate but it may break some tests\n    # output_shape = ['None', params.outputVectorSize] if params.params.sequenceOutput else [1, 2 *params.outputVectorSize]\n    output_shape = ['None', 2 * params.outputVectorSize]\n    state_shape = [1, params.outputVectorSize]\n\n    # TODO: Changing input shapes of an operator is dangerous, this should be move to Topology's _fix_shapes function\n    if len(operator.inputs) > 1:\n        Y_h_in = operator.inputs[1]  # The forward initial hidden state of a single sequence\n        Y_h_in.type.shape = state_shape\n        Y_h_rev_in = operator.inputs[3]  # The backward initial hidden state of a single sequence\n        Y_h_rev_in.type.shape = state_shape\n    if len(operator.inputs) > 2:\n        Y_c_in = operator.inputs[2]  # The forward initial cell state of a single sequence\n        Y_c_in.type.shape = state_shape\n        Y_c_rev_in = operator.inputs[4]  # The backward initial cell state of a single sequence\n        Y_c_rev_in.type.shape = state_shape\n\n    operator.outputs[0].type.shape = output_shape\n    if len(operator.outputs) > 1:\n        operator.outputs[1].type.shape = state_shape\n        operator.outputs[3].type.shape = state_shape\n    if len(operator.outputs) > 2:\n        operator.outputs[2].type.shape = state_shape\n        operator.outputs[4].type.shape = state_shape"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nallowing input/output patterns are 1. [N, C] ---> [N, C'] 2. [N, C, 1, 1] ---> [N, C', 1, 1]", "response": "def calculate_inner_product_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C']\n        2. [N, C, 1, 1] ---> [N, C', 1, 1]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input = operator.inputs[0]\n    output = operator.outputs[0]\n\n    input_shape = input.type.shape\n    if len(input_shape) == 4 and (input_shape[2] != 1 or input_shape[3] != 1):\n        raise RuntimeError('If input is a 4-D tensor, its shape must be [N, C, 1, 1]')\n\n    params = operator.raw_operator.innerProduct\n\n    if input_shape[1] != params.inputChannels:\n        raise RuntimeError('Dimension mismatch along C-axis. Expected %s but got %s' %\n                           (params.inputChannels, input_shape[1]))\n\n    if len(input_shape) == 4:\n        output.type.shape = [input_shape[0], params.outputChannels, 1, 1]\n    elif len(input_shape) == 2:\n        output.type.shape = [input_shape[0], params.outputChannels]\n    else:\n        raise RuntimeError('Input must be a 2-D or a 4-D tensor')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading an ONNX model from a file.", "response": "def load_model(file_path):\n    \"\"\"\n    Loads an ONNX model to a ProtoBuf object.\n\n    :param file_path: ONNX file (full file name)\n    :return: ONNX model.\n\n    Example:\n\n    ::\n\n        from onnxmltools.utils import load_model\n        onnx_model = load_model(\"SqueezeNet.onnx\")\n    \"\"\"\n    if not path.exists(file_path):\n        raise FileNotFoundError(\"{0} was not found.\".format(file_path))\n    model = onnx_proto.ModelProto()\n    with open(file_path, 'rb') as f:\n        model.ParseFromString(f.read())\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave an ONNX model to a ProtoBuf object.", "response": "def save_model(model, file_path):\n    \"\"\"\n    Saves an ONNX model to a ProtoBuf object.\n    :param model: ONNX model\n    :param file_path: ONNX file (full file name)\n\n    Example:\n\n    ::\n\n        from onnxmltools.utils import save_model\n        save_model(onnx_model, 'c:/test_model.onnx')\n    \"\"\"\n    if model is None or not isinstance(model, onnx_proto.ModelProto):\n        raise ValueError(\"Model is not a valid ONNX model.\")\n    directory = os.path.dirname(os.path.abspath(file_path))\n    if not path.exists(directory):\n        raise FileNotFoundError(\"Directory does not exist {0}\".format(directory))\n    with open(file_path, 'wb') as f:\n        f.write(model.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the domain on the ONNX model.", "response": "def set_model_domain(model, domain):\n    \"\"\"\n    Sets the domain on the ONNX model.\n\n    :param model: instance of an ONNX model\n    :param domain: string containing the domain name of the model\n\n    Example:\n\n    ::\n        from onnxmltools.utils import set_model_domain\n        onnx_model = load_model(\"SqueezeNet.onnx\")\n        set_model_domain(onnx_model, \"com.acme\")\n    \"\"\"\n    if model is None or not isinstance(model, onnx_proto.ModelProto):\n        raise ValueError(\"Model is not a valid ONNX model.\")\n    if not convert_utils.is_string_type(domain):\n        raise ValueError(\"Domain must be a string type.\")\n    model.domain = domain"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_model_version(model, version):\n    if model is None or not isinstance(model, onnx_proto.ModelProto):\n        raise ValueError(\"Model is not a valid ONNX model.\")\n    if not convert_utils.is_numeric_type(version):\n        raise ValueError(\"Version must be a numeric type.\")\n    model.model_version = version", "response": "Sets the version of the ONNX model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_model_doc_string(model, doc, override=False):\n    if model is None or not isinstance(model, onnx_proto.ModelProto):\n        raise ValueError(\"Model is not a valid ONNX model.\")\n    if not convert_utils.is_string_type(doc):\n        raise ValueError(\"Doc must be a string type.\")\n    if model.doc_string and not doc and override is False:\n        raise ValueError(\"Failing to overwrite the doc string with a blank string, set override to True if intentional.\")\n    model.doc_string = doc", "response": "Sets the doc string of the ONNX model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a TensorProto into the initializer list of the final ONNX model.", "response": "def add_initializer(self, name, onnx_type, shape, content):\n        '''\n        Add a TensorProto into the initializer list of the final ONNX model\n\n        :param name: Variable name in the produced ONNX model.\n        :param onnx_type: Element types allowed in ONNX tensor, e.g., TensorProto.FLOAT and TensorProto.STRING.\n        :param shape: Tensor shape, a list of integers.\n        :param content: Flattened tensor values (i.e., a float list or a float array).\n        '''\n        if any(d is None for d in shape):\n            raise ValueError('Shape of initializer cannot contain None')\n        tensor = helper.make_tensor(name, onnx_type, shape, content)\n        self.initializers.append(tensor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_node(self, op_type, inputs, outputs, op_domain='', op_version=1, **attrs):\n        '''\n        Add a NodeProto into the node list of the final ONNX model. If the input operator's domain-version information\n        cannot be found in our domain-version pool (a Python set), we may add it.\n\n        :param op_type: A string (e.g., Pool and Conv) indicating the type of the NodeProto\n        :param inputs: A list of strings. They are the input variables' names of the considered NodeProto\n        :param outputs: A list of strings. They are the output variables' names of the considered NodeProto\n        :param op_domain: The domain name (e.g., ai.onnx.ml) of the operator we are trying to add.\n        :param op_version: The version number (e.g., 0 and 1) of the operator we are trying to add.\n        :param attrs: A Python dictionary. Keys and values are attributes' names and attributes' values, respectively.\n        '''\n\n        if isinstance(inputs, (six.string_types, six.text_type)):\n            inputs = [inputs]\n        if isinstance(outputs, (six.string_types, six.text_type)):\n            outputs = [outputs]\n        if not isinstance(inputs, list) or not all(isinstance(s, (six.string_types, six.text_type)) for s in inputs):\n            type_list = ','.join(list(str(type(s)) for s in inputs))\n            raise ValueError('Inputs must be a list of string but get [%s]' % type_list)\n        if not isinstance(outputs, list) or not all(isinstance(s, (six.string_types, six.text_type)) for s in outputs):\n            type_list = ','.join(list(str(type(s)) for s in outputs))\n            raise ValueError('Outputs must be a list of string but get [%s]' % type_list)\n        for k, v in attrs.items():\n            if v is None:\n                raise ValueError('Failed to create ONNX node. Undefined attribute pair (%s, %s) found' % (k, v))\n\n        node = helper.make_node(op_type, inputs, outputs, **attrs)\n        node.domain = op_domain\n\n        self.node_domain_version_pair_sets.add((op_domain, op_version))\n        self.nodes.append(node)", "response": "Adds a NodeProto into the ONNX model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nallow input/output patterns are 1. [N, C] ---> [N, C] 2. [N, C, H, W] ---> [N, C * H * W]", "response": "def calculate_flatten_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C]\n        2. [N, C, H, W] ---> [N, C * H * W]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input = operator.inputs[0]\n    output = operator.outputs[0]\n\n    if len(input.type.shape) not in [2, 4]:\n        raise RuntimeError('Input must be 2-D or 4-D float tensor')\n\n    input_shape = input.type.shape\n    output_shape = [input_shape[0], 1]\n\n    # Calculate the multiplication of C, H, and W.\n    for i in input_shape[1:]:\n        if i != 'None':\n            output_shape[1] *= i\n        else:\n            # If any of C, H, W-dimensions is unknown, the flatten C-dimension is unknown\n            output_shape[1] = 'None'\n            break\n\n    output.type.shape = output_shape"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_one_hot_encoder_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, 1] ---> [N, C']\n\n    C' is the total number of categorical values.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n\n    if operator.inputs[0].type.shape[1] != 1 or len(operator.inputs[0].type.shape) > 2:\n        raise RuntimeError('Input must be [N, 1]-tensor')\n\n    int_categories = operator.raw_operator.oneHotEncoder.int64Categories.vector\n    str_categories = operator.raw_operator.oneHotEncoder.stringCategories.vector\n\n    N = operator.inputs[0].type.shape[0]\n\n    if len(int_categories) > 0:\n        operator.outputs[0].type = FloatTensorType([N, len(int_categories)],\n                                                   doc_string=operator.outputs[0].type.doc_string)\n    elif len(str_categories) > 0 and type(operator.inputs[0].type) == StringTensorType:\n        operator.outputs[0].type = FloatTensorType([N, len(str_categories)],\n                                                   doc_string=operator.outputs[0].type.doc_string)\n    else:\n        raise ValueError('Categorical indexes are missing')", "response": "This function calculates the output shapes of one - hot encoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_dot_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C], [N, C] ---> [N, 1]\n        2. [N, C, 1, 1], [N, C, 1, 1] ---> [N, 1, 1, 1]\n    '''\n    check_input_and_output_numbers(operator, input_count_range=2, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    if operator.inputs[0].type.shape != operator.inputs[1].type.shape:\n        raise RuntimeError('Input shapes must be identical')\n\n    # Assume that inputs are [N, C]- or [N, C, 1, 1]-tensors\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    output_shape[1] = 1\n    operator.outputs[0].type.shape = output_shape", "response": "This function calculates the output shapes of the dot operator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nallow input/output patterns are ONNX < 1.2 1. [1, C] ---> ---> A map 2. [1, C_1, ..., C_n] ---> A map ONNX >= 1.2 1. [N, C] ---> ---> A sequence of maps 2. [N, C_1, ..., C_n] ---> A sequence of maps Note that N must be 1 currently if you're using ONNX<1.2 because old ZipMap doesn't produce a seqneuce of map If the input is not [N, C], it will be reshaped into [N, C_1 x C_2, x ... x C_n] before being fed into ONNX ZipMap.", "response": "def calculate_tensor_to_probability_map_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n    ONNX < 1.2\n        1. [1, C] ---> ---> A map\n        2. [1, C_1, ..., C_n] ---> A map\n    ONNX >= 1.2\n        1. [N, C] ---> ---> A sequence of maps\n        2. [N, C_1, ..., C_n] ---> A sequence of maps\n\n    Note that N must be 1 currently if you're using ONNX<1.2 because old ZipMap doesn't produce a seqneuce of map If the\n    input is not [N, C], it will be reshaped into [N, C_1 x C_2, x ... x C_n] before being fed into ONNX ZipMap.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    model_type = operator.raw_operator.WhichOneof('Type')\n    if model_type == 'neuralNetworkClassifier':\n        class_label_type = operator.raw_operator.neuralNetworkClassifier.WhichOneof('ClassLabels')\n    else:\n        raise TypeError('%s has no class label' % model_type)\n\n    N = operator.inputs[0].type.shape[0]\n    doc_string = operator.outputs[0].type.doc_string\n    if class_label_type == 'stringClassLabels':\n        if operator.target_opset < 7:\n            operator.outputs[0].type = DictionaryType(StringTensorType([1]), FloatTensorType([1]), doc_string)\n        else:\n            operator.outputs[0].type = \\\n                SequenceType(DictionaryType(StringTensorType([]), FloatTensorType([])), N, doc_string)\n    elif class_label_type == 'int64ClassLabels':\n        if operator.target_opset < 7:\n            operator.outputs[0].type = DictionaryType(Int64TensorType([1]), FloatTensorType([1]), doc_string)\n        else:\n            operator.outputs[0].type = \\\n                SequenceType(DictionaryType(Int64TensorType([]), FloatTensorType([])), N, doc_string)\n    else:\n        raise ValueError('Unsupported label type')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xgboost_installed():\n    try:\n        import xgboost\n    except ImportError:\n        return False\n    from xgboost.core import _LIB\n    try:\n        _LIB.XGBoosterDumpModelEx\n    except AttributeError:\n        # The version is not recent enough even though it is version 0.6.\n        # You need to install xgboost from github and not from pypi.\n        return False\n    from xgboost import __version__\n    vers = LooseVersion(__version__)\n    allowed = LooseVersion('0.7')\n    if vers < allowed:\n        warnings.warn('The converter works for xgboost >= 0.7. Earlier versions might not.')\n    return True", "response": "Checks that xgboost is installed and returns True if it is."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_input_and_output_numbers(operator, input_count_range=None, output_count_range=None):\n    '''\n    Check if the number of input(s)/output(s) is correct\n\n    :param operator: A Operator object\n    :param input_count_range: A list of two integers or an integer. If it's a list the first/second element is the\n    minimal/maximal number of inputs. If it's an integer, it is equivalent to specify that number twice in a list. For\n    infinite ranges like 5 to infinity, you need to use [5, None].\n    :param output_count_range: A list of two integers or an integer. See input_count_range for its format.\n    '''\n    if isinstance(input_count_range, list):\n        min_input_count = input_count_range[0]\n        max_input_count = input_count_range[1]\n    elif isinstance(input_count_range, int) or input_count_range is None:\n        min_input_count = input_count_range\n        max_input_count = input_count_range\n    else:\n        raise RuntimeError('input_count_range must be a list or an integer')\n\n    if isinstance(output_count_range, list):\n        min_output_count = output_count_range[0]\n        max_output_count = output_count_range[1]\n    elif isinstance(output_count_range, int) or output_count_range is None:\n        min_output_count = output_count_range\n        max_output_count = output_count_range\n    else:\n        raise RuntimeError('output_count_range must be a list or an integer')\n\n    if min_input_count is not None and len(operator.inputs) < min_input_count:\n        raise RuntimeError(\n            'For operator %s (type: %s), at least %s input(s) is(are) required but we got %s input(s) which are %s' \\\n            % (operator.full_name, operator.type, min_input_count, len(operator.inputs), operator.input_full_names))\n\n    if max_input_count is not None and len(operator.inputs) > max_input_count:\n        raise RuntimeError(\n            'For operator %s (type: %s), at most %s input(s) is(are) supported but we got %s output(s) which are %s' \\\n            % (operator.full_name, operator.type, max_input_count, len(operator.inputs), operator.input_full_names))\n\n    if min_output_count is not None and len(operator.outputs) < min_output_count:\n        raise RuntimeError(\n            'For operator %s (type: %s), at least %s output(s) is(are) produced but we got %s output(s) which are %s' \\\n            % (operator.full_name, operator.type, min_output_count, len(operator.outputs), operator.output_full_names))\n\n    if max_output_count is not None and len(operator.outputs) > max_output_count:\n        raise RuntimeError(\n            'For operator %s (type: %s), at most %s outputs(s) is(are) supported but we got %s output(s) which are %s' \\\n            % (operator.full_name, operator.type, max_output_count, len(operator.outputs), operator.output_full_names))", "response": "Check if the number of input and output numbers of the object is correct."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_input_and_output_types(operator, good_input_types=None, good_output_types=None):\n    '''\n    Check if the type(s) of input(s)/output(s) is(are) correct\n\n    :param operator: A Operator object\n    :param good_input_types: A list of allowed input types (e.g., [FloatTensorType, Int64TensorType]) or None. None\n    means that we skip the check of the input types.\n    :param good_output_types: A list of allowed output types. See good_input_types for its format.\n    '''\n    if good_input_types is not None:\n        for variable in operator.inputs:\n            if type(variable.type) not in good_input_types:\n                raise RuntimeError('Operator %s (type: %s) got an input %s with a wrong type %s. Only %s are allowed' \\\n                                   % (operator.full_name, operator.type, variable.full_name, type(variable.type),\n                                      good_input_types))\n\n    if good_output_types is not None:\n        for variable in operator.outputs:\n            if type(variable.type) not in good_output_types:\n                raise RuntimeError('Operator %s (type: %s) got an output %s with a wrong type %s. Only %s are allowed' \\\n                                   % (operator.full_name, operator.type, variable.full_name, type(variable.type),\n                                      good_output_types))", "response": "Checks if the types of input and output types of the given operator are correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_variable_for_input(scope, input_name, global_inputs, output_dict):\n    '''\n    Find the corresponding Variable for a given raw operator (model) name\n    The variable is either supplied as graph/global inputs or has been generated as output by previous ops\n    :param input_name:\n    :param global_inputs:\n    :param output_dict:\n    :return:\n    '''\n    if input_name in output_dict:\n        value = output_dict[input_name]\n        ref_count = value[0]\n        variable = value[1]\n        output_dict[input_name] = [ref_count+1, variable]\n        return variable\n\n    matches = [x for x in global_inputs if x.raw_name == input_name]\n    if matches:\n        return matches[0]\n    #\n    # create a new Var\n    #\n    return scope.declare_local_variable(input_name)", "response": "Find the corresponding Variable for a given raw operator name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_sparkml(spark, scope, model, global_inputs, output_dict):\n    '''\n    This is a delegate function. It doesn't nothing but invoke the correct parsing function according to the input\n    model's type.\n    :param scope: Scope object\n    :param model: A spark-ml object (e.g., OneHotEncoder and LogisticRegression)\n    :param inputs: A list of variables\n    :return: The output variables produced by the input model\n    '''\n    if isinstance(model, PipelineModel):\n        return _parse_sparkml_pipeline(spark, scope, model, global_inputs, output_dict)\n    else:\n        return _parse_sparkml_simple_model(spark, scope, model, global_inputs, output_dict)", "response": "This is a delegate function. It does not invoke the correct parsing function according to the input SparkML model type. It does not invoke the correct parsing function according to the input SparkML model type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_tensor_float_to_float16(tensor):\n    '''\n    Convert tensor float to float16.\n\n    :param tensor: TensorProto object\n    :return tensor_float16: converted TensorProto object\n\n    Example:\n\n    ::\n\n        from onnxmltools.utils.float16_converter import convert_tensor_float_to_float16\n        new_tensor = convert_tensor_float_to_float16(tensor)\n\n    '''\n    if not isinstance(tensor, onnx_proto.TensorProto):\n        raise ValueError('Expected input type is an ONNX TensorProto but got %s' % type(tensor))\n\n    if tensor.data_type == onnx_proto.TensorProto.FLOAT:\n        tensor.data_type = onnx_proto.TensorProto.FLOAT16\n        # convert float_data (float type) to float16 and write to int32_data\n        if tensor.float_data:\n            int_list = _npfloat16_to_int(np.float16(tensor.float_data))\n            tensor.int32_data[:] = int_list\n            tensor.float_data[:] = []\n        # convert raw_data (bytes type)\n        if tensor.raw_data:\n            # convert n.raw_data to float\n            float32_list = np.fromstring(tensor.raw_data, dtype='float32')\n            # convert float to float16\n            float16_list = np.float16(float32_list)\n            # convert float16 to bytes and write back to raw_data\n            tensor.raw_data = float16_list.tostring()\n    return tensor", "response": "Convert tensor float to float16."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_float_to_float16(model):\n    '''\n    Convert tensor float type in the ONNX ModelProto input to tensor float16.\n\n    :param model: ONNX ModelProto object\n    :return: converted ONNX ModelProto object\n\n    Examples:\n\n    ::\n\n        Example 1: Convert ONNX ModelProto object:\n        from onnxmltools.utils.float16_converter import convert_float_to_float16\n        new_onnx_model = convert_float_to_float16(onnx_model)\n\n        Example 2: Convert ONNX model binary file:\n        from onnxmltools.utils.float16_converter import convert_float_to_float16\n        from onnxmltools.utils import load_model, save_model\n        onnx_model = load_model('model.onnx')\n        new_onnx_model = convert_float_to_float16(onnx_model)\n        save_model(new_onnx_model, 'new_model.onnx')\n\n    '''\n    func_infer_shape = None\n    if onnx.__version__ >= '1.2':\n        try:\n            from onnx.shape_inference import infer_shapes\n            func_infer_shape = infer_shapes\n        finally:\n            pass\n\n    domain_flag = 0\n    if not isinstance(model, onnx_proto.ModelProto):\n        raise ValueError('Expected model type is an ONNX ModelProto but got %s' % type(model))\n\n    # create black list\n    op_black_list = ['ArrayFeatureExtractor', 'Binarizer', 'CastMap', 'CategoryMapper', 'DictVectorizer',\n                     'FeatureVectorizer', 'Imputer', 'LabelEncoder', 'LinearClassifier', 'LinearRegressor', 'Normalizer',\n                     'OneHotEncoder', 'SVMClassifier', 'SVMRegressor', 'Scaler', 'TreeEnsembleClassifier',\n                     'TreeEnsembleRegressor', 'ZipMap']\n    # create a queue for BFS\n    queue = []\n    value_info_list = []\n    node_list = []\n    # type inference on input model\n    if func_infer_shape is not None:\n        model = func_infer_shape(model)\n    queue.append(model)\n    while queue:\n        next_level = []\n        for q in queue:\n            # if q is model, push q.graph (GraphProto)\n            if isinstance(q, onnx_proto.ModelProto):\n                next_level.append(q.graph)\n            # if q is model.graph, push q.node.attribute (AttributeProto)\n            if isinstance(q, onnx_proto.GraphProto):\n                for n in q.node:\n                    # if n is in the black list (doesn't support float16), no conversion for the node,\n                    # and save the node for further processing\n                    if n.op_type in op_black_list:\n                        node_list.append(n)\n                    else:\n                        if n.op_type == 'Cast':\n                            for attr in n.attribute:\n                                if attr.name == 'to' and attr.i == 1:\n                                    attr.i = 10\n                        next_level.append(n.attribute)\n            # if q is model.graph.node.attribute, push q.g and q.graphs (GraphProto)\n            if isinstance(q, onnx_proto.AttributeProto):\n                next_level.append(q.g)\n                for n in q.graphs:\n                    next_level.append(n)\n            # if q is graph, process graph.initializer(TensorProto), input, output and value_info (ValueInfoProto)\n            if isinstance(q, onnx_proto.GraphProto):\n                for n in q.initializer:  # TensorProto type\n                    n = convert_tensor_float_to_float16(n)\n                # for all ValueInfoProto with tensor(float) type in input, output and value_info, convert them to\n                # tensor(float16) except map and seq(map). And save them in value_info_list for further processing\n                for n in itertools.chain(q.input, q.output, q.value_info):\n                    if n.type.tensor_type.elem_type == onnx_proto.TensorProto.FLOAT:\n                        n.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT16\n                        value_info_list.append(n)\n            # if q is node.attribute, process node.attribute.t and node.attribute.tensors (TensorProto)\n            if isinstance(q, onnx_proto.AttributeProto):\n                for n in itertools.chain(q.t, q.tensors):\n                    n = convert_tensor_float_to_float16(n)\n        queue = next_level\n\n    # process the nodes in black list that doesn't support tensor(float16)\n    for node in node_list:\n        # if input's name is in the value_info_list meaning input is tensor(float16) type, insert a float16 to float Cast node\n        # before the node, change current node's input name and create new value_info for the new name\n        for i in range(len(node.input)):\n            input = node.input[i]\n            for value_info in value_info_list:\n                if input == value_info.name:\n                    # create new value_info for current node's new input name\n                    new_value_info = model.graph.value_info.add()\n                    new_value_info.CopyFrom(value_info)\n                    new_value_info.name = input + '_casted'\n                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT\n                    # add Cast node (from tensor(float16) to tensor(float) before current node\n                    attrs = {'name': input + 'Cast'}\n                    attrs['to'] = onnx_proto.TensorProto.FLOAT\n                    nodes = [helper.make_node('Cast', input, input + '_casted', kwargs=attrs)]\n                    model.graph.node.extend(nodes)\n                    # change current node's input name\n                    node.input[i] = input + '_casted'\n                    domain_flag = 1\n                    continue\n        # if output's name is in the value_info_list meaning output is tensor(float16) type, insert a float to\n        # float16 Cast node after the node, change current node's output name and create new value_info for the new name\n        for i in range(len(node.output)):\n            output = node.output[i]\n            for value_info in value_info_list:\n                if output == value_info.name:\n                    # create new value_info for current node's new output\n                    new_value_info = model.graph.value_info.add()\n                    new_value_info.CopyFrom(value_info)\n                    new_value_info.name = output + '_casted'\n                    new_value_info.type.tensor_type.elem_type = onnx_proto.TensorProto.FLOAT\n                    # add Cast node (from tensor(float) to tensor(float16) after current node\n                    attrs = {'name': output + 'Cast'}\n                    attrs['to'] = onnx_proto.TensorProto.FLOAT16\n                    nodes = [helper.make_node('Cast', output + '_casted', output, kwarg=attrs)]\n                    model.graph.node.extend(nodes)\n                    # change current node's input name\n                    node.output[i] = output + '_casted'\n                    domain_flag = 1\n                    continue\n    if domain_flag:\n        # Create operator set for cast node\n        op_set = model.opset_import.add()\n        op_set.domain = \"\"\n        op_set.version = 7\n    return model", "response": "Convert tensor float type in the ONNX ModelProto input to tensor float16."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate_metadata(metadata_props):\n    '''\n    Validate metadata properties and possibly show warnings or throw exceptions.\n\n    :param metadata_props: A dictionary of metadata properties, with property names and values (see :func:`~onnxmltools.utils.metadata_props.add_metadata_props` for examples)\n    '''\n    if len(CaseInsensitiveDict(metadata_props)) != len(metadata_props):\n        raise RuntimeError('Duplicate metadata props found')\n\n    for key, value in metadata_props.items():\n        valid_values = KNOWN_METADATA_PROPS.get(key)\n        if valid_values and value.lower() not in valid_values:\n            warnings.warn('Key {} has invalid value {}. Valid values are {}'.format(key, value, valid_values))", "response": "Validate metadata properties and possibly show warnings or throw exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd metadata properties to ONNX model.", "response": "def add_metadata_props(onnx_model, metadata_props, target_opset):\n    '''\n    Add metadata properties to the model. See recommended key names at:\n    `Extensibility - Metadata <https://github.com/onnx/onnx/blob/296953db87b79c0137c5d9c1a8f26dfaa2495afc/docs/IR.md#metadata>`_ and\n    `Optional Metadata <https://github.com/onnx/onnx/blob/master/docs/IR.md#optional-metadata>`_\n\n\n    :param onnx_model: ONNX model object\n    :param metadata_props: A dictionary of metadata properties, with property names and values (example: `{ 'model_author': 'Alice', 'model_license': 'MIT' }`)\n    :param target_opset: Target ONNX opset\n    '''\n    if target_opset < 7:\n        warnings.warn('Metadata properties are not supported in targeted opset - %d' % target_opset)\n        return\n    _validate_metadata(metadata_props)\n    new_metadata = CaseInsensitiveDict({x.key: x.value for x in onnx_model.metadata_props})\n    new_metadata.update(metadata_props)\n    del onnx_model.metadata_props[:]\n    onnx_model.metadata_props.extend(\n        onnx_proto.StringStringEntryProto(key=key, value=value)\n        for key, value in metadata_props.items()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_denotation(onnx_model, input_name, denotation, target_opset, dimension_denotation=None):\n    '''\n    Set input type denotation and dimension denotation.\n\n    Type denotation is a feature in ONNX 1.2.1 that let's the model specify the content of a tensor (e.g. IMAGE or AUDIO).\n    This information can be used by the backend. One example where it is useful is in images: Whenever data is bound to\n    a tensor with type denotation IMAGE, the backend can process the data (such as transforming the color space and\n    pixel format) based on model metadata properties.\n\n    :param onnx_model: ONNX model object\n    :param input_name: Name of input tensor to edit (example: `'data0'`)\n    :param denotation: Input type denotation (`documentation <https://github.com/onnx/onnx/blob/master/docs/TypeDenotation.md#type-denotation-definition>`_)\n    (example: `'IMAGE'`)\n    :param target_opset: Target ONNX opset\n    :param dimension_denotation: List of dimension type denotations. The length of the list must be the same of the number of dimensions in the tensor\n    (`documentation https://github.com/onnx/onnx/blob/master/docs/DimensionDenotation.md#denotation-definition>`_)\n    (example: `['DATA_BATCH', 'DATA_CHANNEL', 'DATA_FEATURE', 'DATA_FEATURE']`)\n    '''\n    if target_opset < 7:\n        warnings.warn('Denotation is not supported in targeted opset - %d' % target_opset)\n        return\n    for graph_input in onnx_model.graph.input:\n        if graph_input.name == input_name:\n            graph_input.type.denotation = denotation\n            if dimension_denotation:\n                dimensions = graph_input.type.tensor_type.shape.dim\n                if len(dimension_denotation) != len(dimensions):\n                    raise RuntimeError('Wrong number of dimensions: input \"{}\" has {} dimensions'.format(input_name, len(dimensions)))\n                for dimension, channel_denotation in zip(dimensions, dimension_denotation):\n                    dimension.denotation = channel_denotation\n            return onnx_model\n    raise RuntimeError('Input \"{}\" not found'.format(input_name))", "response": "Set denotation for ONNX model input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the output shapes of the slice layer operator.", "response": "def calculate_slice_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C', H, W]\n        2. [N, C, H, W] ---> [N, C, H', W]\n        3. [N, C, H, W] ---> [N, C, H, W']\n    '''\n    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n\n    params = operator.raw_operator.slice\n\n    from coremltools.proto.NeuralNetwork_pb2 import SliceLayerParams as Params\n    axis_map = {Params.CHANNEL_AXIS: 1, Params.HEIGHT_AXIS: 2, Params.WIDTH_AXIS: 3}\n\n    if params.startIndex >= 0:\n        output_shape[axis_map[Params.CHANNEL_AXIS]] = params.endIndex - params.startIndex\n    else:\n        output_shape[axis_map[Params.CHANNEL_AXIS]] += 1 + params.endIndex - params.startIndex\n\n    operator.outputs[0].type = FloatTensorType(output_shape, doc_string=operator.outputs[0].type.doc_string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nallows input/output patterns are 1. [N, C_1, H_1, W_1], ..., [N, C_n, H_n, W_n] ---> [N, max(C_1, ..., C_n), max(H_1, ..., H_n), max(W_1, ..., W_n)] If 'None' happens at any coordinate, that coordinate's final dimension would be 'None'.", "response": "def calculate_merge_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C_1, H_1, W_1], ..., [N, C_n, H_n, W_n] --->\n            [N, max(C_1, ..., C_n), max(H_1, ..., H_n), max(W_1, ..., W_n)]\n\n    If 'None' happens at any coordinate, that coordinate's final dimension would be 'None'.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, None], output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    # [TODO] Fix reduce-like shape inference. We now assume all inputs are 4-D.\n    n_dims = max(len(variable.type.shape) for variable in operator.inputs)\n    output_shape = [0] * n_dims\n    for i in range(n_dims):\n        input_dims = [variable.type.shape[i] for variable in operator.inputs if len(variable.type.shape) > i]\n        if 'None' in input_dims:\n            output_shape[i] = 'None'\n        else:\n            output_shape[i] = max(input_dims)\n\n    operator.outputs[0].type.shape = output_shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_io_name_map():\r\n    '''\r\n    map of spark models to input-output tuples\r\n    Each lambda gets the corresponding input or output column name from the model\r\n    '''\r\n    map = {\r\n        \"pyspark.ml.feature.BucketedRandomProjectionLSHModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.regression.AFTSurvivalRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.ElementwiseProduct\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.MinHashLSHModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.Word2VecModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.IndexToString\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.ChiSqSelectorModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.OneVsRestModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.regression.GBTRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.GBTClassificationModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\"), 'probability']\r\n        ),\r\n        \"pyspark.ml.feature.DCT\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.PCAModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.PolynomialExpansion\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.Tokenizer\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.NaiveBayesModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\"), model.getOrDefault(\"probabilityCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.VectorSlicer\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.StopWordsRemover\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.NGram\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.Bucketizer\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.regression.RandomForestRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.RandomForestClassificationModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\"), model.getOrDefault(\"probabilityCol\")]\r\n        ),\r\n        \"pyspark.ml.regression.DecisionTreeRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.DecisionTreeClassificationModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\"), model.getOrDefault(\"probabilityCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.VectorIndexerModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.regression.GeneralizedLinearRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.regression.LinearRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.ImputerModel\": (\r\n            lambda model: model.getOrDefault(\"inputCols\"),\r\n            lambda model: model.getOrDefault(\"outputCols\")\r\n        ),\r\n        \"pyspark.ml.feature.MaxAbsScalerModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.MinMaxScalerModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.StandardScalerModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.Normalizer\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.Binarizer\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.LinearSVCModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\")]\r\n        ),\r\n        \"pyspark.ml.classification.LogisticRegressionModel\": (\r\n            lambda model: [model.getOrDefault(\"featuresCol\")],\r\n            lambda model: [model.getOrDefault(\"predictionCol\"), model.getOrDefault(\"probabilityCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.OneHotEncoderModel\": (\r\n            lambda model: model.getOrDefault(\"inputCols\"),\r\n            lambda model: model.getOrDefault(\"outputCols\")\r\n        ),\r\n        \"pyspark.ml.feature.StringIndexerModel\": (\r\n            lambda model: [model.getOrDefault(\"inputCol\")],\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        ),\r\n        \"pyspark.ml.feature.VectorAssembler\": (\r\n            lambda model: model.getOrDefault(\"inputCols\"),\r\n            lambda model: [model.getOrDefault(\"outputCol\")]\r\n        )\r\n    }\r\n    return map", "response": "Returns a map of spark models to input - output tuples\r\n            models to input - output tuples\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_feature_vectorizer_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C_1], ..., [N, C_n] ---> [N, C_1 + ... + C_n]\n\n    Feature vectorizer concatenates all input tensors along the C-axis, so the output dimension along C-axis is simply\n    a sum of all input features.\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, None], output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType, Int64TensorType, FloatType, Int64Type])\n\n    if any(len(variable.type.shape) != 2 for variable in operator.inputs):\n        raise RuntimeError('Input(s) must be 2-D tensor(s)')\n\n    # Find the first batch size which is not unknown\n    N = 'None'\n    for variable in operator.inputs:\n        if variable.type.shape[0] != 'None':\n            N = variable.type.shape[0]\n            break\n    for variable in operator.inputs:\n        if variable.type.shape[0] not in ['None', N]:\n            raise RuntimeError('The batch dimensions should be the same to all input tensors.')\n\n    C = sum(info.inputDimensions for info in operator.raw_operator.featureVectorizer.inputList)\n\n    # Currently, we only expect numerical inputs. If both of integers and floats exist, we may convert integers into\n    # floats before concatenating them. Thus, the output type is integer-like only if all inputs are integer-like.\n    doc_string = operator.outputs[0].type.doc_string\n    if all(isinstance(variable.type, (Int64TensorType, Int64Type)) for variable in operator.inputs):\n        operator.outputs[0].type = Int64TensorType([N, C], doc_string=doc_string)\n    elif isinstance(operator.inputs[0].type, (FloatTensorType, FloatType)):\n        operator.outputs[0].type = FloatTensorType([N, C], doc_string=doc_string)\n    else:\n        raise ValueError('Unsupported input type: %s' % type(operator.inputs[0].type))", "response": "This function calculates the output shapes of the feature vectorizer output."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_crop_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C, H, W] ---> [N, C, H', W']\n        2. [N, C, H, W],  shape-ref [N', C', H', W'] ---> [N, C, H', W']\n    '''\n    check_input_and_output_numbers(operator, input_count_range=[1, 2], output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n\n    params = operator.raw_operator.crop\n    if len(operator.inputs) == 1:\n        if len(params.cropAmounts.borderAmounts) > 0:\n            output_shape[2] -= params.cropAmounts.borderAmounts[0].startEdgeSize\n            output_shape[2] -= params.cropAmounts.borderAmounts[0].endEdgeSize\n            output_shape[3] -= params.cropAmounts.borderAmounts[1].startEdgeSize\n            output_shape[3] -= params.cropAmounts.borderAmounts[1].endEdgeSize\n    elif len(operator.inputs) == 2:\n        output_shape[2] = operator.inputs[1].type.shape[2]\n        output_shape[3] = operator.inputs[1].type.shape[3]\n    else:\n        raise RuntimeError('Too many inputs for Crop operator')\n\n    operator.outputs[0].type.shape = output_shape", "response": "This function calculates the output shapes of the Crop operator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a conversion function for the given operator.", "response": "def register_converter(operator_name, conversion_function, overwrite=False):\n    '''\n    :param operator_name: A unique operator ID. It is usually a string but you can use a type as well\n    :param conversion_function: A callable object\n    :param overwrite: By default, we raise an exception if the caller of this function is trying to assign an existing\n    key (i.e., operator_name) a new value (i.e., conversion_function). Set this flag to True to enable overwriting.\n    '''\n    if not overwrite and operator_name in _converter_pool:\n        raise ValueError('We do not overwrite registrated converter by default')\n    _converter_pool[operator_name] = conversion_function"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a shape calculator function for the given operator.", "response": "def register_shape_calculator(operator_name, calculator_function, overwrite=False):\n    '''\n    :param operator_name: A unique operator ID. It is usually a string but you can use a type as well\n    :param calculator_function: A callable object\n    :param overwrite:  By default, we raise an exception if the caller of this function is trying to assign an existing\n    key (i.e., operator_name) a new value (i.e., calculator_function). Set this flag to True to enable overwriting.\n    '''\n    if not overwrite and operator_name in _shape_calculator_pool:\n        raise ValueError('We do not overwrite registrated shape calculator by default')\n    _shape_calculator_pool[operator_name] = calculator_function"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef concatenate_variables(scope, variables, container):\n    '''\n    This function allocate operators to from a float tensor by concatenating all input variables. Notice that if all\n    integer inputs would be converted to floats before concatenation.\n    '''\n\n    # Check if it's possible to concatenate those inputs.\n    type_set = set(type(variable.type) for variable in variables)\n    number_type_set = {FloatType, FloatTensorType, Int64Type, Int64TensorType}\n    if StringType in type_set and any(number_type in type_set for number_type in number_type_set):\n        raise RuntimeError('We are not able to concatenate numerical tensor(s) and string tensor(s)')\n\n    input_names = []  # input variables' names we want to concatenate\n    input_dims = []  # dimensions of the variables that is going to be concatenated\n\n    # Collect input variable names and do cast if needed\n    for variable in variables:\n        if isinstance(variable.type, (Int64TensorType, Int64Type)):\n            input_names.append(convert_integer_to_float(scope, variable, container))\n        else:\n            input_names.append(variable.full_name)\n        # We assume input variables' shape are [1, C_1], ..., [1, C_n] if there are n inputs.\n        input_dims.append(variable.type.shape[1])\n\n    if len(input_names) == 1:\n        # No need to concatenate tensors if there is only one input\n        return input_names[0]\n    else:\n        # To combine all inputs, we need a FeatureVectorizer\n        op_type = 'FeatureVectorizer'\n        attrs = {'name': scope.get_unique_operator_name(op_type), 'inputdimensions': input_dims}\n        # Create a variable name to capture feature vectorizer's output\n        concatenated_name = scope.get_unique_variable_name('concatenated')\n        # Set up our FeatureVectorizer\n        container.add_node(op_type, input_names, concatenated_name, op_domain='ai.onnx.ml', **attrs)\n\n        return concatenated_name", "response": "This function allocate operators to from a float tensor by concatenating all input variables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_type_conversion(source_type, target_type):\n    if type(source_type) == type(target_type):\n        return 'identity'\n    elif type(target_type) == FloatTensorType:\n        return 'imageToFloatTensor'\n    else:\n        raise ValueError('Unsupported type conversion from %s to %s' % (\n                         source_type, target_type))", "response": "Find the operator name for converting source_type into target_type\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cool_off() -> Optional[timedelta]:\n\n    cool_off = settings.AXES_COOLOFF_TIME\n\n    if isinstance(cool_off, int):\n        return timedelta(hours=cool_off)\n    return cool_off", "response": "Returns the login cool off time interpreted from settings. AxES_COOLOFF_TIME"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cool_off_iso8601(delta: timedelta) -> str:\n\n    seconds = delta.total_seconds()\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    days, hours = divmod(hours, 24)\n\n    days_str = f'{days:.0f}D' if days else ''\n\n    time_str = ''.join(\n        f'{value:.0f}{designator}'\n        for value, designator\n        in [\n            [hours, 'H'],\n            [minutes, 'M'],\n            [seconds, 'S'],\n        ]\n        if value\n    )\n\n    if time_str:\n        return f'P{days_str}T{time_str}'\n    return f'P{days_str}'", "response": "Returns datetime. timedelta translated to ISO 8601 formatted duration for use in eg. cool offs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_credentials(username: str = None, **kwargs) -> dict:\n\n    credentials = {settings.AXES_USERNAME_FORM_FIELD: username}\n    credentials.update(kwargs)\n    return credentials", "response": "Calculate credentials for Axes to use internally from given username and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_client_username(request: AxesHttpRequest, credentials: dict = None) -> str:\n\n    if settings.AXES_USERNAME_CALLABLE:\n        log.debug('Using settings.AXES_USERNAME_CALLABLE to get username')\n\n        if callable(settings.AXES_USERNAME_CALLABLE):\n            return settings.AXES_USERNAME_CALLABLE(request, credentials)\n        if isinstance(settings.AXES_USERNAME_CALLABLE, str):\n            return import_string(settings.AXES_USERNAME_CALLABLE)(request, credentials)\n        raise TypeError('settings.AXES_USERNAME_CALLABLE needs to be a string, callable, or None.')\n\n    if credentials:\n        log.debug('Using parameter credentials to get username with key settings.AXES_USERNAME_FORM_FIELD')\n        return credentials.get(settings.AXES_USERNAME_FORM_FIELD, None)\n\n    log.debug('Using parameter request.POST to get username with key settings.AXES_USERNAME_FORM_FIELD')\n    return request.POST.get(settings.AXES_USERNAME_FORM_FIELD, None)", "response": "Resolve client username from request or credentials if supplied."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_client_ip_address(request: HttpRequest) -> str:\n\n    client_ip_address, _ = ipware.ip2.get_client_ip(\n        request,\n        proxy_order=settings.AXES_PROXY_ORDER,\n        proxy_count=settings.AXES_PROXY_COUNT,\n        proxy_trusted_ips=settings.AXES_PROXY_TRUSTED_IPS,\n        request_header_order=settings.AXES_META_PRECEDENCE_ORDER,\n    )\n\n    return str(ip_address(client_ip_address))", "response": "Get the client IP address as configured by the user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary that can be used to filter the AccessAttempt queryset.", "response": "def get_client_parameters(username: str, ip_address: str, user_agent: str) -> dict:\n    \"\"\"\n    Get query parameters for filtering AccessAttempt queryset.\n\n    This method returns a dict that guarantees iteration order for keys and values,\n    and can so be used in e.g. the generation of hash keys or other deterministic functions.\n    \"\"\"\n\n    filter_kwargs = dict()\n\n    if settings.AXES_ONLY_USER_FAILURES:\n        # 1. Only individual usernames can be tracked with parametrization\n        filter_kwargs['username'] = username\n    else:\n        if settings.AXES_LOCK_OUT_BY_COMBINATION_USER_AND_IP:\n            # 2. A combination of username and IP address can be used as well\n            filter_kwargs['username'] = username\n            filter_kwargs['ip_address'] = ip_address\n        else:\n            # 3. Default case is to track the IP address only, which is the most secure option\n            filter_kwargs['ip_address'] = ip_address\n\n        if settings.AXES_USE_USER_AGENT:\n            # 4. The HTTP User-Agent can be used to track e.g. one browser\n            filter_kwargs['user_agent'] = user_agent\n\n    return filter_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a string that can be used in e. g. logging to distinguish client requests.", "response": "def get_client_str(username: str, ip_address: str, user_agent: str, path_info: str) -> str:\n    \"\"\"\n    Get a readable string that can be used in e.g. logging to distinguish client requests.\n\n    Example log format would be ``{username: \"example\", ip_address: \"127.0.0.1\", path_info: \"/example/\"}``\n    \"\"\"\n\n    client_dict = dict()\n\n    if settings.AXES_VERBOSE:\n        # Verbose mode logs every attribute that is available\n        client_dict['username'] = username\n        client_dict['ip_address'] = ip_address\n        client_dict['user_agent'] = user_agent\n    else:\n        # Other modes initialize the attributes that are used for the actual lockouts\n        client_dict = get_client_parameters(username, ip_address, user_agent)\n\n    # Path info is always included as last component in the client string for traceability purposes\n    if path_info and isinstance(path_info, (tuple, list)):\n        path_info = path_info[0]\n    client_dict['path_info'] = path_info\n\n    # Template the internal dictionary representation into a readable and concatenated key: \"value\" format\n    template = ', '.join(\n        f'{key}: \"{value}\"'\n        for key, value\n        in client_dict.items()\n    )\n\n    # Wrap the internal dict into a single {key: \"value\"} bracing in the output\n    # which requires double braces when done with the Python string templating system\n    # i.e. {{key: \"value\"}} becomes {key: \"value\"} when run through a .format() call\n    template = '{{' + template + '}}'\n\n    return template.format(client_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a query dictionary into an easy - to - read list of key - value pairs.", "response": "def get_query_str(query: Type[QueryDict], max_length: int = 1024) -> str:\n    \"\"\"\n    Turns a query dictionary into an easy-to-read list of key-value pairs.\n\n    If a field is called either ``'password'`` or ``settings.AXES_PASSWORD_FORM_FIELD`` it will be excluded.\n\n    The length of the output is limited to max_length to avoid a DoS attack via excessively large payloads.\n    \"\"\"\n\n    query_dict = query.copy()\n    query_dict.pop('password', None)\n    query_dict.pop(settings.AXES_PASSWORD_FORM_FIELD, None)\n\n    query_str = '\\n'.join(\n        f'{key}={value}'\n        for key, value\n        in query_dict.items()\n    )\n\n    return query_str[:max_length]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_client_ip_address_whitelisted(request: AxesHttpRequest):\n\n    if settings.AXES_NEVER_LOCKOUT_WHITELIST and is_ip_address_in_whitelist(request.axes_ip_address):\n        return True\n\n    if settings.AXES_ONLY_WHITELIST and is_ip_address_in_whitelist(request.axes_ip_address):\n        return True\n\n    return False", "response": "Check if the given request refers to a whitelisted IP."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the given request refers to a blacklisted IP.", "response": "def is_client_ip_address_blacklisted(request: AxesHttpRequest) -> bool:\n    \"\"\"\n    Check if the given request refers to a blacklisted IP.\n    \"\"\"\n\n    if is_ip_address_in_blacklist(request.axes_ip_address):\n        return True\n\n    if settings.AXES_ONLY_WHITELIST and not is_ip_address_in_whitelist(request.axes_ip_address):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_client_method_whitelisted(request: AxesHttpRequest) -> bool:\n\n    if settings.AXES_NEVER_LOCKOUT_GET and request.method == 'GET':\n        return True\n\n    return False", "response": "Check if the given request uses a whitelisted method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a cache key name from request or AccessAttempt object.", "response": "def get_client_cache_key(request_or_attempt: Union[HttpRequest, Any], credentials: dict = None) -> str:\n    \"\"\"\n    Build cache key name from request or AccessAttempt object.\n\n    :param request_or_attempt: HttpRequest or AccessAttempt object\n    :param credentials: credentials containing user information\n    :return cache_key: Hash key that is usable for Django cache backends\n    \"\"\"\n\n    if isinstance(request_or_attempt, HttpRequest):\n        username = get_client_username(request_or_attempt, credentials)\n        ip_address = get_client_ip_address(request_or_attempt)\n        user_agent = get_client_user_agent(request_or_attempt)\n    else:\n        username = request_or_attempt.username\n        ip_address = request_or_attempt.ip_address\n        user_agent = request_or_attempt.user_agent\n\n    filter_kwargs = get_client_parameters(username, ip_address, user_agent)\n\n    cache_key_components = ''.join(filter_kwargs.values())\n    cache_key_digest = md5(cache_key_components.encode()).hexdigest()\n    cache_key = f'axes-{cache_key_digest}'\n\n    return cache_key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_logged_in(self, sender, request: AxesHttpRequest, user, **kwargs):  # pylint: disable=unused-argument\n\n        # 1. database query: Clean up expired user attempts from the database\n        clean_expired_user_attempts(request.axes_attempt_time)\n\n        username = user.get_username()\n        credentials = get_credentials(username)\n        client_str = get_client_str(username, request.axes_ip_address, request.axes_user_agent, request.axes_path_info)\n\n        log.info('AXES: Successful login by %s.', client_str)\n\n        if not settings.AXES_DISABLE_SUCCESS_ACCESS_LOG:\n            # 2. database query: Insert new access logs with login time\n            AccessLog.objects.create(\n                username=username,\n                ip_address=request.axes_ip_address,\n                user_agent=request.axes_user_agent,\n                http_accept=request.axes_http_accept,\n                path_info=request.axes_path_info,\n                attempt_time=request.axes_attempt_time,\n            )\n\n        if settings.AXES_RESET_ON_SUCCESS:\n            # 3. database query: Reset failed attempts for the logging in user\n            count = reset_user_attempts(request, credentials)\n            log.info('AXES: Deleted %d failed login attempts by %s from database.', count, client_str)", "response": "When user logs in update the AccessLog related to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_logged_out(self, sender, request: AxesHttpRequest, user, **kwargs):  # pylint: disable=unused-argument\n\n        # 1. database query: Clean up expired user attempts from the database\n        clean_expired_user_attempts(request.axes_attempt_time)\n\n        username = user.get_username()\n        client_str = get_client_str(username, request.axes_ip_address, request.axes_user_agent, request.axes_path_info)\n\n        log.info('AXES: Successful logout by %s.', client_str)\n\n        if username and not settings.AXES_DISABLE_ACCESS_LOG:\n            # 2. database query: Update existing attempt logs with logout time\n            AccessLog.objects.filter(\n                username=username,\n                logout_time__isnull=True,\n            ).update(\n                logout_time=request.axes_attempt_time,\n            )", "response": "When user logs out update the AccessLog related to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_login_failed(\n            self,\n            sender,\n            credentials: dict,\n            request: AxesHttpRequest = None,\n            **kwargs\n    ):  # pylint: disable=too-many-locals\n        \"\"\"\n        When user login fails, save attempt record in cache and lock user out if necessary.\n\n        :raises AxesSignalPermissionDenied: if user should be locked out.\n        \"\"\"\n\n        if request is None:\n            log.error('AXES: AxesCacheHandler.user_login_failed does not function without a request.')\n            return\n\n        username = get_client_username(request, credentials)\n        client_str = get_client_str(username, request.axes_ip_address, request.axes_user_agent, request.axes_path_info)\n\n        if self.is_whitelisted(request, credentials):\n            log.info('AXES: Login failed from whitelisted client %s.', client_str)\n            return\n\n        failures_since_start = 1 + self.get_failures(request, credentials)\n\n        if failures_since_start > 1:\n            log.warning(\n                'AXES: Repeated login failure by %s. Count = %d of %d. Updating existing record in the cache.',\n                client_str,\n                failures_since_start,\n                settings.AXES_FAILURE_LIMIT,\n            )\n        else:\n            log.warning(\n                'AXES: New login failure by %s. Creating new record in the cache.',\n                client_str,\n            )\n\n        cache_key = get_client_cache_key(request, credentials)\n        self.cache.set(cache_key, failures_since_start, self.cache_timeout)\n\n        if failures_since_start >= settings.AXES_FAILURE_LIMIT:\n            log.warning('AXES: Locking out %s after repeated login failures.', client_str)\n\n            user_locked_out.send(\n                'axes',\n                request=request,\n                username=username,\n                ip_address=request.axes_ip_address,\n            )\n\n            raise AxesSignalPermissionDenied('Locked out due to repeated login failures.')", "response": "Called by AxesClient when user login fails."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_request(self, request: HttpRequest):\n\n        request.axes_attempt_time = now()\n        request.axes_ip_address = get_client_ip_address(request)\n        request.axes_user_agent = get_client_user_agent(request)\n        request.axes_path_info = get_client_path_info(request)\n        request.axes_http_accept = get_client_http_accept(request)", "response": "Update given request with necessary attributes\n        before passing it on the get_response for further processing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_allowed(self, request: AxesHttpRequest, credentials: dict = None) -> bool:\n\n        if self.is_blacklisted(request, credentials):\n            return False\n\n        if self.is_whitelisted(request, credentials):\n            return True\n\n        if self.is_locked(request, credentials):\n            return False\n\n        return True", "response": "Checks if the user is allowed to access the given Axes request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the request or given credentials are blacklisted from access.", "response": "def is_blacklisted(self, request: AxesHttpRequest, credentials: dict = None) -> bool:  # pylint: disable=unused-argument\n        \"\"\"\n        Checks if the request or given credentials are blacklisted from access.\n        \"\"\"\n\n        if is_client_ip_address_blacklisted(request):\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_whitelisted(self, request: AxesHttpRequest, credentials: dict = None) -> bool:  # pylint: disable=unused-argument\n\n        if is_client_ip_address_whitelisted(request):\n            return True\n\n        if is_client_method_whitelisted(request):\n            return True\n\n        return False", "response": "Checks if the request or given credentials are whitelisted for access."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the request or given credentials are locked.", "response": "def is_locked(self, request: AxesHttpRequest, credentials: dict = None) -> bool:\n        \"\"\"\n        Checks if the request or given credentials are locked.\n        \"\"\"\n\n        if settings.AXES_LOCK_OUT_AT_FAILURE:\n            return self.get_failures(request, credentials) >= settings.AXES_FAILURE_LIMIT\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(ip: str = None, username: str = None) -> int:\n\n    attempts = AccessAttempt.objects.all()\n\n    if ip:\n        attempts = attempts.filter(ip_address=ip)\n    if username:\n        attempts = attempts.filter(username=username)\n\n    count, _ = attempts.delete()\n    log.info('AXES: Reset %s access attempts from database.', count)\n\n    return count", "response": "Reset records that match IP or username and return the count of removed attempts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cool_off_threshold(attempt_time: datetime = None) -> datetime:\n\n    cool_off = get_cool_off()\n    if cool_off is None:\n        raise TypeError('Cool off threshold can not be calculated with settings.AXES_COOLOFF_TIME set to None')\n\n    if attempt_time is None:\n        return now() - cool_off\n    return attempt_time - cool_off", "response": "Get the cool off threshold for fetching access attempts from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters the list of AccessAttempts that match the given request and credentials.", "response": "def filter_user_attempts(request: AxesHttpRequest, credentials: dict = None) -> QuerySet:\n    \"\"\"\n    Return a queryset of AccessAttempts that match the given request and credentials.\n    \"\"\"\n\n    username = get_client_username(request, credentials)\n\n    filter_kwargs = get_client_parameters(username, request.axes_ip_address, request.axes_user_agent)\n\n    return AccessAttempt.objects.filter(**filter_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_attempts(request: AxesHttpRequest, credentials: dict = None) -> QuerySet:\n\n    attempts = filter_user_attempts(request, credentials)\n\n    if settings.AXES_COOLOFF_TIME is None:\n        log.debug('AXES: Getting all access attempts from database because no AXES_COOLOFF_TIME is configured')\n        return attempts\n\n    threshold = get_cool_off_threshold(request.axes_attempt_time)\n    log.debug('AXES: Getting access attempts that are newer than %s', threshold)\n    return attempts.filter(attempt_time__gte=threshold)", "response": "Get valid user attempts that match the given request and credentials."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean expired user attempts from the database.", "response": "def clean_expired_user_attempts(attempt_time: datetime = None) -> int:\n    \"\"\"\n    Clean expired user attempts from the database.\n    \"\"\"\n\n    if settings.AXES_COOLOFF_TIME is None:\n        log.debug('AXES: Skipping clean for expired access attempts because no AXES_COOLOFF_TIME is configured')\n        return 0\n\n    threshold = get_cool_off_threshold(attempt_time)\n    count, _ = AccessAttempt.objects.filter(attempt_time__lt=threshold).delete()\n    log.info('AXES: Cleaned up %s expired access attempts from database that were older than %s', count, threshold)\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_user_attempts(request: AxesHttpRequest, credentials: dict = None) -> int:\n\n    attempts = filter_user_attempts(request, credentials)\n\n    count, _ = attempts.delete()\n    log.info('AXES: Reset %s access attempts from database.', count)\n\n    return count", "response": "Reset all user attempts that match the given request and credentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the given request or credentials refer to a whitelisted username.", "response": "def is_user_attempt_whitelisted(request: AxesHttpRequest, credentials: dict = None) -> bool:\n    \"\"\"\n    Check if the given request or credentials refer to a whitelisted username.\n\n    A whitelisted user has the magic ``nolockout`` property set.\n\n    If the property is unknown or False or the user can not be found,\n    this implementation fails gracefully and returns True.\n    \"\"\"\n\n    username_field = getattr(get_user_model(), 'USERNAME_FIELD', 'username')\n    username_value = get_client_username(request, credentials)\n    kwargs = {\n        username_field: username_value\n    }\n\n    user_model = get_user_model()\n\n    try:\n        user = user_model.objects.get(**kwargs)\n        return user.nolockout\n    except (user_model.DoesNotExist, AttributeError):\n        pass\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_implementation(cls, force: bool = False) -> AxesHandler:\n\n        if force or not cls.implementation:\n            cls.implementation = import_string(settings.AXES_HANDLER)()\n        return cls.implementation", "response": "Fetch and initialize the AxesHandler implementation and memoize it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes Axes logging and show version information.", "response": "def initialize(cls):\n        \"\"\"\n        Initialize Axes logging and show version information.\n\n        This method is re-entrant and can be called multiple times.\n        It displays version information exactly once at application startup.\n        \"\"\"\n\n        if cls.logging_initialized:\n            return\n        cls.logging_initialized = True\n\n        if not settings.AXES_VERBOSE:\n            return\n\n        log.info('AXES: BEGIN LOG')\n        log.info('AXES: Using django-axes %s', get_version())\n\n        if settings.AXES_ONLY_USER_FAILURES:\n            log.info('AXES: blocking by username only.')\n        elif settings.AXES_LOCK_OUT_BY_COMBINATION_USER_AND_IP:\n            log.info('AXES: blocking by combination of username and IP.')\n        else:\n            log.info('AXES: blocking by IP only.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if user is allowed to log in and returns the user s identity.", "response": "def authenticate(self, request: AxesHttpRequest, username: str = None, password: str = None, **kwargs: dict):\n        \"\"\"\n        Checks user lockout status and raise a PermissionDenied if user is not allowed to log in.\n\n        This method interrupts the login flow and inserts  error message directly to the\n        ``response_context`` attribute that is supplied as a keyword argument.\n\n        :keyword response_context: kwarg that will be have its ``error`` attribute updated with context.\n        :raises AxesBackendRequestParameterRequired: if request parameter is not passed.\n        :raises AxesBackendPermissionDenied: if user is already locked out.\n        \"\"\"\n\n        if request is None:\n            raise AxesBackendRequestParameterRequired('AxesBackend requires a request as an argument to authenticate')\n\n        credentials = get_credentials(username=username, password=password, **kwargs)\n\n        if AxesProxyHandler.is_allowed(request, credentials):\n            return\n\n        # Locked out, don't try to authenticate, just update response_context and return.\n        # Its a bit weird to pass a context and expect a response value but its nice to get a \"why\" back.\n\n        error_msg = get_lockout_message()\n        response_context = kwargs.get('response_context', {})\n        response_context['error'] = error_msg\n\n        # Raise an error that stops the authentication flows at django.contrib.auth.authenticate.\n        # This error stops bubbling up at the authenticate call which catches backend PermissionDenied errors.\n        # After this error is caught by authenticate it emits a signal indicating user login failed,\n        # which is processed by axes.signals.log_user_login_failed which logs the attempt and raises\n        # a second exception which bubbles up the middleware stack and produces a HTTP 403 Forbidden reply\n        # in the axes.middleware.AxesMiddleware.process_exception middleware exception handler.\n\n        raise AxesBackendPermissionDenied('AxesBackend detected that the given user is locked out')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting grant_type and route to the designated handler.", "response": "def create_token_response(self, uri, http_method='GET', body=None,\n                              headers=None, credentials=None):\n        \"\"\"Extract grant_type and route to the designated handler.\"\"\"\n        django_request = headers.pop(\"Django-request-object\", None)\n        request = Request(\n            uri, http_method=http_method, body=body, headers=headers)\n        request.scopes = None\n        request.extra_credentials = credentials\n        request.django_request = django_request\n        grant_type_handler = self.grant_types.get(request.grant_type,\n                                                  self.default_grant_type_handler)\n        log.debug('Dispatching grant_type %s request to %r.',\n                  request.grant_type, grant_type_handler)\n        return grant_type_handler.create_token_response(\n            request, self.default_token_type)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef authenticate(self, request):\n        auth_header = get_authorization_header(request).decode(HTTP_HEADER_ENCODING)\n        auth = auth_header.split()\n\n        if not auth or auth[0].lower() != 'bearer':\n            return None\n\n        if len(auth) == 1:\n            msg = 'Invalid token header. No backend provided.'\n            raise exceptions.AuthenticationFailed(msg)\n        elif len(auth) == 2:\n            msg = 'Invalid token header. No credentials provided.'\n            raise exceptions.AuthenticationFailed(msg)\n        elif len(auth) > 3:\n            msg = 'Invalid token header. Token string should not contain spaces.'\n            raise exceptions.AuthenticationFailed(msg)\n\n        token = auth[2]\n        backend = auth[1]\n\n        strategy = load_strategy(request=request)\n\n        try:\n            backend = load_backend(strategy, backend, reverse(NAMESPACE + \":complete\", args=(backend,)))\n        except MissingBackend:\n            msg = 'Invalid token header. Invalid backend.'\n            raise exceptions.AuthenticationFailed(msg)\n\n        try:\n            user = backend.do_auth(access_token=token)\n        except requests.HTTPError as e:\n            msg = e.response.text\n            raise exceptions.AuthenticationFailed(msg)\n\n        if not user:\n            msg = 'Bad credentials.'\n            raise exceptions.AuthenticationFailed(msg)\n        return user, token", "response": "Authenticate the user with the given request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot live points ln weights and log - volume weights for each nested entry in the run.", "response": "def runplot(results, span=None, logplot=False, kde=True, nkde=1000,\n            color='blue', plot_kwargs=None, label_kwargs=None, lnz_error=True,\n            lnz_truth=None, truth_color='red', truth_kwargs=None,\n            max_x_ticks=8, max_y_ticks=3, use_math_text=True,\n            mark_final_live=True, fig=None):\n    \"\"\"\n    Plot live points, ln(likelihood), ln(weight), and ln(evidence)\n    as a function of ln(prior volume).\n\n    Parameters\n    ----------\n    results : :class:`~dynesty.results.Results` instance\n        A :class:`~dynesty.results.Results` instance from a nested\n        sampling run.\n\n    span : iterable with shape (4,), optional\n        A list where each element is either a length-2 tuple containing\n        lower and upper bounds *or* a float from `(0., 1.]` giving the\n        fraction below the maximum. If a fraction is provided,\n        the bounds are chosen to be equal-tailed. An example would be::\n\n            span = [(0., 10.), 0.001, 0.2, (5., 6.)]\n\n        Default is `(0., 1.05 * max(data))` for each element.\n\n    logplot : bool, optional\n        Whether to plot the evidence on a log scale. Default is `False`.\n\n    kde : bool, optional\n        Whether to use kernel density estimation to estimate and plot\n        the PDF of the importance weights as a function of log-volume\n        (as opposed to the importance weights themselves). Default is\n        `True`.\n\n    nkde : int, optional\n        The number of grid points used when plotting the kernel density\n        estimate. Default is `1000`.\n\n    color : str or iterable with shape (4,), optional\n        A `~matplotlib`-style color (either a single color or a different\n        value for each subplot) used when plotting the lines in each subplot.\n        Default is `'blue'`.\n\n    plot_kwargs : dict, optional\n        Extra keyword arguments that will be passed to `plot`.\n\n    label_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_xlabel` and\n        `~matplotlib.axes.Axes.set_ylabel` methods.\n\n    lnz_error : bool, optional\n        Whether to plot the 1, 2, and 3-sigma approximate error bars\n        derived from the ln(evidence) error approximation over the course\n        of the run. Default is `True`.\n\n    lnz_truth : float, optional\n        A reference value for the evidence that will be overplotted on the\n        evidence subplot if provided.\n\n    truth_color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color used when plotting :data:`lnz_truth`.\n        Default is `'red'`.\n\n    truth_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting\n        :data:`lnz_truth`.\n\n    max_x_ticks : int, optional\n        Maximum number of ticks allowed for the x axis. Default is `8`.\n\n    max_y_ticks : int, optional\n        Maximum number of ticks allowed for the y axis. Default is `4`.\n\n    use_math_text : bool, optional\n        Whether the axis tick labels for very large/small exponents should be\n        displayed as powers of 10 rather than using `e`. Default is `False`.\n\n    mark_final_live : bool, optional\n        Whether to indicate the final addition of recycled live points\n        (if they were added to the resulting samples) using\n        a dashed vertical line. Default is `True`.\n\n    fig : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`), optional\n        If provided, overplot the run onto the provided figure.\n        Otherwise, by default an internal figure is generated.\n\n    Returns\n    -------\n    runplot : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`)\n        Output summary plot.\n\n    \"\"\"\n\n    # Initialize values.\n    if label_kwargs is None:\n        label_kwargs = dict()\n    if plot_kwargs is None:\n        plot_kwargs = dict()\n    if truth_kwargs is None:\n        truth_kwargs = dict()\n\n    # Set defaults.\n    plot_kwargs['linewidth'] = plot_kwargs.get('linewidth', 5)\n    plot_kwargs['alpha'] = plot_kwargs.get('alpha', 0.7)\n    truth_kwargs['linestyle'] = truth_kwargs.get('linestyle', 'solid')\n    truth_kwargs['linewidth'] = truth_kwargs.get('linewidth', 3)\n\n    # Extract results.\n    niter = results['niter']  # number of iterations\n    logvol = results['logvol']  # ln(prior volume)\n    logl = results['logl'] - max(results['logl'])  # ln(normalized likelihood)\n    logwt = results['logwt'] - results['logz'][-1]  # ln(importance weight)\n    logz = results['logz']  # ln(evidence)\n    logzerr = results['logzerr']  # error in ln(evidence)\n    logzerr[~np.isfinite(logzerr)] = 0.\n    nsamps = len(logwt)  # number of samples\n\n    # Check whether the run was \"static\" or \"dynamic\".\n    try:\n        nlive = results['samples_n']\n        mark_final_live = False\n    except:\n        nlive = np.ones(niter) * results['nlive']\n        if nsamps - niter == results['nlive']:\n            nlive_final = np.arange(1, results['nlive'] + 1)[::-1]\n            nlive = np.append(nlive, nlive_final)\n\n    # Check if the final set of live points were added to the results.\n    if mark_final_live:\n        if nsamps - niter == results['nlive']:\n            live_idx = niter\n        else:\n            warnings.warn(\"The number of iterations and samples differ \"\n                          \"by an amount that isn't the number of final \"\n                          \"live points. `mark_final_live` has been disabled.\")\n            mark_final_live = False\n\n    # Determine plotting bounds for each subplot.\n    data = [nlive, np.exp(logl), np.exp(logwt), np.exp(logz)]\n    if kde:\n        # Derive kernel density estimate.\n        wt_kde = gaussian_kde(resample_equal(-logvol, data[2]))  # KDE\n        logvol_new = np.linspace(logvol[0], logvol[-1], nkde)  # resample\n        data[2] = wt_kde.pdf(-logvol_new)  # evaluate KDE PDF\n    if span is None:\n        span = [(0., 1.05 * max(d)) for d in data]\n        no_span = True\n    else:\n        no_span = False\n    span = list(span)\n    if len(span) != 4:\n        raise ValueError(\"More bounds provided in `span` than subplots!\")\n    for i, _ in enumerate(span):\n        try:\n            ymin, ymax = span[i]\n        except:\n            span[i] = (max(data[i]) * span[i], max(data[i]))\n    if lnz_error and no_span:\n        if logplot:\n            zspan = (np.exp(logz[-1] - 1.3 * 3. * logzerr[-1]),\n                     np.exp(logz[-1] + 1.3 * 3. * logzerr[-1]))\n        else:\n            zspan = (0., 1.05 * np.exp(logz[-1] + 3. * logzerr[-1]))\n        span[3] = zspan\n\n    # Setting up default plot layout.\n    if fig is None:\n        fig, axes = pl.subplots(4, 1, figsize=(16, 16))\n        xspan = [(0., -min(logvol)) for _ax in axes]\n        yspan = span\n    else:\n        fig, axes = fig\n        try:\n            axes.reshape(4, 1)\n        except:\n            raise ValueError(\"Provided axes do not match the required shape \"\n                             \"for plotting samples.\")\n        # If figure is provided, keep previous bounds if they were larger.\n        xspan = [ax.get_xlim() for ax in axes]\n        yspan = [ax.get_ylim() for ax in axes]\n        # One exception: if the bounds are the plotting default `(0., 1.)`,\n        # overwrite them.\n        xspan = [t if t != (0., 1.) else (None, None) for t in xspan]\n        yspan = [t if t != (0., 1.) else (None, None) for t in yspan]\n\n    # Set up bounds for plotting.\n    for i in range(4):\n        if xspan[i][0] is None:\n            xmin = None\n        else:\n            xmin = min(0., xspan[i][0])\n        if xspan[i][1] is None:\n            xmax = -min(logvol)\n        else:\n            xmax = max(-min(logvol), xspan[i][1])\n        if yspan[i][0] is None:\n            ymin = None\n        else:\n            ymin = min(span[i][0], yspan[i][0])\n        if yspan[i][1] is None:\n            ymax = span[i][1]\n        else:\n            ymax = max(span[i][1], yspan[i][1])\n        axes[i].set_xlim([xmin, xmax])\n        axes[i].set_ylim([ymin, ymax])\n\n    # Plotting.\n    labels = ['Live Points', 'Likelihood\\n(normalized)',\n              'Importance\\nWeight', 'Evidence']\n    if kde:\n        labels[2] += ' PDF'\n\n    for i, d in enumerate(data):\n\n        # Establish axes.\n        ax = axes[i]\n        # Set color(s)/colormap(s).\n        if isinstance(color, str_type):\n            c = color\n        else:\n            c = color[i]\n        # Setup axes.\n        if max_x_ticks == 0:\n            ax.xaxis.set_major_locator(NullLocator())\n        else:\n            ax.xaxis.set_major_locator(MaxNLocator(max_x_ticks))\n        if max_y_ticks == 0:\n            ax.yaxis.set_major_locator(NullLocator())\n        else:\n            ax.yaxis.set_major_locator(MaxNLocator(max_y_ticks))\n        # Label axes.\n        sf = ScalarFormatter(useMathText=use_math_text)\n        ax.yaxis.set_major_formatter(sf)\n        ax.set_xlabel(r\"$-\\ln X$\", **label_kwargs)\n        ax.set_ylabel(labels[i], **label_kwargs)\n        # Plot run.\n        if logplot and i == 3:\n            ax.semilogy(-logvol, d, color=c, **plot_kwargs)\n            yspan = [ax.get_ylim() for _ax in axes]\n        elif kde and i == 2:\n            ax.plot(-logvol_new, d, color=c, **plot_kwargs)\n        else:\n            ax.plot(-logvol, d, color=c, **plot_kwargs)\n        if i == 3 and lnz_error:\n            [ax.fill_between(-logvol, np.exp(logz + s*logzerr),\n                             np.exp(logz - s*logzerr), color=c, alpha=0.2)\n             for s in range(1, 4)]\n        # Mark addition of final live points.\n        if mark_final_live:\n            ax.axvline(-logvol[live_idx], color=c, ls=\"dashed\", lw=2,\n                       **plot_kwargs)\n            if i == 0:\n                ax.axhline(live_idx, color=c, ls=\"dashed\", lw=2,\n                           **plot_kwargs)\n        # Add truth value(s).\n        if i == 3 and lnz_truth is not None:\n            ax.axhline(np.exp(lnz_truth), color=truth_color, **truth_kwargs)\n\n    return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot traces and marginalized posteriors for each parameter.", "response": "def traceplot(results, span=None, quantiles=[0.025, 0.5, 0.975], smooth=0.02,\n              post_color='blue', post_kwargs=None, kde=True, nkde=1000,\n              trace_cmap='plasma', trace_color=None, trace_kwargs=None,\n              connect=False, connect_highlight=10, connect_color='red',\n              connect_kwargs=None, max_n_ticks=5, use_math_text=False,\n              labels=None, label_kwargs=None,\n              show_titles=False, title_fmt=\".2f\", title_kwargs=None,\n              truths=None, truth_color='red', truth_kwargs=None,\n              verbose=False, fig=None):\n    \"\"\"\n    Plot traces and marginalized posteriors for each parameter.\n\n    Parameters\n    ----------\n    results : :class:`~dynesty.results.Results` instance\n        A :class:`~dynesty.results.Results` instance from a nested\n        sampling run. **Compatible with results derived from**\n        `nestle <http://kylebarbary.com/nestle/>`_.\n\n    span : iterable with shape (ndim,), optional\n        A list where each element is either a length-2 tuple containing\n        lower and upper bounds or a float from `(0., 1.]` giving the\n        fraction of (weighted) samples to include. If a fraction is provided,\n        the bounds are chosen to be equal-tailed. An example would be::\n\n            span = [(0., 10.), 0.95, (5., 6.)]\n\n        Default is `0.999999426697` (5-sigma credible interval) for each\n        parameter.\n\n    quantiles : iterable, optional\n        A list of fractional quantiles to overplot on the 1-D marginalized\n        posteriors as vertical dashed lines. Default is `[0.025, 0.5, 0.975]`\n        (the 95%/2-sigma credible interval).\n\n    smooth : float or iterable with shape (ndim,), optional\n        The standard deviation (either a single value or a different value for\n        each subplot) for the Gaussian kernel used to smooth the 1-D\n        marginalized posteriors, expressed as a fraction of the span.\n        Default is `0.02` (2% smoothing). If an integer is provided instead,\n        this will instead default to a simple (weighted) histogram with\n        `bins=smooth`.\n\n    post_color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color (either a single color or a different\n        value for each subplot) used when plotting the histograms.\n        Default is `'blue'`.\n\n    post_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting the\n        marginalized 1-D posteriors.\n\n    kde : bool, optional\n        Whether to use kernel density estimation to estimate and plot\n        the PDF of the importance weights as a function of log-volume\n        (as opposed to the importance weights themselves). Default is\n        `True`.\n\n    nkde : int, optional\n        The number of grid points used when plotting the kernel density\n        estimate. Default is `1000`.\n\n    trace_cmap : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style colormap (either a single colormap or a\n        different colormap for each subplot) used when plotting the traces,\n        where each point is colored according to its weight. Default is\n        `'plasma'`.\n\n    trace_color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color (either a single color or a\n        different color for each subplot) used when plotting the traces.\n        This overrides the `trace_cmap` option by giving all points\n        the same color. Default is `None` (not used).\n\n    trace_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting the traces.\n\n    connect : bool, optional\n        Whether to draw lines connecting the paths of unique particles.\n        Default is `False`.\n\n    connect_highlight : int or iterable, optional\n        If `connect=True`, highlights the paths of a specific set of\n        particles. If an integer is passed, :data:`connect_highlight`\n        random particle paths will be highlighted. If an iterable is passed,\n        then the particle paths corresponding to the provided indices\n        will be highlighted.\n\n    connect_color : str, optional\n        The color of the highlighted particle paths. Default is `'red'`.\n\n    connect_kwargs : dict, optional\n        Extra keyword arguments used for plotting particle paths.\n\n    max_n_ticks : int, optional\n        Maximum number of ticks allowed. Default is `5`.\n\n    use_math_text : bool, optional\n        Whether the axis tick labels for very large/small exponents should be\n        displayed as powers of 10 rather than using `e`. Default is `False`.\n\n    labels : iterable with shape (ndim,), optional\n        A list of names for each parameter. If not provided, the default name\n        used when plotting will follow :math:`x_i` style.\n\n    label_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_xlabel` and\n        `~matplotlib.axes.Axes.set_ylabel` methods.\n\n    show_titles : bool, optional\n        Whether to display a title above each 1-D marginalized posterior\n        showing the 0.5 quantile along with the upper/lower bounds associated\n        with the 0.025 and 0.975 (95%/2-sigma credible interval) quantiles.\n        Default is `True`.\n\n    title_fmt : str, optional\n        The format string for the quantiles provided in the title. Default is\n        `'.2f'`.\n\n    title_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_title` command.\n\n    truths : iterable with shape (ndim,), optional\n        A list of reference values that will be overplotted on the traces and\n        marginalized 1-D posteriors as solid horizontal/vertical lines.\n        Individual values can be exempt using `None`. Default is `None`.\n\n    truth_color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color (either a single color or a different\n        value for each subplot) used when plotting `truths`.\n        Default is `'red'`.\n\n    truth_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting the vertical\n        and horizontal lines with `truths`.\n\n    verbose : bool, optional\n        Whether to print the values of the computed quantiles associated with\n        each parameter. Default is `False`.\n\n    fig : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`), optional\n        If provided, overplot the traces and marginalized 1-D posteriors\n        onto the provided figure. Otherwise, by default an\n        internal figure is generated.\n\n    Returns\n    -------\n    traceplot : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`)\n        Output trace plot.\n\n    \"\"\"\n\n    # Initialize values.\n    if title_kwargs is None:\n        title_kwargs = dict()\n    if label_kwargs is None:\n        label_kwargs = dict()\n    if trace_kwargs is None:\n        trace_kwargs = dict()\n    if connect_kwargs is None:\n        connect_kwargs = dict()\n    if post_kwargs is None:\n        post_kwargs = dict()\n    if truth_kwargs is None:\n        truth_kwargs = dict()\n\n    # Set defaults.\n    connect_kwargs['alpha'] = connect_kwargs.get('alpha', 0.7)\n    post_kwargs['alpha'] = post_kwargs.get('alpha', 0.6)\n    trace_kwargs['s'] = trace_kwargs.get('s', 3)\n    truth_kwargs['linestyle'] = truth_kwargs.get('linestyle', 'solid')\n    truth_kwargs['linewidth'] = truth_kwargs.get('linewidth', 2)\n\n    # Extract weighted samples.\n    samples = results['samples']\n    logvol = results['logvol']\n    try:\n        weights = np.exp(results['logwt'] - results['logz'][-1])\n    except:\n        weights = results['weights']\n    if kde:\n        # Derive kernel density estimate.\n        wt_kde = gaussian_kde(resample_equal(-logvol, weights))  # KDE\n        logvol_grid = np.linspace(logvol[0], logvol[-1], nkde)  # resample\n        wt_grid = wt_kde.pdf(-logvol_grid)  # evaluate KDE PDF\n        wts = np.interp(-logvol, -logvol_grid, wt_grid)  # interpolate\n    else:\n        wts = weights\n\n    # Deal with 1D results. A number of extra catches are also here\n    # in case users are trying to plot other results besides the `Results`\n    # instance generated by `dynesty`.\n    samples = np.atleast_1d(samples)\n    if len(samples.shape) == 1:\n        samples = np.atleast_2d(samples)\n    else:\n        assert len(samples.shape) == 2, \"Samples must be 1- or 2-D.\"\n        samples = samples.T\n    assert samples.shape[0] <= samples.shape[1], \"There are more \" \\\n                                                 \"dimensions than samples!\"\n    ndim, nsamps = samples.shape\n\n    # Check weights.\n    if weights.ndim != 1:\n        raise ValueError(\"Weights must be 1-D.\")\n    if nsamps != weights.shape[0]:\n        raise ValueError(\"The number of weights and samples disagree!\")\n\n    # Check ln(volume).\n    if logvol.ndim != 1:\n        raise ValueError(\"Ln(volume)'s must be 1-D.\")\n    if nsamps != logvol.shape[0]:\n        raise ValueError(\"The number of ln(volume)'s and samples disagree!\")\n\n    # Check sample IDs.\n    if connect:\n        try:\n            samples_id = results['samples_id']\n            uid = np.unique(samples_id)\n        except:\n            raise ValueError(\"Sample IDs are not defined!\")\n        try:\n            ids = connect_highlight[0]\n            ids = connect_highlight\n        except:\n            ids = np.random.choice(uid, size=connect_highlight, replace=False)\n\n    # Determine plotting bounds for marginalized 1-D posteriors.\n    if span is None:\n        span = [0.999999426697 for i in range(ndim)]\n    span = list(span)\n    if len(span) != ndim:\n        raise ValueError(\"Dimension mismatch between samples and span.\")\n    for i, _ in enumerate(span):\n        try:\n            xmin, xmax = span[i]\n        except:\n            q = [0.5 - 0.5 * span[i], 0.5 + 0.5 * span[i]]\n            span[i] = _quantile(samples[i], q, weights=weights)\n\n    # Setting up labels.\n    if labels is None:\n        labels = [r\"$x_{\"+str(i+1)+\"}$\" for i in range(ndim)]\n\n    # Setting up smoothing.\n    if (isinstance(smooth, int_type) or isinstance(smooth, float_type)):\n        smooth = [smooth for i in range(ndim)]\n\n    # Setting up default plot layout.\n    if fig is None:\n        fig, axes = pl.subplots(ndim, 2, figsize=(12, 3*ndim))\n    else:\n        fig, axes = fig\n        try:\n            axes.reshape(ndim, 2)\n        except:\n            raise ValueError(\"Provided axes do not match the required shape \"\n                             \"for plotting samples.\")\n\n    # Plotting.\n    for i, x in enumerate(samples):\n\n        # Plot trace.\n\n        # Establish axes.\n        if np.shape(samples)[0] == 1:\n            ax = axes[1]\n        else:\n            ax = axes[i, 0]\n        # Set color(s)/colormap(s).\n        if trace_color is not None:\n            if isinstance(trace_color, str_type):\n                color = trace_color\n            else:\n                color = trace_color[i]\n        else:\n            color = wts\n        if isinstance(trace_cmap, str_type):\n            cmap = trace_cmap\n        else:\n            cmap = trace_cmap[i]\n        # Setup axes.\n        ax.set_xlim([0., -min(logvol)])\n        ax.set_ylim([min(x), max(x)])\n        if max_n_ticks == 0:\n            ax.xaxis.set_major_locator(NullLocator())\n            ax.yaxis.set_major_locator(NullLocator())\n        else:\n            ax.xaxis.set_major_locator(MaxNLocator(max_n_ticks))\n            ax.yaxis.set_major_locator(MaxNLocator(max_n_ticks))\n        # Label axes.\n        sf = ScalarFormatter(useMathText=use_math_text)\n        ax.yaxis.set_major_formatter(sf)\n        ax.set_xlabel(r\"$-\\ln X$\", **label_kwargs)\n        ax.set_ylabel(labels[i], **label_kwargs)\n        # Generate scatter plot.\n        ax.scatter(-logvol, x, c=color, cmap=cmap, **trace_kwargs)\n        if connect:\n            # Add lines highlighting specific particle paths.\n            for j in ids:\n                sel = (samples_id == j)\n                ax.plot(-logvol[sel], x[sel], color=connect_color,\n                        **connect_kwargs)\n        # Add truth value(s).\n        if truths is not None and truths[i] is not None:\n            try:\n                [ax.axhline(t, color=truth_color, **truth_kwargs)\n                 for t in truths[i]]\n            except:\n                ax.axhline(truths[i], color=truth_color, **truth_kwargs)\n\n        # Plot marginalized 1-D posterior.\n\n        # Establish axes.\n        if np.shape(samples)[0] == 1:\n            ax = axes[0]\n        else:\n            ax = axes[i, 1]\n        # Set color(s).\n        if isinstance(post_color, str_type):\n            color = post_color\n        else:\n            color = post_color[i]\n        # Setup axes\n        ax.set_xlim(span[i])\n        if max_n_ticks == 0:\n            ax.xaxis.set_major_locator(NullLocator())\n            ax.yaxis.set_major_locator(NullLocator())\n        else:\n            ax.xaxis.set_major_locator(MaxNLocator(max_n_ticks))\n            ax.yaxis.set_major_locator(NullLocator())\n        # Label axes.\n        sf = ScalarFormatter(useMathText=use_math_text)\n        ax.xaxis.set_major_formatter(sf)\n        ax.set_xlabel(labels[i], **label_kwargs)\n        # Generate distribution.\n        s = smooth[i]\n        if isinstance(s, int_type):\n            # If `s` is an integer, plot a weighted histogram with\n            # `s` bins within the provided bounds.\n            n, b, _ = ax.hist(x, bins=s, weights=weights, color=color,\n                              range=np.sort(span[i]), **post_kwargs)\n            x0 = np.array(list(zip(b[:-1], b[1:]))).flatten()\n            y0 = np.array(list(zip(n, n))).flatten()\n        else:\n            # If `s` is a float, oversample the data relative to the\n            # smoothing filter by a factor of 10, then use a Gaussian\n            # filter to smooth the results.\n            bins = int(round(10. / s))\n            n, b = np.histogram(x, bins=bins, weights=weights,\n                                range=np.sort(span[i]))\n            n = norm_kde(n, 10.)\n            x0 = 0.5 * (b[1:] + b[:-1])\n            y0 = n\n            ax.fill_between(x0, y0, color=color, **post_kwargs)\n        ax.set_ylim([0., max(y0) * 1.05])\n        # Plot quantiles.\n        if quantiles is not None and len(quantiles) > 0:\n            qs = _quantile(x, quantiles, weights=weights)\n            for q in qs:\n                ax.axvline(q, lw=2, ls=\"dashed\", color=color)\n            if verbose:\n                print(\"Quantiles:\")\n                print(labels[i], [blob for blob in zip(quantiles, qs)])\n        # Add truth value(s).\n        if truths is not None and truths[i] is not None:\n            try:\n                [ax.axvline(t, color=truth_color, **truth_kwargs)\n                 for t in truths[i]]\n            except:\n                ax.axvline(truths[i], color=truth_color, **truth_kwargs)\n        # Set titles.\n        if show_titles:\n            title = None\n            if title_fmt is not None:\n                ql, qm, qh = _quantile(x, [0.025, 0.5, 0.975], weights=weights)\n                q_minus, q_plus = qm - ql, qh - qm\n                fmt = \"{{0:{0}}}\".format(title_fmt).format\n                title = r\"${{{0}}}_{{-{1}}}^{{+{2}}}$\"\n                title = title.format(fmt(qm), fmt(q_minus), fmt(q_plus))\n                title = \"{0} = {1}\".format(labels[i], title)\n                ax.set_title(title, **title_kwargs)\n\n    return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cornerpoints(results, thin=1, span=None, cmap='plasma', color=None,\n                 kde=True, nkde=1000, plot_kwargs=None, labels=None,\n                 label_kwargs=None, truths=None, truth_color='red',\n                 truth_kwargs=None, max_n_ticks=5, use_math_text=False,\n                 fig=None):\n    \"\"\"\n    Generate a (sub-)corner plot of (weighted) samples.\n\n    Parameters\n    ----------\n    results : :class:`~dynesty.results.Results` instance\n        A :class:`~dynesty.results.Results` instance from a nested\n        sampling run. **Compatible with results derived from**\n        `nestle <http://kylebarbary.com/nestle/>`_.\n\n    thin : int, optional\n        Thin the samples so that only each `thin`-th sample is plotted.\n        Default is `1` (no thinning).\n\n    span : iterable with shape (ndim,), optional\n        A list where each element is either a length-2 tuple containing\n        lower and upper bounds or a float from `(0., 1.]` giving the\n        fraction of (weighted) samples to include. If a fraction is provided,\n        the bounds are chosen to be equal-tailed. An example would be::\n\n            span = [(0., 10.), 0.95, (5., 6.)]\n\n        Default is `1.` for all parameters (no bound).\n\n    cmap : str, optional\n        A `~matplotlib`-style colormap used when plotting the points,\n        where each point is colored according to its weight. Default is\n        `'plasma'`.\n\n    color : str, optional\n        A `~matplotlib`-style color used when plotting the points.\n        This overrides the `cmap` option by giving all points\n        the same color. Default is `None` (not used).\n\n    kde : bool, optional\n        Whether to use kernel density estimation to estimate and plot\n        the PDF of the importance weights as a function of log-volume\n        (as opposed to the importance weights themselves). Default is\n        `True`.\n\n    nkde : int, optional\n        The number of grid points used when plotting the kernel density\n        estimate. Default is `1000`.\n\n    plot_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting the points.\n\n    labels : iterable with shape (ndim,), optional\n        A list of names for each parameter. If not provided, the default name\n        used when plotting will follow :math:`x_i` style.\n\n    label_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_xlabel` and\n        `~matplotlib.axes.Axes.set_ylabel` methods.\n\n    truths : iterable with shape (ndim,), optional\n        A list of reference values that will be overplotted on the traces and\n        marginalized 1-D posteriors as solid horizontal/vertical lines.\n        Individual values can be exempt using `None`. Default is `None`.\n\n    truth_color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color (either a single color or a different\n        value for each subplot) used when plotting `truths`.\n        Default is `'red'`.\n\n    truth_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting the vertical\n        and horizontal lines with `truths`.\n\n    max_n_ticks : int, optional\n        Maximum number of ticks allowed. Default is `5`.\n\n    use_math_text : bool, optional\n        Whether the axis tick labels for very large/small exponents should be\n        displayed as powers of 10 rather than using `e`. Default is `False`.\n\n    fig : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`), optional\n        If provided, overplot the points onto the provided figure object.\n        Otherwise, by default an internal figure is generated.\n\n    Returns\n    -------\n    cornerpoints : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`)\n        Output (sub-)corner plot of (weighted) samples.\n\n    \"\"\"\n\n    # Initialize values.\n    if truth_kwargs is None:\n        truth_kwargs = dict()\n    if label_kwargs is None:\n        label_kwargs = dict()\n    if plot_kwargs is None:\n        plot_kwargs = dict()\n\n    # Set defaults.\n    plot_kwargs['s'] = plot_kwargs.get('s', 1)\n    truth_kwargs['linestyle'] = truth_kwargs.get('linestyle', 'solid')\n    truth_kwargs['linewidth'] = truth_kwargs.get('linewidth', 2)\n    truth_kwargs['alpha'] = truth_kwargs.get('alpha', 0.7)\n\n    # Extract weighted samples.\n    samples = results['samples']\n    logvol = results['logvol']\n    try:\n        weights = np.exp(results['logwt'] - results['logz'][-1])\n    except:\n        weights = results['weights']\n    if kde:\n        # Derive kernel density estimate.\n        wt_kde = gaussian_kde(resample_equal(-logvol, weights))  # KDE\n        logvol_grid = np.linspace(logvol[0], logvol[-1], nkde)  # resample\n        wt_grid = wt_kde.pdf(-logvol_grid)  # evaluate KDE PDF\n        weights = np.interp(-logvol, -logvol_grid, wt_grid)  # interpolate\n\n    # Deal with 1D results. A number of extra catches are also here\n    # in case users are trying to plot other results besides the `Results`\n    # instance generated by `dynesty`.\n    samples = np.atleast_1d(samples)\n    if len(samples.shape) == 1:\n        samples = np.atleast_2d(samples)\n    else:\n        assert len(samples.shape) == 2, \"Samples must be 1- or 2-D.\"\n        samples = samples.T\n    assert samples.shape[0] <= samples.shape[1], \"There are more \" \\\n                                                 \"dimensions than samples!\"\n    ndim, nsamps = samples.shape\n\n    # Check weights.\n    if weights.ndim != 1:\n        raise ValueError(\"Weights must be 1-D.\")\n    if nsamps != weights.shape[0]:\n        raise ValueError(\"The number of weights and samples disagree!\")\n\n    # Determine plotting bounds.\n    if span is not None:\n        if len(span) != ndim:\n            raise ValueError(\"Dimension mismatch between samples and span.\")\n        for i, _ in enumerate(span):\n            try:\n                xmin, xmax = span[i]\n            except:\n                q = [0.5 - 0.5 * span[i], 0.5 + 0.5 * span[i]]\n                span[i] = _quantile(samples[i], q, weights=weights)\n\n    # Set labels\n    if labels is None:\n        labels = [r\"$x_{\"+str(i+1)+\"}$\" for i in range(ndim)]\n\n    # Set colormap.\n    if color is None:\n        color = weights\n\n    # Setup axis layout (from `corner.py`).\n    factor = 2.0  # size of side of one panel\n    lbdim = 0.5 * factor  # size of left/bottom margin\n    trdim = 0.2 * factor  # size of top/right margin\n    whspace = 0.05  # size of width/height margin\n    plotdim = factor * (ndim - 1.) + factor * (ndim - 2.) * whspace\n    dim = lbdim + plotdim + trdim  # total size\n\n    # Initialize figure.\n    if fig is None:\n        fig, axes = pl.subplots(ndim - 1, ndim - 1, figsize=(dim, dim))\n    else:\n        try:\n            fig, axes = fig\n            axes = np.array(axes).reshape((ndim - 1, ndim - 1))\n        except:\n            raise ValueError(\"Mismatch between axes and dimension.\")\n\n    # Format figure.\n    lb = lbdim / dim\n    tr = (lbdim + plotdim) / dim\n    fig.subplots_adjust(left=lb, bottom=lb, right=tr, top=tr,\n                        wspace=whspace, hspace=whspace)\n\n    # Plot the 2-D projected samples.\n    for i, x in enumerate(samples[1:]):\n        for j, y in enumerate(samples[:-1]):\n            try:\n                ax = axes[i, j]\n            except:\n                ax = axes\n            # Setup axes.\n            if span is not None:\n                ax.set_xlim(span[j])\n                ax.set_ylim(span[i])\n            if j > i:\n                ax.set_frame_on(False)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                continue\n            if max_n_ticks == 0:\n                ax.xaxis.set_major_locator(NullLocator())\n                ax.yaxis.set_major_locator(NullLocator())\n            else:\n                ax.xaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                       prune=\"lower\"))\n                ax.yaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                       prune=\"lower\"))\n            # Label axes.\n            sf = ScalarFormatter(useMathText=use_math_text)\n            ax.xaxis.set_major_formatter(sf)\n            ax.yaxis.set_major_formatter(sf)\n            if i < ndim - 2:\n                ax.set_xticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_xticklabels()]\n                ax.set_xlabel(labels[j], **label_kwargs)\n                ax.xaxis.set_label_coords(0.5, -0.3)\n            if j > 0:\n                ax.set_yticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_yticklabels()]\n                ax.set_ylabel(labels[i+1], **label_kwargs)\n                ax.yaxis.set_label_coords(-0.3, 0.5)\n            # Plot distribution.\n            in_bounds = np.ones_like(y).astype('bool')\n            if span is not None and span[i] is not None:\n                in_bounds *= ((x >= span[i][0]) & (x <= span[i][1]))\n            if span is not None and span[j] is not None:\n                in_bounds *= ((y >= span[j][0]) & (y <= span[j][1]))\n            ax.scatter(y[in_bounds][::thin], x[in_bounds][::thin],\n                       c=color, cmap=cmap, **plot_kwargs)\n            # Add truth values\n            if truths is not None:\n                if truths[j] is not None:\n                    try:\n                        [ax.axvline(t, color=truth_color,  **truth_kwargs)\n                         for t in truths[j]]\n                    except:\n                        ax.axvline(truths[j], color=truth_color,\n                                   **truth_kwargs)\n                if truths[i+1] is not None:\n                    try:\n                        [ax.axhline(t, color=truth_color, **truth_kwargs)\n                         for t in truths[i+1]]\n                    except:\n                        ax.axhline(truths[i+1], color=truth_color,\n                                   **truth_kwargs)\n\n    return (fig, axes)", "response": "Generates a sub - corner plot of weighted samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a corner plot of the 1 - D and 2 - D marginalized posteriors.", "response": "def cornerplot(results, span=None, quantiles=[0.025, 0.5, 0.975],\n               color='black', smooth=0.02, hist_kwargs=None,\n               hist2d_kwargs=None, labels=None, label_kwargs=None,\n               show_titles=False, title_fmt=\".2f\", title_kwargs=None,\n               truths=None, truth_color='red', truth_kwargs=None,\n               max_n_ticks=5, top_ticks=False, use_math_text=False,\n               verbose=False, fig=None):\n    \"\"\"\n    Generate a corner plot of the 1-D and 2-D marginalized posteriors.\n\n    Parameters\n    ----------\n    results : :class:`~dynesty.results.Results` instance\n        A :class:`~dynesty.results.Results` instance from a nested\n        sampling run. **Compatible with results derived from**\n        `nestle <http://kylebarbary.com/nestle/>`_.\n\n    span : iterable with shape (ndim,), optional\n        A list where each element is either a length-2 tuple containing\n        lower and upper bounds or a float from `(0., 1.]` giving the\n        fraction of (weighted) samples to include. If a fraction is provided,\n        the bounds are chosen to be equal-tailed. An example would be::\n\n            span = [(0., 10.), 0.95, (5., 6.)]\n\n        Default is `0.999999426697` (5-sigma credible interval).\n\n    quantiles : iterable, optional\n        A list of fractional quantiles to overplot on the 1-D marginalized\n        posteriors as vertical dashed lines. Default is `[0.025, 0.5, 0.975]`\n        (spanning the 95%/2-sigma credible interval).\n\n    color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color (either a single color or a different\n        value for each subplot) used when plotting the histograms.\n        Default is `'black'`.\n\n    smooth : float or iterable with shape (ndim,), optional\n        The standard deviation (either a single value or a different value for\n        each subplot) for the Gaussian kernel used to smooth the 1-D and 2-D\n        marginalized posteriors, expressed as a fraction of the span.\n        Default is `0.02` (2% smoothing). If an integer is provided instead,\n        this will instead default to a simple (weighted) histogram with\n        `bins=smooth`.\n\n    hist_kwargs : dict, optional\n        Extra keyword arguments to send to the 1-D (smoothed) histograms.\n\n    hist2d_kwargs : dict, optional\n        Extra keyword arguments to send to the 2-D (smoothed) histograms.\n\n    labels : iterable with shape (ndim,), optional\n        A list of names for each parameter. If not provided, the default name\n        used when plotting will follow :math:`x_i` style.\n\n    label_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_xlabel` and\n        `~matplotlib.axes.Axes.set_ylabel` methods.\n\n    show_titles : bool, optional\n        Whether to display a title above each 1-D marginalized posterior\n        showing the 0.5 quantile along with the upper/lower bounds associated\n        with the 0.025 and 0.975 (95%/2-sigma credible interval) quantiles.\n        Default is `True`.\n\n    title_fmt : str, optional\n        The format string for the quantiles provided in the title. Default is\n        `'.2f'`.\n\n    title_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_title` command.\n\n    truths : iterable with shape (ndim,), optional\n        A list of reference values that will be overplotted on the traces and\n        marginalized 1-D posteriors as solid horizontal/vertical lines.\n        Individual values can be exempt using `None`. Default is `None`.\n\n    truth_color : str or iterable with shape (ndim,), optional\n        A `~matplotlib`-style color (either a single color or a different\n        value for each subplot) used when plotting `truths`.\n        Default is `'red'`.\n\n    truth_kwargs : dict, optional\n        Extra keyword arguments that will be used for plotting the vertical\n        and horizontal lines with `truths`.\n\n    max_n_ticks : int, optional\n        Maximum number of ticks allowed. Default is `5`.\n\n    top_ticks : bool, optional\n        Whether to label the top (rather than bottom) ticks. Default is\n        `False`.\n\n    use_math_text : bool, optional\n        Whether the axis tick labels for very large/small exponents should be\n        displayed as powers of 10 rather than using `e`. Default is `False`.\n\n    verbose : bool, optional\n        Whether to print the values of the computed quantiles associated with\n        each parameter. Default is `False`.\n\n    fig : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`), optional\n        If provided, overplot the traces and marginalized 1-D posteriors\n        onto the provided figure. Otherwise, by default an\n        internal figure is generated.\n\n    Returns\n    -------\n    cornerplot : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`)\n        Output corner plot.\n\n    \"\"\"\n\n    # Initialize values.\n    if quantiles is None:\n        quantiles = []\n    if truth_kwargs is None:\n        truth_kwargs = dict()\n    if label_kwargs is None:\n        label_kwargs = dict()\n    if title_kwargs is None:\n        title_kwargs = dict()\n    if hist_kwargs is None:\n        hist_kwargs = dict()\n    if hist2d_kwargs is None:\n        hist2d_kwargs = dict()\n\n    # Set defaults.\n    hist_kwargs['alpha'] = hist_kwargs.get('alpha', 0.6)\n    hist2d_kwargs['alpha'] = hist2d_kwargs.get('alpha', 0.6)\n    truth_kwargs['linestyle'] = truth_kwargs.get('linestyle', 'solid')\n    truth_kwargs['linewidth'] = truth_kwargs.get('linewidth', 2)\n    truth_kwargs['alpha'] = truth_kwargs.get('alpha', 0.7)\n\n    # Extract weighted samples.\n    samples = results['samples']\n    try:\n        weights = np.exp(results['logwt'] - results['logz'][-1])\n    except:\n        weights = results['weights']\n\n    # Deal with 1D results. A number of extra catches are also here\n    # in case users are trying to plot other results besides the `Results`\n    # instance generated by `dynesty`.\n    samples = np.atleast_1d(samples)\n    if len(samples.shape) == 1:\n        samples = np.atleast_2d(samples)\n    else:\n        assert len(samples.shape) == 2, \"Samples must be 1- or 2-D.\"\n        samples = samples.T\n    assert samples.shape[0] <= samples.shape[1], \"There are more \" \\\n                                                 \"dimensions than samples!\"\n    ndim, nsamps = samples.shape\n\n    # Check weights.\n    if weights.ndim != 1:\n        raise ValueError(\"Weights must be 1-D.\")\n    if nsamps != weights.shape[0]:\n        raise ValueError(\"The number of weights and samples disagree!\")\n\n    # Determine plotting bounds.\n    if span is None:\n        span = [0.999999426697 for i in range(ndim)]\n    span = list(span)\n    if len(span) != ndim:\n        raise ValueError(\"Dimension mismatch between samples and span.\")\n    for i, _ in enumerate(span):\n        try:\n            xmin, xmax = span[i]\n        except:\n            q = [0.5 - 0.5 * span[i], 0.5 + 0.5 * span[i]]\n            span[i] = _quantile(samples[i], q, weights=weights)\n\n    # Set labels\n    if labels is None:\n        labels = [r\"$x_{\"+str(i+1)+\"}$\" for i in range(ndim)]\n\n    # Setting up smoothing.\n    if (isinstance(smooth, int_type) or isinstance(smooth, float_type)):\n        smooth = [smooth for i in range(ndim)]\n\n    # Setup axis layout (from `corner.py`).\n    factor = 2.0  # size of side of one panel\n    lbdim = 0.5 * factor  # size of left/bottom margin\n    trdim = 0.2 * factor  # size of top/right margin\n    whspace = 0.05  # size of width/height margin\n    plotdim = factor * ndim + factor * (ndim - 1.) * whspace  # plot size\n    dim = lbdim + plotdim + trdim  # total size\n\n    # Initialize figure.\n    if fig is None:\n        fig, axes = pl.subplots(ndim, ndim, figsize=(dim, dim))\n    else:\n        try:\n            fig, axes = fig\n            axes = np.array(axes).reshape((ndim, ndim))\n        except:\n            raise ValueError(\"Mismatch between axes and dimension.\")\n\n    # Format figure.\n    lb = lbdim / dim\n    tr = (lbdim + plotdim) / dim\n    fig.subplots_adjust(left=lb, bottom=lb, right=tr, top=tr,\n                        wspace=whspace, hspace=whspace)\n\n    # Plotting.\n    for i, x in enumerate(samples):\n        if np.shape(samples)[0] == 1:\n            ax = axes\n        else:\n            ax = axes[i, i]\n\n        # Plot the 1-D marginalized posteriors.\n\n        # Setup axes\n        ax.set_xlim(span[i])\n        if max_n_ticks == 0:\n            ax.xaxis.set_major_locator(NullLocator())\n            ax.yaxis.set_major_locator(NullLocator())\n        else:\n            ax.xaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                   prune=\"lower\"))\n            ax.yaxis.set_major_locator(NullLocator())\n        # Label axes.\n        sf = ScalarFormatter(useMathText=use_math_text)\n        ax.xaxis.set_major_formatter(sf)\n        if i < ndim - 1:\n            if top_ticks:\n                ax.xaxis.set_ticks_position(\"top\")\n                [l.set_rotation(45) for l in ax.get_xticklabels()]\n            else:\n                ax.set_xticklabels([])\n        else:\n            [l.set_rotation(45) for l in ax.get_xticklabels()]\n            ax.set_xlabel(labels[i], **label_kwargs)\n            ax.xaxis.set_label_coords(0.5, -0.3)\n        # Generate distribution.\n        sx = smooth[i]\n        if isinstance(sx, int_type):\n            # If `sx` is an integer, plot a weighted histogram with\n            # `sx` bins within the provided bounds.\n            n, b, _ = ax.hist(x, bins=sx, weights=weights, color=color,\n                              range=np.sort(span[i]), **hist_kwargs)\n        else:\n            # If `sx` is a float, oversample the data relative to the\n            # smoothing filter by a factor of 10, then use a Gaussian\n            # filter to smooth the results.\n            bins = int(round(10. / sx))\n            n, b = np.histogram(x, bins=bins, weights=weights,\n                                range=np.sort(span[i]))\n            n = norm_kde(n, 10.)\n            b0 = 0.5 * (b[1:] + b[:-1])\n            n, b, _ = ax.hist(b0, bins=b, weights=n,\n                              range=np.sort(span[i]), color=color,\n                              **hist_kwargs)\n        ax.set_ylim([0., max(n) * 1.05])\n        # Plot quantiles.\n        if quantiles is not None and len(quantiles) > 0:\n            qs = _quantile(x, quantiles, weights=weights)\n            for q in qs:\n                ax.axvline(q, lw=2, ls=\"dashed\", color=color)\n            if verbose:\n                print(\"Quantiles:\")\n                print(labels[i], [blob for blob in zip(quantiles, qs)])\n        # Add truth value(s).\n        if truths is not None and truths[i] is not None:\n            try:\n                [ax.axvline(t, color=truth_color, **truth_kwargs)\n                 for t in truths[i]]\n            except:\n                ax.axvline(truths[i], color=truth_color, **truth_kwargs)\n        # Set titles.\n        if show_titles:\n            title = None\n            if title_fmt is not None:\n                ql, qm, qh = _quantile(x, [0.025, 0.5, 0.975], weights=weights)\n                q_minus, q_plus = qm - ql, qh - qm\n                fmt = \"{{0:{0}}}\".format(title_fmt).format\n                title = r\"${{{0}}}_{{-{1}}}^{{+{2}}}$\"\n                title = title.format(fmt(qm), fmt(q_minus), fmt(q_plus))\n                title = \"{0} = {1}\".format(labels[i], title)\n                ax.set_title(title, **title_kwargs)\n\n        for j, y in enumerate(samples):\n            if np.shape(samples)[0] == 1:\n                ax = axes\n            else:\n                ax = axes[i, j]\n\n            # Plot the 2-D marginalized posteriors.\n\n            # Setup axes.\n            if j > i:\n                ax.set_frame_on(False)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                continue\n            elif j == i:\n                continue\n\n            if max_n_ticks == 0:\n                ax.xaxis.set_major_locator(NullLocator())\n                ax.yaxis.set_major_locator(NullLocator())\n            else:\n                ax.xaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                       prune=\"lower\"))\n                ax.yaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                       prune=\"lower\"))\n            # Label axes.\n            sf = ScalarFormatter(useMathText=use_math_text)\n            ax.xaxis.set_major_formatter(sf)\n            ax.yaxis.set_major_formatter(sf)\n            if i < ndim - 1:\n                ax.set_xticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_xticklabels()]\n                ax.set_xlabel(labels[j], **label_kwargs)\n                ax.xaxis.set_label_coords(0.5, -0.3)\n            if j > 0:\n                ax.set_yticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_yticklabels()]\n                ax.set_ylabel(labels[i], **label_kwargs)\n                ax.yaxis.set_label_coords(-0.3, 0.5)\n            # Generate distribution.\n            sy = smooth[j]\n            check_ix = isinstance(sx, int_type)\n            check_iy = isinstance(sy, int_type)\n            if check_ix and check_iy:\n                fill_contours = False\n                plot_contours = False\n            else:\n                fill_contours = True\n                plot_contours = True\n            hist2d_kwargs['fill_contours'] = hist2d_kwargs.get('fill_contours',\n                                                               fill_contours)\n            hist2d_kwargs['plot_contours'] = hist2d_kwargs.get('plot_contours',\n                                                               plot_contours)\n            _hist2d(y, x, ax=ax, span=[span[j], span[i]],\n                    weights=weights, color=color, smooth=[sy, sx],\n                    **hist2d_kwargs)\n            # Add truth values\n            if truths is not None:\n                if truths[j] is not None:\n                    try:\n                        [ax.axvline(t, color=truth_color, **truth_kwargs)\n                         for t in truths[j]]\n                    except:\n                        ax.axvline(truths[j], color=truth_color,\n                                   **truth_kwargs)\n                if truths[i] is not None:\n                    try:\n                        [ax.axhline(t, color=truth_color, **truth_kwargs)\n                         for t in truths[i]]\n                    except:\n                        ax.axhline(truths[i], color=truth_color,\n                                   **truth_kwargs)\n\n    return (fig, axes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the bounding distribution of the nested sampling run at a given iteration.", "response": "def boundplot(results, dims, it=None, idx=None, prior_transform=None,\n              periodic=None, ndraws=5000, color='gray', plot_kwargs=None,\n              labels=None, label_kwargs=None, max_n_ticks=5,\n              use_math_text=False, show_live=False, live_color='darkviolet',\n              live_kwargs=None, span=None, fig=None):\n    \"\"\"\n    Return the bounding distribution used to propose either (1) live points\n    at a given iteration or (2) a specific dead point during\n    the course of a run, projected onto the two dimensions specified\n    by `dims`.\n\n    Parameters\n    ----------\n    results : :class:`~dynesty.results.Results` instance\n        A :class:`~dynesty.results.Results` instance from a nested\n        sampling run.\n\n    dims : length-2 tuple\n        The dimensions used to plot the bounding.\n\n    it : int, optional\n        If provided, returns the bounding distribution at the specified\n        iteration of the nested sampling run. **Note that this option and\n        `idx` are mutually exclusive.**\n\n    idx : int, optional\n        If provided, returns the bounding distribution used to propose the\n        dead point at the specified iteration of the nested sampling run.\n        **Note that this option and `it` are mutually exclusive.**\n\n    prior_transform : func, optional\n        The function transforming samples within the unit cube back to samples\n        in the native model space. If provided, the transformed bounding\n        distribution will be plotted in the native model space.\n\n    periodic : iterable, optional\n        A list of indices for parameters with periodic boundary conditions.\n        These parameters *will not* have their positions constrained to be\n        within the unit cube, enabling smooth behavior for parameters\n        that may wrap around the edge. It is assumed that their periodicity\n        is dealt with in the `prior_transform`.\n        Default is `None` (i.e. no periodic boundary conditions).\n\n    ndraws : int, optional\n        The number of random samples to draw from the bounding distribution\n        when plotting. Default is `5000`.\n\n    color : str, optional\n        The color of the points randomly sampled from the bounding\n        distribution. Default is `'gray'`.\n\n    plot_kwargs : dict, optional\n        Extra keyword arguments used when plotting the bounding draws.\n\n    labels : iterable with shape (ndim,), optional\n        A list of names for each parameter. If not provided, the default name\n        used when plotting will follow :math:`x_i` style.\n\n    label_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_xlabel` and\n        `~matplotlib.axes.Axes.set_ylabel` methods.\n\n    max_n_ticks : int, optional\n        Maximum number of ticks allowed. Default is `5`.\n\n    use_math_text : bool, optional\n        Whether the axis tick labels for very large/small exponents should be\n        displayed as powers of 10 rather than using `e`. Default is `False`.\n\n    show_live : bool, optional\n        Whether the live points at a given iteration (for `it`) or\n        associated with the bounding (for `idx`) should be highlighted.\n        Default is `False`. In the dynamic case, only the live points\n        associated with the batch used to construct the relevant bound\n        are plotted.\n\n    live_color : str, optional\n        The color of the live points. Default is `'darkviolet'`.\n\n    live_kwargs : dict, optional\n        Extra keyword arguments used when plotting the live points.\n\n    span : iterable with shape (2,), optional\n        A list where each element is a length-2 tuple containing\n        lower and upper bounds. Default is `None` (no bound).\n\n    fig : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`), optional\n        If provided, overplot the draws onto the provided figure.\n        Otherwise, by default an internal figure is generated.\n\n    Returns\n    -------\n    bounding_plot : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`)\n        Output plot of the bounding distribution.\n\n    \"\"\"\n\n    # Initialize values.\n    if plot_kwargs is None:\n        plot_kwargs = dict()\n    if label_kwargs is None:\n        label_kwargs = dict()\n    if live_kwargs is None:\n        live_kwargs = dict()\n\n    # Check that either `idx` or `it` has been specified.\n    if (it is None and idx is None) or (it is not None and idx is not None):\n        raise ValueError(\"You must specify either an iteration or an index!\")\n\n    # Set defaults.\n    plot_kwargs['marker'] = plot_kwargs.get('marker', 'o')\n    plot_kwargs['linestyle'] = plot_kwargs.get('linestyle', 'None')\n    plot_kwargs['markersize'] = plot_kwargs.get('markersize', 1)\n    plot_kwargs['alpha'] = plot_kwargs.get('alpha', 0.4)\n    live_kwargs['marker'] = live_kwargs.get('marker', 'o')\n    live_kwargs['linestyle'] = live_kwargs.get('linestyle', 'None')\n    live_kwargs['markersize'] = live_kwargs.get('markersize', 1)\n\n    # Extract bounding distributions.\n    try:\n        bounds = results['bound']\n    except:\n        raise ValueError(\"No bounds were saved in the results!\")\n    nsamps = len(results['samples'])\n\n    # Gather non-periodic boundary conditions.\n    if periodic is not None:\n        nonperiodic = np.ones(bounds[0].n, dtype='bool')\n        nonperiodic[periodic] = False\n    else:\n        nonperiodic = None\n\n    if it is not None:\n        if it >= nsamps:\n            raise ValueError(\"The iteration requested goes beyond the \"\n                             \"number of iterations in the run.\")\n        # Extract bound iterations.\n        try:\n            bound_iter = np.array(results['bound_iter'])\n        except:\n            raise ValueError(\"Cannot reconstruct the bound used at the \"\n                             \"specified iteration since bound \"\n                             \"iterations were not saved in the results.\")\n\n        # Find bound at the specified iteration.\n        if it == 0:\n            pidx = 0\n        else:\n            pidx = bound_iter[it]\n    else:\n        if idx >= nsamps:\n            raise ValueError(\"The index requested goes beyond the \"\n                             \"number of samples in the run.\")\n        try:\n            samples_bound = results['samples_bound']\n        except:\n            raise ValueError(\"Cannot reconstruct the bound used to \"\n                             \"compute the specified dead point since \"\n                             \"sample bound indices were not saved \"\n                             \"in the results.\")\n        # Grab relevant bound.\n        pidx = samples_bound[idx]\n\n    # Get desired bound.\n    bound = bounds[pidx]\n\n    # Do we want to show the live points at the specified iteration?\n    # If so, we need to rewind our bound to check.\n    # (We could also go forward; this is an arbitrary choice.)\n    if show_live:\n        try:\n            # We can only reconstruct the run if the final set of live points\n            # were added to the results. This is true by default for dynamic\n            # nested sampling runs but not guaranteeed for standard runs.\n            nlive = results['nlive']\n            niter = results['niter']\n            if nsamps - niter != nlive:\n                raise ValueError(\"Cannot reconstruct bound because the \"\n                                 \"final set of live points are not included \"\n                                 \"in the results.\")\n            # Grab our final set of live points (with proper IDs).\n            samples = results['samples_u']\n            samples_id = results['samples_id']\n            ndim = samples.shape[1]\n            live_u = np.empty((nlive, ndim))\n            live_u[samples_id[-nlive:]] = samples[-nlive:]\n            # Find generating bound ID if necessary.\n            if it is None:\n                it = results['samples_it'][idx]\n            # Run our sampling backwards.\n            for i in range(1, niter - it + 1):\n                r = -(nlive + i)\n                uidx = samples_id[r]\n                live_u[uidx] = samples[r]\n        except:\n            # In the dynamic sampling case, we will show the live points used\n            # during the batch associated with a particular iteration/bound.\n            batch = results['samples_batch'][it]  # select batch\n            nbatch = results['batch_nlive'][batch]  # nlive in the batch\n            bsel = results['samples_batch'] == batch  # select batch\n            niter_eff = sum(bsel) - nbatch  # \"effective\" iterations in batch\n            # Grab our final set of live points (with proper IDs).\n            samples = results['samples_u'][bsel]\n            samples_id = results['samples_id'][bsel]\n            samples_id -= min(samples_id)  # re-index to start at zero\n            ndim = samples.shape[1]\n            live_u = np.empty((nbatch, ndim))\n            live_u[samples_id[-nbatch:]] = samples[-nbatch:]\n            # Find generating bound ID if necessary.\n            if it is None:\n                it = results['samples_it'][idx]\n            it_eff = sum(bsel[:it+1])  # effective iteration in batch\n            # Run our sampling backwards.\n            for i in range(1, niter_eff - it_eff + 1):\n                r = -(nbatch + i)\n                uidx = samples_id[r]\n                live_u[uidx] = samples[r]\n\n    # Draw samples from the bounding distribution.\n    try:\n        # If bound is \"fixed\", go ahead and draw samples from it.\n        psamps = bound.samples(ndraws)\n    except:\n        # If bound is based on the distribution of live points at a\n        # specific iteration, we need to reconstruct what those were.\n        if not show_live:\n            try:\n                # Only reconstruct the run if we haven't done it already.\n                nlive = results['nlive']\n                niter = results['niter']\n                if nsamps - niter != nlive:\n                    raise ValueError(\"Cannot reconstruct bound because the \"\n                                     \"final set of live points are not \"\n                                     \"included in the results.\")\n                # Grab our final set of live points (with proper IDs).\n                samples = results['samples_u']\n                samples_id = results['samples_id']\n                ndim = samples.shape[1]\n                live_u = np.empty((nlive, ndim))\n                live_u[samples_id[-nlive:]] = samples[-nlive:]\n                # Run our sampling backwards.\n                if it is None:\n                    it = results['samples_it'][idx]\n                for i in range(1, niter - it + 1):\n                    r = -(nlive + i)\n                    uidx = samples_id[r]\n                    live_u[uidx] = samples[r]\n            except:\n                raise ValueError(\"Live point tracking currently not \"\n                                 \"implemented for dynamic sampling results.\")\n        # Construct a KDTree to speed up nearest-neighbor searches.\n        kdtree = spatial.KDTree(live_u)\n        # Draw samples.\n        psamps = bound.samples(ndraws, live_u, kdtree=kdtree)\n\n    # Projecting samples to input dimensions and possibly\n    # the native model space.\n    if prior_transform is None:\n        x1, x2 = psamps[:, dims].T\n        if show_live:\n            l1, l2 = live_u[:, dims].T\n    else:\n        # Remove points outside of the unit cube as appropriate.\n        sel = [unitcheck(point, nonperiodic) for point in psamps]\n        vsamps = np.array(list(map(prior_transform, psamps[sel])))\n        x1, x2 = vsamps[:, dims].T\n        if show_live:\n            lsamps = np.array(list(map(prior_transform, live_u)))\n            l1, l2 = lsamps[:, dims].T\n\n    # Setting up default plot layout.\n    if fig is None:\n        fig, axes = pl.subplots(1, 1, figsize=(6, 6))\n    else:\n        fig, axes = fig\n        try:\n            axes.plot()\n        except:\n            raise ValueError(\"Provided axes do not match the required shape \"\n                             \"for plotting samples.\")\n\n    # Plotting.\n    axes.plot(x1, x2, color=color, zorder=1, **plot_kwargs)\n    if show_live:\n        axes.plot(l1, l2, color=live_color, zorder=2, **live_kwargs)\n\n    # Setup axes\n    if span is not None:\n        axes.set_xlim(span[0])\n        axes.set_ylim(span[1])\n    if max_n_ticks == 0:\n        axes.xaxis.set_major_locator(NullLocator())\n        axes.yaxis.set_major_locator(NullLocator())\n    else:\n        axes.xaxis.set_major_locator(MaxNLocator(max_n_ticks))\n        axes.yaxis.set_major_locator(MaxNLocator(max_n_ticks))\n    # Label axes.\n    sf = ScalarFormatter(useMathText=use_math_text)\n    axes.xaxis.set_major_formatter(sf)\n    axes.yaxis.set_major_formatter(sf)\n    if labels is not None:\n        axes.set_xlabel(labels[0], **label_kwargs)\n        axes.set_ylabel(labels[1], **label_kwargs)\n    else:\n        axes.set_xlabel(r\"$x_{\"+str(dims[0]+1)+\"}$\", **label_kwargs)\n        axes.set_ylabel(r\"$x_{\"+str(dims[1]+1)+\"}$\", **label_kwargs)\n\n    return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the bounding distribution of the live points at a particular iteration or at a specific dead point during a nested sampling run.", "response": "def cornerbound(results, it=None, idx=None, prior_transform=None,\n                periodic=None, ndraws=5000, color='gray', plot_kwargs=None,\n                labels=None, label_kwargs=None, max_n_ticks=5,\n                use_math_text=False, show_live=False, live_color='darkviolet',\n                live_kwargs=None, span=None, fig=None):\n    \"\"\"\n    Return the bounding distribution used to propose either (1) live points\n    at a given iteration or (2) a specific dead point during\n    the course of a run, projected onto all pairs of dimensions.\n\n    Parameters\n    ----------\n    results : :class:`~dynesty.results.Results` instance\n        A :class:`~dynesty.results.Results` instance from a nested\n        sampling run.\n\n    it : int, optional\n        If provided, returns the bounding distribution at the specified\n        iteration of the nested sampling run. **Note that this option and\n        `idx` are mutually exclusive.**\n\n    idx : int, optional\n        If provided, returns the bounding distribution used to propose the\n        dead point at the specified iteration of the nested sampling run.\n        **Note that this option and `it` are mutually exclusive.**\n\n    prior_transform : func, optional\n        The function transforming samples within the unit cube back to samples\n        in the native model space. If provided, the transformed bounding\n        distribution will be plotted in the native model space.\n\n    periodic : iterable, optional\n        A list of indices for parameters with periodic boundary conditions.\n        These parameters *will not* have their positions constrained to be\n        within the unit cube, enabling smooth behavior for parameters\n        that may wrap around the edge. It is assumed that their periodicity\n        is dealt with in the `prior_transform`.\n        Default is `None` (i.e. no periodic boundary conditions).\n\n    ndraws : int, optional\n        The number of random samples to draw from the bounding distribution\n        when plotting. Default is `5000`.\n\n    color : str, optional\n        The color of the points randomly sampled from the bounding\n        distribution. Default is `'gray'`.\n\n    plot_kwargs : dict, optional\n        Extra keyword arguments used when plotting the bounding draws.\n\n    labels : iterable with shape (ndim,), optional\n        A list of names for each parameter. If not provided, the default name\n        used when plotting will be in :math:`x_i` style.\n\n    label_kwargs : dict, optional\n        Extra keyword arguments that will be sent to the\n        `~matplotlib.axes.Axes.set_xlabel` and\n        `~matplotlib.axes.Axes.set_ylabel` methods.\n\n    max_n_ticks : int, optional\n        Maximum number of ticks allowed. Default is `5`.\n\n    use_math_text : bool, optional\n        Whether the axis tick labels for very large/small exponents should be\n        displayed as powers of 10 rather than using `e`. Default is `False`.\n\n    show_live : bool, optional\n        Whether the live points at a given iteration (for `it`) or\n        associated with the bounding (for `idx`) should be highlighted.\n        Default is `False`. In the dynamic case, only the live points\n        associated with the batch used to construct the relevant bound\n        are plotted.\n\n    live_color : str, optional\n        The color of the live points. Default is `'darkviolet'`.\n\n    live_kwargs : dict, optional\n        Extra keyword arguments used when plotting the live points.\n\n    span : iterable with shape (2,), optional\n        A list where each element is a length-2 tuple containing\n        lower and upper bounds. Default is `None` (no bound).\n\n    fig : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`), optional\n        If provided, overplot the draws onto the provided figure.\n        Otherwise, by default an internal figure is generated.\n\n\n    Returns\n    -------\n    cornerbound : (`~matplotlib.figure.Figure`, `~matplotlib.axes.Axes`)\n        Output corner plot of the bounding distribution.\n\n    \"\"\"\n\n    # Initialize values.\n    if label_kwargs is None:\n        label_kwargs = dict()\n    if plot_kwargs is None:\n        plot_kwargs = dict()\n    if live_kwargs is None:\n        live_kwargs = dict()\n\n    # Check that either `idx` or `it` is specified.\n    if (it is None and idx is None) or (it is not None and idx is not None):\n        raise ValueError(\"You must specify either an iteration or an index!\")\n\n    # Set defaults.\n    plot_kwargs['marker'] = plot_kwargs.get('marker', 'o')\n    plot_kwargs['linestyle'] = plot_kwargs.get('linestyle', 'None')\n    plot_kwargs['markersize'] = plot_kwargs.get('markersize', 1)\n    plot_kwargs['alpha'] = plot_kwargs.get('alpha', 0.4)\n    live_kwargs['marker'] = live_kwargs.get('marker', 'o')\n    live_kwargs['linestyle'] = live_kwargs.get('linestyle', 'None')\n    live_kwargs['markersize'] = live_kwargs.get('markersize', 1)\n\n    # Extract bounding distributions.\n    try:\n        bounds = results['bound']\n    except:\n        raise ValueError(\"No bounds were saved in the results!\")\n    nsamps = len(results['samples'])\n\n    # Gather non-periodic boundary conditions.\n    if periodic is not None:\n        nonperiodic = np.ones(bounds[0].n, dtype='bool')\n        nonperiodic[periodic] = False\n    else:\n        nonperiodic = None\n\n    if it is not None:\n        if it >= nsamps:\n            raise ValueError(\"The iteration requested goes beyond the \"\n                             \"number of iterations in the run.\")\n        # Extract bound iterations.\n        try:\n            bound_iter = np.array(results['bound_iter'])\n        except:\n            raise ValueError(\"Cannot reconstruct the bound used at the \"\n                             \"specified iteration since bound \"\n                             \"iterations were not saved in the results.\")\n\n        # Find bound at the specified iteration.\n        if it == 0:\n            pidx = 0\n        else:\n            pidx = bound_iter[it]\n    else:\n        if idx >= nsamps:\n            raise ValueError(\"The index requested goes beyond the \"\n                             \"number of samples in the run.\")\n        try:\n            samples_bound = results['samples_bound']\n        except:\n            raise ValueError(\"Cannot reconstruct the bound used to \"\n                             \"compute the specified dead point since \"\n                             \"sample bound indices were not saved \"\n                             \"in the results.\")\n        # Grab relevant bound.\n        pidx = samples_bound[idx]\n\n    # Get desired bound.\n    bound = bounds[pidx]\n\n    # Do we want to show the live points at the specified iteration?\n    # If so, we need to rewind our bound to check.\n    # (We could also go forward; this is an arbitrary choice.)\n    if show_live:\n        try:\n            # We can only reconstruct the run if the final set of live points\n            # were added to the results. This is true by default for dynamic\n            # nested sampling runs but not guaranteeed for standard runs.\n            nlive = results['nlive']\n            niter = results['niter']\n            if nsamps - niter != nlive:\n                raise ValueError(\"Cannot reconstruct bound because the \"\n                                 \"final set of live points are not included \"\n                                 \"in the results.\")\n            # Grab our final set of live points (with proper IDs).\n            samples = results['samples_u']\n            samples_id = results['samples_id']\n            ndim = samples.shape[1]\n            live_u = np.empty((nlive, ndim))\n            live_u[samples_id[-nlive:]] = samples[-nlive:]\n            # Find generating bound ID if necessary.\n            if it is None:\n                it = results['samples_it'][idx]\n            # Run our sampling backwards.\n            for i in range(1, niter - it + 1):\n                r = -(nlive + i)\n                uidx = samples_id[r]\n                live_u[uidx] = samples[r]\n        except:\n            # In the dynamic sampling case, we will show the live points used\n            # during the batch associated with a particular iteration/bound.\n            if it is not None:\n                batch = results['samples_batch'][it]  # select batch\n            else:\n                batch = results['samples_batch'][idx]\n            nbatch = results['batch_nlive'][batch]  # nlive in the batch\n            bsel = results['samples_batch'] == batch  # select batch\n            niter_eff = sum(bsel) - nbatch  # \"effective\" iterations in batch\n            # Grab our final set of live points (with proper IDs).\n            samples = results['samples_u'][bsel]\n            samples_id = results['samples_id'][bsel]\n            samples_id -= min(samples_id)  # re-index to start at zero\n            ndim = samples.shape[1]\n            live_u = np.empty((nbatch, ndim))\n            live_u[samples_id[-nbatch:]] = samples[-nbatch:]\n            # Find generating bound ID if necessary.\n            if it is None:\n                it = results['samples_it'][idx]\n            it_eff = sum(bsel[:it+1])  # effective iteration in batch\n            # Run our sampling backwards.\n            for i in range(1, niter_eff - it_eff + 1):\n                r = -(nbatch + i)\n                uidx = samples_id[r]\n                live_u[uidx] = samples[r]\n\n    # Draw samples from the bounding distribution.\n    try:\n        # If bound is \"fixed\", go ahead and draw samples from it.\n        psamps = bound.samples(ndraws)\n    except:\n        # If bound is based on the distribution of live points at a\n        # specific iteration, we need to reconstruct what those were.\n        if not show_live:\n            # Only reconstruct the run if we haven't done it already.\n            nlive = results['nlive']\n            niter = results['niter']\n            if nsamps - niter != nlive:\n                raise ValueError(\"Cannot reconstruct bound because the \"\n                                 \"final set of live points are not included \"\n                                 \"in the results.\")\n            # Grab our final set of live points (with proper IDs).\n            samples = results['samples_u']\n            samples_id = results['samples_id']\n            ndim = samples.shape[1]\n            live_u = np.empty((nlive, ndim))\n            live_u[samples_id[-nlive:]] = samples[-nlive:]\n            # Run our sampling backwards.\n            if it is None:\n                it = results['samples_it'][idx]\n            for i in range(1, niter - it + 1):\n                r = -(nlive + i)\n                uidx = samples_id[r]\n                live_u[uidx] = samples[r]\n        # Construct a KDTree to speed up nearest-neighbor searches.\n        kdtree = spatial.KDTree(live_u)\n        # Draw samples.\n        psamps = bound.samples(ndraws, live_u, kdtree=kdtree)\n\n    # Projecting samples to input dimensions and possibly\n    # the native model space.\n    if prior_transform is None:\n        psamps = psamps.T\n        if show_live:\n            lsamps = live_u.T\n    else:\n        # Remove points outside of the unit cube.\n        sel = [unitcheck(point, nonperiodic) for point in psamps]\n        psamps = np.array(list(map(prior_transform, psamps[sel])))\n        psamps = psamps.T\n        if show_live:\n            lsamps = np.array(list(map(prior_transform, live_u)))\n            lsamps = lsamps.T\n\n    # Set labels\n    ndim = psamps.shape[0]\n    if labels is None:\n        labels = [r\"$x_{\"+str(i+1)+\"}$\" for i in range(ndim)]\n\n    # Setup axis layout (from `corner.py`).\n    factor = 2.0  # size of side of one panel\n    lbdim = 0.5 * factor  # size of left/bottom margin\n    trdim = 0.2 * factor  # size of top/right margin\n    whspace = 0.05  # size of width/height margin\n    plotdim = factor * (ndim - 1.) + factor * (ndim - 2.) * whspace\n    dim = lbdim + plotdim + trdim  # total size\n\n    # Initialize figure.\n    if fig is None:\n        fig, axes = pl.subplots(ndim - 1, ndim - 1, figsize=(dim, dim))\n    else:\n        try:\n            fig, axes = fig\n            axes = np.array(axes).reshape((ndim - 1, ndim - 1))\n        except:\n            raise ValueError(\"Mismatch between axes and dimension.\")\n\n    # Format figure.\n    lb = lbdim / dim\n    tr = (lbdim + plotdim) / dim\n    fig.subplots_adjust(left=lb, bottom=lb, right=tr, top=tr,\n                        wspace=whspace, hspace=whspace)\n\n    # Plot the 2-D projected samples.\n    for i, x in enumerate(psamps[1:]):\n        for j, y in enumerate(psamps[:-1]):\n            try:\n                ax = axes[i, j]\n            except:\n                ax = axes\n            # Setup axes.\n            if span is not None:\n                ax.set_xlim(span[j])\n                ax.set_ylim(span[i])\n            if j > i:\n                ax.set_frame_on(False)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                continue\n            if max_n_ticks == 0:\n                ax.xaxis.set_major_locator(NullLocator())\n                ax.yaxis.set_major_locator(NullLocator())\n            else:\n                ax.xaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                       prune=\"lower\"))\n                ax.yaxis.set_major_locator(MaxNLocator(max_n_ticks,\n                                                       prune=\"lower\"))\n            # Label axes.\n            sf = ScalarFormatter(useMathText=use_math_text)\n            ax.xaxis.set_major_formatter(sf)\n            ax.yaxis.set_major_formatter(sf)\n            if i < ndim - 2:\n                ax.set_xticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_xticklabels()]\n                ax.set_xlabel(labels[j], **label_kwargs)\n                ax.xaxis.set_label_coords(0.5, -0.3)\n            if j > 0:\n                ax.set_yticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_yticklabels()]\n                ax.set_ylabel(labels[i+1], **label_kwargs)\n                ax.yaxis.set_label_coords(-0.3, 0.5)\n            # Plot distribution.\n            ax.plot(y, x, c=color, **plot_kwargs)\n            # Add live points.\n            if show_live:\n                ax.plot(lsamps[j], lsamps[i+1], c=live_color, **live_kwargs)\n\n    return (fig, axes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _hist2d(x, y, smooth=0.02, span=None, weights=None, levels=None,\n            ax=None, color='gray', plot_datapoints=False, plot_density=True,\n            plot_contours=True, no_fill_contours=False, fill_contours=True,\n            contour_kwargs=None, contourf_kwargs=None, data_kwargs=None,\n            **kwargs):\n    \"\"\"\n    Internal function called by :meth:`cornerplot` used to generate a\n    a 2-D histogram/contour of samples.\n\n    Parameters\n    ----------\n    x : interable with shape (nsamps,)\n       Sample positions in the first dimension.\n\n    y : iterable with shape (nsamps,)\n       Sample positions in the second dimension.\n\n    span : iterable with shape (ndim,), optional\n        A list where each element is either a length-2 tuple containing\n        lower and upper bounds or a float from `(0., 1.]` giving the\n        fraction of (weighted) samples to include. If a fraction is provided,\n        the bounds are chosen to be equal-tailed. An example would be::\n\n            span = [(0., 10.), 0.95, (5., 6.)]\n\n        Default is `0.999999426697` (5-sigma credible interval).\n\n    weights : iterable with shape (nsamps,)\n        Weights associated with the samples. Default is `None` (no weights).\n\n    levels : iterable, optional\n        The contour levels to draw. Default are `[0.5, 1, 1.5, 2]`-sigma.\n\n    ax : `~matplotlib.axes.Axes`, optional\n        An `~matplotlib.axes.axes` instance on which to add the 2-D histogram.\n        If not provided, a figure will be generated.\n\n    color : str, optional\n        The `~matplotlib`-style color used to draw lines and color cells\n        and contours. Default is `'gray'`.\n\n    plot_datapoints : bool, optional\n        Whether to plot the individual data points. Default is `False`.\n\n    plot_density : bool, optional\n        Whether to draw the density colormap. Default is `True`.\n\n    plot_contours : bool, optional\n        Whether to draw the contours. Default is `True`.\n\n    no_fill_contours : bool, optional\n        Whether to add absolutely no filling to the contours. This differs\n        from `fill_contours=False`, which still adds a white fill at the\n        densest points. Default is `False`.\n\n    fill_contours : bool, optional\n        Whether to fill the contours. Default is `True`.\n\n    contour_kwargs : dict\n        Any additional keyword arguments to pass to the `contour` method.\n\n    contourf_kwargs : dict\n        Any additional keyword arguments to pass to the `contourf` method.\n\n    data_kwargs : dict\n        Any additional keyword arguments to pass to the `plot` method when\n        adding the individual data points.\n\n    \"\"\"\n\n    if ax is None:\n        ax = pl.gca()\n\n    # Determine plotting bounds.\n    data = [x, y]\n    if span is None:\n        span = [0.999999426697 for i in range(2)]\n    span = list(span)\n    if len(span) != 2:\n        raise ValueError(\"Dimension mismatch between samples and span.\")\n    for i, _ in enumerate(span):\n        try:\n            xmin, xmax = span[i]\n        except:\n            q = [0.5 - 0.5 * span[i], 0.5 + 0.5 * span[i]]\n            span[i] = _quantile(data[i], q, weights=weights)\n\n    # The default \"sigma\" contour levels.\n    if levels is None:\n        levels = 1.0 - np.exp(-0.5 * np.arange(0.5, 2.1, 0.5) ** 2)\n\n    # Color map for the density plot, over-plotted to indicate the\n    # density of the points near the center.\n    density_cmap = LinearSegmentedColormap.from_list(\n        \"density_cmap\", [color, (1, 1, 1, 0)])\n\n    # Color map used to hide the points at the high density areas.\n    white_cmap = LinearSegmentedColormap.from_list(\n        \"white_cmap\", [(1, 1, 1), (1, 1, 1)], N=2)\n\n    # This \"color map\" is the list of colors for the contour levels if the\n    # contours are filled.\n    rgba_color = colorConverter.to_rgba(color)\n    contour_cmap = [list(rgba_color) for l in levels] + [rgba_color]\n    for i, l in enumerate(levels):\n        contour_cmap[i][-1] *= float(i) / (len(levels)+1)\n\n    # Initialize smoothing.\n    if (isinstance(smooth, int_type) or isinstance(smooth, float_type)):\n        smooth = [smooth, smooth]\n    bins = []\n    svalues = []\n    for s in smooth:\n        if isinstance(s, int_type):\n            # If `s` is an integer, the weighted histogram has\n            # `s` bins within the provided bounds.\n            bins.append(s)\n            svalues.append(0.)\n        else:\n            # If `s` is a float, oversample the data relative to the\n            # smoothing filter by a factor of 2, then use a Gaussian\n            # filter to smooth the results.\n            bins.append(int(round(2. / s)))\n            svalues.append(2.)\n\n    # We'll make the 2D histogram to directly estimate the density.\n    try:\n        H, X, Y = np.histogram2d(x.flatten(), y.flatten(), bins=bins,\n                                 range=list(map(np.sort, span)),\n                                 weights=weights)\n    except ValueError:\n        raise ValueError(\"It looks like at least one of your sample columns \"\n                         \"have no dynamic range.\")\n\n    # Smooth the results.\n    if not np.all(svalues == 0.):\n        H = norm_kde(H, svalues)\n\n    # Compute the density levels.\n    Hflat = H.flatten()\n    inds = np.argsort(Hflat)[::-1]\n    Hflat = Hflat[inds]\n    sm = np.cumsum(Hflat)\n    sm /= sm[-1]\n    V = np.empty(len(levels))\n    for i, v0 in enumerate(levels):\n        try:\n            V[i] = Hflat[sm <= v0][-1]\n        except:\n            V[i] = Hflat[0]\n    V.sort()\n    m = (np.diff(V) == 0)\n    if np.any(m) and plot_contours:\n        logging.warning(\"Too few points to create valid contours.\")\n    while np.any(m):\n        V[np.where(m)[0][0]] *= 1.0 - 1e-4\n        m = (np.diff(V) == 0)\n    V.sort()\n\n    # Compute the bin centers.\n    X1, Y1 = 0.5 * (X[1:] + X[:-1]), 0.5 * (Y[1:] + Y[:-1])\n\n    # Extend the array for the sake of the contours at the plot edges.\n    H2 = H.min() + np.zeros((H.shape[0] + 4, H.shape[1] + 4))\n    H2[2:-2, 2:-2] = H\n    H2[2:-2, 1] = H[:, 0]\n    H2[2:-2, -2] = H[:, -1]\n    H2[1, 2:-2] = H[0]\n    H2[-2, 2:-2] = H[-1]\n    H2[1, 1] = H[0, 0]\n    H2[1, -2] = H[0, -1]\n    H2[-2, 1] = H[-1, 0]\n    H2[-2, -2] = H[-1, -1]\n    X2 = np.concatenate([X1[0] + np.array([-2, -1]) * np.diff(X1[:2]), X1,\n                         X1[-1] + np.array([1, 2]) * np.diff(X1[-2:])])\n    Y2 = np.concatenate([Y1[0] + np.array([-2, -1]) * np.diff(Y1[:2]), Y1,\n                         Y1[-1] + np.array([1, 2]) * np.diff(Y1[-2:])])\n\n    # Plot the data points.\n    if plot_datapoints:\n        if data_kwargs is None:\n            data_kwargs = dict()\n        data_kwargs[\"color\"] = data_kwargs.get(\"color\", color)\n        data_kwargs[\"ms\"] = data_kwargs.get(\"ms\", 2.0)\n        data_kwargs[\"mec\"] = data_kwargs.get(\"mec\", \"none\")\n        data_kwargs[\"alpha\"] = data_kwargs.get(\"alpha\", 0.1)\n        ax.plot(x, y, \"o\", zorder=-1, rasterized=True, **data_kwargs)\n\n    # Plot the base fill to hide the densest data points.\n    if (plot_contours or plot_density) and not no_fill_contours:\n        ax.contourf(X2, Y2, H2.T, [V.min(), H.max()],\n                    cmap=white_cmap, antialiased=False)\n\n    if plot_contours and fill_contours:\n        if contourf_kwargs is None:\n            contourf_kwargs = dict()\n        contourf_kwargs[\"colors\"] = contourf_kwargs.get(\"colors\", contour_cmap)\n        contourf_kwargs[\"antialiased\"] = contourf_kwargs.get(\"antialiased\",\n                                                             False)\n        ax.contourf(X2, Y2, H2.T, np.concatenate([[0], V, [H.max()*(1+1e-4)]]),\n                    **contourf_kwargs)\n\n    # Plot the density map. This can't be plotted at the same time as the\n    # contour fills.\n    elif plot_density:\n        ax.pcolor(X, Y, H.max() - H.T, cmap=density_cmap)\n\n    # Plot the contour edge colors.\n    if plot_contours:\n        if contour_kwargs is None:\n            contour_kwargs = dict()\n        contour_kwargs[\"colors\"] = contour_kwargs.get(\"colors\", color)\n        ax.contour(X2, Y2, H2.T, V, **contour_kwargs)\n\n    ax.set_xlim(span[0])\n    ax.set_ylim(span[1])", "response": "Internal function that generates a 2 - D histogram of samples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef weight_function(results, args=None, return_weights=False):\n\n    # Initialize hyperparameters.\n    if args is None:\n        args = dict({})\n    pfrac = args.get('pfrac', 0.8)\n    if not 0. <= pfrac <= 1.:\n        raise ValueError(\"The provided `pfrac` {0} is not between 0. and 1.\"\n                         .format(pfrac))\n    maxfrac = args.get('maxfrac', 0.8)\n    if not 0. <= maxfrac <= 1.:\n        raise ValueError(\"The provided `maxfrac` {0} is not between 0. and 1.\"\n                         .format(maxfrac))\n    lpad = args.get('pad', 1)\n    if lpad < 0:\n        raise ValueError(\"`lpad` {0} is less than zero.\".format(lpad))\n\n    # Derive evidence weights.\n    logz = results.logz  # final ln(evidence)\n    logz_remain = results.logl[-1] + results.logvol[-1]  # remainder\n    logz_tot = np.logaddexp(logz[-1], logz_remain)  # estimated upper bound\n    lzones = np.ones_like(logz)\n    logzin = misc.logsumexp([lzones * logz_tot, logz], axis=0,\n                            b=[lzones, -lzones])  # ln(remaining evidence)\n    logzweight = logzin - np.log(results.samples_n)  # ln(evidence weight)\n    logzweight -= misc.logsumexp(logzweight)  # normalize\n    zweight = np.exp(logzweight)  # convert to linear scale\n\n    # Derive posterior weights.\n    pweight = np.exp(results.logwt - results.logz[-1])  # importance weight\n    pweight /= sum(pweight)  # normalize\n\n    # Compute combined weights.\n    weight = (1. - pfrac) * zweight + pfrac * pweight\n\n    # Compute logl bounds\n    nsamps = len(logz)\n    bounds = np.arange(nsamps)[weight > maxfrac * max(weight)]\n    bounds = (min(bounds) - lpad, min(max(bounds) + lpad, nsamps - 1))\n    if bounds[0] < 0:\n        logl_min = -np.inf\n    else:\n        logl_min = results.logl[bounds[0]]\n    logl_max = results.logl[bounds[1]]\n\n    if return_weights:\n        return (logl_min, logl_max), (pweight, zweight, weight)\n    else:\n        return (logl_min, logl_max)", "response": "Default weight function utilized by DynamicSampler."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefaults stopping function for the Kullback - Leibler.", "response": "def stopping_function(results, args=None, rstate=None, M=None,\n                      return_vals=False):\n    \"\"\"\n    The default stopping function utilized by :class:`DynamicSampler`.\n    Zipped parameters are passed to the function via :data:`args`.\n    Assigns the run a stopping value based on a weighted average of the\n    stopping values for the posterior and evidence::\n\n        stop = pfrac * stop_post + (1.- pfrac) * stop_evid\n\n    The evidence stopping value is based on the estimated evidence error\n    (i.e. standard deviation) relative to a given threshold::\n\n        stop_evid = evid_std / evid_thresh\n\n    The posterior stopping value is based on the fractional error (i.e.\n    standard deviation / mean) in the Kullback-Leibler (KL) divergence\n    relative to a given threshold::\n\n        stop_post = (kld_std / kld_mean) / post_thresh\n\n    Estimates of the mean and standard deviation are computed using `n_mc`\n    realizations of the input using a provided `'error'` keyword (either\n    `'jitter'` or `'simulate'`, which call related functions :meth:`jitter_run`\n    and :meth:`simulate_run` in :mod:`dynesty.utils`, respectively, or\n    `'sim_approx'`, which boosts `'jitter'` by a factor of two).\n\n    Returns the boolean `stop <= 1`. If `True`, the :class:`DynamicSampler`\n    will stop adding new samples to our results.\n\n    Parameters\n    ----------\n    results : :class:`Results` instance\n        :class:`Results` instance.\n\n    args : dictionary of keyword arguments, optional\n        Arguments used to set the stopping values. Default values are\n        `pfrac = 1.0`, `evid_thresh = 0.1`, `post_thresh = 0.02`,\n        `n_mc = 128`, `error = 'sim_approx'`, and `approx = True`.\n\n    rstate : `~numpy.random.RandomState`, optional\n        `~numpy.random.RandomState` instance.\n\n    M : `map` function, optional\n        An alias to a `map`-like function. This allows users to pass\n        functions from pools (e.g., `pool.map`) to compute realizations in\n        parallel. By default the standard `map` function is used.\n\n    return_vals : bool, optional\n        Whether to return the stopping value (and its components). Default\n        is `False`.\n\n    Returns\n    -------\n    stop_flag : bool\n        Boolean flag indicating whether we have passed the desired stopping\n        criteria.\n\n    stop_vals : tuple of shape (3,), optional\n        The individual stopping values `(stop_post, stop_evid, stop)` used\n        to determine the stopping criteria.\n\n    \"\"\"\n\n    # Initialize values.\n    if args is None:\n        args = dict({})\n    if rstate is None:\n        rstate = np.random\n    if M is None:\n        M = map\n\n    # Initialize hyperparameters.\n    pfrac = args.get('pfrac', 1.0)\n    if not 0. <= pfrac <= 1.:\n        raise ValueError(\"The provided `pfrac` {0} is not between 0. and 1.\"\n                         .format(pfrac))\n    evid_thresh = args.get('evid_thresh', 0.1)\n    if pfrac < 1. and evid_thresh < 0.:\n        raise ValueError(\"The provided `evid_thresh` {0} is not non-negative \"\n                         \"even though `1. - pfrac` is {1}.\"\n                         .format(evid_thresh, 1. - pfrac))\n    post_thresh = args.get('post_thresh', 0.02)\n    if pfrac > 0. and post_thresh < 0.:\n        raise ValueError(\"The provided `post_thresh` {0} is not non-negative \"\n                         \"even though `pfrac` is {1}.\"\n                         .format(post_thresh, pfrac))\n    n_mc = args.get('n_mc', 128)\n    if n_mc <= 1:\n        raise ValueError(\"The number of realizations {0} must be greater \"\n                         \"than 1.\".format(n_mc))\n    elif n_mc < 20:\n        warnings.warn(\"Using a small number of realizations might result in \"\n                      \"excessively noisy stopping value estimates.\")\n    error = args.get('error', 'sim_approx')\n    if error not in {'jitter', 'simulate', 'sim_approx'}:\n        raise ValueError(\"The chosen `'error'` option {0} is not valid.\"\n                         .format(error))\n    if error == 'sim_approx':\n        error = 'jitter'\n        boost = 2.\n    else:\n        boost = 1.\n    approx = args.get('approx', True)\n\n    # Compute realizations of ln(evidence) and the KL divergence.\n    rlist = [results for i in range(n_mc)]\n    error_list = [error for i in range(n_mc)]\n    approx_list = [approx for i in range(n_mc)]\n    args = zip(rlist, error_list, approx_list)\n    outputs = list(M(_kld_error, args))\n    kld_arr, lnz_arr = np.array([(kld[-1], res.logz[-1])\n                                 for kld, res in outputs]).T\n\n    # Evidence stopping value.\n    lnz_std = np.std(lnz_arr)\n    stop_evid = np.sqrt(boost) * lnz_std / evid_thresh\n\n    # Posterior stopping value.\n    kld_mean, kld_std = np.mean(kld_arr), np.std(kld_arr)\n    stop_post = boost * (kld_std / kld_mean) / post_thresh\n\n    # Effective stopping value.\n    stop = pfrac * stop_post + (1. - pfrac) * stop_evid\n\n    if return_vals:\n        return stop <= 1., (stop_post, stop_evid, stop)\n    else:\n        return stop <= 1."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of dictionaries containing the saved samples and bounds.", "response": "def results(self):\n        \"\"\"Saved results from the dynamic nested sampling run. All saved\n        bounds are also returned.\"\"\"\n\n        # Add all saved samples (and ancillary quantities) to the results.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            results = [('niter', self.it - 1),\n                       ('ncall', np.array(self.saved_nc)),\n                       ('eff', self.eff),\n                       ('samples', np.array(self.saved_v)),\n                       ('samples_id', np.array(self.saved_id)),\n                       ('samples_batch', np.array(self.saved_batch,\n                                                  dtype='int')),\n                       ('samples_it', np.array(self.saved_it)),\n                       ('samples_u', np.array(self.saved_u)),\n                       ('samples_n', np.array(self.saved_n)),\n                       ('logwt', np.array(self.saved_logwt)),\n                       ('logl', np.array(self.saved_logl)),\n                       ('logvol', np.array(self.saved_logvol)),\n                       ('logz', np.array(self.saved_logz)),\n                       ('logzerr', np.sqrt(np.array(self.saved_logzvar))),\n                       ('information', np.array(self.saved_h)),\n                       ('batch_nlive', np.array(self.saved_batch_nlive,\n                                                dtype='int')),\n                       ('batch_bounds', np.array(self.saved_batch_bounds))]\n\n        # Add any saved bounds (and ancillary quantities) to the results.\n        if self.sampler.save_bounds:\n            results.append(('bound', copy.deepcopy(self.bound)))\n            results.append(('bound_iter',\n                            np.array(self.saved_bounditer, dtype='int')))\n            results.append(('samples_bound',\n                            np.array(self.saved_boundidx, dtype='int')))\n            results.append(('scale', np.array(self.saved_scale)))\n\n        return Results(results)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a series of initial samples from a nested sampling run.", "response": "def sample_initial(self, nlive=500, update_interval=None,\n                       first_update=None, maxiter=None, maxcall=None,\n                       logl_max=np.inf, dlogz=0.01, live_points=None):\n        \"\"\"\n        Generate a series of initial samples from a nested sampling\n        run using a fixed number of live points using an internal\n        sampler from :mod:`~dynesty.nestedsamplers`. Instantiates a\n        generator that will be called by the user.\n\n        Parameters\n        ----------\n        nlive : int, optional\n            The number of live points to use for the baseline nested\n            sampling run. Default is `500`.\n\n        update_interval : int or float, optional\n            If an integer is passed, only update the bounding distribution\n            every `update_interval`-th likelihood call. If a float is passed,\n            update the bound after every `round(update_interval * nlive)`-th\n            likelihood call. Larger update intervals can be more efficient\n            when the likelihood function is quick to evaluate. If no value is\n            provided, defaults to the value passed during initialization.\n\n        first_update : dict, optional\n            A dictionary containing parameters governing when the sampler will\n            first update the bounding distribution from the unit cube\n            (`'none'`) to the one specified by `sample`.\n\n        maxiter : int, optional\n            Maximum number of iterations. Iteration may stop earlier if the\n            termination condition is reached. Default is `sys.maxsize`\n            (no limit).\n\n        maxcall : int, optional\n            Maximum number of likelihood evaluations. Iteration may stop\n            earlier if termination condition is reached. Default is\n            `sys.maxsize` (no limit).\n\n        dlogz : float, optional\n            Iteration will stop when the estimated contribution of the\n            remaining prior volume to the total evidence falls below\n            this threshold. Explicitly, the stopping criterion is\n            `ln(z + z_est) - ln(z) < dlogz`, where `z` is the current\n            evidence from all saved samples and `z_est` is the estimated\n            contribution from the remaining volume. The default is\n            `0.01`.\n\n        logl_max : float, optional\n            Iteration will stop when the sampled ln(likelihood) exceeds the\n            threshold set by `logl_max`. Default is no bound (`np.inf`).\n\n        live_points : list of 3 `~numpy.ndarray` each with shape (nlive, ndim)\n            A set of live points used to initialize the nested sampling run.\n            Contains `live_u`, the coordinates on the unit cube, `live_v`, the\n            transformed variables, and `live_logl`, the associated\n            loglikelihoods. By default, if these are not provided the initial\n            set of live points will be drawn from the unit `npdim`-cube.\n            **WARNING: It is crucial that the initial set of live points have\n            been sampled from the prior. Failure to provide a set of valid\n            live points will lead to incorrect results.**\n\n        Returns\n        -------\n        worst : int\n            Index of the live point with the worst likelihood. This is our\n            new dead point sample.\n\n        ustar : `~numpy.ndarray` with shape (npdim,)\n            Position of the sample.\n\n        vstar : `~numpy.ndarray` with shape (ndim,)\n            Transformed position of the sample.\n\n        loglstar : float\n            Ln(likelihood) of the sample.\n\n        logvol : float\n            Ln(prior volume) within the sample.\n\n        logwt : float\n            Ln(weight) of the sample.\n\n        logz : float\n            Cumulative ln(evidence) up to the sample (inclusive).\n\n        logzvar : float\n            Estimated cumulative variance on `logz` (inclusive).\n\n        h : float\n            Cumulative information up to the sample (inclusive).\n\n        nc : int\n            Number of likelihood calls performed before the new\n            live point was accepted.\n\n        worst_it : int\n            Iteration when the live (now dead) point was originally proposed.\n\n        boundidx : int\n            Index of the bound the dead point was originally drawn from.\n\n        bounditer : int\n            Index of the bound being used at the current iteration.\n\n        eff : float\n            The cumulative sampling efficiency (in percent).\n\n        delta_logz : float\n            The estimated remaining evidence expressed as the ln(ratio) of the\n            current evidence.\n\n        \"\"\"\n\n        # Initialize inputs.\n        if maxcall is None:\n            maxcall = sys.maxsize\n        if maxiter is None:\n            maxiter = sys.maxsize\n        if nlive <= 2 * self.npdim:\n            warnings.warn(\"Beware: `nlive_init <= 2 * ndim`!\")\n\n        # Reset saved results to avoid any possible conflicts.\n        self.reset()\n\n        # Initialize the first set of live points.\n        if live_points is None:\n            self.nlive_init = nlive\n            self.live_u = self.rstate.rand(self.nlive_init, self.npdim)\n            if self.use_pool_ptform:\n                self.live_v = np.array(list(self.M(self.prior_transform,\n                                                   np.array(self.live_u))))\n            else:\n                self.live_v = np.array(list(map(self.prior_transform,\n                                                np.array(self.live_u))))\n            if self.use_pool_logl:\n                self.live_logl = np.array(list(self.M(self.loglikelihood,\n                                                      np.array(self.live_v))))\n            else:\n                self.live_logl = np.array(list(map(self.loglikelihood,\n                                                   np.array(self.live_v))))\n        else:\n            self.live_u, self.live_v, self.live_logl = live_points\n            self.nlive_init = len(self.live_u)\n\n        # Convert all `-np.inf` log-likelihoods to finite large numbers.\n        # Necessary to keep estimators in our sampler from breaking.\n        for i, logl in enumerate(self.live_logl):\n            if not np.isfinite(logl):\n                if np.sign(logl) < 0:\n                    self.live_logl[i] = -1e300\n                else:\n                    raise ValueError(\"The log-likelihood ({0}) of live \"\n                                     \"point {1} located at u={2} v={3} \"\n                                     \" is invalid.\"\n                                     .format(logl, i, self.live_u[i],\n                                             self.live_v[i]))\n\n        # (Re-)bundle live points.\n        live_points = [self.live_u, self.live_v, self.live_logl]\n        self.live_init = [np.array(l) for l in live_points]\n        self.ncall += self.nlive_init\n        self.live_bound = np.zeros(self.nlive_init, dtype='int')\n        self.live_it = np.zeros(self.nlive_init, dtype='int')\n\n        # Initialize the internal `sampler` object.\n        if update_interval is None:\n            update_interval = self.update_interval\n        if isinstance(update_interval, float):\n            update_interval = int(round(self.update_interval * nlive))\n        bounding = self.bounding\n        if bounding == 'none':\n            update_interval = np.inf  # no need to update with no bounds\n        if first_update is None:\n            first_update = self.first_update\n        self.sampler = _SAMPLERS[bounding](self.loglikelihood,\n                                           self.prior_transform,\n                                           self.npdim, self.live_init,\n                                           self.method, update_interval,\n                                           first_update,\n                                           self.rstate, self.queue_size,\n                                           self.pool, self.use_pool,\n                                           self.kwargs)\n        self.bound = self.sampler.bound\n\n        # Run the sampler internally as a generator.\n        for i in range(1):\n            for it, results in enumerate(self.sampler.sample(maxiter=maxiter,\n                                         save_samples=False,\n                                         maxcall=maxcall, dlogz=dlogz)):\n                # Grab results.\n                (worst, ustar, vstar, loglstar, logvol, logwt,\n                 logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                 eff, delta_logz) = results\n\n                # Save our base run (which we will use later).\n                self.base_id.append(worst)\n                self.base_u.append(ustar)\n                self.base_v.append(vstar)\n                self.base_logl.append(loglstar)\n                self.base_logvol.append(logvol)\n                self.base_logwt.append(logwt)\n                self.base_logz.append(logz)\n                self.base_logzvar.append(logzvar)\n                self.base_h.append(h)\n                self.base_nc.append(nc)\n                self.base_it.append(worst_it)\n                self.base_n.append(self.nlive_init)\n                self.base_boundidx.append(boundidx)\n                self.base_bounditer.append(bounditer)\n                self.base_scale.append(self.sampler.scale)\n\n                # Save a copy of the results.\n                self.saved_id.append(worst)\n                self.saved_u.append(ustar)\n                self.saved_v.append(vstar)\n                self.saved_logl.append(loglstar)\n                self.saved_logvol.append(logvol)\n                self.saved_logwt.append(logwt)\n                self.saved_logz.append(logz)\n                self.saved_logzvar.append(logzvar)\n                self.saved_h.append(h)\n                self.saved_nc.append(nc)\n                self.saved_it.append(worst_it)\n                self.saved_n.append(self.nlive_init)\n                self.saved_boundidx.append(boundidx)\n                self.saved_bounditer.append(bounditer)\n                self.saved_scale.append(self.sampler.scale)\n\n                # Increment relevant counters.\n                self.ncall += nc\n                self.eff = 100. * self.it / self.ncall\n                self.it += 1\n\n                yield (worst, ustar, vstar, loglstar, logvol, logwt,\n                       logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                       self.eff, delta_logz)\n\n            for it, results in enumerate(self.sampler.add_live_points()):\n                # Grab results.\n                (worst, ustar, vstar, loglstar, logvol, logwt,\n                 logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                 eff, delta_logz) = results\n\n                # Save our base run (which we will use later).\n                self.base_id.append(worst)\n                self.base_u.append(ustar)\n                self.base_v.append(vstar)\n                self.base_logl.append(loglstar)\n                self.base_logvol.append(logvol)\n                self.base_logwt.append(logwt)\n                self.base_logz.append(logz)\n                self.base_logzvar.append(logzvar)\n                self.base_h.append(h)\n                self.base_nc.append(nc)\n                self.base_it.append(worst_it)\n                self.base_n.append(self.nlive_init - it)\n                self.base_boundidx.append(boundidx)\n                self.base_bounditer.append(bounditer)\n                self.base_scale.append(self.sampler.scale)\n\n                # Save a copy of the results.\n                self.saved_id.append(worst)\n                self.saved_u.append(ustar)\n                self.saved_v.append(vstar)\n                self.saved_logl.append(loglstar)\n                self.saved_logvol.append(logvol)\n                self.saved_logwt.append(logwt)\n                self.saved_logz.append(logz)\n                self.saved_logzvar.append(logzvar)\n                self.saved_h.append(h)\n                self.saved_nc.append(nc)\n                self.saved_it.append(worst_it)\n                self.saved_n.append(self.nlive_init - it)\n                self.saved_boundidx.append(boundidx)\n                self.saved_bounditer.append(bounditer)\n                self.saved_scale.append(self.sampler.scale)\n\n                # Increment relevant counters.\n                self.eff = 100. * self.it / self.ncall\n                self.it += 1\n\n                yield (worst, ustar, vstar, loglstar, logvol, logwt,\n                       logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                       self.eff, delta_logz)\n\n        self.base = True  # baseline run complete\n        self.saved_batch = np.zeros(len(self.saved_id), dtype='int')  # batch\n        self.saved_batch_nlive.append(self.nlive_init)  # initial nlive\n        self.saved_batch_bounds.append((-np.inf, np.inf))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample_batch(self, nlive_new=500, update_interval=None,\n                     logl_bounds=None, maxiter=None, maxcall=None,\n                     save_bounds=True):\n        \"\"\"\n        Generate an additional series of nested samples that will be combined\n        with the previous set of dead points. Works by hacking the internal\n        `sampler` object.\n        Instantiates a generator that will be called by the user.\n\n        Parameters\n        ----------\n        nlive_new : int\n            Number of new live points to be added. Default is `500`.\n\n        update_interval : int or float, optional\n            If an integer is passed, only update the bounding distribution\n            every `update_interval`-th likelihood call. If a float is passed,\n            update the bound after every `round(update_interval * nlive)`-th\n            likelihood call. Larger update intervals can be more efficient\n            when the likelihood function is quick to evaluate. If no value is\n            provided, defaults to the value passed during initialization.\n\n        logl_bounds : tuple of size (2,), optional\n            The ln(likelihood) bounds used to bracket the run. If `None`,\n            the default bounds span the entire range covered by the\n            original run.\n\n        maxiter : int, optional\n            Maximum number of iterations. Iteration may stop earlier if the\n            termination condition is reached. Default is `sys.maxsize`\n            (no limit).\n\n        maxcall : int, optional\n            Maximum number of likelihood evaluations. Iteration may stop\n            earlier if termination condition is reached. Default is\n            `sys.maxsize` (no limit).\n\n        save_bounds : bool, optional\n            Whether or not to save past distributions used to bound\n            the live points internally. Default is `True`.\n\n        Returns\n        -------\n        worst : int\n            Index of the live point with the worst likelihood. This is our\n            new dead point sample. **Negative values indicate the index\n            of a new live point generated when initializing a new batch.**\n\n        ustar : `~numpy.ndarray` with shape (npdim,)\n            Position of the sample.\n\n        vstar : `~numpy.ndarray` with shape (ndim,)\n            Transformed position of the sample.\n\n        loglstar : float\n            Ln(likelihood) of the sample.\n\n        nc : int\n            Number of likelihood calls performed before the new\n            live point was accepted.\n\n        worst_it : int\n            Iteration when the live (now dead) point was originally proposed.\n\n        boundidx : int\n            Index of the bound the dead point was originally drawn from.\n\n        bounditer : int\n            Index of the bound being used at the current iteration.\n\n        eff : float\n            The cumulative sampling efficiency (in percent).\n\n        \"\"\"\n\n        # Initialize default values.\n        if maxcall is None:\n            maxcall = sys.maxsize\n        if maxiter is None:\n            maxiter = sys.maxsize\n        if nlive_new <= 2 * self.npdim:\n            warnings.warn(\"Beware: `nlive_batch <= 2 * ndim`!\")\n        self.sampler.save_bounds = save_bounds\n\n        # Initialize starting values.\n        h = 0.0  # Information, initially *0.*\n        logz = -1.e300  # ln(evidence), initially *0.*\n        logvol = 0.  # initially contains the whole prior (volume=1.)\n\n        # Grab results from base run.\n        base_id = np.array(self.base_id)\n        base_u = np.array(self.base_u)\n        base_v = np.array(self.base_v)\n        base_logl = np.array(self.base_logl)\n        base_n = np.array(self.base_n)\n        base_scale = np.array(self.base_scale)\n        nbase = len(base_n)\n        nblive = self.nlive_init\n\n        # Reset \"new\" results.\n        self.new_id = []\n        self.new_u = []\n        self.new_v = []\n        self.new_logl = []\n        self.new_nc = []\n        self.new_it = []\n        self.new_n = []\n        self.new_boundidx = []\n        self.new_bounditer = []\n        self.new_scale = []\n        self.new_logl_min, self.new_logl_max = -np.inf, np.inf\n\n        # Initialize ln(likelihood) bounds.\n        if logl_bounds is None:\n            logl_min, logl_max = -np.inf, max(base_logl[:-nblive])\n        else:\n            logl_min, logl_max = logl_bounds\n        self.new_logl_min, self.new_logl_max = logl_min, logl_max\n\n        # Check whether the lower bound encompasses all previous base samples.\n        psel = np.all(logl_min <= base_logl)\n        vol = 1. - 1. / nblive  # starting ln(prior volume)\n        if psel:\n            # If the lower bound encompasses all base samples, we want\n            # to propose a new set of points from the unit cube.\n            live_u = self.rstate.rand(nlive_new, self.npdim)\n            if self.use_pool_ptform:\n                live_v = np.array(list(self.M(self.prior_transform,\n                                              np.array(live_u))))\n            else:\n                live_v = np.array(list(map(self.prior_transform,\n                                           np.array(live_u))))\n            if self.use_pool_logl:\n                live_logl = np.array(list(self.M(self.loglikelihood,\n                                                 np.array(live_v))))\n            else:\n                live_logl = np.array(list(map(self.loglikelihood,\n                                              np.array(live_v))))\n            # Convert all `-np.inf` log-likelihoods to finite large numbers.\n            # Necessary to keep estimators in our sampler from breaking.\n            for i, logl in enumerate(live_logl):\n                if not np.isfinite(logl):\n                    if np.sign(logl) < 0:\n                        live_logl[i] = -1e300\n                    else:\n                        raise ValueError(\"The log-likelihood ({0}) of live \"\n                                         \"point {1} located at u={2} v={3} \"\n                                         \" is invalid.\"\n                                         .format(logl, i, live_u[i],\n                                                 live_v[i]))\n            live_bound = np.zeros(nlive_new, dtype='int')\n            live_it = np.zeros(nlive_new, dtype='int') + self.it\n            live_nc = np.ones(nlive_new, dtype='int')\n            self.ncall += nlive_new\n            # Return live points in generator format.\n            for i in range(nlive_new):\n                yield (-i - 1, live_u[i], live_v[i], live_logl[i], live_nc[i],\n                       live_it[i], 0, 0, self.eff)\n        else:\n            # If the lower bound doesn't encompass all base samples, we need\n            # to \"rewind\" our previous base run until we arrive at the\n            # relevant set of live points (and scale) at the bound.\n            live_u = np.empty((nblive, self.npdim))\n            live_v = np.empty((nblive, base_v.shape[1]))\n            live_logl = np.empty(nblive)\n            live_u[base_id[-nblive:]] = base_u[-nblive:]\n            live_v[base_id[-nblive:]] = base_v[-nblive:]\n            live_logl[base_id[-nblive:]] = base_logl[-nblive:]\n            for i in range(1, nbase - nblive):\n                r = -(nblive + i)\n                uidx = base_id[r]\n                live_u[uidx] = base_u[r]\n                live_v[uidx] = base_v[r]\n                live_logl[uidx] = base_logl[r]\n                if live_logl[uidx] <= logl_min:\n                    break\n            live_scale = base_scale[r]\n\n            # Hack the internal sampler by overwriting the live points\n            # and scale factor.\n            self.sampler.nlive = nblive\n            self.sampler.live_u = np.array(live_u)\n            self.sampler.live_v = np.array(live_v)\n            self.sampler.live_logl = np.array(live_logl)\n            self.sampler.scale = live_scale\n\n            # Trigger an update of the internal bounding distribution based\n            # on the \"new\" set of live points.\n            vol = math.exp(- 1. * (nbase + r) / nblive)\n            loglmin = min(live_logl)\n            if self.sampler._beyond_unit_bound(loglmin):\n                bound = self.sampler.update(vol / nblive)\n                if save_bounds:\n                    self.sampler.bound.append(copy.deepcopy(bound))\n                self.sampler.nbound += 1\n                self.sampler.since_update = 0\n\n            # Sample a new batch of `nlive_new` live points using the\n            # internal sampler given the `logl_min` constraint.\n            live_u = np.empty((nlive_new, self.npdim))\n            live_v = np.empty((nlive_new, base_v.shape[1]))\n            live_logl = np.empty(nlive_new)\n            live_bound = np.zeros(nlive_new, dtype='int')\n            if self.sampler._beyond_unit_bound(loglmin):\n                live_bound += self.sampler.nbound - 1\n            live_it = np.empty(nlive_new, dtype='int')\n            live_nc = np.empty(nlive_new, dtype='int')\n            for i in range(nlive_new):\n                (live_u[i], live_v[i], live_logl[i],\n                 live_nc[i]) = self.sampler._new_point(logl_min, math.log(vol))\n                live_it[i] = self.it\n                self.ncall += live_nc[i]\n                # Return live points in generator format.\n                yield (-i - 1, live_u[i], live_v[i], live_logl[i], live_nc[i],\n                       live_it[i], live_bound[i], live_bound[i], self.eff)\n\n        # Overwrite the previous set of live points in our internal sampler\n        # with the new batch of points we just generated.\n        self.sampler.nlive = nlive_new\n        self.sampler.live_u = np.array(live_u)\n        self.sampler.live_v = np.array(live_v)\n        self.sampler.live_logl = np.array(live_logl)\n        self.sampler.live_bound = np.array(live_bound)\n        self.sampler.live_it = np.array(live_it)\n\n        # Trigger an update of the internal bounding distribution (again).\n        loglmin = min(live_logl)\n        if self.sampler._beyond_unit_bound(loglmin):\n            bound = self.sampler.update(vol / nlive_new)\n            if save_bounds:\n                self.sampler.bound.append(copy.deepcopy(bound))\n            self.sampler.nbound += 1\n            self.sampler.since_update = 0\n\n        # Copy over bound reference.\n        self.bound = self.sampler.bound\n\n        # Update `update_interval` based on our new set of live points.\n        if update_interval is None:\n            update_interval = self.update_interval\n        if isinstance(update_interval, float):\n            update_interval = int(round(self.update_interval * nlive_new))\n        if self.bounding == 'none':\n            update_interval = np.inf  # no need to update with no bounds\n        self.sampler.update_interval = update_interval\n\n        # Update internal ln(prior volume)-based quantities used to set things\n        # like `pointvol` that help to prevent constructing over-constrained\n        # bounding distributions.\n        if self.new_logl_min == -np.inf:\n            bound_logvol = 0.\n        else:\n            vol_idx = np.argmin(abs(self.saved_logl - self.new_logl_min))\n            bound_logvol = self.saved_logvol[vol_idx]\n        bound_dlv = math.log((nlive_new + 1.) / nlive_new)\n        self.sampler.saved_logvol[-1] = bound_logvol\n        self.sampler.dlv = bound_dlv\n\n        # Tell the sampler *not* to try and remove the previous addition of\n        # live points. All the hacks above make the internal results\n        # garbage anyways.\n        self.sampler.added_live = False\n\n        # Run the sampler internally as a generator until we hit\n        # the lower likelihood threshold. Afterwards, we add in our remaining\n        # live points *as if* we had terminated the run. This allows us to\n        # sample past the original bounds \"for free\".\n        for i in range(1):\n            for it, results in enumerate(self.sampler.sample(dlogz=0.,\n                                         logl_max=logl_max,\n                                         maxiter=maxiter-nlive_new-1,\n                                         maxcall=maxcall-sum(live_nc),\n                                         save_samples=False,\n                                         save_bounds=save_bounds)):\n\n                # Grab results.\n                (worst, ustar, vstar, loglstar, logvol, logwt,\n                 logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                 eff, delta_logz) = results\n\n                # Save results.\n                self.new_id.append(worst)\n                self.new_u.append(ustar)\n                self.new_v.append(vstar)\n                self.new_logl.append(loglstar)\n                self.new_nc.append(nc)\n                self.new_it.append(worst_it)\n                self.new_n.append(nlive_new)\n                self.new_boundidx.append(boundidx)\n                self.new_bounditer.append(bounditer)\n                self.new_scale.append(self.sampler.scale)\n\n                # Increment relevant counters.\n                self.ncall += nc\n                self.eff = 100. * self.it / self.ncall\n                self.it += 1\n\n                yield (worst, ustar, vstar, loglstar, nc,\n                       worst_it, boundidx, bounditer, self.eff)\n\n            for it, results in enumerate(self.sampler.add_live_points()):\n                # Grab results.\n                (worst, ustar, vstar, loglstar, logvol, logwt,\n                 logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                 eff, delta_logz) = results\n\n                # Save results.\n                self.new_id.append(worst)\n                self.new_u.append(ustar)\n                self.new_v.append(vstar)\n                self.new_logl.append(loglstar)\n                self.new_nc.append(live_nc[worst])\n                self.new_it.append(worst_it)\n                self.new_n.append(nlive_new - it)\n                self.new_boundidx.append(boundidx)\n                self.new_bounditer.append(bounditer)\n                self.new_scale.append(self.sampler.scale)\n\n                # Increment relevant counters.\n                self.eff = 100. * self.it / self.ncall\n                self.it += 1\n\n                yield (worst, ustar, vstar, loglstar, live_nc[worst],\n                       worst_it, boundidx, bounditer, self.eff)", "response": "Generates a series of nested samples from the current set of dead points."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges the most recent run into the previous run by stepping through both runs simultaneously.", "response": "def combine_runs(self):\n        \"\"\" Merge the most recent run into the previous (combined) run by\n        \"stepping through\" both runs simultaneously.\"\"\"\n\n        # Make sure we have a run to add.\n        if len(self.new_id) == 0:\n            raise ValueError(\"No new samples are currently saved.\")\n\n        # Grab results from saved run.\n        saved_id = np.array(self.saved_id)\n        saved_u = np.array(self.saved_u)\n        saved_v = np.array(self.saved_v)\n        saved_logl = np.array(self.saved_logl)\n        saved_nc = np.array(self.saved_nc)\n        saved_boundidx = np.array(self.saved_boundidx)\n        saved_it = np.array(self.saved_it)\n        saved_n = np.array(self.saved_n)\n        saved_bounditer = np.array(self.saved_bounditer)\n        saved_scale = np.array(self.saved_scale)\n        saved_batch = np.array(self.saved_batch)\n        nsaved = len(saved_n)\n\n        # Grab results from new run.\n        new_id = np.array(self.new_id) + max(saved_id) + 1\n        new_u = np.array(self.new_u)\n        new_v = np.array(self.new_v)\n        new_logl = np.array(self.new_logl)\n        new_nc = np.array(self.new_nc)\n        new_boundidx = np.array(self.new_boundidx)\n        new_it = np.array(self.new_it)\n        new_n = np.array(self.new_n)\n        new_bounditer = np.array(self.new_bounditer)\n        new_scale = np.array(self.new_scale)\n        nnew = len(new_n)\n        llmin, llmax = self.new_logl_min, self.new_logl_max\n\n        # Reset saved results.\n        self.saved_id = []\n        self.saved_u = []\n        self.saved_v = []\n        self.saved_logl = []\n        self.saved_logvol = []\n        self.saved_logwt = []\n        self.saved_logz = []\n        self.saved_logzvar = []\n        self.saved_h = []\n        self.saved_nc = []\n        self.saved_boundidx = []\n        self.saved_it = []\n        self.saved_n = []\n        self.saved_bounditer = []\n        self.saved_scale = []\n        self.saved_batch = []\n\n        # Start our counters at the beginning of each set of dead points.\n        idx_saved, idx_new = 0, 0  # start of our dead points\n        logl_s, logl_n = saved_logl[idx_saved], new_logl[idx_new]\n        nlive_s, nlive_n = saved_n[idx_saved], new_n[idx_new]\n\n        # Iteratively walk through both set of samples to simulate\n        # a combined run.\n        ntot = nsaved + nnew\n        logvol = 0.\n        for i in range(ntot):\n            if logl_s > self.new_logl_min:\n                # If our saved samples are past the lower log-likelihood\n                # bound, both runs are now \"active\" and should be used.\n                nlive = nlive_s + nlive_n\n            else:\n                # If instead our collection of dead points are below\n                # the bound, just use our collection of saved samples.\n                nlive = nlive_s\n            # Increment our position along depending on\n            # which dead point (saved or new) is worse.\n            if logl_s <= logl_n:\n                self.saved_id.append(saved_id[idx_saved])\n                self.saved_u.append(saved_u[idx_saved])\n                self.saved_v.append(saved_v[idx_saved])\n                self.saved_logl.append(saved_logl[idx_saved])\n                self.saved_nc.append(saved_nc[idx_saved])\n                self.saved_boundidx.append(saved_boundidx[idx_saved])\n                self.saved_it.append(saved_it[idx_saved])\n                self.saved_bounditer.append(saved_bounditer[idx_saved])\n                self.saved_scale.append(saved_scale[idx_saved])\n                self.saved_batch.append(saved_batch[idx_saved])\n                idx_saved += 1\n            else:\n                self.saved_id.append(new_id[idx_new])\n                self.saved_u.append(new_u[idx_new])\n                self.saved_v.append(new_v[idx_new])\n                self.saved_logl.append(new_logl[idx_new])\n                self.saved_nc.append(new_nc[idx_new])\n                self.saved_boundidx.append(new_boundidx[idx_new])\n                self.saved_it.append(new_it[idx_new])\n                self.saved_bounditer.append(new_bounditer[idx_new])\n                self.saved_scale.append(new_scale[idx_new])\n                self.saved_batch.append(self.batch + 1)\n                idx_new += 1\n\n            # Save the number of live points and expected ln(volume).\n            logvol -= math.log((nlive + 1.) / nlive)\n            self.saved_n.append(nlive)\n            self.saved_logvol.append(logvol)\n\n            # Attempt to step along our samples. If we're out of samples,\n            # set values to defaults.\n            try:\n                logl_s = saved_logl[idx_saved]\n                nlive_s = saved_n[idx_saved]\n            except:\n                logl_s = np.inf\n                nlive_s = 0\n            try:\n                logl_n = new_logl[idx_new]\n                nlive_n = new_n[idx_new]\n            except:\n                logl_n = np.inf\n                nlive_n = 0\n\n        # Compute quantities of interest.\n        h = 0.\n        logz = -1.e300\n        loglstar = -1.e300\n        logzvar = 0.\n        logvols_pad = np.concatenate(([0.], self.saved_logvol))\n        logdvols = misc.logsumexp(a=np.c_[logvols_pad[:-1], logvols_pad[1:]],\n                                  axis=1, b=np.c_[np.ones(ntot),\n                                                  -np.ones(ntot)])\n        logdvols += math.log(0.5)\n        dlvs = logvols_pad[:-1] - logvols_pad[1:]\n        for i in range(ntot):\n            loglstar_new = self.saved_logl[i]\n            logdvol, dlv = logdvols[i], dlvs[i]\n            logwt = np.logaddexp(loglstar_new, loglstar) + logdvol\n            logz_new = np.logaddexp(logz, logwt)\n            lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                      math.exp(loglstar_new - logz_new) * loglstar_new)\n            h_new = (math.exp(logdvol) * lzterm +\n                     math.exp(logz - logz_new) * (h + logz) -\n                     logz_new)\n            dh = h_new - h\n            h = h_new\n            logz = logz_new\n            logzvar += dh * dlv\n            loglstar = loglstar_new\n            self.saved_logwt.append(logwt)\n            self.saved_logz.append(logz)\n            self.saved_logzvar.append(logzvar)\n            self.saved_h.append(h)\n\n        # Reset results.\n        self.new_id = []\n        self.new_u = []\n        self.new_v = []\n        self.new_logl = []\n        self.new_nc = []\n        self.new_it = []\n        self.new_n = []\n        self.new_boundidx = []\n        self.new_bounditer = []\n        self.new_scale = []\n        self.new_logl_min, self.new_logl_max = -np.inf, np.inf\n\n        # Increment batch counter.\n        self.batch += 1\n\n        # Saved batch quantities.\n        self.saved_batch_nlive.append(max(new_n))\n        self.saved_batch_bounds.append((llmin, llmax))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new batch of samples to the current state of the sampler.", "response": "def add_batch(self, nlive=500, wt_function=None, wt_kwargs=None,\n                  maxiter=None, maxcall=None, save_bounds=True,\n                  print_progress=True, print_func=None, stop_val=None):\n        \"\"\"\n        Allocate an additional batch of (nested) samples based on\n        the combined set of previous samples using the specified\n        weight function.\n\n        Parameters\n        ----------\n        nlive : int, optional\n            The number of live points used when adding additional samples\n            in the batch. Default is `500`.\n\n        wt_function : func, optional\n            A cost function that takes a `Results` instance\n            and returns a log-likelihood range over which a new batch of\n            samples should be generated. The default function simply\n            computes a weighted average of the posterior and evidence\n            information content as::\n\n                weight = pfrac * pweight + (1. - pfrac) * zweight\n\n        wt_kwargs : dict, optional\n            Extra arguments to be passed to the weight function.\n\n        maxiter : int, optional\n            Maximum number of iterations allowed. Default is `sys.maxsize`\n            (no limit).\n\n        maxcall : int, optional\n            Maximum number of likelihood evaluations allowed.\n            Default is `sys.maxsize` (no limit).\n\n        save_bounds : bool, optional\n            Whether or not to save distributions used to bound\n            the live points internally during dynamic live point allocations.\n            Default is `True`.\n\n        print_progress : bool, optional\n            Whether to output a simple summary of the current run that\n            updates each iteration. Default is `True`.\n\n        print_func : function, optional\n            A function that prints out the current state of the sampler.\n            If not provided, the default :meth:`results.print_fn` is used.\n\n        stop_val : float, optional\n            The value of the stopping criteria to be passed to\n            :meth:`print_func`. Used internally within :meth:`run_nested` to\n            keep track of progress.\n\n        \"\"\"\n\n        # Initialize values.\n        if maxcall is None:\n            maxcall = sys.maxsize\n        if maxiter is None:\n            maxiter = sys.maxsize\n        if wt_function is None:\n            wt_function = weight_function\n        if wt_kwargs is None:\n            wt_kwargs = dict()\n        if print_func is None:\n            print_func = print_fn\n\n        # If we have either likelihood calls or iterations remaining,\n        # add our new batch of live points.\n        ncall, niter, n = self.ncall, self.it - 1, self.batch\n        if maxcall > 0 and maxiter > 0:\n            # Compute our sampling bounds using the provided\n            # weight function.\n            res = self.results\n            lnz, lnzerr = res.logz[-1], res.logzerr[-1]\n            logl_bounds = wt_function(res, wt_kwargs)\n            for results in self.sample_batch(nlive_new=nlive,\n                                             logl_bounds=logl_bounds,\n                                             maxiter=maxiter,\n                                             maxcall=maxcall,\n                                             save_bounds=save_bounds):\n                (worst, ustar, vstar, loglstar, nc,\n                 worst_it, boundidx, bounditer, eff) = results\n\n                # When initializing a batch (i.e. when `worst < 0`),\n                # don't increment our call counter or our current\n                # number of iterations.\n                if worst >= 0:\n                    ncall += nc\n                    niter += 1\n\n                # Reorganize results.\n                results = (worst, ustar, vstar, loglstar, np.nan, np.nan,\n                           lnz, lnzerr**2, np.nan, nc, worst_it, boundidx,\n                           bounditer, eff, np.nan)\n\n                # Print progress.\n                if print_progress:\n                    print_func(results, niter, ncall, nbatch=n+1,\n                               stop_val=stop_val,\n                               logl_min=logl_bounds[0],\n                               logl_max=logl_bounds[1])\n\n            # Combine batch with previous runs.\n            self.combine_runs()\n\n        # Pass back info.\n        return ncall, niter, logl_bounds, results"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the volume constant for an n - dimensional sphere with an norm.", "response": "def vol_prefactor(n, p=2.):\n    \"\"\"\n    Returns the volume constant for an `n`-dimensional sphere with an\n    :math:`L^p` norm. The constant is defined as::\n\n        f = (2. * Gamma(1./p + 1))**n / Gamma(n/p + 1.)\n\n    By default the `p=2.` norm is used (i.e. the standard Euclidean norm).\n\n    \"\"\"\n\n    p *= 1.  # convert to float in case user inputs an integer\n    f = (2 * special.gamma(1./p + 1.))**n / special.gamma(n/p + 1)\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logvol_prefactor(n, p=2.):\n\n    p *= 1.  # convert to float in case user inputs an integer\n    lnf = (n * np.log(2.) + n * special.gammaln(1./p + 1.) -\n           special.gammaln(n/p + 1))\n\n    return lnf", "response": "Returns the log - volume constant for an integer n - dimensional sphere with an optional prefactor p."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw a point uniformly within an n - dimensional unit sphere.", "response": "def randsphere(n, rstate=None):\n    \"\"\"Draw a point uniformly within an `n`-dimensional unit sphere.\"\"\"\n\n    if rstate is None:\n        rstate = np.random\n\n    z = rstate.randn(n)  # initial n-dim vector\n    zhat = z / lalg.norm(z)  # normalize\n    xhat = zhat * rstate.rand()**(1./n)  # scale\n\n    return xhat"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new n - sphere with the given set of points.", "response": "def bounding_ellipsoid(points, pointvol=0.):\n    \"\"\"\n    Calculate the bounding ellipsoid containing a collection of points.\n\n    Parameters\n    ----------\n    points : `~numpy.ndarray` with shape (npoints, ndim)\n        A set of coordinates.\n\n    pointvol : float, optional\n        The minimum volume occupied by a single point. When provided,\n        used to set a minimum bound on the ellipsoid volume\n        as `npoints * pointvol`. Default is `0.`.\n\n    Returns\n    -------\n    ellipsoid : :class:`Ellipsoid`\n        The bounding :class:`Ellipsoid` object.\n\n    \"\"\"\n\n    npoints, ndim = points.shape\n\n    # Check for valid `pointvol` value if provided.\n    if pointvol < 0.:\n        raise ValueError(\"You must specify a non-negative value \"\n                         \"for `pointvol`.\")\n\n    # If there is only a single point, return an n-sphere with volume\n    # `pointvol` centered at the point.\n    if npoints == 1:\n        if pointvol > 0.:\n            ctr = points[0]\n            r = np.exp((np.log(pointvol) - logvol_prefactor(ndim)) / ndim)\n            covar = r**2 * np.identity(ndim)\n            return Ellipsoid(ctr, covar)\n        else:\n            raise ValueError(\"Cannot compute a bounding ellipsoid to a \"\n                             \"single point if `pointvol` is not specified.\")\n\n    # Calculate covariance of points.\n    ctr = np.mean(points, axis=0)\n    cov = mle_cov(points, rowvar=False)\n\n    # When ndim = 1, `np.cov` returns a 0-d array. Make it a 1x1 2-d array.\n    if ndim == 1:\n        cov = np.atleast_2d(cov)\n\n    # For a ball of uniformly distributed points, the sample covariance\n    # will be smaller than the true covariance by a factor of 1/(n+2)\n    # [see, e.g., goo.gl/UbsjYl]. Since we are assuming all points are\n    # uniformly distributed within the unit cube, they are uniformly\n    # distributed within any sub-volume within the cube. We expand\n    # our sample covariance `cov` to compensate for this.\n    cov *= (ndim + 2)\n\n    # Define the axes of our ellipsoid. Ensures that `cov` is\n    # nonsingular to deal with pathological cases where the ellipsoid has\n    # \"zero\" volume. This can occur when `npoints <= ndim` or when enough\n    # points are linear combinations of other points.\n    covar = np.array(cov)\n    for trials in range(100):\n        try:\n            # Check if matrix is invertible.\n            am = lalg.pinvh(covar)\n            l, v = lalg.eigh(covar)  # compute eigenvalues/vectors\n            if np.all((l > 0.) & (np.isfinite(l))):\n                break\n            else:\n                raise RuntimeError(\"The eigenvalue/eigenvector decomposition \"\n                                   \"failed!\")\n        except:\n            # If the matrix remains singular/unstable,\n            # suppress the off-diagonal elements.\n            coeff = 1.1**(trials+1) / 1.1**100\n            covar = (1. - coeff) * cov + coeff * np.eye(ndim)\n            pass\n    else:\n        warnings.warn(\"Failed to guarantee the ellipsoid axes will be \"\n                      \"non-singular. Defaulting to a sphere.\")\n        am = np.eye(ndim)\n\n    # Calculate expansion factor necessary to bound each point.\n    # Points should obey `(x-v)^T A (x-v) <= 1`, so we calculate this for\n    # each point and then scale A up or down to make the\n    # \"outermost\" point obey `(x-v)^T A (x-v) = 1`. This can be done\n    # quickly using `einsum` and `tensordot` to iterate over all points.\n    delta = points - ctr\n    f = np.einsum('...i, ...i', np.tensordot(delta, am, axes=1), delta)\n    fmax = np.max(f)\n\n    # Due to round-off errors, we actually scale the ellipsoid so the\n    # outermost point obeys `(x-v)^T A (x-v) < 1 - (a bit) < 1`.\n    one_minus_a_bit = 1. - SQRTEPS\n\n    if fmax > one_minus_a_bit:\n        covar *= fmax / one_minus_a_bit\n\n    # Initialize our ellipsoid.\n    ell = Ellipsoid(ctr, covar)\n\n    # Expand our ellipsoid to encompass a minimum volume.\n    if pointvol > 0.:\n        minvol = npoints * pointvol\n        if ell.vol < minvol:\n            ell.scale_to_vol(minvol)\n\n    return ell"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _bounding_ellipsoids(points, ell, pointvol=0., vol_dec=0.5,\n                         vol_check=2.):\n    \"\"\"\n    Internal method used to compute a set of bounding ellipsoids when a\n    bounding ellipsoid for the entire set has already been calculated.\n\n    Parameters\n    ----------\n    points : `~numpy.ndarray` with shape (npoints, ndim)\n        A set of coordinates.\n\n    ell : Ellipsoid\n        The bounding ellipsoid containing :data:`points`.\n\n    pointvol : float, optional\n        Volume represented by a single point. When provided,\n        used to set a minimum bound on the ellipsoid volume\n        as `npoints * pointvol`. Default is `0.`.\n\n    vol_dec : float, optional\n        The required fractional reduction in volume after splitting an\n        ellipsoid in order to to accept the split. Default is `0.5`.\n\n    vol_check : float, optional\n        The factor used to when checking whether the volume of the\n        original bounding ellipsoid is large enough to warrant more\n        trial splits via `ell.vol > vol_check * npoints * pointvol`.\n        Default is `2.0`.\n\n    Returns\n    -------\n    ells : list of :class:`Ellipsoid` objects\n        List of :class:`Ellipsoid` objects used to bound the\n        collection of points. Used to initialize the :class:`MultiEllipsoid`\n        object returned in :meth:`bounding_ellipsoids`.\n\n    \"\"\"\n\n    npoints, ndim = points.shape\n\n    # Starting cluster centers are initialized using the major-axis\n    # endpoints of the original bounding ellipsoid.\n    p1, p2 = ell.major_axis_endpoints()\n    start_ctrs = np.vstack((p1, p2))  # shape is (k, ndim) = (2, ndim)\n\n    # Split points into two clusters using k-means clustering with k=2.\n    try:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            k2_res = kmeans2(points, k=start_ctrs, iter=10, minit='matrix',\n                             check_finite=False)\n        labels = k2_res[1]  # cluster identifier ; shape is (npoints,)\n\n        # Get points in each cluster.\n        points_k = [points[labels == k, :] for k in (0, 1)]\n\n        # If either cluster has less than ndim+1 points, the bounding ellipsoid\n        # will be ill-constrained. Reject the split and simply return the\n        # original ellipsoid bounding all the points.\n        if points_k[0].shape[0] < 2 * ndim or points_k[1].shape[0] < 2 * ndim:\n            return [ell]\n\n        # Bounding ellipsoid for each cluster, possibly enlarged\n        # to a minimum volume.\n        ells = [bounding_ellipsoid(points_j, pointvol=pointvol)\n                for points_j in points_k]\n\n        # If the total volume decreased by a factor of `vol_dec`, we accept\n        # the split into subsets. We then recursively split each subset.\n        if ells[0].vol + ells[1].vol < vol_dec * ell.vol:\n            return (_bounding_ellipsoids(points_k[0], ells[0],\n                                         pointvol=pointvol, vol_dec=vol_dec,\n                                         vol_check=vol_check) +\n                    _bounding_ellipsoids(points_k[1], ells[1],\n                                         pointvol=pointvol, vol_dec=vol_dec,\n                                         vol_check=vol_check))\n\n        # Otherwise, see if the total ellipsoid volume is larger than the\n        # minimum volume by a factor of `vol_check`. If it is, this indicates\n        # that there may be more than 2 clusters and we should try to\n        # subdivide further.\n        if ell.vol > vol_check * npoints * pointvol:\n            out = (_bounding_ellipsoids(points_k[0], ells[0],\n                                        pointvol=pointvol, vol_dec=vol_dec,\n                                        vol_check=vol_check) +\n                   _bounding_ellipsoids(points_k[1], ells[1],\n                                        pointvol=pointvol, vol_dec=vol_dec,\n                                        vol_check=vol_check))\n\n            # Only accept the split if the volume decreased significantly.\n            if sum(e.vol for e in out) < vol_dec * ell.vol:\n                return out\n    except:\n        pass\n\n    # Otherwise, we are happy with the single bounding ellipsoid.\n    return [ell]", "response": "Internal method that computes a set of bounding ellipsoids for a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bounding_ellipsoids(points, pointvol=0., vol_dec=0.5, vol_check=2.):\n\n    if not HAVE_KMEANS:\n        raise ValueError(\"scipy.cluster.vq.kmeans2 is required to compute \"\n                         \"ellipsoid decompositions.\")  # pragma: no cover\n\n    # Calculate the bounding ellipsoid for the points possibly\n    # enlarged to a minimum volume.\n    ell = bounding_ellipsoid(points, pointvol=pointvol)\n\n    # Recursively split the bounding ellipsoid until the volume of each\n    # split no longer decreases by a factor of `vol_dec`.\n    ells = _bounding_ellipsoids(points, ell, pointvol=pointvol,\n                                vol_dec=vol_dec, vol_check=vol_check)\n\n    return MultiEllipsoid(ells=ells)", "response": "Returns a set of Elevation objects that are bound to a collection of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ellipsoid_bootstrap_expand(args):\n\n    # Unzipping.\n    points, pointvol = args\n    rstate = np.random\n\n    # Resampling.\n    npoints, ndim = points.shape\n    idxs = rstate.randint(npoints, size=npoints)  # resample\n    idx_in = np.unique(idxs)  # selected objects\n    sel = np.ones(npoints, dtype='bool')\n    sel[idx_in] = False\n    idx_out = np.arange(npoints)[sel]  # \"missing\" objects\n    if len(idx_out) < 2:  # edge case\n        idx_out = np.append(idx_out, [0, 1])\n    points_in, points_out = points[idx_in], points[idx_out]\n\n    # Compute bounding ellipsoid.\n    ell = bounding_ellipsoid(points_in, pointvol=pointvol)\n\n    # Compute normalized distances to missing points.\n    dists = [ell.distance(p) for p in points_out]\n\n    # Compute expansion factor.\n    expand = max(1., max(dists))\n\n    return expand", "response": "Internal method used to compute the expansion factor for a bounding\n    ellipsoid based on bootstrapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ellipsoids_bootstrap_expand(args):\n\n    # Unzipping.\n    points, pointvol, vol_dec, vol_check = args\n    rstate = np.random\n\n    # Resampling.\n    npoints, ndim = points.shape\n    idxs = rstate.randint(npoints, size=npoints)  # resample\n    idx_in = np.unique(idxs)  # selected objects\n    sel = np.ones(npoints, dtype='bool')\n    sel[idx_in] = False\n    idx_out = np.where(sel)[0]  # \"missing\" objects\n    if len(idx_out) < 2:  # edge case\n        idx_out = np.append(idx_out, [0, 1])\n    points_in, points_out = points[idx_in], points[idx_out]\n\n    # Compute bounding ellipsoids.\n    ell = bounding_ellipsoid(points_in, pointvol=pointvol)\n    ells = _bounding_ellipsoids(points_in, ell, pointvol=pointvol,\n                                vol_dec=vol_dec, vol_check=vol_check)\n\n    # Compute normalized distances to missing points.\n    dists = [min([el.distance(p) for el in ells]) for p in points_out]\n\n    # Compute expansion factor.\n    expand = max(1., max(dists))\n\n    return expand", "response": "Internal method used to compute the expansion factor for a collection\n    of bounding ellipsoids using bootstrapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _friends_bootstrap_radius(args):\n\n    # Unzipping.\n    points, ftype = args\n    rstate = np.random\n\n    # Resampling.\n    npoints, ndim = points.shape\n    idxs = rstate.randint(npoints, size=npoints)  # resample\n    idx_in = np.unique(idxs)  # selected objects\n    sel = np.ones(npoints, dtype='bool')\n    sel[idx_in] = False\n    idx_out = np.where(sel)[0]  # \"missing\" objects\n    if len(idx_out) < 2:  # edge case\n        idx_out = np.append(idx_out, [0, 1])\n    points_in, points_out = points[idx_in], points[idx_out]\n\n    # Construct KDTree to enable quick nearest-neighbor lookup for\n    # our resampled objects.\n    kdtree = spatial.KDTree(points_in)\n\n    if ftype == 'balls':\n        # Compute distances from our \"missing\" points its closest neighbor\n        # among the resampled points using the Euclidean norm\n        # (i.e. \"radius\" of n-sphere).\n        dists, ids = kdtree.query(points_out, k=1, eps=0, p=2)\n    elif ftype == 'cubes':\n        # Compute distances from our \"missing\" points its closest neighbor\n        # among the resampled points using the Euclidean norm\n        # (i.e. \"half-side-length\" of n-cube).\n        dists, ids = kdtree.query(points_out, k=1, eps=0, p=np.inf)\n\n    # Conservative upper-bound on radius.\n    dist = max(dists)\n\n    return dist", "response": "Internal method used to compute the radius for each ball used in RadFriends using\n    bootstrapping."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample(self, rstate=None):\n\n        if rstate is None:\n            rstate = np.random\n\n        return rstate.rand(self.n)", "response": "Draw a sample uniformly distributed within the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw nsamples samples randomly distributed within the unit cube.", "response": "def samples(self, nsamples, rstate=None):\n        \"\"\"\n        Draw `nsamples` samples randomly distributed within the unit cube.\n\n        Returns\n        -------\n        x : `~numpy.ndarray` with shape (nsamples, ndim)\n            A collection of coordinates within the unit cube.\n\n        \"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        xs = np.array([self.sample(rstate=rstate) for i in range(nsamples)])\n\n        return xs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscale ellipoid to a target volume.", "response": "def scale_to_vol(self, vol):\n        \"\"\"Scale ellipoid to a target volume.\"\"\"\n\n        f = np.exp((np.log(vol) - np.log(self.vol)) / self.n)  # linear factor\n        self.expand *= f\n        self.cov *= f**2\n        self.am *= f**-2\n        self.axlens *= f\n        self.axes *= f\n        self.vol = vol"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the endpoints of the major axis.", "response": "def major_axis_endpoints(self):\n        \"\"\"Return the endpoints of the major axis.\"\"\"\n\n        i = np.argmax(self.axlens)  # find the major axis\n        v = self.paxes[:, i]  # vector from center to major axis endpoint\n\n        return self.ctr - v, self.ctr + v"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the normalized distance to x from the center of the ellipsoid.", "response": "def distance(self, x):\n        \"\"\"Compute the normalized distance to `x` from the center of the\n        ellipsoid.\"\"\"\n\n        d = x - self.ctr\n\n        return np.sqrt(np.dot(np.dot(d, self.am), d))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a random offset from the center of the ellipsoid.", "response": "def randoffset(self, rstate=None):\n        \"\"\"Return a random offset from the center of the ellipsoid.\"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        return np.dot(self.axes, randsphere(self.n, rstate=rstate))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample(self, rstate=None):\n\n        if rstate is None:\n            rstate = np.random\n\n        return self.ctr + self.randoffset(rstate=rstate)", "response": "Draw a sample uniformly distributed within the ellipsoid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unitcube_overlap(self, ndraws=10000, rstate=None):\n\n        if rstate is None:\n            rstate = np.random\n\n        samples = [self.sample(rstate=rstate) for i in range(ndraws)]\n        nin = sum([unitcheck(x) for x in samples])\n\n        return 1. * nin / ndraws", "response": "Using ndraws Monte Carlo draws estimate the fraction of\n        overlap between the ellipsoid and the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, points, pointvol=0., rstate=None, bootstrap=0,\n               pool=None, mc_integrate=False):\n        \"\"\"\n        Update the ellipsoid to bound the collection of points.\n\n        Parameters\n        ----------\n        points : `~numpy.ndarray` with shape (npoints, ndim)\n            The set of points to bound.\n\n        pointvol : float, optional\n            The minimum volume associated with each point. Default is `0.`.\n\n        rstate : `~numpy.random.RandomState`, optional\n            `~numpy.random.RandomState` instance.\n\n        bootstrap : int, optional\n            The number of bootstrapped realizations of the ellipsoid. The\n            maximum distance to the set of points \"left out\" during each\n            iteration is used to enlarge the resulting volumes.\n            Default is `0`.\n\n        pool : user-provided pool, optional\n            Use this pool of workers to execute operations in parallel.\n\n        mc_integrate : bool, optional\n            Whether to use Monte Carlo methods to compute the effective\n            overlap of the final ellipsoid with the unit cube.\n            Default is `False`.\n\n        \"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        # Compute new bounding ellipsoid.\n        ell = bounding_ellipsoid(points, pointvol=pointvol)\n        self.n = ell.n\n        self.ctr = ell.ctr\n        self.cov = ell.cov\n        self.am = ell.am\n        self.vol = ell.vol\n        self.axlens = ell.axlens\n        self.axes = ell.axes\n        self.paxes = ell.paxes\n        self.expand = ell.expand\n\n        # Use bootstrapping to determine the volume expansion factor.\n        if bootstrap > 0:\n\n            # If provided, compute bootstraps in parallel using a pool.\n            if pool is None:\n                M = map\n            else:\n                M = pool.map\n            ps = [points for it in range(bootstrap)]\n            pvs = [pointvol for it in range(bootstrap)]\n            args = zip(ps, pvs)\n            expands = list(M(_ellipsoid_bootstrap_expand, args))\n\n            # Conservatively set the expansion factor to be the maximum\n            # factor derived from our set of bootstraps.\n            expand = max(expands)\n\n            # If our ellipsoid is over-constrained, expand it.\n            if expand > 1.:\n                v = self.vol * expand**self.n\n                self.scale_to_vol(v)\n\n        # Estimate the fractional overlap with the unit cube using\n        # Monte Carlo integration.\n        if mc_integrate:\n            self.funit = self.unitcube_overlap()", "response": "Update the internal state of the object with the given set of points."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scale_to_vols(self, vols):\n\n        [self.ells[i].scale_to_vol(vols[i]) for i in range(self.nells)]\n        self.vols = np.array(vols)\n        self.expands = np.array([self.ells[i].expand\n                                 for i in range(self.nells)])\n        vol_tot = sum(vols)\n        self.expand_tot *= vol_tot / self.vol_tot\n        self.vol_tot = vol_tot", "response": "Scale ellipoids to a corresponding set of\n        target volumes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks which ellipsoid(s) x falls within skipping the j - th ellipsoid.", "response": "def within(self, x, j=None):\n        \"\"\"Checks which ellipsoid(s) `x` falls within, skipping the `j`-th\n        ellipsoid.\"\"\"\n\n        # Loop through distance calculations if there aren't too many.\n        idxs = np.where([self.ells[i].contains(x) if i != j else True\n                         for i in range(self.nells)])[0]\n\n        return idxs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef overlap(self, x, j=None):\n\n        q = len(self.within(x, j=j))\n\n        return q", "response": "Checks how many ellipsoids x falls within j."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsampling a point uniformly distributed within the union of ellipsoids.", "response": "def sample(self, rstate=None, return_q=False):\n        \"\"\"\n        Sample a point uniformly distributed within the *union* of ellipsoids.\n\n        Returns\n        -------\n        x : `~numpy.ndarray` with shape (ndim,)\n            A coordinate within the set of ellipsoids.\n\n        idx : int\n            The index of the ellipsoid `x` was sampled from.\n\n        q : int, optional\n            The number of ellipsoids `x` falls within.\n\n        \"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        # If there is only one ellipsoid, sample from it.\n        if self.nells == 1:\n            x = self.ells[0].sample(rstate=rstate)\n            idx = 0\n            q = 1\n            if return_q:\n                return x, idx, q\n            else:\n                return x, idx\n\n        # Select an ellipsoid at random proportional to its volume.\n        idx = rstate.choice(self.nells, p=self.vols/self.vol_tot)\n\n        # Select a point from the chosen ellipsoid.\n        x = self.ells[idx].sample(rstate=rstate)\n\n        # Check how many ellipsoids the point lies within, passing over\n        # the `idx`-th ellipsoid `x` was sampled from.\n        q = self.overlap(x, j=idx) + 1\n\n        if return_q:\n            # If `q` is being returned, assume the user wants to\n            # explicitly apply the `1. / q` acceptance criterion to\n            # properly sample from the union of ellipsoids.\n            return x, idx, q\n        else:\n            # If `q` is not being returned, assume the user wants this\n            # done internally.\n            while rstate.rand() > (1. / q):\n                idx = rstate.choice(self.nells, p=self.vols/self.vol_tot)\n                x = self.ells[idx].sample(rstate=rstate)\n                q = self.overlap(x, j=idx) + 1\n\n            return x, idx"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef monte_carlo_vol(self, ndraws=10000, rstate=None,\n                        return_overlap=True):\n        \"\"\"Using `ndraws` Monte Carlo draws, estimate the volume of the\n        *union* of ellipsoids. If `return_overlap=True`, also returns the\n        estimated fractional overlap with the unit cube.\"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        # Estimate volume using Monte Carlo integration.\n        samples = [self.sample(rstate=rstate, return_q=True)\n                   for i in range(ndraws)]\n        qsum = sum([q for (x, idx, q) in samples])\n        vol = 1. * ndraws / qsum * self.vol_tot\n\n        if return_overlap:\n            # Estimate the fractional amount of overlap with the\n            # unit cube using the same set of samples.\n            qin = sum([q * unitcheck(x) for (x, idx, q) in samples])\n            overlap = 1. * qin / qsum\n            return vol, overlap\n        else:\n            return vol", "response": "Estimate the volume of the the\n            using Monte Carlo draws. If return_overlap = True also returns the estimated fractional overlap with the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the set of ellipsoids to bound the collection of points.", "response": "def update(self, points, pointvol=0., vol_dec=0.5, vol_check=2.,\n               rstate=None, bootstrap=0, pool=None, mc_integrate=False):\n        \"\"\"\n        Update the set of ellipsoids to bound the collection of points.\n\n        Parameters\n        ----------\n        points : `~numpy.ndarray` with shape (npoints, ndim)\n            The set of points to bound.\n\n        pointvol : float, optional\n            The minimum volume associated with each point. Default is `0.`.\n\n        vol_dec : float, optional\n            The required fractional reduction in volume after splitting\n            an ellipsoid in order to to accept the split.\n            Default is `0.5`.\n\n        vol_check : float, optional\n            The factor used when checking if the volume of the original\n            bounding ellipsoid is large enough to warrant `> 2` splits\n            via `ell.vol > vol_check * nlive * pointvol`.\n            Default is `2.0`.\n\n        rstate : `~numpy.random.RandomState`, optional\n            `~numpy.random.RandomState` instance.\n\n        bootstrap : int, optional\n            The number of bootstrapped realizations of the ellipsoids. The\n            maximum distance to the set of points \"left out\" during each\n            iteration is used to enlarge the resulting volumes.\n            Default is `0`.\n\n        pool : user-provided pool, optional\n            Use this pool of workers to execute operations in parallel.\n\n        mc_integrate : bool, optional\n            Whether to use Monte Carlo methods to compute the effective\n            volume and fractional overlap of the final union of ellipsoids\n            with the unit cube. Default is `False`.\n\n        \"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        if not HAVE_KMEANS:\n            raise ValueError(\"scipy.cluster.vq.kmeans2 is required \"\n                             \"to compute ellipsoid decompositions.\")\n\n        npoints, ndim = points.shape\n\n        # Calculate the bounding ellipsoid for the points, possibly\n        # enlarged to a minimum volume.\n        firstell = bounding_ellipsoid(points, pointvol=pointvol)\n\n        # Recursively split the bounding ellipsoid using `vol_check`\n        # until the volume of each split no longer decreases by a\n        # factor of `vol_dec`.\n        ells = _bounding_ellipsoids(points, firstell, pointvol=pointvol,\n                                    vol_dec=vol_dec, vol_check=vol_check)\n\n        # Update the set of ellipsoids.\n        self.nells = len(ells)\n        self.ells = ells\n        self.ctrs = np.array([ell.ctr for ell in self.ells])\n        self.covs = np.array([ell.cov for ell in self.ells])\n        self.ams = np.array([ell.am for ell in self.ells])\n        self.vols = np.array([ell.vol for ell in self.ells])\n        self.vol_tot = sum(self.vols)\n\n        # Compute expansion factor.\n        expands = np.array([ell.expand for ell in self.ells])\n        vols_orig = self.vols / expands\n        vol_tot_orig = sum(vols_orig)\n        self.expand_tot = self.vol_tot / vol_tot_orig\n\n        # Use bootstrapping to determine the volume expansion factor.\n        if bootstrap > 0:\n\n            # If provided, compute bootstraps in parallel using a pool.\n            if pool is None:\n                M = map\n            else:\n                M = pool.map\n            ps = [points for it in range(bootstrap)]\n            pvs = [pointvol for it in range(bootstrap)]\n            vds = [vol_dec for it in range(bootstrap)]\n            vcs = [vol_check for it in range(bootstrap)]\n            args = zip(ps, pvs, vds, vcs)\n            expands = list(M(_ellipsoids_bootstrap_expand, args))\n\n            # Conservatively set the expansion factor to be the maximum\n            # factor derived from our set of bootstraps.\n            expand = max(expands)\n\n            # If our ellipsoids are overly constrained, expand them.\n            if expand > 1.:\n                vs = self.vols * expand**ndim\n                self.scale_to_vols(vs)\n\n        # Estimate the volume and fractional overlap with the unit cube\n        # using Monte Carlo integration.\n        if mc_integrate:\n            self.vol, self.funit = self.monte_carlo_vol(return_overlap=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scale_to_vol(self, vol):\n\n        f = (vol / self.vol_ball) ** (1.0 / self.n)  # linear factor\n        self.expand *= f\n        self.radius *= f\n        self.vol_ball = vol", "response": "Scale ball to encompass a target volume."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef within(self, x, ctrs, kdtree=None):\n\n        if kdtree is None:\n            # If no K-D Tree is provided, execute a brute-force\n            # search over all balls.\n            idxs = np.where(lalg.norm(ctrs - x, axis=1) <= self.radius)[0]\n        else:\n            # If a K-D Tree is provided, find all points within `self.radius`.\n            idxs = kdtree.query_ball_point(x, self.radius, p=2.0, eps=0)\n\n        return idxs", "response": "Check which balls x falls within. Uses a K - D Tree to find all points within self. radius."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef overlap(self, x, ctrs, kdtree=None):\n\n        q = len(self.within(x, ctrs, kdtree=kdtree))\n\n        return q", "response": "Check how many balls x falls within. Uses a K - D Tree to find the overlap."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contains(self, x, ctrs, kdtree=None):\n\n        return self.overlap(x, ctrs, kdtree=kdtree) > 0", "response": "Check if the set of balls contains x. Uses a K - D Tree to perform the search if provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, points, pointvol=0., rstate=None, bootstrap=0,\n               pool=None, kdtree=None, mc_integrate=False):\n        \"\"\"\n        Update the radii of our balls.\n\n        Parameters\n        ----------\n        points : `~numpy.ndarray` with shape (npoints, ndim)\n            The set of points to bound.\n\n        pointvol : float, optional\n            The minimum volume associated with each point. Default is `0.`.\n\n        rstate : `~numpy.random.RandomState`, optional\n            `~numpy.random.RandomState` instance.\n\n        bootstrap : int, optional\n            The number of bootstrapped realizations of the ellipsoids. The\n            maximum distance to the set of points \"left out\" during each\n            iteration is used to enlarge the resulting volumes.\n            Default is `0`.\n\n        pool : user-provided pool, optional\n            Use this pool of workers to execute operations in parallel.\n\n        kdtree : `~scipy.spatial.KDTree`, optional\n            K-D Tree used to perform nearest neighbor searches.\n\n        mc_integrate : bool, optional\n            Whether to use Monte Carlo methods to compute the effective\n            volume and fractional overlap of the final union of balls\n            with the unit cube. Default is `False`.\n\n        \"\"\"\n\n        # If possible, compute bootstraps in parallel using a pool.\n        if pool is None:\n            M = map\n        else:\n            M = pool.map\n\n        if bootstrap == 0.:\n            # Construct radius using leave-one-out if no bootstraps used.\n            radii = _friends_leaveoneout_radius(points, 'balls')\n        else:\n            # Bootstrap radius using the set of live points.\n            ps = [points for it in range(bootstrap)]\n            ftypes = ['balls' for it in range(bootstrap)]\n            args = zip(ps, ftypes)\n            radii = list(M(_friends_bootstrap_radius, args))\n\n        # Conservatively set radius to be maximum of the set.\n        rmax = max(radii)\n        self.radius = rmax\n        self.vol_ball = vol_prefactor(self.n) * self.radius**self.n\n        self.expand = 1.\n\n        # Expand our ball to encompass a minimum volume.\n        if pointvol > 0.:\n            v = pointvol\n            if self.vol_ball < v:\n                self.scale_to_vol(v)\n\n        # Estimate the volume and fractional overlap with the unit cube\n        # using Monte Carlo integration.\n        if mc_integrate:\n            self.vol, self.funit = self.monte_carlo_vol(points, kdtree=kdtree,\n                                                        return_overlap=True)", "response": "Update the radius of our balls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscale cube to encompass a target volume.", "response": "def scale_to_vol(self, vol):\n        \"\"\"Scale cube to encompass a target volume.\"\"\"\n\n        f = (vol / self.vol_cube) ** (1.0 / self.n)  # linear factor\n        self.expand *= f\n        self.hside *= f\n        self.vol_cube = vol"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef within(self, x, ctrs, kdtree=None):\n\n        if kdtree is None:\n            # If no KDTree is provided, execute a brute-force search\n            # over all cubes.\n            idxs = np.where(np.max(np.abs(ctrs - x), axis=1) <= self.hside)[0]\n        else:\n            # If a KDTree is provided, find all points within r (`hside`).\n            idxs = kdtree.query_ball_point(x, self.hside, p=np.inf, eps=0)\n\n        return idxs", "response": "Checks which cubes x falls within. Uses a K - D Tree to perform the search if provided."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample(self, ctrs, rstate=None, return_q=False, kdtree=None):\n\n        if rstate is None:\n            rstate = np.random\n\n        nctrs = len(ctrs)  # number of cubes\n\n        # If there is only one cube, sample from it.\n        if nctrs == 1:\n            dx = self.hside * (2. * rstate.rand(self.n) - 1.)\n            x = ctrs[0] + dx\n            if return_q:\n                return x, 1\n            else:\n                return x\n\n        # Select a cube at random.\n        idx = rstate.randint(nctrs)\n\n        # Select a point from the chosen cube.\n        dx = self.hside * (2. * rstate.rand(self.n) - 1.)\n        x = ctrs[idx] + dx\n\n        # Check how many cubes the point lies within, passing over\n        # the `idx`-th cube `x` was sampled from.\n        q = self.overlap(x, ctrs, kdtree=kdtree)\n\n        if return_q:\n            # If `q` is being returned, assume the user wants to\n            # explicitly apply the `1. / q` acceptance criterion to\n            # properly sample from the union of balls.\n            return x, q\n        else:\n            # If `q` is not being returned, assume the user wants this\n            # done internally.\n            while rstate.rand() > (1. / q):\n                idx = rstate.randint(nctrs)\n                dx = self.hside * (2. * rstate.rand(self.n) - 1.)\n                x = ctrs[idx] + dx\n                q = self.overlap(x, ctrs, kdtree=kdtree)\n            return x", "response": "Sample a point uniformly distributed within the union of cubes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate the volume of the cluster using Monte Carlo draws.", "response": "def monte_carlo_vol(self, ctrs, ndraws=10000, rstate=None,\n                        return_overlap=False, kdtree=None):\n        \"\"\"Using `ndraws` Monte Carlo draws, estimate the volume of the\n        *union* of cubes. If `return_overlap=True`, also returns the\n        estimated fractional overlap with the unit cube. Uses a K-D Tree\n        to perform the search if provided.\"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        # Estimate the volume using Monte Carlo integration.\n        samples = [self.sample(ctrs, rstate=rstate, return_q=True,\n                               kdtree=kdtree)\n                   for i in range(ndraws)]\n        qsum = sum([q for (x, q) in samples])\n        vol = 1. * ndraws / qsum * len(ctrs) * self.vol_cube\n\n        if return_overlap:\n            # Estimate the fractional overlap with the unit cube using\n            # the same set of samples.\n            qin = sum([q * unitcheck(x) for (x, q) in samples])\n            overlap = 1. * qin / qsum\n            return vol, overlap\n        else:\n            return vol"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, points, pointvol=0., rstate=None, bootstrap=0,\n               pool=None, kdtree=None, mc_integrate=False):\n        \"\"\"\n        Update the half-side-lengths of our cubes.\n\n        Parameters\n        ----------\n        points : `~numpy.ndarray` with shape (npoints, ndim)\n            The set of points to bound.\n\n        pointvol : float, optional\n            The minimum volume associated with each point. Default is `0.`.\n\n        rstate : `~numpy.random.RandomState`, optional\n            `~numpy.random.RandomState` instance.\n\n        bootstrap : int, optional\n            The number of bootstrapped realizations of the ellipsoids. The\n            maximum distance to the set of points \"left out\" during each\n            iteration is used to enlarge the resulting volumes.\n            Default is `0`.\n\n        pool : user-provided pool, optional\n            Use this pool of workers to execute operations in parallel.\n\n        kdtree : `~scipy.spatial.KDTree`, optional\n            K-D Tree used to perform nearest neighbor searches.\n\n        mc_integrate : bool, optional\n            Whether to use Monte Carlo methods to compute the effective\n            volume and fractional overlap of the final union of balls\n            with the unit cube. Default is `False`.\n\n        \"\"\"\n\n        if rstate is None:\n            rstate = np.random\n\n        # If possible, compute bootstraps in parallel using a pool.\n        if pool is None:\n            M = map\n        else:\n            M = pool.map\n\n        if bootstrap == 0.:\n            # Construct radius using leave-one-out if no bootstraps used.\n            hsides = _friends_leaveoneout_radius(points, 'cubes')\n        else:\n            # Bootstrap radius using the set of live points.\n            ps = [points for it in range(bootstrap)]\n            ftypes = ['cubes' for it in range(bootstrap)]\n            args = zip(ps, ftypes)\n            hsides = list(M(_friends_bootstrap_radius, args))\n\n        # Conservatively set radius to be maximum of the set.\n        hsmax = max(hsides)\n        self.hside = hsmax\n        self.vol_cube = (2. * self.hside)**self.n\n        self.expand = 1.\n\n        # Expand our cube to encompass a minimum volume.\n        if pointvol > 0.:\n            v = pointvol\n            if self.vol_cube < v:\n                self.scale_to_vol(v)\n\n        # Estimate the volume and fractional overlap with the unit cube\n        # using Monte Carlo integration.\n        if mc_integrate:\n            self.vol, self.funit = self.monte_carlo_vol(points, kdtree=kdtree,\n                                                        return_overlap=True)", "response": "Update the half - side lengths of our cubes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate a new point sampled uniformly from a bounding proposal distribution.", "response": "def sample_unif(args):\n    \"\"\"\n    Evaluate a new point sampled uniformly from a bounding proposal\n    distribution. Parameters are zipped within `args` to utilize\n    `pool.map`-style functions.\n\n    Parameters\n    ----------\n    u : `~numpy.ndarray` with shape (npdim,)\n        Position of the initial sample.\n\n    loglstar : float\n        Ln(likelihood) bound. **Not applicable here.**\n\n    axes : `~numpy.ndarray` with shape (ndim, ndim)\n        Axes used to propose new points. **Not applicable here.**\n\n    scale : float\n        Value used to scale the provided axes. **Not applicable here.**\n\n    prior_transform : function\n        Function transforming a sample from the a unit cube to the parameter\n        space of interest according to the prior.\n\n    loglikelihood : function\n        Function returning ln(likelihood) given parameters as a 1-d `~numpy`\n        array of length `ndim`.\n\n    kwargs : dict\n        A dictionary of additional method-specific parameters.\n        **Not applicable here.**\n\n    Returns\n    -------\n    u : `~numpy.ndarray` with shape (npdim,)\n        Position of the final proposed point within the unit cube. **For\n        uniform sampling this is the same as the initial input position.**\n\n    v : `~numpy.ndarray` with shape (ndim,)\n        Position of the final proposed point in the target parameter space.\n\n    logl : float\n        Ln(likelihood) of the final proposed point.\n\n    nc : int\n        Number of function calls used to generate the sample. For uniform\n        sampling this is `1` by construction.\n\n    blob : dict\n        Collection of ancillary quantities used to tune :data:`scale`. **Not\n        applicable for uniform sampling.**\n\n    \"\"\"\n\n    # Unzipping.\n    (u, loglstar, axes, scale,\n     prior_transform, loglikelihood, kwargs) = args\n\n    # Evaluate.\n    v = prior_transform(np.array(u))\n    logl = loglikelihood(np.array(v))\n    nc = 1\n    blob = None\n\n    return u, v, logl, nc, blob"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new live point proposed by random walking away from an existing live point.", "response": "def sample_rwalk(args):\n    \"\"\"\n    Return a new live point proposed by random walking away from an\n    existing live point.\n\n    Parameters\n    ----------\n    u : `~numpy.ndarray` with shape (npdim,)\n        Position of the initial sample. **This is a copy of an existing live\n        point.**\n\n    loglstar : float\n        Ln(likelihood) bound.\n\n    axes : `~numpy.ndarray` with shape (ndim, ndim)\n        Axes used to propose new points. For random walks new positions are\n        proposed using the :class:`~dynesty.bounding.Ellipsoid` whose\n        shape is defined by axes.\n\n    scale : float\n        Value used to scale the provided axes.\n\n    prior_transform : function\n        Function transforming a sample from the a unit cube to the parameter\n        space of interest according to the prior.\n\n    loglikelihood : function\n        Function returning ln(likelihood) given parameters as a 1-d `~numpy`\n        array of length `ndim`.\n\n    kwargs : dict\n        A dictionary of additional method-specific parameters.\n\n    Returns\n    -------\n    u : `~numpy.ndarray` with shape (npdim,)\n        Position of the final proposed point within the unit cube.\n\n    v : `~numpy.ndarray` with shape (ndim,)\n        Position of the final proposed point in the target parameter space.\n\n    logl : float\n        Ln(likelihood) of the final proposed point.\n\n    nc : int\n        Number of function calls used to generate the sample.\n\n    blob : dict\n        Collection of ancillary quantities used to tune :data:`scale`.\n\n    \"\"\"\n\n    # Unzipping.\n    (u, loglstar, axes, scale,\n     prior_transform, loglikelihood, kwargs) = args\n    rstate = np.random\n\n    # Periodicity.\n    nonperiodic = kwargs.get('nonperiodic', None)\n\n    # Setup.\n    n = len(u)\n    walks = kwargs.get('walks', 25)  # number of steps\n    accept = 0\n    reject = 0\n    fail = 0\n    nfail = 0\n    nc = 0\n    ncall = 0\n\n    drhat, dr, du, u_prop, logl_prop = np.nan, np.nan, np.nan, np.nan, np.nan\n    while nc < walks or accept == 0:\n        while True:\n\n            # Check scale-factor.\n            if scale == 0.:\n                raise RuntimeError(\"The random walk sampling is stuck! \"\n                                   \"Some useful output quantities:\\n\"\n                                   \"u: {0}\\n\"\n                                   \"drhat: {1}\\n\"\n                                   \"dr: {2}\\n\"\n                                   \"du: {3}\\n\"\n                                   \"u_prop: {4}\\n\"\n                                   \"loglstar: {5}\\n\"\n                                   \"logl_prop: {6}\\n\"\n                                   \"axes: {7}\\n\"\n                                   \"scale: {8}.\"\n                                   .format(u, drhat, dr, du, u_prop,\n                                           loglstar, logl_prop, axes, scale))\n\n            # Propose a direction on the unit n-sphere.\n            drhat = rstate.randn(n)\n            drhat /= linalg.norm(drhat)\n\n            # Scale based on dimensionality.\n            dr = drhat * rstate.rand()**(1./n)\n\n            # Transform to proposal distribution.\n            du = np.dot(axes, dr)\n            u_prop = u + scale * du\n\n            # Check unit cube constraints.\n            if unitcheck(u_prop, nonperiodic):\n                break\n            else:\n                fail += 1\n                nfail += 1\n\n            # Check if we're stuck generating bad numbers.\n            if fail > 100 * walks:\n                warnings.warn(\"Random number generation appears to be \"\n                              \"extremely inefficient. Adjusting the \"\n                              \"scale-factor accordingly.\")\n                fail = 0\n                scale *= math.exp(-1. / n)\n\n        # Check proposed point.\n        v_prop = prior_transform(np.array(u_prop))\n        logl_prop = loglikelihood(np.array(v_prop))\n        if logl_prop >= loglstar:\n            u = u_prop\n            v = v_prop\n            logl = logl_prop\n            accept += 1\n        else:\n            reject += 1\n        nc += 1\n        ncall += 1\n\n        # Check if we're stuck generating bad points.\n        if nc > 50 * walks:\n            scale *= math.exp(-1. / n)\n            warnings.warn(\"Random walk proposals appear to be \"\n                          \"extremely inefficient. Adjusting the \"\n                          \"scale-factor accordingly.\")\n            nc, accept, reject = 0, 0, 0  # reset values\n\n    blob = {'accept': accept, 'reject': reject, 'fail': nfail, 'scale': scale}\n\n    return u, v, logl, ncall, blob"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_slice(args):\n\n    # Unzipping.\n    (u, loglstar, axes, scale,\n     prior_transform, loglikelihood, kwargs) = args\n    rstate = np.random\n\n    # Periodicity.\n    nonperiodic = kwargs.get('nonperiodic', None)\n\n    # Setup.\n    n = len(u)\n    slices = kwargs.get('slices', 5)  # number of slices\n    nc = 0\n    nexpand = 0\n    ncontract = 0\n    fscale = []\n\n    # Modifying axes and computing lengths.\n    axes = scale * axes.T  # scale based on past tuning\n    axlens = [linalg.norm(axis) for axis in axes]\n\n    # Slice sampling loop.\n    for it in range(slices):\n\n        # Shuffle axis update order.\n        idxs = np.arange(n)\n        rstate.shuffle(idxs)\n\n        # Slice sample along a random direction.\n        for idx in idxs:\n\n            # Select axis.\n            axis = axes[idx]\n            axlen = axlens[idx]\n\n            # Define starting \"window\".\n            r = rstate.rand()  # initial scale/offset\n            u_l = u - r * axis  # left bound\n            if unitcheck(u_l, nonperiodic):\n                v_l = prior_transform(np.array(u_l))\n                logl_l = loglikelihood(np.array(v_l))\n            else:\n                logl_l = -np.inf\n            nc += 1\n            nexpand += 1\n            u_r = u + (1 - r) * axis  # right bound\n            if unitcheck(u_r, nonperiodic):\n                v_r = prior_transform(np.array(u_r))\n                logl_r = loglikelihood(np.array(v_r))\n            else:\n                logl_r = -np.inf\n            nc += 1\n            nexpand += 1\n\n            # \"Stepping out\" the left and right bounds.\n            while logl_l >= loglstar:\n                u_l -= axis\n                if unitcheck(u_l, nonperiodic):\n                    v_l = prior_transform(np.array(u_l))\n                    logl_l = loglikelihood(np.array(v_l))\n                else:\n                    logl_l = -np.inf\n                nc += 1\n                nexpand += 1\n            while logl_r >= loglstar:\n                u_r += axis\n                if unitcheck(u_r, nonperiodic):\n                    v_r = prior_transform(np.array(u_r))\n                    logl_r = loglikelihood(np.array(v_r))\n                else:\n                    logl_r = -np.inf\n                nc += 1\n                nexpand += 1\n\n            # Sample within limits. If the sample is not valid, shrink\n            # the limits until we hit the `loglstar` bound.\n            while True:\n                u_hat = u_r - u_l\n                u_prop = u_l + rstate.rand() * u_hat  # scale from left\n                if unitcheck(u_prop, nonperiodic):\n                    v_prop = prior_transform(np.array(u_prop))\n                    logl_prop = loglikelihood(np.array(v_prop))\n                else:\n                    logl_prop = -np.inf\n                nc += 1\n                ncontract += 1\n                # If we succeed, move to the new position.\n                if logl_prop >= loglstar:\n                    window = linalg.norm(u_hat)  # length of window\n                    fscale.append(window / axlen)\n                    u = u_prop\n                    break\n                # If we fail, check if the new point is to the left/right of\n                # our original point along our proposal axis and update\n                # the bounds accordingly.\n                else:\n                    s = np.dot(u_prop - u, u_hat)  # check sign (+/-)\n                    if s < 0:  # left\n                        u_l = u_prop\n                    elif s > 0:  # right\n                        u_r = u_prop\n                    else:\n                        raise RuntimeError(\"Slice sampler has failed to find \"\n                                           \"a valid point. Some useful \"\n                                           \"output quantities:\\n\"\n                                           \"u: {0}\\n\"\n                                           \"u_left: {1}\\n\"\n                                           \"u_right: {2}\\n\"\n                                           \"u_hat: {3}\\n\"\n                                           \"u_prop: {4}\\n\"\n                                           \"loglstar: {5}\\n\"\n                                           \"logl_prop: {6}\\n\"\n                                           \"axes: {7}\\n\"\n                                           \"axlens: {8}\\n\"\n                                           \"s: {9}.\"\n                                           .format(u, u_l, u_r, u_hat, u_prop,\n                                                   loglstar, logl_prop,\n                                                   axes, axlens, s))\n\n    blob = {'fscale': np.mean(fscale),\n            'nexpand': nexpand, 'ncontract': ncontract}\n\n    return u_prop, v_prop, logl_prop, nc, blob", "response": "Returns a new live point proposed by a series of random slices away from an existing live point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new live point proposed by Hamiltonian Slice Sampling", "response": "def sample_hslice(args):\n    \"\"\"\n    Return a new live point proposed by \"Hamiltonian\" Slice Sampling\n    using a series of random trajectories away from an existing live point.\n    Each trajectory is based on the provided axes and samples are determined\n    by moving forwards/backwards in time until the trajectory hits an edge\n    and approximately reflecting off the boundaries.\n    Once a series of reflections has been established, we propose a new live\n    point by slice sampling across the entire path.\n\n    Parameters\n    ----------\n    u : `~numpy.ndarray` with shape (npdim,)\n        Position of the initial sample. **This is a copy of an existing live\n        point.**\n\n    loglstar : float\n        Ln(likelihood) bound.\n\n    axes : `~numpy.ndarray` with shape (ndim, ndim)\n        Axes used to propose new slice directions.\n\n    scale : float\n        Value used to scale the provided axes.\n\n    prior_transform : function\n        Function transforming a sample from the a unit cube to the parameter\n        space of interest according to the prior.\n\n    loglikelihood : function\n        Function returning ln(likelihood) given parameters as a 1-d `~numpy`\n        array of length `ndim`.\n\n    kwargs : dict\n        A dictionary of additional method-specific parameters.\n\n    Returns\n    -------\n    u : `~numpy.ndarray` with shape (npdim,)\n        Position of the final proposed point within the unit cube.\n\n    v : `~numpy.ndarray` with shape (ndim,)\n        Position of the final proposed point in the target parameter space.\n\n    logl : float\n        Ln(likelihood) of the final proposed point.\n\n    nc : int\n        Number of function calls used to generate the sample.\n\n    blob : dict\n        Collection of ancillary quantities used to tune :data:`scale`.\n\n    \"\"\"\n\n    # Unzipping.\n    (u, loglstar, axes, scale,\n     prior_transform, loglikelihood, kwargs) = args\n    rstate = np.random\n\n    # Periodicity.\n    nonperiodic = kwargs.get('nonperiodic', None)\n\n    # Setup.\n    n = len(u)\n    slices = kwargs.get('slices', 5)  # number of slices\n    grad = kwargs.get('grad', None)  # gradient of log-likelihood\n    max_move = kwargs.get('max_move', 100)  # limit for `ncall`\n    compute_jac = kwargs.get('compute_jac', False)  # whether Jacobian needed\n    jitter = 0.25  # 25% jitter\n    nc = 0\n    nmove = 0\n    nreflect = 0\n    ncontract = 0\n\n    # Slice sampling loop.\n    for it in range(slices):\n        # Define the left, \"inner\", and right \"nodes\" for a given chord.\n        # We will plan to slice sampling using these chords.\n        nodes_l, nodes_m, nodes_r = [], [], []\n\n        # Propose a direction on the unit n-sphere.\n        drhat = rstate.randn(n)\n        drhat /= linalg.norm(drhat)\n\n        # Transform and scale based on past tuning.\n        axis = np.dot(axes, drhat) * scale * 0.01\n\n        # Create starting window.\n        vel = np.array(axis)  # current velocity\n        u_l = u - rstate.uniform(1. - jitter, 1. + jitter) * vel\n        u_r = u + rstate.uniform(1. - jitter, 1. + jitter) * vel\n        nodes_l.append(np.array(u_l))\n        nodes_m.append(np.array(u))\n        nodes_r.append(np.array(u_r))\n\n        # Progress \"right\" (i.e. \"forwards\" in time).\n        reverse, reflect = False, False\n        u_r = np.array(u)\n        ncall = 0\n        while ncall <= max_move:\n\n            # Iterate until we can bracket the edge of the distribution.\n            nodes_l.append(np.array(u_r))\n            u_out, u_in = None, []\n            while True:\n                # Step forward.\n                u_r += rstate.uniform(1. - jitter, 1. + jitter) * vel\n                # Evaluate point.\n                if unitcheck(u_r, nonperiodic):\n                    v_r = prior_transform(np.array(u_r))\n                    logl_r = loglikelihood(np.array(v_r))\n                    nc += 1\n                    ncall += 1\n                    nmove += 1\n                else:\n                    logl_r = -np.inf\n                # Check if we satisfy the log-likelihood constraint\n                # (i.e. are \"in\" or \"out\" of bounds).\n                if logl_r < loglstar:\n                    if reflect:\n                        # If we are out of bounds and just reflected, we\n                        # reverse direction and terminate immediately.\n                        reverse = True\n                        nodes_l.pop()  # remove since chord does not exist\n                        break\n                    else:\n                        # If we're already in bounds, then we're safe.\n                        u_out = np.array(u_r)\n                        logl_out = logl_r\n                    # Check if we could compute gradients assuming we\n                    # terminated with the current `u_out`.\n                    if np.isfinite(logl_out):\n                        reverse = False\n                    else:\n                        reverse = True\n                else:\n                    reflect = False\n                    u_in.append(np.array(u_r))\n                # Check if we've bracketed the edge.\n                if u_out is not None:\n                    break\n            # Define the rest of our chord.\n            if len(nodes_l) == len(nodes_r) + 1:\n                try:\n                    u_in = u_in[rstate.choice(len(u_in))]  # pick point randomly\n                except:\n                    u_in = np.array(u)\n                    pass\n                nodes_m.append(np.array(u_in))\n                nodes_r.append(np.array(u_out))\n            # Check if we have turned around.\n            if reverse:\n                break\n\n            # Reflect off the boundary.\n            u_r, logl_r = u_out, logl_out\n            if grad is None:\n                # If the gradient is not provided, we will attempt to\n                # approximate it numerically using 2nd-order methods.\n                h = np.zeros(n)\n                for i in range(n):\n                    u_r_l, u_r_r = np.array(u_r), np.array(u_r)\n                    # right side\n                    u_r_r[i] += 1e-10\n                    if unitcheck(u_r_r, nonperiodic):\n                        v_r_r = prior_transform(np.array(u_r_r))\n                        logl_r_r = loglikelihood(np.array(v_r_r))\n                    else:\n                        logl_r_r = -np.inf\n                        reverse = True  # can't compute gradient\n                    nc += 1\n                    # left side\n                    u_r_l[i] -= 1e-10\n                    if unitcheck(u_r_l, nonperiodic):\n                        v_r_l = prior_transform(np.array(u_r_l))\n                        logl_r_l = loglikelihood(np.array(v_r_l))\n                    else:\n                        logl_r_l = -np.inf\n                        reverse = True  # can't compute gradient\n                    if reverse:\n                        break  # give up because we have to turn around\n                    nc += 1\n                    # compute dlnl/du\n                    h[i] = (logl_r_r - logl_r_l) / 2e-10\n            else:\n                # If the gradient is provided, evaluate it.\n                h = grad(v_r)\n                if compute_jac:\n                    jac = []\n                    # Evaluate and apply Jacobian dv/du if gradient\n                    # is defined as d(lnL)/dv instead of d(lnL)/du.\n                    for i in range(n):\n                        u_r_l, u_r_r = np.array(u_r), np.array(u_r)\n                        # right side\n                        u_r_r[i] += 1e-10\n                        if unitcheck(u_r_r, nonperiodic):\n                            v_r_r = prior_transform(np.array(u_r_r))\n                        else:\n                            reverse = True  # can't compute Jacobian\n                            v_r_r = np.array(v_r)  # assume no movement\n                        # left side\n                        u_r_l[i] -= 1e-10\n                        if unitcheck(u_r_l, nonperiodic):\n                            v_r_l = prior_transform(np.array(u_r_l))\n                        else:\n                            reverse = True  # can't compute Jacobian\n                            v_r_r = np.array(v_r)  # assume no movement\n                        if reverse:\n                            break  # give up because we have to turn around\n                        jac.append((v_r_r - v_r_l) / 2e-10)\n                    jac = np.array(jac)\n                    h = np.dot(jac, h)  # apply Jacobian\n                nc += 1\n            # Compute specular reflection off boundary.\n            vel_ref = vel - 2 * h * np.dot(vel, h) / linalg.norm(h)**2\n            dotprod = np.dot(vel_ref, vel)\n            dotprod /= linalg.norm(vel_ref) * linalg.norm(vel)\n            # Check angle of reflection.\n            if dotprod < -0.99:\n                # The reflection angle is sufficiently small that it might\n                # as well be a reflection.\n                reverse = True\n                break\n            else:\n                # If the reflection angle is sufficiently large, we\n                # proceed as normal to the new position.\n                vel = vel_ref\n                u_out = None\n                reflect = True\n                nreflect += 1\n\n        # Progress \"left\" (i.e. \"backwards\" in time).\n        reverse, reflect = False, False\n        vel = -np.array(axis)  # current velocity\n        u_l = np.array(u)\n        ncall = 0\n        while ncall <= max_move:\n\n            # Iterate until we can bracket the edge of the distribution.\n            # Use a doubling approach to try and locate the bounds faster.\n            nodes_r.append(np.array(u_l))\n            u_out, u_in = None, []\n            while True:\n                # Step forward.\n                u_l += rstate.uniform(1. - jitter, 1. + jitter) * vel\n                # Evaluate point.\n                if unitcheck(u_l, nonperiodic):\n                    v_l = prior_transform(np.array(u_l))\n                    logl_l = loglikelihood(np.array(v_l))\n                    nc += 1\n                    ncall += 1\n                    nmove += 1\n                else:\n                    logl_l = -np.inf\n                # Check if we satisfy the log-likelihood constraint\n                # (i.e. are \"in\" or \"out\" of bounds).\n                if logl_l < loglstar:\n                    if reflect:\n                        # If we are out of bounds and just reflected, we\n                        # reverse direction and terminate immediately.\n                        reverse = True\n                        nodes_r.pop()  # remove since chord does not exist\n                        break\n                    else:\n                        # If we're already in bounds, then we're safe.\n                        u_out = np.array(u_l)\n                        logl_out = logl_l\n                    # Check if we could compute gradients assuming we\n                    # terminated with the current `u_out`.\n                    if np.isfinite(logl_out):\n                        reverse = False\n                    else:\n                        reverse = True\n                else:\n                    reflect = False\n                    u_in.append(np.array(u_l))\n                # Check if we've bracketed the edge.\n                if u_out is not None:\n                    break\n            # Define the rest of our chord.\n            if len(nodes_r) == len(nodes_l) + 1:\n                try:\n                    u_in = u_in[rstate.choice(len(u_in))]  # pick point randomly\n                except:\n                    u_in = np.array(u)\n                    pass\n                nodes_m.append(np.array(u_in))\n                nodes_l.append(np.array(u_out))\n            # Check if we have turned around.\n            if reverse:\n                break\n\n            # Reflect off the boundary.\n            u_l, logl_l = u_out, logl_out\n            if grad is None:\n                # If the gradient is not provided, we will attempt to\n                # approximate it numerically using 2nd-order methods.\n                h = np.zeros(n)\n                for i in range(n):\n                    u_l_l, u_l_r = np.array(u_l), np.array(u_l)\n                    # right side\n                    u_l_r[i] += 1e-10\n                    if unitcheck(u_l_r, nonperiodic):\n                        v_l_r = prior_transform(np.array(u_l_r))\n                        logl_l_r = loglikelihood(np.array(v_l_r))\n                    else:\n                        logl_l_r = -np.inf\n                        reverse = True  # can't compute gradient\n                    nc += 1\n                    # left side\n                    u_l_l[i] -= 1e-10\n                    if unitcheck(u_l_l, nonperiodic):\n                        v_l_l = prior_transform(np.array(u_l_l))\n                        logl_l_l = loglikelihood(np.array(v_l_l))\n                    else:\n                        logl_l_l = -np.inf\n                        reverse = True  # can't compute gradient\n                    if reverse:\n                        break  # give up because we have to turn around\n                    nc += 1\n                    # compute dlnl/du\n                    h[i] = (logl_l_r - logl_l_l) / 2e-10\n            else:\n                # If the gradient is provided, evaluate it.\n                h = grad(v_l)\n                if compute_jac:\n                    jac = []\n                    # Evaluate and apply Jacobian dv/du if gradient\n                    # is defined as d(lnL)/dv instead of d(lnL)/du.\n                    for i in range(n):\n                        u_l_l, u_l_r = np.array(u_l), np.array(u_l)\n                        # right side\n                        u_l_r[i] += 1e-10\n                        if unitcheck(u_l_r, nonperiodic):\n                            v_l_r = prior_transform(np.array(u_l_r))\n                        else:\n                            reverse = True  # can't compute Jacobian\n                            v_l_r = np.array(v_l)  # assume no movement\n                        # left side\n                        u_l_l[i] -= 1e-10\n                        if unitcheck(u_l_l, nonperiodic):\n                            v_l_l = prior_transform(np.array(u_l_l))\n                        else:\n                            reverse = True  # can't compute Jacobian\n                            v_l_r = np.array(v_l)  # assume no movement\n                        if reverse:\n                            break  # give up because we have to turn around\n                        jac.append((v_l_r - v_l_l) / 2e-10)\n                    jac = np.array(jac)\n                    h = np.dot(jac, h)  # apply Jacobian\n                nc += 1\n            # Compute specular reflection off boundary.\n            vel_ref = vel - 2 * h * np.dot(vel, h) / linalg.norm(h)**2\n            dotprod = np.dot(vel_ref, vel)\n            dotprod /= linalg.norm(vel_ref) * linalg.norm(vel)\n            # Check angle of reflection.\n            if dotprod < -0.99:\n                # The reflection angle is sufficiently small that it might\n                # as well be a reflection.\n                reverse = True\n                break\n            else:\n                # If the reflection angle is sufficiently large, we\n                # proceed as normal to the new position.\n                vel = vel_ref\n                u_out = None\n                reflect = True\n                nreflect += 1\n\n        # Initialize lengths of chords.\n        if len(nodes_l) > 1:\n            # remove initial fallback chord\n            nodes_l.pop(0)\n            nodes_m.pop(0)\n            nodes_r.pop(0)\n        nodes_l, nodes_m, nodes_r = (np.array(nodes_l), np.array(nodes_m),\n                                     np.array(nodes_r))\n        Nchords = len(nodes_l)\n        axlen = np.zeros(Nchords, dtype='float')\n        for i, (nl, nm, nr) in enumerate(zip(nodes_l, nodes_m, nodes_r)):\n            axlen[i] = linalg.norm(nr - nl)\n\n        # Slice sample from all chords simultaneously. This is equivalent to\n        # slice sampling in *time* along our trajectory.\n        while True:\n            # Select chord.\n            axprob = axlen / np.sum(axlen)\n            idx = rstate.choice(Nchords, p=axprob)\n            # Define chord.\n            u_l, u_m, u_r = nodes_l[idx], nodes_m[idx], nodes_r[idx]\n            u_hat = u_r - u_l\n            rprop = rstate.rand()\n            u_prop = u_l + rprop * u_hat  # scale from left\n            if unitcheck(u_prop, nonperiodic):\n                v_prop = prior_transform(np.array(u_prop))\n                logl_prop = loglikelihood(np.array(v_prop))\n            else:\n                logl_prop = -np.inf\n            nc += 1\n            ncontract += 1\n            # If we succeed, move to the new position.\n            if logl_prop >= loglstar:\n                u = u_prop\n                break\n            # If we fail, check if the new point is to the left/right of\n            # the point interior to the bounds (`u_m`) and update\n            # the bounds accordingly.\n            else:\n                s = np.dot(u_prop - u_m, u_hat)  # check sign (+/-)\n                if s < 0:  # left\n                    nodes_l[idx] = u_prop\n                    axlen[idx] *= 1 - rprop\n                elif s > 0:  # right\n                    nodes_r[idx] = u_prop\n                    axlen[idx] *= rprop\n                else:\n                    raise RuntimeError(\"Slice sampler has failed to find \"\n                                       \"a valid point. Some useful \"\n                                       \"output quantities:\\n\"\n                                       \"u: {0}\\n\"\n                                       \"u_left: {1}\\n\"\n                                       \"u_right: {2}\\n\"\n                                       \"u_hat: {3}\\n\"\n                                       \"u_prop: {4}\\n\"\n                                       \"loglstar: {5}\\n\"\n                                       \"logl_prop: {6}.\"\n                                       .format(u, u_l, u_r, u_hat, u_prop,\n                                               loglstar, logl_prop))\n\n    blob = {'nmove': nmove, 'nreflect': nreflect, 'ncontract': ncontract}\n\n    return u_prop, v_prop, logl_prop, nc, blob"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Results object containing the nested sampling run s saved results.", "response": "def results(self):\n        \"\"\"Saved results from the nested sampling run. If bounding\n        distributions were saved, those are also returned.\"\"\"\n\n        # Add all saved samples to the results.\n        if self.save_samples:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                results = [('nlive', self.nlive),\n                           ('niter', self.it - 1),\n                           ('ncall', np.array(self.saved_nc)),\n                           ('eff', self.eff),\n                           ('samples', np.array(self.saved_v)),\n                           ('samples_id', np.array(self.saved_id)),\n                           ('samples_it', np.array(self.saved_it)),\n                           ('samples_u', np.array(self.saved_u)),\n                           ('logwt', np.array(self.saved_logwt)),\n                           ('logl', np.array(self.saved_logl)),\n                           ('logvol', np.array(self.saved_logvol)),\n                           ('logz', np.array(self.saved_logz)),\n                           ('logzerr', np.sqrt(np.array(self.saved_logzvar))),\n                           ('information', np.array(self.saved_h))]\n        else:\n            raise ValueError(\"You didn't save any samples!\")\n\n        # Add any saved bounds (and ancillary quantities) to the results.\n        if self.save_bounds:\n            results.append(('bound', copy.deepcopy(self.bound)))\n            results.append(('bound_iter',\n                            np.array(self.saved_bounditer, dtype='int')))\n            results.append(('samples_bound',\n                            np.array(self.saved_boundidx, dtype='int')))\n            results.append(('scale', np.array(self.saved_scale)))\n\n        return Results(results)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether we should update our bound beyond the initial unit cube.", "response": "def _beyond_unit_bound(self, loglstar):\n        \"\"\"Check whether we should update our bound beyond the initial\n        unit cube.\"\"\"\n\n        if self.logl_first_update is None:\n            # If we haven't already updated our bounds, check if we satisfy\n            # the provided criteria for establishing the first bounding update.\n            check = (self.ncall > self.ubound_ncall and\n                     self.eff < self.ubound_eff)\n            if check:\n                # Save the log-likelihood where our first update took place.\n                self.logl_first_update = loglstar\n            return check\n        else:\n            # If we've already update our bounds, check if we've exceeded the\n            # saved log-likelihood threshold. (This is useful when sampling\n            # within `dynamicsampler`).\n            return loglstar >= self.logl_first_update"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps all live point proposals currently on the queue.", "response": "def _empty_queue(self):\n        \"\"\"Dump all live point proposals currently on the queue.\"\"\"\n\n        while True:\n            try:\n                # Remove unused points from the queue.\n                self.queue.pop()\n                self.unused += 1  # add to the total number of unused points\n                self.nqueue -= 1\n            except:\n                # If the queue is empty, we're done!\n                self.nqueue = 0\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngrab the first live point proposal in the queue.", "response": "def _get_point_value(self, loglstar):\n        \"\"\"Grab the first live point proposal in the queue.\"\"\"\n\n        # If the queue is empty, refill it.\n        if self.nqueue <= 0:\n            self._fill_queue(loglstar)\n\n        # Grab the earliest entry.\n        u, v, logl, nc, blob = self.queue.pop(0)\n        self.used += 1  # add to the total number of used points\n        self.nqueue -= 1\n\n        return u, v, logl, nc, blob"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nproposing points until a new point that satisfies the log - likelihood constraint loglstar is found.", "response": "def _new_point(self, loglstar, logvol):\n        \"\"\"Propose points until a new point that satisfies the log-likelihood\n        constraint `loglstar` is found.\"\"\"\n\n        ncall, nupdate = 0, 0\n        while True:\n            # Get the next point from the queue\n            u, v, logl, nc, blob = self._get_point_value(loglstar)\n            ncall += nc\n\n            # Bounding checks.\n            ucheck = ncall >= self.update_interval * (1 + nupdate)\n            bcheck = self._beyond_unit_bound(loglstar)\n\n            # If our queue is empty, update any tuning parameters associated\n            # with our proposal (sampling) method.\n            if blob is not None and self.nqueue <= 0 and bcheck:\n                self.update_proposal(blob)\n\n            # If we satisfy the log-likelihood constraint, we're done!\n            if logl >= loglstar:\n                break\n\n            # If there has been more than `update_interval` function calls\n            # made *and* we satisfy the criteria for moving beyond sampling\n            # from the unit cube, update the bound.\n            if ucheck and bcheck:\n                pointvol = math.exp(logvol) / self.nlive\n                bound = self.update(pointvol)\n                if self.save_bounds:\n                    self.bound.append(bound)\n                self.nbound += 1\n                nupdate += 1\n                self.since_update = -ncall  # ncall will be added back later\n\n        return u, v, logl, ncall"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the remaining set of live points to the current set of dead points. Instantiates a generator that will be called bysample. Returns the same outputs as self. sample.", "response": "def add_live_points(self):\n        \"\"\"Add the remaining set of live points to the current set of dead\n        points. Instantiates a generator that will be called by\n        the user. Returns the same outputs as :meth:`sample`.\"\"\"\n\n        # Check if the remaining live points have already been added\n        # to the output set of samples.\n        if self.added_live:\n            raise ValueError(\"The remaining live points have already \"\n                             \"been added to the list of samples!\")\n        else:\n            self.added_live = True\n\n        # After N samples have been taken out, the remaining volume is\n        # `e^(-N / nlive)`. The remaining points are distributed uniformly\n        # within the remaining volume so that the expected volume enclosed\n        # by the `i`-th worst likelihood is\n        # `e^(-N / nlive) * (nlive + 1 - i) / (nlive + 1)`.\n        logvols = self.saved_logvol[-1]\n        logvols += np.log(1. - (np.arange(self.nlive)+1.) / (self.nlive+1.))\n        logvols_pad = np.concatenate(([self.saved_logvol[-1]], logvols))\n        logdvols = logsumexp(a=np.c_[logvols_pad[:-1], logvols_pad[1:]],\n                             axis=1, b=np.c_[np.ones(self.nlive),\n                                             -np.ones(self.nlive)])\n        logdvols += math.log(0.5)\n\n        # Defining change in `logvol` used in `logzvar` approximation.\n        dlvs = logvols_pad[:-1] - logvols_pad[1:]\n\n        # Sorting remaining live points.\n        lsort_idx = np.argsort(self.live_logl)\n        loglmax = max(self.live_logl)\n\n        # Grabbing relevant values from the last dead point.\n        logz = self.saved_logz[-1]\n        logzvar = self.saved_logzvar[-1]\n        h = self.saved_h[-1]\n        loglstar = self.saved_logl[-1]\n        if self._beyond_unit_bound(loglstar):\n            bounditer = self.nbound - 1\n        else:\n            bounditer = 0\n\n        # Add contributions from the remaining live points in order\n        # from the lowest to the highest log-likelihoods.\n        for i in range(self.nlive):\n\n            # Grab live point with `i`-th lowest log-likelihood along with\n            # ancillary quantities.\n            idx = lsort_idx[i]\n            logvol, logdvol, dlv = logvols[i], logdvols[i], dlvs[i]\n            ustar = np.array(self.live_u[idx])\n            vstar = np.array(self.live_v[idx])\n            loglstar_new = self.live_logl[idx]\n            boundidx = self.live_bound[idx]\n            point_it = self.live_it[idx]\n\n            # Compute relative contribution to results.\n            logwt = np.logaddexp(loglstar_new, loglstar) + logdvol  # weight\n            logz_new = np.logaddexp(logz, logwt)  # ln(evidence)\n            lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                      math.exp(loglstar_new - logz_new) * loglstar_new)\n            h_new = (math.exp(logdvol) * lzterm +\n                     math.exp(logz - logz_new) * (h + logz) -\n                     logz_new)  # information\n            dh = h_new - h\n            h = h_new\n            logz = logz_new\n            logzvar += dh * dlv  # var[ln(evidence)] estimate\n            loglstar = loglstar_new\n            logz_remain = loglmax + logvol  # remaining ln(evidence)\n            delta_logz = np.logaddexp(logz, logz_remain) - logz  # dlogz\n\n            # Save results.\n            if self.save_samples:\n                self.saved_id.append(idx)\n                self.saved_u.append(ustar)\n                self.saved_v.append(vstar)\n                self.saved_logl.append(loglstar)\n                self.saved_logvol.append(logvol)\n                self.saved_logwt.append(logwt)\n                self.saved_logz.append(logz)\n                self.saved_logzvar.append(logzvar)\n                self.saved_h.append(h)\n                self.saved_nc.append(1)\n                self.saved_boundidx.append(boundidx)\n                self.saved_it.append(point_it)\n                self.saved_bounditer.append(bounditer)\n                self.saved_scale.append(self.scale)\n            self.eff = 100. * (self.it + i) / self.ncall  # efficiency\n\n            # Return our new \"dead\" point and ancillary quantities.\n            yield (idx, ustar, vstar, loglstar, logvol, logwt,\n                   logz, logzvar, h, 1, point_it, boundidx, bounditer,\n                   self.eff, delta_logz)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_live_points(self):\n\n        if self.added_live:\n            self.added_live = False\n            if self.save_samples:\n                del self.saved_id[-self.nlive:]\n                del self.saved_u[-self.nlive:]\n                del self.saved_v[-self.nlive:]\n                del self.saved_logl[-self.nlive:]\n                del self.saved_logvol[-self.nlive:]\n                del self.saved_logwt[-self.nlive:]\n                del self.saved_logz[-self.nlive:]\n                del self.saved_logzvar[-self.nlive:]\n                del self.saved_h[-self.nlive:]\n                del self.saved_nc[-self.nlive:]\n                del self.saved_boundidx[-self.nlive:]\n                del self.saved_it[-self.nlive:]\n                del self.saved_bounditer[-self.nlive:]\n                del self.saved_scale[-self.nlive:]\n        else:\n            raise ValueError(\"No live points were added to the \"\n                             \"list of samples!\")", "response": "Removes the final set of live points if they were previously added to the current set of dead points."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(self, maxiter=None, maxcall=None, dlogz=0.01,\n               logl_max=np.inf, save_bounds=True, save_samples=True):\n        \"\"\"\n        **The main nested sampling loop.** Iteratively replace the worst live\n        point with a sample drawn uniformly from the prior until the\n        provided stopping criteria are reached. Instantiates a generator\n        that will be called by the user.\n\n        Parameters\n        ----------\n        maxiter : int, optional\n            Maximum number of iterations. Iteration may stop earlier if the\n            termination condition is reached. Default is `sys.maxsize`\n            (no limit).\n\n        maxcall : int, optional\n            Maximum number of likelihood evaluations. Iteration may stop\n            earlier if termination condition is reached. Default is\n            `sys.maxsize` (no limit).\n\n        dlogz : float, optional\n            Iteration will stop when the estimated contribution of the\n            remaining prior volume to the total evidence falls below\n            this threshold. Explicitly, the stopping criterion is\n            `ln(z + z_est) - ln(z) < dlogz`, where `z` is the current\n            evidence from all saved samples and `z_est` is the estimated\n            contribution from the remaining volume. Default is `0.01`.\n\n        logl_max : float, optional\n            Iteration will stop when the sampled ln(likelihood) exceeds the\n            threshold set by `logl_max`. Default is no bound (`np.inf`).\n\n        save_bounds : bool, optional\n            Whether or not to save past distributions used to bound\n            the live points internally. Default is `True`.\n\n        save_samples : bool, optional\n            Whether or not to save past samples from the nested sampling run\n            (along with other ancillary quantities) internally.\n            Default is `True`.\n\n        Returns\n        -------\n        worst : int\n            Index of the live point with the worst likelihood. This is our\n            new dead point sample.\n\n        ustar : `~numpy.ndarray` with shape (npdim,)\n            Position of the sample.\n\n        vstar : `~numpy.ndarray` with shape (ndim,)\n            Transformed position of the sample.\n\n        loglstar : float\n            Ln(likelihood) of the sample.\n\n        logvol : float\n            Ln(prior volume) within the sample.\n\n        logwt : float\n            Ln(weight) of the sample.\n\n        logz : float\n            Cumulative ln(evidence) up to the sample (inclusive).\n\n        logzvar : float\n            Estimated cumulative variance on `logz` (inclusive).\n\n        h : float\n            Cumulative information up to the sample (inclusive).\n\n        nc : int\n            Number of likelihood calls performed before the new\n            live point was accepted.\n\n        worst_it : int\n            Iteration when the live (now dead) point was originally proposed.\n\n        boundidx : int\n            Index of the bound the dead point was originally drawn from.\n\n        bounditer : int\n            Index of the bound being used at the current iteration.\n\n        eff : float\n            The cumulative sampling efficiency (in percent).\n\n        delta_logz : float\n            The estimated remaining evidence expressed as the ln(ratio) of the\n            current evidence.\n\n        \"\"\"\n\n        # Initialize quantities.\n        if maxcall is None:\n            maxcall = sys.maxsize\n        if maxiter is None:\n            maxiter = sys.maxsize\n        self.save_samples = save_samples\n        self.save_bounds = save_bounds\n        ncall = 0\n\n        # Check whether we're starting fresh or continuing a previous run.\n        if self.it == 1:\n            # Initialize values for nested sampling loop.\n            h = 0.  # information, initially *0.*\n            logz = -1.e300  # ln(evidence), initially *0.*\n            logzvar = 0.  # var[ln(evidence)], initially *0.*\n            logvol = 0.  # initially contains the whole prior (volume=1.)\n            loglstar = -1.e300  # initial ln(likelihood)\n            delta_logz = 1.e300  # ln(ratio) of total/current evidence\n\n            # Check if we should initialize a different bounding distribution\n            # instead of using the unit cube.\n            pointvol = 1. / self.nlive\n            if self._beyond_unit_bound(loglstar):\n                bound = self.update(pointvol)\n                if self.save_bounds:\n                    self.bound.append(bound)\n                    self.nbound += 1\n                self.since_update = 0\n        else:\n            # Remove live points (if added) from previous run.\n            if self.added_live:\n                self._remove_live_points()\n\n            # Get final state from previous run.\n            h = self.saved_h[-1]  # information\n            logz = self.saved_logz[-1]  # ln(evidence)\n            logzvar = self.saved_logzvar[-1]  # var[ln(evidence)]\n            logvol = self.saved_logvol[-1]  # ln(volume)\n            loglstar = min(self.live_logl)  # ln(likelihood)\n            delta_logz = np.logaddexp(logz, np.max(self.live_logl) +\n                                      logvol) - logz  # log-evidence ratio\n\n        # The main nested sampling loop.\n        for it in range(sys.maxsize):\n\n            # Stopping criterion 1: current number of iterations\n            # exceeds `maxiter`.\n            if it > maxiter:\n                # If dumping past states, save only the required quantities.\n                if not self.save_samples:\n                    self.saved_logz.append(logz)\n                    self.saved_logzvar.append(logzvar)\n                    self.saved_h.append(h)\n                    self.saved_logvol.append(logvol)\n                    self.saved_logl.append(loglstar)\n                break\n\n            # Stopping criterion 2: current number of `loglikelihood`\n            # calls exceeds `maxcall`.\n            if ncall > maxcall:\n                if not self.save_samples:\n                    self.saved_logz.append(logz)\n                    self.saved_logzvar.append(logzvar)\n                    self.saved_h.append(h)\n                    self.saved_logvol.append(logvol)\n                    self.saved_logl.append(loglstar)\n                break\n\n            # Stopping criterion 3: estimated (fractional) remaining evidence\n            # lies below some threshold set by `dlogz`.\n            logz_remain = np.max(self.live_logl) + logvol\n            delta_logz = np.logaddexp(logz, logz_remain) - logz\n            if dlogz is not None:\n                if delta_logz < dlogz:\n                    if not self.save_samples:\n                        self.saved_logz.append(logz)\n                        self.saved_logzvar.append(logzvar)\n                        self.saved_h.append(h)\n                        self.saved_logvol.append(logvol)\n                        self.saved_logl.append(loglstar)\n                    break\n\n            # Stopping criterion 4: last dead point exceeded the upper\n            # `logl_max` bound.\n            if loglstar > logl_max:\n                if not self.save_samples:\n                    self.saved_logz.append(logz)\n                    self.saved_logzvar.append(logzvar)\n                    self.saved_h.append(h)\n                    self.saved_logvol.append(logvol)\n                    self.saved_logl.append(loglstar)\n                break\n\n            # Expected ln(volume) shrinkage.\n            logvol -= self.dlv\n\n            # After `update_interval` interations have passed *and* we meet\n            # the criteria for moving beyond sampling from the unit cube,\n            # update the bound using the current set of live points.\n            ucheck = self.since_update >= self.update_interval\n            bcheck = self._beyond_unit_bound(loglstar)\n            if ucheck and bcheck:\n                pointvol = math.exp(logvol) / self.nlive\n                bound = self.update(pointvol)\n                if self.save_bounds:\n                    self.bound.append(bound)\n                self.nbound += 1\n                self.since_update = 0\n\n            # Locate the \"live\" point with the lowest `logl`.\n            worst = np.argmin(self.live_logl)  # index\n            worst_it = self.live_it[worst]  # when point was proposed\n            boundidx = self.live_bound[worst]  # associated bound index\n\n            # Set our new worst likelihood constraint.\n            ustar = np.array(self.live_u[worst])  # unit cube position\n            vstar = np.array(self.live_v[worst])  # transformed position\n            loglstar_new = self.live_logl[worst]  # new likelihood\n\n            # Set our new weight using quadratic estimates (trapezoid rule).\n            logdvol = logsumexp(a=[logvol + self.dlv, logvol],\n                                b=[0.5, -0.5])  # ln(dvol)\n            logwt = np.logaddexp(loglstar_new, loglstar) + logdvol  # ln(wt)\n\n            # Sample a new live point from within the likelihood constraint\n            # `logl > loglstar` using the bounding distribution and sampling\n            # method from our sampler.\n            u, v, logl, nc = self._new_point(loglstar_new, logvol)\n            ncall += nc\n            self.ncall += nc\n            self.since_update += nc\n\n            # Update evidence `logz` and information `h`.\n            logz_new = np.logaddexp(logz, logwt)\n            lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                      math.exp(loglstar_new - logz_new) * loglstar_new)\n            h_new = (math.exp(logdvol) * lzterm +\n                     math.exp(logz - logz_new) * (h + logz) -\n                     logz_new)\n            dh = h_new - h\n            h = h_new\n            logz = logz_new\n            logzvar += dh * self.dlv\n            loglstar = loglstar_new\n\n            # Compute bound index at the current iteration.\n            if self._beyond_unit_bound(loglstar):\n                bounditer = self.nbound - 1\n            else:\n                bounditer = 0\n\n            # Save the worst live point. It is now a \"dead\" point.\n            if self.save_samples:\n                self.saved_id.append(worst)\n                self.saved_u.append(ustar)\n                self.saved_v.append(vstar)\n                self.saved_logl.append(loglstar)\n                self.saved_logvol.append(logvol)\n                self.saved_logwt.append(logwt)\n                self.saved_logz.append(logz)\n                self.saved_logzvar.append(logzvar)\n                self.saved_h.append(h)\n                self.saved_nc.append(nc)\n                self.saved_boundidx.append(boundidx)\n                self.saved_it.append(worst_it)\n                self.saved_bounditer.append(bounditer)\n                self.saved_scale.append(self.scale)\n\n            # Update the live point (previously our \"worst\" point).\n            self.live_u[worst] = u\n            self.live_v[worst] = v\n            self.live_logl[worst] = logl\n            self.live_bound[worst] = bounditer\n            self.live_it[worst] = self.it\n\n            # Compute our sampling efficiency.\n            self.eff = 100. * self.it / self.ncall\n\n            # Increment total number of iterations.\n            self.it += 1\n\n            # Return dead point and ancillary quantities.\n            yield (worst, ustar, vstar, loglstar, logvol, logwt,\n                   logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n                   self.eff, delta_logz)", "response": "Main nested sampling loop."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_final_live(self, print_progress=True, print_func=None):\n\n        # Initialize quantities/\n        if print_func is None:\n            print_func = print_fn\n\n        # Add remaining live points to samples.\n        ncall = self.ncall\n        it = self.it - 1\n        for i, results in enumerate(self.add_live_points()):\n            (worst, ustar, vstar, loglstar, logvol, logwt,\n             logz, logzvar, h, nc, worst_it, boundidx, bounditer,\n             eff, delta_logz) = results\n            if delta_logz > 1e6:\n                delta_logz = np.inf\n            if logz <= -1e6:\n                logz = -np.inf\n\n            # Print progress.\n            if print_progress:\n                print_func(results, it, ncall, add_live_it=i+1, dlogz=0.01)", "response": "Adds the final set of live points to the current nested sampling run."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, **kwargs):\n        for k in self.prior_params:\n            try:\n                self.params[k] = kwargs[self.alias[k]]\n            except(KeyError):\n                pass", "response": "Update the params dictionary using the passed in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(self, nsample=None, **kwargs):\n        if len(kwargs) > 0:\n            self.update(**kwargs)\n        return self.distribution.rvs(*self.args, size=len(self),\n                                     loc=self.loc, scale=self.scale)", "response": "Draw a sample from the prior distribution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngo from a value of the CDF to the corresponding parameter value.", "response": "def unit_transform(self, x, **kwargs):\n        \"\"\"Go from a value of the CDF (between 0 and 1) to the corresponding\n        parameter value.\n\n        :param x:\n            A scalar or vector of same length as the Prior with values between\n            zero and one corresponding to the value of the CDF.\n\n        :returns theta:\n            The parameter value corresponding to the value of the CDF given by\n            `x`.\n        \"\"\"\n        if len(kwargs) > 0:\n            self.update(**kwargs)\n        return self.distribution.ppf(x, *self.args,\n                                     loc=self.loc, scale=self.scale)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inverse_unit_transform(self, x, **kwargs):\n        if len(kwargs) > 0:\n            self.update(**kwargs)\n        return self.distribution.cdf(x, *self.args,\n                                     loc=self.loc, scale=self.scale)", "response": "Go from the parameter value to the unit coordinate using the cdf."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef propose_unif(self):\n\n        u = self.unitcube.sample(rstate=self.rstate)\n        ax = np.identity(self.npdim)\n\n        return u, ax", "response": "Propose a new live point by sampling uniformly *\n        within the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_rwalk(self, blob):\n\n        self.scale = blob['scale']\n        accept, reject = blob['accept'], blob['reject']\n        facc = (1. * accept) / (accept + reject)\n        norm = max(self.facc, 1. - self.facc) * self.npdim\n        self.scale *= math.exp((facc - self.facc) / norm)\n        self.scale = min(self.scale, math.sqrt(self.npdim))", "response": "Update the random walk proposal scale based on the current number of accepted and rejected steps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_slice(self, blob):\n\n        nexpand, ncontract = blob['nexpand'], blob['ncontract']\n        self.scale *= nexpand / (2. * ncontract)", "response": "Update the slice proposal scale based on the relative\n        size of the slices compared to our initial guess."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the Hamiltonian slice proposal scale based on the relative amount of time spent moving vs reflecting.", "response": "def update_hslice(self, blob):\n        \"\"\"Update the Hamiltonian slice proposal scale based\n        on the relative amount of time spent moving vs reflecting.\"\"\"\n\n        nmove, nreflect = blob['nmove'], blob['nreflect']\n        ncontract = blob.get('ncontract', 0)\n        fmove = (1. * nmove) / (nmove + nreflect + ncontract + 2)\n        norm = max(self.fmove, 1. - self.fmove)\n        self.scale *= math.exp((fmove - self.fmove) / norm)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, pointvol):\n\n        # Check if we should use the provided pool for updating.\n        if self.use_pool_update:\n            pool = self.pool\n        else:\n            pool = None\n\n        # Update the ellipsoid.\n        self.ell.update(self.live_u, pointvol=pointvol, rstate=self.rstate,\n                        bootstrap=self.bootstrap, pool=pool)\n        if self.enlarge != 1.:\n            self.ell.scale_to_vol(self.ell.vol * self.enlarge)\n\n        return copy.deepcopy(self.ell)", "response": "Update the bounding ellipsoid using the current set of live points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproposing a new live point by sampling uniformly *ly*unitchecks the ellipsoid.", "response": "def propose_unif(self):\n        \"\"\"Propose a new live point by sampling *uniformly*\n        within the ellipsoid.\"\"\"\n\n        while True:\n            # Sample a point from the ellipsoid.\n            u = self.ell.sample(rstate=self.rstate)\n\n            # Check if `u` is within the unit cube.\n            if unitcheck(u, self.nonperiodic):\n                break  # if it is, we're done!\n\n        return u, self.ell.axes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a live point and axes to be used by other sampling methods.", "response": "def propose_live(self):\n        \"\"\"Return a live point/axes to be used by other sampling methods.\"\"\"\n\n        i = self.rstate.randint(self.nlive)\n        u = self.live_u[i, :]\n\n        # Choose axes.\n        if self.sampling in ['rwalk', 'rstagger', 'rslice']:\n            ax = self.ell.axes\n        elif self.sampling == 'slice':\n            ax = self.ell.paxes\n        else:\n            ax = np.identity(self.npdim)\n\n        return u, ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, pointvol):\n\n        # Check if we should use the pool for updating.\n        if self.use_pool_update:\n            pool = self.pool\n        else:\n            pool = None\n\n        # Update the bounding ellipsoids.\n        self.mell.update(self.live_u, pointvol=pointvol,\n                         vol_dec=self.vol_dec, vol_check=self.vol_check,\n                         rstate=self.rstate, bootstrap=self.bootstrap,\n                         pool=pool)\n        if self.enlarge != 1.:\n            self.mell.scale_to_vols(self.mell.vols * self.enlarge)\n\n        return copy.deepcopy(self.mell)", "response": "Update the bounding ellipsoids using the current set of live points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef propose_unif(self):\n\n        while True:\n            # Sample a point from the union of ellipsoids.\n            # Returns the point `u`, ellipsoid index `idx`, and number of\n            # overlapping ellipsoids `q` at position `u`.\n            u, idx, q = self.mell.sample(rstate=self.rstate, return_q=True)\n\n            # Check if the point is within the unit cube.\n            if unitcheck(u, self.nonperiodic):\n                # Accept the point with probability 1/q to account for\n                # overlapping ellipsoids.\n                if q == 1 or self.rstate.rand() < 1.0 / q:\n                    break  # if successful, we're done!\n\n        return u, self.mell.ells[idx].axes", "response": "Propose a new live point by sampling uniformly within the union of ellipsoids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a live point and axes to be used by other sampling methods.", "response": "def propose_live(self):\n        \"\"\"Return a live point/axes to be used by other sampling methods.\"\"\"\n\n        # Copy a random live point.\n        i = self.rstate.randint(self.nlive)\n        u = self.live_u[i, :]\n\n        # Check for ellipsoid overlap.\n        ell_idxs = self.mell.within(u)\n        nidx = len(ell_idxs)\n\n        # Automatically trigger an update if we're not in any ellipsoid.\n        if nidx == 0:\n            try:\n                # Expected ln(prior volume) at a given iteration.\n                expected_vol = math.exp(self.saved_logvol[-1] - self.dlv)\n            except:\n                # Expected ln(prior volume) at the first iteration.\n                expected_vol = math.exp(-self.dlv)\n            pointvol = expected_vol / self.nlive  # minimum point volume\n\n            # Update the bounding ellipsoids.\n            bound = self.update(pointvol)\n            if self.save_bounds:\n                self.bound.append(bound)\n            self.nbound += 1\n            self.since_update = 0\n\n            # Check for ellipsoid overlap (again).\n            ell_idxs = self.mell.within(u)\n            nidx = len(ell_idxs)\n\n        # Pick a random ellipsoid that encompasses `u`.\n        ell_idx = ell_idxs[self.rstate.randint(nidx)]\n\n        # Choose axes.\n        if self.sampling in ['rwalk', 'rstagger', 'rslice']:\n            ax = self.mell.ells[ell_idx].axes\n        elif self.sampling == 'slice':\n            ax = self.mell.ells[ell_idx].paxes\n        else:\n            ax = np.identity(self.npdim)\n\n        return u, ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, pointvol):\n\n        # Initialize a K-D Tree to assist nearest neighbor searches.\n        if self.use_kdtree:\n            kdtree = spatial.KDTree(self.live_u)\n        else:\n            kdtree = None\n\n        # Check if we should use the provided pool for updating.\n        if self.use_pool_update:\n            pool = self.pool\n        else:\n            pool = None\n\n        # Update the N-spheres.\n        self.radfriends.update(self.live_u, pointvol=pointvol,\n                               rstate=self.rstate, bootstrap=self.bootstrap,\n                               pool=pool, kdtree=kdtree)\n        if self.enlarge != 1.:\n            self.radfriends.scale_to_vol(self.radfriends.vol_ball *\n                                         self.enlarge)\n\n        return copy.deepcopy(self.radfriends)", "response": "Update the N - sphere radii using the current set of live points."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef propose_unif(self):\n\n        # Initialize a K-D Tree to assist nearest neighbor searches.\n        if self.use_kdtree:\n            kdtree = spatial.KDTree(self.live_u)\n        else:\n            kdtree = None\n\n        while True:\n            # Sample a point `u` from the union of N-spheres along with the\n            # number of overlapping spheres `q` at point `u`.\n            u, q = self.radfriends.sample(self.live_u, rstate=self.rstate,\n                                          return_q=True, kdtree=kdtree)\n\n            # Check if our sample is within the unit cube.\n            if unitcheck(u, self.nonperiodic):\n                # Accept the point with probability 1/q to account for\n                # overlapping balls.\n                if q == 1 or self.rstate.rand() < 1.0 / q:\n                    break  # if successful, we're done!\n\n        # Define the axes of the N-sphere.\n        ax = np.identity(self.npdim) * self.radfriends.radius\n\n        return u, ax", "response": "Propose a new live point by sampling uniformly within\n            the union of N - spheres defined by our live points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef propose_live(self):\n\n        i = self.rstate.randint(self.nlive)\n        u = self.live_u[i, :]\n        ax = np.identity(self.npdim) * self.radfriends.radius\n\n        return u, ax", "response": "Propose a live point and axes to be used by other sampling methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summary(self):\n\n        res = (\"nlive: {:d}\\n\"\n               \"niter: {:d}\\n\"\n               \"ncall: {:d}\\n\"\n               \"eff(%): {:6.3f}\\n\"\n               \"logz: {:6.3f} +/- {:6.3f}\"\n               .format(self.nlive, self.niter, sum(self.ncall),\n                       self.eff, self.logz[-1], self.logzerr[-1]))\n\n        print('Summary\\n=======\\n'+res)", "response": "Return a formatted string giving a quick summary\n        of the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks whether u is inside the unit cube. Given a masked array nonperiodic also allows periodic boundaries conditions to exceed the unit cube.", "response": "def unitcheck(u, nonperiodic=None):\n    \"\"\"Check whether `u` is inside the unit cube. Given a masked array\n    `nonperiodic`, also allows periodic boundaries conditions to exceed\n    the unit cube.\"\"\"\n\n    if nonperiodic is None:\n        # No periodic boundary conditions provided.\n        return np.all(u > 0.) and np.all(u < 1.)\n    else:\n        # Alternating periodic and non-periodic boundary conditions.\n        return (np.all(u[nonperiodic] > 0.) and\n                np.all(u[nonperiodic] < 1.) and\n                np.all(u[~nonperiodic] > -0.5) and\n                np.all(u[~nonperiodic] < 1.5))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the weighted mean and covariance of the samples.", "response": "def mean_and_cov(samples, weights):\n    \"\"\"\n    Compute the weighted mean and covariance of the samples.\n\n    Parameters\n    ----------\n    samples : `~numpy.ndarray` with shape (nsamples, ndim)\n        2-D array containing data samples. This ordering is equivalent to\n        using `rowvar=False` in `~numpy.cov`.\n\n    weights : `~numpy.ndarray` with shape (nsamples,)\n        1-D array of sample weights.\n\n    Returns\n    -------\n    mean : `~numpy.ndarray` with shape (ndim,)\n        Weighted sample mean vector.\n\n    cov : `~numpy.ndarray` with shape (ndim, ndim)\n        Weighted sample covariance matrix.\n\n    Notes\n    -----\n    Implements the formulae found `here <https://goo.gl/emWFLR>`_.\n\n    \"\"\"\n\n    # Compute the weighted mean.\n    mean = np.average(samples, weights=weights, axis=0)\n\n    # Compute the weighted covariance.\n    dx = samples - mean\n    wsum = np.sum(weights)\n    w2sum = np.sum(weights**2)\n    cov = wsum / (wsum**2 - w2sum) * np.einsum('i,ij,ik', weights, dx, dx)\n\n    return mean, cov"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resample_equal(samples, weights, rstate=None):\n\n    if rstate is None:\n        rstate = np.random\n\n    if abs(np.sum(weights) - 1.) > SQRTEPS:  # same tol as in np.random.choice.\n        raise ValueError(\"Weights do not sum to 1.\")\n\n    # Make N subdivisions and choose positions with a consistent random offset.\n    nsamples = len(weights)\n    positions = (rstate.random() + np.arange(nsamples)) / nsamples\n\n    # Resample the data.\n    idx = np.zeros(nsamples, dtype=np.int)\n    cumulative_sum = np.cumsum(weights)\n    i, j = 0, 0\n    while i < nsamples:\n        if positions[i] < cumulative_sum[j]:\n            idx[i] = j\n            i += 1\n        else:\n            j += 1\n\n    return samples[idx]", "response": "Resample a set of points from the weighted set of inputs\n    such that they all have equal weight."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing weighted sample quantiles from an input set of samples.", "response": "def quantile(x, q, weights=None):\n    \"\"\"\n    Compute (weighted) quantiles from an input set of samples.\n\n    Parameters\n    ----------\n    x : `~numpy.ndarray` with shape (nsamps,)\n        Input samples.\n\n    q : `~numpy.ndarray` with shape (nquantiles,)\n       The list of quantiles to compute from `[0., 1.]`.\n\n    weights : `~numpy.ndarray` with shape (nsamps,), optional\n        The associated weight from each sample.\n\n    Returns\n    -------\n    quantiles : `~numpy.ndarray` with shape (nquantiles,)\n        The weighted sample quantiles computed at `q`.\n\n    \"\"\"\n\n    # Initial check.\n    x = np.atleast_1d(x)\n    q = np.atleast_1d(q)\n\n    # Quantile check.\n    if np.any(q < 0.0) or np.any(q > 1.0):\n        raise ValueError(\"Quantiles must be between 0. and 1.\")\n\n    if weights is None:\n        # If no weights provided, this simply calls `np.percentile`.\n        return np.percentile(x, list(100.0 * q))\n    else:\n        # If weights are provided, compute the weighted quantiles.\n        weights = np.atleast_1d(weights)\n        if len(x) != len(weights):\n            raise ValueError(\"Dimension mismatch: len(weights) != len(x).\")\n        idx = np.argsort(x)  # sort samples\n        sw = weights[idx]  # sort weights\n        cdf = np.cumsum(sw)[:-1]  # compute CDF\n        cdf /= cdf[-1]  # normalize CDF\n        cdf = np.append(0, cdf)  # ensure proper span\n        quantiles = np.interp(q, cdf, x[idx]).tolist()\n        return quantiles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new nested sampling run with jittered uncertainties.", "response": "def jitter_run(res, rstate=None, approx=False):\n    \"\"\"\n    Probes **statistical uncertainties** on a nested sampling run by\n    explicitly generating a *realization* of the prior volume associated\n    with each sample (dead point). Companion function to :meth:`resample_run`\n    and :meth:`simulate_run`.\n\n    Parameters\n    ----------\n    res : :class:`~dynesty.results.Results` instance\n        The :class:`~dynesty.results.Results` instance taken from a previous\n        nested sampling run.\n\n    rstate : `~numpy.random.RandomState`, optional\n        `~numpy.random.RandomState` instance.\n\n    approx : bool, optional\n        Whether to approximate all sets of uniform order statistics by their\n        associated marginals (from the Beta distribution). Default is `False`.\n\n    Returns\n    -------\n    new_res : :class:`~dynesty.results.Results` instance\n        A new :class:`~dynesty.results.Results` instance with corresponding\n        weights based on our \"jittered\" prior volume realizations.\n\n    \"\"\"\n\n    if rstate is None:\n        rstate = np.random\n\n    # Initialize evolution of live points over the course of the run.\n    nsamps, samples_n = _get_nsamps_samples_n(res)\n    logl = res.logl\n\n    # Simulate the prior volume shrinkage associated with our set of \"dead\"\n    # points. At each iteration, if the number of live points is constant or\n    # increasing, our prior volume compresses by the maximum value of a set\n    # of `K_i` uniformly distributed random numbers (i.e. as `Beta(K_i, 1)`).\n    # If instead the number of live points is decreasing, that means we're\n    # instead  sampling down a set of uniform random variables\n    # (i.e. uniform order statistics).\n    nlive_flag = np.ones(nsamps, dtype='bool')\n    nlive_start, bounds = [], []\n\n    if not approx:\n        # Find all instances where the number of live points is either constant\n        # or increasing.\n        nlive_flag[1:] = np.diff(samples_n) >= 0\n\n        # For all the portions that are decreasing, find out where they start,\n        # where they end, and how many live points are present at that given\n        # iteration.\n\n        if np.any(~nlive_flag):\n            i = 0\n            while i < nsamps:\n                if not nlive_flag[i]:\n                    bound = []\n                    bound.append(i-1)\n                    nlive_start.append(samples_n[i-1])\n                    while i < nsamps and not nlive_flag[i]:\n                        i += 1\n                    bound.append(i)\n                    bounds.append(bound)\n                i += 1\n\n    # The maximum out of a set of `K_i` uniformly distributed random variables\n    # has a marginal distribution of `Beta(K_i, 1)`.\n    t_arr = np.zeros(nsamps)\n    t_arr[nlive_flag] = rstate.beta(a=samples_n[nlive_flag], b=1)\n\n    # If we instead are sampling the set of uniform order statistics,\n    # we note that the jth largest value is marginally distributed as\n    # `Beta(j, K_i-j+1)`. The full joint distribution is::\n    #\n    #     X_(j) / X_N = (Y_1 + ... + Y_j) / (Y_1 + ... + Y_{K+1})\n    #\n    # where X_(j) is the prior volume of the live point with the `j`-th\n    # *best* likelihood (i.e. prior volume shrinks as likelihood increases)\n    # and the `Y_i`'s are i.i.d. exponentially distributed random variables.\n    nunif = len(nlive_start)\n    for i in range(nunif):\n        nstart = nlive_start[i]\n        bound = bounds[i]\n        sn = samples_n[bound[0]:bound[1]]\n        y_arr = rstate.exponential(scale=1.0, size=nstart+1)\n        ycsum = y_arr.cumsum()\n        ycsum /= ycsum[-1]\n        uorder = ycsum[np.append(nstart, sn-1)]\n        rorder = uorder[1:] / uorder[:-1]\n        t_arr[bound[0]:bound[1]] = rorder\n\n    # These are the \"compression factors\" at each iteration. Let's turn\n    # these into associated ln(volumes).\n    logvol = np.log(t_arr).cumsum()\n\n    # Compute weights using quadratic estimator.\n    h = 0.\n    logz = -1.e300\n    loglstar = -1.e300\n    logzvar = 0.\n    logvols_pad = np.concatenate(([0.], logvol))\n    logdvols = misc.logsumexp(a=np.c_[logvols_pad[:-1], logvols_pad[1:]],\n                              axis=1, b=np.c_[np.ones(nsamps),\n                                              -np.ones(nsamps)])\n    logdvols += math.log(0.5)\n    dlvs = -np.diff(np.append(0., res.logvol))\n    saved_logwt, saved_logz, saved_logzvar, saved_h = [], [], [], []\n    for i in range(nsamps):\n        loglstar_new = logl[i]\n        logdvol, dlv = logdvols[i], dlvs[i]\n        logwt = np.logaddexp(loglstar_new, loglstar) + logdvol\n        logz_new = np.logaddexp(logz, logwt)\n        lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                  math.exp(loglstar_new - logz_new) * loglstar_new)\n        h_new = (math.exp(logdvol) * lzterm +\n                 math.exp(logz - logz_new) * (h + logz) -\n                 logz_new)\n        dh = h_new - h\n        h = h_new\n        logz = logz_new\n        logzvar += dh * dlv\n        loglstar = loglstar_new\n        saved_logwt.append(logwt)\n        saved_logz.append(logz)\n        saved_logzvar.append(logzvar)\n        saved_h.append(h)\n\n    # Copy results.\n    new_res = Results([item for item in res.items()])\n\n    # Overwrite items with our new estimates.\n    new_res.logvol = np.array(logvol)\n    new_res.logwt = np.array(saved_logwt)\n    new_res.logz = np.array(saved_logz)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        new_res.logzerr = np.sqrt(np.array(saved_logzvar))\n    new_res.h = np.array(saved_h)\n\n    return new_res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resample_run(res, rstate=None, return_idx=False):\n\n    if rstate is None:\n        rstate = np.random\n\n    # Check whether the final set of live points were added to the\n    # run.\n    nsamps = len(res.ncall)\n    try:\n        # Check if the number of live points explicitly changes.\n        samples_n = res.samples_n\n        samples_batch = res.samples_batch\n        batch_bounds = res.batch_bounds\n        added_final_live = True\n    except:\n        # If the number of live points is constant, compute `samples_n` and\n        # set up the `added_final_live` flag.\n        nlive = res.nlive\n        niter = res.niter\n        if nsamps == niter:\n            samples_n = np.ones(niter, dtype='int') * nlive\n            added_final_live = False\n        elif nsamps == (niter + nlive):\n            samples_n = np.append(np.ones(niter, dtype='int') * nlive,\n                                  np.arange(1, nlive + 1)[::-1])\n            added_final_live = True\n        else:\n            raise ValueError(\"Final number of samples differs from number of \"\n                             \"iterations and number of live points.\")\n        samples_batch = np.zeros(len(samples_n), dtype='int')\n        batch_bounds = np.array([(-np.inf, np.inf)])\n    batch_llmin = batch_bounds[:, 0]\n\n    # Identify unique particles that make up each strand.\n    ids = np.unique(res.samples_id)\n\n    # Split the set of strands into two groups: a \"baseline\" group that\n    # contains points initially sampled from the prior, which gives information\n    # on the evidence, and an \"add-on\" group, which gives additional\n    # information conditioned on our baseline strands.\n    base_ids = []\n    addon_ids = []\n    for i in ids:\n        sbatch = samples_batch[res.samples_id == i]\n        if np.any(batch_llmin[sbatch] == -np.inf):\n            base_ids.append(i)\n        else:\n            addon_ids.append(i)\n    nbase, nadd = len(base_ids), len(addon_ids)\n    base_ids, addon_ids = np.array(base_ids), np.array(addon_ids)\n\n    # Resample strands.\n    if nbase > 0 and nadd > 0:\n        live_idx = np.append(base_ids[rstate.randint(0, nbase, size=nbase)],\n                             addon_ids[rstate.randint(0, nadd, size=nadd)])\n    elif nbase > 0:\n        live_idx = base_ids[rstate.randint(0, nbase, size=nbase)]\n    elif nadd > 0:\n        raise ValueError(\"The provided `Results` does not include any points \"\n                         \"initially sampled from the prior!\")\n    else:\n        raise ValueError(\"The provided `Results` does not appear to have \"\n                         \"any particles!\")\n\n    # Find corresponding indices within the original run.\n    samp_idx = np.arange(len(res.ncall))\n    samp_idx = np.concatenate([samp_idx[res.samples_id == idx]\n                               for idx in live_idx])\n\n    # Derive new sample size.\n    nsamps = len(samp_idx)\n\n    # Sort the loglikelihoods (there will be duplicates).\n    logls = res.logl[samp_idx]\n    idx_sort = np.argsort(logls)\n    samp_idx = samp_idx[idx_sort]\n    logl = res.logl[samp_idx]\n\n    if added_final_live:\n        # Compute the effective number of live points for each sample.\n        samp_n = np.zeros(nsamps, dtype='int')\n        uidxs, uidxs_n = np.unique(live_idx, return_counts=True)\n        for uidx, uidx_n in zip(uidxs, uidxs_n):\n            sel = (res.samples_id == uidx)  # selection flag\n            sbatch = samples_batch[sel][0]  # corresponding batch ID\n            lower = batch_llmin[sbatch]  # lower bound\n            upper = max(res.logl[sel])  # upper bound\n\n            # Add number of live points between endpoints equal to number of\n            # times the strand has been resampled.\n            samp_n[(logl > lower) & (logl < upper)] += uidx_n\n\n            # At the endpoint, divide up the final set of points into `uidx_n`\n            # (roughly) equal chunks and have live points decrease across them.\n            endsel = (logl == upper)\n            endsel_n = np.count_nonzero(endsel)\n            chunk = endsel_n / uidx_n  # define our chunk\n            counters = np.array(np.arange(endsel_n) / chunk, dtype='int')\n            nlive_end = counters[::-1] + 1  # decreasing number of live points\n            samp_n[endsel] += nlive_end  # add live point sequence\n    else:\n        # If we didn't add the final set of live points, the run has a constant\n        # number of live points and can simply be re-ordered.\n        samp_n = samples_n[samp_idx]\n\n    # Assign log(volume) to samples.\n    logvol = np.cumsum(np.log(samp_n / (samp_n + 1.)))\n\n    # Computing weights using quadratic estimator.\n    h = 0.\n    logz = -1.e300\n    loglstar = -1.e300\n    logzvar = 0.\n    logvols_pad = np.concatenate(([0.], logvol))\n    logdvols = misc.logsumexp(a=np.c_[logvols_pad[:-1], logvols_pad[1:]],\n                              axis=1, b=np.c_[np.ones(nsamps),\n                                              -np.ones(nsamps)])\n    logdvols += math.log(0.5)\n    dlvs = logvols_pad[:-1] - logvols_pad[1:]\n    saved_logwt, saved_logz, saved_logzvar, saved_h = [], [], [], []\n    for i in range(nsamps):\n        loglstar_new = logl[i]\n        logdvol, dlv = logdvols[i], dlvs[i]\n        logwt = np.logaddexp(loglstar_new, loglstar) + logdvol\n        logz_new = np.logaddexp(logz, logwt)\n        lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                  math.exp(loglstar_new - logz_new) * loglstar_new)\n        h_new = (math.exp(logdvol) * lzterm +\n                 math.exp(logz - logz_new) * (h + logz) -\n                 logz_new)\n        dh = h_new - h\n        h = h_new\n        logz = logz_new\n        logzvar += dh * dlv\n        loglstar = loglstar_new\n        saved_logwt.append(logwt)\n        saved_logz.append(logz)\n        saved_logzvar.append(logzvar)\n        saved_h.append(h)\n\n    # Compute sampling efficiency.\n    eff = 100. * len(res.ncall[samp_idx]) / sum(res.ncall[samp_idx])\n\n    # Copy results.\n    new_res = Results([item for item in res.items()])\n\n    # Overwrite items with our new estimates.\n    new_res.niter = len(res.ncall[samp_idx])\n    new_res.ncall = res.ncall[samp_idx]\n    new_res.eff = eff\n    new_res.samples = res.samples[samp_idx]\n    new_res.samples_id = res.samples_id[samp_idx]\n    new_res.samples_it = res.samples_it[samp_idx]\n    new_res.samples_u = res.samples_u[samp_idx]\n    new_res.samples_n = samp_n\n    new_res.logwt = np.array(saved_logwt)\n    new_res.logl = logl\n    new_res.logvol = logvol\n    new_res.logz = np.array(saved_logz)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        new_res.logzerr = np.sqrt(np.array(saved_logzvar))\n    new_res.h = np.array(saved_h)\n\n    if return_idx:\n        return new_res, samp_idx\n    else:\n        return new_res", "response": "Resample a nested sampling run using bootstrap\n    resampling techniques to generate a realization of the expected prior volumes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsimulate a nested sampling run.", "response": "def simulate_run(res, rstate=None, return_idx=False, approx=False):\n    \"\"\"\n    Probes **combined uncertainties** (statistical and sampling) on a nested\n    sampling run by wrapping :meth:`jitter_run` and :meth:`resample_run`.\n\n    Parameters\n    ----------\n    res : :class:`~dynesty.results.Results` instance\n        The :class:`~dynesty.results.Results` instance taken from a previous\n        nested sampling run.\n\n    rstate : `~numpy.random.RandomState`, optional\n        `~numpy.random.RandomState` instance.\n\n    return_idx : bool, optional\n        Whether to return the list of resampled indices used to construct\n        the new run. Default is `False`.\n\n    approx : bool, optional\n        Whether to approximate all sets of uniform order statistics by their\n        associated marginals (from the Beta distribution). Default is `False`.\n\n    Returns\n    -------\n    new_res : :class:`~dynesty.results.Results` instance\n        A new :class:`~dynesty.results.Results` instance with corresponding\n        samples and weights based on our \"simulated\" samples and\n        prior volumes.\n\n    \"\"\"\n\n    if rstate is None:\n        rstate = np.random\n\n    # Resample run.\n    new_res, samp_idx = resample_run(res, rstate=rstate, return_idx=True)\n\n    # Jitter run.\n    new_res = jitter_run(new_res, rstate=rstate, approx=approx)\n\n    if return_idx:\n        return new_res, samp_idx\n    else:\n        return new_res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reweight_run(res, logp_new, logp_old=None):\n\n    # Extract info.\n    if logp_old is None:\n        logp_old = res['logl']\n    logrwt = logp_new - logp_old  # ln(reweight)\n    logvol = res['logvol']\n    logl = res['logl']\n    nsamps = len(logvol)\n\n    # Compute weights using quadratic estimator.\n    h = 0.\n    logz = -1.e300\n    loglstar = -1.e300\n    logzvar = 0.\n    logvols_pad = np.concatenate(([0.], logvol))\n    logdvols = misc.logsumexp(a=np.c_[logvols_pad[:-1], logvols_pad[1:]],\n                              axis=1, b=np.c_[np.ones(nsamps),\n                                              -np.ones(nsamps)])\n    logdvols += math.log(0.5)\n    dlvs = -np.diff(np.append(0., logvol))\n    saved_logwt, saved_logz, saved_logzvar, saved_h = [], [], [], []\n    for i in range(nsamps):\n        loglstar_new = logl[i]\n        logdvol, dlv = logdvols[i], dlvs[i]\n        logwt = np.logaddexp(loglstar_new, loglstar) + logdvol + logrwt[i]\n        logz_new = np.logaddexp(logz, logwt)\n        try:\n            lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                      math.exp(loglstar_new - logz_new) * loglstar_new)\n        except:\n            lzterm = 0.\n        h_new = (math.exp(logdvol) * lzterm +\n                 math.exp(logz - logz_new) * (h + logz) -\n                 logz_new)\n        dh = h_new - h\n        h = h_new\n        logz = logz_new\n        logzvar += dh * dlv\n        loglstar = loglstar_new\n        saved_logwt.append(logwt)\n        saved_logz.append(logz)\n        saved_logzvar.append(logzvar)\n        saved_h.append(h)\n\n    # Copy results.\n    new_res = Results([item for item in res.items()])\n\n    # Overwrite items with our new estimates.\n    new_res.logwt = np.array(saved_logwt)\n    new_res.logz = np.array(saved_logz)\n    new_res.logzerr = np.sqrt(np.array(saved_logzvar))\n    new_res.h = np.array(saved_h)\n\n    return new_res", "response": "Reweight a run based on a new target distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunravels a nested sampling run with only 1 live point.", "response": "def unravel_run(res, save_proposals=True, print_progress=True):\n    \"\"\"\n    Unravels a run with `K` live points into `K` \"strands\" (a nested sampling\n    run with only 1 live point). **WARNING: the anciliary quantities provided\n    with each unraveled \"strand\" are only valid if the point was initialized\n    from the prior.**\n\n    Parameters\n    ----------\n    res : :class:`~dynesty.results.Results` instance\n        The :class:`~dynesty.results.Results` instance taken from a previous\n        nested sampling run.\n\n    save_proposals : bool, optional\n        Whether to save a reference to the proposal distributions from the\n        original run in each unraveled strand. Default is `True`.\n\n    print_progress : bool, optional\n        Whether to output the current progress to `~sys.stderr`.\n        Default is `True`.\n\n    Returns\n    -------\n    new_res : list of :class:`~dynesty.results.Results` instances\n        A list of new :class:`~dynesty.results.Results` instances\n        for each individual strand.\n\n    \"\"\"\n\n    idxs = res.samples_id  # label for each live/dead point\n\n    # Check if we added in the last set of dead points.\n    added_live = True\n    try:\n        if len(idxs) != (res.niter + res.nlive):\n            added_live = False\n    except:\n        pass\n\n    # Recreate the nested sampling run for each strand.\n    new_res = []\n    nstrands = len(np.unique(idxs))\n    for counter, idx in enumerate(np.unique(idxs)):\n        # Select strand `idx`.\n        strand = (idxs == idx)\n        nsamps = sum(strand)\n        logl = res.logl[strand]\n\n        # Assign log(volume) to samples. With K=1 live point, the expected\n        # shrinking in `logvol` at each iteration is `-log(2)` (i.e.\n        # shrinking by 1/2). If the final set of live points were added,\n        # the expected value of the final live point is a uniform\n        # sample and so has an expected value of half the volume\n        # of the final dead point.\n        if added_live:\n            niter = nsamps - 1\n            logvol_dead = -math.log(2) * (1. + np.arange(niter))\n            if niter > 0:\n                logvol_live = logvol_dead[-1] + math.log(0.5)\n                logvol = np.append(logvol_dead, logvol_live)\n            else:  # point always live\n                logvol = np.array([math.log(0.5)])\n        else:\n            niter = nsamps\n            logvol = -math.log(2) * (1. + np.arange(niter))\n\n        # Compute weights using quadratic estimator.\n        h = 0.\n        logz = -1.e300\n        loglstar = -1.e300\n        logzvar = 0.\n        logvols_pad = np.concatenate(([0.], logvol))\n        logdvols = misc.logsumexp(a=np.c_[logvols_pad[:-1], logvols_pad[1:]],\n                                  axis=1, b=np.c_[np.ones(nsamps),\n                                                  -np.ones(nsamps)])\n        logdvols += math.log(0.5)\n        dlvs = logvols_pad[:-1] - logvols_pad[1:]\n        saved_logwt, saved_logz, saved_logzvar, saved_h = [], [], [], []\n        for i in range(nsamps):\n            loglstar_new = logl[i]\n            logdvol, dlv = logdvols[i], dlvs[i]\n            logwt = np.logaddexp(loglstar_new, loglstar) + logdvol\n            logz_new = np.logaddexp(logz, logwt)\n            lzterm = (math.exp(loglstar - logz_new) * loglstar +\n                      math.exp(loglstar_new - logz_new) * loglstar_new)\n            h_new = (math.exp(logdvol) * lzterm +\n                     math.exp(logz - logz_new) * (h + logz) -\n                     logz_new)\n            dh = h_new - h\n            h = h_new\n            logz = logz_new\n            logzvar += dh * dlv\n            loglstar = loglstar_new\n            saved_logwt.append(logwt)\n            saved_logz.append(logz)\n            saved_logzvar.append(logzvar)\n            saved_h.append(h)\n\n        # Compute sampling efficiency.\n        eff = 100. * nsamps / sum(res.ncall[strand])\n\n        # Save results.\n        r = [('nlive', 1),\n             ('niter', niter),\n             ('ncall', res.ncall[strand]),\n             ('eff', eff),\n             ('samples', res.samples[strand]),\n             ('samples_id', res.samples_id[strand]),\n             ('samples_it', res.samples_it[strand]),\n             ('samples_u', res.samples_u[strand]),\n             ('logwt', np.array(saved_logwt)),\n             ('logl', logl),\n             ('logvol', logvol),\n             ('logz', np.array(saved_logz)),\n             ('logzerr', np.sqrt(np.array(saved_logzvar))),\n             ('h', np.array(saved_h))]\n\n        # Add proposal information (if available).\n        if save_proposals:\n            try:\n                r.append(('prop', res.prop))\n                r.append(('prop_iter', res.prop_iter[strand]))\n                r.append(('samples_prop', res.samples_prop[strand]))\n                r.append(('scale', res.scale[strand]))\n            except:\n                pass\n\n        # Add on batch information (if available).\n        try:\n            r.append(('samples_batch', res.samples_batch[strand]))\n            r.append(('batch_bounds', res.batch_bounds))\n        except:\n            pass\n\n        # Append to list of strands.\n        new_res.append(Results(r))\n\n        # Print progress.\n        if print_progress:\n            sys.stderr.write('\\rStrand: {0}/{1}     '\n                             .format(counter + 1, nstrands))\n\n    return new_res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_runs(res_list, print_progress=True):\n\n    ntot = len(res_list)\n    counter = 0\n\n    # Establish our set of baseline runs and \"add-on\" runs.\n    rlist_base = []\n    rlist_add = []\n    for r in res_list:\n        try:\n            if np.any(r.samples_batch == 0):\n                rlist_base.append(r)\n            else:\n                rlist_add.append(r)\n        except:\n            rlist_base.append(r)\n    nbase, nadd = len(rlist_base), len(rlist_add)\n    if nbase == 1 and nadd == 1:\n        rlist_base = res_list\n        rlist_add = []\n\n    # Merge baseline runs while there are > 2 remaining results.\n    if len(rlist_base) > 1:\n        while len(rlist_base) > 2:\n            rlist_new = []\n            nruns = len(rlist_base)\n            i = 0\n            while i < nruns:\n                try:\n                    # Ignore posterior quantities while merging the runs.\n                    r1, r2 = rlist_base[i], rlist_base[i+1]\n                    res = _merge_two(r1, r2, compute_aux=False)\n                    rlist_new.append(res)\n                except:\n                    # Append the odd run to the new list.\n                    rlist_new.append(rlist_base[i])\n                i += 2\n                counter += 1\n                # Print progress.\n                if print_progress:\n                    sys.stderr.write('\\rMerge: {0}/{1}     '.format(counter,\n                                                                    ntot))\n            # Overwrite baseline set of results with merged results.\n            rlist_base = copy.copy(rlist_new)\n\n        # Compute posterior quantities after merging the final baseline runs.\n        res = _merge_two(rlist_base[0], rlist_base[1], compute_aux=True)\n    else:\n        res = rlist_base[0]\n\n    # Iteratively merge any remaining \"add-on\" results.\n    nruns = len(rlist_add)\n    for i, r in enumerate(rlist_add):\n        if i < nruns - 1:\n            res = _merge_two(res, r, compute_aux=False)\n        else:\n            res = _merge_two(res, r, compute_aux=True)\n        counter += 1\n        # Print progress.\n        if print_progress:\n            sys.stderr.write('\\rMerge: {0}/{1}     '.format(counter, ntot))\n\n    nsamps, samples_n = _get_nsamps_samples_n(res)\n    nlive = max(samples_n)\n    niter = res.niter\n    standard_run = False\n\n    # Check if we have a constant number of live points.\n    try:\n        nlive_test = np.ones(niter, dtype='int') * nlive\n        if np.all(samples_n == nlive_test):\n            standard_run = True\n    except:\n        pass\n\n    # Check if we have a constant number of live points where we have\n    # recycled the final set of live points.\n    try:\n        nlive_test = np.append(np.ones(niter - nlive, dtype='int') * nlive,\n                               np.arange(1, nlive + 1)[::-1])\n        if np.all(samples_n == nlive_test):\n            standard_run = True\n    except:\n        pass\n\n    # If the number of live points is consistent with a standard nested\n    # sampling run, slightly modify the format to keep with previous usage.\n    if standard_run:\n        res.__delitem__('samples_n')\n        res.nlive = nlive\n        res.niter = niter - nlive\n\n    return res", "response": "Merges a list of results into one run."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the KL divergence from the discrete probability distributions of res1 to res2.", "response": "def kl_divergence(res1, res2):\n    \"\"\"\n    Computes the `Kullback-Leibler (KL) divergence\n    <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`_ *from* the\n    discrete probability distribution defined by `res2` *to* the discrete\n    probability distribution defined by `res1`.\n\n    Parameters\n    ----------\n    res1 : :class:`~dynesty.results.Results` instance\n        :class:`~dynesty.results.Results` instance for the distribution we are\n        computing the KL divergence *to*. **Note that, by construction,\n        the samples in `res1` *must* be a subset of the samples in `res2`.**\n\n    res2 : :class:`~dynesty.results.Results` instance\n        :class:`~dynesty.results.Results` instance for the distribution we\n        are computing the KL divergence *from*. **Note that, by construction,\n        the samples in `res2` *must* be a superset of the samples in `res1`.**\n\n    Returns\n    -------\n    kld : `~numpy.ndarray` with shape (nsamps,)\n        The cumulative KL divergence defined over `res1`.\n\n    \"\"\"\n\n    # Define our importance weights.\n    logp1, logp2 = res1.logwt - res1.logz[-1], res2.logwt - res2.logz[-1]\n\n    # Define the positions where the discrete probability distributions exists.\n    samples1, samples2 = res1.samples, res2.samples\n    samples1_id, samples2_id = res1.samples_id, res2.samples_id\n    nsamps1, nsamps2 = len(samples1), len(samples2)\n\n    # Compute the KL divergence.\n    if nsamps1 == nsamps2 and np.all(samples1_id == samples2_id):\n        # If our runs have the same particles in the same order, compute\n        # the KL divergence in one go.\n        kld = np.exp(logp1) * (logp1 - logp2)\n    else:\n        # Otherwise, compute the components of the KL divergence one at a time.\n        uidxs = np.unique(samples1_id)  # unique particle IDs\n        count1, count2 = np.arange(nsamps1), np.arange(nsamps2)\n        kld = np.zeros(nsamps1)\n        for uidx in uidxs:\n\n            # Select matching particles.\n            sel1 = count1[samples1_id == uidx]\n            sel2 = count2[samples2_id == uidx]\n\n            # Select corresponding positions.\n            pos1, pos2 = samples1[sel1], samples2[sel2]\n            for s, p in zip(sel1, pos1):\n                # Search for a matching position.\n                pos_sel = sel2[np.all(np.isclose(pos2, p), axis=1)]\n                npos = len(pos_sel)\n                if npos > 1:\n                    # If there are several possible matches, pick the\n                    # one with the closet importance weight.\n                    diff = logp1[s] - logp2[pos_sel]\n                    # Compute the `s`-th term.\n                    kld[s] = np.exp(logp1[s]) * diff[np.argmin(abs(diff))]\n                elif npos == 1:\n                    # If there is only one match, compute the result directly.\n                    kld[s] = np.exp(logp1[s]) * (logp1[s] - logp2[pos_sel])\n                else:\n                    raise ValueError(\"Distribution from `res2` undefined at \"\n                                     \"position {0}.\".format(p))\n\n    return np.cumsum(kld)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kld_error(res, error='simulate', rstate=None, return_new=False,\n              approx=False):\n    \"\"\"\n    Computes the `Kullback-Leibler (KL) divergence\n    <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`_ *from* the\n    discrete probability distribution defined by `res` *to* the discrete\n    probability distribution defined by a **realization** of `res`.\n\n    Parameters\n    ----------\n    res : :class:`~dynesty.results.Results` instance\n        :class:`~dynesty.results.Results` instance for the distribution we\n        are computing the KL divergence *from*.\n\n    error : {`'jitter'`, `'resample'`, `'simulate'`}, optional\n        The error method employed, corresponding to :meth:`jitter_run`,\n        :meth:`resample_run`, and :meth:`simulate_run`, respectively.\n        Default is `'simulate'`.\n\n    rstate : `~numpy.random.RandomState`, optional\n        `~numpy.random.RandomState` instance.\n\n    return_new : bool, optional\n        Whether to return the realization of the run used to compute the\n        KL divergence. Default is `False`.\n\n    approx : bool, optional\n        Whether to approximate all sets of uniform order statistics by their\n        associated marginals (from the Beta distribution). Default is `False`.\n\n    Returns\n    -------\n    kld : `~numpy.ndarray` with shape (nsamps,)\n        The cumulative KL divergence defined *from* `res` *to* a\n        random realization of `res`.\n\n    new_res : :class:`~dynesty.results.Results` instance, optional\n        The :class:`~dynesty.results.Results` instance corresponding to\n        the random realization we computed the KL divergence *to*.\n\n    \"\"\"\n\n    # Define our original importance weights.\n    logp2 = res.logwt - res.logz[-1]\n\n    # Compute a random realization of our run.\n    if error == 'jitter':\n        new_res = jitter_run(res, rstate=rstate, approx=approx)\n    elif error == 'resample':\n        new_res, samp_idx = resample_run(res, rstate=rstate, return_idx=True)\n        logp2 = logp2[samp_idx]  # re-order our original results to match\n    elif error == 'simulate':\n        new_res, samp_idx = resample_run(res, rstate=rstate, return_idx=True)\n        new_res = jitter_run(new_res)\n        logp2 = logp2[samp_idx]  # re-order our original results to match\n    else:\n        raise ValueError(\"Input `'error'` option '{0}' is not valid.\"\n                         .format(error))\n\n    # Define our new importance weights.\n    logp1 = new_res.logwt - new_res.logz[-1]\n\n    # Compute the KL divergence.\n    kld = np.cumsum(np.exp(logp1) * (logp1 - logp2))\n\n    if return_new:\n        return kld, new_res\n    else:\n        return kld", "response": "Compute the Kullback - Leibler divergence from a discrete distribution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing and returns a sampler object for Static Nested Sampling. Parameters ---------- loglikelihood : function Function returning ln(likelihood) given parameters as a 1-d `~numpy` array of length `ndim`. prior_transform : function Function translating a unit cube to the parameter space according to the prior. The input is a 1-d `~numpy` array with length `ndim`, where each value is in the range [0, 1). The return value should also be a 1-d `~numpy` array with length `ndim`, where each value is a parameter. The return value is passed to the loglikelihood function. For example, for a 2 parameter model with flat priors in the range [0, 2), the function would be:: def prior_transform(u): return 2.0 * u ndim : int Number of parameters returned by `prior_transform` and accepted by `loglikelihood`. nlive : int, optional Number of \"live\" points. Larger numbers result in a more finely sampled posterior (more accurate evidence), but also a larger number of iterations required to converge. Default is `500`. bound : {`'none'`, `'single'`, `'multi'`, `'balls'`, `'cubes'`}, optional Method used to approximately bound the prior using the current set of live points. Conditions the sampling methods used to propose new live points. Choices are no bound (`'none'`), a single bounding ellipsoid (`'single'`), multiple bounding ellipsoids (`'multi'`), balls centered on each live point (`'balls'`), and cubes centered on each live point (`'cubes'`). Default is `'multi'`. sample : {`'auto'`, `'unif'`, `'rwalk'`, `'rstagger'`, `'slice'`, `'rslice'`, `'hslice'`}, optional Method used to sample uniformly within the likelihood constraint, conditioned on the provided bounds. Unique methods available are: uniform sampling within the bounds(`'unif'`), random walks with fixed proposals (`'rwalk'`), random walks with variable (\"staggering\") proposals (`'rstagger'`), multivariate slice sampling along preferred orientations (`'slice'`), \"random\" slice sampling along all orientations (`'rslice'`), and \"Hamiltonian\" slices along random trajectories (`'hslice'`). `'auto'` selects the sampling method based on the dimensionality of the problem (from `ndim`). When `ndim < 10`, this defaults to `'unif'`. When `10 <= ndim <= 20`, this defaults to `'rwalk'`. When `ndim > 20`, this defaults to `'hslice'` if a `gradient` is provided and `'slice'` otherwise. `'rstagger'` and `'rslice'` are provided as alternatives for `'rwalk'` and `'slice'`, respectively. Default is `'auto'`. periodic : iterable, optional A list of indices for parameters with periodic boundary conditions. These parameters *will not* have their positions constrained to be within the unit cube, enabling smooth behavior for parameters that may wrap around the edge. It is assumed that their periodicity is dealt with in the `prior_transform` and/or `loglikelihood` functions. Default is `None` (i.e. no periodic boundary conditions). update_interval : int or float, optional If an integer is passed, only update the proposal distribution every `update_interval`-th likelihood call. If a float is passed, update the proposal after every `round(update_interval * nlive)`-th likelihood call. Larger update intervals larger can be more efficient when the likelihood function is quick to evaluate. Default behavior is to target a roughly constant change in prior volume, with `1.5` for `'unif'`, `0.15 * walks` for `'rwalk'` and `'rstagger'`, `0.9 * ndim * slices` for `'slice'`, `2.0 * slices` for `'rslice'`, and `25.0 * slices` for `'hslice'`. first_update : dict, optional A dictionary containing parameters governing when the sampler should first update the bounding distribution from the unit cube (`'none'`) to the one specified by `sample`. Options are the minimum number of likelihood calls (`'min_ncall'`) and the minimum allowed overall efficiency in percent (`'min_eff'`). Defaults are `2 * nlive` and `10.`, respectively. npdim : int, optional Number of parameters accepted by `prior_transform`. This might differ from `ndim` in the case where a parameter of loglikelihood is dependent upon multiple independently distributed parameters, some of which may be nuisance parameters. rstate : `~numpy.random.RandomState`, optional `~numpy.random.RandomState` instance. If not given, the global random state of the `~numpy.random` module will be used. queue_size : int, optional Carry out likelihood evaluations in parallel by queueing up new live point proposals using (at most) `queue_size` many threads. Each thread independently proposes new live points until the proposal distribution is updated. If no value is passed, this defaults to `pool.size` (if a `pool` has been provided) and `1` otherwise (no parallelism). pool : user-provided pool, optional Use this pool of workers to execute operations in parallel. use_pool : dict, optional A dictionary containing flags indicating where a pool should be used to execute operations in parallel. These govern whether `prior_transform` is executed in parallel during initialization (`'prior_transform'`), `loglikelihood` is executed in parallel during initialization (`'loglikelihood'`), live points are proposed in parallel during a run (`'propose_point'`), and bounding distributions are updated in parallel during a run (`'update_bound'`). Default is `True` for all options. live_points : list of 3 `~numpy.ndarray` each with shape (nlive, ndim) A set of live points used to initialize the nested sampling run. Contains `live_u`, the coordinates on the unit cube, `live_v`, the transformed variables, and `live_logl`, the associated loglikelihoods. By default, if these are not provided the initial set of live points will be drawn uniformly from the unit `npdim`-cube. **WARNING: It is crucial that the initial set of live points have been sampled from the prior. Failure to provide a set of valid live points will result in incorrect results.** logl_args : dict, optional Additional arguments that can be passed to `loglikelihood`. logl_kwargs : dict, optional Additional keyword arguments that can be passed to `loglikelihood`. ptform_args : dict, optional Additional arguments that can be passed to `prior_transform`. ptform_kwargs : dict, optional Additional keyword arguments that can be passed to `prior_transform`. gradient : function, optional A function which returns the gradient corresponding to the provided `loglikelihood` *with respect to the unit cube*. If provided, this will be used when computing reflections when sampling with `'hslice'`. If not provided, gradients are approximated numerically using 2-sided differencing. grad_args : dict, optional Additional arguments that can be passed to `gradient`. grad_kwargs : dict, optional Additional keyword arguments that can be passed to `gradient`. compute_jac : bool, optional Whether to compute and apply the Jacobian `dv/du` from the target space `v` to the unit cube `u` when evaluating the `gradient`. If `False`, the gradient provided is assumed to be already defined with respect to the unit cube. If `True`, the gradient provided is assumed to be defined with respect to the target space so the Jacobian needs to be numerically computed and applied. Default is `False`. enlarge : float, optional Enlarge the volumes of the specified bounding object(s) by this fraction. The preferred method is to determine this organically using bootstrapping. If `bootstrap > 0`, this defaults to `1.0`. If `bootstrap = 0`, this instead defaults to `1.25`. bootstrap : int, optional Compute this many bootstrapped realizations of the bounding objects. Use the maximum distance found to the set of points left out during each iteration to enlarge the resulting volumes. Can lead to unstable bounding ellipsoids. Default is `0` (no bootstrap). vol_dec : float, optional For the `'multi'` bounding option, the required fractional reduction in volume after splitting an ellipsoid in order to to accept the split. Default is `0.5`. vol_check : float, optional For the `'multi'` bounding option, the factor used when checking if the volume of the original bounding ellipsoid is large enough to warrant `> 2` splits via `ell.vol > vol_check * nlive * pointvol`. Default is `2.0`. walks : int, optional For the `'rwalk'` sampling option, the minimum number of steps (minimum 2) before proposing a new live point. Default is `25`. facc : float, optional The target acceptance fraction for the `'rwalk'` sampling option. Default is `0.5`. Bounded to be between `[1. / walks, 1.]`. slices : int, optional For the `'slice'`, `'rslice'`, and `'hslice'` sampling options, the number of times to execute a \"slice update\" before proposing a new live point. Default is `5`. Note that `'slice'` cycles through **all dimensions** when executing a \"slice update\". fmove : float, optional The target fraction of samples that are proposed along a trajectory (i.e. not reflecting) for the `'hslice'` sampling option. Default is `0.9`. max_move : int, optional The maximum number of timesteps allowed for `'hslice'` per proposal forwards and backwards in time. Default is `100`. Returns ------- sampler : sampler from :mod:`~dynesty.nestedsamplers` An initialized instance of the chosen sampler specified via `bound`.", "response": "def NestedSampler(loglikelihood, prior_transform, ndim, nlive=500,\n                  bound='multi', sample='auto', periodic=None,\n                  update_interval=None, first_update=None,\n                  npdim=None, rstate=None, queue_size=None, pool=None,\n                  use_pool=None, live_points=None,\n                  logl_args=None, logl_kwargs=None,\n                  ptform_args=None, ptform_kwargs=None,\n                  gradient=None, grad_args=None, grad_kwargs=None,\n                  compute_jac=False,\n                  enlarge=None, bootstrap=0, vol_dec=0.5, vol_check=2.0,\n                  walks=25, facc=0.5, slices=5, fmove=0.9, max_move=100,\n                  **kwargs):\n    \"\"\"\n    Initializes and returns a sampler object for Static Nested Sampling.\n\n    Parameters\n    ----------\n    loglikelihood : function\n        Function returning ln(likelihood) given parameters as a 1-d `~numpy`\n        array of length `ndim`.\n\n    prior_transform : function\n        Function translating a unit cube to the parameter space according to\n        the prior. The input is a 1-d `~numpy` array with length `ndim`, where\n        each value is in the range [0, 1). The return value should also be a\n        1-d `~numpy` array with length `ndim`, where each value is a parameter.\n        The return value is passed to the loglikelihood function. For example,\n        for a 2 parameter model with flat priors in the range [0, 2), the\n        function would be::\n\n            def prior_transform(u):\n                return 2.0 * u\n\n    ndim : int\n        Number of parameters returned by `prior_transform` and accepted by\n        `loglikelihood`.\n\n    nlive : int, optional\n        Number of \"live\" points. Larger numbers result in a more finely\n        sampled posterior (more accurate evidence), but also a larger\n        number of iterations required to converge. Default is `500`.\n\n    bound : {`'none'`, `'single'`, `'multi'`, `'balls'`, `'cubes'`}, optional\n        Method used to approximately bound the prior using the current\n        set of live points. Conditions the sampling methods used to\n        propose new live points. Choices are no bound (`'none'`), a single\n        bounding ellipsoid (`'single'`), multiple bounding ellipsoids\n        (`'multi'`), balls centered on each live point (`'balls'`), and\n        cubes centered on each live point (`'cubes'`). Default is `'multi'`.\n\n    sample : {`'auto'`, `'unif'`, `'rwalk'`, `'rstagger'`,\n              `'slice'`, `'rslice'`, `'hslice'`}, optional\n        Method used to sample uniformly within the likelihood constraint,\n        conditioned on the provided bounds. Unique methods available are:\n        uniform sampling within the bounds(`'unif'`),\n        random walks with fixed proposals (`'rwalk'`),\n        random walks with variable (\"staggering\") proposals (`'rstagger'`),\n        multivariate slice sampling along preferred orientations (`'slice'`),\n        \"random\" slice sampling along all orientations (`'rslice'`), and\n        \"Hamiltonian\" slices along random trajectories (`'hslice'`).\n        `'auto'` selects the sampling method based on the dimensionality\n        of the problem (from `ndim`).\n        When `ndim < 10`, this defaults to `'unif'`.\n        When `10 <= ndim <= 20`, this defaults to `'rwalk'`.\n        When `ndim > 20`, this defaults to `'hslice'` if a `gradient` is\n        provided and `'slice'` otherwise. `'rstagger'` and `'rslice'`\n        are provided as alternatives for `'rwalk'` and `'slice'`, respectively.\n        Default is `'auto'`.\n\n    periodic : iterable, optional\n        A list of indices for parameters with periodic boundary conditions.\n        These parameters *will not* have their positions constrained to be\n        within the unit cube, enabling smooth behavior for parameters\n        that may wrap around the edge. It is assumed that their periodicity\n        is dealt with in the `prior_transform` and/or `loglikelihood`\n        functions. Default is `None` (i.e. no periodic boundary conditions).\n\n    update_interval : int or float, optional\n        If an integer is passed, only update the proposal distribution every\n        `update_interval`-th likelihood call. If a float is passed, update the\n        proposal after every `round(update_interval * nlive)`-th likelihood\n        call. Larger update intervals larger can be more efficient\n        when the likelihood function is quick to evaluate. Default behavior\n        is to target a roughly constant change in prior volume, with\n        `1.5` for `'unif'`, `0.15 * walks` for `'rwalk'` and `'rstagger'`,\n        `0.9 * ndim * slices` for `'slice'`, `2.0 * slices` for `'rslice'`,\n        and `25.0 * slices` for `'hslice'`.\n\n    first_update : dict, optional\n        A dictionary containing parameters governing when the sampler should\n        first update the bounding distribution from the unit cube (`'none'`)\n        to the one specified by `sample`. Options are the minimum number of\n        likelihood calls (`'min_ncall'`) and the minimum allowed overall\n        efficiency in percent (`'min_eff'`). Defaults are `2 * nlive` and\n        `10.`, respectively.\n\n    npdim : int, optional\n        Number of parameters accepted by `prior_transform`. This might differ\n        from `ndim` in the case where a parameter of loglikelihood is dependent\n        upon multiple independently distributed parameters, some of which may\n        be nuisance parameters.\n\n    rstate : `~numpy.random.RandomState`, optional\n        `~numpy.random.RandomState` instance. If not given, the\n         global random state of the `~numpy.random` module will be used.\n\n    queue_size : int, optional\n        Carry out likelihood evaluations in parallel by queueing up new live\n        point proposals using (at most) `queue_size` many threads. Each thread\n        independently proposes new live points until the proposal distribution\n        is updated. If no value is passed, this defaults to `pool.size` (if\n        a `pool` has been provided) and `1` otherwise (no parallelism).\n\n    pool : user-provided pool, optional\n        Use this pool of workers to execute operations in parallel.\n\n    use_pool : dict, optional\n        A dictionary containing flags indicating where a pool should be used to\n        execute operations in parallel. These govern whether `prior_transform`\n        is executed in parallel during initialization (`'prior_transform'`),\n        `loglikelihood` is executed in parallel during initialization\n        (`'loglikelihood'`), live points are proposed in parallel during a run\n        (`'propose_point'`), and bounding distributions are updated in\n        parallel during a run (`'update_bound'`). Default is `True` for all\n        options.\n\n    live_points : list of 3 `~numpy.ndarray` each with shape (nlive, ndim)\n        A set of live points used to initialize the nested sampling run.\n        Contains `live_u`, the coordinates on the unit cube, `live_v`, the\n        transformed variables, and `live_logl`, the associated loglikelihoods.\n        By default, if these are not provided the initial set of live points\n        will be drawn uniformly from the unit `npdim`-cube.\n        **WARNING: It is crucial that the initial set of live points have been\n        sampled from the prior. Failure to provide a set of valid live points\n        will result in incorrect results.**\n\n    logl_args : dict, optional\n        Additional arguments that can be passed to `loglikelihood`.\n\n    logl_kwargs : dict, optional\n        Additional keyword arguments that can be passed to `loglikelihood`.\n\n    ptform_args : dict, optional\n        Additional arguments that can be passed to `prior_transform`.\n\n    ptform_kwargs : dict, optional\n        Additional keyword arguments that can be passed to `prior_transform`.\n\n    gradient : function, optional\n        A function which returns the gradient corresponding to\n        the provided `loglikelihood` *with respect to the unit cube*.\n        If provided, this will be used when computing reflections\n        when sampling with `'hslice'`. If not provided, gradients are\n        approximated numerically using 2-sided differencing.\n\n    grad_args : dict, optional\n        Additional arguments that can be passed to `gradient`.\n\n    grad_kwargs : dict, optional\n        Additional keyword arguments that can be passed to `gradient`.\n\n    compute_jac : bool, optional\n        Whether to compute and apply the Jacobian `dv/du`\n        from the target space `v` to the unit cube `u` when evaluating the\n        `gradient`. If `False`, the gradient provided is assumed to be\n        already defined with respect to the unit cube. If `True`, the gradient\n        provided is assumed to be defined with respect to the target space\n        so the Jacobian needs to be numerically computed and applied. Default\n        is `False`.\n\n    enlarge : float, optional\n        Enlarge the volumes of the specified bounding object(s) by this\n        fraction. The preferred method is to determine this organically\n        using bootstrapping. If `bootstrap > 0`, this defaults to `1.0`.\n        If `bootstrap = 0`, this instead defaults to `1.25`.\n\n    bootstrap : int, optional\n        Compute this many bootstrapped realizations of the bounding\n        objects. Use the maximum distance found to the set of points left\n        out during each iteration to enlarge the resulting volumes. Can\n        lead to unstable bounding ellipsoids. Default is `0` (no bootstrap).\n\n    vol_dec : float, optional\n        For the `'multi'` bounding option, the required fractional reduction\n        in volume after splitting an ellipsoid in order to to accept the split.\n        Default is `0.5`.\n\n    vol_check : float, optional\n        For the `'multi'` bounding option, the factor used when checking if\n        the volume of the original bounding ellipsoid is large enough to\n        warrant `> 2` splits via `ell.vol > vol_check * nlive * pointvol`.\n        Default is `2.0`.\n\n    walks : int, optional\n        For the `'rwalk'` sampling option, the minimum number of steps\n        (minimum 2) before proposing a new live point. Default is `25`.\n\n    facc : float, optional\n        The target acceptance fraction for the `'rwalk'` sampling option.\n        Default is `0.5`. Bounded to be between `[1. / walks, 1.]`.\n\n    slices : int, optional\n        For the `'slice'`, `'rslice'`, and `'hslice'` sampling\n        options, the number of times to execute a \"slice update\"\n        before proposing a new live point. Default is `5`.\n        Note that `'slice'` cycles through **all dimensions**\n        when executing a \"slice update\".\n\n    fmove : float, optional\n        The target fraction of samples that are proposed along a trajectory\n        (i.e. not reflecting) for the `'hslice'` sampling option.\n        Default is `0.9`.\n\n    max_move : int, optional\n        The maximum number of timesteps allowed for `'hslice'`\n        per proposal forwards and backwards in time. Default is `100`.\n\n    Returns\n    -------\n    sampler : sampler from :mod:`~dynesty.nestedsamplers`\n        An initialized instance of the chosen sampler specified via `bound`.\n\n    \"\"\"\n\n    # Prior dimensions.\n    if npdim is None:\n        npdim = ndim\n\n    # Bounding method.\n    if bound not in _SAMPLERS:\n        raise ValueError(\"Unknown bounding method: '{0}'\".format(bound))\n\n    # Sampling method.\n    if sample == 'auto':\n        if npdim < 10:\n            sample = 'unif'\n        elif 10 <= npdim <= 20:\n            sample = 'rwalk'\n        else:\n            if gradient is None:\n                sample = 'slice'\n            else:\n                sample = 'hslice'\n    if sample not in _SAMPLING:\n        raise ValueError(\"Unknown sampling method: '{0}'\".format(sample))\n\n    # Dimensional warning check.\n    if nlive <= 2 * ndim:\n        warnings.warn(\"Beware! Having `nlive <= 2 * ndim` is extremely risky!\")\n    elif nlive < ndim * (ndim + 1) // 2 and bound in ['single', 'multi']:\n        warnings.warn(\"A note of caution: \"\n                      \"having `nlive < ndim * (ndim + 1) // 2` may result in \"\n                      \"unconstrained bounding distributions.\")\n\n    # Gather non-periodic boundary conditions.\n    if periodic is not None:\n        nonperiodic = np.ones(npdim, dtype='bool')\n        nonperiodic[periodic] = False\n    else:\n        nonperiodic = None\n    kwargs['nonperiodic'] = nonperiodic\n\n    # Update interval for bounds.\n    if update_interval is None:\n        if sample == 'unif':\n            update_interval = 1.5\n        elif sample == 'rwalk' or sample == 'rstagger':\n            update_interval = 0.15 * walks\n        elif sample == 'slice':\n            update_interval = 0.9 * npdim * slices\n        elif sample == 'rslice':\n            update_interval = 2.0 * slices\n        elif sample == 'hslice':\n            update_interval = 25.0 * slices\n        else:\n            raise ValueError(\"Unknown sampling method: '{0}'\".format(sample))\n    if bound == 'none':\n        update_interval = sys.maxsize  # no need to update with no bounds\n    if isinstance(update_interval, float):\n        update_interval = max(1, round(update_interval * nlive))\n\n    # Keyword arguments controlling the first update.\n    if first_update is None:\n        first_update = dict()\n\n    # Random state.\n    if rstate is None:\n        rstate = np.random\n\n    # Log-likelihood.\n    if logl_args is None:\n        logl_args = []\n    if logl_kwargs is None:\n        logl_kwargs = {}\n\n    # Prior transform.\n    if ptform_args is None:\n        ptform_args = []\n    if ptform_kwargs is None:\n        ptform_kwargs = {}\n\n    # gradient\n    if grad_args is None:\n        grad_args = []\n    if grad_kwargs is None:\n        grad_kwargs = {}\n\n    # Bounding distribution modifications.\n    if enlarge is not None:\n        kwargs['enlarge'] = enlarge\n    if bootstrap is not None:\n        kwargs['bootstrap'] = bootstrap\n    if vol_dec is not None:\n        kwargs['vol_dec'] = vol_dec\n    if vol_check is not None:\n        kwargs['vol_check'] = vol_check\n\n    # Sampling.\n    if walks is not None:\n        kwargs['walks'] = walks\n    if facc is not None:\n        kwargs['facc'] = facc\n    if slices is not None:\n        kwargs['slices'] = slices\n    if fmove is not None:\n        kwargs['fmove'] = fmove\n    if max_move is not None:\n        kwargs['max_move'] = max_move\n\n    # Set up parallel (or serial) evaluation.\n    if queue_size is not None and queue_size < 1:\n        raise ValueError(\"The queue must contain at least one element!\")\n    elif (queue_size == 1) or (pool is None and queue_size is None):\n        M = map\n        queue_size = 1\n    elif pool is not None:\n        M = pool.map\n        if queue_size is None:\n            try:\n                queue_size = pool.size\n            except:\n                raise ValueError(\"Cannot initialize `queue_size` because \"\n                                 \"`pool.size` has not been provided. Please\"\n                                 \"define `pool.size` or specify `queue_size` \"\n                                 \"explicitly.\")\n    else:\n        raise ValueError(\"`queue_size > 1` but no `pool` provided.\")\n    if use_pool is None:\n        use_pool = dict()\n\n    # Wrap functions.\n    ptform = _function_wrapper(prior_transform, ptform_args, ptform_kwargs,\n                               name='prior_transform')\n    loglike = _function_wrapper(loglikelihood, logl_args, logl_kwargs,\n                                name='loglikelihood')\n\n    # Add in gradient.\n    if gradient is not None:\n        grad = _function_wrapper(gradient, grad_args, grad_kwargs,\n                                 name='gradient')\n        kwargs['grad'] = grad\n        kwargs['compute_jac'] = compute_jac\n\n    # Initialize live points and calculate log-likelihoods.\n    if live_points is None:\n        live_u = rstate.rand(nlive, npdim)  # positions in unit cube\n        if use_pool.get('prior_transform', True):\n            live_v = np.array(list(M(ptform,\n                                     np.array(live_u))))  # real parameters\n        else:\n            live_v = np.array(list(map(ptform,\n                                       np.array(live_u))))\n        if use_pool.get('loglikelihood', True):\n            live_logl = np.array(list(M(loglike,\n                                        np.array(live_v))))  # log likelihood\n        else:\n            live_logl = np.array(list(map(loglike,\n                                          np.array(live_v))))\n        live_points = [live_u, live_v, live_logl]\n\n    # Convert all `-np.inf` log-likelihoods to finite large numbers.\n    # Necessary to keep estimators in our sampler from breaking.\n    for i, logl in enumerate(live_points[2]):\n        if not np.isfinite(logl):\n            if np.sign(logl) < 0:\n                live_points[2][i] = -1e300\n            else:\n                raise ValueError(\"The log-likelihood ({0}) of live point {1} \"\n                                 \"located at u={2} v={3} is invalid.\"\n                                 .format(logl, i, live_points[0][i],\n                                         live_points[1][i]))\n\n    # Initialize our nested sampler.\n    sampler = _SAMPLERS[bound](loglike, ptform, npdim,\n                               live_points, sample, update_interval,\n                               first_update, rstate, queue_size, pool,\n                               use_pool, kwargs)\n\n    return sampler"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing and returns a sampler object for Dynamic Nested Sampling. Parameters ---------- loglikelihood : function Function returning ln(likelihood) given parameters as a 1-d `~numpy` array of length `ndim`. prior_transform : function Function translating a unit cube to the parameter space according to the prior. The input is a 1-d `~numpy` array with length `ndim`, where each value is in the range [0, 1). The return value should also be a 1-d `~numpy` array with length `ndim`, where each value is a parameter. The return value is passed to the loglikelihood function. For example, for a 2 parameter model with flat priors in the range [0, 2), the function would be:: def prior_transform(u): return 2.0 * u ndim : int Number of parameters returned by `prior_transform` and accepted by `loglikelihood`. bound : {`'none'`, `'single'`, `'multi'`, `'balls'`, `'cubes'`}, optional Method used to approximately bound the prior using the current set of live points. Conditions the sampling methods used to propose new live points. Choices are no bound (`'none'`), a single bounding ellipsoid (`'single'`), multiple bounding ellipsoids (`'multi'`), balls centered on each live point (`'balls'`), and cubes centered on each live point (`'cubes'`). Default is `'multi'`. sample : {`'auto'`, `'unif'`, `'rwalk'`, `'rstagger'`, `'slice'`, `'rslice'`, `'hslice'`}, optional Method used to sample uniformly within the likelihood constraint, conditioned on the provided bounds. Unique methods available are: uniform sampling within the bounds(`'unif'`), random walks with fixed proposals (`'rwalk'`), random walks with variable (\"staggering\") proposals (`'rstagger'`), multivariate slice sampling along preferred orientations (`'slice'`), \"random\" slice sampling along all orientations (`'rslice'`), and \"Hamiltonian\" slices along random trajectories (`'hslice'`). `'auto'` selects the sampling method based on the dimensionality of the problem (from `ndim`). When `ndim < 10`, this defaults to `'unif'`. When `10 <= ndim <= 20`, this defaults to `'rwalk'`. When `ndim > 20`, this defaults to `'hslice'` if a `gradient` is provided and `'slice'` otherwise. `'rstagger'` and `'rslice'` are provided as alternatives for `'rwalk'` and `'slice'`, respectively. Default is `'auto'`. periodic : iterable, optional A list of indices for parameters with periodic boundary conditions. These parameters *will not* have their positions constrained to be within the unit cube, enabling smooth behavior for parameters that may wrap around the edge. It is assumed that their periodicity is dealt with in the `prior_transform` and/or `loglikelihood` functions. Default is `None` (i.e. no periodic boundary conditions). update_interval : int or float, optional If an integer is passed, only update the proposal distribution every `update_interval`-th likelihood call. If a float is passed, update the proposal after every `round(update_interval * nlive)`-th likelihood call. Larger update intervals larger can be more efficient when the likelihood function is quick to evaluate. Default behavior is to target a roughly constant change in prior volume, with `1.5` for `'unif'`, `0.15 * walks` for `'rwalk'` and `'rstagger'`, `0.9 * ndim * slices` for `'slice'`, `2.0 * slices` for `'rslice'`, and `25.0 * slices` for `'hslice'`. first_update : dict, optional A dictionary containing parameters governing when the sampler should first update the bounding distribution from the unit cube (`'none'`) to the one specified by `sample`. Options are the minimum number of likelihood calls (`'min_ncall'`) and the minimum allowed overall efficiency in percent (`'min_eff'`). Defaults are `2 * nlive` and `10.`, respectively. npdim : int, optional Number of parameters accepted by `prior_transform`. This might differ from `ndim` in the case where a parameter of loglikelihood is dependent upon multiple independently distributed parameters, some of which may be nuisance parameters. rstate : `~numpy.random.RandomState`, optional `~numpy.random.RandomState` instance. If not given, the global random state of the `~numpy.random` module will be used. queue_size : int, optional Carry out likelihood evaluations in parallel by queueing up new live point proposals using (at most) `queue_size` many threads. Each thread independently proposes new live points until the proposal distribution is updated. If no value is passed, this defaults to `pool.size` (if a `pool` has been provided) and `1` otherwise (no parallelism). pool : user-provided pool, optional Use this pool of workers to execute operations in parallel. use_pool : dict, optional A dictionary containing flags indicating where a pool should be used to execute operations in parallel. These govern whether `prior_transform` is executed in parallel during initialization (`'prior_transform'`), `loglikelihood` is executed in parallel during initialization (`'loglikelihood'`), live points are proposed in parallel during a run (`'propose_point'`), bounding distributions are updated in parallel during a run (`'update_bound'`), and the stopping criteria is evaluated in parallel during a run (`'stop_function'`). Default is `True` for all options. logl_args : dict, optional Additional arguments that can be passed to `loglikelihood`. logl_kwargs : dict, optional Additional keyword arguments that can be passed to `loglikelihood`. ptform_args : dict, optional Additional arguments that can be passed to `prior_transform`. ptform_kwargs : dict, optional Additional keyword arguments that can be passed to `prior_transform`. gradient : function, optional A function which returns the gradient corresponding to the provided `loglikelihood` *with respect to the unit cube*. If provided, this will be used when computing reflections when sampling with `'hslice'`. If not provided, gradients are approximated numerically using 2-sided differencing. grad_args : dict, optional Additional arguments that can be passed to `gradient`. grad_kwargs : dict, optional Additional keyword arguments that can be passed to `gradient`. compute_jac : bool, optional Whether to compute and apply the Jacobian `dv/du` from the target space `v` to the unit cube `u` when evaluating the `gradient`. If `False`, the gradient provided is assumed to be already defined with respect to the unit cube. If `True`, the gradient provided is assumed to be defined with respect to the target space so the Jacobian needs to be numerically computed and applied. Default is `False`. enlarge : float, optional Enlarge the volumes of the specified bounding object(s) by this fraction. The preferred method is to determine this organically using bootstrapping. If `bootstrap > 0`, this defaults to `1.0`. If `bootstrap = 0`, this instead defaults to `1.25`. bootstrap : int, optional Compute this many bootstrapped realizations of the bounding objects. Use the maximum distance found to the set of points left out during each iteration to enlarge the resulting volumes. Can lead to unstable bounding ellipsoids. Default is `0` (no bootstrap). vol_dec : float, optional For the `'multi'` bounding option, the required fractional reduction in volume after splitting an ellipsoid in order to to accept the split. Default is `0.5`. vol_check : float, optional For the `'multi'` bounding option, the factor used when checking if the volume of the original bounding ellipsoid is large enough to warrant `> 2` splits via `ell.vol > vol_check * nlive * pointvol`. Default is `2.0`. walks : int, optional For the `'rwalk'` sampling option, the minimum number of steps (minimum 2) before proposing a new live point. Default is `25`. facc : float, optional The target acceptance fraction for the `'rwalk'` sampling option. Default is `0.5`. Bounded to be between `[1. / walks, 1.]`. slices : int, optional For the `'slice'`, `'rslice'`, and `'hslice'` sampling options, the number of times to execute a \"slice update\" before proposing a new live point. Default is `5`. Note that `'slice'` cycles through **all dimensions** when executing a \"slice update\". fmove : float, optional The target fraction of samples that are proposed along a trajectory (i.e. not reflecting) for the `'hslice'` sampling option. Default is `0.9`. max_move : int, optional The maximum number of timesteps allowed for `'hslice'` per proposal forwards and backwards in time. Default is `100`. Returns ------- sampler : a :class:`dynesty.DynamicSampler` instance An initialized instance of the dynamic nested sampler.", "response": "def DynamicNestedSampler(loglikelihood, prior_transform, ndim,\n                         bound='multi', sample='auto', periodic=None,\n                         update_interval=None, first_update=None,\n                         npdim=None, rstate=None, queue_size=None, pool=None,\n                         use_pool=None, logl_args=None, logl_kwargs=None,\n                         ptform_args=None, ptform_kwargs=None,\n                         gradient=None, grad_args=None, grad_kwargs=None,\n                         compute_jac=False,\n                         enlarge=None, bootstrap=0,\n                         vol_dec=0.5, vol_check=2.0,\n                         walks=25, facc=0.5,\n                         slices=5, fmove=0.9, max_move=100,\n                         **kwargs):\n    \"\"\"\n    Initializes and returns a sampler object for Dynamic Nested Sampling.\n\n    Parameters\n    ----------\n    loglikelihood : function\n        Function returning ln(likelihood) given parameters as a 1-d `~numpy`\n        array of length `ndim`.\n\n    prior_transform : function\n        Function translating a unit cube to the parameter space according to\n        the prior. The input is a 1-d `~numpy` array with length `ndim`, where\n        each value is in the range [0, 1). The return value should also be a\n        1-d `~numpy` array with length `ndim`, where each value is a parameter.\n        The return value is passed to the loglikelihood function. For example,\n        for a 2 parameter model with flat priors in the range [0, 2), the\n        function would be::\n\n            def prior_transform(u):\n                return 2.0 * u\n\n    ndim : int\n        Number of parameters returned by `prior_transform` and accepted by\n        `loglikelihood`.\n\n    bound : {`'none'`, `'single'`, `'multi'`, `'balls'`, `'cubes'`}, optional\n        Method used to approximately bound the prior using the current\n        set of live points. Conditions the sampling methods used to\n        propose new live points. Choices are no bound (`'none'`), a single\n        bounding ellipsoid (`'single'`), multiple bounding ellipsoids\n        (`'multi'`), balls centered on each live point (`'balls'`), and\n        cubes centered on each live point (`'cubes'`). Default is `'multi'`.\n\n    sample : {`'auto'`, `'unif'`, `'rwalk'`, `'rstagger'`,\n              `'slice'`, `'rslice'`, `'hslice'`}, optional\n        Method used to sample uniformly within the likelihood constraint,\n        conditioned on the provided bounds. Unique methods available are:\n        uniform sampling within the bounds(`'unif'`),\n        random walks with fixed proposals (`'rwalk'`),\n        random walks with variable (\"staggering\") proposals (`'rstagger'`),\n        multivariate slice sampling along preferred orientations (`'slice'`),\n        \"random\" slice sampling along all orientations (`'rslice'`), and\n        \"Hamiltonian\" slices along random trajectories (`'hslice'`).\n        `'auto'` selects the sampling method based on the dimensionality\n        of the problem (from `ndim`).\n        When `ndim < 10`, this defaults to `'unif'`.\n        When `10 <= ndim <= 20`, this defaults to `'rwalk'`.\n        When `ndim > 20`, this defaults to `'hslice'` if a `gradient` is\n        provided and `'slice'` otherwise. `'rstagger'` and `'rslice'`\n        are provided as alternatives for `'rwalk'` and `'slice'`, respectively.\n        Default is `'auto'`.\n\n    periodic : iterable, optional\n        A list of indices for parameters with periodic boundary conditions.\n        These parameters *will not* have their positions constrained to be\n        within the unit cube, enabling smooth behavior for parameters\n        that may wrap around the edge. It is assumed that their periodicity\n        is dealt with in the `prior_transform` and/or `loglikelihood`\n        functions. Default is `None` (i.e. no periodic boundary conditions).\n\n    update_interval : int or float, optional\n        If an integer is passed, only update the proposal distribution every\n        `update_interval`-th likelihood call. If a float is passed, update the\n        proposal after every `round(update_interval * nlive)`-th likelihood\n        call. Larger update intervals larger can be more efficient\n        when the likelihood function is quick to evaluate. Default behavior\n        is to target a roughly constant change in prior volume, with\n        `1.5` for `'unif'`, `0.15 * walks` for `'rwalk'` and `'rstagger'`,\n        `0.9 * ndim * slices` for `'slice'`, `2.0 * slices` for `'rslice'`,\n        and `25.0 * slices` for `'hslice'`.\n\n    first_update : dict, optional\n        A dictionary containing parameters governing when the sampler should\n        first update the bounding distribution from the unit cube (`'none'`)\n        to the one specified by `sample`. Options are the minimum number of\n        likelihood calls (`'min_ncall'`) and the minimum allowed overall\n        efficiency in percent (`'min_eff'`). Defaults are `2 * nlive` and\n        `10.`, respectively.\n\n    npdim : int, optional\n        Number of parameters accepted by `prior_transform`. This might differ\n        from `ndim` in the case where a parameter of loglikelihood is dependent\n        upon multiple independently distributed parameters, some of which may\n        be nuisance parameters.\n\n    rstate : `~numpy.random.RandomState`, optional\n        `~numpy.random.RandomState` instance. If not given, the\n         global random state of the `~numpy.random` module will be used.\n\n    queue_size : int, optional\n        Carry out likelihood evaluations in parallel by queueing up new live\n        point proposals using (at most) `queue_size` many threads. Each thread\n        independently proposes new live points until the proposal distribution\n        is updated. If no value is passed, this defaults to `pool.size` (if\n        a `pool` has been provided) and `1` otherwise (no parallelism).\n\n    pool : user-provided pool, optional\n        Use this pool of workers to execute operations in parallel.\n\n    use_pool : dict, optional\n        A dictionary containing flags indicating where a pool should be used to\n        execute operations in parallel. These govern whether `prior_transform`\n        is executed in parallel during initialization (`'prior_transform'`),\n        `loglikelihood` is executed in parallel during initialization\n        (`'loglikelihood'`), live points are proposed in parallel during a run\n        (`'propose_point'`), bounding distributions are updated in\n        parallel during a run (`'update_bound'`), and the stopping criteria\n        is evaluated in parallel during a run (`'stop_function'`).\n        Default is `True` for all options.\n\n    logl_args : dict, optional\n        Additional arguments that can be passed to `loglikelihood`.\n\n    logl_kwargs : dict, optional\n        Additional keyword arguments that can be passed to `loglikelihood`.\n\n    ptform_args : dict, optional\n        Additional arguments that can be passed to `prior_transform`.\n\n    ptform_kwargs : dict, optional\n        Additional keyword arguments that can be passed to `prior_transform`.\n\n    gradient : function, optional\n        A function which returns the gradient corresponding to\n        the provided `loglikelihood` *with respect to the unit cube*.\n        If provided, this will be used when computing reflections\n        when sampling with `'hslice'`. If not provided, gradients are\n        approximated numerically using 2-sided differencing.\n\n    grad_args : dict, optional\n        Additional arguments that can be passed to `gradient`.\n\n    grad_kwargs : dict, optional\n        Additional keyword arguments that can be passed to `gradient`.\n\n    compute_jac : bool, optional\n        Whether to compute and apply the Jacobian `dv/du`\n        from the target space `v` to the unit cube `u` when evaluating the\n        `gradient`. If `False`, the gradient provided is assumed to be\n        already defined with respect to the unit cube. If `True`, the gradient\n        provided is assumed to be defined with respect to the target space\n        so the Jacobian needs to be numerically computed and applied. Default\n        is `False`.\n\n    enlarge : float, optional\n        Enlarge the volumes of the specified bounding object(s) by this\n        fraction. The preferred method is to determine this organically\n        using bootstrapping. If `bootstrap > 0`, this defaults to `1.0`.\n        If `bootstrap = 0`, this instead defaults to `1.25`.\n\n    bootstrap : int, optional\n        Compute this many bootstrapped realizations of the bounding\n        objects. Use the maximum distance found to the set of points left\n        out during each iteration to enlarge the resulting volumes. Can lead\n        to unstable bounding ellipsoids. Default is `0` (no bootstrap).\n\n    vol_dec : float, optional\n        For the `'multi'` bounding option, the required fractional reduction\n        in volume after splitting an ellipsoid in order to to accept the split.\n        Default is `0.5`.\n\n    vol_check : float, optional\n        For the `'multi'` bounding option, the factor used when checking if\n        the volume of the original bounding ellipsoid is large enough to\n        warrant `> 2` splits via `ell.vol > vol_check * nlive * pointvol`.\n        Default is `2.0`.\n\n    walks : int, optional\n        For the `'rwalk'` sampling option, the minimum number of steps\n        (minimum 2) before proposing a new live point. Default is `25`.\n\n    facc : float, optional\n        The target acceptance fraction for the `'rwalk'` sampling option.\n        Default is `0.5`. Bounded to be between `[1. / walks, 1.]`.\n\n    slices : int, optional\n        For the `'slice'`, `'rslice'`, and `'hslice'` sampling\n        options, the number of times to execute a \"slice update\"\n        before proposing a new live point. Default is `5`.\n        Note that `'slice'` cycles through **all dimensions**\n        when executing a \"slice update\".\n\n    fmove : float, optional\n        The target fraction of samples that are proposed along a trajectory\n        (i.e. not reflecting) for the `'hslice'` sampling option.\n        Default is `0.9`.\n\n    max_move : int, optional\n        The maximum number of timesteps allowed for `'hslice'`\n        per proposal forwards and backwards in time. Default is `100`.\n\n    Returns\n    -------\n    sampler : a :class:`dynesty.DynamicSampler` instance\n        An initialized instance of the dynamic nested sampler.\n\n    \"\"\"\n\n    # Prior dimensions.\n    if npdim is None:\n        npdim = ndim\n\n    # Bounding method.\n    if bound not in _SAMPLERS:\n        raise ValueError(\"Unknown bounding method: '{0}'\".format(bound))\n\n    # Sampling method.\n    if sample == 'auto':\n        if npdim < 10:\n            sample = 'unif'\n        elif 10 <= npdim <= 20:\n            sample = 'rwalk'\n        else:\n            if gradient is None:\n                sample = 'slice'\n            else:\n                sample = 'hslice'\n    if sample not in _SAMPLING:\n        raise ValueError(\"Unknown sampling method: '{0}'\".format(sample))\n\n    # Gather non-periodic boundary conditions.\n    if periodic is not None:\n        nonperiodic = np.ones(npdim, dtype='bool')\n        nonperiodic[periodic] = False\n    else:\n        nonperiodic = None\n    kwargs['nonperiodic'] = nonperiodic\n\n    # Update interval for bounds.\n    if update_interval is None:\n        if sample == 'unif':\n            update_interval = 1.5\n        elif sample == 'rwalk' or sample == 'rstagger':\n            update_interval = 0.15 * walks\n        elif sample == 'slice':\n            update_interval = 0.9 * npdim * slices\n        elif sample == 'rslice':\n            update_interval = 2.0 * slices\n        elif sample == 'hslice':\n            update_interval = 25.0 * slices\n        else:\n            raise ValueError(\"Unknown sampling method: '{0}'\".format(sample))\n    if bound == 'none':\n        update_interval = sys.maxsize  # no need to update with no bounds\n\n    # Keyword arguments controlling the first update.\n    if first_update is None:\n        first_update = dict()\n\n    # Random state.\n    if rstate is None:\n        rstate = np.random\n\n    # Log-likelihood.\n    if logl_args is None:\n        logl_args = []\n    if logl_kwargs is None:\n        logl_kwargs = {}\n\n    # Prior transform.\n    if ptform_args is None:\n        ptform_args = []\n    if ptform_kwargs is None:\n        ptform_kwargs = dict()\n\n    # gradient\n    if grad_args is None:\n        grad_args = []\n    if grad_kwargs is None:\n        grad_kwargs = {}\n\n    # Bounding distribution modifications.\n    if enlarge is not None:\n        kwargs['enlarge'] = enlarge\n    if bootstrap is not None:\n        kwargs['bootstrap'] = bootstrap\n    if vol_dec is not None:\n        kwargs['vol_dec'] = vol_dec\n    if vol_check is not None:\n        kwargs['vol_check'] = vol_check\n\n    # Sampling.\n    if walks is not None:\n        kwargs['walks'] = walks\n    if facc is not None:\n        kwargs['facc'] = facc\n    if slices is not None:\n        kwargs['slices'] = slices\n    if fmove is not None:\n        kwargs['fmove'] = fmove\n    if max_move is not None:\n        kwargs['max_move'] = max_move\n\n    # Set up parallel (or serial) evaluation.\n    if queue_size is not None and queue_size < 1:\n        raise ValueError(\"The queue must contain at least one element!\")\n    elif (queue_size == 1) or (pool is None and queue_size is None):\n        queue_size = 1\n    elif pool is not None:\n        if queue_size is None:\n            try:\n                queue_size = pool.size\n            except:\n                raise ValueError(\"Cannot initialize `queue_size` because \"\n                                 \"`pool.size` has not been provided. Please \"\n                                 \"define `pool.size` or specify `queue_size` \"\n                                 \"explicitly.\")\n    else:\n        raise ValueError(\"`queue_size > 1` but no `pool` provided.\")\n    if use_pool is None:\n        use_pool = dict()\n\n    # Wrap functions.\n    ptform = _function_wrapper(prior_transform, ptform_args, ptform_kwargs,\n                               name='prior_transform')\n    loglike = _function_wrapper(loglikelihood, logl_args, logl_kwargs,\n                                name='loglikelihood')\n\n    # Add in gradient.\n    if gradient is not None:\n        grad = _function_wrapper(gradient, grad_args, grad_kwargs,\n                                 name='gradient')\n        kwargs['grad'] = grad\n        kwargs['compute_jac'] = compute_jac\n\n    # Initialize our nested sampler.\n    sampler = DynamicSampler(loglike, ptform, npdim,\n                             bound, sample, update_interval, first_update,\n                             rstate, queue_size, pool, use_pool, kwargs)\n\n    return sampler"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate an enumeration of the given type.", "response": "def enum(enum_type='enum', base_classes=None, methods=None, **attrs):\n    \"\"\"\n    Generates a enumeration with the given attributes.\n    \"\"\"\n    # Enumerations can not be initalized as a new instance\n    def __init__(instance, *args, **kwargs):\n        raise RuntimeError('%s types can not be initialized.' % enum_type)\n\n    if base_classes is None:\n        base_classes = ()\n\n    if methods is None:\n        methods = {}\n\n    base_classes = base_classes + (object,)\n    for k, v in methods.items():\n        methods[k] = classmethod(v)\n\n    attrs['enums'] = attrs.copy()\n    methods.update(attrs)\n    methods['__init__'] = __init__\n    return type(to_string(enum_type), base_classes, methods)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quick_api(api_key, secret_key, port=8000):\r\n    auth = LinkedInAuthentication(api_key, secret_key, 'http://localhost:8000/',\r\n                                  PERMISSIONS.enums.values())\r\n    app = LinkedInApplication(authentication=auth)\r\n    print auth.authorization_url\r\n    _wait_for_user_to_enter_browser(app, port)\r\n    return app", "response": "This method helps you get access to linkedin api quickly when using it\r\n    from the interpreter"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the Flask application.", "response": "def create_app():\n    \"\"\" Flask application factory \"\"\"\n    \n    # Setup Flask and load app.config\n    app = Flask(__name__)\n    app.config.from_object(__name__+'.ConfigClass')\n\n    # Setup Flask-MongoEngine\n    db = MongoEngine(app)\n\n    # Define the User document.\n    # NB: Make sure to add flask_user UserMixin !!!\n    class User(db.Document, UserMixin):\n        active = db.BooleanField(default=True)\n\n        # User authentication information\n        username = db.StringField(default='')\n        password = db.StringField()\n\n        # User information\n        first_name = db.StringField(default='')\n        last_name = db.StringField(default='')\n\n        # Relationships\n        roles = db.ListField(db.StringField(), default=[])\n\n    # Setup Flask-User and specify the User data-model\n    user_manager = UserManager(app, db, User)\n\n    # The Home page is accessible to anyone\n    @app.route('/')\n    def home_page():\n        # String-based templates\n        return render_template_string(\"\"\"\n            {% extends \"flask_user_layout.html\" %}\n            {% block content %}\n                <h2>Home page</h2>\n                <p><a href={{ url_for('user.register') }}>Register</a></p>\n                <p><a href={{ url_for('user.login') }}>Sign in</a></p>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (accessible to anyone)</p>\n                <p><a href={{ url_for('member_page') }}>Member page</a> (login required)</p>\n                <p><a href={{ url_for('user.logout') }}>Sign out</a></p>\n            {% endblock %}\n            \"\"\")\n\n    # The Members page is only accessible to authenticated users via the @login_required decorator\n    @app.route('/members')\n    @login_required    # User must be authenticated\n    def member_page():\n        # String-based templates\n        return render_template_string(\"\"\"\n            {% extends \"flask_user_layout.html\" %}\n            {% block content %}\n                <h2>Members page</h2>\n                <p><a href={{ url_for('user.register') }}>Register</a></p>\n                <p><a href={{ url_for('user.login') }}>Sign in</a></p>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (accessible to anyone)</p>\n                <p><a href={{ url_for('member_page') }}>Member page</a> (login required)</p>\n                <p><a href={{ url_for('user.logout') }}>Sign out</a></p>\n            {% endblock %}\n            \"\"\")\n\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an email message via Flask - Sendmail.", "response": "def send_email_message(self, recipient, subject, html_message, text_message, sender_email, sender_name):\n        \"\"\" Send email message via Flask-Sendmail.\n\n        Args:\n            recipient: Email address or tuple of (Name, Email-address).\n            subject: Subject line.\n            html_message: The message body in HTML.\n            text_message: The message body in plain text.\n        \"\"\"\n\n        if not current_app.testing:  # pragma: no cover\n\n            # Prepare email message\n            from flask_sendmail import Message\n            message = Message(\n                subject,\n                recipients=[recipient],\n                html=html_message,\n                body=text_message)\n\n            # Send email message\n            self.mail.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassociate a role name with a user.", "response": "def add_user_role(self, user, role_name):\n        \"\"\"Associate a role name with a user.\"\"\"\n\n        # For SQL: user.roles is list of pointers to Role objects\n        if isinstance(self.db_adapter, SQLDbAdapter):\n            # user.roles is a list of Role IDs\n            # Get or add role\n            role = self.db_adapter.find_first_object(self.RoleClass, name=role_name)\n            if not role:\n                role = self.RoleClass(name=role_name)\n                self.db_adapter.add_object(role)\n            user.roles.append(role)\n\n        # For others: user.roles is a list of role names\n        else:\n            # user.roles is a list of role names\n            user.roles.append(role_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_user(self, **kwargs):\n        user = self.UserClass(**kwargs)\n        if hasattr(user, 'active'):\n            user.active = True\n        self.db_adapter.add_object(user)\n        return user", "response": "Add a User object with properties specified in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_user_email(self, user, **kwargs):\n        # If User and UserEmail are separate classes\n        if self.UserEmailClass:\n            user_email = self.UserEmailClass(user=user, **kwargs)\n            self.db_adapter.add_object(user_email)\n\n        # If there is only one User class\n        else:\n            for key, value in kwargs.items():\n                setattr(user, key, value)\n            user_email = user\n\n        return user_email", "response": "Add a UserEmail object with properties specified in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a UserInvitation object to the database.", "response": "def add_user_invitation(self, **kwargs):\n        \"\"\"Add a UserInvitation object, with properties specified in ``**kwargs``.\"\"\"\n        user_invitation = self.UserInvitationClass(**kwargs)\n        self.db_adapter.add_object(user_invitation)\n        return user_invitation"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds a User object by username.", "response": "def find_user_by_username(self, username):\n        \"\"\"Find a User object by username.\"\"\"\n        return self.db_adapter.ifind_first_object(self.UserClass, username=username)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_user_emails(self, user):\n        user_emails = self.db_adapter.find_objects(self.UserEmailClass, user_id=user.id)\n        return user_emails", "response": "Find all the UserEmail objects belonging to a user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the primary UserEmail object.", "response": "def get_primary_user_email_object(self, user):\n        \"\"\"Retrieve the email from User object or the primary UserEmail object (if multiple emails\n        per user are enabled).\"\"\"\n        if self.UserEmailClass:\n            user_email = self.db_adapter.find_first_object(\n                self.UserEmailClass,\n                user_id=user.id,\n                is_primary=True)\n            return user_email\n        else:\n            return user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_user_and_user_email_by_id(self, user_or_user_email_id):\n        if self.UserEmailClass:\n            user_email = self.db_adapter.get_object(self.UserEmailClass, user_or_user_email_id)\n            user = user_email.user if user_email else None\n        else:\n            user = self.db_adapter.get_object(self.UserClass, user_or_user_email_id)\n            user_email = user\n        return (user, user_email)", "response": "Retrieve the User and UserEmail object by ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_and_user_email_by_email(self, email):\n        if self.UserEmailClass:\n            user_email = self.db_adapter.ifind_first_object(self.UserEmailClass, email=email)\n            user = user_email.user if user_email else None\n        else:\n            user = self.db_adapter.ifind_first_object(self.UserClass, email=email)\n            user_email = user\n        return (user, user_email)", "response": "Retrieve the User and UserEmail object by email address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_by_id(self, id):\n        return self.db_adapter.get_object(self.UserClass, id=id)", "response": "Retrieve a User object by ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a UserInvitation object by ID.", "response": "def get_user_invitation_by_id(self, id):\n        \"\"\"Retrieve a UserInvitation object by ID.\"\"\"\n        return self.db_adapter.get_object(self.UserInvitationClass, id=id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a list of user role names.", "response": "def get_user_roles(self, user):\n        \"\"\"Retrieve a list of user role names.\n\n        .. note::\n\n            Database management methods.\n        \"\"\"\n\n        # For SQL: user.roles is list of pointers to Role objects\n        if isinstance(self.db_adapter, SQLDbAdapter):\n            # user.roles is a list of Role IDs\n            user_roles = [role.name for role in user.roles]\n\n\n        # For others: user.roles is a list of role names\n        else:\n            # user.roles is a list of role names\n            user_roles = user.roles\n\n        return user_roles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the User and UserEmail object.", "response": "def save_user_and_user_email(self, user, user_email):\n        \"\"\"Save the User and UserEmail object.\"\"\"\n        if self.UserEmailClass:\n            self.db_adapter.save_object(user_email)\n        self.db_adapter.save_object(user)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if user has a confirmed email.", "response": "def user_has_confirmed_email(self, user):\n        \"\"\"| Return True if user has a confirmed email.\n        | Return False otherwise.\"\"\"\n        if not self.user_manager.USER_ENABLE_EMAIL: return True\n        if not self.user_manager.USER_ENABLE_CONFIRM_EMAIL: return True\n\n        db_adapter = self.db_adapter\n\n        # Handle multiple emails per user: Find at least one confirmed email\n        if self.UserEmailClass:\n            has_confirmed_email = False\n            user_emails = db_adapter.find_objects(self.UserEmailClass, user_id=user.id)\n            for user_email in user_emails:\n                if user_email.email_confirmed_at:\n                    has_confirmed_email = True\n                    break\n\n        # Handle single email per user\n        else:\n            has_confirmed_email = True if user.email_confirmed_at else False\n\n        return has_confirmed_email"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if new_username is still available.", "response": "def username_is_available(self, new_username):\n        \"\"\"Check if ``new_username`` is still available.\n\n        | Returns True if ``new_username`` does not exist or belongs to the current user.\n        | Return False otherwise.\n        \"\"\"\n\n        # Return True if new_username equals current user's username\n        if self.user_manager.call_or_get(current_user.is_authenticated):\n            if new_username == current_user.username:\n                return True\n\n        # Return True if new_username does not exist,\n        # Return False otherwise.\n        return self.find_user_by_username(new_username) == None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the Flask application factory", "response": "def create_app():\n    \"\"\" Flask application factory \"\"\"\n    \n    # Create Flask app load app.config\n    app = Flask(__name__)\n    app.config.from_object(__name__+'.ConfigClass')\n\n    # Initialize Flask-SQLAlchemy\n    db = SQLAlchemy(app)\n\n    # Define the User data-model.\n    # NB: Make sure to add flask_user UserMixin !!!\n    class User(db.Model, UserMixin):\n        __tablename__ = 'users'\n        id = db.Column(db.Integer, primary_key=True)\n        active = db.Column('is_active', db.Boolean(), nullable=False, server_default='1')\n\n        # User authentication information. The collation='NOCASE' is required\n        # to search case insensitively when USER_IFIND_MODE is 'nocase_collation'.\n        username = db.Column(db.String(100, collation='NOCASE'), nullable=False, unique=True)\n        password = db.Column(db.String(255), nullable=False, server_default='')\n        email_confirmed_at = db.Column(db.DateTime())\n\n        # User information\n        first_name = db.Column(db.String(100, collation='NOCASE'), nullable=False, server_default='')\n        last_name = db.Column(db.String(100, collation='NOCASE'), nullable=False, server_default='')\n\n    # Create all database tables\n    db.create_all()\n\n    # Setup Flask-User and specify the User data-model\n    user_manager = UserManager(app, db, User)\n\n    # The Home page is accessible to anyone\n    @app.route('/')\n    def home_page():\n        # String-based templates\n        return render_template_string(\"\"\"\n            {% extends \"flask_user_layout.html\" %}\n            {% block content %}\n                <h2>Home page</h2>\n                <p><a href={{ url_for('user.register') }}>Register</a></p>\n                <p><a href={{ url_for('user.login') }}>Sign in</a></p>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (accessible to anyone)</p>\n                <p><a href={{ url_for('member_page') }}>Member page</a> (login required)</p>\n                <p><a href={{ url_for('user.logout') }}>Sign out</a></p>\n            {% endblock %}\n            \"\"\")\n\n    # The Members page is only accessible to authenticated users via the @login_required decorator\n    @app.route('/members')\n    @login_required    # User must be authenticated\n    def member_page():\n        # String-based templates\n        return render_template_string(\"\"\"\n            {% extends \"flask_user_layout.html\" %}\n            {% block content %}\n                <h2>Members page</h2>\n                <p><a href={{ url_for('user.register') }}>Register</a></p>\n                <p><a href={{ url_for('user.login') }}>Sign in</a></p>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (accessible to anyone)</p>\n                <p><a href={{ url_for('member_page') }}>Member page</a> (login required)</p>\n                <p><a href={{ url_for('user.logout') }}>Sign out</a></p>\n            {% endblock %}\n            \"\"\")\n\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nview function for changing the user s password.", "response": "def change_password_view(self):\n        \"\"\" Prompt for old password and new password and change the user's password.\"\"\"\n\n        # Initialize form\n        form = self.ChangePasswordFormClass(request.form)\n\n        # Process valid POST\n        if request.method == 'POST' and form.validate():\n            # Hash password\n            new_password = form.new_password.data\n            password_hash = self.hash_password(new_password)\n\n            # Update user.password\n            current_user.password = password_hash\n            self.db_manager.save_object(current_user)\n            self.db_manager.commit()\n\n            # Send password_changed email\n            if self.USER_ENABLE_EMAIL and self.USER_SEND_PASSWORD_CHANGED_EMAIL:\n                self.email_manager.send_password_changed_email(current_user)\n\n            # Send changed_password signal\n            signals.user_changed_password.send(current_app._get_current_object(), user=current_user)\n\n            # Flash a system message\n            flash(_('Your password has been changed successfully.'), 'success')\n\n            # Redirect to 'next' URL\n            safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_CHANGE_PASSWORD_ENDPOINT)\n            return redirect(safe_next_url)\n\n        # Render form\n        self.prepare_domain_translations()\n        return render_template(self.USER_CHANGE_PASSWORD_TEMPLATE, form=form)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprompts for new username and old password and change the user s username.", "response": "def change_username_view(self):\n        \"\"\" Prompt for new username and old password and change the user's username.\"\"\"\n\n        # Initialize form\n        form = self.ChangeUsernameFormClass(request.form)\n\n        # Process valid POST\n        if request.method == 'POST' and form.validate():\n\n            # Change username\n            new_username = form.new_username.data\n            current_user.username=new_username\n            self.db_manager.save_object(current_user)\n            self.db_manager.commit()\n\n            # Send username_changed email\n            self.email_manager.send_username_changed_email(current_user)\n\n            # Send changed_username signal\n            signals.user_changed_username.send(current_app._get_current_object(), user=current_user)\n\n            # Flash a system message\n            flash(_(\"Your username has been changed to '%(username)s'.\", username=new_username), 'success')\n\n            # Redirect to 'next' URL\n            safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_CHANGE_USERNAME_ENDPOINT)\n            return redirect(safe_next_url)\n\n        # Render form\n        self.prepare_domain_translations()\n        return render_template(self.USER_CHANGE_USERNAME_TEMPLATE, form=form)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies email confirmation token and activate the user account.", "response": "def confirm_email_view(self, token):\n        \"\"\" Verify email confirmation token and activate the user account.\"\"\"\n        # Verify token\n        data_items = self.token_manager.verify_token(\n            token,\n            self.USER_CONFIRM_EMAIL_EXPIRATION)\n\n        # Retrieve user, user_email by ID\n        user = None\n        user_email = None\n        if data_items:\n            user, user_email = self.db_manager.get_user_and_user_email_by_id(data_items[0])\n\n        if not user or not user_email:\n            flash(_('Invalid confirmation token.'), 'error')\n            return redirect(url_for('user.login'))\n\n        # Set UserEmail.email_confirmed_at\n        user_email.email_confirmed_at=datetime.utcnow()\n        self.db_manager.save_user_and_user_email(user, user_email)\n        self.db_manager.commit()\n\n        # Send confirmed_email signal\n        signals.user_confirmed_email.send(current_app._get_current_object(), user=user)\n\n        # Flash a system message\n        flash(_('Your email has been confirmed.'), 'success')\n\n        # Auto-login after confirm or redirect to login page\n        safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_CONFIRM_ENDPOINT)\n        if self.USER_AUTO_LOGIN_AFTER_CONFIRM:\n            return self._do_login_user(user, safe_next_url)  # auto-login\n        else:\n            return redirect(url_for('user.login') + '?next=' + quote(safe_next_url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef email_action_view(self, id, action):\n\n        # Retrieve UserEmail by id\n        user_email = self.db_manager.get_user_email_by_id(id=id)\n\n        # Users may only change their own UserEmails\n        if not user_email or user_email.user_id != current_user.id:\n            return self.unauthorized_view()\n\n        # Delete UserEmail\n        if action == 'delete':\n            # Primary UserEmail can not be deleted\n            if user_email.is_primary:\n                return self.unauthorized_view()\n            # Delete UserEmail\n            self.db_manager.delete_object(user_email)\n            self.db_manager.commit()\n\n        # Set UserEmail.is_primary\n        elif action == 'make-primary':\n            # Disable previously primary emails\n            user_emails = self.db_manager.find_user_emails(current_user)\n            for other_user_email in user_emails:\n                if other_user_email.is_primary:\n                    other_user_email.is_primary=False\n                    self.db_manager.save_object(other_user_email)\n            # Enable current primary email\n            user_email.is_primary=True\n            self.db_manager.save_object(user_email)\n            self.db_manager.commit()\n\n        # Send confirm email\n        elif action == 'confirm':\n            self._send_confirm_email_email(user_email.user, user_email)\n        else:\n            return self.unauthorized_view()\n\n        return redirect(url_for('user.manage_emails'))", "response": "Perform action on UserEmail object id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forgot_password_view(self):\n\n        # Initialize form\n        form = self.ForgotPasswordFormClass(request.form)\n\n        # Process valid POST\n        if request.method == 'POST' and form.validate():\n            # Get User and UserEmail by email\n            email = form.email.data\n            user, user_email = self.db_manager.get_user_and_user_email_by_email(email)\n\n            if user and user_email:\n                # Send reset_password email\n                self.email_manager.send_reset_password_email(user, user_email)\n\n                # Send forgot_password signal\n                signals.user_forgot_password.send(current_app._get_current_object(), user=user)\n\n            # Flash a system message\n            flash(_(\n                \"A reset password email has been sent to '%(email)s'. Open that email and follow the instructions to reset your password.\",\n                email=email), 'success')\n\n            # Redirect to the login page\n            return redirect(self._endpoint_url(self.USER_AFTER_FORGOT_PASSWORD_ENDPOINT))\n\n        # Render form\n        self.prepare_domain_translations()\n        return render_template(self.USER_FORGOT_PASSWORD_TEMPLATE, form=form)", "response": "Prompt for email and send reset password email."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nallowing users to send invitations to register an account", "response": "def invite_user_view(self):\n        \"\"\" Allows users to send invitations to register an account \"\"\"\n\n        invite_user_form = self.InviteUserFormClass(request.form)\n\n        if request.method == 'POST' and invite_user_form.validate():\n            # Find User and UserEmail by email\n            email = invite_user_form.email.data\n            user, user_email = self.db_manager.get_user_and_user_email_by_email(email)\n            if user:\n                flash(\"User with that email has already registered\", \"error\")\n                return redirect(url_for('user.invite_user'))\n\n            # Add UserInvitation\n            user_invitation = self.db_manager.add_user_invitation(\n                email=email,\n                invited_by_user_id=current_user.id)\n            self.db_manager.commit()\n\n            try:\n                # Send invite_user email\n                self.email_manager.send_invite_user_email(current_user, user_invitation)\n            except Exception as e:\n                # delete new UserInvitation object if send fails\n                self.db_manager.delete_object(user_invitation)\n                self.db_manager.commit()\n                raise\n\n            # Send sent_invitation signal\n            signals \\\n                .user_sent_invitation \\\n                .send(current_app._get_current_object(), user_invitation=user_invitation,\n                      form=invite_user_form)\n\n            # Flash a system message\n            flash(_('Invitation has been sent.'), 'success')\n\n            # Redirect\n            safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_INVITE_ENDPOINT)\n            return redirect(safe_next_url)\n\n        self.prepare_domain_translations()\n        return render_template(self.USER_INVITE_USER_TEMPLATE, form=invite_user_form)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef login_view(self):\n\n        # Authenticate username/email and login authenticated users.\n\n        safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_LOGIN_ENDPOINT)\n        safe_reg_next = self._get_safe_next_url('reg_next', self.USER_AFTER_REGISTER_ENDPOINT)\n\n        # Immediately redirect already logged in users\n        if self.call_or_get(current_user.is_authenticated) and self.USER_AUTO_LOGIN_AT_LOGIN:\n            return redirect(safe_next_url)\n\n        # Initialize form\n        login_form = self.LoginFormClass(request.form)  # for login.html\n        register_form = self.RegisterFormClass()  # for login_or_register.html\n        if request.method != 'POST':\n            login_form.next.data = register_form.next.data = safe_next_url\n            login_form.reg_next.data = register_form.reg_next.data = safe_reg_next\n\n        # Process valid POST\n        if request.method == 'POST' and login_form.validate():\n            # Retrieve User\n            user = None\n            user_email = None\n            if self.USER_ENABLE_USERNAME:\n                # Find user record by username\n                user = self.db_manager.find_user_by_username(login_form.username.data)\n\n                # Find user record by email (with form.username)\n                if not user and self.USER_ENABLE_EMAIL:\n                    user, user_email = self.db_manager.get_user_and_user_email_by_email(login_form.username.data)\n            else:\n                # Find user by email (with form.email)\n                user, user_email = self.db_manager.get_user_and_user_email_by_email(login_form.email.data)\n\n            if user:\n                # Log user in\n                safe_next_url = self.make_safe_url(login_form.next.data)\n                return self._do_login_user(user, safe_next_url, login_form.remember_me.data)\n\n        # Render form\n        self.prepare_domain_translations()\n        template_filename = self.USER_LOGIN_AUTH0_TEMPLATE if self.USER_ENABLE_AUTH0 else self.USER_LOGIN_TEMPLATE\n        return render_template(template_filename,\n                      form=login_form,\n                      login_form=login_form,\n                      register_form=register_form)", "response": "Prepare and process the login form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logout_view(self):\n        \"\"\" Sign the user out.\"\"\"\n\n        # Send user_logged_out signal\n        signals.user_logged_out.send(current_app._get_current_object(), user=current_user)\n\n        # Use Flask-Login to sign out user\n        logout_user()\n\n        # Flash a system message\n        flash(_('You have signed out successfully.'), 'success')\n\n        # Redirect to logout_next endpoint or '/'\n        safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_LOGOUT_ENDPOINT)\n        return redirect(safe_next_url)", "response": "Process the logout link."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_view(self):\n\n        safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_LOGIN_ENDPOINT)\n        safe_reg_next_url = self._get_safe_next_url('reg_next', self.USER_AFTER_REGISTER_ENDPOINT)\n\n        # Initialize form\n        login_form = self.LoginFormClass()  # for login_or_register.html\n        register_form = self.RegisterFormClass(request.form)  # for register.html\n\n        # invite token used to determine validity of registeree\n        invite_token = request.values.get(\"token\")\n\n        # require invite without a token should disallow the user from registering\n        if self.USER_REQUIRE_INVITATION and not invite_token:\n            flash(\"Registration is invite only\", \"error\")\n            return redirect(url_for('user.login'))\n\n        user_invitation = None\n        if invite_token and self.db_manager.UserInvitationClass:\n            data_items = self.token_manager.verify_token(invite_token, self.USER_INVITE_EXPIRATION)\n            if data_items:\n                user_invitation_id = data_items[0]\n                user_invitation = self.db_manager.get_user_invitation_by_id(user_invitation_id)\n\n            if not user_invitation:\n                flash(\"Invalid invitation token\", \"error\")\n                return redirect(url_for('user.login'))\n\n            register_form.invite_token.data = invite_token\n\n        if request.method != 'POST':\n            login_form.next.data = register_form.next.data = safe_next_url\n            login_form.reg_next.data = register_form.reg_next.data = safe_reg_next_url\n            if user_invitation:\n                register_form.email.data = user_invitation.email\n\n        # Process valid POST\n        if request.method == 'POST' and register_form.validate():\n            user = self.db_manager.add_user()\n            register_form.populate_obj(user)\n            user_email = self.db_manager.add_user_email(user=user, is_primary=True)\n            register_form.populate_obj(user_email)\n\n            # Store password hash instead of password\n            user.password = self.hash_password(user.password)\n\n            # Email confirmation depends on the USER_ENABLE_CONFIRM_EMAIL setting\n            request_email_confirmation = self.USER_ENABLE_CONFIRM_EMAIL\n            # Users that register through an invitation, can skip this process\n            # but only when they register with an email that matches their invitation.\n            if user_invitation:\n                if user_invitation.email.lower() == register_form.email.data.lower():\n                    user_email.email_confirmed_at=datetime.utcnow()\n                    request_email_confirmation = False\n\n            self.db_manager.save_user_and_user_email(user, user_email)\n            self.db_manager.commit()\n\n            # Send 'registered' email and delete new User object if send fails\n            if self.USER_SEND_REGISTERED_EMAIL:\n                try:\n                    # Send 'confirm email' or 'registered' email\n                    self._send_registered_email(user, user_email, request_email_confirmation)\n                except Exception as e:\n                    # delete new User object if send  fails\n                    self.db_manager.delete_object(user)\n                    self.db_manager.commit()\n                    raise\n\n            # Send user_registered signal\n            signals.user_registered.send(current_app._get_current_object(),\n                                         user=user,\n                                         user_invitation=user_invitation)\n\n            # Redirect if USER_ENABLE_CONFIRM_EMAIL is set\n            if self.USER_ENABLE_CONFIRM_EMAIL and request_email_confirmation:\n                safe_reg_next_url = self.make_safe_url(register_form.reg_next.data)\n                return redirect(safe_reg_next_url)\n\n            # Auto-login after register or redirect to login page\n            if 'reg_next' in request.args:\n                safe_reg_next_url = self.make_safe_url(register_form.reg_next.data)\n            else:\n                safe_reg_next_url = self._endpoint_url(self.USER_AFTER_CONFIRM_ENDPOINT)\n            if self.USER_AUTO_LOGIN_AFTER_REGISTER:\n                return self._do_login_user(user, safe_reg_next_url)  # auto-login\n            else:\n                return redirect(url_for('user.login') + '?next=' + quote(safe_reg_next_url))  # redirect to login page\n\n        # Render form\n        self.prepare_domain_translations()\n        return render_template(self.USER_REGISTER_TEMPLATE,\n                      form=register_form,\n                      login_form=login_form,\n                      register_form=register_form)", "response": "Display registration form and create new User."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprompting for email and re - send email conformation email.", "response": "def resend_email_confirmation_view(self):\n        \"\"\"Prompt for email and re-send email conformation email.\"\"\"\n\n        # Initialize form\n        form = self.ResendEmailConfirmationFormClass(request.form)\n\n        # Process valid POST\n        if request.method == 'POST' and form.validate():\n\n            # Find user by email\n            email = form.email.data\n            user, user_email = self.db_manager.get_user_and_user_email_by_email(email)\n\n            # Send confirm_email email\n            if user:\n                self._send_confirm_email_email(user, user_email)\n\n            # Redirect to the login page\n            return redirect(self._endpoint_url(self.USER_AFTER_RESEND_EMAIL_CONFIRMATION_ENDPOINT))\n\n        # Render form\n        self.prepare_domain_translations()\n        return render_template(self.USER_RESEND_CONFIRM_EMAIL_TEMPLATE, form=form)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nview function for resetting password.", "response": "def reset_password_view(self, token):\n        \"\"\" Verify the password reset token, Prompt for new password, and set the user's password.\"\"\"\n        # Verify token\n\n        if self.call_or_get(current_user.is_authenticated):\n            logout_user()\n\n        data_items = self.token_manager.verify_token(\n            token,\n            self.USER_RESET_PASSWORD_EXPIRATION)\n\n        user = None\n        if data_items:\n            # Get User by user ID\n            user_id = data_items[0]\n            user = self.db_manager.get_user_by_id(user_id)\n\n            # Mark email as confirmed\n            user_or_user_email_object = self.db_manager.get_primary_user_email_object(user)\n            user_or_user_email_object.email_confirmed_at = datetime.utcnow()\n            self.db_manager.save_object(user_or_user_email_object)\n            self.db_manager.commit()\n\n        if not user:\n            flash(_('Your reset password token is invalid.'), 'error')\n            return redirect(self._endpoint_url('user.login'))\n\n\n        # Initialize form\n        form = self.ResetPasswordFormClass(request.form)\n\n        # Process valid POST\n        if request.method == 'POST' and form.validate():\n            # Change password\n            password_hash = self.hash_password(form.new_password.data)\n            user.password=password_hash\n            self.db_manager.save_object(user)\n            self.db_manager.commit()\n\n            # Send 'password_changed' email\n            if self.USER_ENABLE_EMAIL and self.USER_SEND_PASSWORD_CHANGED_EMAIL:\n                self.email_manager.send_password_changed_email(user)\n\n            # Send reset_password signal\n            signals.user_reset_password.send(current_app._get_current_object(), user=user)\n\n            # Flash a system message\n            flash(_(\"Your password has been reset successfully.\"), 'success')\n\n            # Auto-login after reset password or redirect to login page\n            safe_next_url = self._get_safe_next_url('next', self.USER_AFTER_RESET_PASSWORD_ENDPOINT)\n            if self.USER_AUTO_LOGIN_AFTER_RESET_PASSWORD:\n                return self._do_login_user(user, safe_next_url)  # auto-login\n            else:\n                return redirect(url_for('user.login') + '?next=' + quote(safe_next_url))  # redirect to login page\n\n        # Render form\n        self.prepare_domain_translations()\n        return render_template(self.USER_RESET_PASSWORD_TEMPLATE, form=form)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unauthenticated_view(self):\n        # Prepare Flash message\n        url = request.url\n        flash(_(\"You must be signed in to access '%(url)s'.\", url=url), 'error')\n\n        # Redirect to USER_UNAUTHENTICATED_ENDPOINT\n        safe_next_url = self.make_safe_url(url)\n        return redirect(self._endpoint_url(self.USER_UNAUTHENTICATED_ENDPOINT)+'?next='+quote(safe_next_url))", "response": "Prepare a Flash message and redirect to USER_UNAUTHENTICATED_ENDPOINT"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares a Flash message and redirect to USER_UNAUTHORIZED_ENDPOINT", "response": "def unauthorized_view(self):\n        \"\"\" Prepare a Flash message and redirect to USER_UNAUTHORIZED_ENDPOINT\"\"\"\n        # Prepare Flash message\n        url = request.script_root + request.path\n        flash(_(\"You do not have permission to access '%(url)s'.\", url=url), 'error')\n\n        # Redirect to USER_UNAUTHORIZED_ENDPOINT\n        return redirect(self._endpoint_url(self.USER_UNAUTHORIZED_ENDPOINT))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_logged_in_with_confirmed_email(user_manager):\n    # User must be logged in\n    if user_manager.call_or_get(current_user.is_authenticated):\n        # Is unconfirmed email allowed for this view by @allow_unconfirmed_email?\n        unconfirmed_email_allowed = \\\n            getattr(g, '_flask_user_allow_unconfirmed_email', False)\n        \n        # unconfirmed_email_allowed must be True or\n        # User must have at least one confirmed email address\n        if unconfirmed_email_allowed or user_manager.db_manager.user_has_confirmed_email(current_user):\n            return True\n\n    return False", "response": "Returns True if user is logged in and has a confirmed email address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef login_required(view_function):\n    @wraps(view_function)    # Tells debuggers that is is a function wrapper\n    def decorator(*args, **kwargs):\n        user_manager = current_app.user_manager\n        \n        # User must be logged in with a confirmed email address\n        allowed = _is_logged_in_with_confirmed_email(user_manager)\n        if not allowed:\n            # Redirect to unauthenticated page\n            return user_manager.unauthenticated_view()\n\n        # It's OK to call the view\n        return view_function(*args, **kwargs)\n\n    return decorator", "response": "Decorator that ensures that the current user is logged in with a confirmed email address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef allow_unconfirmed_email(view_function):\n    @wraps(view_function)    # Tells debuggers that is is a function wrapper\n    def decorator(*args, **kwargs):\n        # Sets a boolean on the global request context\n        g._flask_user_allow_unconfirmed_email = True\n\n        # Catch exceptions to properly unset boolean on exceptions\n        try:\n            user_manager = current_app.user_manager\n\n            # User must be logged in with a confirmed email address\n            allowed = _is_logged_in_with_confirmed_email(user_manager)\n            if not allowed:\n                # Redirect to unauthenticated page\n                return user_manager.unauthenticated_view()\n\n            # It's OK to call the view\n            return view_function(*args, **kwargs)\n\n        finally:\n            # Allways unset the boolean, whether exceptions occurred or not\n            g._flask_user_allow_unconfirmed_email = False\n\n    return decorator", "response": "Decorator that allows the user to access the unconfirmed email address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_token(self, *args):\n        concatenated_str = self.encode_data_items(*args)\n        token = self.encrypt_string(concatenated_str)\n        return token", "response": "Generates a token for a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify token signature and decrypt token.", "response": "def verify_token(self, token, expiration_in_seconds=None):\n        \"\"\" Verify token signature, verify token expiration, and decrypt token.\n\n        | Returns None if token is expired or invalid.\n        | Returns a list of strings and integers on success.\n\n        Implemented as::\n\n            concatenated_str = self.decrypt_string(token, expiration_in_seconds)\n            data_items = self.decode_data_items(concatenated_str)\n            return data_items\n\n        Example:\n\n        ::\n\n            # Verify that a User with ``user_id`` has a password that ends in ``password_ends_with``.\n            token_is_valid = False\n            data_items = token_manager.verify(token, expiration_in_seconds)\n            if data_items:\n                user_id = data_items[0]\n                password_ends_with = data_items[1]\n                user = user_manager.db_manager.get_user_by_id(user_id)\n                token_is_valid = user and user.password[-8:]==password_ends_with\n        \"\"\"\n\n        from cryptography.fernet import InvalidToken\n\n        try:\n            concatenated_str = self.decrypt_string(token, expiration_in_seconds)\n            data_items = self.decode_data_items(concatenated_str)\n        except InvalidToken:\n            data_items = None\n\n        return data_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencrypts a string into a token using cryptography. fernet. Fernet", "response": "def encrypt_string(self, concatenated_str):\n        \"\"\"Timestamp, sign and encrypt a string into a token using ``cryptography.fernet.Fernet()``.\"\"\"\n\n        # Convert string to bytes\n        concatenated_bytes = concatenated_str.encode()\n\n        # Encrypt, timestamp, sign, and base64-encode\n        encrypted_bytes = self.fernet.encrypt(concatenated_bytes)\n\n        # Convert bytes to string\n        encrypted_str = encrypted_bytes.decode('utf-8')\n\n        # Remove '=' padding if needed\n        token_str = encrypted_str.strip('=')\n        return token_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decrypt_string(self, token_str, expiration_in_seconds=None):\n\n        # Add '=' padding if needed\n        if len(token_str) % 4:\n            token_str += '=' * (4 - len(token_str) % 4)\n\n        # Convert string to bytes\n        encrypted_bytes = token_str.encode()\n\n        # Verify signature, verify expiration, and decrypt using ``cryptography.fernet.Fernet()``\n        concatenated_bytes = self.fernet.decrypt(encrypted_bytes, expiration_in_seconds)\n        concatenated_str = concatenated_bytes.decode('utf-8')\n\n        return concatenated_str", "response": "Verify signature verify timestamp and decrypt a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes a list of integers and strings into a concatenated string.", "response": "def encode_data_items(self, *args):\n        \"\"\" Encodes a list of integers and strings into a concatenated string.\n\n        - encode string items as-is.\n        - encode integer items as base-64 with a ``'~'`` prefix.\n        - concatenate encoded items with a ``'|'`` separator.\n\n        Example:\n            ``encode_data_items('abc', 123, 'xyz')`` returns ``'abc|~B7|xyz'``\n        \"\"\"\n        str_list = []\n        for arg in args:\n\n            # encode string items as-is\n            if isinstance(arg, str):\n                arg_str = arg\n\n            # encode integer items as base-64 strings with a '~' character in front\n            elif isinstance(arg, int):\n                arg_str = self.INTEGER_PREFIX + self.encode_int(arg)\n\n            # convert other types to string\n            else:\n                arg_str = str(arg)\n\n            str_list.append(arg_str)\n\n        # Concatenate strings with '|' separators\n        concatenated_str = self.SEPARATOR.join(str_list)\n\n        return concatenated_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_data_items(self, concatenated_str):\n\n        data_items = []\n        str_list = concatenated_str.split(self.SEPARATOR)\n        for str in str_list:\n\n            # '~base-64-strings' are decoded into integers.\n            if len(str)>=1 and str[0]==self.INTEGER_PREFIX:\n                item = self.decode_int(str[1:])\n\n            # Strings are decoded as-is.\n            else:\n                item = str\n\n            data_items.append(item)\n\n        # Return list of data items\n        return data_items", "response": "Decodes a concatenated string into a list of integers and strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_int(self, n):\n        str = []\n        while True:\n            n, r = divmod(n, self.BASE)\n            str.append(self.ALPHABET[r])\n            if n == 0: break\n        return ''.join(reversed(str))", "response": "Encodes an integer into a short Base64 string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_int(self, str):\n        n = 0\n        for c in str:\n            n = n * self.BASE + self.ALPHABET_REVERSE[c]\n        return n", "response": "Decodes a short Base64 string into an integer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending the email confirmation email to the user.", "response": "def send_confirm_email_email(self, user, user_email):\n        \"\"\"Send the 'email confirmation' email.\"\"\"\n        \n        # Verify config settings\n        if not self.user_manager.USER_ENABLE_EMAIL: return\n        if not self.user_manager.USER_ENABLE_CONFIRM_EMAIL: return\n\n        # The confirm_email email is sent to a specific user_email.email or user.email\n        email = user_email.email if user_email else user.email\n\n        # Generate a confirm_email_link\n        object_id = user_email.id if user_email else user.id\n        token = self.user_manager.generate_token(object_id)\n        confirm_email_link = url_for('user.confirm_email', token=token, _external=True)\n\n        # Render email from templates and send it via the configured EmailAdapter\n        self._render_and_send_email(\n            email,\n            user,\n            self.user_manager.USER_CONFIRM_EMAIL_TEMPLATE,\n            confirm_email_link=confirm_email_link,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_password_changed_email(self, user):\n\n        # Verify config settings\n        if not self.user_manager.USER_ENABLE_EMAIL: return\n        if not self.user_manager.USER_SEND_PASSWORD_CHANGED_EMAIL: return\n\n        # Notification emails are sent to the user's primary email address\n        user_or_user_email_object = self.user_manager.db_manager.get_primary_user_email_object(user)\n        email = user_or_user_email_object.email\n\n        # Render email from templates and send it via the configured EmailAdapter\n        self._render_and_send_email(\n            email,\n            user,\n            self.user_manager.USER_PASSWORD_CHANGED_EMAIL_TEMPLATE,\n        )", "response": "Send the password has changed notification email."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending the reset password email to the user.", "response": "def send_reset_password_email(self, user, user_email):\n        \"\"\"Send the 'reset password' email.\"\"\"\n\n        # Verify config settings\n        if not self.user_manager.USER_ENABLE_EMAIL: return\n        assert self.user_manager.USER_ENABLE_FORGOT_PASSWORD\n\n        # The reset_password email is sent to a specific user_email.email or user.email\n        email = user_email.email if user_email else user.email\n\n        # Generate a reset_password_link\n        token = self.user_manager.generate_token(user.id)\n        reset_password_link = url_for('user.reset_password', token=token, _external=True)\n\n        # Render email from templates and send it via the configured EmailAdapter\n        self._render_and_send_email(\n            email,\n            user,\n            self.user_manager.USER_RESET_PASSWORD_EMAIL_TEMPLATE,\n            reset_password_link=reset_password_link,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_invite_user_email(self, user, user_invitation):\n\n        # Verify config settings\n        if not self.user_manager.USER_ENABLE_EMAIL: return\n        if not self.user_manager.USER_ENABLE_INVITE_USER: return\n\n        # The user param points to the inviter\n        # The user_invitation param points to the invitee\n        invited_by_user = user\n\n        # Use the invitation email\n        email = user_invitation.email\n\n        # Create a dummy user object to an empty name for the invitee\n        user = self.user_manager.db_manager.UserClass(email=email)\n\n        # Generate a accept_invitation_link\n        token = self.user_manager.generate_token(user_invitation.id)\n        accept_invitation_link = url_for('user.register', token=token, _external=True)\n\n        # Render email from templates and send it via the configured EmailAdapter\n        self._render_and_send_email(\n            email,\n            user,\n            self.user_manager.USER_INVITE_USER_EMAIL_TEMPLATE,\n            accept_invitation_link=accept_invitation_link,\n            invited_by_user=invited_by_user,\n        )", "response": "Send the user invitation email via EmailAdapter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_registered_email(self, user, user_email, request_email_confirmation):\n\n        # Verify config settings\n        if not self.user_manager.USER_ENABLE_EMAIL: return\n        if not self.user_manager.USER_SEND_REGISTERED_EMAIL: return\n\n        # The registered email is sent to a specific user_email.email or user.email\n        email = user_email.email if user_email else user.email\n\n        # Add a request to confirm email if needed\n        if request_email_confirmation:\n            # Generate a confirm_email_link\n            token = self.user_manager.generate_token(user_email.id if user_email else user.id)\n            confirm_email_link = url_for('user.confirm_email', token=token, _external=True)\n        else:\n            confirm_email_link = None\n\n        # Render email from templates and send it via the configured EmailAdapter\n        self._render_and_send_email(\n            email,\n            user,\n            self.user_manager.USER_REGISTERED_EMAIL_TEMPLATE,\n            confirm_email_link=confirm_email_link,\n        )", "response": "Send the user has registered notification email."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends the username has changed notification email.", "response": "def send_username_changed_email(self, user):\n        \"\"\"Send the 'username has changed' notification email.\"\"\"\n\n        # Verify config settings\n        if not self.user_manager.USER_ENABLE_EMAIL: return\n        if not self.user_manager.USER_SEND_USERNAME_CHANGED_EMAIL: return\n\n        # Notification emails are sent to the user's primary email address\n        user_or_user_email_object = self.user_manager.db_manager.get_primary_user_email_object(user)\n        email = user_or_user_email_object.email\n\n        # Render email from templates and send it via the configured EmailAdapter\n        self._render_and_send_email(\n            email,\n            user,\n            self.user_manager.USER_USERNAME_CHANGED_EMAIL_TEMPLATE,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the Flask application and return it", "response": "def create_app():\n    \"\"\" Flask application factory \"\"\"\n\n    # Setup Flask app and app.config\n    app = Flask(__name__)\n    app.config.from_object(__name__ + '.ConfigClass')\n\n    # Initialize Flask extensions\n    db = Flywheel(app)  # Initialize Flask-Flywheel\n    mail = Mail(app)  # Initialize Flask-Mail\n\n    # Define the User data model. Make sure to add flask_user UserMixin !!!\n    class User(Model, UserMixin):\n        __metadata__ = {\n            \"_name\": \"users\",\n            'throughput': {\n                'read': 1,\n                'write': 1,\n            },\n            'global_indexes': [\n                GlobalIndex.all('email-username', 'username').throughput(read=1, write=1),\n                GlobalIndex.all('email-index', 'email').throughput(read=1, write=1)\n            ],\n        }\n\n        id = Field(hash_key=True)\n        active = Field(data_type=bool)\n\n        # User authentication information\n        username = Field()\n        password = Field()\n\n        # User email information\n        email = Field()\n        email_confirmed_at = Field(data_type=datetime)\n\n        # User information\n        first_name = Field()\n        last_name = Field()\n\n        def get_id(self):\n            if self.id is None:\n                self.id = str(uuid.uuid1())\n            return self.id\n\n    # Setup Flask-User\n    user_manager = UserManager(app, db, User)\n\n    # Create all database tables\n    db.engine.register(User)\n    print('create_schema()')\n    db.engine.create_schema()\n    print('created_schema()')\n\n    # The Home page is accessible to anyone\n    @app.route('/')\n    def home_page():\n        return render_template_string(\"\"\"\n            {% extends \"base.html\" %}\n            {% block content %}\n                <h2>Home page</h2>\n                <p>This page can be accessed by anyone.</p><br/>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (anyone)</p>\n                <p><a href={{ url_for('members_page') }}>Members page</a> (login required)</p>\n            {% endblock %}\n            \"\"\")\n\n    # The Members page is only accessible to authenticated users\n    @app.route('/members')\n    @login_required  # Use of @login_required decorator\n    def members_page():\n        return render_template_string(\"\"\"\n            {% extends \"base.html\" %}\n            {% block content %}\n                <h2>Members page</h2>\n                <p>This page can only be accessed by authenticated users.</p><br/>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (anyone)</p>\n                <p><a href={{ url_for('members_page') }}>Members page</a> (login required)</p>\n            {% endblock %}\n            \"\"\")\n\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_id(self):\n\n        # This function is used by Flask-Login to store a User ID securely as a browser cookie.\n        # The last part of the password is included to invalidate tokens when password change.\n        # user_id and password_ends_with are encrypted, timestamped and signed.\n        # This function works in tandem with UserMixin.get_user_by_token()\n        user_manager = current_app.user_manager\n\n        user_id = self.id\n        password_ends_with = '' if user_manager.USER_ENABLE_AUTH0 else self.password[-8:]\n        user_token = user_manager.generate_token(\n            user_id,               # User ID\n            password_ends_with,    # Last 8 characters of user password\n        )\n        # print(\"UserMixin.get_id: ID:\", self.id, \"token:\", user_token)\n        return user_token", "response": "Converts a User ID and parts of a User password hash to a token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the user has all of the specified roles.", "response": "def has_roles(self, *requirements):\n        \"\"\" Return True if the user has all of the specified roles. Return False otherwise.\n\n            has_roles() accepts a list of requirements:\n                has_role(requirement1, requirement2, requirement3).\n\n            Each requirement is either a role_name, or a tuple_of_role_names.\n                role_name example:   'manager'\n                tuple_of_role_names: ('funny', 'witty', 'hilarious')\n            A role_name-requirement is accepted when the user has this role.\n            A tuple_of_role_names-requirement is accepted when the user has ONE of these roles.\n            has_roles() returns true if ALL of the requirements have been accepted.\n\n            For example:\n                has_roles('a', ('b', 'c'), d)\n            Translates to:\n                User has role 'a' AND (role 'b' OR role 'c') AND role 'd'\"\"\"\n\n        # Translates a list of role objects to a list of role_names\n        user_manager = current_app.user_manager\n        role_names = user_manager.db_manager.get_user_roles(self)\n\n        # has_role() accepts a list of requirements\n        for requirement in requirements:\n            if isinstance(requirement, (list, tuple)):\n                # this is a tuple_of_role_names requirement\n                tuple_of_role_names = requirement\n                authorized = False\n                for role_name in tuple_of_role_names:\n                    if role_name in role_names:\n                        # tuple_of_role_names requirement was met: break out of loop\n                        authorized = True\n                        break\n                if not authorized:\n                    return False                    # tuple_of_role_names requirement failed: return False\n            else:\n                # this is a role_name requirement\n                role_name = requirement\n                # the user must have this role\n                if not role_name in role_names:\n                    return False                    # role_name requirement failed: return False\n\n        # All requirements have been met: return True\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_app():\n    \n    # Setup Flask app and app.config\n    app = Flask(__name__)\n    app.config.from_object(__name__+'.ConfigClass')\n\n    # Initialize Flask extensions\n    db = SQLAlchemy(app)                            # Initialize Flask-SQLAlchemy\n\n    # Define the User data-model. Make sure to add flask_user UserMixin !!!\n    class User(db.Model, UserMixin):\n        __tablename__ = 'users'\n        id = db.Column(db.Integer, primary_key=True)\n\n        # User authentication information\n        username = db.Column(db.String(50, collation='NOCASE'), nullable=False, unique=True)\n        password = db.Column(db.String(255), nullable=False, server_default='')\n\n        # User information\n        active = db.Column('is_active', db.Boolean(), nullable=False, server_default='0')\n        first_name = db.Column(db.String(100, collation='NOCASE'), nullable=False, server_default='')\n        last_name = db.Column(db.String(100, collation='NOCASE'), nullable=False, server_default='')\n\n        # Relationship\n        user_emails = db.relationship('UserEmail')\n\n\n    # Define UserEmail DataModel.\n    class UserEmail(db.Model):\n        __tablename__ = 'user_emails'\n        id = db.Column(db.Integer, primary_key=True)\n        user_id = db.Column(db.Integer, db.ForeignKey('users.id'))\n\n        # User email information\n        email = db.Column(db.String(255, collation='NOCASE'), nullable=False, unique=True)\n        email_confirmed_at = db.Column(db.DateTime())\n        is_primary = db.Column(db.Boolean(), nullable=False, default=False)\n\n        # Relationship\n        user = db.relationship('User', uselist=False)\n\n\n    # Create all database tables\n    db.create_all()\n\n    # Setup Flask-User\n    db_adapter = SQLAlchemyAdapter(db, User, UserEmailClass=UserEmail)        # Register the User data-model\n    user_manager = UserManager(db_adapter, app)     # Initialize Flask-User\n\n    # The Home page is accessible to anyone\n    @app.route('/')\n    def home_page():\n        return render_template_string(\"\"\"\n            {% extends \"flask_user_layout.html\" %}\n            {% block content %}\n                <h2>Home page</h2>\n                <p>This page can be accessed by anyone.</p><br/>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (anyone)</p>\n                <p><a href={{ url_for('member_page') }}>Members page</a> (login required)</p>\n            {% endblock %}\n            \"\"\")\n\n    # The Members page is only accessible to authenticated users\n    @app.route('/members')\n    @login_required                                 # Use of @login_required decorator\n    def member_page():\n        return render_template_string(\"\"\"\n            {% extends \"flask_user_layout.html\" %}\n            {% block content %}\n                <h2>Members page</h2>\n                <p>This page can only be accessed by authenticated users.</p><br/>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (anyone)</p>\n                <p><a href={{ url_for('member_page') }}>Members page</a> (login required)</p>\n            {% endblock %}\n            \"\"\")\n\n    return app", "response": "Create the Flask application factory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a new email is available in the current user s cache.", "response": "def email_is_available(self, new_email):\n        \"\"\"Check if ``new_email`` is available.\n\n        | Returns True if ``new_email`` does not exist or belongs to the current user.\n        | Return False otherwise.\n        \"\"\"\n\n        user, user_email = self.db_manager.get_user_and_user_email_by_email(new_email)\n        return (user == None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_safe_url(self, url):\n\n        # Split the URL into scheme, netloc, path, query and fragment\n        parts = list(urlsplit(url))\n\n        # Clear scheme and netloc and rebuild URL\n        parts[0] = ''   # Empty scheme\n        parts[1] = ''   # Empty netloc (hostname:port)\n        safe_url = urlunsplit(parts)\n        return safe_url", "response": "Makes a URL safe by removing optional hostname and port."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef password_validator(self, form, field):\n\n        # Convert string to list of characters\n        password = list(field.data)\n        password_length = len(password)\n\n        # Count lowercase, uppercase and numbers\n        lowers = uppers = digits = 0\n        for ch in password:\n            if ch.islower(): lowers += 1\n            if ch.isupper(): uppers += 1\n            if ch.isdigit(): digits += 1\n\n        # Password must have one lowercase letter, one uppercase letter and one digit\n        is_valid = password_length >= 6 and lowers and uppers and digits\n        if not is_valid:\n            raise ValidationError(\n                _('Password must have at least 6 characters with one lowercase letter, one uppercase letter and one number'))", "response": "Ensure that the password is at least 6 characters with one lowercase letter one uppercase letter and one number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef username_validator(self, form, field):\n        username = field.data\n        if len(username) < 3:\n            raise ValidationError(\n                _('Username must be at least 3 characters long'))\n        valid_chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-._'\n        chars = list(username)\n        for char in chars:\n            if char not in valid_chars:\n                raise ValidationError(\n                    _(\"Username may only contain letters, numbers, '-', '.' and '_'\"))", "response": "Ensure that the username field contains at least 3 alphanumeric characters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify required settings. Produce a helpful error messages for incorrect settings.", "response": "def _check_settings(self, app):\n        \"\"\"Verify required settings. Produce a helpful error messages for incorrect settings.\"\"\"\n\n        # Check for invalid settings\n        # --------------------------\n\n        # Check self.UserInvitationClass and USER_ENABLE_INVITE_USER\n        if self.USER_ENABLE_INVITE_USER and not self.UserInvitationClass:\n            raise ConfigError(\n                'UserInvitationClass is missing while USER_ENABLE_INVITE_USER is True.' \\\n                ' Specify UserInvitationClass with UserManager(app, db, User, UserInvitationClass=...' \\\n                ' or set USER_ENABLE_INVITE_USER=False.')\n\n        # Check for deprecated settings\n        # -----------------------------\n\n        # Check for deprecated USER_ENABLE_CONFIRM_EMAIL\n        setting = app.config.get('USER_ENABLE_LOGIN_WITHOUT_CONFIRM_EMAIL', None)\n        if setting is not None:\n            print(\n                'Deprecation warning: USER_ENABLE_LOGIN_WITHOUT_CONFIRM_EMAIL'\\\n                ' will be deprecated.' \\\n                ' It has been replaced by USER_ALLOW_LOGIN_WITHOUT_CONFIRMED_EMAIL.'\\\n                ' Please change this as soon as possible.')\n            self.USER_ALLOW_LOGIN_WITHOUT_CONFIRMED_EMAIL = setting\n\n        # Check for deprecated USER_ENABLE_RETYPE_PASSWORD\n        setting = app.config.get('USER_ENABLE_RETYPE_PASSWORD', None)\n        if setting is not None:\n            print(\n                'Deprecation warning: USER_ENABLE_RETYPE_PASSWORD'\\\n                ' will be deprecated.' \\\n                ' It has been replaced with USER_REQUIRE_RETYPE_PASSWORD.'\\\n                ' Please change this as soon as possible.')\n            self.USER_REQUIRE_RETYPE_PASSWORD = setting\n\n        # Check for deprecated USER_SHOW_USERNAME_EMAIL_DOES_NOT_EXIST\n        setting = app.config.get('USER_SHOW_USERNAME_EMAIL_DOES_NOT_EXIST', None)\n        if setting is not None:\n            print(\n                'Deprecation warning: USER_SHOW_USERNAME_EMAIL_DOES_NOT_EXIST' \\\n                ' will be deprecated.' \\\n                ' It has been replaced with USER_SHOW_USERNAME_DOES_NOT_EXIST'\n                ' and USER_SHOW_EMAIL_DOES_NOT_EXIST.'\n                ' Please change this as soon as possible.')\n            self.USER_SHOW_USERNAME_DOES_NOT_EXIST = setting\n            self.USER_SHOW_EMAIL_DOES_NOT_EXIST = setting\n\n        # Check for deprecated USER_PASSWORD_HASH\n        setting = app.config.get('USER_PASSWORD_HASH', None)\n        if setting is not None:\n            print(\n                \"Deprecation warning: USER_PASSWORD_HASH=<string>\"\\\n                \" will be deprecated.\"\\\n                \" It has been replaced with USER_PASSLIB_CRYPTCONTEXT_SCHEMES=<list>.\"\n                \" Please change USER_PASSWORD_HASH='something' to\"\\\n                \" USER_PASSLIB_CRYPTCONTEXT_SCHEMES=['something'] as soon as possible.\")\n            self.USER_PASSLIB_CRYPTCONTEXT_SCHEMES = [setting]\n\n        # Check that USER_EMAIL_SENDER_EMAIL is set when USER_ENABLE_EMAIL is True\n        if not self.USER_EMAIL_SENDER_EMAIL and self.USER_ENABLE_EMAIL:\n            raise ConfigError(\n                'USER_EMAIL_SENDER_EMAIL is missing while USER_ENABLE_EMAIL is True.'\\\n                ' specify USER_EMAIL_SENDER_EMAIL (and USER_EMAIL_SENDER_NAME) or set USER_ENABLE_EMAIL to False.')\n\n        # Disable settings that rely on a feature setting that's not enabled\n        # ------------------------------------------------------------------\n\n        # USER_ENABLE_REGISTER=True must have USER_ENABLE_USERNAME=True or USER_ENABLE_EMAIL=True.\n        if not self.USER_ENABLE_USERNAME and not self.USER_ENABLE_EMAIL:\n            self.USER_ENABLE_REGISTER = False\n\n        # Settings that depend on USER_ENABLE_EMAIL\n        if not self.USER_ENABLE_EMAIL:\n            self.USER_ENABLE_CONFIRM_EMAIL = False\n            self.USER_ENABLE_MULTIPLE_EMAILS = False\n            self.USER_ENABLE_FORGOT_PASSWORD = False\n            self.USER_SEND_PASSWORD_CHANGED_EMAIL = False\n            self.USER_SEND_REGISTERED_EMAIL = False\n            self.USER_SEND_USERNAME_CHANGED_EMAIL = False\n            self.USER_REQUIRE_INVITATION = False\n\n        # Settings that depend on USER_ENABLE_USERNAME\n        if not self.USER_ENABLE_USERNAME:\n            self.USER_ENABLE_CHANGE_USERNAME = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfigure a list of URLs to route to their corresponding view methods.", "response": "def _add_url_routes(self, app):\n        \"\"\"Configure a list of URLs to route to their corresponding view method..\"\"\"\n\n        # Because methods contain an extra ``self`` parameter, URL routes are mapped\n        # to stub functions, which simply call the corresponding method.\n\n        # For testing purposes, we map all available URLs to stubs, but the stubs\n        # contain config checks to return 404 when a feature is disabled.\n\n        # Define the stubs\n        # ----------------\n\n        # def auth0_callback_stub():\n        #     if not self.USER_ENABLE_AUTH0: abort(404)\n        #     return self.auth0_callback_view()\n\n        def change_password_stub():\n            if not self.USER_ENABLE_CHANGE_PASSWORD: abort(404)\n            return self.change_password_view()\n\n        def change_username_stub():\n            if not self.USER_ENABLE_CHANGE_USERNAME: abort(404)\n            return self.change_username_view()\n\n        def confirm_email_stub(token):\n            if not self.USER_ENABLE_CONFIRM_EMAIL: abort(404)\n            return self.confirm_email_view(token)\n\n        def edit_user_profile_stub():\n            return self.edit_user_profile_view()\n\n        def email_action_stub(id, action):\n            if not self.USER_ENABLE_MULTIPLE_EMAILS or not self.db_manager.UserEmailClass: abort(404)\n            return self.email_action_view(id, action)\n\n        def forgot_password_stub():\n            if not self.USER_ENABLE_FORGOT_PASSWORD: abort(404)\n            return self.forgot_password_view()\n\n        def manage_emails_stub():\n            if not self.USER_ENABLE_MULTIPLE_EMAILS or not self.db_manager.UserEmailClass: abort(404)\n            return self.manage_emails_view()\n\n        def invite_user_stub():\n            if not self.USER_ENABLE_INVITE_USER: abort(404)\n            return self.invite_user_view()\n\n        def login_stub():\n            return self.login_view()\n\n        def logout_stub():\n            return self.logout_view()\n\n        def register_stub():\n            if not self.USER_ENABLE_REGISTER: abort(404)\n            return self.register_view()\n\n        def resend_email_confirmation_stub():\n            if not self.USER_ENABLE_CONFIRM_EMAIL: abort(404)\n            return self.resend_email_confirmation_view()\n\n        def reset_password_stub(token):\n            if not self.USER_ENABLE_FORGOT_PASSWORD: abort(404)\n            return self.reset_password_view(token)\n\n        # def unconfirmed_email_stub():\n        #     return self.unconfirmed_email_view()\n\n        def unauthorized_stub():\n            return self.unauthorized_view()\n\n\n        # Add the URL routes\n        # ------------------\n\n        # app.add_url_rule('/callbacks/auth0', 'user.auth0_callback', auth0_callback_stub)\n        app.add_url_rule(self.USER_CHANGE_PASSWORD_URL, 'user.change_password', change_password_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_CHANGE_USERNAME_URL, 'user.change_username', change_username_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_CONFIRM_EMAIL_URL, 'user.confirm_email', confirm_email_stub)\n        app.add_url_rule(self.USER_EDIT_USER_PROFILE_URL, 'user.edit_user_profile', edit_user_profile_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_EMAIL_ACTION_URL, 'user.email_action', email_action_stub)\n        app.add_url_rule(self.USER_FORGOT_PASSWORD_URL, 'user.forgot_password', forgot_password_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_INVITE_USER_URL, 'user.invite_user', invite_user_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_LOGIN_URL, 'user.login', login_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_LOGOUT_URL, 'user.logout', logout_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_MANAGE_EMAILS_URL, 'user.manage_emails', manage_emails_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_REGISTER_URL, 'user.register', register_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_RESEND_EMAIL_CONFIRMATION_URL, 'user.resend_email_confirmation',\n                         resend_email_confirmation_stub,\n                         methods=['GET', 'POST'])\n        app.add_url_rule(self.USER_RESET_PASSWORD_URL, 'user.reset_password', reset_password_stub,\n                         methods=['GET', 'POST'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve object of type ObjectClass by id.", "response": "def get_object(self, ObjectClass, id):\n        \"\"\" Retrieve object of type ``ObjectClass`` by ``id``.\n\n        | Returns object on success.\n        | Returns None otherwise.\n        \"\"\"\n        try:\n            object = ObjectClass.objects.get(id=id)\n        except (ObjectClass.DoesNotExist, ObjectClass.MultipleObjectsReturned):\n            object = None\n        return object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the first object of type ObjectClass matching the specified filters in kwargs.", "response": "def ifind_first_object(self, ObjectClass, **kwargs):\n        \"\"\" Retrieve the first object of type ``ObjectClass``,\n        matching the specified filters in ``**kwargs`` -- case insensitive.\n\n        | If USER_IFIND_MODE is 'nocase_collation' this method maps to find_first_object().\n        | If USER_IFIND_MODE is 'ifind' this method performs a case insensitive find.\n        \"\"\"\n        # Call regular find() if USER_IFIND_MODE is nocase_collation\n        if self.user_manager.USER_IFIND_MODE=='nocase_collation':\n            return self.find_first_object(ObjectClass, **kwargs)\n\n        # Convert ...(email=value) to ...(email__iexact=value)\n        iexact_kwargs = {}\n        for key, value in kwargs.items():\n            iexact_kwargs[key+'__iexact'] = value\n        # Retrieve first object -- case insensitive\n        return ObjectClass.objects(**iexact_kwargs).first()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_all_tables(self):\n\n        # Retrieve database name from application config\n        app = self.db.app\n        mongo_settings = app.config['MONGODB_SETTINGS']\n        database_name = mongo_settings['db']\n\n        # Flask-MongoEngine is built on MongoEngine, which is built on PyMongo.\n        # To drop database collections, we need to access the PyMongo Database object,\n        # which is stored in the PyMongo MongoClient object,\n        # which is stored in app.extensions['mongoengine'][self]['conn']\n        py_mongo_mongo_client = app.extensions['mongoengine'][self.db]['conn']\n        py_mongo_database = py_mongo_mongo_client[database_name]\n\n        # Use the PyMongo Database object\n        for collection_name in py_mongo_database.collection_names():\n            py_mongo_database.drop_collection(collection_name)", "response": "Drop all document collections of the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_objects(self, ObjectClass, **kwargs):\n\n        # Convert each name/value pair in '**kwargs' into a filter\n        query = ObjectClass.query\n        for field_name, field_value in kwargs.items():\n\n            # Make sure that ObjectClass has a 'field_name' property\n            field = getattr(ObjectClass, field_name, None)\n            if field is None:\n                raise KeyError(\"BaseAlchemyAdapter.find_first_object(): Class '%s' has no field '%s'.\" % (ObjectClass, field_name))\n\n            # Add a filter to the query\n            query = query.filter(field==field_value)\n\n        # Execute query\n        return query.all()", "response": "Find all objects of type ObjectClass matching the filters specified in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_first_object(self, ObjectClass, **kwargs):\n\n        # Convert each name/value pair in 'kwargs' into a filter\n        query = ObjectClass.query\n        for field_name, field_value in kwargs.items():\n\n            # Make sure that ObjectClass has a 'field_name' property\n            field = getattr(ObjectClass, field_name, None)\n            if field is None:\n                raise KeyError(\"BaseAlchemyAdapter.find_first_object(): Class '%s' has no field '%s'.\" % (ObjectClass, field_name))\n\n            # Add a case sensitive filter to the query\n            query = query.filter(field==field_value)  # case sensitive!!\n\n        # Execute query\n        return query.first()", "response": "Retrieve the first object of type ObjectClass matching the specified filters in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the first object of type ObjectClass matching the specified filters in kwargs.", "response": "def ifind_first_object(self, ObjectClass, **kwargs):\n        \"\"\" Retrieve the first object of type ``ObjectClass``,\n        matching the specified filters in ``**kwargs`` -- case insensitive.\n\n        | If USER_IFIND_MODE is 'nocase_collation' this method maps to find_first_object().\n        | If USER_IFIND_MODE is 'ifind' this method performs a case insensitive find.\n        \"\"\"\n\n        # Call regular find() if USER_IFIND_MODE is nocase_collation\n        if self.user_manager.USER_IFIND_MODE=='nocase_collation':\n            return self.find_first_object(ObjectClass, **kwargs)\n\n        # Convert each name/value pair in 'kwargs' into a filter\n        query = ObjectClass.query\n        for field_name, field_value in kwargs.items():\n\n            # Make sure that ObjectClass has a 'field_name' property\n            field = getattr(ObjectClass, field_name, None)\n            if field is None:\n                raise KeyError(\"BaseAlchemyAdapter.find_first_object(): Class '%s' has no field '%s'.\" % (ObjectClass, field_name))\n\n            # Add a case sensitive filter to the query\n            query = query.filter(field.ifind(field_value))  # case insensitive!!\n\n        # Execute query\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding object to db session. Only for session - centric object - database mappers.", "response": "def add_object(self, object):\n        \"\"\"Add object to db session. Only for session-centric object-database mappers.\"\"\"\n        if object.id is None:\n            object.get_id()\n        self.db.engine.save(object)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve object of type ObjectClass by id.", "response": "def get_object(self, ObjectClass, id):\n        \"\"\" Retrieve object of type ``ObjectClass`` by ``id``.\n\n        | Returns object on success.\n        | Returns None otherwise.\n        \"\"\"\n        print('dynamo.get(%s, %s)' % (ObjectClass, str(id)))\n        resp = self.db.engine.get(ObjectClass, [id])\n        if resp:\n            return resp[0]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves all objects of type ObjectClass matching the filters specified in kwargs.", "response": "def find_objects(self, ObjectClass, **kwargs):\n        \"\"\" Retrieve all objects of type ``ObjectClass``,\n        matching the filters specified in ``**kwargs`` -- case sensitive.\n        \"\"\"\n\n        print('dynamo.find_objects(%s, %s)' % (ObjectClass, str(kwargs)))\n\n        query = self.db.engine.query(ObjectClass)\n        for field_name, field_value in kwargs.items():\n\n            # Make sure that ObjectClass has a 'field_name' property\n            field = getattr(ObjectClass, field_name, None)\n            if field is None:\n                raise KeyError(\"DynamoDBAdapter.find_objects(): Class '%s' has no field '%s'.\" % (ObjectClass, field_name))\n\n            # Add a case sensitive filter to the query\n            query = query.filter(field == field_value)\n\n        # Execute query\n        return query.all(desc=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_first_object(self, ObjectClass, **kwargs):\n\n        print('dynamo.find_first_object(%s, %s)' % (ObjectClass, str(kwargs)))\n        query = self.db.engine.query(ObjectClass)\n        for field_name, field_value in kwargs.items():\n\n            # Make sure that ObjectClass has a 'field_name' property\n            field = getattr(ObjectClass, field_name, None)\n            if field is None:\n                raise KeyError(\"DynamoDBAdapter.find_first_object(): Class '%s' has no field '%s'.\" % (ObjectClass, field_name))\n\n            # Add a case sensitive filter to the query\n            query = query.filter(field == field_value)\n\n        # Execute query\n        out = query.first(desc=True)#, attributes=['password'])\n        return out", "response": "Retrieve the first object of type ObjectClass matching the filters specified in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ifind_first_object(self, ObjectClass, **kwargs):\n        # Call regular find() if USER_IFIND_MODE is nocase_collation\n        if self.user_manager.USER_IFIND_MODE=='nocase_collation':\n            return self.find_first_object(ObjectClass, **kwargs)\n\n        raise NotImplementedError", "response": "Retrieve the first object of type ObjectClass matching the specified filters in **kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_object(self, object):\n        #pdb.set_trace()\n        self.db.engine.delete_key(object)#, userid='abc123', id='1')\n        print('dynamo.delete_object(%s)' % object)", "response": "Delete object specified by object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_app():\n\n    # Setup Flask app and app.config\n    app = Flask(__name__)\n    app.config.from_object(__name__ + '.ConfigClass')\n\n    # Initialize Flask extensions\n    db = None\n    mail = Mail(app)  # Initialize Flask-Mail\n\n    # Define the User data model. Make sure to add flask_user UserMixin !!!\n    class UsernameIndex(GlobalSecondaryIndex):\n        class Meta:\n            read_capacity_units = 1\n            write_capacity_units = 1\n            projection = AllProjection()\n\n        username = UnicodeAttribute(hash_key=True)\n\n    class EmailIndex(GlobalSecondaryIndex):\n        class Meta:\n            read_capacity_units = 1\n            write_capacity_units = 1\n            projection = AllProjection()\n\n        email = UnicodeAttribute(hash_key=True)\n\n    class User(Model, UserMixin):\n        class Meta:\n            table_name = 'users'\n\n        id = UnicodeAttribute(hash_key=True, default=lambda: str(uuid.uuid1()))\n        active = BooleanAttribute()\n\n        # User authentication information\n        username = UnicodeAttribute(null=True)\n        password = UnicodeAttribute(null=True)\n\n        username_index = UsernameIndex()\n\n        # User email information\n        email = UnicodeAttribute(null=True)\n        email_confirmed_at = UTCDateTimeAttribute(null=True)\n\n        email_index = EmailIndex()\n\n        # User information\n        first_name = UnicodeAttribute(null=True)\n        last_name = UnicodeAttribute(null=True)\n\n\n    # Setup Flask-User\n    user_manager = UserManager(app, db, User)\n\n    # Create all database tables\n    print('create_schema()')\n    user_manager.db_manager.create_all_tables()\n    print('created_schema()')\n\n    # The Home page is accessible to anyone\n    @app.route('/')\n    def home_page():\n        return render_template_string(\"\"\"\n            {% block content %}\n                <h2>Home page</h2>\n                <p>This page can be accessed by anyone.</p><br/>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (anyone)</p>\n                <p><a href={{ url_for('members_page') }}>Members page</a> (login required)</p>\n            {% endblock %}\n            \"\"\")\n\n    # The Members page is only accessible to authenticated users\n    @app.route('/members')\n    @login_required  # Use of @login_required decorator\n    def members_page():\n        return render_template_string(\"\"\"\n            {% block content %}\n                <h2>Members page</h2>\n                <p>This page can only be accessed by authenticated users.</p><br/>\n                <p><a href={{ url_for('home_page') }}>Home page</a> (anyone)</p>\n                <p><a href={{ url_for('members_page') }}>Members page</a> (login required)</p>\n            {% endblock %}\n            \"\"\")\n\n    return app", "response": "Create the Flask application factory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unique_username_validator(form, field):\n    user_manager =  current_app.user_manager\n    if not user_manager.db_manager.username_is_available(field.data):\n        raise ValidationError(_('This Username is already in use. Please try another one.'))", "response": "Ensure that the Username is unique. This validator may NOT be customized."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unique_email_validator(form, field):\n    user_manager =  current_app.user_manager\n    if not user_manager.email_is_available(field.data):\n        raise ValidationError(_('This Email is already in use. Please try another one.'))", "response": "Username must be unique. This validator may NOT be customized."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_email_message(self, recipient, subject, html_message, text_message, sender_email, sender_name):\n\n        # Construct sender from sender_name and sender_email\n        sender = '\"%s\" <%s>' % (sender_name, sender_email) if sender_name else sender_email\n\n        # Send email via SMTP except when we're testing\n        if not current_app.testing:  # pragma: no cover\n            try:\n                # Prepare email message\n                from flask_mail import Message\n                message = Message(\n                    subject,\n                    sender=sender,\n                    recipients=[recipient],\n                    html=html_message,\n                    body=text_message)\n\n                # Send email message\n                self.mail.send(message)\n\n            # Print helpful error messages on exceptions\n            except (socket.gaierror, socket.error) as e:\n                raise EmailError('SMTP Connection error: Check your MAIL_SERVER and MAIL_PORT settings.')\n            except smtplib.SMTPAuthenticationError:\n                raise EmailError('SMTP Authentication error: Check your MAIL_USERNAME and MAIL_PASSWORD settings.')", "response": "Send an email message via Flask - Mail."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_password(self, password, password_hash):\n\n        # Print deprecation warning if called with (password, user) instead of (password, user.password)\n        if isinstance(password_hash, self.user_manager.db_manager.UserClass):\n            print(\n                'Deprecation warning: verify_password(password, user) has been changed'\\\n                ' to: verify_password(password, password_hash). The user param will be deprecated.'\\\n                ' Please change your call with verify_password(password, user) into'\\\n                ' a call with verify_password(password, user.password)'\n                ' as soon as possible.')\n            password_hash = password_hash.password   # effectively user.password\n\n        # Use passlib's CryptContext to verify a password\n        return self.password_crypt_context.verify(password, password_hash)", "response": "Verify plaintext password against hashed password."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_email_message(self, recipient, subject, html_message, text_message, sender_email, sender_name):\n\n        if not current_app.testing:  # pragma: no cover\n            try:\n                # Prepare Sendgrid helper objects\n                from sendgrid.helpers.mail import Email, Content, Substitution, Mail\n                from_email = Email(sender_email, sender_name)\n                to_email = Email(recipient)\n                text_content = Content('text/plain', text_message)\n                html_content = Content('text/html', html_message)\n                # Prepare Sendgrid Mail object\n                # Note: RFC 1341: text must be first, followed by html\n                mail = Mail(from_email, subject, to_email, text_content)\n                mail.add_content(html_content)\n                # Send mail via the Sendgrid API\n                response = self.sg.client.mail.send.post(request_body=mail.get())\n                print(response.status_code)\n                print(response.body)\n                print(response.headers)\n            except ImportError:\n                raise ConfigError(SENDGRID_IMPORT_ERROR_MESSAGE)\n            except Exception as e:\n                print(e)\n                print(e.body)\n                raise", "response": "Send an email message via Sendgrid API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_app():\n    \n    # Create Flask app load app.config\n    app = Flask(__name__)\n    app.config.from_object(__name__+'.ConfigClass')\n\n    # Initialize Flask-BabelEx\n    babel = Babel(app)\n\n    # Initialize Flask-SQLAlchemy\n    db = SQLAlchemy(app)\n\n    # Define the User data-model.\n    # NB: Make sure to add flask_user UserMixin !!!\n    class User(db.Model, UserMixin):\n        __tablename__ = 'users'\n        id = db.Column(db.Integer, primary_key=True)\n        active = db.Column('is_active', db.Boolean(), nullable=False, server_default='1')\n\n        # User authentication information. The collation='NOCASE' is required\n        # to search case insensitively when USER_IFIND_MODE is 'nocase_collation'.\n        email = db.Column(db.String(255, collation='NOCASE'), nullable=False, unique=True)\n        email_confirmed_at = db.Column(db.DateTime())\n        password = db.Column(db.String(255), nullable=False, server_default='')\n\n        # User information\n        first_name = db.Column(db.String(100, collation='NOCASE'), nullable=False, server_default='')\n        last_name = db.Column(db.String(100, collation='NOCASE'), nullable=False, server_default='')\n\n        # Define the relationship to Role via UserRoles\n        roles = db.relationship('Role', secondary='user_roles')\n\n    # Define the Role data-model\n    class Role(db.Model):\n        __tablename__ = 'roles'\n        id = db.Column(db.Integer(), primary_key=True)\n        name = db.Column(db.String(50), unique=True)\n\n    # Define the UserRoles association table\n    class UserRoles(db.Model):\n        __tablename__ = 'user_roles'\n        id = db.Column(db.Integer(), primary_key=True)\n        user_id = db.Column(db.Integer(), db.ForeignKey('users.id', ondelete='CASCADE'))\n        role_id = db.Column(db.Integer(), db.ForeignKey('roles.id', ondelete='CASCADE'))\n\n    # Setup Flask-User and specify the User data-model\n    user_manager = UserManager(app, db, User)\n\n    # Create all database tables\n    db.create_all()\n\n    # Create 'member@example.com' user with no roles\n    if not User.query.filter(User.email == 'member@example.com').first():\n        user = User(\n            email='member@example.com',\n            email_confirmed_at=datetime.datetime.utcnow(),\n            password=user_manager.hash_password('Password1'),\n        )\n        db.session.add(user)\n        db.session.commit()\n\n    # Create 'admin@example.com' user with 'Admin' and 'Agent' roles\n    if not User.query.filter(User.email == 'admin@example.com').first():\n        user = User(\n            email='admin@example.com',\n            email_confirmed_at=datetime.datetime.utcnow(),\n            password=user_manager.hash_password('Password1'),\n        )\n        user.roles.append(Role(name='Admin'))\n        user.roles.append(Role(name='Agent'))\n        db.session.add(user)\n        db.session.commit()\n\n    # The Home page is accessible to anyone\n    @app.route('/')\n    def home_page():\n        return render_template_string(\"\"\"\n                {% extends \"flask_user_layout.html\" %}\n                {% block content %}\n                    <h2>{%trans%}Home page{%endtrans%}</h2>\n                    <p><a href={{ url_for('user.register') }}>{%trans%}Register{%endtrans%}</a></p>\n                    <p><a href={{ url_for('user.login') }}>{%trans%}Sign in{%endtrans%}</a></p>\n                    <p><a href={{ url_for('home_page') }}>{%trans%}Home Page{%endtrans%}</a> (accessible to anyone)</p>\n                    <p><a href={{ url_for('member_page') }}>{%trans%}Member Page{%endtrans%}</a> (login_required: member@example.com / Password1)</p>\n                    <p><a href={{ url_for('admin_page') }}>{%trans%}Admin Page{%endtrans%}</a> (role_required: admin@example.com / Password1')</p>\n                    <p><a href={{ url_for('user.logout') }}>{%trans%}Sign out{%endtrans%}</a></p>\n                {% endblock %}\n                \"\"\")\n\n    # The Members page is only accessible to authenticated users\n    @app.route('/members')\n    @login_required    # Use of @login_required decorator\n    def member_page():\n        return render_template_string(\"\"\"\n                {% extends \"flask_user_layout.html\" %}\n                {% block content %}\n                    <h2>{%trans%}Members page{%endtrans%}</h2>\n                    <p><a href={{ url_for('user.register') }}>{%trans%}Register{%endtrans%}</a></p>\n                    <p><a href={{ url_for('user.login') }}>{%trans%}Sign in{%endtrans%}</a></p>\n                    <p><a href={{ url_for('home_page') }}>{%trans%}Home Page{%endtrans%}</a> (accessible to anyone)</p>\n                    <p><a href={{ url_for('member_page') }}>{%trans%}Member Page{%endtrans%}</a> (login_required: member@example.com / Password1)</p>\n                    <p><a href={{ url_for('admin_page') }}>{%trans%}Admin Page{%endtrans%}</a> (role_required: admin@example.com / Password1')</p>\n                    <p><a href={{ url_for('user.logout') }}>{%trans%}Sign out{%endtrans%}</a></p>\n                {% endblock %}\n                \"\"\")\n\n    # The Admin page requires an 'Admin' role.\n    @app.route('/admin')\n    @roles_required('Admin')    # Use of @roles_required decorator\n    def admin_page():\n        return render_template_string(\"\"\"\n                {% extends \"flask_user_layout.html\" %}\n                {% block content %}\n                    <h2>{%trans%}Admin Page{%endtrans%}</h2>\n                    <p><a href={{ url_for('user.register') }}>{%trans%}Register{%endtrans%}</a></p>\n                    <p><a href={{ url_for('user.login') }}>{%trans%}Sign in{%endtrans%}</a></p>\n                    <p><a href={{ url_for('home_page') }}>{%trans%}Home Page{%endtrans%}</a> (accessible to anyone)</p>\n                    <p><a href={{ url_for('member_page') }}>{%trans%}Member Page{%endtrans%}</a> (login_required: member@example.com / Password1)</p>\n                    <p><a href={{ url_for('admin_page') }}>{%trans%}Admin Page{%endtrans%}</a> (role_required: admin@example.com / Password1')</p>\n                    <p><a href={{ url_for('user.logout') }}>{%trans%}Sign out{%endtrans%}</a></p>\n                {% endblock %}\n                \"\"\")\n\n    return app", "response": "Create the application factory"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves all objects of type ObjectClass matching the specified filters.", "response": "def find_objects(self, ObjectClass, **kwargs):\n        \"\"\" Retrieve all objects of type ``ObjectClass``,\n        matching the specified filters in ``**kwargs`` -- case sensitive.\n        \"\"\"\n        filter = None\n        for k, v in kwargs.items():\n            cond = ObjectClass.getattr(k) == v\n            filter = cond if filter is None else filter & cond\n\n        return ObjectClass.scan(filter)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the first object of type ObjectClass matching the specified filters in kwargs -- case sensitive.", "response": "def find_first_object(self, ObjectClass, **kwargs):\n        \"\"\" Retrieve the first object of type ``ObjectClass``,\n        matching the specified filters in ``**kwargs`` -- case sensitive.\n        \"\"\"\n        filter = None\n        for k, v in kwargs.items():\n            cond = getattr(ObjectClass, k) == v\n            filter = cond if filter is None else filter & cond\n\n        return list(ObjectClass.scan(filter, limit=1))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ifind_first_object(self, ObjectClass, **kwargs):\n        from pynamodb.attributes import UnicodeAttribute\n\n        if self.user_manager.USER_IFIND_MODE == 'nocase_collation':\n            return self.find_first_object(ObjectClass, **kwargs)\n\n        # The below is horrendously slow on a large user database, but DynamoDB\n        # has no support for case insensitive search, so we have to scan.\n        # We try and be a little smart and use any non-unicode filters in the scan, thought.\n\n        tfilters = {k: v.lower() for k, v in kwargs.items() if type(getattr(ObjectClass, k)) == UnicodeAttribute}\n\n        ntfilter = None\n        for k in [k for k in kwargs if k not in tfilters]:\n            cond = getattr(ObjectClass, k) == kwargs[k]\n            ntfilter = cond if ntfilter is None else ntfilter & cond\n\n        for o in ObjectClass.scan(ntfilter):\n            for k in tfilters:\n                if getattr(o, k, None).lower() != kwargs[k]:\n                    break\n            else:\n                # all match\n                return o\n\n        return None", "response": "Return the first object of type ObjectClass matching the specified filters in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_all_tables(self):\n        for klass in self.__get_classes():\n            if not klass.exists():\n                klass.create_table(read_capacity_units=1, write_capacity_units=1, wait=True)", "response": "Create database tables for all known database data - models."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw(self):\n        self.draw_nodes()\n        self.draw_edges()\n        # note that self.groups only exists on condition\n        # that group_label_position was given!\n        if hasattr(self, \"groups\") and self.groups:\n            self.draw_group_labels()\n        logging.debug(\"DRAW: {0}\".format(self.sm))\n        if self.sm:\n            self.figure.subplots_adjust(right=0.8)\n            cax = self.figure.add_axes([0.85, 0.2, 0.05, 0.6])\n            self.figure.colorbar(self.sm, cax=cax)\n        self.ax.relim()\n        self.ax.autoscale_view()\n        self.ax.set_aspect(\"equal\")", "response": "Draws the plot to screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the node colors. Also computes the colorbar.", "response": "def compute_node_colors(self):\n        \"\"\"Compute the node colors. Also computes the colorbar.\"\"\"\n        data = [self.graph.node[n][self.node_color] for n in self.nodes]\n\n        if self.group_order == \"alphabetically\":\n            data_reduced = sorted(list(set(data)))\n        elif self.group_order == \"default\":\n            data_reduced = list(unique_everseen(data))\n\n        dtype = infer_data_type(data)\n        n_grps = num_discrete_groups(data)\n\n        if dtype == \"categorical\" or dtype == \"ordinal\":\n            if n_grps <= 8:\n                cmap = get_cmap(\n                    cmaps[\"Accent_{0}\".format(n_grps)].mpl_colormap\n                )\n            else:\n                cmap = n_group_colorpallet(n_grps)\n        elif dtype == \"continuous\" and not is_data_diverging(data):\n            cmap = get_cmap(cmaps[\"continuous\"].mpl_colormap)\n        elif dtype == \"continuous\" and is_data_diverging(data):\n            cmap = get_cmap(cmaps[\"diverging\"].mpl_colormap)\n\n        for d in data:\n            idx = data_reduced.index(d) / n_grps\n            self.node_colors.append(cmap(idx))\n\n        # Add colorbar if required.ListedColormap\n        logging.debug(\"length of data_reduced: {0}\".format(len(data_reduced)))\n        logging.debug(\"dtype: {0}\".format(dtype))\n        if len(data_reduced) > 1 and dtype == \"continuous\":\n            self.sm = plt.cm.ScalarMappable(\n                cmap=cmap,\n                norm=plt.Normalize(\n                    vmin=min(data_reduced),\n                    vmax=max(data_reduced),  # noqa  # noqa\n                ),\n            )\n            self.sm._A = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_group_colors(self):\n        seen = set()\n        self.group_label_color = [\n            x for x in self.node_colors if not (x in seen or seen.add(x))\n        ]", "response": "Computes the group colors according to node colors"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_edge_colors(self):\n        data = [self.graph.edges[n][self.edge_color] for n in self.edges]\n        data_reduced = sorted(list(set(data)))\n\n        dtype = infer_data_type(data)\n        n_grps = num_discrete_groups(data)\n        if dtype == \"categorical\" or dtype == \"ordinal\":\n            if n_grps <= 8:\n                cmap = get_cmap(\n                    cmaps[\"Accent_{0}\".format(n_grps)].mpl_colormap\n                )\n            else:\n                cmap = n_group_colorpallet(n_grps)\n        elif dtype == \"continuous\" and not is_data_diverging(data):\n            cmap = get_cmap(cmaps[\"weights\"])\n\n        for d in data:\n            idx = data_reduced.index(d) / n_grps\n            self.edge_colors.append(cmap(idx))\n        # Add colorbar if required.\n        logging.debug(\"length of data_reduced: {0}\".format(len(data_reduced)))\n        logging.debug(\"dtype: {0}\".format(dtype))\n        if len(data_reduced) > 1 and dtype == \"continuous\":\n            self.sm = plt.cm.ScalarMappable(\n                cmap=cmap,\n                norm=plt.Normalize(\n                    vmin=min(data_reduced),\n                    vmax=max(data_reduced),  # noqa  # noqa\n                ),\n            )\n            self.sm._A = []", "response": "Compute the edge colors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_node_sizes(self):\n        if type(self.node_size) is str:\n            nodes = self.graph.nodes\n            self.node_sizes = [nodes[n][self.node_size] for n in self.nodes]\n        else:\n            self.node_sizes = self.node_size", "response": "Compute the node sizes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_edge_widths(self):\n        if type(self.edge_width) is str:\n            edges = self.graph.edges\n            self.edge_widths = [edges[n][self.edge_width] for n in self.edges]\n        else:\n            self.edge_widths = self.edge_width", "response": "Compute the edge widths."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngroups and then sorts the nodes according to the criteria passed into the Plot constructor.", "response": "def group_and_sort_nodes(self):\n        \"\"\"\n        Groups and then sorts the nodes according to the criteria passed into\n        the Plot constructor.\n        \"\"\"\n        if self.node_grouping and not self.node_order:\n            if self.group_order == \"alphabetically\":\n                self.nodes = [\n                    n\n                    for n, d in sorted(\n                        self.graph.nodes(data=True),\n                        key=lambda x: x[1][self.node_grouping],\n                    )\n                ]\n\n            elif self.group_order == \"default\":\n                grp = [\n                    d[self.node_grouping]\n                    for _, d in self.graph.nodes(data=True)\n                ]\n                grp_name = list(unique_everseen(grp))\n                nodes = []\n                for key in grp_name:\n                    nodes.extend(\n                        [\n                            n\n                            for n, d in self.graph.nodes(data=True)\n                            if key in d.values()\n                        ]\n                    )\n                self.nodes = nodes\n\n        elif self.node_order and not self.node_grouping:\n            self.nodes = [\n                n\n                for n, _ in sorted(\n                    self.graph.nodes(data=True),\n                    key=lambda x: x[1][self.node_order],\n                )\n            ]\n\n        elif self.node_grouping and self.node_order:\n            if self.group_order == \"alphabetically\":\n                self.nodes = [\n                    n\n                    for n, d in sorted(\n                        self.graph.nodes(data=True),\n                        key=lambda x: (\n                            x[1][self.node_grouping],\n                            x[1][self.node_order],\n                        ),\n                    )\n                ]\n            elif self.group_order == \"default\":\n                grp = [\n                    d[self.node_grouping]\n                    for _, d in self.graph.nodes(data=True)\n                ]\n                grp_name = list(unique_everseen(grp))\n                nodes = []\n                for key in grp_name:\n                    nodes.extend(\n                        [\n                            n\n                            for n, d in sorted(\n                                self.graph.nodes(data=True),\n                                key=lambda x: x[1][self.node_order],\n                            )\n                            if key in d.values()\n                        ]\n                    )\n                self.nodes = nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_group_label_positions(self):\n        assert self.group_label_position in [\"beginning\", \"middle\", \"end\"]\n        data = [self.graph.node[n][self.node_grouping] for n in self.nodes]\n        node_length = len(data)\n        groups = items_in_groups(data)\n\n        edge_of_plot = self.plot_radius + self.nodeprops[\"radius\"]\n        # The 1.02 serves as padding\n        radius = 1.02 * edge_of_plot + self.group_label_offset\n        xs = []\n        ys = []\n        has = []\n        vas = []\n        node_idcs = np.cumsum(list(groups.values()))\n        node_idcs = np.insert(node_idcs, 0, 0)\n        if self.group_label_position == \"beginning\":\n            for idx in node_idcs[:-1]:\n                x, y = get_cartesian(\n                    r=radius, theta=group_theta(node_length, idx)\n                )\n                ha, va = text_alignment(x, y)\n                xs.append(x)\n                ys.append(y)\n                has.append(ha)\n                vas.append(va)\n\n        elif self.group_label_position == \"middle\":\n            node_idcs = node_idcs.reshape(len(node_idcs), 1)\n            node_idcs = np.concatenate((node_idcs[:-1], node_idcs[1:]), axis=1)\n            for idx in node_idcs:\n                theta1 = group_theta(node_length, idx[0])\n                theta2 = group_theta(node_length, idx[1] - 1)\n                x, y = get_cartesian(r=radius, theta=(theta1 + theta2) / 2)\n                ha, va = text_alignment(x, y)\n                xs.append(x)\n                ys.append(y)\n                has.append(ha)\n                vas.append(va)\n\n        elif self.group_label_position == \"end\":\n            for idx in node_idcs[1::]:\n                x, y = get_cartesian(\n                    r=radius, theta=group_theta(node_length, idx - 1)\n                )\n                ha, va = text_alignment(x, y)\n                xs.append(x)\n                ys.append(y)\n                has.append(ha)\n                vas.append(va)\n\n        self.group_label_coords = {\"x\": xs, \"y\": ys}\n        self.group_label_aligns = {\"has\": has, \"vas\": vas}\n        self.groups = groups.keys()", "response": "Computes the x y positions of the group labels."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the positions of each node in the Circos plot.", "response": "def compute_node_positions(self):\n        \"\"\"\n        Uses the get_cartesian function to compute the positions of each node\n        in the Circos plot.\n        \"\"\"\n        xs = []\n        ys = []\n        node_r = self.nodeprops[\"radius\"]\n        radius = circos_radius(n_nodes=len(self.graph.nodes()), node_r=node_r)\n        self.plot_radius = radius\n        self.nodeprops[\"linewidth\"] = radius * 0.01\n        for node in self.nodes:\n            x, y = get_cartesian(r=radius, theta=node_theta(self.nodes, node))\n            xs.append(x)\n            ys.append(y)\n        self.node_coords = {\"x\": xs, \"y\": ys}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_node_label_positions(self):\n        self.init_node_label_meta()\n\n        for node in self.nodes:\n\n            # Define radius 'radius' and circumference 'theta'\n            theta = node_theta(self.nodes, node)\n            # multiplication factor 1.02 moved below\n            radius = self.plot_radius + self.nodeprops[\"radius\"]\n\n            # Coordinates of text inside nodes\n            if self.node_label_layout == \"numbers\":\n                radius_adjustment = 1.0 - (1.0 / radius)\n            else:\n                radius_adjustment = 1.02\n            x, y = get_cartesian(r=radius * radius_adjustment, theta=theta)\n\n            # ----- For numbered nodes -----\n\n            # Node label x-axis coordinate\n            tx, _ = get_cartesian(r=radius, theta=theta)\n            # Create the quasi-circular positioning on the x axis\n            tx *= 1 - np.log(np.cos(theta) * self.nonzero_sign(np.cos(theta)))\n            # Move each node a little further away from the circos\n            tx += self.nonzero_sign(x)\n\n            # Node label y-axis coordinate numerator\n            numerator = radius * (\n                theta % (self.nonzero_sign(y) * self.nonzero_sign(x) * np.pi)\n            )\n            # Node label y-axis coordinate denominator\n            denominator = self.nonzero_sign(x) * np.pi\n            # Node label y-axis coordinate\n            ty = 2 * (numerator / denominator)\n\n            # ----- For rotated nodes -----\n\n            # Computes the text rotation\n            theta_deg = to_degrees(theta)\n            if theta_deg >= -90 and theta_deg < 90:  # right side\n                rot = theta_deg\n            else:  # left side\n                rot = theta_deg - 180\n\n            # Store values\n            self.store_node_label_meta(x, y, tx, ty, rot)", "response": "Compute the positions of each node in the Circos plot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_node_label_meta(self, x, y, tx, ty, rot):\n\n        # Store computed values\n        self.node_label_coords[\"x\"].append(x)\n        self.node_label_coords[\"y\"].append(y)\n        self.node_label_coords[\"tx\"].append(tx)\n        self.node_label_coords[\"ty\"].append(ty)\n\n        # Computes the text alignment for x\n        if x == 0:\n            self.node_label_aligns[\"has\"].append(\"center\")\n        elif x > 0:\n            self.node_label_aligns[\"has\"].append(\"left\")\n        else:\n            self.node_label_aligns[\"has\"].append(\"right\")\n\n        # Computes the text alignment for y\n        if self.node_label_layout == \"rotate\" or y == 0:\n            self.node_label_aligns[\"vas\"].append(\"center\")\n        elif y > 0:\n            self.node_label_aligns[\"vas\"].append(\"bottom\")\n        else:\n            self.node_label_aligns[\"vas\"].append(\"top\")\n\n        self.node_label_rotation.append(rot)", "response": "This function stores coordinates - related metadate for a node label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef draw_nodes(self):\n        node_r = self.nodeprops[\"radius\"]\n        lw = self.nodeprops[\"linewidth\"]\n        for i, node in enumerate(self.nodes):\n            x = self.node_coords[\"x\"][i]\n            y = self.node_coords[\"y\"][i]\n            color = self.node_colors[i]\n            node_patch = patches.Circle(\n                (x, y), node_r, lw=lw, color=color, zorder=2\n            )\n            self.ax.add_patch(node_patch)\n            if self.node_labels:\n                label_x = self.node_label_coords[\"x\"][i]\n                label_y = self.node_label_coords[\"y\"][i]\n                label_tx = self.node_label_coords[\"tx\"][i]\n                label_ty = self.node_label_coords[\"ty\"][i]\n                label_ha = self.node_label_aligns[\"has\"][i]\n                label_va = self.node_label_aligns[\"vas\"][i]\n\n                # ----- Node label rotation layout -----\n\n                if self.node_label_layout == \"rotation\":\n                    rot = self.node_label_rotation[i]\n\n                    self.ax.text(\n                        s=node,\n                        x=label_x,\n                        y=label_y,\n                        ha=label_ha,\n                        va=label_va,\n                        rotation=rot,\n                        rotation_mode=\"anchor\",\n                        color=self.node_label_color[i],\n                        fontsize=self.fontsize,\n                        family=self.fontfamily,\n                    )\n\n                # ----- Node label numbering layout -----\n\n                elif self.node_label_layout == \"numbers\":\n\n                    # Draw descriptions for labels\n                    desc = \"%s - %s\" % ((i, node) if (x > 0) else (node, i))\n                    self.ax.text(\n                        s=desc,\n                        x=label_tx,\n                        y=label_ty,\n                        ha=label_ha,\n                        va=label_va,\n                        color=self.node_label_color[i],\n                        fontsize=self.fontsize,\n                        family=self.fontfamily,\n                    )\n\n                    # Add numbers to nodes\n                    self.ax.text(\n                        s=i, x=label_x, y=label_y, ha=\"center\", va=\"center\"\n                    )\n\n                # Standard node label layout\n                else:\n\n                    # Draw node text straight from the nodes\n                    self.ax.text(\n                        s=node,\n                        x=label_x,\n                        y=label_y,\n                        ha=label_ha,\n                        va=label_va,\n                        color=self.node_label_color[i],\n                        fontsize=self.fontsize,\n                        family=self.fontfamily,\n                    )", "response": "Draws nodes to the figure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering edges to the figure.", "response": "def draw_edges(self):\n        \"\"\"\n        Renders edges to the figure.\n        \"\"\"\n        for i, (start, end) in enumerate(self.graph.edges()):\n            start_theta = node_theta(self.nodes, start)\n            end_theta = node_theta(self.nodes, end)\n            verts = [\n                get_cartesian(self.plot_radius, start_theta),\n                (0, 0),\n                get_cartesian(self.plot_radius, end_theta),\n            ]\n            color = self.edge_colors[i]\n            codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3]\n            lw = self.edge_widths[i]\n            path = Path(verts, codes)\n            patch = patches.PathPatch(\n                path, lw=lw, edgecolor=color, zorder=1, **self.edgeprops\n            )\n            self.ax.add_patch(patch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_group_labels(self):\n        for i, label in enumerate(self.groups):\n            label_x = self.group_label_coords[\"x\"][i]\n            label_y = self.group_label_coords[\"y\"][i]\n            label_ha = self.group_label_aligns[\"has\"][i]\n            label_va = self.group_label_aligns[\"vas\"][i]\n            color = self.group_label_color[i]\n            self.ax.text(\n                s=label,\n                x=label_x,\n                y=label_y,\n                ha=label_ha,\n                va=label_va,\n                color=color,\n                fontsize=self.fontsize,\n                family=self.fontfamily,\n            )", "response": "Renders group labels to the figure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw(self):\n        matrix = nx.to_numpy_matrix(self.graph, nodelist=self.nodes)\n        self.ax.matshow(matrix, cmap=self.cmap)", "response": "Draws the matrix plot to screen."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the positions of the nodes in a line starting at ( x y ) = 0. 5 units.", "response": "def compute_node_positions(self):\n        \"\"\"\n        Computes nodes positions.\n\n        Arranges nodes in a line starting at (x,y) = (0,0). Node radius is\n        assumed to be equal to 0.5 units. Nodes are placed at integer\n        locations.\n        \"\"\"\n        xs = [0] * len(self.nodes)\n        ys = [0] * len(self.nodes)\n        for i, _ in enumerate(self.nodes[1:], start=1):\n            prev_r = self.node_sizes[i - 1] / 2\n            curr_r = self.node_sizes[i] / 2\n            xs[i] = xs[i - 1] + prev_r + curr_r\n\n        self.node_coords = {\"x\": xs, \"y\": ys}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_nodes(self):\n        node_r = self.node_sizes\n        for i, node in enumerate(self.nodes):\n            x = self.node_coords[\"x\"][i]\n            y = self.node_coords[\"y\"][i]\n            color = self.node_colors[i]\n            node_patch = patches.Ellipse(\n                (x, y), node_r[i], node_r[i], lw=0, color=color, zorder=2\n            )\n            self.ax.add_patch(node_patch)", "response": "Draw nodes to screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws edges to the figure.", "response": "def draw_edges(self):\n        \"\"\"\n        Renders edges to the figure.\n        \"\"\"\n        for i, (start, end) in enumerate(self.graph.edges()):\n            start_idx = self.nodes.index(start)\n            start_x = self.node_coords[\"x\"][start_idx]\n            start_y = self.node_coords[\"y\"][start_idx]\n\n            end_idx = self.nodes.index(end)\n            end_x = self.node_coords[\"x\"][end_idx]\n            end_y = self.node_coords[\"y\"][end_idx]\n\n            arc_radius = abs(end_x - start_x) / 2\n            # we do min(start_x, end_x) just in case start_x is greater than\n            # end_x.\n            middle_x = min(start_x, end_x) + arc_radius\n            middle_y = arc_radius * 2\n\n            verts = [(start_x, start_y), (middle_x, middle_y), (end_x, end_y)]\n\n            color = self.edge_colors[i]\n            codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3]\n            lw = self.edge_widths[i]\n            path = Path(verts, codes)\n            patch = patches.PathPatch(\n                path, lw=lw, edgecolor=color, zorder=1, **self.edgeprops\n            )\n            self.ax.add_patch(patch)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the node positions based on the specified longitude and latitude keyword arguments.", "response": "def compute_node_positions(self):\n        \"\"\"\n        Extracts the node positions based on the specified longitude and\n        latitude keyword arguments.\n        \"\"\"\n        xs = []\n        ys = []\n        self.locs = dict()\n\n        for node in self.nodes:\n            x = self.graph.node[node][self.node_lon]\n            y = self.graph.node[node][self.node_lat]\n            xs.append(x)\n            ys.append(y)\n            self.locs[node] = (x, y)\n\n        self.node_coords = {\"x\": xs, \"y\": ys}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_nodes(self):\n        if self.backend == \"matplotlib\":\n            node_r = 0.005  # temporarily hardcoded.\n            for i, node in enumerate(self.nodes):\n                x = self.node_coords[\"x\"][i]\n                y = self.node_coords[\"y\"][i]\n                color = self.node_colors[i]\n                node_patch = patches.Ellipse(\n                    (x, y), node_r, node_r, lw=0, color=color, zorder=2\n                )\n                self.ax.add_patch(node_patch)\n        elif self.backend == \"altair\":\n            self.node_chart = (\n                alt.Chart(self.node_df)\n                .mark_point()\n                .encode(\n                    alt.X(f\"{self.node_lon}:Q\", scale=alt.Scale(zero=False)),\n                    alt.Y(f\"{self.node_lat}:Q\", scale=alt.Scale(zero=False)),\n                )\n            )", "response": "Draws nodes to the screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndraws edges to screen.", "response": "def draw_edges(self):\n        \"\"\"\n        Draws edges to screen.\n        \"\"\"\n        if self.backend == \"matplotlib\":\n            for i, (n1, n2) in enumerate(self.edges):\n                x1, y1 = self.locs[n1]\n                x2, y2 = self.locs[n2]\n                color = self.edge_colors[i]\n                line = Line2D(\n                    xdata=[x1, x2],\n                    ydata=[y1, y2],\n                    color=color,\n                    zorder=0,\n                    alpha=0.3,\n                )\n                self.ax.add_line(line)\n        elif self.backend == \"altair\":\n            marker_attrs = dict()\n            marker_attrs[\"color\"] = \"black\"  # MAGICNUMBER\n            marker_attrs[\"strokeWidth\"] = 1  # MAGICNUMBER\n            self.edge_chart = (\n                alt.Chart(self.edge_df)\n                .mark_line(**marker_attrs)\n                .encode(\n                    alt.X(f\"{self.node_lon}:Q\"),\n                    alt.Y(f\"{self.node_lat}:Q\"),\n                    detail=\"edge\",\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_travis_deploy_password(encrypted_password):\n    config = load_yaml_config(TRAVIS_CONFIG_FILE)\n\n    config[\"deploy\"][\"password\"] = dict(secure=encrypted_password)\n\n    save_yaml_config(TRAVIS_CONFIG_FILE, config)\n\n    line = (\n        \"# This file was autogenerated and will overwrite\"\n        \" each time you run travis_pypi_setup.py\\n\"\n    )\n    prepend_line(TRAVIS_CONFIG_FILE, line)", "response": "Update the deploy section of the. travis. yml file\n    to use the given encrypted password."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild an undirected graph from a pandas dataframe containing the data of a node - pair.", "response": "def graph_from_dataframe(\n    dataframe,\n    threshold_by_percent_unique=0.1,\n    threshold_by_count_unique=None,\n    node_id_columns=[],\n    node_property_columns=[],\n    edge_property_columns=[],\n    node_type_key=\"type\",\n    edge_type_key=\"type\",\n    collapse_edges=True,\n    edge_agg_key=\"weight\",\n):\n    \"\"\"\n    Build an undirected graph from a pandas dataframe.\n\n    This function attempts to infer which cells should become nodes\n    based on either:\n\n        a. what percentage of the column are unique values (defaults to 10%)\n        b. an explicit count of unique values (i.e. any column with 7 unique\n           values or less)\n        c. an explicit list of column keys (i.e.\n           ['employee_id', 'location_code'])\n\n    Column headers are preserved as node and edge 'types'. By default, this is\n    stored using the key 'type' which is used by some graph import processes\n    but can be reconfigured.\n\n    This function uses a MultiGraph structure during the build phase so that it\n    is possible to make multiple connections between nodes. By default, at the\n    end of the build phase, the MultiGraph is converted to a Graph and the\n    count of edges between each node-pair is written as a 'weight' property.\n\n    :param pandas.DataFrame dataframe: A pandas dataframe containing the data\n        to be converted into a graph.\n    :param float threshold_by_percent_unique: A percent value used to determine\n        whether a column should be used to generate nodes based on its\n        cardinality (i.e. in a dataframe with 100 rows, treat any column with\n        10 or less unique values as a node)\n    :param int threshold_by_count_unique: A numeric value used to determine\n        whether a column should be used to generate nodes based on its\n        cardinality (i.e. if 7 is supplied, treat any column with 7 or less\n        unique values as a node) - supplying a value will take priority over\n        percent_unique\n    :param list node_id_columns: A list of column headers to use for generating\n        nodes. Suppyling any value will take precedence over\n        threshold_by_percent_unique or threshold_by_count_unique. Note: this\n        can cause the size of the graph to expand significantly since every\n        unique value in a column will become a node.\n    :param list node_property_columns: A list of column headers to use for\n        generating properties of nodes. These can include the same column\n        headers used for the node id.\n    :param list edge_property_columns: A list of column headers to use for\n        generating properties of edges.\n    :param str node_type_key: A string that sets the key will be used to\n        preserve the column name as node property (this is useful for importing\n        networkx graphs to databases that distinguish between node 'types' or\n        for visually encoding those types in plots).\n    :param str edge_type_key: A string that sets the key will be used to keep\n        track of edge relationships an 'types' (this is useful for importing\n        networkx graphs to databases that distinguish between edge'types' or\n        for visually encoding those types in plots). Edge type values are\n        automatically set to <node_a_id>_<node_b_id>.\n    :param bool collapse_edges: Graphs are instantiated as a 'MultiGraph'\n        (allow multiple edges between nodes) and then collapsed into a 'Graph'\n        which only has a single edge between any two nodes. Information is\n        preserved by aggregating the count of those edges as a 'weight' value.\n        Set this value to False to return the MultiGraph. Note: this can cause\n        the size of the graph to expand significantly since each row can\n        potentially have n! edges where n is the number of columns in the\n        dataframe.\n    :param str edge_agg_key: A string that sets the key the edge count will be\n        assigned to when edges are aggregated.\n    :returns: A networkx Graph (or MultiGraph if collapse_edges is set to\n        False).\n    \"\"\"\n\n    assert isinstance(\n        dataframe, pd.DataFrame\n    ), \"{} is not a pandas DataFrame\".format(dataframe)\n\n    M = MultiGraph()\n\n    # if explicit specification of node_id_columns is provided, use those\n    if len(node_id_columns) > 0:\n        node_columns = node_id_columns\n    else:\n        # otherwise, compute with thresholds based on the dataframe\n        if threshold_by_count_unique:\n            node_columns = sorted(\n                [\n                    col\n                    for col in dataframe.columns\n                    if dataframe[col].nunique() <= threshold_by_count_unique\n                ]\n            )\n        else:\n            node_columns = sorted(\n                [\n                    col\n                    for col in dataframe.columns\n                    if dataframe[col].nunique() / dataframe.shape[0]\n                    <= threshold_by_percent_unique  # NOQA to preserve meaningful variable names\n                ]\n            )\n\n    # use the unique values for each node column as node types\n    for node_type in node_columns:\n        M.add_nodes_from(\n            [\n                (node, {node_type_key: node_type})\n                for node in dataframe[node_type].unique()\n            ]\n        )\n\n    # iterate over the rows and generate an edge for each pair of node columns\n    for i, row in dataframe.iterrows():\n        # assemble the edge properties as a dictionary\n        edge_properties = {k: row[k] for k in edge_property_columns}\n\n        # iterate over the node_ids in each node_column of the dataframe row\n        node_buffer = []\n        for node_type in node_columns:\n            node_id = row[node_type]\n\n            # get a reference to the node and assign any specified properties\n            node = M.nodes[node_id]\n            for k in node_property_columns:\n                # if values are not identical, append with a pipe delimiter\n                if k not in node:\n                    node[k] = row[k]\n                elif isinstance(node[k], str) and str(row[k]) not in node[k]:\n                    node[k] += \"|{}\".format(str(row[k]))\n                elif str(row[k]) not in str(node[k]):\n                    node[k] = str(node[k]) + \"|{}\".format(str(row[k]))\n\n            # build edges using precomputed edge properties\n            for other_node_id, other_node_type in node_buffer:\n                # sort node_type so undirected edges all share the same type\n                ordered_name = \"_\".join(sorted([node_type, other_node_type]))\n                edge_properties[edge_type_key] = ordered_name\n                M.add_edge(node_id, other_node_id, **edge_properties)\n\n            # store the node from this column in the buffer for future edges\n            node_buffer.append((node_id, node_type))\n\n    if collapse_edges:\n        # convert the MultiGraph to a Graph\n        G = Graph(M)\n        k = edge_agg_key\n        # preserve the edge count as a sum of the weight values\n        for u, v, data in M.edges(data=True):\n            w = data[k] if k in data else 1.0\n            edge = G[u][v]\n            edge[k] = (w + edge[k]) if k in edge else w\n        return G\n\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_data_homogenous(data_container):\n    data_types = set([type(i) for i in data_container])\n    return len(data_types) == 1", "response": "Checks that all of the data in the container are of the same Python data type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer the type of data in a given container of data.", "response": "def infer_data_type(data_container):\n    \"\"\"\n    For a given container of data, infer the type of data as one of\n    continuous, categorical, or ordinal.\n\n    For now, it is a one-to-one mapping as such:\n\n    - str:   categorical\n    - int:   ordinal\n    - float: continuous\n\n    There may be better ways that are not currently implemented below. For\n    example, with a list of numbers, we can check whether the number of unique\n    entries is less than or equal to 12, but has over 10000+ entries. This\n    would be a good candidate for floats being categorical.\n\n    :param data_container: A generic container of data points.\n    :type data_container: `iterable`\n\n    \"\"\"\n    # Defensive programming checks.\n    # 0. Ensure that we are dealing with lists or tuples, and nothing else.\n    assert isinstance(data_container, list) or isinstance(\n        data_container, tuple\n    ), \"data_container should be a list or tuple.\"\n    # 1. Don't want to deal with only single values.\n    assert (\n        len(set(data_container)) > 1\n    ), \"There should be more than one value in the data container.\"\n    # 2. Don't want to deal with mixed data.\n    assert is_data_homogenous(\n        data_container\n    ), \"Data are not of a homogenous type!\"\n\n    # Once we check that the data type of the container is homogenous, we only\n    # need to check the first element in the data container for its type.\n    datum = data_container[0]\n\n    # Return statements below\n    # treat binomial data as categorical\n    # TODO: make tests for this.\n    if len(set(data_container)) == 2:\n        return \"categorical\"\n\n    elif isinstance(datum, str):\n        return \"categorical\"\n\n    elif isinstance(datum, int):\n        return \"ordinal\"\n\n    elif isinstance(datum, float):\n        return \"continuous\"\n\n    else:\n        raise ValueError(\"Not possible to tell what the data type is.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_data_diverging(data_container):\n    assert infer_data_type(data_container) in [\n        \"ordinal\",\n        \"continuous\",\n    ], \"Data type should be ordinal or continuous\"\n\n    # Check whether the data contains negative and positive values.\n    has_negative = False\n    has_positive = False\n    for i in data_container:\n        if i < 0:\n            has_negative = True\n        elif i > 0:\n            has_positive = True\n    if has_negative and has_positive:\n        return True\n    else:\n        return False", "response": "Checks whether the data points are diverging or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert nodes in the graph into a pandas DataFrame.", "response": "def to_pandas_nodes(G):  # noqa: N803\n    \"\"\"\n    Convert nodes in the graph into a pandas DataFrame.\n    \"\"\"\n    data = []\n    for n, meta in G.nodes(data=True):\n        d = dict()\n        d[\"node\"] = n\n        d.update(meta)\n        data.append(d)\n    return pd.DataFrame(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Graph edges to pandas DataFrame that s readable to Altair.", "response": "def to_pandas_edges(G, x_kw, y_kw, **kwargs):  # noqa: N803\n    \"\"\"\n    Convert Graph edges to pandas DataFrame that's readable to Altair.\n    \"\"\"\n    # Get all attributes in nodes\n    attributes = [\"source\", \"target\", \"x\", \"y\", \"edge\", \"pair\"]\n    for e in G.edges():\n        attributes += list(G.edges[e].keys())\n    attributes = list(set(attributes))\n\n    # Build a dataframe for all edges and their attributes\n    df = pd.DataFrame(index=range(G.size() * 2), columns=attributes)\n\n    # Add node data to dataframe.\n    for i, (n1, n2, d) in enumerate(G.edges(data=True)):\n        idx = i * 2\n        x = G.node[n1][x_kw]\n        y = G.node[n1][y_kw]\n        data1 = dict(\n            edge=i, source=n1, target=n2, pair=(n1, n2), x=x, y=y, **d\n        )\n\n        data2 = dict(\n            edge=i, source=n1, target=n2, pair=(n1, n2), x=x, y=y, **d\n        )\n\n        df.loc[idx] = data1\n        df.loc[idx + 1] = data2\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef node_theta(nodelist, node):\n    assert len(nodelist) > 0, \"nodelist must be a list of items.\"\n    assert node in nodelist, \"node must be inside nodelist.\"\n\n    i = nodelist.index(node)\n    theta = -np.pi + i * 2 * np.pi / len(nodelist)\n\n    return theta", "response": "Maps node to Angle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group_theta(node_length, node_idx):\n    theta = -np.pi + node_idx * 2 * np.pi / node_length\n    return theta", "response": "Returns an angle corresponding to a node of interest in radians."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef text_alignment(x, y):\n    if x == 0:\n        ha = \"center\"\n    elif x > 0:\n        ha = \"left\"\n    else:\n        ha = \"right\"\n    if y == 0:\n        va = \"center\"\n    elif y > 0:\n        va = \"bottom\"\n    else:\n        va = \"top\"\n\n    return ha, va", "response": "Aligns text labels based on the x - and y - axis coordinate values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef circos_radius(n_nodes, node_r):\n    A = 2 * np.pi / n_nodes  # noqa\n    B = (np.pi - A) / 2  # noqa\n    a = 2 * node_r\n    return a * np.sin(B) / np.sin(A)", "response": "Calculates the origin - to - node centre radius of the Circos plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_polar(x, y, theta_units=\"radians\"):\n    assert theta_units in [\n        \"radians\",\n        \"degrees\",\n    ], \"kwarg theta_units must specified in radians or degrees\"\n\n    theta = atan2(y, x)\n    r = sqrt(x ** 2 + y ** 2)\n\n    if theta_units == \"degrees\":\n        theta = to_degrees(theta)\n\n    return r, theta", "response": "Converts cartesian x y to polar r theta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_proper_radians(theta):\n    if theta > pi or theta < -pi:\n        theta = theta % pi\n    return theta", "response": "Converts theta to radians in the proper range of pi."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n\n    # Hack related to #58\n    if sys.platform == \"win32\":\n        os.system(\"chcp 65001\");\n\n    parser = argparse.ArgumentParser(description='SoundScrape. Scrape an artist from SoundCloud.\\n')\n    parser.add_argument('artist_url', metavar='U', type=str, nargs='*',\n                        help='An artist\\'s SoundCloud username or URL')\n    parser.add_argument('-n', '--num-tracks', type=int, default=sys.maxsize,\n                        help='The number of tracks to download')\n    parser.add_argument('-g', '--group', action='store_true',\n                        help='Use if downloading tracks from a SoundCloud group')\n    parser.add_argument('-b', '--bandcamp', action='store_true',\n                        help='Use if downloading from Bandcamp rather than SoundCloud')\n    parser.add_argument('-m', '--mixcloud', action='store_true',\n                        help='Use if downloading from Mixcloud rather than SoundCloud')\n    parser.add_argument('-a', '--audiomack', action='store_true',\n                        help='Use if downloading from Audiomack rather than SoundCloud')\n    parser.add_argument('-c', '--hive', action='store_true',\n                        help='Use if downloading from Hive.co rather than SoundCloud')\n    parser.add_argument('-l', '--likes', action='store_true',\n                        help='Download all of a user\\'s Likes.')\n    parser.add_argument('-L', '--login', type=str, default='soundscrape123@mailinator.com',\n                        help='Set login')\n    parser.add_argument('-d', '--downloadable', action='store_true',\n                        help='Only fetch tracks with a Downloadable link.')\n    parser.add_argument('-t', '--track', type=str, default='',\n                        help='The name of a specific track by an artist')\n    parser.add_argument('-f', '--folders', action='store_true',\n                        help='Organize saved songs in folders by artists')\n    parser.add_argument('-p', '--path', type=str, default='',\n                        help='Set directory path where downloads should be saved to')\n    parser.add_argument('-P', '--password', type=str, default='soundscraperocks',\n                        help='Set password')\n    parser.add_argument('-o', '--open', action='store_true',\n                        help='Open downloaded files after downloading.')\n    parser.add_argument('-k', '--keep', action='store_true',\n                        help='Keep 30-second preview tracks')\n    parser.add_argument('-v', '--version', action='store_true', default=False,\n                        help='Display the current version of SoundScrape')\n\n    args = parser.parse_args()\n    vargs = vars(args)\n\n    if vargs['version']:\n        import pkg_resources\n        version = pkg_resources.require(\"soundscrape\")[0].version\n        print(version)\n        return\n\n    if not vargs['artist_url']:\n        parser.error('Please supply an artist\\'s username or URL!')\n\n    if sys.version_info < (3,0,0):\n        vargs['artist_url'] = urllib.quote(vargs['artist_url'][0], safe=':/')\n    else:\n        vargs['artist_url'] = urllib.parse.quote(vargs['artist_url'][0], safe=':/')\n\n    artist_url = vargs['artist_url']\n\n    if not exists(vargs['path']):\n        if not access(dirname(vargs['path']), W_OK):\n            vargs['path'] = ''\n        else:\n            mkdir(vargs['path'])\n\n    if 'bandcamp.com' in artist_url or vargs['bandcamp']:\n        process_bandcamp(vargs)\n    elif 'mixcloud.com' in artist_url or vargs['mixcloud']:\n        process_mixcloud(vargs)\n    elif 'audiomack.com' in artist_url or vargs['audiomack']:\n        process_audiomack(vargs)\n    elif 'hive.co' in artist_url or vargs['hive']:\n        process_hive(vargs)\n    elif 'musicbed.com' in artist_url:\n        process_musicbed(vargs)\n    else:\n        process_soundcloud(vargs)", "response": "Main function for the base command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_track(track, album_name=u'', keep_previews=False, folders=False, filenames=[], custom_path=''):\n\n    hard_track_url = get_hard_track_url(track['id'])\n\n    # We have no info on this track whatsoever.\n    if not 'title' in track:\n        return None\n\n    if not keep_previews:\n        if (track.get('duration', 0) < track.get('full_duration', 0)):\n            puts_safe(colored.yellow(\"Skipping preview track\") + colored.white(\": \" + track['title']))\n            return None\n\n    # May not have a \"full name\"\n    name = track['user'].get('full_name', '')\n    if name == '':\n        name = track['user']['username']\n\n    filename = sanitize_filename(name + ' - ' + track['title'] + '.mp3')\n\n    if folders:\n        name_path = join(custom_path, name)\n        if not exists(name_path):\n            mkdir(name_path)\n        filename = join(name_path, filename)\n    else:\n        filename = join(custom_path, filename)\n\n    if exists(filename):\n        puts_safe(colored.yellow(\"Track already downloaded: \") + colored.white(track['title']))\n        return None\n\n    # Skip already downloaded track.\n    if filename in filenames:\n        return None\n\n    if hard_track_url:\n        puts_safe(colored.green(\"Scraping\") + colored.white(\": \" + track['title']))\n    else:\n        # Region coded?\n        puts_safe(colored.yellow(\"Unable to download\") + colored.white(\": \" + track['title']))\n        return None\n\n    filename = download_file(hard_track_url, filename)\n    tagged = tag_file(filename,\n             artist=name,\n             title=track['title'],\n             year=track['created_at'][:4],\n             genre=track['genre'],\n             album=album_name,\n             artwork_url=track['artwork_url'])\n    if not tagged:\n        wav_filename = filename[:-3] + 'wav'\n        os.rename(filename, wav_filename)\n        filename = wav_filename\n\n    return filename", "response": "Downloads a track and returns a new file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading a list of tracks.", "response": "def download_tracks(client, tracks, num_tracks=sys.maxsize, downloadable=False, folders=False, custom_path='', id3_extras={}):\n    \"\"\"\n    Given a list of tracks, iteratively download all of them.\n\n    \"\"\"\n\n    filenames = []\n\n    for i, track in enumerate(tracks):\n\n        # \"Track\" and \"Resource\" objects are actually different,\n        # even though they're the same.\n        if isinstance(track, soundcloud.resource.Resource):\n\n            try:\n\n                t_track = {}\n                t_track['downloadable'] = track.downloadable\n                t_track['streamable'] = track.streamable\n                t_track['title'] = track.title\n                t_track['user'] = {'username': track.user['username']}\n                t_track['release_year'] = track.release\n                t_track['genre'] = track.genre\n                t_track['artwork_url'] = track.artwork_url\n                if track.downloadable:\n                    t_track['stream_url'] = track.download_url\n                else:\n                    if downloadable:\n                        puts_safe(colored.red(\"Skipping\") + colored.white(\": \" + track.title))\n                        continue\n                    if hasattr(track, 'stream_url'):\n                        t_track['stream_url'] = track.stream_url\n                    else:\n                        t_track['direct'] = True\n                        streams_url = \"https://api.soundcloud.com/i1/tracks/%s/streams?client_id=%s&app_version=%s\" % (\n                        str(track.id), AGGRESSIVE_CLIENT_ID, APP_VERSION)\n                        response = requests.get(streams_url).json()\n                        t_track['stream_url'] = response['http_mp3_128_url']\n\n                track = t_track\n            except Exception as e:\n                puts_safe(colored.white(track.title) + colored.red(' is not downloadable.'))\n                continue\n\n        if i > num_tracks - 1:\n            continue\n        try:\n            if not track.get('stream_url', False):\n                puts_safe(colored.white(track['title']) + colored.red(' is not downloadable.'))\n                continue\n            else:\n                track_artist = sanitize_filename(track['user']['username'])\n                track_title = sanitize_filename(track['title'])\n                track_filename = track_artist + ' - ' + track_title + '.mp3'\n\n                if folders:\n                    track_artist_path = join(custom_path, track_artist)\n                    if not exists(track_artist_path):\n                        mkdir(track_artist_path)\n                    track_filename = join(track_artist_path, track_filename)\n                else:\n                    track_filename = join(custom_path, track_filename)\n\n                if exists(track_filename):\n                    puts_safe(colored.yellow(\"Track already downloaded: \") + colored.white(track_title))\n                    continue\n\n                puts_safe(colored.green(\"Downloading\") + colored.white(\": \" + track['title']))\n\n\n                if track.get('direct', False):\n                    location = track['stream_url']\n                else:\n                    stream = client.get(track['stream_url'], allow_redirects=False, limit=200)\n                    if hasattr(stream, 'location'):\n                        location = stream.location\n                    else:\n                        location = stream.url\n\n                filename = download_file(location, track_filename)\n                tagged = tag_file(filename,\n                         artist=track['user']['username'],\n                         title=track['title'],\n                         year=track['release_year'],\n                         genre=track['genre'],\n                         album=id3_extras.get('album', None),\n                         artwork_url=track['artwork_url'])\n\n                if not tagged:\n                    wav_filename = filename[:-3] + 'wav'\n                    os.rename(filename, wav_filename)\n                    filename = wav_filename\n\n                filenames.append(filename)\n        except Exception as e:\n            puts_safe(colored.red(\"Problem downloading \") + colored.white(track['title']))\n            puts_safe(e)\n\n    return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_soundcloud_data(url):\n\n    data = {}\n\n    request = requests.get(url)\n\n    title_tag = request.text.split('<title>')[1].split('</title')[0]\n    data['title'] = title_tag.split(' by ')[0].strip()\n    data['artist'] = title_tag.split(' by ')[1].split('|')[0].strip()\n    # XXX Do more..\n\n    return data", "response": "Scrapes a SoundCloud page for a track s important information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscrape the new API. Returns the parsed JSON response.", "response": "def get_soundcloud_api2_data(artist_id):\n    \"\"\"\n    Scrape the new API. Returns the parsed JSON response.\n    \"\"\"\n\n    v2_url = \"https://api-v2.soundcloud.com/stream/users/%s?limit=500&client_id=%s&app_version=%s\" % (\n    artist_id, AGGRESSIVE_CLIENT_ID, APP_VERSION)\n    response = requests.get(v2_url)\n    parsed = response.json()\n\n    return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscraping the new API. Returns the parsed JSON response.", "response": "def get_soundcloud_api_playlist_data(playlist_id):\n    \"\"\"\n    Scrape the new API. Returns the parsed JSON response.\n    \"\"\"\n\n    url = \"https://api.soundcloud.com/playlists/%s?representation=full&client_id=02gUJC0hH2ct1EGOcYXQIzRFU91c72Ea&app_version=1467724310\" % (\n        playlist_id)\n    response = requests.get(url)\n    parsed = response.json()\n\n    return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_hard_track_url(item_id):\n\n    streams_url = \"https://api.soundcloud.com/i1/tracks/%s/streams/?client_id=%s&app_version=%s\" % (\n    item_id, AGGRESSIVE_CLIENT_ID, APP_VERSION)\n    response = requests.get(streams_url)\n    json_response = response.json()\n\n    if response.status_code == 200:\n        hard_track_url = json_response['http_mp3_128_url']\n        return hard_track_url\n    else:\n        return None", "response": "Get the hard - scrapes url for a track."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_bandcamp(vargs):\n\n    artist_url = vargs['artist_url']\n\n    if 'bandcamp.com' in artist_url or ('://' in artist_url and vargs['bandcamp']):\n        bc_url = artist_url\n    else:\n        bc_url = 'https://' + artist_url + '.bandcamp.com/music'\n\n    filenames = scrape_bandcamp_url(bc_url, num_tracks=vargs['num_tracks'], folders=vargs['folders'], custom_path=vargs['path'])\n\n    # check if we have lists inside a list, which indicates the\n    # scraping has gone recursive, so we must format the output\n    # ( reference: http://stackoverflow.com/a/5251706 )\n    if any(isinstance(elem, list) for elem in filenames):\n        # first, remove any empty sublists inside our outter list\n        # ( reference: http://stackoverflow.com/a/19875634 )\n        filenames = [sub for sub in filenames if sub]\n        # now, make sure we \"flatten\" the list\n        # ( reference: http://stackoverflow.com/a/11264751 )\n        filenames = [val for sub in filenames for val in sub]\n\n    if vargs['open']:\n        open_files(filenames)\n\n    return", "response": "Process the BandCamp file and return the list of the available modules."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scrape_bandcamp_url(url, num_tracks=sys.maxsize, folders=False, custom_path=''):\n\n    filenames = []\n    album_data = get_bandcamp_metadata(url)\n\n    # If it's a list, we're dealing with a list of Album URLs,\n    # so we call the scrape_bandcamp_url() method for each one\n    if type(album_data) is list:\n        for album_url in album_data:\n            filenames.append(scrape_bandcamp_url(album_url, num_tracks, folders, custom_path))\n        return filenames\n\n    artist = album_data[\"artist\"]\n    album_name = album_data[\"album_name\"]\n\n    if folders:\n        if album_name:\n            directory = artist + \" - \" + album_name\n        else:\n            directory = artist\n        directory = sanitize_filename(directory)\n        directory = join(custom_path, directory)\n        if not exists(directory):\n            mkdir(directory)\n\n    for i, track in enumerate(album_data[\"trackinfo\"]):\n\n        if i > num_tracks - 1:\n            continue\n\n        try:\n            track_name = track[\"title\"]\n            if track[\"track_num\"]:\n                track_number = str(track[\"track_num\"]).zfill(2)\n            else:\n                track_number = None\n            if track_number and folders:\n                track_filename = '%s - %s.mp3' % (track_number, track_name)\n            else:\n                track_filename = '%s.mp3' % (track_name)\n            track_filename = sanitize_filename(track_filename)\n\n            if folders:\n                path = join(directory, track_filename)\n            else:\n                path = join(custom_path, sanitize_filename(artist) + ' - ' + track_filename)\n\n            if exists(path):\n                puts_safe(colored.yellow(\"Track already downloaded: \") + colored.white(track_name))\n                continue\n\n            if not track['file']:\n                puts_safe(colored.yellow(\"Track unavailble for scraping: \") + colored.white(track_name))\n                continue\n\n            puts_safe(colored.green(\"Downloading\") + colored.white(\": \" + track_name))\n            path = download_file(track['file']['mp3-128'], path)\n\n            album_year = album_data['album_release_date']\n            if album_year:\n                album_year = datetime.strptime(album_year, \"%d %b %Y %H:%M:%S GMT\").year\n\n            tag_file(path,\n                     artist,\n                     track_name,\n                     album=album_name,\n                     year=album_year,\n                     genre=album_data['genre'],\n                     artwork_url=album_data['artFullsizeUrl'],\n                     track_number=track_number,\n                     url=album_data['url'])\n\n            filenames.append(path)\n\n        except Exception as e:\n            puts_safe(colored.red(\"Problem downloading \") + colored.white(track_name))\n            print(e)\n    return filenames", "response": "Scrape a Bandcamp URL and return a list of filenames to open."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bandcamp_metadata(url):\n    request = requests.get(url)\n    try:\n        sloppy_json = request.text.split(\"var TralbumData = \")\n        sloppy_json = sloppy_json[1].replace('\" + \"', \"\")\n        sloppy_json = sloppy_json.replace(\"'\", \"\\'\")\n        sloppy_json = sloppy_json.split(\"};\")[0] + \"};\"\n        sloppy_json = sloppy_json.replace(\"};\", \"}\")\n        output = demjson.decode(sloppy_json)\n    # if the JSON parser failed, we should consider it's a \"/music\" page,\n    # so we generate a list of albums/tracks and return it immediately\n    except Exception as e:\n        regex_all_albums = r'<a href=\"(/(?:album|track)/[^>]+)\">'\n        all_albums = re.findall(regex_all_albums, request.text, re.MULTILINE)\n        album_url_list = list()\n        for album in all_albums:\n            album_url = re.sub(r'music/?$', '', url) + album\n            album_url_list.append(album_url)\n        return album_url_list\n    # if the JSON parser was successful, use a regex to get all tags\n    # from this album/track, join them and set it as the \"genre\"\n    regex_tags = r'<a class=\"tag\" href[^>]+>([^<]+)</a>'\n    tags = re.findall(regex_tags, request.text, re.MULTILINE)\n    # make sure we treat integers correctly with join()\n    # according to http://stackoverflow.com/a/7323861\n    # (very unlikely, but better safe than sorry!)\n    output['genre'] = ' '.join(s for s in tags)\n    # make sure we always get the correct album name, even if this is a\n    # track URL (unless this track does not belong to any album, in which\n    # case the album name remains set as None.\n    output['album_name'] = None\n    regex_album_name = r'album_title\\s*:\\s*\"([^\"]+)\"\\s*,'\n    match = re.search(regex_album_name, request.text, re.MULTILINE)\n    if match:\n        output['album_name'] = match.group(1)\n\n    try:\n        artUrl = request.text.split(\"\\\"tralbumArt\\\">\")[1].split(\"\\\">\")[0].split(\"href=\\\"\")[1]\n        output['artFullsizeUrl'] = artUrl\n    except:\n        puts_safe(colored.red(\"Couldn't get full artwork\") + \"\")\n        output['artFullsizeUrl'] = None\n\n    return output", "response": "Get the Bandcamp metadata from a given URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_mixcloud(vargs):\n\n    artist_url = vargs['artist_url']\n\n    if 'mixcloud.com' in artist_url:\n        mc_url = artist_url\n    else:\n        mc_url = 'https://mixcloud.com/' + artist_url\n\n    filenames = scrape_mixcloud_url(mc_url, num_tracks=vargs['num_tracks'], folders=vargs['folders'], custom_path=vargs['path'])\n\n    if vargs['open']:\n        open_files(filenames)\n\n    return", "response": "Process a mixcloud artist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nscrape a mixcloud file and tag it", "response": "def scrape_mixcloud_url(mc_url, num_tracks=sys.maxsize, folders=False, custom_path=''):\n    \"\"\"\n    Returns:\n        list: filenames to open\n\n    \"\"\"\n\n    try:\n        data = get_mixcloud_data(mc_url)\n    except Exception as e:\n        puts_safe(colored.red(\"Problem downloading \") + mc_url)\n        print(e)\n        return []\n\n    filenames = []\n\n    track_artist = sanitize_filename(data['artist'])\n    track_title = sanitize_filename(data['title'])\n    track_filename = track_artist + ' - ' + track_title + data['mp3_url'][-4:]\n\n    if folders:\n        track_artist_path = join(custom_path, track_artist)\n        if not exists(track_artist_path):\n            mkdir(track_artist_path)\n        track_filename = join(track_artist_path, track_filename)\n        if exists(track_filename):\n            puts_safe(colored.yellow(\"Skipping\") + colored.white(': ' + data['title'] + \" - it already exists!\"))\n            return []\n    else:\n        track_filename = join(custom_path, track_filename)\n\n    puts_safe(colored.green(\"Downloading\") + colored.white(\n        ': ' + data['artist'] + \" - \" + data['title'] + \" (\" + track_filename[-4:] + \")\"))\n    download_file(data['mp3_url'], track_filename)\n    if track_filename[-4:] == '.mp3':\n        tag_file(track_filename,\n                 artist=data['artist'],\n                 title=data['title'],\n                 year=data['year'],\n                 genre=\"Mix\",\n                 artwork_url=data['artwork_url'])\n    filenames.append(track_filename)\n\n    return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mixcloud_data(url):\n\n    data = {}\n    request = requests.get(url)\n    preview_mp3_url = request.text.split('m-preview=\"')[1].split('\" m-preview-light')[0]\n    song_uuid = request.text.split('m-preview=\"')[1].split('\" m-preview-light')[0].split('previews/')[1].split('.mp3')[0]\n\n    # Fish for the m4a..\n    for server in range(1, 23):\n        # Ex: https://stream6.mixcloud.com/c/m4a/64/1/2/0/9/30fe-23aa-40da-9bf3-4bee2fba649d.m4a\n        mp3_url = \"https://stream\" + str(server) + \".mixcloud.com/c/m4a/64/\" + song_uuid + '.m4a'\n        try:\n            if requests.head(mp3_url).status_code == 200:\n                if '?' in mp3_url:\n                    mp3_url = mp3_url.split('?')[0]\n                break\n        except Exception as e:\n            continue\n\n    full_title = request.text.split(\"<title>\")[1].split(\" | Mixcloud\")[0]\n    title = full_title.split(' by ')[0].strip()\n    artist = full_title.split(' by ')[1].strip()\n\n    img_thumbnail_url = request.text.split('m-thumbnail-url=\"')[1].split(\" ng-class\")[0]\n    artwork_url = img_thumbnail_url.replace('60/', '300/').replace('60/', '300/').replace('//', 'https://').replace('\"',\n                                                                                                                    '')\n\n    data['mp3_url'] = mp3_url\n    data['title'] = title\n    data['artist'] = artist\n    data['artwork_url'] = artwork_url\n    data['year'] = None\n\n    return data", "response": "Scrapes a Mixcloud page for a track s important information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing Audiomack s files.", "response": "def process_audiomack(vargs):\n    \"\"\"\n    Main Audiomack path.\n    \"\"\"\n\n    artist_url = vargs['artist_url']\n\n    if 'audiomack.com' in artist_url:\n        mc_url = artist_url\n    else:\n        mc_url = 'https://audiomack.com/' + artist_url\n\n    filenames = scrape_audiomack_url(mc_url, num_tracks=vargs['num_tracks'], folders=vargs['folders'], custom_path=vargs['path'])\n\n    if vargs['open']:\n        open_files(filenames)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscrapes a Mixcloud page for a track s important information.", "response": "def get_audiomack_data(url):\n    \"\"\"\n    Scrapes a Mixcloud page for a track's important information.\n\n    Returns:\n        dict: containing audio data\n\n    \"\"\"\n\n    data = {}\n    request = requests.get(url)\n\n    mp3_url = request.text.split('class=\"player-icon download-song\" title=\"Download\" href=\"')[1].split('\"')[0]\n    artist = request.text.split('<span class=\"artist\">')[1].split('</span>')[0].strip()\n    title = request.text.split('<span class=\"artist\">')[1].split('</span>')[1].split('</h1>')[0].strip()\n    artwork_url = request.text.split('<a class=\"lightbox-trigger\" href=\"')[1].split('\" data')[0].strip()\n\n    data['mp3_url'] = mp3_url\n    data['title'] = title\n    data['artist'] = artist\n    data['artwork_url'] = artwork_url\n    data['year'] = None\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_hive(vargs):\n\n    artist_url = vargs['artist_url']\n\n    if 'hive.co' in artist_url:\n        mc_url = artist_url\n    else:\n        mc_url = 'https://www.hive.co/downloads/download/' + artist_url\n\n    filenames = scrape_hive_url(mc_url, num_tracks=vargs['num_tracks'], folders=vargs['folders'], custom_path=vargs['path'])\n\n    if vargs['open']:\n        open_files(filenames)\n\n    return", "response": "Process the main Hive. co path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scrape_hive_url(mc_url, num_tracks=sys.maxsize, folders=False, custom_path=''):\n\n    try:\n        data = get_hive_data(mc_url)\n    except Exception as e:\n        puts_safe(colored.red(\"Problem downloading \") + mc_url)\n        print(e)\n\n    filenames = []\n\n    # track_artist = sanitize_filename(data['artist'])\n    # track_title = sanitize_filename(data['title'])\n    # track_filename = track_artist + ' - ' + track_title + '.mp3'\n\n    # if folders:\n    #     track_artist_path = join(custom_path, track_artist)\n    #     if not exists(track_artist_path):\n    #         mkdir(track_artist_path)\n    #     track_filename = join(track_artist_path, track_filename)\n    #     if exists(track_filename):\n    #         puts_safe(colored.yellow(\"Skipping\") + colored.white(': ' + data['title'] + \" - it already exists!\"))\n    #         return []\n\n    # puts_safe(colored.green(\"Downloading\") + colored.white(': ' + data['artist'] + \" - \" + data['title']))\n    # download_file(data['mp3_url'], track_filename)\n    # tag_file(track_filename,\n    #         artist=data['artist'],\n    #         title=data['title'],\n    #         year=data['year'],\n    #         genre=None,\n    #         artwork_url=data['artwork_url'])\n    # filenames.append(track_filename)\n\n    return filenames", "response": "Scrape a Hive. co download page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_musicbed(vargs):\n\n    # let's validate given MusicBed url\n    validated = False\n    if vargs['artist_url'].startswith( 'https://www.musicbed.com/' ):\n        splitted = vargs['artist_url'][len('https://www.musicbed.com/'):].split( '/' )\n        if len( splitted ) == 3:\n            if ( splitted[0] == 'artists' or splitted[0] == 'albums' or splitted[0] == 'songs' ) and splitted[2].isdigit():\n                validated = True\n\n    if not validated:\n        puts( colored.red( 'process_musicbed: you provided incorrect MusicBed url. Aborting.' ) )\n        puts( colored.white( 'Please make sure that url is either artist-url, album-url or song-url.' ) )\n        puts( colored.white( 'Example of correct artist-url: https://www.musicbed.com/artists/lights-motion/5188' ) )\n        puts( colored.white( 'Example of correct album-url:  https://www.musicbed.com/albums/be-still/2828' ) )\n        puts( colored.white( 'Example of correct song-url:   https://www.musicbed.com/songs/be-still/24540' ) )\n        return\n\n    filenames = scrape_musicbed_url(vargs['artist_url'], vargs['login'], vargs['password'], num_tracks=vargs['num_tracks'], folders=vargs['folders'], custom_path=vargs['path'])\n\n    if vargs['open']:\n        open_files(filenames)", "response": "Process a MusicBed file and return the main MusicBed file path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scrape_musicbed_url(url, login, password, num_tracks=sys.maxsize, folders=False, custom_path=''):\n\n    session = requests.Session()\n\n    response = session.get( url )\n    if response.status_code != 200:\n        puts( colored.red( 'scrape_musicbed_url: couldn\\'t open provided url. Status code: ' + str( response.status_code ) + '. Aborting.' ) )\n        session.close()\n        return []\n\n    albums = []\n    # let's determine what url type we got\n    # '/artists/' - search for and download many albums\n    # '/albums/'  - means we're downloading 1 album\n    # '/songs/'   - means 1 album as well, but we're forcing num_tracks=1 in order to download only first relevant track\n    if url.startswith( 'https://www.musicbed.com/artists/' ):\n        # a hackjob code to get a list of available albums\n        main_index = 0\n        while response.text.find( 'https://www.musicbed.com/albums/', main_index ) != -1:\n            start_index = response.text.find( 'https://www.musicbed.com/albums/', main_index )\n            end_index   = response.text.find( '\">', start_index )\n            albums.append( response.text[start_index:end_index] )\n            main_index = end_index\n    elif url.startswith( 'https://www.musicbed.com/songs/' ):\n        albums.append( url )\n        num_tracks = 1\n    else: # url.startswith( 'https://www.musicbed.com/albums/' )\n        albums.append( url )\n\n    # let's get our token and try to login (csrf_token seems to be present on every page)\n    token = response.text.split( 'var csrf_token = \"' )[1].split( '\";' )[0]\n    details = { '_token': token, 'login': login, 'password': password }\n    response = session.post( 'https://www.musicbed.com/ajax/login', data=details )\n    if response.status_code != 200:\n        puts( colored.red( 'scrape_musicbed_url: couldn\\'t login. Aborting. ' ) + colored.white( 'Couldn\\'t access login page.' ) )\n        session.close()\n        return []\n    login_response_data = demjson.decode( response.text )\n    if not login_response_data['body']['status']:\n        puts( colored.red( 'scrape_musicbed_url: couldn\\'t login. Aborting. ' ) + colored.white( 'Did you provide correct login and password?' ) )\n        session.close()\n        return []\n\n    # now let's actually scrape collected pages\n    filenames = []\n    for each_album_url in albums:\n        response = session.get( each_album_url )\n        if response.status_code != 200:\n            puts_safe( colored.red( 'scrape_musicbed_url: couldn\\'t open url: ' + each_album_url +\n                                    '. Status code: ' + str( response.status_code ) + '. Skipping.' ) )\n            continue\n\n        # actually not a JSON, but a JS object, but so far so good\n        json = response.text.split( 'App.components.SongRows = ' )[1].split( '</script>' )[0]\n        data = demjson.decode( json )\n\n        song_count = 1\n        for each_song in data['loadedSongs']:\n            if song_count > num_tracks:\n                break\n\n            try:\n                url, params = each_song['playback_url'].split( '?' )\n\n                details = dict()\n                for each_param in params.split( '&' ):\n                    name, value = each_param.split( '=' )\n                    details.update( { name: value } )\n                # musicbed warns about it if it's not fixed\n                details['X-Amz-Credential'] = details['X-Amz-Credential'].replace( '%2F', '/' )\n\n                directory = custom_path\n                if folders:\n                    sanitized_artist = sanitize_filename( each_song['album']['data']['artist']['data']['name'] )\n                    sanitized_album  = sanitize_filename( each_song['album']['data']['name'] )\n                    directory = join( directory, sanitized_artist + ' - ' + sanitized_album )\n                    if not exists( directory ):\n                        mkdir( directory )\n                filename = join( directory, str( song_count ) + ' - ' + sanitize_filename( each_song['name'] ) + '.mp3' )\n\n                if exists( filename ):\n                    puts_safe( colored.yellow( 'Skipping' ) + colored.white( ': ' + each_song['name'] + ' - it already exists!' ) )\n                    song_count += 1\n                    continue\n\n                puts_safe( colored.green( 'Downloading' ) + colored.white( ': ' + each_song['name'] ) )\n                path = download_file( url, filename, session=session, params=details )\n\n                # example of genre_string:\n                # \"<a href=\\\"https://www.musicbed.com/genres/ambient/2\\\">Ambient</a> <a href=\\\"https://www.musicbed.com/genres/cinematic/4\\\">Cinematic</a>\"\n                genres = ''\n                for each in each_song['genre_string'].split( '</a>' ):\n                    if ( each != \"\" ):\n                        genres += each.split( '\">' )[1] + '/'\n                genres = genres[:-1] # removing last '/\n\n                tag_file(path,\n                         each_song['album']['data']['artist']['data']['name'],\n                         each_song['name'],\n                         album=each_song['album']['data']['name'],\n                         year=int( each_song['album']['data']['released_at'].split( '-' )[0] ),\n                         genre=genres,\n                         artwork_url=each_song['album']['data']['imageObject']['data']['paths']['original'],\n                         track_number=str( song_count ),\n                         url=each_song['song_url'])\n\n                filenames.append( path )\n                song_count += 1\n            except:\n                puts_safe( colored.red( 'Problem downloading ' ) + colored.white( each_song['name'] ) + '. Skipping.' )\n                song_count += 1\n\n    session.close()\n\n    return filenames", "response": "Scrapes provided MusicBed url and returns a list of filenames."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading a file from the specified url to the specified path.", "response": "def download_file(url, path, session=None, params=None):\n    \"\"\"\n    Download an individual file.\n    \"\"\"\n\n    if url[0:2] == '//':\n        url = 'https://' + url[2:]\n\n    # Use a temporary file so that we don't import incomplete files.\n    tmp_path = path + '.tmp'\n\n    if session and params:\n        r = session.get( url, params=params, stream=True )\n    elif session and not params:\n        r = session.get( url, stream=True )\n    else:\n        r = requests.get(url, stream=True)\n    with open(tmp_path, 'wb') as f:\n        total_length = int(r.headers.get('content-length', 0))\n        for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length / 1024) + 1):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n                f.flush()\n\n    os.rename(tmp_path, path)\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an ID3 tag for a file.", "response": "def tag_file(filename, artist, title, year=None, genre=None, artwork_url=None, album=None, track_number=None, url=None):\n    \"\"\"\n    Attempt to put ID3 tags on a file.\n\n    Args:\n        artist (str):\n        title (str):\n        year (int):\n        genre (str):\n        artwork_url (str):\n        album (str):\n        track_number (str):\n        filename (str):\n        url (str):\n    \"\"\"\n\n    try:\n        audio = EasyMP3(filename)\n        audio.tags = None\n        audio[\"artist\"] = artist\n        audio[\"title\"] = title\n        if year:\n            audio[\"date\"] = str(year)\n        if album:\n            audio[\"album\"] = album\n        if track_number:\n            audio[\"tracknumber\"] = track_number\n        if genre:\n            audio[\"genre\"] = genre\n        if url: # saves the tag as WOAR\n            audio[\"website\"] = url\n        audio.save()\n\n        if artwork_url:\n\n            artwork_url = artwork_url.replace('https', 'http')\n\n            mime = 'image/jpeg'\n            if '.jpg' in artwork_url:\n                mime = 'image/jpeg'\n            if '.png' in artwork_url:\n                mime = 'image/png'\n\n            if '-large' in artwork_url:\n                new_artwork_url = artwork_url.replace('-large', '-t500x500')\n                try:\n                    image_data = requests.get(new_artwork_url).content\n                except Exception as e:\n                    # No very large image available.\n                    image_data = requests.get(artwork_url).content\n            else:\n                image_data = requests.get(artwork_url).content\n\n            audio = MP3(filename, ID3=OldID3)\n            audio.tags.add(\n                APIC(\n                    encoding=3,  # 3 is for utf-8\n                    mime=mime,\n                    type=3,  # 3 is for the cover image\n                    desc='Cover',\n                    data=image_data\n                )\n            )\n            audio.save()\n\n        # because there is software that doesn't seem to use WOAR we save url tag again as WXXX\n        if url:\n            audio = MP3(filename, ID3=OldID3)\n            audio.tags.add( WXXX( encoding=3, url=url ) )\n            audio.save()\n\n        return True\n\n    except Exception as e:\n        puts(colored.red(\"Problem tagging file: \") + colored.white(\"Is this file a WAV?\"))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall the system open command on a file.", "response": "def open_files(filenames):\n    \"\"\"\n    Call the system 'open' command on a file.\n    \"\"\"\n    command = ['open'] + filenames\n    process = Popen(command, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking sure filenames are valid paths.", "response": "def sanitize_filename(filename):\n    \"\"\"\n    Make sure filenames are valid paths.\n\n    Returns:\n        str:\n    \"\"\"\n    sanitized_filename = re.sub(r'[/\\\\:*?\"<>|]', '-', filename)\n    sanitized_filename = sanitized_filename.replace('&', 'and')\n    sanitized_filename = sanitized_filename.replace('\"', '')\n    sanitized_filename = sanitized_filename.replace(\"'\", '')\n    sanitized_filename = sanitized_filename.replace(\"/\", '')\n    sanitized_filename = sanitized_filename.replace(\"\\\\\", '')\n\n    # Annoying.\n    if sanitized_filename[0] == '.':\n        sanitized_filename = u'dot' + sanitized_filename[1:]\n\n    return sanitized_filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, byte_arr):\n        if byte_arr:\n            self.value = self.calculate(byte_arr, self.value)", "response": "Read bytes and update the CRC computed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes CRC for input bytes.", "response": "def calculate(cls, byte_arr, crc=0):\n        \"\"\"Compute CRC for input bytes.\"\"\"\n        for byte in byte_iter(byte_arr):\n            # Taken verbatim from FIT SDK docs\n            tmp = cls.CRC_TABLE[crc & 0xF]\n            crc = (crc >> 4) & 0x0FFF\n            crc = crc ^ tmp ^ cls.CRC_TABLE[byte & 0xF]\n\n            tmp = cls.CRC_TABLE[crc & 0xF]\n            crc = (crc >> 4) & 0x0FFF\n            crc = crc ^ tmp ^ cls.CRC_TABLE[(byte >> 4) & 0xF]\n        return crc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _scrub_method_name(self, method_name):\n        if method_name not in self._scrubbed_method_names:\n            self._scrubbed_method_names[method_name] = (\n                scrub_method_name(method_name))\n\n        return self._scrubbed_method_names[method_name]", "response": "Scrubs a method name returning result from local cache if available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_field_processor(self, field_data):\n        if field_data.name.endswith(\"_speed\"):\n            self.process_field_speed(field_data)\n        else:\n            super(StandardUnitsDataProcessor, self).run_field_processor(field_data)", "response": "Convert all '*_speed' fields using process_field_speed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a file - ihs object.", "response": "def fileish_open(fileish, mode):\n    \"\"\"\n    Convert file-ish object to BytesIO like object.\n    :param fileish: the file-ihs object (str, BytesIO, bytes, file contents)\n    :param str mode: mode for the open function.\n    :rtype: BytesIO\n    \"\"\"\n    if mode is not None and any(m in mode for m in ['+', 'w', 'a', 'x']):\n        attr = 'write'\n    else:\n        attr = 'read'\n    if hasattr(fileish, attr) and hasattr(fileish, 'seek'):\n        # BytesIO-like object\n        return fileish\n    elif isinstance(fileish, str):\n        # Python2 - file path, file contents in the case of a TypeError\n        # Python3 - file path\n        try:\n            return open(fileish, mode)\n        except TypeError:\n            return io.BytesIO(fileish)\n    else:\n        # Python 3 - file contents\n        return io.BytesIO(fileish)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notify(title,\n           message,\n           user_key,\n           api_token='aUnsraBiEZVsmrG89AZp47K3S2dX2a',\n           device=None,\n           sound=None,\n           priority=0,\n           expire=None,\n           retry=None,\n           callback=None,\n           url=None,\n           url_title=None,\n           html=False,\n           retcode=None):\n    \"\"\"\n    Required parameters:\n        * ``user_key``\n\n    Optional parameters:\n        * ``sound``\n        * ``priority``\n        * ``expire``\n        * ``retry``\n        * ``callback``\n        * ``api_token`` - use your own application token\n        * ``device`` - target a device, if omitted, target all devices\n        * ``url``\n        * ``url_title``\n        * ``html``\n    \"\"\"\n\n    data = {\n        'message': message,\n        'token': api_token,\n        'user': user_key,\n        'title': title,\n    }\n    if device:\n        data['device'] = device\n\n    if sound:\n        data['sound'] = sound\n\n    if url:\n        data['url'] = url\n\n    if url_title:\n        if not url:\n            logging.getLogger(__name__).warning(\n                'url_title specified without specifying url')\n        else:\n            data['url_title'] = url_title\n\n    if html:\n        data['html'] = 1\n\n    priority = int(priority)\n    if priority <= 2 and priority >= -2:\n        if priority != 0:\n            data['priority'] = priority\n\n        # Expire, Retry, and Callback only apply to an Emergency Priority\n        if priority == 2:\n            # Retry can not be less than 30 per the API\n            if not retry or retry < 30:\n                logging.getLogger(__name__).error(\n                    'retry is less than 30 or is not set, '\n                    'setting retry to 30 to comply with '\n                    'pushover API requirements')\n                data['retry'] = 30\n            else:\n                data['retry'] = retry\n\n            # Expire can not be more than 86400 (24 hours)\n            if not expire or expire > 86400:\n                logging.getLogger(__name__).error(\n                    'expire is greater than 86400 seconds or is not set,'\n                    'setting expire to 86400 to comply with'\n                    'pushover API requirements')\n                data['expire'] = 86400\n            elif expire <= 86400:\n                data['expire'] = expire\n\n            if callback:\n                data['callback'] = callback\n        else:\n            if retry:\n                logging.getLogger(__name__).warning(\n                    'Non-emergency, ignoring retry set in config')\n            if expire:\n                logging.getLogger(__name__).warning(\n                    'Non-emergency, ignoring expire set in config')\n            if callback:\n                logging.getLogger(__name__).warning(\n                    'Non-emergency, ignoring callback set in config')\n\n    else:\n        raise ValueError('priority must be an integer from -2 to 2')\n\n    resp = requests.post(\n        'https://api.pushover.net/1/messages.json',\n        data=data,\n        headers={\n            'User-Agent': USER_AGENT,\n        })\n\n    if resp.status_code == 429:\n        print(\"ntfy's default api_token has reached pushover's rate limit\")\n        print(\"create your own app at https://pushover.net/apps/clone/ntfy\")\n        print(\"and set api_token in your config file.\")\n        return 1\n\n    resp.raise_for_status()", "response": "Sends a notification to the specified user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef notify(title, message, retcode=None):\n    try:\n        import Foundation\n        import objc\n    except ImportError:\n        import sys\n        import logging\n\n        logger = logging.getLogger(__name__)\n        if sys.platform.startswith('darwin') and hasattr(sys, 'real_prefix'):\n            logger.error(\n                \"Using ntfy with the MacOS Notification Center doesn't \"\n                \"work within a virtualenv\")\n            sys.exit(1)\n        else:\n            raise\n\n    NSUserNotification = objc.lookUpClass('NSUserNotification')\n    NSUserNotificationCenter = objc.lookUpClass('NSUserNotificationCenter')\n\n    notification = NSUserNotification.alloc().init()\n    notification.setTitle_(title)\n    if message is not None:\n        notification.setInformativeText_(message)\n    notification.setDeliveryDate_(Foundation.NSDate.date())\n\n    NSUserNotificationCenter.defaultUserNotificationCenter()\\\n        .scheduleNotification_(notification)", "response": "Sends a notification to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notify(title,\n           message,\n           access_token,\n           device_iden=None,\n           email=None,\n           retcode=None):\n    \"\"\"\n    Required parameter:\n        * ``access_token`` - Your Pushbullet access token, created at\n            https://www.pushbullet.com/#settings/account\n\n    Optional parameters:\n        * ``device_iden`` - a device identifier, if omited, notification is\n                            sent to all devices\n        * ``email`` - send notification to pushbullte user with the specified\n                      email or send an email if they aren't a pushullet user\n    \"\"\"\n\n    data = {\n        'type': 'note',\n        'title': title,\n        'body': message,\n    }\n    if device_iden is not None:\n        data['device_iden'] = device_iden\n    if email is not None:\n        data['email'] = email\n\n    headers = {'Access-Token': access_token, 'User-Agent': USER_AGENT}\n\n    resp = requests.post(\n        'https://api.pushbullet.com/v2/pushes', data=data, headers=headers)\n\n    resp.raise_for_status()", "response": "Send a notification to all pushbullte devices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notify(title, message, key, event=None, retcode=None):\n\n    data = {\n        'title': title if len(title) <= 20 else title[:19] + u'\\u2026',\n        'msg': message,\n        'key': key\n    }\n\n    if event:\n        data['event'] = event\n\n    headers = {'User-Agent': USER_AGENT}\n\n    endpoint = \"https://api.simplepush.io\"\n\n    resp = requests.post(endpoint + '/send', data=data, headers=headers)\n\n    resp.raise_for_status()", "response": "Send a message to a SimplePush server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef notify(title, message, retcode=None, webhook=None):\n\n    logger = logging.getLogger(__name__)\n    if webhook is None:\n        logger.error('please set webhook variable under '\n                     'notifico backend of the config file')\n        return\n    response = requests.get(\n        webhook,\n        params={\n            'payload': '{title}\\n{message}'.format(\n                title=title, message=message)\n        })\n    response.raise_for_status()", "response": "Send a notification to the user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend an Instapush notification to the specified trackers.", "response": "def notify(title, message, event_name, appid, secret, trackers, retcode=None):\n    \"\"\"\n    Required parameter:\n        * ``event_name`` - Instapush event (the notification template)\n        * ``appid`` - The appid found on the dashboard\n        * ``secret`` - The secret found on the dashboard\n        * ``traskers`` - List of the placeholders for the selected event\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    _msgs = re.split(r'(?<!\\\\):', message)\n    msgs = []\n\n    for msg in _msgs:\n        msg = msg.replace(\"\\\\:\", \":\")\n        msgs.append(msg)\n\n    if len(msgs) != len(trackers):\n        logger.error(('Wrong number of messages! There are {} trackers so you '\n                      'have to provide {} messages. Remember to separate each '\n                      'message with \\':\\' (example: send \"msg1:msg2\")').format(\n                          len(trackers), len(trackers)))\n        raise WrongMessageCountException()\n\n    to_send = {}\n\n    for tracker, msg in zip(trackers, msgs):\n        to_send[tracker] = msg\n\n    app = App(appid=appid, secret=secret)\n    res = app.notify(event_name=event_name, trackers=to_send)\n\n    if res[\"status\"] != 200:\n        logger.error(res[\"msg\"])\n        raise ApiException()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to the system logger.", "response": "def notify(title,\n           message,\n           prio='ALERT',\n           facility='LOCAL5',\n           fmt='[{title}] {message}',\n           retcode=None):\n    \"\"\"\n    Uses the ``syslog`` core Python module, which is not available on Windows\n    platforms.\n\n    Optional parameters:\n        * ``prio`` - Syslog prority level.  Default is ``ALERT``.  Possible\n          values are:\n\n          * EMERG\n          * ALERT\n          * CRIT\n          * ERR\n          * WARNING\n          * NOTICE\n          * INFO\n          * DEBUG\n\n        * ``facility`` - Syslog facility.  Default is ``LOCAL5``.  Possible\n          values are:\n\n          * KERN\n          * USER\n          * MAIL\n          * DAEMON\n          * AUTH\n          * LPR\n          * NEWS\n          * UUCP\n          * CRON\n          * SYSLOG\n          * LOCAL0\n          * LOCAL1\n          * LOCAL2\n          * LOCAL3\n          * LOCAL4\n          * LOCAL5\n          * LOCAL6\n          * LOCAL7\n\n      * ``fmt`` - Format of the message to be sent to the system logger.  The\n        title and the message are specified using the following placeholders:\n\n        * ``{title}``\n        * ``{message}``\n\n        Default is ``[{title}] {message}``.\n    \"\"\"\n\n    prio_map = {\n        'EMERG': syslog.LOG_EMERG,\n        'ALERT': syslog.LOG_ALERT,\n        'CRIT': syslog.LOG_CRIT,\n        'ERR': syslog.LOG_ERR,\n        'WARNING': syslog.LOG_WARNING,\n        'NOTICE': syslog.LOG_NOTICE,\n        'INFO': syslog.LOG_INFO,\n        'DEBUG': syslog.LOG_DEBUG,\n    }\n\n    facility_map = {\n        'KERN': syslog.LOG_KERN,\n        'USER': syslog.LOG_USER,\n        'MAIL': syslog.LOG_MAIL,\n        'DAEMON': syslog.LOG_DAEMON,\n        'AUTH': syslog.LOG_AUTH,\n        'LPR': syslog.LOG_LPR,\n        'NEWS': syslog.LOG_NEWS,\n        'UUCP': syslog.LOG_UUCP,\n        'CRON': syslog.LOG_CRON,\n        'SYSLOG': syslog.LOG_SYSLOG,\n        'LOCAL0': syslog.LOG_LOCAL0,\n        'LOCAL1': syslog.LOG_LOCAL1,\n        'LOCAL2': syslog.LOG_LOCAL2,\n        'LOCAL3': syslog.LOG_LOCAL3,\n        'LOCAL4': syslog.LOG_LOCAL4,\n        'LOCAL5': syslog.LOG_LOCAL5,\n        'LOCAL6': syslog.LOG_LOCAL6,\n        'LOCAL7': syslog.LOG_LOCAL7,\n    }\n\n    if prio not in prio_map:\n        raise ValueError('invalid syslog priority')\n    elif facility not in facility_map:\n        raise ValueError('invalid syslog facility')\n\n    msg = fmt.format(title=title, message=message)\n    for line in msg.splitlines():\n        syslog.syslog(facility_map[facility] | prio_map[prio], line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notify(title,\n           message,\n           jid,\n           password,\n           recipient,\n           hostname=None,\n           port=5222,\n           path_to_certs=None,\n           mtype=None,\n           retcode=None):\n    \"\"\"\n    Optional parameters\n        * ``hostname`` (if not from jid)\n        * ``port``\n        * ``path_to_certs``\n        * ``mtype`` ('chat' required for Google Hangouts)\n\n    To verify the SSL certificates offered by a server:\n    path_to_certs = \"path/to/ca/cert\"\n\n    Without dnspython library installed, you will need\n    to specify the server hostname if it doesn't match the jid.\n\n    For example, to use Google Talk you would need to use:\n    hostname = 'talk.google.com'\n\n    Specify port if other than 5222.\n    NOTE: Ignored without specified hostname\n    \"\"\"\n\n    xmpp_bot = NtfySendMsgBot(jid, password, recipient, title, message, mtype)\n\n    # NOTE: Below plugins weren't needed for Google Hangouts\n    # but may be useful (from original sleekxmpp example)\n    # xmpp_bot.register_plugin('xep_0030') # Service Discovery\n    # xmpp_bot.register_plugin('xep_0199') # XMPP Ping\n\n    if path_to_certs and os.path.isdir(path_to_certs):\n        xmpp_bot.ca_certs = path_to_certs\n\n    # Connect to the XMPP server and start processing XMPP stanzas.\n    if xmpp_bot.connect(*([(hostname, int(port)) if hostname else []])):\n        xmpp_bot.process(block=True)\n    else:\n        logging.getLogger(__name__).error('Unable to connect', exc_info=True)", "response": "Send a message to the XMPP server and return a new XMPP object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef notify(title,\n           message,\n           api_key=NTFY_API_KEY,\n           provider_key=None,\n           priority=0,\n           url=None,\n           retcode=None):\n    \"\"\"\n    Optional parameters:\n        * ``api_key`` - use your own application token\n        * ``provider_key`` - if you are whitelisted\n        * ``priority``\n        * ``url``\n    \"\"\"\n\n    data = {\n        'apikey': api_key,\n        'application': 'ntfy',\n        'event': title,\n        'description': message,\n    }\n\n    if MIN_PRIORITY <= priority <= MAX_PRIORITY:\n        data['priority'] = priority\n    else:\n        raise ValueError('priority must be an integer from {:d} to {:d}'\n                         .format(MIN_PRIORITY, MAX_PRIORITY))\n\n    if url is not None:\n        data['url'] = url\n\n    if provider_key is not None:\n        data['providerkey'] = provider_key\n\n    resp = requests.post(\n        API_URL, data=data, headers={\n            'User-Agent': USER_AGENT,\n        })\n\n    resp.raise_for_status()", "response": "Send a notification to the NTFY server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to the pushalot server.", "response": "def notify(title,\n           message,\n           auth_token,\n           source=None,\n           url=None,\n           url_title=None,\n           image=None,\n           ttl=None,\n           important=False,\n           silent=False,\n           retcode=None):\n    \"\"\"\n    Required parameters:\n        * ``auth_token``\n\n    Optional parameters:\n        * ``source``\n        * ``url``\n        * ``url_title``\n        * ``image``\n        * ``ttl``\n        * ``important``\n        * ``silent``\n    \"\"\"\n\n    data = {\n        'Title': title,\n        'Body': message,\n        'AuthorizationToken': auth_token,\n    }\n\n    if source:\n        data['Source'] = source\n    if url:\n        data['Link'] = url\n    if url and url_title:\n        data['LinkTitle'] = url_title\n    if image:\n        data['Image'] = image\n    if ttl:\n        data['TimeToLive'] = int(ttl)\n    if important:\n        data['IsImportant'] = 'True'\n    if silent:\n        data['IsSilent'] = 'True'\n\n    headers = {'User-Agent': USER_AGENT}\n    response = requests.post(PUSHALOT_API_URL, data=data, headers=headers)\n    response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notify(title, message, **kwargs):\n    for os in ['linux', 'win32', 'darwin']:\n        if platform.startswith(os):\n            module = import_module('ntfy.backends.{}'.format(os))\n            try:\n                module.notify(title=title, message=message, **kwargs)\n            except Exception as e:\n                raise DefaultNotifierError(e, module)\n            break", "response": "Sends a notification to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message over Telegram using telegram - send.", "response": "def notify(title, message, retcode=None):\n    \"\"\"Sends message over Telegram using telegram-send, title is ignored.\"\"\"\n    if not path.exists(config_file):\n        if not path.exists(config_dir):\n            makedirs(config_dir)\n        print(\"Follow the instructions to configure the Telegram backend.\\n\")\n        configure(config_file)\n    send(messages=[message], conf=config_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notify(title, message, icon=icon.ico, retcode=None):\n\n    import win32api\n    import win32con\n    import win32gui\n\n    class WindowsBalloonTip:\n        def __init__(self, title, msg):\n            message_map = {\n                win32con.WM_DESTROY: self.OnDestroy,\n            }\n            # Register the Window class.\n            wc = win32gui.WNDCLASS()\n            hinst = wc.hInstance = win32api.GetModuleHandle(None)\n            wc.lpszClassName = \"PythonTaskbar\"\n            wc.lpfnWndProc = message_map  # could also specify a wndproc.\n            classAtom = win32gui.RegisterClass(wc)\n            # Create the Window.\n            style = win32con.WS_OVERLAPPED | win32con.WS_SYSMENU\n            self.hwnd = win32gui.CreateWindow(\n                classAtom, \"Taskbar\", style, 0, 0, win32con.CW_USEDEFAULT,\n                win32con.CW_USEDEFAULT, 0, 0, hinst, None)\n            win32gui.UpdateWindow(self.hwnd)\n            iconPathName = os.path.abspath(icon)\n            icon_flags = win32con.LR_LOADFROMFILE | win32con.LR_DEFAULTSIZE\n            try:\n                hicon = win32gui.LoadImage(\n                    hinst, iconPathName, win32con.IMAGE_ICON, 0, 0, icon_flags)\n            except:\n                hicon = win32gui.LoadIcon(0, win32con.IDI_APPLICATION)\n            flags = win32gui.NIF_ICON | win32gui.NIF_MESSAGE | win32gui.NIF_TIP\n            nid = (self.hwnd, 0, flags, win32con.WM_USER + 20, hicon,\n                   \"tooltip\")\n            win32gui.Shell_NotifyIcon(win32gui.NIM_ADD, nid)\n            win32gui.Shell_NotifyIcon(\n                win32gui.NIM_MODIFY,\n                (self.hwnd, 0, win32gui.NIF_INFO, win32con.WM_USER + 20, hicon,\n                 \"Balloon tooltip\", title, 200, msg),\n            )\n            win32gui.DestroyWindow(self.hwnd)\n            win32gui.UnregisterClass(wc.lpszClassName, None)\n\n        def OnDestroy(self, hwnd, msg, wparam, lparam):\n            win32api.PostQuitMessage(0)  # Terminate the app.\n\n    WindowsBalloonTip(message, title)", "response": "Create a new Taskbar with the given title and message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a message to the Pushjet service.", "response": "def notify(title,\n           message,\n           secret,\n           endpoint=None,\n           level=3,\n           link=None,\n           retcode=None):\n    \"\"\"\n    Required parameter:\n        * ``secret`` - The Pushjet service secret token, created with\n            http://docs.pushjet.io/docs/creating-a-new-service\n\n    Optional parameters:\n        * ``endpoint`` - custom Pushjet API endpoint\n            (defaults to https://api.pushjet.io)\n        * ``level`` - The importance level from 1(low) to 5(high)\n        * ``link``\n    \"\"\"\n\n    data = {\n        'title': title,\n        'message': message,\n        'level': level,\n        'secret': secret,\n    }\n\n    if link:\n        data['link'] = link\n\n    headers = {'User-Agent': USER_AGENT}\n\n    if endpoint is None:\n        endpoint = 'https://api.pushjet.io'\n\n    resp = requests.post(endpoint + '/message', data=data, headers=headers)\n\n    resp.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rainbow(n=1000, saturation=1, value=1):\n    for i in range(n):\n        hue = i / float(n)\n        color = [int(x * 255) for x in colorsys.hsv_to_rgb(hue, saturation, value)]\n        yield (\"#%02x%02x%02x\" % tuple(color)).upper()", "response": "A generator that yields n hues from the rainbow in the hex format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef minute_change(device):\n    '''When we reach a minute change, animate it.'''\n    hours = datetime.now().strftime('%H')\n    minutes = datetime.now().strftime('%M')\n\n    def helper(current_y):\n        with canvas(device) as draw:\n            text(draw, (0, 1), hours, fill=\"white\", font=proportional(CP437_FONT))\n            text(draw, (15, 1), \":\", fill=\"white\", font=proportional(TINY_FONT))\n            text(draw, (17, current_y), minutes, fill=\"white\", font=proportional(CP437_FONT))\n        time.sleep(0.1)\n    for current_y in range(1, 9):\n        helper(current_y)\n    minutes = datetime.now().strftime('%M')\n    for current_y in range(9, 1, -1):\n        helper(current_y)", "response": "When we reach a minute change animate it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nanimates the whole thing moving it into or out of the abyss.", "response": "def animation(device, from_y, to_y):\n    '''Animate the whole thing, moving it into/out of the abyss.'''\n    hourstime = datetime.now().strftime('%H')\n    mintime = datetime.now().strftime('%M')\n    current_y = from_y\n    while current_y != to_y:\n        with canvas(device) as draw:\n            text(draw, (0, current_y), hourstime, fill=\"white\", font=proportional(CP437_FONT))\n            text(draw, (15, current_y), \":\", fill=\"white\", font=proportional(TINY_FONT))\n            text(draw, (17, current_y), mintime, fill=\"white\", font=proportional(CP437_FONT))\n        time.sleep(0.1)\n        current_y += 1 if to_y > from_y else -1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clock(seg, seconds):\n    interval = 0.5\n    for i in range(int(seconds / interval)):\n        now = datetime.now()\n        seg.text = now.strftime(\"%H-%M-%S\")\n\n        # calculate blinking dot\n        if i % 2 == 0:\n            seg.text = now.strftime(\"%H-%M-%S\")\n        else:\n            seg.text = now.strftime(\"%H %M %S\")\n\n        time.sleep(interval)", "response": "Display current time on device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms the inherited behviour and the LED matrix.", "response": "def preprocess(self, image):\n        \"\"\"\n        Performs the inherited behviour (if any), and if the LED matrix\n        orientation is declared to need correction, each 8x8 block of pixels\n        is rotated 90\u00b0 clockwise or counter-clockwise.\n        \"\"\"\n        image = super(max7219, self).preprocess(image)\n\n        if self._correction_angle != 0:\n            image = image.copy()\n            for y in range(0, self._h, 8):\n                for x in range(0, self._w, 8):\n                    box = (x, y, x + 8, y + 8)\n                    rotated_block = image.crop(box).rotate(self._correction_angle)\n                    image.paste(rotated_block, box)\n        if self.blocks_arranged_in_reverse_order:\n            old_image = image.copy()\n            for y in range(8):\n                for x in range(8):\n                    for i in range(self.cascaded):\n                        image.putpixel((8 * (self.cascaded - 1) - i * 8 + x, y), old_image.getpixel((i * 8 + x, y)))\n\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display(self, image):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        image = self.preprocess(image)\n\n        i = 0\n        d0 = self._const.DIGIT_0\n        step = 2 * self.cascaded\n        offsets = self._offsets\n        rows = self._rows\n\n        buf = bytearray(8 * step)\n        pix = list(image.getdata())\n\n        for digit in range(8):\n            for daisychained_device in offsets:\n                byte = 0\n                idx = daisychained_device + digit\n                for y in rows:\n                    if pix[idx] > 0:\n                        byte |= 1 << y\n                    idx += self._w\n\n                buf[i] = digit + d0\n                buf[i + 1] = byte\n                i += 2\n\n        buf = list(buf)\n        for i in range(0, len(buf), step):\n            self.data(buf[i:i + step])", "response": "Displays the image as a LED matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contrast(self, value):\n        assert(0x00 <= value <= 0xFF)\n        self.data([self._const.INTENSITY, value >> 4] * self.cascaded)", "response": "Sets the contrast level of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef display(self, image):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        ws = self._ws\n        m = self._mapping\n        for idx, (red, green, blue) in enumerate(image.getdata()):\n            color = (red << 16) | (green << 8) | blue\n            ws.ws2811_led_set(self._channel, m[idx], color)\n\n        self._flush()", "response": "Displays the image to the neopixels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimulate switching the display mode ON.", "response": "def show(self):\n        \"\"\"\n        Simulates switching the display mode ON; this is achieved by restoring\n        the contrast to the level prior to the last time hide() was called.\n        \"\"\"\n        if self._prev_contrast is not None:\n            self.contrast(self._prev_contrast)\n            self._prev_contrast = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the display mode OFF.", "response": "def hide(self):\n        \"\"\"\n        Simulates switching the display mode OFF; this is achieved by setting\n        the contrast level to zero.\n        \"\"\"\n        if self._prev_contrast is None:\n            self._prev_contrast = self._contrast\n            self.contrast(0x00)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the contrast level of the current channel.", "response": "def contrast(self, value):\n        \"\"\"\n        Sets the LED intensity to the desired level, in the range 0-255.\n\n        :param level: Desired contrast level in the range of 0-255.\n        :type level: int\n        \"\"\"\n        assert(0x00 <= value <= 0xFF)\n        self._contrast = value\n        self._ws.ws2811_channel_t_brightness_set(self._channel, value)\n        self._flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean up the internal state of the object.", "response": "def cleanup(self):\n        \"\"\"\n        Attempt to reset the device & switching it off prior to exiting the\n        python process.\n        \"\"\"\n        self.hide()\n        self.clear()\n\n        if self._leds is not None:\n            self._ws.ws2811_fini(self._leds)\n            self._ws.delete_ws2811_t(self._leds)\n            self._leds = None\n            self._channel = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef display(self, image):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n        self._last_image = image.copy()\n\n        # Send zeros to reset, then pixel values then zeros at end\n        sz = image.width * image.height * 4\n        buf = bytearray(sz * 3)\n\n        m = self._mapping\n        for idx, (r, g, b, a) in enumerate(image.getdata()):\n            offset = sz + m[idx] * 4\n            brightness = (a >> 4) if a != 0xFF else self._brightness\n            buf[offset] = (0xE0 | brightness)\n            buf[offset + 1] = b\n            buf[offset + 2] = g\n            buf[offset + 3] = r\n\n        self._serial_interface.data(list(buf))", "response": "Takes an image and dumps it to the daisy - chained APA102 neopixels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contrast(self, value):\n        assert(0x00 <= value <= 0xFF)\n        self._brightness = value >> 4\n        if self._last_image is not None:\n            self.display(self._last_image)", "response": "Sets the contrast level of the current LED."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrotates the image 90 degrees", "response": "def rotate_image_180():\n    ''' Rotate the image '''\n\n    # Create the media service\n    mycam = ONVIFCamera('192.168.0.112', 80, 'admin', '12345')\n    media_service = mycam.create_media_service()\n\n    profiles = media_service.GetProfiles()\n\n    # Use the first profile and Profiles have at least one\n    token = profiles[0]._token\n\n    # Get all video source configurations\n    configurations_list = media_service.GetVideoSourceConfigurations()\n\n    # Use the first profile and Profiles have at least one\n    video_source_configuration = configurations_list[0]\n\n    # Enable rotate\n    video_source_configuration.Extension[0].Rotate[0].Mode[0] = 'OFF'\n\n    # Create request type instance\n    request = media_service.create_type('SetVideoSourceConfiguration')\n    request.Configuration = video_source_configuration\n\n    # ForcePersistence is obsolete and should always be assumed to be True\n    request.ForcePersistence = True\n\n    # Set the video source configuration\n    media_service.SetVideoSourceConfiguration(request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef media_profile_configuration():\n    '''\n    A media profile consists of configuration entities such as video/audio\n    source configuration, video/audio encoder configuration,\n    or PTZ configuration. This use case describes how to change one\n    configuration entity which has been already added to the media profile.\n    '''\n\n    # Create the media service\n    mycam = ONVIFCamera('192.168.0.112', 80, 'admin', '12345')\n    media_service = mycam.create_media_service()\n\n    profiles = media_service.GetProfiles()\n\n    # Use the first profile and Profiles have at least one\n    token = profiles[0]._token\n\n    # Get all video encoder configurations\n    configurations_list = media_service.GetVideoEncoderConfigurations()\n\n    # Use the first profile and Profiles have at least one\n    video_encoder_configuration = configurations_list[0]\n\n    # Get video encoder configuration options\n    options = media_service.GetVideoEncoderConfigurationOptions({'ProfileToken':token})\n\n    # Setup stream configuration\n    video_encoder_configuration.Encoding = 'H264'\n    # Setup Resolution\n    video_encoder_configuration.Resolution.Width = \\\n                    options.H264.ResolutionsAvailable[0].Width\n    video_encoder_configuration.Resolution.Height = \\\n                    options.H264.ResolutionsAvailable[0].Height\n    # Setup Quality\n    video_encoder_configuration.Quality = options.QualityRange.Min\n    # Setup FramRate\n    video_encoder_configuration.RateControl.FrameRateLimit = \\\n                                    options.H264.FrameRateRange.Min\n    # Setup EncodingInterval\n    video_encoder_configuration.RateControl.EncodingInterval = \\\n                                    options.H264.EncodingIntervalRange.Min\n    # Setup Bitrate\n    video_encoder_configuration.RateControl.BitrateLimit = \\\n                            options.Extension.H264[0].BitrateRange[0].Min[0]\n\n    # Create request type instance\n    request = media_service.create_type('SetVideoEncoderConfiguration')\n    request.Configuration = video_encoder_configuration\n    # ForcePersistence is obsolete and should always be assumed to be True\n    request.ForcePersistence = True\n\n    # Set the video encoder configuration\n    media_service.SetVideoEncoderConfiguration(request)", "response": "A function to create a new one - order media profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_definition(self, name):\n        '''Returns xaddr and wsdl of specified service'''\n        # Check if the service is supported\n        if name not in SERVICES:\n            raise ONVIFError('Unknown service %s' % name)\n        wsdl_file = SERVICES[name]['wsdl']\n        ns = SERVICES[name]['ns']\n\n        wsdlpath = os.path.join(self.wsdl_dir, wsdl_file)\n        if not os.path.isfile(wsdlpath):\n            raise ONVIFError('No such file: %s' % wsdlpath)\n\n        # XAddr for devicemgmt is fixed:\n        if name == 'devicemgmt':\n            xaddr = 'http://%s:%s/onvif/device_service' % (self.host, self.port)\n            return xaddr, wsdlpath\n\n        # Get other XAddr\n        xaddr = self.xaddrs.get(ns)\n        if not xaddr:\n            raise ONVIFError('Device doesn`t support service: %s' % name)\n\n        return xaddr, wsdlpath", "response": "Returns xaddr and wsdl of specified service"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_onvif_service(self, name, from_template=True, portType=None):\n        '''Create ONVIF service client'''\n\n        name = name.lower()\n        xaddr, wsdl_file = self.get_definition(name)\n\n        with self.services_lock:\n            svt = self.services_template.get(name)\n            # Has a template, clone from it. Faster.\n            if svt and from_template and self.use_services_template.get(name):\n                service = ONVIFService.clone(svt, xaddr, self.user,\n                                             self.passwd, wsdl_file,\n                                             self.cache_location,\n                                             self.cache_duration,\n                                             self.encrypt,\n                                             self.daemon,\n                                             no_cache=self.no_cache, portType=portType, dt_diff=self.dt_diff)\n            # No template, create new service from wsdl document.\n            # A little time-comsuming\n            else:\n                service = ONVIFService(xaddr, self.user, self.passwd,\n                                       wsdl_file, self.cache_location,\n                                       self.cache_duration, self.encrypt,\n                                       self.daemon, no_cache=self.no_cache, portType=portType, dt_diff=self.dt_diff)\n\n            self.services[name] = service\n\n            setattr(self, name, service)\n            if not self.services_template.get(name):\n                self.services_template[name] = service\n\n        return service", "response": "Create ONVIF service client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self, args):\n        ''' `args`: Instance of `argparse.ArgumentParser` '''\n        # Create onvif camera client\n        self.client = ONVIFCamera(args.host, args.port,\n                                  args.user, args.password,\n                                  args.wsdl, encrypt=args.encrypt)\n\n\n        # Create cmd argument parser\n        self.create_cmd_parser()", "response": "Setup the ONVIFCamera and cmd parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_cmd(self, line):\n        '''Usage: CMD service operation [parameters]'''\n        try:\n            args = self.cmd_parser.parse_args(line.split())\n        except ValueError as err:\n            return error(err)\n\n        # Check if args.service is valid\n        if args.service not in SUPPORTED_SERVICES:\n            return error('No Service: ' + args.service)\n\n        args.params = ''.join(args.params)\n        # params is optional\n        if not args.params.strip():\n            args.params = '{}'\n\n        # params must be a dictionary format string\n        match = re.match(r\"^.*?(\\{.*\\}).*$\", args.params)\n        if not match:\n            return error('Invalid params')\n\n        try:\n            args.params = dict(literal_eval(match.group(1)))\n        except ValueError as err:\n            return error('Invalid params')\n\n        try:\n            # Get ONVIF service\n            service = self.client.get_service(args.service)\n            # Actually execute the command and get the response\n            response = getattr(service, args.operation)(args.params)\n        except MethodNotFound as err:\n            return error('No Operation: %s' % args.operation)\n        except Exception as err:\n            return error(err)\n\n        if isinstance(response, (Text, bool)):\n            return success(response)\n        # Try to convert instance to dictionary\n        try:\n            success(ONVIFService.to_dict(response))\n        except ONVIFError:\n            error({})", "response": "Command line interface to ONVIF service get and execute the command"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the rectangles dictionary for the current node.", "response": "def build_rectangles(self, rectangles):\n        \"\"\" Process data to construct rectangles\n\n        This method is built from the assumption that the rectangles parameter\n        is a list of:\n            lists :a list with 4 elements indicating [north, west, south, east]\n            tuples:a tuple with 4 elements indicating (north, west, south,east)\n            tuple of tuples: a tuple of 2 tuple elements of length 2 indicating\n            (north_west, south_east)\n            dicts: a dictionary with rectangle attributes\n\n        So, for instance, we have this general scenario as a input parameter:\n            [[22.345,45.44,23.345, 45.55],\n             (22.345,45.44,23.345,45.55),\n             ((22.345,45.44),(23.345,45.55)),\n             [(22.345,45.44),(23.345,45.55)],\n             {\n            'stroke_color': stroke_color,\n            'stroke_opacity': stroke_opacity,\n            'stroke_weight': stroke_weight,\n            'fill_color': fill_color,\n            'fill_opacity': fill_opacity,\n            'bounds': {'north': north,\n                       'east': east,\n                       'south': south,\n                       'west': west,\n                       }\n            }]\n        \"\"\"\n\n        if not rectangles:\n            return\n        if not isinstance(rectangles, list):\n            raise AttributeError('rectangles only accept lists as parameters')\n        for rect in rectangles:\n\n            # Check the instance of one rectangle in the list. Can be\n            # list, tuple or dict\n            if isinstance(rect, (list, tuple)):\n\n                # If the rectangle bounds doesn't have size 4 or 2\n                # an AttributeError is raised\n                if len(rect) not in (2, 4):\n                    raise AttributeError('The bound must have length'\n                                         ' 4 or 2')\n\n                # If the tuple or list has size 4, the bounds order are\n                # especified as north, west, south, east\n                if len(rect) == 4:\n                    rect_dict = self.build_rectangle_dict(*rect)\n                    self.add_rectangle(**rect_dict)\n\n                # Otherwise size 2, the tuple or list have the north_west and\n                # south_east tuples. If the tuples doesn't have the correct\n                # size, an AttributeError is raised.\n                elif len(rect) == 2:\n                    if len(rect[0]) != 2 or len(rect[1]) != 2:\n                        raise AttributeError('Wrong size of rectangle bounds')\n                    rect_dict = self.build_rectangle_dict(rect[0][0],\n                                                          rect[0][1],\n                                                          rect[1][0],\n                                                          rect[1][1])\n                    self.add_rectangle(**rect_dict)\n                else:\n                    raise AttributeError('Wrong bounds input size')\n            elif isinstance(rect, dict):\n                self.add_rectangle(**rect)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_rectangle_dict(self,\n                             north,\n                             west,\n                             south,\n                             east,\n                             stroke_color='#FF0000',\n                             stroke_opacity=.8,\n                             stroke_weight=2,\n                             fill_color='#FF0000',\n                             fill_opacity=.3,\n                             ):\n        \"\"\" Set a dictionary with the javascript class Rectangle parameters\n\n        This function sets a default drawing configuration if the user just\n        pass the rectangle bounds, but also allows to set each parameter\n        individually if the user wish so.\n\n        Args:\n            north (float): The north latitude bound\n            west (float): The west longitude bound\n            south (float): The south latitude bound\n            east (float): The east longitude bound\n            stroke_color (str): Sets the color of the rectangle border using\n                hexadecimal color notation\n            stroke_opacity (float): Sets the opacity of the rectangle border\n                in percentage. If stroke_opacity = 0, the border is transparent\n            stroke_weight (int): Sets the stroke girth in pixels.\n            fill_color (str): Sets the color of the rectangle fill using\n                hexadecimal color notation\n            fill_opacity (float): Sets the opacity of the rectangle fill\n        \"\"\"\n        rectangle = {\n            'stroke_color': stroke_color,\n            'stroke_opacity': stroke_opacity,\n            'stroke_weight': stroke_weight,\n            'fill_color': fill_color,\n            'fill_opacity': fill_opacity,\n            'bounds': {'north': north,\n                       'west': west,\n                       'south': south,\n                       'east': east,\n                       }\n        }\n\n        return rectangle", "response": "Sets a dictionary with the javascript class Rectangle parameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a rectangle dict to the Map. rectangles attribute.", "response": "def add_rectangle(self,\n                      north=None,\n                      west=None,\n                      south=None,\n                      east=None,\n                      **kwargs):\n        \"\"\" Adds a rectangle dict to the Map.rectangles attribute\n\n        The Google Maps API describes a rectangle using the LatLngBounds\n        object, which defines the bounds to be drawn. The bounds use the\n        concept of 2 delimiting points, a northwest and a southeast points,\n        were each coordinate is defined by each parameter.\n\n        It accepts a rectangle dict representation as well.\n\n        Args:\n            north (float): The north latitude\n            west (float): The west longitude\n            south (float): The south latitude\n            east (float): The east longitude\n\n        .. _LatLngBoundsLiteral:\n            https://developers.google.com/maps/documen\n            tation/javascript/reference#LatLngBoundsLiteral\n\n        .. _Rectangles:\n            https://developers.google.com/maps/documen\n            tation/javascript/shapes#rectangles\n        \"\"\"\n        kwargs.setdefault('bounds', {})\n\n        if north:\n            kwargs['bounds']['north'] = north\n        if west:\n            kwargs['bounds']['west'] = west\n        if south:\n            kwargs['bounds']['south'] = south\n        if east:\n            kwargs['bounds']['east'] = east\n\n        if set(\n            ('north', 'east', 'south', 'west')\n        ) != set(kwargs['bounds'].keys()):\n            raise AttributeError('rectangle bounds required to rectangles')\n\n        kwargs.setdefault('stroke_color', '#FF0000')\n        kwargs.setdefault('stroke_opacity', .8)\n        kwargs.setdefault('stroke_weight', 2)\n        kwargs.setdefault('fill_color', '#FF0000')\n        kwargs.setdefault('fill_opacity', .3)\n\n        self.rectangles.append(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_circles(self, circles):\n        if not circles:\n            return\n        if not isinstance(circles, list):\n            raise AttributeError('circles accepts only lists')\n\n        for circle in circles:\n            if isinstance(circle, dict):\n                self.add_circle(**circle)\n            elif isinstance(circle, (tuple, list)):\n                if len(circle) != 3:\n                    raise AttributeError('circle requires center and radius')\n                circle_dict = self.build_circle_dict(circle[0],\n                                                     circle[1],\n                                                     circle[2])\n                self.add_circle(**circle_dict)", "response": "This method builds the rectangles for the specified circles."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_circle_dict(self,\n                          center_lat,\n                          center_lng,\n                          radius,\n                          stroke_color='#FF0000',\n                          stroke_opacity=.8,\n                          stroke_weight=2,\n                          fill_color='#FF0000',\n                          fill_opacity=.3,\n                          ):\n        \"\"\" Set a dictionary with the javascript class Circle parameters\n\n        This function sets a default drawing configuration if the user just\n        pass the rectangle bounds, but also allows to set each parameter\n        individually if the user wish so.\n\n        Args:\n            center_lat (float): The circle center latitude\n            center_lng (float): The circle center longitude\n            radius  (float): The circle radius, in meters\n            stroke_color (str): Sets the color of the rectangle border using\n                hexadecimal color notation\n            stroke_opacity (float): Sets the opacity of the rectangle border\n                in percentage. If stroke_opacity = 0, the border is transparent\n            stroke_weight (int): Sets the stroke girth in pixels.\n            fill_color (str): Sets the color of the circle fill using\n                hexadecimal color notation\n            fill_opacity (float): Sets the opacity of the circle fill\n        \"\"\"\n\n        circle = {\n            'stroke_color': stroke_color,\n            'stroke_opacity': stroke_opacity,\n            'stroke_weight': stroke_weight,\n            'fill_color': fill_color,\n            'fill_opacity': fill_opacity,\n            'center': {'lat': center_lat,\n                       'lng': center_lng},\n            'radius': radius,\n        }\n\n        return circle", "response": "Builds a dictionary with the javascript class Circle parameters"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a circle dict to the Map. circles attribute.", "response": "def add_circle(self,\n                   center_lat=None,\n                   center_lng=None,\n                   radius=None,\n                   **kwargs):\n        \"\"\" Adds a circle dict to the Map.circles attribute\n\n        The circle in a sphere is called \"spherical cap\" and is defined in the\n        Google Maps API by at least the center coordinates and its radius, in\n        meters. A circle has color and opacity both for the border line and the\n        inside area.\n\n        It accepts a circle dict representation as well.\n\n        Args:\n            center_lat (float): The circle center latitude\n            center_lng (float): The circle center longitude\n            radius  (float): The circle radius, in meters\n\n        .. _Circle:\n            https://developers.google.com/maps/documen\n            tation/javascript/reference#Circle\n        \"\"\"\n\n        kwargs.setdefault('center', {})\n        if center_lat:\n            kwargs['center']['lat'] = center_lat\n        if center_lng:\n            kwargs['center']['lng'] = center_lng\n        if radius:\n            kwargs['radius'] = radius\n\n        if set(('lat', 'lng')) != set(kwargs['center'].keys()):\n            raise AttributeError('circle center coordinates required')\n        if 'radius' not in kwargs:\n            raise AttributeError('circle radius definition required')\n\n        kwargs.setdefault('stroke_color', '#FF0000')\n        kwargs.setdefault('stroke_opacity', .8)\n        kwargs.setdefault('stroke_weight', 2)\n        kwargs.setdefault('fill_color', '#FF0000')\n        kwargs.setdefault('fill_opacity', .3)\n\n        self.circles.append(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_polylines(self, polylines):\n        if not polylines:\n            return\n        if not isinstance(polylines, (list, tuple)):\n            raise AttributeError('A list or tuple of polylines is required')\n\n        for points in polylines:\n            if isinstance(points, dict):\n                self.add_polyline(**points)\n            elif isinstance(points, (tuple, list)):\n                path = []\n                for coords in points:\n                    if len(coords) != 2:\n                        raise AttributeError('A point needs two coordinates')\n                    path.append({'lat': coords[0],\n                                 'lng': coords[1]})\n                polyline_dict = self.build_polyline_dict(path)\n                self.add_polyline(**polyline_dict)", "response": "Builds the polylines for the current base class"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_polyline_dict(self,\n                            path,\n                            stroke_color='#FF0000',\n                            stroke_opacity=.8,\n                            stroke_weight=2):\n        \"\"\" Set a dictionary with the javascript class Polyline parameters\n\n        This function sets a default drawing configuration if the user just\n        pass the polyline path, but also allows to set each parameter\n        individually if the user wish so.\n\n        Args:\n            path (list): A list of latitude and longitude point for the\n            polyline stroke_color (str): Sets the color of the rectangle\n            border using hexadecimal color notation\n            stroke_opacity (float): Sets the opacity of the rectangle border\n                in percentage. If stroke_opacity = 0, the border is transparent\n            stroke_weight (int): Sets the stroke girth in pixels.\n        \"\"\"\n\n        if not isinstance(path, list):\n            raise AttributeError('To build a map path a list of dictionaries'\n                                 ' of latitude and logitudes is required')\n\n        polyline = {\n            'path': path,\n            'stroke_color': stroke_color,\n            'stroke_opacity': stroke_opacity,\n            'stroke_weight': stroke_weight,\n        }\n\n        return polyline", "response": "This function builds a dictionary with the javascript class Polyline parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a polyline dict to the Map. polyline attribute.", "response": "def add_polyline(self, path=None, **kwargs):\n        \"\"\" Adds a polyline dict to the Map.polylines attribute\n\n        The Google Maps API describes a polyline as a \"linear overlay of\n        connected line segments on the map\". The linear paths are defined\n        by a list of Latitude and Longitude coordinate pairs, like so:\n\n            { 'lat': y, 'lng': x }\n\n        with each one being a point of the polyline path.\n\n        It accepts a polyline dict representation as well.\n\n        Args:\n            path (list(dict)): The set of points of the path\n\n        .. _Polyline:\n            https://developers.google.com/maps/documen\n            tation/javascript/reference#Polyline\n        \"\"\"\n\n        if path:\n            if not isinstance(path, list):\n                raise AttributeError('The path is a list of dictionary of'\n                                     'latitude and longitudes por path points')\n            for i, point in enumerate(path):\n                if not isinstance(point, dict):\n                    if isinstance(point, (list, tuple)) and len(point) == 2:\n                        path[i] = {'lat': point[0], 'lng': point[1]}\n                    else:\n                        raise AttributeError(\n                            'All points in the path must be dicts'\n                            ' of latitudes and longitudes, list or tuple'\n                        )\n            kwargs['path'] = path\n\n        kwargs.setdefault('stroke_color', '#FF0000')\n        kwargs.setdefault('stroke_opacity', .8)\n        kwargs.setdefault('stroke_weight', 2)\n\n        self.polylines.append(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_polygons(self, polygons):\n        if not polygons:\n            return\n        if not isinstance(polygons, (list, tuple)):\n            raise AttributeError('A list or tuple of polylines is required')\n\n        for points in polygons:\n            if isinstance(points, dict):\n                self.add_polygon(**points)\n            elif isinstance(points, (tuple, list)):\n                path = []\n                for coords in points:\n                    if len(coords) != 2:\n                        raise AttributeError('A point needs two coordinates')\n                    path.append({'lat': coords[0],\n                                 'lng': coords[1]})\n                polygon_dict = self.build_polygon_dict(path)\n                self.add_polygon(**polygon_dict)", "response": "This method builds the polygons for the current base class"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a dictionary with the javascript class Polygon parameters This function sets a default drawing configuration if the user just pass the polygon path, but also allows to set each parameter individually if the user wish so. Args: path (list): A list of latitude and longitude point for the polygon stroke_color (str): Sets the color of the polygon border using hexadecimal color notation stroke_opacity (float): Sets the opacity of the polygon border in percentage. If stroke_opacity = 0, the border is transparent stroke_weight (int): Sets the stroke girth in pixels. fill_color (str): Sets the color of the polygon fill using hexadecimal color notation fill_opacity (float): Sets the opacity of the polygon fill", "response": "def build_polygon_dict(self,\n                           path,\n                           stroke_color='#FF0000',\n                           stroke_opacity=.8,\n                           stroke_weight=2,\n                           fill_color='#FF0000',\n                           fill_opacity=0.3):\n        \"\"\" Set a dictionary with the javascript class Polygon parameters\n\n        This function sets a default drawing configuration if the user just\n        pass the polygon path, but also allows to set each parameter\n        individually if the user wish so.\n\n        Args:\n            path (list): A list of latitude and longitude point for the polygon\n            stroke_color (str): Sets the color of the polygon border using\n                hexadecimal color notation\n            stroke_opacity (float): Sets the opacity of the polygon border\n                in percentage. If stroke_opacity = 0, the border is transparent\n            stroke_weight (int): Sets the stroke girth in pixels.\n\n            fill_color (str): Sets the color of the polygon fill using\n                hexadecimal color notation\n            fill_opacity (float): Sets the opacity of the polygon fill\n        \"\"\"\n\n        if not isinstance(path, list):\n            raise AttributeError('To build a map path a list of dictionaries'\n                                 ' of latitude and logitudes is required')\n\n        polygon = {\n            'path': path,\n            'stroke_color': stroke_color,\n            'stroke_opacity': stroke_opacity,\n            'stroke_weight': stroke_weight,\n            'fill_color': fill_color,\n            'fill_opacity': fill_opacity\n        }\n\n        return polygon"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a polygon dict to the Map. polygons attribute.", "response": "def add_polygon(self, path=None, **kwargs):\n        \"\"\" Adds a polygon dict to the Map.polygons attribute\n\n        The Google Maps API describes a polyline as a \"linear overlay of\n        connected line segments on the map\" and \"form a closed loop and define\n        a filled region.\". The linear paths are defined by a list of Latitude\n        and Longitude coordinate pairs, like so:\n\n            { 'lat': y, 'lng': x }\n\n        with each one being a point of the polyline path.\n\n        It accepts a polygon dict representation as well.\n\n        Args:\n            path (list(dict)): The set of points of the path\n\n        .. _Polygon:\n            https://developers.google.com/maps/documen\n            tation/javascript/reference#Polygon\n        \"\"\"\n\n        if path:\n            if not isinstance(path, list):\n                raise AttributeError('The path is a list of dictionary of'\n                                     'latitude and longitudes por path points')\n            for i, point in enumerate(path):\n                if not isinstance(point, dict):\n                    if isinstance(point, (list, tuple)) and len(point) == 2:\n                        path[i] = {'lat': point[0], 'lng': point[1]}\n                    else:\n                        raise AttributeError(\n                            'All points in the path must be dicts'\n                            ' of latitudes and longitudes, list or tuple'\n                        )\n            kwargs['path'] = path\n\n        kwargs.setdefault('stroke_color', '#FF0000')\n        kwargs.setdefault('stroke_opacity', .8)\n        kwargs.setdefault('stroke_weight', 2)\n        kwargs.setdefault('fill_color', '#FF0000')\n        kwargs.setdefault('fill_opacity', .3)\n\n        self.polygons.append(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate an Image Captcha of the given characters.", "response": "def generate(self, chars, format='png'):\n        \"\"\"Generate an Image Captcha of the given characters.\n\n        :param chars: text to be generated.\n        :param format: image file format\n        \"\"\"\n        im = self.generate_image(chars)\n        out = BytesIO()\n        im.save(out, format=format)\n        out.seek(0)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, chars, output, format='png'):\n        im = self.generate_image(chars)\n        return im.save(output, format=format)", "response": "Generate and write an image CAPTCHA data to the output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_captcha_image(self, chars, color, background):\n        image = Image.new('RGB', (self._width, self._height), background)\n        draw = Draw(image)\n\n        def _draw_character(c):\n            font = random.choice(self.truefonts)\n            w, h = draw.textsize(c, font=font)\n\n            dx = random.randint(0, 4)\n            dy = random.randint(0, 6)\n            im = Image.new('RGBA', (w + dx, h + dy))\n            Draw(im).text((dx, dy), c, font=font, fill=color)\n\n            # rotate\n            im = im.crop(im.getbbox())\n            im = im.rotate(random.uniform(-30, 30), Image.BILINEAR, expand=1)\n\n            # warp\n            dx = w * random.uniform(0.1, 0.3)\n            dy = h * random.uniform(0.2, 0.3)\n            x1 = int(random.uniform(-dx, dx))\n            y1 = int(random.uniform(-dy, dy))\n            x2 = int(random.uniform(-dx, dx))\n            y2 = int(random.uniform(-dy, dy))\n            w2 = w + abs(x1) + abs(x2)\n            h2 = h + abs(y1) + abs(y2)\n            data = (\n                x1, y1,\n                -x1, h2 - y2,\n                w2 + x2, h2 + y2,\n                w2 - x2, -y1,\n            )\n            im = im.resize((w2, h2))\n            im = im.transform((w, h), Image.QUAD, data)\n            return im\n\n        images = []\n        for c in chars:\n            if random.random() > 0.5:\n                images.append(_draw_character(\" \"))\n            images.append(_draw_character(c))\n\n        text_width = sum([im.size[0] for im in images])\n\n        width = max(text_width, self._width)\n        image = image.resize((width, self._height))\n\n        average = int(text_width / len(chars))\n        rand = int(0.25 * average)\n        offset = int(average * 0.1)\n\n        for im in images:\n            w, h = im.size\n            mask = im.convert('L').point(table)\n            image.paste(im, (offset, int((self._height - h) / 2)), mask)\n            offset = offset + w + random.randint(-rand, 0)\n\n        if width > self._width:\n            image = image.resize((self._width, self._height))\n\n        return image", "response": "Create the CAPTCHA image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_image(self, chars):\n        background = random_color(238, 255)\n        color = random_color(10, 200, random.randint(220, 255))\n        im = self.create_captcha_image(chars, color, background)\n        self.create_noise_dots(im, color)\n        self.create_noise_curve(im, color)\n        im = im.filter(ImageFilter.SMOOTH)\n        return im", "response": "Generate the image of the given characters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the voice speed of the wave body.", "response": "def change_speed(body, speed=1):\n    \"\"\"Change the voice speed of the wave body.\"\"\"\n    if speed == 1:\n        return body\n\n    length = int(len(body) * speed)\n    rv = bytearray(length)\n\n    step = 0\n    for v in body:\n        i = int(step)\n        while i < int(step + speed) and i < length:\n            rv[i] = v\n            i += 1\n        step += speed\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npatching the wave header to the given wave content body.", "response": "def patch_wave_header(body):\n    \"\"\"Patch header to the given wave body.\n\n    :param body: the wave content body, it should be bytearray.\n    \"\"\"\n    length = len(body)\n\n    padded = length + length % 2\n    total = WAVE_HEADER_LENGTH + padded\n\n    header = copy.copy(WAVE_HEADER)\n    # fill the total length position\n    header[4:8] = bytearray(struct.pack('<I', total))\n    header += bytearray(struct.pack('<I', length))\n\n    data = header + body\n\n    # the total length is even\n    if length != padded:\n        data = data + bytearray([0])\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_noise(length, level=4):\n    noise = bytearray(length)\n    adjust = 128 - int(level / 2)\n    i = 0\n    while i < length:\n        v = random.randint(0, 256)\n        noise[i] = v % level + adjust\n        i += 1\n    return noise", "response": "Create white noise for background"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a piece of silence.", "response": "def create_silence(length):\n    \"\"\"Create a piece of silence.\"\"\"\n    data = bytearray(length)\n    i = 0\n    while i < length:\n        data[i] = 128\n        i += 1\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmixing two wave body into one.", "response": "def mix_wave(src, dst):\n    \"\"\"Mix two wave body into one.\"\"\"\n    if len(src) > len(dst):\n        # output should be longer\n        dst, src = src, dst\n\n    for i, sv in enumerate(src):\n        dv = dst[i]\n        if sv < 128 and dv < 128:\n            dst[i] = int(sv * dv / 128)\n        else:\n            dst[i] = int(2 * (sv + dv) - sv * dv / 128 - 256)\n    return dst"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef choices(self):\n        if self._choices:\n            return self._choices\n        for n in os.listdir(self._voicedir):\n            if len(n) == 1 and os.path.isdir(os.path.join(self._voicedir, n)):\n                self._choices.append(n)\n        return self._choices", "response": "Available choices for characters to be generated."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate audio CAPTCHA data. The return data is a bytearray.", "response": "def generate(self, chars):\n        \"\"\"Generate audio CAPTCHA data. The return data is a bytearray.\n\n        :param chars: text to be generated.\n        \"\"\"\n        if not self._cache:\n            self.load()\n        body = self.create_wave_body(chars)\n        return patch_wave_header(body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates and write audio CAPTCHA data to the output.", "response": "def write(self, chars, output):\n        \"\"\"Generate and write audio CAPTCHA data to the output.\n\n        :param chars: text to be generated.\n        :param output: output destionation.\n        \"\"\"\n        data = self.generate(chars)\n        with open(output, 'wb') as f:\n            return f.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef can_edit(self, user=None, request=None):\n        can = False\n        if request and not self.owner:\n            if (getattr(settings, \"UMAP_ALLOW_ANONYMOUS\", False)\n                    and self.is_anonymous_owner(request)):\n                can = True\n        if self.edit_status == self.ANONYMOUS:\n            can = True\n        elif not user.is_authenticated:\n            pass\n        elif user == self.owner:\n            can = True\n        elif self.edit_status == self.EDITORS and user in self.editors.all():\n            can = True\n        return can", "response": "Returns True if the user can edit the instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an image tag with a tile of the tilelayer.", "response": "def tilelayer_preview(tilelayer):\n    \"\"\"\n    Return an <img> tag with a tile of the tilelayer.\n    \"\"\"\n    output = '<img src=\"{src}\" alt=\"{alt}\" title=\"{title}\" />'\n    url = tilelayer.url_template.format(s=\"a\", z=9, x=265, y=181)\n    output = output.format(src=url, alt=tilelayer.name, title=tilelayer.name)\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_to_json(templates, context, request):\n    html = render_to_string(\n        templates,\n        context=context,\n        request=request\n    )\n    _json = json.dumps({\n        \"html\": html\n    })\n    return HttpResponse(_json)", "response": "Generate a JSON HttpResponse with rendered template HTML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndispatching template according to the kind of request ajax or normal.", "response": "def get_template_names(self):\n        \"\"\"\n        Dispatch template according to the kind of request: ajax or normal.\n        \"\"\"\n        if self.request.is_ajax():\n            return [self.list_template_name]\n        else:\n            return super(UserMaps, self).get_template_names()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_template_names(self):\n        if self.request.is_ajax():\n            return [self.list_template_name]\n        else:\n            return super(Search, self).get_template_names()", "response": "Dispatch template according to the kind of request ajax or normal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef path(self):\n        path = self._path()\n        statobj = os.stat(path)\n        ae = self.request.META.get('HTTP_ACCEPT_ENCODING', '')\n        if re_accepts_gzip.search(ae) and getattr(settings, 'UMAP_GZIP', True):\n            gzip_path = \"{path}{ext}\".format(path=path, ext=self.EXT)\n            up_to_date = True\n            if not os.path.exists(gzip_path):\n                up_to_date = False\n            else:\n                gzip_statobj = os.stat(gzip_path)\n                if statobj.st_mtime > gzip_statobj.st_mtime:\n                    up_to_date = False\n            if not up_to_date:\n                gzip_file(path, gzip_path)\n            path = gzip_path\n        return path", "response": "Returns the path of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing for URLs dealing with the map.", "response": "def map_permissions_check(view_func):\n    \"\"\"\n    Used for URLs dealing with the map.\n    \"\"\"\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        map_inst = get_object_or_404(Map, pk=kwargs['map_id'])\n        user = request.user\n        kwargs['map_inst'] = map_inst  # Avoid rerequesting the map in the view\n        if map_inst.edit_status >= map_inst.EDITORS:\n            can_edit = map_inst.can_edit(user=user, request=request)\n            if not can_edit:\n                if map_inst.owner and not user.is_authenticated:\n                    return simple_json_response(login_required=str(LOGIN_URL))\n                return HttpResponseForbidden()\n        return view_func(request, *args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(self):\n        code = _ean.EDGE[:]\n        pattern = _ean.LEFT_PATTERN[int(self.ean[0])]\n        for i, number in enumerate(self.ean[1:7]):\n            code += _ean.CODES[pattern[i]][int(number)]\n        code += _ean.MIDDLE\n        for number in self.ean[7:]:\n            code += _ean.CODES['C'][int(number)]\n        code += _ean.EDGE\n        return [code]", "response": "Builds the barcode pattern from self. ean."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build(self):\n        code = _ean.EDGE[:]\n        for number in self.ean[:4]:\n            code += _ean.CODES['A'][int(number)]\n        code += _ean.MIDDLE\n        for number in self.ean[4:]:\n            code += _ean.CODES['C'][int(number)]\n        code += _ean.EDGE\n        return [code]", "response": "Builds the barcode pattern from self. ean."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the size of the barcode in pixel.", "response": "def calculate_size(self, modules_per_line, number_of_lines, dpi=300):\n        \"\"\"Calculates the size of the barcode in pixel.\n\n        :parameters:\n            modules_per_line : Integer\n                Number of modules in one line.\n            number_of_lines : Integer\n                Number of lines of the barcode.\n            dpi : Integer\n                DPI to calculate.\n\n        :returns: Width and height of the barcode in pixel.\n        :rtype: Tuple\n        \"\"\"\n        width = 2 * self.quiet_zone + modules_per_line * self.module_width\n        height = 2.0 + self.module_height * number_of_lines\n        if self.font_size and self.text:\n            height += pt2mm(self.font_size) / 2 + self.text_distance\n        return int(mm2px(width, dpi)), int(mm2px(height, dpi))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_options(self, options):\n        for key, val in options.items():\n            key = key.lstrip('_')\n            if hasattr(self, key):\n                setattr(self, key, val)", "response": "Sets the given options as instance attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders the barcode to whatever the inheriting writer provides.", "response": "def render(self, code):\n        \"\"\"Renders the barcode to whatever the inheriting writer provides,\n        using the registered callbacks.\n\n        :parameters:\n            code : List\n                List of strings matching the writer spec\n                (only contain 0 or 1).\n        \"\"\"\n        if self._callbacks['initialize'] is not None:\n            self._callbacks['initialize'](code)\n        ypos = 1.0\n        for cc, line in enumerate(code):\n            \"\"\"\n            Pack line to list give better gfx result, otherwise in can\n            result in aliasing gaps\n            '11010111' -> [2, -1, 1, -1, 3]\n            \"\"\"\n            line += ' '\n            c = 1\n            mlist = []\n            for i in range(0, len(line) - 1):\n                if line[i] == line[i+1]:\n                    c += 1\n                else:\n                    if line[i] == \"1\":\n                        mlist.append(c)\n                    else:\n                        mlist.append(-c)\n                    c = 1\n            # Left quiet zone is x startposition\n            xpos = self.quiet_zone\n            bxs = xpos  # x start of barcode\n            for mod in mlist:\n                if mod < 1:\n                    color = self.background\n                else:\n                    color = self.foreground\n                # remove painting for background colored tiles?\n                self._callbacks['paint_module'](\n                    xpos, ypos, self.module_width * abs(mod), color\n                )\n                xpos += self.module_width * abs(mod)\n            bxe = xpos\n            # Add right quiet zone to every line, except last line,\n            # quiet zone already provided with background,\n            # should it be removed complety?\n            if (cc + 1) != len(code):\n                self._callbacks['paint_module'](\n                    xpos, ypos, self.quiet_zone, self.background\n                )\n            ypos += self.module_height\n        if self.text and self._callbacks['paint_text'] is not None:\n            ypos += self.text_distance\n            if self.center_text:\n                # better center position for text\n                xpos = bxs + ((bxe - bxs) / 2.0)\n            else:\n                xpos = bxs\n            self._callbacks['paint_text'](xpos, ypos)\n        return self._callbacks['finish']()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, filename, options=None, text=None):\n        if text:\n            output = self.render(options, text)\n        else:\n            output = self.render(options)\n\n        _filename = self.writer.save(filename, output)\n        return _filename", "response": "Renders the barcode and saves it in filename."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering the barcode and writes it to the file like object fp.", "response": "def write(self, fp, options=None, text=None):\n        \"\"\"Renders the barcode and writes it to the file like object\n        `fp`.\n\n        :parameters:\n            fp : File like object\n                Object to write the raw data in.\n            options : Dict\n                The same as in `self.render`.\n            text : str (unicode on Python 2)\n                Text to render under the barcode.\n        \"\"\"\n        output = self.render(options, text)\n        if hasattr(output, 'tostring'):\n            output.save(fp, format=self.writer.format)\n        else:\n            fp.write(output)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the checksum for the UPCA or UPC codes.", "response": "def calculate_checksum(self):\n        \"\"\"Calculates the checksum for UPCA/UPC codes\n\n        :return: The checksum for 'self.upc'\n        :rtype: Integer\n        \"\"\"\n        def sum_(x, y): return int(x) + int(y)\n        upc = self.upc[0:self.digits]\n        oddsum = reduce(sum_, upc[::2])\n        evensum = reduce(sum_, upc[1::2])\n        check = (evensum + oddsum * 3) % 10\n        if check == 0:\n            return 0\n        else:\n            return 10 - check"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self):\n        code = _upc.EDGE[:]\n\n        for i, number in enumerate(self.upc[0:6]):\n            code += _upc.CODES['L'][int(number)]\n\n        code += _upc.MIDDLE\n\n        for number in self.upc[6:]:\n            code += _upc.CODES['R'][int(number)]\n\n        code += _upc.EDGE\n\n        return [code]", "response": "Builds the barcode pattern from self. upc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming issue search for given stats instance", "response": "def search(query, stats):\n        \"\"\" Perform issue search for given stats instance \"\"\"\n        log.debug(\"Search query: {0}\".format(query))\n        issues = []\n        # Fetch data from the server in batches of MAX_RESULTS issues\n        for batch in range(MAX_BATCHES):\n            response = stats.parent.session.get(\n                \"{0}/rest/api/latest/search?{1}\".format(\n                    stats.parent.url, urllib.urlencode({\n                        \"jql\": query,\n                        \"fields\": \"summary,comment\",\n                        \"maxResults\": MAX_RESULTS,\n                        \"startAt\": batch * MAX_RESULTS})))\n            data = response.json()\n            log.debug(\"Batch {0} result: {1} fetched\".format(\n                batch, listed(data[\"issues\"], \"issue\")))\n            log.data(pretty(data))\n            issues.extend(data[\"issues\"])\n            # If all issues fetched, we're done\n            if len(issues) >= data[\"total\"]:\n                break\n        # Return the list of issue objects\n        return [Issue(issue, prefix=stats.parent.prefix) for issue in issues]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntruing if the issue was commented by given user", "response": "def updated(self, user, options):\n        \"\"\" True if the issue was commented by given user \"\"\"\n        for comment in self.comments:\n            created = dateutil.parser.parse(comment[\"created\"]).date()\n            try:\n                if (comment[\"author\"][\"emailAddress\"] == user.email and\n                        created >= options.since.date and\n                        created < options.until.date):\n                    return True\n            except KeyError:\n                pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of user - link history entries.", "response": "def user_link_history(self, created_before=None, created_after=None,\n                          limit=100, **kwargs):\n        \"\"\"  Bit.ly API - user_link_history wrapper\"\"\"\n        \"\"\" Bit.ly link\n\n        Link History Keys\n        -----------------\n\n            [u'aggregate_link', u'archived', u'campaign_ids',\n             u'client_id', u'created_at', u'keyword_link',\n             u'link', u'long_url', u'modified_at',\n             u'private', u'tags', u'title', u'user_ts']\n        \"\"\"\n        # bit.ly API doesn't seem to like anything other than int's\n        limit = int(limit)\n        created_after = int(created_after)\n        created_before = int(created_before)\n        hist = self.api.user_link_history(\n            limit=limit, created_before=created_before,\n            created_after=created_after)\n\n        # FIXME: if we have more than 100 objects we need to PAGINATE\n        record = \"{0} - {1}\"\n        links = []\n        for r in hist:\n            link = r.get('keyword_link') or r['link']\n            title = r['title'] or '<< NO TITLE >>'\n            links.append(record.format(link, title))\n        log.debug(\"First 3 Links fetched:\")\n        log.debug(pretty(hist[0:3], indent=4))\n        return links"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch(self):\n        '''\n        Bit.ly API expect unix timestamps\n        '''\n        since = time.mktime(self.options.since.datetime.timetuple())\n        until = time.mktime(self.options.until.datetime.timetuple())\n        log.info(\"Searching for links saved by {0}\".format(self.user))\n        self.stats = self.parent.bitly.user_link_history(created_after=since,\n                                                         created_before=until)", "response": "Fetch the user s links"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all activites (fetch only once)", "response": "def activities(self):\n        \"\"\" Return all activites (fetch only once) \"\"\"\n        if self._activities is None:\n            self._activities = self._fetch_activities()\n        return self._activities"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef issues(self, kind, email):\n        return list(set([unicode(activity.issue)\n            for activity in self.activities()\n            if kind == activity.kind and activity.user['email'] == email]))", "response": "Filter unique issues for given activity type and email"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching the organization activity and return the list of relevant Sentry activities.", "response": "def _fetch_activities(self):\n        \"\"\" Get organization activity, handle pagination \"\"\"\n        activities = []\n        # Prepare url of the first page\n        url = '{0}/organizations/{1}/activity/'.format(\n            self.url, self.organization)\n        while url:\n            # Fetch one page of activities\n            try:\n                log.debug('Fetching activity data: {0}'.format(url))\n                response = requests.get(url, headers=self.headers)\n                if not response.ok:\n                    log.error(response.text)\n                    raise ReportError('Failed to fetch Sentry activities.')\n                data = response.json()\n                log.data(\"Response headers:\\n{0}\".format(\n                    pretty(response.headers)))\n                log.debug(\"Fetched {0}.\".format(listed(len(data), 'activity')))\n                log.data(pretty(data))\n                for activity in [Activity(item) for item in data]:\n                    # We've reached the last page, older records not relevant\n                    if activity.created < self.stats.options.since.date:\n                        return activities\n                    # Store only relevant activites (before until date)\n                    if activity.created < self.stats.options.until.date:\n                        log.details(\"Activity: {0}\".format(activity))\n                        activities.append(activity)\n            except requests.RequestException as error:\n                log.debug(error)\n                raise ReportError(\n                    'Failed to fetch Sentry activities from {0}'.format(url))\n            # Check for possible next page\n            try:\n                url = NEXT_PAGE.search(response.headers['Link']).groups()[0]\n            except AttributeError:\n                url = None\n        return activities"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cases(self):\n        import nitrate\n        if self._cases is None:\n            log.info(u\"Searching for cases created by {0}\".format(self.user))\n            self._cases = [\n                case for case in nitrate.TestCase.search(\n                    author__email=self.user.email,\n                    create_date__gt=str(self.options.since),\n                    create_date__lt=str(self.options.until))\n                if case.status != nitrate.CaseStatus(\"DISABLED\")]\n        return self._cases", "response": "Returns a list of all test cases created by the user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authorized_http(client_id, client_secret, apps, file=None):\n    if not os.path.exists(CREDENTIAL_DIR):\n        os.makedirs(CREDENTIAL_DIR)\n\n    credential_path = file or CREDENTIAL_PATH\n    storage = Storage(credential_path)\n    credentials = storage.get()\n\n    scopes = set([\n        \"https://www.googleapis.com/auth/{0}.readonly\".format(app)\n        for app in apps\n        ])\n\n    if (not credentials or credentials.invalid\n            or not scopes <= credentials.scopes):\n        flow = OAuth2WebServerFlow(\n            client_id=client_id,\n            client_secret=client_secret,\n            scope=scopes,\n            redirect_uri=REDIRECT_URI)\n        flow.user_agent = USER_AGENT\n\n        # Do not parse did command-line options by OAuth client\n        flags = tools.argparser.parse_args(args=[])\n        credentials = tools.run_flow(flow, storage, flags)\n\n    return credentials.authorize(httplib2.Http())", "response": "Start an authorized HTTP session."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef events(self, **kwargs):\n        events_result = self.service.events().list(**kwargs).execute()\n        return [Event(event) for event in events_result.get(\"items\", [])]", "response": "Fetch events meeting specified criteria"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef attended_by(self, email):\n        for attendee in self[\"attendees\"] or []:\n            if (attendee[\"email\"] == email\n                    and attendee[\"responseStatus\"] == \"accepted\"):\n                return True\n        return False", "response": "Check if user attended the event"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching tasks specified criteria", "response": "def tasks(self, **kwargs):\n        \"\"\" Fetch tasks specified criteria \"\"\"\n        tasks_result = self.service.tasks().list(**kwargs).execute()\n        return [Task(task) for task in tasks_result.get(\"items\", [])]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all events in calendar within specified time range", "response": "def events(self):\n        \"\"\" All events in calendar within specified time range \"\"\"\n        if self._events is None:\n            self._events = self.parent.calendar.events(\n                calendarId=\"primary\", singleEvents=True, orderBy=\"startTime\",\n                timeMin=self.since, timeMax=self.until)\n        return self._events"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tasks(self):\n        if self._tasks is None:\n            self._tasks = self.parent.tasks.tasks(\n                tasklist=\"@default\", showCompleted=\"true\", showHidden=\"true\",\n                completedMin=self.since, completedMax=self.until)\n        log.info(u\"NB TASKS {0}\".format(len(self._tasks)))\n        return self._tasks", "response": "All completed tasks within specified time range"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the name of the class.", "response": "def name(self):\n        \"\"\" Use the first line of docs string unless name set. \"\"\"\n        if self._name:\n            return self._name\n        return [\n            line.strip() for line in self.__doc__.split(\"\\n\")\n            if line.strip()][0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_option(self, group):\n        group.add_argument(\n            \"--{0}\".format(self.option), action=\"store_true\", help=self.name)", "response": "Add option for this object to the parser group object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enabled(self):\n        # Cache into ._enabled\n        if self._enabled is None:\n            if self.parent is not None and self.parent.enabled():\n                self._enabled = True\n            else:\n                # Default to Enabled if not otherwise disabled\n                self._enabled = getattr(self.options, self.dest, True)\n        return self._enabled", "response": "Check whether we re enabled or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check(self):\n        if not self.enabled():\n            return\n        try:\n            self.fetch()\n        except (xmlrpclib.Fault, did.base.ConfigError) as error:\n            log.error(error)\n            self._error = True\n            # Raise the exception if debugging\n            if not self.options or self.options.debug:\n                raise\n        # Show the results stats (unless merging)\n        if self.options and not self.options.merge:\n            self.show()", "response": "Check the stats if enabled."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd option group and all children options.", "response": "def add_option(self, parser):\n        \"\"\" Add option group and all children options. \"\"\"\n\n        group = parser.add_argument_group(self.name)\n        for stat in self.stats:\n            stat.add_option(group)\n\n        group.add_argument(\n            \"--{0}\".format(self.option), action=\"store_true\", help=\"All above\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge(self, other):\n        for this, other in zip(self.stats, other.stats):\n            this.merge(other)", "response": "Merge all children stats."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, path):\n        # Generate token\n        service_name = gssapi.Name('HTTP@{0}'.format(self.url.netloc),\n                                   gssapi.NameType.hostbased_service)\n        ctx = gssapi.SecurityContext(usage=\"initiate\", name=service_name)\n        data = b64encode(ctx.step()).decode()\n\n        # Make the connection\n        connection = httplib.HTTPSConnection(self.url.netloc, 443)\n        log.debug(\"GET {0}\".format(path))\n        connection.putrequest(\"GET\", path)\n        connection.putheader(\"Authorization\", \"Negotiate {0}\".format(data))\n        connection.putheader(\"Referer\", self.url_string)\n        connection.endheaders()\n\n        # Perform the request, convert response into lines\n        response = connection.getresponse()\n        if response.status != 200:\n            raise ReportError(\n                \"Failed to fetch tickets: {0}\".format(response.status))\n        lines = response.read().decode(\"utf8\").strip().split(\"\\n\")[1:]\n        log.debug(\"Tickets fetched:\")\n        log.debug(pretty(lines))\n        return lines", "response": "Perform a GET request with GSSAPI authentication"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms request tracker search", "response": "def search(self, query):\n        \"\"\" Perform request tracker search \"\"\"\n        # Prepare the path\n        log.debug(\"Query: {0}\".format(query))\n        path = self.url.path + '?Format=__id__+__Subject__'\n        path += \"&Order=ASC&OrderBy=id&Query=\" + urllib.quote(query)\n\n        # Get the tickets\n        lines = self.get(path)\n        log.info(u\"Fetched tickets: {0}\".format(len(lines)))\n        return [self.parent.ticket(line, self.parent) for line in lines]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef server(self):\n        if self._server is None:\n            self._server = bugzilla.Bugzilla(url=self.parent.url)\n        return self._server", "response": "Connection to the server"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef summary(self):\n        if not self.bug.resolution:\n            return self.bug.summary\n        if (self.bug.resolution.lower() in self.parent.resolutions\n                or \"all\" in self.parent.resolutions):\n            return \"{0} [{1}]\".format(\n                self.bug.summary, self.bug.resolution.lower())\n        return self.bug.summary", "response": "Bug summary including resolution if enabled"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logs(self):\n        for record in self.history:\n            if (record[\"when\"] >= self.options.since.date\n                    and record[\"when\"] < self.options.until.date):\n                for change in record[\"changes\"]:\n                    yield record[\"who\"], change", "response": "Return relevant who - did - what pairs from the bug history"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verified(self):\n        for who, record in self.logs:\n            if record[\"field_name\"] == \"status\" \\\n                    and record[\"added\"] == \"VERIFIED\":\n                return True\n        return False", "response": "True if bug was verified in given time frame"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef returned(self, user):\n        for who, record in self.logs:\n            if (record[\"field_name\"] == \"status\"\n                    and record[\"added\"] == \"ASSIGNED\"\n                    and record[\"removed\"] != \"NEW\"\n                    and who == user.email or who == user.name):\n                return True\n        return False", "response": "Returns True if the log was successfully deleted or moved to ASSIGNED by given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fixed(self):\n        decision = False\n        for record in self.history:\n            # Completely ignore older changes\n            if record[\"when\"] < self.options.since.date:\n                continue\n            # Look for status change to MODIFIED (unless already found)\n            if not decision and record[\"when\"] < self.options.until.date:\n                for change in record[\"changes\"]:\n                    if (change[\"field_name\"] == \"status\"\n                            and change[\"added\"] == \"MODIFIED\"\n                            and change[\"removed\"] != \"CLOSED\"):\n                        decision = True\n            # Make sure that the bug has not been later moved to ASSIGNED.\n            # (This would mean the issue has not been fixed properly.)\n            else:\n                for change in record[\"changes\"]:\n                    if (change[\"field_name\"] == \"status\"\n                            and change[\"added\"] == \"ASSIGNED\"):\n                        decision = False\n        return decision", "response": "Check if the issue has not been later moved to MODIFIED and not later moved to ASSIGNED."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef closed(self, user):\n        decision = False\n        for record in self.history:\n            # Completely ignore older changes\n            if record[\"when\"] < self.options.since.date:\n                continue\n            # Look for status change to CLOSED (unless already found)\n            if not decision and record[\"when\"] < self.options.until.date:\n                for change in record[\"changes\"]:\n                    if (change[\"field_name\"] == \"status\"\n                            and change[\"added\"] == \"CLOSED\"\n                            and record[\"who\"] in [user.email, user.name]):\n                        decision = True\n            # Make sure that the bug has not been later moved from CLOSED.\n            # (This would mean the bug was not closed for a proper reason.)\n            else:\n                for change in record[\"changes\"]:\n                    if (change[\"field_name\"] == \"status\"\n                            and change[\"removed\"] == \"CLOSED\"):\n                        decision = False\n        return decision", "response": "Returns True if the bug was not later moved to ASSIGNED False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrue if bug was moved to POST in given time frame", "response": "def posted(self):\n        \"\"\" True if bug was moved to POST in given time frame \"\"\"\n        for who, record in self.logs:\n            if record[\"field_name\"] == \"status\" and record[\"added\"] == \"POST\":\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef commented(self, user):\n        for comment in self.comments:\n            # Description (comment #0) is not considered as a comment\n            if comment[\"count\"] == 0:\n                continue\n            if (comment.get('author', comment.get('creator')) == user.email and\n                    comment[\"creation_time\"] >= self.options.since.date and\n                    comment[\"creation_time\"] < self.options.until.date):\n                return True\n        return False", "response": "True if comment was added in given time frame"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subscribed(self, user):\n        for who, record in self.logs:\n            if (record[\"field_name\"] == \"cc\" and\n                    user.email in record[\"added\"]):\n                return True\n        return False", "response": "True if the given user is subscribed to the given time frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare Full Name <email@eg. com > string", "response": "def set_name_email(configurator, question, answer):\n    '''\n    prepare \"Full Name\" <email@eg.com>\" string\n    '''\n    name = configurator.variables['author.name']\n    configurator.variables['author.name_email'] = '\"{0}\" <{1}>'.format(\n        name, answer)\n    return answer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload all available plugins and return a list of them", "response": "def load():\n    \"\"\" Check available plugins and attempt to import them \"\"\"\n    # Code is based on beaker-client's command.py script\n    plugins = []\n    for filename in os.listdir(PLUGINS_PATH):\n        if not filename.endswith(\".py\") or filename.startswith(\"_\"):\n            continue\n        if not os.path.isfile(os.path.join(PLUGINS_PATH, filename)):\n            continue\n        plugin = filename[:-3]\n        if plugin in FAILED_PLUGINS:\n            # Skip loading plugins that already failed before\n            continue\n        try:\n            __import__(PLUGINS.__name__, {}, {}, [plugin])\n            plugins.append(plugin)\n            log.debug(\"Successfully imported {0} plugin\".format(plugin))\n        except (ImportError, SyntaxError) as error:\n            # Give a warning only when the plugin is configured\n            message = \"Failed to import {0} plugin ({1})\".format(plugin, error)\n            if Config().sections(kind=plugin):\n                log.warn(message)\n            else:\n                log.debug(message)\n            FAILED_PLUGINS.append(plugin)\n    return plugins"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect():\n\n    # Load plugins and config\n    plugins = load()\n    config = Config()\n\n    # Make sure that all sections have a valid plugin type defined\n    for section in config.sections():\n        if section == 'general':\n            continue\n        try:\n            type_ = config.item(section, 'type')\n        except ConfigError:\n            raise ConfigError(\n                \"Plugin type not defined in section '{0}'.\".format(section))\n        if type_ not in plugins:\n            raise ConfigError(\n                \"Invalid plugin type '{0}' in section '{1}'.\".format(\n                    type_, section))\n\n    # Detect classes inherited from StatsGroup and return them sorted\n    stats = []\n    for plugin in plugins:\n        module = getattr(PLUGINS, plugin)\n        for object_name in dir(module):\n            statsgroup = getattr(module, object_name)\n            # Filter out anything except for StatsGroup descendants\n            if (not isinstance(statsgroup, (type, types.ClassType))\n                    or not issubclass(statsgroup, StatsGroup)\n                    or statsgroup is StatsGroup\n                    or statsgroup is EmptyStatsGroup):\n                continue\n            # Search config for sections with type matching the plugin,\n            # use order provided there or class default otherwise\n            for section in config.sections(kind=plugin):\n                try:\n                    order = int(config.item(section, \"order\"))\n                except ConfigError:\n                    order = statsgroup.order\n                except ValueError:\n                    log.warn(\"Invalid {0} stats order: '{1}'\".format(\n                        section, config.item(section, \"order\")))\n                    order = statsgroup.order\n                stats.append((section, statsgroup, order))\n                log.info(\"Found {0}, an instance of {1}, order {2}\".format(\n                    section, statsgroup.__name__, order))\n                # Custom stats are handled with a single instance\n                if statsgroup.__name__ == \"CustomStats\":\n                    break\n    for section, statsgroup, _ in sorted(stats, key=lambda x: x[2]):\n        yield section, statsgroup", "response": "Detect available plugins and return enabled or configured stats."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the user email address of the current user.", "response": "def email(self):\n        \"\"\" User email(s) \"\"\"\n        try:\n            return self.parser.get(\"general\", \"email\")\n        except NoSectionError as error:\n            log.debug(error)\n            raise ConfigFileError(\n                \"No general section found in the config file.\")\n        except NoOptionError as error:\n            log.debug(error)\n            raise ConfigFileError(\n                \"No email address defined in the config file.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all sections of given kind", "response": "def sections(self, kind=None):\n        \"\"\" Return all sections (optionally of given kind only) \"\"\"\n        result = []\n        for section in self.parser.sections():\n            # Selected kind only if provided\n            if kind is not None:\n                try:\n                    section_type = self.parser.get(section, \"type\")\n                    if section_type != kind:\n                        continue\n                except NoOptionError:\n                    continue\n            result.append(section)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning section items, skip selected (type/order by default)", "response": "def section(self, section, skip=['type', 'order']):\n        \"\"\" Return section items, skip selected (type/order by default) \"\"\"\n        return [(key, val) for key, val in self.parser.items(section)\n                if key not in skip]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef item(self, section, it):\n        for key, value in self.section(section, skip=[]):\n            if key == it:\n                return value\n        raise ConfigError(\n            \"Item '{0}' not found in section '{1}'\".format(it, section))", "response": "Return content of given item in selected section"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetect config file path", "response": "def path():\n        \"\"\" Detect config file path \"\"\"\n        # Detect config directory\n        try:\n            directory = os.environ[\"DID_DIR\"]\n        except KeyError:\n            directory = CONFIG\n        # Detect config file (even before options are parsed)\n        filename = \"config\"\n        matched = re.search(\"--confi?g?[ =](\\S+)\", \" \".join(sys.argv))\n        if matched:\n            filename = matched.groups()[0]\n        return directory.rstrip(\"/\") + \"/\" + filename"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning start and end date of the current week.", "response": "def this_week():\n        \"\"\" Return start and end date of the current week. \"\"\"\n        since = TODAY + delta(weekday=MONDAY(-1))\n        until = since + delta(weeks=1)\n        return Date(since), Date(until)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn start and end date of this month.", "response": "def this_month():\n        \"\"\" Return start and end date of this month. \"\"\"\n        since = TODAY + delta(day=1)\n        until = since + delta(months=1)\n        return Date(since), Date(until)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns start and end date of this month.", "response": "def last_month():\n        \"\"\" Return start and end date of this month. \"\"\"\n        since = TODAY + delta(day=1, months=-1)\n        until = since + delta(months=1)\n        return Date(since), Date(until)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn start and end date of this quarter.", "response": "def this_quarter():\n        \"\"\" Return start and end date of this quarter. \"\"\"\n        since = TODAY + delta(day=1)\n        while since.month % 3 != 0:\n            since -= delta(months=1)\n        until = since + delta(months=3)\n        return Date(since), Date(until)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef last_quarter():\n        since, until = Date.this_quarter()\n        since = since.date - delta(months=3)\n        until = until.date - delta(months=3)\n        return Date(since), Date(until)", "response": "Return start and end date of this quarter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef this_year():\n        since = TODAY\n        while since.month != 3 or since.day != 1:\n            since -= delta(days=1)\n        until = since + delta(years=1)\n        return Date(since), Date(until)", "response": "Return start and end date of this fiscal year"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef last_year():\n        since, until = Date.this_year()\n        since = since.date - delta(years=1)\n        until = until.date - delta(years=1)\n        return Date(since), Date(until)", "response": "Return start and end date of the last fiscal year"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef period(argument):\n        since, until, period = None, None, None\n        if \"today\" in argument:\n            since = Date(\"today\")\n            until = Date(\"today\")\n            until.date += delta(days=1)\n            period = \"today\"\n        elif \"yesterday\" in argument:\n            since = Date(\"yesterday\")\n            until = Date(\"yesterday\")\n            until.date += delta(days=1)\n            period = \"yesterday\"\n        elif \"year\" in argument:\n            if \"last\" in argument:\n                since, until = Date.last_year()\n                period = \"the last fiscal year\"\n            else:\n                since, until = Date.this_year()\n                period = \"this fiscal year\"\n        elif \"quarter\" in argument:\n            if \"last\" in argument:\n                since, until = Date.last_quarter()\n                period = \"the last quarter\"\n            else:\n                since, until = Date.this_quarter()\n                period = \"this quarter\"\n        elif \"month\" in argument:\n            if \"last\" in argument:\n                since, until = Date.last_month()\n            else:\n                since, until = Date.this_month()\n            period = since.datetime.strftime(\"%B\")\n        else:\n            if \"last\" in argument:\n                since, until = Date.last_week()\n            else:\n                since, until = Date.this_week()\n            period = \"the week {0}\".format(since.datetime.strftime(\"%V\"))\n        return since, until, period", "response": "Detect the desired time period for the argument"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the login and email alias if configured.", "response": "def alias(self, aliases, stats):\n        \"\"\" Apply the login/email alias if configured. \"\"\"\n        login = email = None\n        if stats is None:\n            return\n        # Attempt to use alias directly from the config section\n        try:\n            config = dict(Config().section(stats))\n            try:\n                email = config[\"email\"]\n            except KeyError:\n                pass\n            try:\n                login = config[\"login\"]\n            except KeyError:\n                pass\n        except (ConfigFileError, NoSectionError):\n            pass\n        # Check for aliases specified in the email string\n        if aliases is not None:\n            try:\n                aliases = dict([\n                    re.split(r\"\\s*:\\s*\", definition, 1)\n                    for definition in re.split(r\"\\s*;\\s*\", aliases.strip())])\n            except ValueError:\n                raise ConfigError(\n                    \"Invalid alias definition: '{0}'\".format(aliases))\n            if stats in aliases:\n                if \"@\" in aliases[stats]:\n                    email = aliases[stats]\n                else:\n                    login = aliases[stats]\n        # Update login/email if alias detected\n        if email is not None:\n            self.email = email\n            log.info(\"Using email alias '{0}' for '{1}'\".format(email, stats))\n            if login is None:\n                login = email.split(\"@\")[0]\n        if login is not None:\n            self.login = login\n            log.info(\"Using login alias '{0}' for '{1}'\".format(login, stats))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef history(self, user=None):\n        for event in self.changelog:\n            when, who, what, old, new, ignore = event\n            if (when >= self.options.since.date and\n                    when <= self.options.until.date):\n                if user is None or who.startswith(user.login):\n                    yield who, what, old, new", "response": "Return relevant who - did - what logs from the ticket history."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef updated(self, user):\n        for who, what, old, new in self.history(user):\n            if (what == \"comment\" or what == \"description\") and new != \"\":\n                return True\n        return False", "response": "True if the user has commented the ticket in given time frame False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrue if ticket was closed in given time frame False otherwise.", "response": "def closed(self):\n        \"\"\" True if ticket was closed in given time frame \"\"\"\n        for who, what, old, new in self.history():\n            if what == \"status\" and new == \"closed\":\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the commits for a given user.", "response": "def commits(self, user, options):\n        \"\"\" List commits for given user. \"\"\"\n        # Prepare the command\n        command = \"git log --all --author={0}\".format(user.login).split()\n        command.append(\"--format=format:%h - %s\")\n        command.append(\"--since='{0} 00:00:00'\".format(options.since))\n        command.append(\"--until='{0} 00:00:00'\".format(options.until))\n        if options.verbose:\n            command.append(\"--name-only\")\n        log.info(u\"Checking commits in {0}\".format(self.path))\n        log.details(pretty(command))\n\n        # Get the commit messages\n        try:\n            process = subprocess.Popen(\n                command, cwd=self.path,\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        except OSError as error:\n            log.debug(error)\n            raise did.base.ReportError(\n                \"Unable to access git repo '{0}'\".format(self.path))\n        output, errors = process.communicate()\n        log.debug(\"git log output:\")\n        log.debug(output)\n        if process.returncode == 0:\n            if not output:\n                return []\n            else:\n                if not options.verbose:\n                    return unicode(output, \"utf8\").split(\"\\n\")\n                commits = []\n                for commit in unicode(output, \"utf8\").split(\"\\n\\n\"):\n                    summary = commit.split(\"\\n\")[0]\n                    directory = re.sub(\"/[^/]+$\", \"\", commit.split(\"\\n\")[1])\n                    commits.append(\"{0}\\n{1}* {2}\".format(\n                        summary, 8 * \" \", directory))\n                return commits\n        else:\n            log.debug(errors.strip())\n            log.warn(\"Unable to check commits in '{0}'\".format(self.path))\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shorted(text, width=79):\n    if len(text) <= width:\n        return text\n    # We remove any word after first overlapping non-word character\n    return u\"{0}...\".format(re.sub(r\"\\W+\\w*$\", \"\", text[:width - 2]))", "response": "Shorten text to be cut in the middle of a word"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listed(items, singular=None, plural=None, max=None, quote=\"\"):\n\n    # Convert items to list if necessary\n    items = range(items) if isinstance(items, int) else list(items)\n    more = \" more\"\n    # Description mode expected when singular provided but no maximum set\n    if singular is not None and max is None:\n        max = 0\n        more = \"\"\n    # Set the default plural form\n    if singular is not None and plural is None:\n        plural = pluralize(singular)\n    # Convert to strings and optionally quote each item\n    items = [\"{0}{1}{0}\".format(quote, item) for item in items]\n\n    # Select the maximum of items and describe the rest if max provided\n    if max is not None:\n        # Special case when the list is empty (0 items)\n        if max == 0 and len(items) == 0:\n            return \"0 {0}\".format(plural)\n        # Cut the list if maximum exceeded\n        if len(items) > max:\n            rest = len(items[max:])\n            items = items[:max]\n            if singular is not None:\n                more += \" {0}\".format(singular if rest == 1 else plural)\n            items.append(\"{0}{1}\".format(rest, more))\n\n    # For two and more items use 'and' instead of the last comma\n    if len(items) < 2:\n        return \"\".join(items)\n    else:\n        return \", \".join(items[0:-2] + [\" and \".join(items[-2:])])", "response": "Convert an iterable into a nice human readable list or description."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split(values, separator=re.compile(\"[ ,]+\")):\n    if not isinstance(values, list):\n        values = [values]\n    return sum([separator.split(value) for value in values], [])", "response": "Convert space - or - comma - separated values into a list of strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ascii(text):\n    if not isinstance(text, unicode):\n        text = unicode(text)\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')", "response": "Transliterate special unicode characters into pure ascii"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_logger(name='did', level=None):\n        # Create logger, handler and formatter\n        logger = logging.getLogger(name)\n        handler = logging.StreamHandler()\n        handler.setFormatter(Logging.ColoredFormatter())\n        logger.addHandler(handler)\n        # Save log levels in the logger itself (backward compatibility)\n        for level in Logging.LEVELS:\n            setattr(logger, level, getattr(logging, level))\n        # Additional logging constants and methods for details and data\n        logger.DATA = LOG_DATA\n        logger.DETAILS = LOG_DETAILS\n        logger.ALL = LOG_ALL\n        logger.details = lambda message: logger.log(\n            LOG_DETAILS, message) # NOQA\n        logger.data = lambda message: logger.log(\n            LOG_DATA, message) # NOQA\n        logger.all = lambda message: logger.log(\n            LOG_ALL, message) # NOQA\n        return logger", "response": "Create a logger with the given name and level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, level=None):\n        # If level specified, use given\n        if level is not None:\n            Logging._level = level\n        # Otherwise attempt to detect from the environment\n        else:\n            try:\n                Logging._level = Logging.MAPPING[int(os.environ[\"DEBUG\"])]\n            except StandardError:\n                Logging._level = logging.WARN\n        self.logger.setLevel(Logging._level)", "response": "Set the default log level for the given log level."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the coloring mode of the current locale.", "response": "def set(self, mode=None):\n        \"\"\"\n        Set the coloring mode\n\n        If enabled, some objects (like case run Status) are printed in color\n        to easily spot failures, errors and so on. By default the feature is\n        enabled when script is attached to a terminal. Possible values are::\n\n            COLOR=0 ... COLOR_OFF .... coloring disabled\n            COLOR=1 ... COLOR_ON ..... coloring enabled\n            COLOR=2 ... COLOR_AUTO ... if terminal attached (default)\n\n        Environment variable COLOR can be used to set up the coloring to the\n        desired mode without modifying code.\n        \"\"\"\n        # Detect from the environment if no mode given (only once)\n        if mode is None:\n            # Nothing to do if already detected\n            if self._mode is not None:\n                return\n            # Detect from the environment variable COLOR\n            try:\n                mode = int(os.environ[\"COLOR\"])\n            except StandardError:\n                mode = COLOR_AUTO\n        elif mode < 0 or mode > 2:\n            raise RuntimeError(\"Invalid color mode '{0}'\".format(mode))\n        self._mode = mode\n        log.debug(\n            \"Coloring {0} ({1})\".format(\n                \"enabled\" if self.enabled() else \"disabled\",\n                self.MODES[self._mode]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrue if coloring is currently enabled.", "response": "def enabled(self):\n        \"\"\" True if coloring is currently enabled \"\"\"\n        # In auto-detection mode color enabled when terminal attached\n        if self._mode == COLOR_AUTO:\n            return sys.stdout.isatty()\n        return self._mode == COLOR_ON"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(arguments=None):\n    try:\n        # Parse options, initialize gathered stats\n        options, header = Options(arguments).parse()\n        gathered_stats = []\n\n        # Check for user email addresses (command line or config)\n        emails = options.emails or did.base.Config().email\n        emails = utils.split(emails, separator=re.compile(r\"\\s*,\\s*\"))\n        users = [did.base.User(email=email) for email in emails]\n\n        # Print header and prepare team stats object for data merging\n        utils.eprint(header)\n        team_stats = UserStats(options=options)\n        if options.merge:\n            utils.header(\"Total Report\")\n            utils.item(\"Users: {0}\".format(len(users)), options=options)\n\n        # Check individual user stats\n        for user in users:\n            if options.merge:\n                utils.item(user, 1, options=options)\n            else:\n                utils.header(user)\n            user_stats = UserStats(user=user, options=options)\n            user_stats.check()\n            team_stats.merge(user_stats)\n            gathered_stats.append(user_stats)\n\n        # Display merged team report\n        if options.merge or options.total:\n            if options.total:\n                utils.header(\"Total Report\")\n            team_stats.show()\n\n        # Return all gathered stats objects\n        return gathered_stats, team_stats\n\n    except did.base.ConfigFileError as error:\n        utils.info(\"Create at least a minimum config file {0}:\\n{1}\".format(\n            did.base.Config.path(), did.base.Config.example().strip()))\n        raise", "response": "Main function for the gather stats and show the results\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepare_arguments(self, arguments):\n        # Split arguments if given as string\n        if arguments is not None:\n            if isinstance(arguments, basestring):\n                self.arguments = arguments.split()\n            else:\n                self.arguments = arguments\n        # Otherwise properly decode command line arguments\n        else:\n            self.arguments = [arg.decode(\"utf-8\") for arg in sys.argv[1:]]", "response": "Prepare arguments for the command line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self):\n        # Run the parser\n        opt, arg = self.parser.parse_known_args(self.arguments)\n        self.opt = opt\n        self.arg = arg\n        self.check()\n\n        # Enable --all if no particular stat or group selected\n        opt.all = not any([\n            getattr(opt, stat.dest) or getattr(opt, group.dest)\n            for group in self.sample_stats.stats\n            for stat in group.stats])\n\n        # Time period handling\n        if opt.since is None and opt.until is None:\n            opt.since, opt.until, period = did.base.Date.period(arg)\n        else:\n            opt.since = did.base.Date(opt.since or \"1993-01-01\")\n            opt.until = did.base.Date(opt.until or \"today\")\n            # Make the 'until' limit inclusive\n            opt.until.date += delta(days=1)\n            period = \"given date range\"\n\n        # Validate the date range\n        if not opt.since.date < opt.until.date:\n            raise RuntimeError(\n                \"Invalid date range ({0} to {1})\".format(\n                    opt.since, opt.until.date - delta(days=1)))\n        header = \"Status report for {0} ({1} to {2}).\".format(\n            period, opt.since, opt.until.date - delta(days=1))\n\n        # Finito\n        log.debug(\"Gathered options:\")\n        log.debug('options = {0}'.format(opt))\n        return opt, header", "response": "Parse the options and return the options and the header."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms additional check for given options", "response": "def check(self):\n        \"\"\" Perform additional check for given options \"\"\"\n        keywords = \"today yesterday this last week month quarter year\".split()\n        for argument in self.arg:\n            if argument not in keywords:\n                raise did.base.OptionError(\n                    \"Invalid argument: '{0}'\".format(argument))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_actions(self, filters, since=None, before=None, limit=1000):\n        if limit > 1000:\n            raise NotImplementedError(\n                \"Fetching more than 1000 items is not implemented\")\n        resp = self.stats.session.open(\n            \"{0}/members/{1}/actions?{2}\".format(\n                self.stats.url, self.username, urllib.urlencode({\n                    \"key\": self.key,\n                    \"token\": self.token,\n                    \"filter\": filters,\n                    \"limit\": limit,\n                    \"since\": str(since),\n                    \"before\": str(before)})))\n\n        actions = json.loads(resp.read())\n        log.data(pretty(actions))\n        # print[act for act in actions if \"shortLink\" not in\n        # act['data']['board'].keys()]\n        actions = [act for act in actions if act['data']\n                   ['board']['id'] in self.board_ids]\n        return actions", "response": "Get the list of actions for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef board_links_to_ids(self):\n        resp = self.stats.session.open(\n            \"{0}/members/{1}/boards?{2}\".format(\n                self.stats.url, self.username, urllib.urlencode({\n                    \"key\": self.key,\n                    \"token\": self.token,\n                    \"fields\": \"shortLink\"})))\n        boards = json.loads(resp.read())\n\n        return [board['id'] for board in boards if self.board_links == [\"\"]\n                or board['shortLink'] in self.board_links]", "response": "Convert board links to ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_value(self, value):\n        if value == '' or value is None:\n            return None\n        return formats.localize_input(value, self.format)", "response": "Return a value as it should appear when rendered in a template."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmisses method of django. forms. widgets. Widget class.", "response": "def get_context(self, name, value, attrs):\n        \"\"\"Missing method of django.forms.widgets.Widget class.\"\"\"\n        context = {}\n        context['widget'] = {\n            'name': name,\n            'type': 'text',\n            'is_hidden': self.is_hidden,\n            'required': self.is_required,\n            'value': self.format_value(value),\n            'attrs': {**self.attrs, **(attrs or {})},\n            'template_name': self.template_name,\n        }\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the widget as an HTML string.", "response": "def render(self, name, value, attrs=None, renderer=None):\n        \"\"\"\n        Render the widget as an HTML string.\n\n        Missing method of django.forms.widgets.Widget class\n        \"\"\"\n        context = self.get_context(name, value, attrs)\n        return self._render(self.template_name, context, renderer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render(self, template_name, context, renderer=None):\n        if renderer is None:\n            renderer = get_default_renderer()\n        return mark_safe(renderer.render(template_name, context))", "response": "Render a template with the given context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _link_to(self, linked_picker):\n        yformat = self.config['options']['format'].replace('-01-01', '-12-31')\n        self.config['options']['format'] = yformat", "response": "Customize the options when linked with other date - time input"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert python datetime format to moment datetime format.", "response": "def format_py2js(cls, datetime_format):\n        \"\"\"Convert python datetime format to moment datetime format.\"\"\"\n        for js_format, py_format in cls.format_map:\n            datetime_format = datetime_format.replace(py_format, js_format)\n        return datetime_format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_js2py(cls, datetime_format):\n        for js_format, py_format in cls.format_map:\n            datetime_format = datetime_format.replace(js_format, py_format)\n        return datetime_format", "response": "Convert moment datetime format to python datetime format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_options(self):\n        _options = self._default_options.copy()\n        _options.update(self.options)\n        if self.options_param:\n            _options.update(self.options_param)\n        return _options", "response": "Calculate and Return the options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calculate_format(self):\n        _format = self.format_param if self.format_param else self.format\n        if self.config['options'].get('format'):\n            _format = self.format_js2py(self.config['options'].get('format'))\n        else:\n            self.config['options']['format'] = self.format_py2js(_format)\n        return _format", "response": "Calculate and Return the datetime format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_context(self, name, value, attrs):\n        context = super().get_context(\n            name, value, attrs)\n        context['widget']['attrs']['dp_config'] = json_dumps(self.config)\n        return context", "response": "Return widget context dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the Date - picker as the end - date of a date - range.", "response": "def end_of(self, event_id, import_options=True):\n        \"\"\"\n        Set Date-Picker as the end-date of a date-range.\n\n        Args:\n            - event_id (string): User-defined unique id for linking two fields\n            - import_options (bool): inherit options from start-date input,\n              default: TRUE\n        \"\"\"\n        event_id = str(event_id)\n        if event_id in DatePickerDictionary.items:\n            linked_picker = DatePickerDictionary.items[event_id]\n            self.config['linked_to'] = linked_picker.config['id']\n            if import_options:\n                backup_moment_format = self.config['options']['format']\n                self.config['options'].update(linked_picker.config['options'])\n                self.config['options'].update(self.options_param)\n                if self.format_param or 'format' in self.options_param:\n                    self.config['options']['format'] = backup_moment_format\n                else:\n                    self.format = linked_picker.format\n            # Setting useCurrent is necessary, see following issue\n            # https://github.com/Eonasdan/bootstrap-datetimepicker/issues/1075\n            self.config['options']['useCurrent'] = False\n            self._link_to(linked_picker)\n        else:\n            raise KeyError(\n                'start-date not specified for event_id \"%s\"' % event_id)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_base_input(test=False):\n    from django.forms.widgets import DateTimeBaseInput\n    if 'get_context' in dir(DateTimeBaseInput) and not test:\n        # django version 1.11 and above\n        base_input = DateTimeBaseInput\n    else:\n        # django version below 1.11\n        from bootstrap_datepicker_plus._compatibility import (\n            CompatibleDateTimeBaseInput\n        )\n        base_input = CompatibleDateTimeBaseInput\n    return base_input", "response": "Returns a base input class for the current version of django."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the source file and the destination file into the destination tree.", "response": "def convert(srctree, dsttree=dsttree, readonly=False, dumpall=False,\n            ignore_exceptions=False, fullcomp=False):\n    \"\"\"Walk the srctree, and convert/copy all python files\n    into the dsttree\n\n    \"\"\"\n\n    if fullcomp:\n        allow_ast_comparison()\n\n    parse_file = code_to_ast.parse_file\n    find_py_files = code_to_ast.find_py_files\n    srctree = os.path.normpath(srctree)\n\n    if not readonly:\n        dsttree = os.path.normpath(dsttree)\n        logging.info('')\n        logging.info('Trashing ' + dsttree)\n        shutil.rmtree(dsttree, True)\n\n    unknown_src_nodes = set()\n    unknown_dst_nodes = set()\n    badfiles = set()\n    broken = []\n\n    oldpath = None\n\n    allfiles = find_py_files(srctree, None if readonly else dsttree)\n    for srcpath, fname in allfiles:\n        # Create destination directory\n        if not readonly and srcpath != oldpath:\n            oldpath = srcpath\n            if srcpath >= srctree:\n                dstpath = srcpath.replace(srctree, dsttree, 1)\n                if not dstpath.startswith(dsttree):\n                    raise ValueError(\"%s not a subdirectory of %s\" %\n                                     (dstpath, dsttree))\n            else:\n                assert srctree.startswith(srcpath)\n                dstpath = dsttree\n            os.makedirs(dstpath)\n\n        srcfname = os.path.join(srcpath, fname)\n        logging.info('Converting %s' % srcfname)\n        try:\n            srcast = parse_file(srcfname)\n        except SyntaxError:\n            badfiles.add(srcfname)\n            continue\n\n        try:\n            dsttxt = to_source(srcast)\n        except:\n            if not ignore_exceptions:\n                raise\n            dsttxt = ''\n\n        if not readonly:\n            dstfname = os.path.join(dstpath, fname)\n            try:\n                with open(dstfname, 'wb') as f:\n                    f.write(out_prep(dsttxt))\n            except UnicodeEncodeError:\n                badfiles.add(dstfname)\n\n        # As a sanity check, make sure that ASTs themselves\n        # round-trip OK\n        try:\n            dstast = ast.parse(dsttxt) if readonly else parse_file(dstfname)\n        except SyntaxError:\n            dstast = []\n        if fullcomp:\n            unknown_src_nodes.update(strip_tree(srcast))\n            unknown_dst_nodes.update(strip_tree(dstast))\n            bad = srcast != dstast\n        else:\n            bad = not fast_compare(srcast, dstast)\n        if dumpall or bad:\n            srcdump = dump_tree(srcast)\n            dstdump = dump_tree(dstast)\n            logging.warning('    calculating dump -- %s' %\n                            ('bad' if bad else 'OK'))\n            if bad:\n                broken.append(srcfname)\n            if dumpall or bad:\n                if not readonly:\n                    try:\n                        with open(dstfname[:-3] + '.srcdmp', 'wb') as f:\n                            f.write(out_prep(srcdump))\n                    except UnicodeEncodeError:\n                        badfiles.add(dstfname[:-3] + '.srcdmp')\n                    try:\n                        with open(dstfname[:-3] + '.dstdmp', 'wb') as f:\n                            f.write(out_prep(dstdump))\n                    except UnicodeEncodeError:\n                        badfiles.add(dstfname[:-3] + '.dstdmp')\n                elif dumpall:\n                    sys.stdout.write('\\n\\nAST:\\n\\n    ')\n                    sys.stdout.write(srcdump.replace('\\n', '\\n    '))\n                    sys.stdout.write('\\n\\nDecompile:\\n\\n    ')\n                    sys.stdout.write(dsttxt.replace('\\n', '\\n    '))\n                    sys.stdout.write('\\n\\nNew AST:\\n\\n    ')\n                    sys.stdout.write('(same as old)' if dstdump == srcdump\n                                     else dstdump.replace('\\n', '\\n    '))\n                    sys.stdout.write('\\n')\n\n    if badfiles:\n        logging.warning('\\nFiles not processed due to syntax errors:')\n        for fname in sorted(badfiles):\n            logging.warning('    %s' % fname)\n    if broken:\n        logging.warning('\\nFiles failed to round-trip to AST:')\n        for srcfname in broken:\n            logging.warning('    %s' % srcfname)\n\n    ok_to_strip = 'col_offset _precedence _use_parens lineno _p_op _pp'\n    ok_to_strip = set(ok_to_strip.split())\n    bad_nodes = (unknown_dst_nodes | unknown_src_nodes) - ok_to_strip\n    if bad_nodes:\n        logging.error('\\nERROR -- UNKNOWN NODES STRIPPED: %s' % bad_nodes)\n    logging.info('\\n')\n    return broken"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_lines(source, maxline=79):\n    result = []\n    extend = result.extend\n    append = result.append\n    line = []\n    multiline = False\n    count = 0\n    find = str.find\n    for item in source:\n        index = find(item, '\\n')\n        if index:\n            line.append(item)\n            multiline = index > 0\n            count += len(item)\n        else:\n            if line:\n                if count <= maxline or multiline:\n                    extend(line)\n                else:\n                    wrap_line(line, maxline, result)\n                count = 0\n                multiline = False\n                line = []\n            append(item)\n    return result", "response": "Split a list of lines according to lines."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap a line of text into a single tree structure.", "response": "def wrap_line(line, maxline=79, result=[], count=count):\n    \"\"\" We have a line that is too long,\n        so we're going to try to wrap it.\n    \"\"\"\n\n    # Extract the indentation\n\n    append = result.append\n    extend = result.extend\n\n    indentation = line[0]\n    lenfirst = len(indentation)\n    indent = lenfirst - len(indentation.lstrip())\n    assert indent in (0, lenfirst)\n    indentation = line.pop(0) if indent else ''\n\n    # Get splittable/non-splittable groups\n\n    dgroups = list(delimiter_groups(line))\n    unsplittable = dgroups[::2]\n    splittable = dgroups[1::2]\n\n    # If the largest non-splittable group won't fit\n    # on a line, try to add parentheses to the line.\n\n    if max(count(x) for x in unsplittable) > maxline - indent:\n        line = add_parens(line, maxline, indent)\n        dgroups = list(delimiter_groups(line))\n        unsplittable = dgroups[::2]\n        splittable = dgroups[1::2]\n\n    # Deal with the first (always unsplittable) group, and\n    # then set up to deal with the remainder in pairs.\n\n    first = unsplittable[0]\n    append(indentation)\n    extend(first)\n    if not splittable:\n        return result\n    pos = indent + count(first)\n    indentation += '    '\n    indent += 4\n    if indent >= maxline/2:\n        maxline = maxline/2 + indent\n\n    for sg, nsg in zip(splittable, unsplittable[1:]):\n\n        if sg:\n            # If we already have stuff on the line and even\n            # the very first item won't fit, start a new line\n            if pos > indent and pos + len(sg[0]) > maxline:\n                append('\\n')\n                append(indentation)\n                pos = indent\n\n            # Dump lines out of the splittable group\n            # until the entire thing fits\n            csg = count(sg)\n            while pos + csg > maxline:\n                ready, sg = split_group(sg, pos, maxline)\n                if ready[-1].endswith(' '):\n                    ready[-1] = ready[-1][:-1]\n                extend(ready)\n                append('\\n')\n                append(indentation)\n                pos = indent\n                csg = count(sg)\n\n            # Dump the remainder of the splittable group\n            if sg:\n                extend(sg)\n                pos += csg\n\n        # Dump the unsplittable group, optionally\n        # preceded by a linefeed.\n        cnsg = count(nsg)\n        if pos > indent and pos + cnsg > maxline:\n            append('\\n')\n            append(indentation)\n            pos = indent\n        extend(nsg)\n        pos += cnsg"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_group(source, pos, maxline):\n    first = []\n    source.reverse()\n    while source:\n        tok = source.pop()\n        first.append(tok)\n        pos += len(tok)\n        if source:\n            tok = source[-1]\n            allowed = (maxline + 1) if tok.endswith(' ') else (maxline - 4)\n            if pos + len(tok) > allowed:\n                break\n\n    source.reverse()\n    return first, source", "response": "Split a group into two subgroups."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delimiter_groups(line, begin_delim=begin_delim,\n                     end_delim=end_delim):\n    \"\"\"Split a line into alternating groups.\n       The first group cannot have a line feed inserted,\n       the next one can, etc.\n    \"\"\"\n    text = []\n    line = iter(line)\n    while True:\n        # First build and yield an unsplittable group\n        for item in line:\n            text.append(item)\n            if item in begin_delim:\n                break\n        if not text:\n            break\n        yield text\n\n        # Now build and yield a splittable group\n        level = 0\n        text = []\n        for item in line:\n            if item in begin_delim:\n                level += 1\n            elif item in end_delim:\n                level -= 1\n                if level < 0:\n                    yield text\n                    text = [item]\n                    break\n            text.append(item)\n        else:\n            assert not text, text\n            break", "response": "Split a line into alternating groups."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd parentheses around the line to make it splittable.", "response": "def add_parens(line, maxline, indent, statements=statements, count=count):\n    \"\"\"Attempt to add parentheses around the line\n       in order to make it splittable.\n    \"\"\"\n\n    if line[0] in statements:\n        index = 1\n        if not line[0].endswith(' '):\n            index = 2\n            assert line[1] == ' '\n        line.insert(index, '(')\n        if line[-1] == ':':\n            line.insert(-1, ')')\n        else:\n            line.append(')')\n\n    # That was the easy stuff.  Now for assignments.\n    groups = list(get_assign_groups(line))\n    if len(groups) == 1:\n        # So sad, too bad\n        return line\n\n    counts = list(count(x) for x in groups)\n    didwrap = False\n\n    # If the LHS is large, wrap it first\n    if sum(counts[:-1]) >= maxline - indent - 4:\n        for group in groups[:-1]:\n            didwrap = False  # Only want to know about last group\n            if len(group) > 1:\n                group.insert(0, '(')\n                group.insert(-1, ')')\n                didwrap = True\n\n    # Might not need to wrap the RHS if wrapped the LHS\n    if not didwrap or counts[-1] > maxline - indent - 10:\n        groups[-1].insert(0, '(')\n        groups[-1].append(')')\n\n    return [item for group in groups for item in group]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting a line into groups by assignment", "response": "def get_assign_groups(line, ops=ops):\n    \"\"\" Split a line into groups by assignment (including\n        augmented assignment)\n    \"\"\"\n    group = []\n    for item in line:\n        group.append(item)\n        if item in ops:\n            yield group\n            group = []\n    yield group"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the string up and force - feed some replacements to make sure it will round - trip OK", "response": "def _prep_triple_quotes(s, mysplit=mysplit, replacements=replacements):\n    \"\"\" Split the string up and force-feed some replacements\n        to make sure it will round-trip OK\n    \"\"\"\n\n    s = mysplit(s)\n    s[1::2] = (replacements[x] for x in s[1::2])\n    return ''.join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pretty_string(s, embedded, current_line, uni_lit=False,\n                  min_trip_str=20, max_line=100):\n    \"\"\"There are a lot of reasons why we might not want to or\n       be able to return a triple-quoted string.  We can always\n       punt back to the default normal string.\n    \"\"\"\n\n    default = repr(s)\n\n    # Punt on abnormal strings\n    if (isinstance(s, special_unicode) or not isinstance(s, basestring)):\n        return default\n    if uni_lit and isinstance(s, bytes):\n        return 'b' + default\n\n    len_s = len(default)\n\n    if current_line.strip():\n        len_current = len(current_line)\n        second_line_start = s.find('\\n') + 1\n        if embedded > 1 and not second_line_start:\n            return default\n\n        if len_s < min_trip_str:\n            return default\n\n        line_indent = len_current - len(current_line.lstrip())\n\n        # Could be on a line by itself...\n        if embedded and not second_line_start:\n            return default\n\n        total_len = len_current + len_s\n        if total_len < max_line and not _properly_indented(s, line_indent):\n            return default\n\n    fancy = string_triplequote_repr(s)\n\n    # Sometimes this doesn't work.  One reason is that\n    # the AST has no understanding of whether \\r\\n was\n    # entered that way in the string or was a cr/lf in the\n    # file.  So we punt just so we can round-trip properly.\n\n    try:\n        if eval(fancy) == s and '\\r' not in fancy:\n            return fancy\n    except:\n        pass\n    return default", "response": "Return a pretty string for the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self):\n        self.pre_handlers = pre_handlers = {}\n        self.post_handlers = post_handlers = {}\n        for name in sorted(vars(type(self))):\n            if name.startswith('init_'):\n                getattr(self, name)()\n            elif name.startswith('pre_'):\n                pre_handlers[name[4:]] = getattr(self, name)\n            elif name.startswith('post_'):\n                post_handlers[name[5:]] = getattr(self, name)", "response": "This method is called by the object initialization time. It is called by the object s methods to setup the handlers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef walk(self, node, name='', list=list, len=len, type=type):\n        pre_handlers = self.pre_handlers.get\n        post_handlers = self.post_handlers.get\n        nodestack = self.nodestack\n        emptystack = len(nodestack)\n        append, pop = nodestack.append, nodestack.pop\n        append([node, name, list(iter_node(node, name + '_item')), -1])\n        while len(nodestack) > emptystack:\n            node, name, subnodes, index = nodestack[-1]\n            if index >= len(subnodes):\n                handler = (post_handlers(type(node).__name__) or\n                           post_handlers(name + '_name'))\n                if handler is None:\n                    pop()\n                    continue\n                self.cur_node = node\n                self.cur_name = name\n                handler()\n                current = nodestack and nodestack[-1]\n                popstack = current and current[0] is node\n                if popstack and current[-1] >= len(current[-2]):\n                    pop()\n                continue\n            nodestack[-1][-1] = index + 1\n            if index < 0:\n                handler = (pre_handlers(type(node).__name__) or\n                           pre_handlers(name + '_name'))\n                if handler is not None:\n                    self.cur_node = node\n                    self.cur_name = name\n                    if handler():\n                        pop()\n            else:\n                node, name = subnodes[index]\n                append([node, name, list(iter_node(node, name + '_item')), -1])", "response": "Walk the tree starting at a given node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace(self, new_node):\n        cur_node = self.cur_node\n        nodestack = self.nodestack\n        cur = nodestack.pop()\n        prev = nodestack[-1]\n        index = prev[-1] - 1\n        oldnode, name = prev[-2][index]\n        assert cur[0] is cur_node is oldnode, (cur[0], cur_node, prev[-2],\n                                               index)\n        parent = prev[0]\n        if isinstance(parent, list):\n            parent[index] = new_node\n        else:\n            setattr(parent, name, new_node)", "response": "Replace a node after checking integrity of node stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_node(node, name='', unknown=None,\n              # Runtime optimization\n              list=list, getattr=getattr, isinstance=isinstance,\n              enumerate=enumerate, missing=NonExistent):\n    \"\"\"Iterates over an object:\n\n       - If the object has a _fields attribute,\n         it gets attributes in the order of this\n         and returns name, value pairs.\n\n       - Otherwise, if the object is a list instance,\n         it returns name, value pairs for each item\n         in the list, where the name is passed into\n         this function (defaults to blank).\n\n       - Can update an unknown set with information about\n         attributes that do not exist in fields.\n    \"\"\"\n    fields = getattr(node, '_fields', None)\n    if fields is not None:\n        for name in fields:\n            value = getattr(node, name, missing)\n            if value is not missing:\n                yield value, name\n        if unknown is not None:\n            unknown.update(set(vars(node)) - set(fields))\n    elif isinstance(node, list):\n        for value in node:\n            yield value, name", "response": "Iterate over a node s attributes and return a list of name value pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump_tree(node, name=None, initial_indent='', indentation='    ',\n              maxline=120, maxmerged=80,\n              # Runtime optimization\n              iter_node=iter_node, special=ast.AST,\n              list=list, isinstance=isinstance, type=type, len=len):\n    \"\"\"Dumps an AST or similar structure:\n\n       - Pretty-prints with indentation\n       - Doesn't print line/column/ctx info\n\n    \"\"\"\n    def dump(node, name=None, indent=''):\n        level = indent + indentation\n        name = name and name + '=' or ''\n        values = list(iter_node(node))\n        if isinstance(node, list):\n            prefix, suffix = '%s[' % name, ']'\n        elif values:\n            prefix, suffix = '%s%s(' % (name, type(node).__name__), ')'\n        elif isinstance(node, special):\n            prefix, suffix = name + type(node).__name__, ''\n        else:\n            return '%s%s' % (name, repr(node))\n        node = [dump(a, b, level) for a, b in values if b != 'ctx']\n        oneline = '%s%s%s' % (prefix, ', '.join(node), suffix)\n        if len(oneline) + len(indent) < maxline:\n            return '%s' % oneline\n        if node and len(prefix) + len(node[0]) < maxmerged:\n            prefix = '%s%s,' % (prefix, node.pop(0))\n        node = (',\\n%s' % level).join(node).lstrip()\n        return '%s\\n%s%s%s' % (prefix, level, node, suffix)\n    return dump(node, name, initial_indent)", "response": "Dumps an AST or similar structure to a tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef strip_tree(node,\n               # Runtime optimization\n               iter_node=iter_node, special=ast.AST,\n               list=list, isinstance=isinstance, type=type, len=len):\n    \"\"\"Strips an AST by removing all attributes not in _fields.\n\n    Returns a set of the names of all attributes stripped.\n\n    This canonicalizes two trees for comparison purposes.\n    \"\"\"\n    stripped = set()\n\n    def strip(node, indent):\n        unknown = set()\n        leaf = True\n        for subnode, _ in iter_node(node, unknown=unknown):\n            leaf = False\n            strip(subnode, indent + '    ')\n        if leaf:\n            if isinstance(node, special):\n                unknown = set(vars(node))\n        stripped.update(unknown)\n        for name in unknown:\n            delattr(node, name)\n        if hasattr(node, 'ctx'):\n            delattr(node, 'ctx')\n            if 'ctx' in node._fields:\n                mylist = list(node._fields)\n                mylist.remove('ctx')\n                node._fields = mylist\n    strip(node, '')\n    return stripped", "response": "Strips an AST by removing all attributes not in _fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_source(node, indent_with=' ' * 4, add_line_information=False,\n              pretty_string=pretty_string, pretty_source=pretty_source):\n    \"\"\"This function can convert a node tree back into python sourcecode.\n    This is useful for debugging purposes, especially if you're dealing with\n    custom asts not generated by python itself.\n\n    It could be that the sourcecode is evaluable when the AST itself is not\n    compilable / evaluable.  The reason for this is that the AST contains some\n    more data than regular sourcecode does, which is dropped during\n    conversion.\n\n    Each level of indentation is replaced with `indent_with`.  Per default this\n    parameter is equal to four spaces as suggested by PEP 8, but it might be\n    adjusted to match the application's styleguide.\n\n    If `add_line_information` is set to `True` comments for the line numbers\n    of the nodes are added to the output.  This can be used to spot wrong line\n    number information of statement nodes.\n\n    \"\"\"\n    generator = SourceGenerator(indent_with, add_line_information,\n                                pretty_string)\n    generator.visit(node)\n    generator.result.append('\\n')\n    if set(generator.result[0]) == set('\\n'):\n        generator.result[0] = ''\n    return pretty_source(generator.result)", "response": "This function can convert a node tree into a python sourcecode string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an AST node object returns a string containing the symbol.", "response": "def get_op_symbol(obj, fmt='%s', symbol_data=symbol_data, type=type):\n    \"\"\"Given an AST node object, returns a string containing the symbol.\n    \"\"\"\n    return fmt % symbol_data[type(obj)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_py_files(srctree, ignore=None):\n\n        if not os.path.isdir(srctree):\n            yield os.path.split(srctree)\n        for srcpath, _, fnames in os.walk(srctree):\n            # Avoid infinite recursion for silly users\n            if ignore is not None and ignore in srcpath:\n                continue\n            for fname in (x for x in fnames if x.endswith('.py')):\n                yield srcpath, fname", "response": "Return all the python files in a source tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_file(fname):\n        try:\n            with fopen(fname) as f:\n                fstr = f.read()\n        except IOError:\n            if fname != 'stdin':\n                raise\n            sys.stdout.write('\\nReading from stdin:\\n\\n')\n            fstr = sys.stdin.read()\n        fstr = fstr.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n        if not fstr.endswith('\\n'):\n            fstr += '\\n'\n        return ast.parse(fstr, filename=fname)", "response": "Parse a python file into an AST."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_file_info(codeobj):\n        fname = getattr(codeobj, '__file__', None)\n        linenum = 0\n        if fname is None:\n            func_code = codeobj.__code__\n            fname = func_code.co_filename\n            linenum = func_code.co_firstlineno\n        fname = fname.replace('.pyc', '.py')\n        return fname, linenum", "response": "Returns the file and line number of a code object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_token_age(callback_token):\n    try:\n        token = CallbackToken.objects.get(key=callback_token, is_active=True)\n        seconds = (timezone.now() - token.created_at).total_seconds()\n        token_expiry_time = api_settings.PASSWORDLESS_TOKEN_EXPIRE_TIME\n\n        if seconds <= token_expiry_time:\n            return True\n        else:\n            # Invalidate our token.\n            token.is_active = False\n            token.save()\n            return False\n\n    except CallbackToken.DoesNotExist:\n        # No valid token.\n        return False", "response": "Returns True if a given token is within the age expiration limit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_user_alias(user, token):\n    if token.to_alias_type == 'EMAIL':\n        if token.to_alias == getattr(user, api_settings.PASSWORDLESS_USER_EMAIL_FIELD_NAME):\n            setattr(user, api_settings.PASSWORDLESS_USER_EMAIL_VERIFIED_FIELD_NAME, True)\n    elif token.to_alias_type == 'MOBILE':\n        if token.to_alias == getattr(user, api_settings.PASSWORDLESS_USER_MOBILE_FIELD_NAME):\n            setattr(user, api_settings.PASSWORDLESS_USER_MOBILE_VERIFIED_FIELD_NAME, True)\n    else:\n        return False\n    user.save()\n    return True", "response": "Verify a user s contact point based on the accepted token type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an email to the user object with a callback token.", "response": "def send_email_with_callback_token(user, email_token, **kwargs):\n    \"\"\"\n    Sends a Email to user.email.\n\n    Passes silently without sending in test environment\n    \"\"\"\n\n    try:\n        if api_settings.PASSWORDLESS_EMAIL_NOREPLY_ADDRESS:\n            # Make sure we have a sending address before sending.\n\n            # Get email subject and message\n            email_subject = kwargs.get('email_subject',\n                                       api_settings.PASSWORDLESS_EMAIL_SUBJECT)\n            email_plaintext = kwargs.get('email_plaintext',\n                                         api_settings.PASSWORDLESS_EMAIL_PLAINTEXT_MESSAGE)\n            email_html = kwargs.get('email_html',\n                                    api_settings.PASSWORDLESS_EMAIL_TOKEN_HTML_TEMPLATE_NAME)\n\n            # Inject context if user specifies.\n            context = inject_template_context({'callback_token': email_token.key, })\n            html_message = loader.render_to_string(email_html, context,)\n            send_mail(\n                email_subject,\n                email_plaintext % email_token.key,\n                api_settings.PASSWORDLESS_EMAIL_NOREPLY_ADDRESS,\n                [getattr(user, api_settings.PASSWORDLESS_USER_EMAIL_FIELD_NAME)],\n                fail_silently=False,\n                html_message=html_message,)\n\n        else:\n            logger.debug(\"Failed to send token email. Missing PASSWORDLESS_EMAIL_NOREPLY_ADDRESS.\")\n            return False\n        return True\n\n    except Exception as e:\n        logger.debug(\"Failed to send token email to user: %d.\"\n                  \"Possibly no email on user object. Email entered was %s\" %\n                  (user.id, getattr(user, api_settings.PASSWORDLESS_USER_EMAIL_FIELD_NAME)))\n        logger.debug(e)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a SMS to a user using a callback token.", "response": "def send_sms_with_callback_token(user, mobile_token, **kwargs):\n    \"\"\"\n    Sends a SMS to user.mobile via Twilio.\n\n    Passes silently without sending in test environment.\n    \"\"\"\n    base_string = kwargs.get('mobile_message', api_settings.PASSWORDLESS_MOBILE_MESSAGE)\n\n    try:\n\n        if api_settings.PASSWORDLESS_MOBILE_NOREPLY_NUMBER:\n            # We need a sending number to send properly\n            if api_settings.PASSWORDLESS_TEST_SUPPRESSION is True:\n                # we assume success to prevent spamming SMS during testing.\n                return True\n\n            from twilio.rest import Client\n            twilio_client = Client(os.environ['TWILIO_ACCOUNT_SID'], os.environ['TWILIO_AUTH_TOKEN'])\n            twilio_client.messages.create(\n                body=base_string % mobile_token.key,\n                to=getattr(user, api_settings.PASSWORDLESS_USER_MOBILE_FIELD_NAME),\n                from_=api_settings.PASSWORDLESS_MOBILE_NOREPLY_NUMBER\n            )\n            return True\n        else:\n            logger.debug(\"Failed to send token sms. Missing PASSWORDLESS_MOBILE_NOREPLY_NUMBER.\")\n            return False\n    except ImportError:\n        logger.debug(\"Couldn't import Twilio client. Is twilio installed?\")\n        return False\n    except KeyError:\n        logger.debug(\"Couldn't send SMS.\"\n                  \"Did you set your Twilio account tokens and specify a PASSWORDLESS_MOBILE_NOREPLY_NUMBER?\")\n    except Exception as e:\n        logger.debug(\"Failed to send token SMS to user: {}. \"\n                  \"Possibly no mobile number on user object or the twilio package isn't set up yet. \"\n                  \"Number entered was {}\".format(user.id, getattr(user, api_settings.PASSWORDLESS_USER_MOBILE_FIELD_NAME)))\n        logger.debug(e)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvalidate all previously issued tokens as a post_save signal.", "response": "def invalidate_previous_tokens(sender, instance, **kwargs):\n    \"\"\"\n    Invalidates all previously issued tokens as a post_save signal.\n    \"\"\"\n    active_tokens = None\n    if isinstance(instance, CallbackToken):\n        active_tokens = CallbackToken.objects.active().filter(user=instance.user).exclude(id=instance.id)\n\n    # Invalidate tokens\n    if active_tokens:\n        for token in active_tokens:\n            token.is_active = False\n            token.save()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_unique_tokens(sender, instance, **kwargs):\n    if isinstance(instance, CallbackToken):\n        if CallbackToken.objects.filter(key=instance.key, is_active=True).exists():\n            instance.key = generate_numeric_token()", "response": "Ensures that mobile and email tokens are unique."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_alias_verification(sender, instance, **kwargs):\n    if isinstance(instance, User):\n\n        if instance.id:\n\n            if api_settings.PASSWORDLESS_USER_MARK_EMAIL_VERIFIED is True:\n                \"\"\"\n                For marking email aliases as not verified when a user changes it.\n                \"\"\"\n                email_field = api_settings.PASSWORDLESS_USER_EMAIL_FIELD_NAME\n                email_verified_field = api_settings.PASSWORDLESS_USER_EMAIL_VERIFIED_FIELD_NAME\n\n                # Verify that this is an existing instance and not a new one.\n                try:\n                    user_old = User.objects.get(id=instance.id)  # Pre-save object\n                    instance_email = getattr(instance, email_field)  # Incoming Email\n                    old_email = getattr(user_old, email_field)  # Pre-save object email\n\n                    if instance_email != old_email and instance_email != \"\" and instance_email is not None:\n                        # Email changed, verification should be flagged\n                        setattr(instance, email_verified_field, False)\n                        if api_settings.PASSWORDLESS_AUTO_SEND_VERIFICATION_TOKEN is True:\n                            email_subject = api_settings.PASSWORDLESS_EMAIL_VERIFICATION_SUBJECT\n                            email_plaintext = api_settings.PASSWORDLESS_EMAIL_VERIFICATION_PLAINTEXT_MESSAGE\n                            email_html = api_settings.PASSWORDLESS_EMAIL_VERIFICATION_TOKEN_HTML_TEMPLATE_NAME\n                            message_payload = {'email_subject': email_subject,\n                                               'email_plaintext': email_plaintext,\n                                               'email_html': email_html}\n                            success = TokenService.send_token(instance, 'email', **message_payload)\n\n                            if success:\n                                logger.info('drfpasswordless: Successfully sent email on updated address: %s'\n                                            % instance_email)\n                            else:\n                                logger.info('drfpasswordless: Failed to send email to updated address: %s'\n                                            % instance_email)\n\n                except User.DoesNotExist:\n                    # User probably is just initially being created\n                    setattr(instance, email_verified_field, True)\n\n            if api_settings.PASSWORDLESS_USER_MARK_MOBILE_VERIFIED is True:\n                \"\"\"\n                For marking mobile aliases as not verified when a user changes it.\n                \"\"\"\n                mobile_field = api_settings.PASSWORDLESS_USER_MOBILE_FIELD_NAME\n                mobile_verified_field = api_settings.PASSWORDLESS_USER_MOBILE_VERIFIED_FIELD_NAME\n\n                # Verify that this is an existing instance and not a new one.\n                try:\n                    user_old = User.objects.get(id=instance.id)  # Pre-save object\n                    instance_mobile = getattr(instance, mobile_field)  # Incoming mobile\n                    old_mobile = getattr(user_old, mobile_field)  # Pre-save object mobile\n\n                    if instance_mobile != old_mobile and instance_mobile != \"\" and instance_mobile is not None:\n                        # Mobile changed, verification should be flagged\n                        setattr(instance, mobile_verified_field, False)\n                        if api_settings.PASSWORDLESS_AUTO_SEND_VERIFICATION_TOKEN is True:\n                            mobile_message = api_settings.PASSWORDLESS_MOBILE_MESSAGE\n                            message_payload = {'mobile_message': mobile_message}\n                            success = TokenService.send_token(instance, 'mobile', **message_payload)\n\n                            if success:\n                                logger.info('drfpasswordless: Successfully sent SMS on updated mobile: %s'\n                                            % instance_mobile)\n                            else:\n                                logger.info('drfpasswordless: Failed to send SMS to updated mobile: %s'\n                                            % instance_mobile)\n\n                except User.DoesNotExist:\n                    # User probably is just initially being created\n                    setattr(instance, mobile_verified_field, True)", "response": "Updates the email verification token for an existing user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating a dihedral angle between two points.", "response": "def calc_dihedral(point1, point2, point3, point4):\n    \"\"\"Calculates a dihedral angle\n\n    Here, two planes are defined by (point1, point2, point3) and\n    (point2, point3, point4). The angle between them is returned.\n\n    Parameters\n    ----------\n    point1, point2, point3, point4 : array-like, shape=(3,), dtype=float\n        Four points that define two planes\n\n    Returns\n    -------\n    float\n        The dihedral angle between the two planes defined by the four\n        points.\n    \"\"\"\n    points = np.array([point1, point2, point3, point4])\n    x = np.cross(points[1] - points[0], points[2] - points[1])\n    y = np.cross(points[2] - points[1], points[3] - points[2])\n    return angle(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that coordinates are - L and L and then shifts coordinates if necessary.", "response": "def coord_shift(xyz, box):\n    \"\"\"Ensures that coordinates are -L/2, L/2\n\n    Checks if coordinates are -L/2, L/2 and then shifts coordinates\n    if necessary. For example, if coordinates are 0, L, then a shift\n    is applied to move coordinates to -L/2, L/2. If a shift is not\n    necessary, the points are returned unmodified.\n\n    Parameters\n    ----------\n    xyz : numpy.array of points with shape N x 3\n    box : numpy.array specifing the size of box ie [Lx, Ly, Lz]\n\n    Returns\n    -------\n    xyz : numpy.array of points with shape N x 3\n    \"\"\"\n    box = np.asarray(box)\n    assert box.shape == (3,)\n\n    box_max = box/2.\n    box_min = -box_max\n    # Shift all atoms\n    if np.greater(xyz, box_max).any():\n        xyz -= box_max\n    elif np.less(xyz, box_min).any():\n        xyz += box_max\n\n    return xyz"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscales the points in the Pattern.", "response": "def scale(self, by):\n        \"\"\"Scale the points in the Pattern.\n\n        Parameters\n        ----------\n        by : float or np.ndarray, shape=(3,)\n            The factor to scale by. If a scalar, scale all directions isotropically.\n            If np.ndarray, scale each direction independently.\n        \"\"\"\n        self.points *= np.asarray([by])\n        self._adjust_ports()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\narrange copies of a Compound as specified by the Pattern.", "response": "def apply(self, compound, orientation='', compound_port=''):\n        \"\"\"Arrange copies of a Compound as specified by the Pattern.\n\n        Parameters\n        ----------\n        compound\n        orientation\n\n        Returns\n        -------\n\n        \"\"\"\n        compounds = list()\n        if self.orientations.get(orientation):\n            for port in self.orientations[orientation]:\n                new_compound = clone(compound)\n                new_port = new_compound.labels[compound_port]\n                (new_compound, new_port['up'], port['up'])\n\n                compounds.append(new_compound)\n        else:\n            for point in self.points:\n                new_compound = clone(compound)\n                new_compound.translate(point)\n\n                compounds.append(new_compound)\n        return compounds"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_to_compound(self, guest, guest_port_name='down', host=None,\n                          backfill=None, backfill_port_name='up', scale=True):\n        \"\"\"Attach copies of a guest Compound to Ports on a host Compound.\n\n        Parameters\n        ----------\n        guest : mb.Compound\n            The Compound prototype to be applied to the host Compound\n        guest_port_name : str, optional, default='down'\n            The name of the port located on `guest` to attach to the host\n        host : mb.Compound, optional, default=None\n            A Compound with available ports to add copies of `guest` to\n        backfill : mb.Compound, optional, default=None\n            A Compound to add to the remaining available ports on `host`\n            after clones of `guest` have been added for each point in the\n            pattern\n        backfill_port_name : str, optional, default='up'\n            The name of the port located on `backfill` to attach to the host\n        scale : bool, optional, default=True\n            Scale the points in the pattern to the lengths of the `host`'s\n            `boundingbox` and shift them by the `boundingbox`'s mins\n\n        Returns\n        -------\n\n        \"\"\"\n        n_ports = len(host.available_ports())\n        assert n_ports >= self.points.shape[0], \"Not enough ports for pattern.\"\n        assert_port_exists(guest_port_name, guest)\n        box = host.boundingbox\n        if scale:\n            self.scale(box.lengths)\n            self.points += box.mins\n        pattern = self.points\n        port_positions = np.empty(shape=(n_ports, 3))\n        port_list = list()\n        for port_idx, port in enumerate(host.available_ports()):\n            port_positions[port_idx, :] = port['up']['middle'].pos\n            port_list.append(port)\n        used_ports = set()  # Keep track of used ports for backfilling.\n        guests = []\n        for point in pattern:\n            closest_point_idx = np.argmin(host.min_periodic_distance(point, port_positions))\n            closest_port = port_list[closest_point_idx]\n            used_ports.add(closest_port)\n\n            # Attach the guest to the closest port.\n            new_guest = clone(guest)\n            force_overlap(new_guest, new_guest.labels[guest_port_name], closest_port)\n            guests.append(new_guest)\n\n            # Move the port as far away as possible (simpler than removing it).\n            # There may well be a more elegant/efficient way of doing this.\n            port_positions[closest_point_idx, :] = np.array([np.inf, np.inf, np.inf])\n        backfills = []\n        if backfill:\n            assert_port_exists(backfill_port_name, backfill)\n            # Attach the backfilling Compound to unused ports.\n            for port in port_list:\n                if port not in used_ports:\n                    new_backfill = clone(backfill)\n                    # Might make sense to have a backfill_port_name option...\n                    force_overlap(new_backfill,\n                                  new_backfill.labels[backfill_port_name],\n                                  port)\n                    backfills.append(new_backfill)\n        return guests, backfills", "response": "Adds copies of a guest Compound to Ports on a host Compound."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sanitize_inputs(self, lattice_spacing, lattice_vectors,\n                         lattice_points, angles):\n        \"\"\"Check for proper inputs and set instance attributes.\n\n        validate_inputs takes the data passed to the constructor by the user\n        and will ensure that the data is correctly formatted and will then\n        set its instance attributes.\n\n        validate_inputs checks that dimensionality is maintained,\n        the unit cell is right handed, the area or volume of the unit cell\n        is positive and non-zero for 2D and 3D respectively, lattice spacing\n        are provided, basis vectors do not overlap when the unit cell is\n        expanded.\n\n        Exceptions Raised\n        -----------------\n        TypeError : incorrect typing of the input parameters.\n\n        ValueError : values are not within restrictions.\n        \"\"\"\n\n        if angles is not None and lattice_vectors is not None:\n            raise ValueError('Overdefined system: angles and lattice_vectors '\n                             'provided. Only one of these should be passed.')\n\n        self._validate_lattice_spacing(lattice_spacing)\n\n        if angles is not None:\n            self._validate_angles(angles)\n            self.lattice_vectors = self._from_lattice_parameters(self.angles)\n        else:\n            self._validate_lattice_vectors(lattice_vectors)\n            self.angles = self._from_lattice_vectors()\n\n        self._validate_lattice_points(lattice_points)", "response": "Validate the input data and set instance attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that the lattice spacing is provided and correct.", "response": "def _validate_lattice_spacing(self, lattice_spacing):\n        \"\"\"Ensure that lattice spacing is provided and correct.\n\n        _validate_lattice_spacing will ensure that the lattice spacing\n        provided are acceptable values. Additional Numpy errors can also occur\n        due to the conversion to a Numpy array.\n\n        Exceptions Raised\n        -----------------\n        ValueError : Incorrect lattice_spacing input\n        \"\"\"\n\n        dataType = np.float64\n\n        if lattice_spacing is not None:\n            lattice_spacing = np.asarray(lattice_spacing, dtype=dataType)\n            lattice_spacing = lattice_spacing.reshape((3,))\n            if np.shape(lattice_spacing) != (self.dimension,):\n                raise ValueError('Lattice spacing should be a vector of '\n                                 'size:({},). Please include lattice spacing '\n                                 'of size >= 0 depending on desired '\n                                 'dimensionality.'\n                                 .format(self.dimension))\n        else:\n            raise ValueError('No lattice_spacing provided. Please provide '\n                             'lattice spacing\\'s that are >= 0. with size ({},)'\n                             .format((self.dimension)))\n\n        if np.any(np.isnan(lattice_spacing)):\n            raise ValueError('None type or NaN type values present in '\n                             'lattice_spacing: {}.'.format(lattice_spacing))\n        elif np.any(lattice_spacing < 0.0):\n            raise ValueError('Negative lattice spacing value. One of '\n                             'the spacing: {} is negative.'\n                             .format(lattice_spacing))\n\n        self.lattice_spacing = lattice_spacing"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that the angles between the lattice vectors are correct.", "response": "def _validate_angles(self, angles):\n        \"\"\"Ensure that the angles between the lattice_vectors are correct\"\"\"\n\n        dataType = np.float64\n        tempAngles = np.asarray(angles, dtype=dataType)\n        tempAngles = tempAngles.reshape((3,))\n\n        if np.shape(tempAngles) == (self.dimension,):\n            if np.sum(tempAngles) < 360.0 or np.sum(tempAngles) > -360.0:\n                if (np.all(tempAngles != 180.0)\n                        and np.all(tempAngles != 0.0)):\n                    pass\n                else:\n                    raise ValueError('Angles cannot be 180.0 or 0.0')\n            else:\n                raise ValueError('Angles sum: {} is either greater than '\n                                 '360.0 or less than -360.0'\n                                 .format(np.sum(tempAngles)))\n\n            for subset in it.permutations(tempAngles, r=self.dimension):\n                if not subset[0] < np.sum(tempAngles) - subset[0]:\n                    raise ValueError('Each angle provided must be less'\n                                     'than the sum of the other angles. '\n                                     '{} is greater.'.format(subset[0]))\n        else:\n            raise ValueError('Incorrect array size. When converted to a '\n                             'Numpy array, the shape is: {}, expected {}.'\n                             .format(np.shape(tempAngles),\n                                     (3,)))\n        self.angles = tempAngles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that the lattice_vectors are reasonable inputs.", "response": "def _validate_lattice_vectors(self, lattice_vectors):\n        \"\"\"Ensure that the lattice_vectors are reasonable inputs.\n\n        \"\"\"\n        dataType = np.float64\n        if lattice_vectors is None:\n                lattice_vectors = np.identity(self.dimension, dtype=dataType)\n        else:\n            lattice_vectors = np.asarray(lattice_vectors, dtype=dataType)\n\n            if (self.dimension, self.dimension) != np.shape(lattice_vectors):\n                raise ValueError('Dimensionality of lattice_vectors is '\n                                 ' of shape {} not {}.'\n                                 .format(np.shape(lattice_vectors),\n                                         (self.dimension, self.dimension)))\n\n            det = np.linalg.det(lattice_vectors)\n            if abs(det) == 0.0:\n                raise ValueError('Co-linear vectors: {}'\n                                 'have a determinant of 0.0. Does not '\n                                 'define a unit cell.'\n                                 .format(lattice_vectors))\n\n            if det <= 0.0:\n                raise ValueError('Negative Determinant: the determinant '\n                                 'of {} is negative, indicating a left-'\n                                 'handed system.' .format(det))\n        self.lattice_vectors = lattice_vectors"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts Bravais lattice parameters to lattice vectors.", "response": "def _from_lattice_parameters(self, angles):\n        \"\"\"Convert Bravais lattice parameters to lattice vectors.\n\n        _from_lattice_parameters will generate the lattice vectors based on\n        the parameters necessary to build a Bravais Lattice. The lattice\n        vectors are in the lower diagonal matrix form.\n\n        This was adapted from the ASE triclinic.py lattice parameter code.\n\n        S. R. Bahn and K. W. Jacobsen\n        An object-oriented scripting interface to a\n        legacy electronic structure code Comput. Sci. Eng., Vol. 4, 56-66, 2002\n\n        Parameters\n        ----------\n        angles : list-like, required\n            Angles of bravais lattice.\n        \"\"\"\n\n        dataType = np.float64\n        (alpha, beta, gamma) = angles\n\n        radianConversion = np.pi / 180.0\n        cosa = np.cos(alpha * radianConversion)\n        cosb = np.cos(beta * radianConversion)\n        sinb = np.sin(beta * radianConversion)\n        cosg = np.cos(gamma * radianConversion)\n        sing = np.sin(gamma * radianConversion)\n        matCoef_y = (cosa - cosb * cosg) / sing\n        matCoef_z = np.power(sinb, 2, dtype=dataType) - \\\n            np.power(matCoef_y, 2, dtype=dataType)\n\n        if matCoef_z > 0.:\n            matCoef_z = np.sqrt(matCoef_z)\n        else:\n            raise ValueError('Incorrect lattice vector coefficients.'\n                             'Lattice parameters chosen return a non-positive '\n                             'z vector.')\n\n        lattice_vec = [[1, 0, 0],\n                       [cosg, sing, 0],\n                       [cosb, matCoef_y, matCoef_z]]\n\n        return np.asarray(lattice_vec, dtype=np.float64)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the angles between the vectors that define the lattice.", "response": "def _from_lattice_vectors(self):\n        \"\"\"Calculate the angles between the vectors that define the lattice.\n\n        _from_lattice_vectors will calculate the angles alpha, beta, and\n        gamma from the Lattice object attribute lattice_vectors.\n        \"\"\"\n\n        degreeConvsersion = 180.0 / np.pi\n        vector_magnitudes = np.linalg.norm(self.lattice_vectors, axis=1)\n\n        a_dot_b = np.dot(self.lattice_vectors[0], self.lattice_vectors[1])\n        b_dot_c = np.dot(self.lattice_vectors[1], self.lattice_vectors[2])\n        a_dot_c = np.dot(self.lattice_vectors[0], self.lattice_vectors[2])\n\n        alpha_raw = b_dot_c / (vector_magnitudes[1] * vector_magnitudes[2])\n        beta_raw = a_dot_c / (vector_magnitudes[0] * vector_magnitudes[2])\n        gamma_raw = a_dot_b / (vector_magnitudes[0] * vector_magnitudes[1])\n\n        alpha = np.arccos(np.clip(alpha_raw, -1.0, 1.0)) * degreeConvsersion\n        beta = np.arccos(np.clip(beta_raw, -1.0, 1.0)) * degreeConvsersion\n        gamma = np.arccos(np.clip(gamma_raw, -1.0, 1.0)) * degreeConvsersion\n\n        return np.asarray([alpha, beta, gamma], dtype=np.float64)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef populate(self, compound_dict=None, x=1, y=1, z=1):\n        error_dict = {0: 'X', 1: 'Y', 2: 'Z'}\n        try:\n            x = int(x)\n            y = int(y)\n            z = int(z)\n        except (ValueError, TypeError):\n            raise ValueError('Cannot convert replication amounts into '\n                             'integers. x= {}, y= {}, z= {} needs to '\n                             'be an int.'.format(x, y, z))\n\n        for replication_amount in x, y, z:\n            if replication_amount is None:\n                raise ValueError('Attempt to replicate None times. '\n                                 'None is not an acceptable replication '\n                                 'amount, 1 is the default.')\n\n        for replication_amount, index in zip([x, y, z], range(3)):\n            if replication_amount < 1:\n                raise ValueError('Incorrect populate value: {} : {} is < 1. '\n                                 .format(error_dict[index],\n                                         replication_amount))\n\n        if ((isinstance(compound_dict, dict)) or (compound_dict is None)):\n            pass\n        else:\n            raise TypeError('Compound dictionary is not of type dict. '\n                            '{} was passed.'.format(type(compound_dict)))\n\n        cell = defaultdict(list)\n        [a, b, c] = self.lattice_spacing\n\n        transform_mat = self.lattice_vectors\n        # unit vectors\n        transform_mat = np.asarray(transform_mat, dtype=np.float64)\n        transform_mat = np.reshape(transform_mat, newshape=(3,3))\n        norms = np.linalg.norm(transform_mat, axis=1)\n\n        # normalized vectors for change of basis\n        unit_vecs = np.divide(transform_mat.transpose(), norms)\n\n        for key, locations in self.lattice_points.items():\n            for coords in locations:\n                for replication in it.product(range(x), range(y), range(z)):\n                    temp_location = list()\n\n                    new_coords = np.asarray(coords, dtype=np.float64)\n                    new_coords = np.reshape(new_coords, (1, 3), order='C')\n\n                    new_coords[0][0] = new_coords[0][0] + replication[0]\n                    new_coords[0][1] = new_coords[0][1] + replication[1]\n                    new_coords[0][2] = new_coords[0][2] + replication[2]\n\n                    # change of basis to cartesian\n                    new_coords = np.dot(unit_vecs, new_coords.transpose())\n\n                    new_coords[0] = new_coords[0] * a\n                    new_coords[1] = new_coords[1] * b\n                    new_coords[2] = new_coords[2] * c\n                    new_coords = np.reshape(new_coords, (1, 3), order='C')\n\n                    tuple_of_coords = tuple(new_coords.flatten())\n                    cell[key].append(tuple_of_coords)\n\n        ret_lattice = mb.Compound()\n\n        if compound_dict is None:\n            for key_id, all_pos in cell.items():\n                particle = mb.Compound(name=key_id, pos=[0, 0, 0])\n                for pos in all_pos:\n                    particle_to_add = mb.clone(particle)\n                    particle_to_add.translate_to(list(pos))\n                    ret_lattice.add(particle_to_add)\n        else:\n            for key_id, all_pos in cell.items():\n                if isinstance(compound_dict[key_id], mb.Compound):\n                    compound_to_move = compound_dict[key_id]\n                    for pos in all_pos:\n                        tmp_comp = mb.clone(compound_to_move)\n                        tmp_comp.translate_to(list(pos))\n                        ret_lattice.add(tmp_comp)\n                else:\n                    err_type = type(compound_dict.get(key_id))\n                    raise TypeError('Invalid type in provided Compound '\n                                    'dictionary. For key {}, type: {} was '\n                                    'provided, not mbuild.Compound.'\n                                    .format(key_id, err_type))\n        # set periodicity\n        ret_lattice.periodicity = np.asarray([a * x, b * y, c * z], dtype=np.float64)\n        warn('Periodicity of non-rectangular lattices are not valid with '\n                    'default boxes. Only rectangular lattices are valid '\n                    'at this time.')\n\n        # if coordinates are below a certain threshold, set to 0\n        tolerance = 1e-12\n        ret_lattice.xyz_with_ports[ret_lattice.xyz_with_ports <= tolerance] = 0.\n\n        return ret_lattice", "response": "Populate the structure of the structure with the values in the input dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_layer(self, lipid_indices=None, flip_orientation=False):\n        layer = mb.Compound()\n        if not lipid_indices:\n            lipid_indices = list(range(self.n_lipids_per_layer))\n            shuffle(lipid_indices)\n\n        for n_type, n_of_lipid_type in enumerate(self.number_of_each_lipid_per_layer):\n            current_type = self.lipids[n_type][0]\n            for n_this_type, n_this_lipid_type in enumerate(range(n_of_lipid_type)):\n                lipids_placed = n_type + n_this_type\n                new_lipid = clone(current_type)\n                random_index = lipid_indices[lipids_placed]\n                position = self.pattern[random_index]\n\n                # Zero and space in z-direction\n                particles = list(new_lipid.particles())\n                ref_atom = self.ref_atoms[n_type]\n                new_lipid.translate(-particles[ref_atom].pos + self.spacing)\n\n                # Move to point on pattern\n                if flip_orientation == True:\n                    center = new_lipid.center\n                    center[2] = 0.0\n                    new_lipid.translate(-center)\n                    new_lipid.rotate(np.pi, [1, 0, 0])\n                    new_lipid.translate(center)\n                new_lipid.translate(position)\n                layer.add(new_lipid)\n        return layer, lipid_indices", "response": "Create a monolayer of lipids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef solvate_bilayer(self):\n        solvent_number_density = self.solvent.n_particles / np.prod(self.solvent.periodicity)\n\n        lengths = self.lipid_box.lengths\n        water_box_z = self.solvent_per_layer / (lengths[0] * lengths[1] * solvent_number_density)\n\n        mins = self.lipid_box.mins\n        maxs = self.lipid_box.maxs\n        bilayer_solvent_box = mb.Box(mins=[mins[0], mins[1], maxs[2]],\n                                     maxs=[maxs[0], maxs[1], maxs[2] + 2 * water_box_z])\n\n        self.solvent_components.add(mb.fill_box(self.solvent, bilayer_solvent_box))", "response": "Solvate the constructed bilayer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solvent_per_layer(self):\n        if self._solvent_per_layer:\n            return self._solvent_per_layer\n\n        assert not (self.solvent_per_lipid is None and self.n_solvent is None)\n        if self.solvent_per_lipid is not None:\n            assert self.n_solvent is None\n            self._solvent_per_layer = self.n_lipids_per_layer * self.solvent_per_lipid\n        elif self.n_solvent is not None:\n            assert self.solvent_per_lipid is None\n            self._solvent_per_layer = self.n_solvent / 2\n        return self._solvent_per_layer", "response": "Determine the number of solvent molecules per single layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lipid_box(self):\n        if self._lipid_box:\n            return self._lipid_box\n        else:\n            self._lipid_box = self.lipid_components.boundingbox\n            # Add buffer around lipid box.\n            self._lipid_box.mins -= np.array([0.5*np.sqrt(self.apl),\n                                              0.5*np.sqrt(self.apl),\n                                              0.5*np.sqrt(self.apl)])\n            self._lipid_box.maxs += np.array([0.5*np.sqrt(self.apl),\n                                              0.5*np.sqrt(self.apl),\n                                              0.5*np.sqrt(self.apl)])\n            return self._lipid_box", "response": "The box containing all of the lipids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_xyz(filename):\n\n    compound = mb.Compound()\n\n    with open(filename, 'r') as xyz_file:\n        n_atoms = int(xyz_file.readline())\n        xyz_file.readline()\n        coords = np.zeros(shape=(n_atoms, 3), dtype=np.float64)\n        for row, _ in enumerate(coords):\n            line = xyz_file.readline().split()\n            if not line:\n                msg = ('Incorrect number of lines in input file. Based on the '\n                       'number in the first line of the file, {} rows of atoms '\n                       'were expected, but at least one fewer was found.')\n                raise MBuildError(msg.format(n_atoms))\n            coords[row] = line[1:4]\n            coords[row] *= 0.1\n            particle = mb.Compound(pos=coords[row], name=line[0])\n            compound.add(particle)\n\n        # Verify we have read the last line by ensuring the next line in blank\n        line = xyz_file.readline().split()\n        if line:\n            msg = ('Incorrect number of lines in input file. Based on the '\n                   'number in the first line of the file, {} rows of atoms '\n                   'were expected, but at least one more was found.')\n            raise MBuildError(msg.format(n_atoms))\n\n    return compound", "response": "Read an XYZ file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(filename, relative_to_module=None, compound=None, coords_only=False,\n         rigid=False, use_parmed=False, smiles=False, **kwargs):\n    \"\"\"Load a file into an mbuild compound.\n\n    Files are read using the MDTraj package unless the `use_parmed` argument is\n    specified as True. Please refer to http://mdtraj.org/1.8.0/load_functions.html\n    for formats supported by MDTraj and https://parmed.github.io/ParmEd/html/\n    readwrite.html for formats supported by ParmEd.\n\n    Parameters\n    ----------\n    filename : str\n        Name of the file from which to load atom and bond information.\n    relative_to_module : str, optional, default=None\n        Instead of looking in the current working directory, look for the file\n        where this module is defined. This is typically used in Compound\n        classes that will be instantiated from a different directory\n        (such as the Compounds located in mbuild.lib).\n    compound : mb.Compound, optional, default=None\n        Existing compound to load atom and bond information into.\n    coords_only : bool, optional, default=False\n        Only load the coordinates into an existing compoint.\n    rigid : bool, optional, default=False\n        Treat the compound as a rigid body\n    use_parmed : bool, optional, default=False\n        Use readers from ParmEd instead of MDTraj.\n    smiles: bool, optional, default=False\n        Use Open Babel to parse filename as a SMILES string\n        or file containing a SMILES string\n    **kwargs : keyword arguments\n        Key word arguments passed to mdTraj for loading.\n\n    Returns\n    -------\n    compound : mb.Compound\n\n    \"\"\"\n    # Handle mbuild *.py files containing a class that wraps a structure file\n    # in its own folder. E.g., you build a system from ~/foo.py and it imports\n    # from ~/bar/baz.py where baz.py loads ~/bar/baz.pdb.\n    if relative_to_module:\n        script_path = os.path.realpath(\n            sys.modules[relative_to_module].__file__)\n        file_dir = os.path.dirname(script_path)\n        filename = os.path.join(file_dir, filename)\n\n    if compound is None:\n        compound = Compound()\n\n    # Handle the case of a xyz file, which must use an internal reader\n    extension = os.path.splitext(filename)[-1]\n    if extension == '.xyz' and not 'top' in kwargs:\n        if coords_only:\n            tmp = read_xyz(filename)\n            if tmp.n_particles != compound.n_particles:\n                raise ValueError('Number of atoms in {filename} does not match'\n                                 ' {compound}'.format(**locals()))\n            ref_and_compound = zip(tmp._particles(include_ports=False),\n                                   compound.particles(include_ports=False))\n            for ref_particle, particle in ref_and_compound:\n                particle.pos = ref_particle.pos\n        else:\n            compound = read_xyz(filename)\n        return compound\n\n    if use_parmed:\n        warn(\n            \"use_parmed set to True.  Bonds may be inferred from inter-particle \"\n            \"distances and standard residue templates!\")\n        structure = pmd.load_file(filename, structure=True, **kwargs)\n        compound.from_parmed(structure, coords_only=coords_only)\n\n    elif smiles:\n        pybel = import_('pybel')\n        # First we try treating filename as a SMILES string\n        try:\n            mymol = pybel.readstring(\"smi\", filename)\n        # Now we treat it as a filename\n        except(OSError, IOError):\n            # For now, we only support reading in a single smiles molecule,\n            # but pybel returns a generator, so we get the first molecule\n            # and warn the user if there is more\n\n            mymol_generator = pybel.readfile(\"smi\", filename)\n            mymol_list = list(mymol_generator)\n            if len(mymol_list) == 1:\n                mymol = mymol_list[0]\n            else:\n                mymol = mymol_list[0]\n                warn(\"More than one SMILES string in file, more than one SMILES \"\n                     \"string is not supported, using {}\".format(mymol.write(\"smi\")))\n\n        tmp_dir = tempfile.mkdtemp()\n        temp_file = os.path.join(tmp_dir, 'smiles_to_mol2_intermediate.mol2')\n        mymol.make3D()\n        mymol.write(\"MOL2\", temp_file)\n        structure = pmd.load_file(temp_file, structure=True, **kwargs)\n        compound.from_parmed(structure, coords_only=coords_only)\n\n    else:\n        traj = md.load(filename, **kwargs)\n        compound.from_trajectory(traj, frame=-1, coords_only=coords_only)\n\n    if rigid:\n        compound.label_rigid_bodies()\n    return compound", "response": "Loads a file into an mbuild compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef particles(self, include_ports=False):\n        if not self.children:\n            yield self\n        else:\n            for particle in self._particles(include_ports):\n                yield particle", "response": "Yields all Particles of the Compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all Particles of the Compound.", "response": "def _particles(self, include_ports=False):\n        \"\"\"Return all Particles of the Compound. \"\"\"\n        for child in self.successors():\n            if not child.children:\n                if include_ports or not child.port_particle:\n                    yield child"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields the next set of successors of this Compound.", "response": "def successors(self):\n        \"\"\"Yield Compounds below self in the hierarchy.\n\n        Yields\n        -------\n        mb.Compound\n            The next Particle below self in the hierarchy\n\n        \"\"\"\n        if not self.children:\n            return\n        for part in self.children:\n            # Parts local to the current Compound.\n            yield part\n            # Parts further down the hierarchy.\n            for subpart in part.successors():\n                yield subpart"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates all ancestors of the Compound recursively.", "response": "def ancestors(self):\n        \"\"\"Generate all ancestors of the Compound recursively.\n\n        Yields\n        ------\n        mb.Compound\n            The next Compound above self in the hierarchy\n\n        \"\"\"\n        if self.parent is not None:\n            yield self.parent\n            for ancestor in self.parent.ancestors():\n                yield ancestor"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all Particles with a specific name", "response": "def particles_by_name(self, name):\n        \"\"\"Return all Particles of the Compound with a specific name\n\n        Parameters\n        ----------\n        name : str\n            Only particles with this name are returned\n\n        Yields\n        ------\n        mb.Compound\n            The next Particle in the Compound with the user-specified name\n\n        \"\"\"\n        for particle in self.particles():\n            if particle.name == name:\n                yield particle"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contains_rigid(self):\n        if self._check_if_contains_rigid_bodies:\n            self._check_if_contains_rigid_bodies = False\n            if any(particle.rigid_id is not None for particle in self._particles()):\n                self._contains_rigid = True\n            else:\n                self._contains_rigid = False\n        return self._contains_rigid", "response": "Returns True if the Compound contains rigid bodies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the maximum rigid body ID contained in the Compound.", "response": "def max_rigid_id(self):\n        \"\"\"Returns the maximum rigid body ID contained in the Compound.\n\n        This is usually used by compound.root to determine the maximum\n        rigid_id in the containment hierarchy.\n\n        Returns\n        -------\n        int or None\n            The maximum rigid body ID contained in the Compound. If no\n            rigid body IDs are found, None is returned\n\n        \"\"\"\n        try:\n            return max([particle.rigid_id for particle in self.particles()\n                        if particle.rigid_id is not None])\n        except ValueError:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding all particles in rigid bodies.", "response": "def rigid_particles(self, rigid_id=None):\n        \"\"\"Generate all particles in rigid bodies.\n\n        If a rigid_id is specified, then this function will only yield particles\n        with a matching rigid_id.\n\n        Parameters\n        ----------\n        rigid_id : int, optional\n            Include only particles with this rigid body ID\n\n        Yields\n        ------\n        mb.Compound\n            The next particle with a rigid_id that is not None, or the next\n            particle with a matching rigid_id if specified\n\n        \"\"\"\n        for particle in self.particles():\n            if rigid_id is not None:\n                if particle.rigid_id == rigid_id:\n                    yield particle\n            else:\n                if particle.rigid_id is not None:\n                    yield particle"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndesignate which Compounds should be treated as rigid bodies by providing all particles in self with the same rigid_id.", "response": "def label_rigid_bodies(self, discrete_bodies=None, rigid_particles=None):\n        \"\"\"Designate which Compounds should be treated as rigid bodies\n\n        If no arguments are provided, this function will treat the compound\n        as a single rigid body by providing all particles in `self` with the\n        same rigid_id. If `discrete_bodies` is not None, each instance of\n        a Compound with a name found in `discrete_bodies` will be treated as a\n        unique rigid body. If `rigid_particles` is not None, only Particles\n        (Compounds at the bottom of the containment hierarchy) matching this name\n        will be considered part of the rigid body.\n\n        Parameters\n        ----------\n        discrete_bodies : str or list of str, optional, default=None\n            Name(s) of Compound instances to be treated as unique rigid bodies.\n            Compound instances matching this (these) name(s) will be provided\n            with unique rigid_ids\n        rigid_particles : str or list of str, optional, default=None\n            Name(s) of Compound instances at the bottom of the containment\n            hierarchy (Particles) to be included in rigid bodies. Only Particles\n            matching this (these) name(s) will have their rigid_ids altered to\n            match the rigid body number.\n\n        Examples\n        --------\n        Creating a rigid benzene\n\n        >>> import mbuild as mb\n        >>> from mbuild.utils.io import get_fn\n        >>> benzene = mb.load(get_fn('benzene.mol2'))\n        >>> benzene.label_rigid_bodies()\n\n        Creating a semi-rigid benzene, where only the carbons are treated as\n        a rigid body\n\n        >>> import mbuild as mb\n        >>> from mbuild.utils.io import get_fn\n        >>> benzene = mb.load(get_fn('benzene.mol2'))\n        >>> benzene.label_rigid_bodies(rigid_particles='C')\n\n        Create a box of rigid benzenes, where each benzene has a unique rigid\n        body ID.\n\n        >>> import mbuild as mb\n        >>> from mbuild.utils.io import get_fn\n        >>> benzene = mb.load(get_fn('benzene.mol2'))\n        >>> benzene.name = 'Benzene'\n        >>> filled = mb.fill_box(benzene,\n        ...                      n_compounds=10,\n        ...                      box=[0, 0, 0, 4, 4, 4])\n        >>> filled.label_rigid_bodies(distinct_bodies='Benzene')\n\n        Create a box of semi-rigid benzenes, where each benzene has a unique\n        rigid body ID and only the carbon portion is treated as rigid.\n\n        >>> import mbuild as mb\n        >>> from mbuild.utils.io import get_fn\n        >>> benzene = mb.load(get_fn('benzene.mol2'))\n        >>> benzene.name = 'Benzene'\n        >>> filled = mb.fill_box(benzene,\n        ...                      n_compounds=10,\n        ...                      box=[0, 0, 0, 4, 4, 4])\n        >>> filled.label_rigid_bodies(distinct_bodies='Benzene',\n        ...                           rigid_particles='C')\n\n        \"\"\"\n        if discrete_bodies is not None:\n            if isinstance(discrete_bodies, string_types):\n                discrete_bodies = [discrete_bodies]\n        if rigid_particles is not None:\n            if isinstance(rigid_particles, string_types):\n                rigid_particles = [rigid_particles]\n\n        if self.root.max_rigid_id is not None:\n            rigid_id = self.root.max_rigid_id + 1\n            warn(\"{} rigid bodies already exist.  Incrementing 'rigid_id'\"\n                 \"starting from {}.\".format(rigid_id, rigid_id))\n        else:\n            rigid_id = 0\n\n        for successor in self.successors():\n            if discrete_bodies and successor.name not in discrete_bodies:\n                continue\n            for particle in successor.particles():\n                if rigid_particles and particle.name not in rigid_particles:\n                    continue\n                particle.rigid_id = rigid_id\n            if discrete_bodies:\n                rigid_id += 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving all rigid body labels from the Compound.", "response": "def unlabel_rigid_bodies(self):\n        \"\"\"Remove all rigid body labels from the Compound \"\"\"\n        self._check_if_contains_rigid_bodies = True\n        for child in self.children:\n            child._check_if_contains_rigid_bodies = True\n        for particle in self.particles():\n            particle.rigid_id = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nincrementing the rigid_id of all rigid Particles in a Compound that already have an integer rigid_id.", "response": "def _increment_rigid_ids(self, increment):\n        \"\"\"Increment the rigid_id of all rigid Particles in a Compound\n\n        Adds `increment` to the rigid_id of all Particles in `self` that\n        already have an integer rigid_id.\n        \"\"\"\n        for particle in self.particles():\n            if particle.rigid_id is not None:\n                particle.rigid_id += increment"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _reorder_rigid_ids(self):\n        max_rigid = self.max_rigid_id\n        unique_rigid_ids = sorted(\n            set([p.rigid_id for p in self.rigid_particles()]))\n        n_unique_rigid = len(unique_rigid_ids)\n        if max_rigid and n_unique_rigid != max_rigid + 1:\n            missing_rigid_id = (\n                unique_rigid_ids[-1] * (unique_rigid_ids[-1] + 1)) / 2 - sum(unique_rigid_ids)\n            for successor in self.successors():\n                if successor.rigid_id is not None:\n                    if successor.rigid_id > missing_rigid_id:\n                        successor.rigid_id -= 1\n            if self.rigid_id:\n                if self.rigid_id > missing_rigid_id:\n                    self.rigid_id -= 1", "response": "Reorder rigid body IDs ensuring consecutiveness."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(self, new_child, label=None, containment=True, replace=False,\n            inherit_periodicity=True, reset_rigid_ids=True):\n        \"\"\"Add a part to the Compound.\n\n        Note:\n            This does not necessarily add the part to self.children but may\n            instead be used to add a reference to the part to self.labels. See\n            'containment' argument.\n\n        Parameters\n        ----------\n        new_child : mb.Compound or list-like of mb.Compound\n            The object(s) to be added to this Compound.\n        label : str, optional\n            A descriptive string for the part.\n        containment : bool, optional, default=True\n            Add the part to self.children.\n        replace : bool, optional, default=True\n            Replace the label if it already exists.\n        inherit_periodicity : bool, optional, default=True\n            Replace the periodicity of self with the periodicity of the\n            Compound being added\n        reset_rigid_ids : bool, optional, default=True\n            If the Compound to be added contains rigid bodies, reset the\n            rigid_ids such that values remain distinct from rigid_ids\n            already present in `self`. Can be set to False if attempting\n            to add Compounds to an existing rigid body.\n\n        \"\"\"\n        # Support batch add via lists, tuples and sets.\n        if (isinstance(new_child, collections.Iterable) and\n                not isinstance(new_child, string_types)):\n            for child in new_child:\n                self.add(child, reset_rigid_ids=reset_rigid_ids)\n            return\n\n        if not isinstance(new_child, Compound):\n            raise ValueError('Only objects that inherit from mbuild.Compound '\n                             'can be added to Compounds. You tried to add '\n                             '\"{}\".'.format(new_child))\n\n        if new_child.contains_rigid or new_child.rigid_id is not None:\n            if self.contains_rigid and reset_rigid_ids:\n                new_child._increment_rigid_ids(increment=self.max_rigid_id + 1)\n            self._check_if_contains_rigid_bodies = True\n        if self.rigid_id is not None:\n            self.rigid_id = None\n\n        # Create children and labels on the first add operation\n        if self.children is None:\n            self.children = OrderedSet()\n        if self.labels is None:\n            self.labels = OrderedDict()\n\n        if containment:\n            if new_child.parent is not None:\n                raise MBuildError('Part {} already has a parent: {}'.format(\n                    new_child, new_child.parent))\n            self.children.add(new_child)\n            new_child.parent = self\n\n            if new_child.bond_graph is not None:\n                if self.root.bond_graph is None:\n                    self.root.bond_graph = new_child.bond_graph\n                else:\n                    self.root.bond_graph.compose(new_child.bond_graph)\n\n                new_child.bond_graph = None\n\n        # Add new_part to labels. Does not currently support batch add.\n        if label is None:\n            label = '{0}[$]'.format(new_child.__class__.__name__)\n\n        if label.endswith('[$]'):\n            label = label[:-3]\n            if label not in self.labels:\n                self.labels[label] = []\n            label_pattern = label + '[{}]'\n\n            count = len(self.labels[label])\n            self.labels[label].append(new_child)\n            label = label_pattern.format(count)\n\n        if not replace and label in self.labels:\n            raise MBuildError('Label \"{0}\" already exists in {1}.'.format(\n                label, self))\n        else:\n            self.labels[label] = new_child\n        new_child.referrers.add(self)\n\n        if (inherit_periodicity and isinstance(new_child, Compound) and\n                new_child.periodicity.any()):\n            self.periodicity = new_child.periodicity", "response": "Adds a new child to the Compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the children of the given objects from the Compound.", "response": "def remove(self, objs_to_remove):\n        \"\"\"Remove children from the Compound.\n\n        Parameters\n        ----------\n        objs_to_remove : mb.Compound or list of mb.Compound\n            The Compound(s) to be removed from self\n\n        \"\"\"\n        if not self.children:\n            return\n\n        if not hasattr(objs_to_remove, '__iter__'):\n            objs_to_remove = [objs_to_remove]\n        objs_to_remove = set(objs_to_remove)\n\n        if len(objs_to_remove) == 0:\n            return\n\n        remove_from_here = objs_to_remove.intersection(self.children)\n        self.children -= remove_from_here\n        yet_to_remove = objs_to_remove - remove_from_here\n\n        for removed in remove_from_here:\n            for child in removed.children:\n                removed.remove(child)\n\n        for removed_part in remove_from_here:\n            if removed_part.rigid_id is not None:\n                for ancestor in removed_part.ancestors():\n                    ancestor._check_if_contains_rigid_bodies = True\n            if self.root.bond_graph and self.root.bond_graph.has_node(\n                    removed_part):\n                for neighbor in self.root.bond_graph.neighbors(removed_part):\n                    self.root.remove_bond((removed_part, neighbor))\n                self.root.bond_graph.remove_node(removed_part)\n            self._remove_references(removed_part)\n\n        # Remove the part recursively from sub-compounds.\n        for child in self.children:\n            child.remove(yet_to_remove)\n            if child.contains_rigid:\n                self.root._reorder_rigid_ids()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving references pointing to this part and vice versa.", "response": "def _remove_references(self, removed_part):\n        \"\"\"Remove labels pointing to this part and vice versa. \"\"\"\n        removed_part.parent = None\n\n        # Remove labels in the hierarchy pointing to this part.\n        referrers_to_remove = set()\n        for referrer in removed_part.referrers:\n            if removed_part not in referrer.ancestors():\n                for label, referred_part in list(referrer.labels.items()):\n                    if referred_part is removed_part:\n                        del referrer.labels[label]\n                        referrers_to_remove.add(referrer)\n        removed_part.referrers -= referrers_to_remove\n\n        # Remove labels in this part pointing into the hierarchy.\n        labels_to_delete = []\n        if isinstance(removed_part, Compound):\n            for label, part in list(removed_part.labels.items()):\n                if not isinstance(part, Compound):\n                    for p in part:\n                        self._remove_references(p)\n                elif removed_part not in part.ancestors():\n                    try:\n                        part.referrers.discard(removed_part)\n                    except KeyError:\n                        pass\n                    else:\n                        labels_to_delete.append(label)\n        for label in labels_to_delete:\n            removed_part.labels.pop(label, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all Ports referenced by this Compound.", "response": "def referenced_ports(self):\n        \"\"\"Return all Ports referenced by this Compound.\n\n        Returns\n        -------\n        list of mb.Compound\n            A list of all ports referenced by the Compound\n\n        \"\"\"\n        from mbuild.port import Port\n        return [port for port in self.labels.values()\n                if isinstance(port, Port)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all Ports referenced by this Compound and its successors Returns ------- list of mb. Compound A list of all Ports referenced by this Compound and its successors", "response": "def all_ports(self):\n        \"\"\"Return all Ports referenced by this Compound and its successors\n\n        Returns\n        -------\n        list of mb.Compound\n            A list of all Ports referenced by this Compound and its successors\n\n        \"\"\"\n        from mbuild.port import Port\n        return [successor for successor in self.successors()\n                if isinstance(successor, Port)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef available_ports(self):\n        from mbuild.port import Port\n        return [port for port in self.labels.values()\n                if isinstance(port, Port) and not port.used]", "response": "Return all unoccupied Ports referenced by this Compound."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all bonds in the Compound and sub - Compounds.", "response": "def bonds(self):\n        \"\"\"Return all bonds in the Compound and sub-Compounds.\n\n        Yields\n        -------\n        tuple of mb.Compound\n            The next bond in the Compound\n\n        See Also\n        --------\n        bond_graph.edges_iter : Iterates over all edges in a BondGraph\n\n        \"\"\"\n        if self.root.bond_graph:\n            if self.root == self:\n                return self.root.bond_graph.edges_iter()\n            else:\n                return self.root.bond_graph.subgraph(\n                    self.particles()).edges_iter()\n        else:\n            return iter(())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a bond between two Particles.", "response": "def add_bond(self, particle_pair):\n        \"\"\"Add a bond between two Particles.\n\n        Parameters\n        ----------\n        particle_pair : indexable object, length=2, dtype=mb.Compound\n            The pair of Particles to add a bond between\n\n        \"\"\"\n        if self.root.bond_graph is None:\n            self.root.bond_graph = BondGraph()\n\n        self.root.bond_graph.add_edge(particle_pair[0], particle_pair[1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating all pairs of types a and b within [ dmin dmax )", "response": "def generate_bonds(self, name_a, name_b, dmin, dmax):\n        \"\"\"Add Bonds between all pairs of types a/b within [dmin, dmax].\n\n        Parameters\n        ----------\n        name_a : str\n            The name of one of the Particles to be in each bond\n        name_b : str\n            The name of the other Particle to be in each bond\n        dmin : float\n            The minimum distance between Particles for considering a bond\n        dmax : float\n            The maximum distance between Particles for considering a bond\n\n        \"\"\"\n        particle_kdtree = PeriodicCKDTree(\n            data=self.xyz, bounds=self.periodicity)\n        particle_array = np.array(list(self.particles()))\n        added_bonds = list()\n        for p1 in self.particles_by_name(name_a):\n            nearest = self.particles_in_range(p1, dmax, max_particles=20,\n                                              particle_kdtree=particle_kdtree,\n                                              particle_array=particle_array)\n            for p2 in nearest:\n                if p2 == p1:\n                    continue\n                bond_tuple = (p1, p2) if id(p1) < id(p2) else (p2, p1)\n                if bond_tuple in added_bonds:\n                    continue\n                min_dist = self.min_periodic_distance(p2.pos, p1.pos)\n                if (p2.name == name_b) and (dmin <= min_dist <= dmax):\n                    self.add_bond((p1, p2))\n                    added_bonds.append(bond_tuple)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_bond(self, particle_pair):\n        from mbuild.port import Port\n        if self.root.bond_graph is None or not self.root.bond_graph.has_edge(\n                *particle_pair):\n            warn(\"Bond between {} and {} doesn't exist!\".format(*particle_pair))\n            return\n        self.root.bond_graph.remove_edge(*particle_pair)\n        bond_vector = particle_pair[0].pos - particle_pair[1].pos\n        if np.allclose(bond_vector, np.zeros(3)):\n            warn(\"Particles {} and {} overlap! Ports will not be added.\"\n                 \"\".format(*particle_pair))\n            return\n        distance = np.linalg.norm(bond_vector)\n        particle_pair[0].parent.add(Port(anchor=particle_pair[0],\n                                         orientation=-bond_vector,\n                                         separation=distance / 2), 'port[$]')\n        particle_pair[1].parent.add(Port(anchor=particle_pair[1],\n                                         orientation=bond_vector,\n                                         separation=distance / 2), 'port[$]')", "response": "Deletes a bond between a pair of Particles\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all particle coordinates in this compound.", "response": "def xyz(self):\n        \"\"\"Return all particle coordinates in this compound.\n\n        Returns\n        -------\n        pos : np.ndarray, shape=(n, 3), dtype=float\n            Array with the positions of all particles.\n        \"\"\"\n        if not self.children:\n            pos = np.expand_dims(self._pos, axis=0)\n        else:\n            arr = np.fromiter(itertools.chain.from_iterable(\n                particle.pos for particle in self.particles()), dtype=float)\n            pos = arr.reshape((-1, 3))\n        return pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all particle coordinates in this compound including ports.", "response": "def xyz_with_ports(self):\n        \"\"\"Return all particle coordinates in this compound including ports.\n\n        Returns\n        -------\n        pos : np.ndarray, shape=(n, 3), dtype=float\n            Array with the positions of all particles and ports.\n\n        \"\"\"\n        if not self.children:\n            pos = self._pos\n        else:\n            arr = np.fromiter(\n                itertools.chain.from_iterable(\n                    particle.pos for particle in self.particles(\n                        include_ports=True)), dtype=float)\n            pos = arr.reshape((-1, 3))\n        return pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xyz(self, arrnx3):\n        if not self.children:\n            if not arrnx3.shape[0] == 1:\n                raise ValueError(\n                    'Trying to set position of {} with more than one'\n                    'coordinate: {}'.format(\n                        self, arrnx3))\n            self.pos = np.squeeze(arrnx3)\n        else:\n            for atom, coords in zip(\n                self._particles(\n                    include_ports=False), arrnx3):\n                atom.pos = coords", "response": "Set the positions of the particles in the Compound excluding the Ports."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xyz_with_ports(self, arrnx3):\n        if not self.children:\n            if not arrnx3.shape[0] == 1:\n                raise ValueError(\n                    'Trying to set position of {} with more than one'\n                    'coordinate: {}'.format(\n                        self, arrnx3))\n            self.pos = np.squeeze(arrnx3)\n        else:\n            for atom, coords in zip(\n                self._particles(\n                    include_ports=True), arrnx3):\n                atom.pos = coords", "response": "Set the positions of the particles in the Compound including the Ports."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef center(self):\n\n        if np.all(np.isfinite(self.xyz)):\n            return np.mean(self.xyz, axis=0)", "response": "Returns the cartesian center of the Compound based on its Particles."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef boundingbox(self):\n        xyz = self.xyz\n        return Box(mins=xyz.min(axis=0), maxs=xyz.max(axis=0))", "response": "Compute the bounding box of the compound.\n            Returns ------- mb. Box\n            The bounding box of the compound.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind particles within a specified range of another particle. Parameters ---------- compound : mb.Compound Reference particle to find other particles in range of dmax : float Maximum distance from 'compound' to look for Particles max_particles : int, optional, default=20 Maximum number of Particles to return particle_kdtree : mb.PeriodicCKDTree, optional KD-tree for looking up nearest neighbors. If not provided, a KD- tree will be generated from all Particles in self particle_array : np.ndarray, shape=(n,), dtype=mb.Compound, optional Array of possible particles to consider for return. If not provided, this defaults to all Particles in self Returns ------- np.ndarray, shape=(n,), dtype=mb.Compound Particles in range of compound according to user-defined limits See Also -------- periodic_kdtree.PerioidicCKDTree : mBuild implementation of kd-trees scipy.spatial.ckdtree : Further details on kd-trees", "response": "def particles_in_range(\n            self,\n            compound,\n            dmax,\n            max_particles=20,\n            particle_kdtree=None,\n            particle_array=None):\n        \"\"\"Find particles within a specified range of another particle.\n\n        Parameters\n        ----------\n        compound : mb.Compound\n            Reference particle to find other particles in range of\n        dmax : float\n            Maximum distance from 'compound' to look for Particles\n        max_particles : int, optional, default=20\n            Maximum number of Particles to return\n        particle_kdtree : mb.PeriodicCKDTree, optional\n            KD-tree for looking up nearest neighbors. If not provided, a KD-\n            tree will be generated from all Particles in self\n        particle_array : np.ndarray, shape=(n,), dtype=mb.Compound, optional\n            Array of possible particles to consider for return. If not\n            provided, this defaults to all Particles in self\n\n        Returns\n        -------\n        np.ndarray, shape=(n,), dtype=mb.Compound\n            Particles in range of compound according to user-defined limits\n\n        See Also\n        --------\n        periodic_kdtree.PerioidicCKDTree : mBuild implementation of kd-trees\n        scipy.spatial.ckdtree : Further details on kd-trees\n\n        \"\"\"\n        if particle_kdtree is None:\n            particle_kdtree = PeriodicCKDTree(\n                data=self.xyz, bounds=self.periodicity)\n        _, idxs = particle_kdtree.query(\n            compound.pos, k=max_particles, distance_upper_bound=dmax)\n        idxs = idxs[idxs != self.n_particles]\n        if particle_array is None:\n            particle_array = np.array(list(self.particles()))\n        return particle_array[idxs]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visualize(self, show_ports=False):\n        nglview = import_('nglview')\n        from mdtraj.geometry.sasa import _ATOMIC_RADII\n        if run_from_ipython():\n            remove_digits = lambda x: ''.join(i for i in x if not i.isdigit()\n                                              or i == '_')\n            for particle in self.particles():\n                particle.name = remove_digits(particle.name).upper()\n                if not particle.name:\n                    particle.name = 'UNK'\n            tmp_dir = tempfile.mkdtemp()\n            self.save(os.path.join(tmp_dir, 'tmp.mol2'),\n                      show_ports=show_ports,\n                      overwrite=True)\n            widget = nglview.show_file(os.path.join(tmp_dir, 'tmp.mol2'))\n            widget.clear()\n            widget.add_ball_and_stick(cylinderOnly=True)\n            elements = set([particle.name for particle in self.particles()])\n            scale = 50.0\n            for element in elements:\n                try:\n                    widget.add_ball_and_stick('_{}'.format(\n                        element.upper()), aspect_ratio=_ATOMIC_RADII[element.title()]**1.5 * scale)\n                except KeyError:\n                    ids = [str(i) for i, particle in enumerate(self.particles())\n                           if particle.name == element]\n                    widget.add_ball_and_stick(\n                        '@{}'.format(\n                            ','.join(ids)),\n                        aspect_ratio=0.17**1.5 * scale,\n                        color='grey')\n            if show_ports:\n                widget.add_ball_and_stick('_VS',\n                                          aspect_ratio=1.0, color='#991f00')\n            return widget\n        else:\n            raise RuntimeError('Visualization is only supported in Jupyter '\n                               'Notebooks.')", "response": "Visualize the Compound using nglview."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_coordinates(self, filename, update_port_locations=True):\n        if update_port_locations:\n            xyz_init = self.xyz\n            self = load(filename, compound=self, coords_only=True)\n            self._update_port_locations(xyz_init)\n        else:\n            self = load(filename, compound=self, coords_only=True)", "response": "Update the coordinates of this Compound from a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadjust the port locations after particles have moved and moves the anchor Particles between self and an array of base coordinates.", "response": "def _update_port_locations(self, initial_coordinates):\n        \"\"\"Adjust port locations after particles have moved\n\n        Compares the locations of Particles between 'self' and an array of\n        reference coordinates.  Shifts Ports in accordance with how far anchors\n        have been moved.  This conserves the location of Ports with respect to\n        their anchor Particles, but does not conserve the orientation of Ports\n        with respect to the molecule as a whole.\n\n        Parameters\n        ----------\n        initial_coordinates : np.ndarray, shape=(n, 3), dtype=float\n            Reference coordinates to use for comparing how far anchor Particles\n            have shifted.\n\n        \"\"\"\n        particles = list(self.particles())\n        for port in self.all_ports():\n            if port.anchor:\n                idx = particles.index(port.anchor)\n                shift = particles[idx].pos - initial_coordinates[idx]\n                port.translate(shift)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform an energy minimization on a Compound Default beahvior utilizes Open Babel (http://openbabel.org/docs/dev/) to perform an energy minimization/geometry optimization on a Compound by applying a generic force field Can also utilize OpenMM (http://openmm.org/) to energy minimize after atomtyping a Compound using Foyer (https://github.com/mosdef-hub/foyer) to apply a forcefield XML file that contains valid SMARTS strings. This function is primarily intended to be used on smaller components, with sizes on the order of 10's to 100's of particles, as the energy minimization scales poorly with the number of particles. Parameters ---------- steps : int, optional, default=1000 The number of optimization iterations forcefield : str, optional, default='UFF' The generic force field to apply to the Compound for minimization. Valid options are 'MMFF94', 'MMFF94s', ''UFF', 'GAFF', and 'Ghemical'. Please refer to the Open Babel documentation (http://open-babel. readthedocs.io/en/latest/Forcefields/Overview.html) when considering your choice of force field. Utilizing OpenMM for energy minimization requires a forcefield XML file with valid SMARTS strings. Please refer to (http://docs. openmm.org/7.0.0/userguide/application.html#creating-force-fields) for more information. Keyword Arguments ------------ algorithm : str, optional, default='cg' The energy minimization algorithm. Valid options are 'steep', 'cg', and 'md', corresponding to steepest descent, conjugate gradient, and equilibrium molecular dynamics respectively. For _energy_minimize_openbabel scale_bonds : float, optional, default=1 Scales the bond force constant (1 is completely on). For _energy_minimize_openmm scale_angles : float, optional, default=1 Scales the angle force constant (1 is completely on) For _energy_minimize_openmm scale_torsions : float, optional, default=1 Scales the torsional force constants (1 is completely on) For _energy_minimize_openmm Note: Only Ryckaert-Bellemans style torsions are currently supported scale_nonbonded : float, optional, default=1 Scales epsilon (1 is completely on) For _energy_minimize_openmm References ---------- If using _energy_minimize_openmm(), please cite: .. [1] P. Eastman, M. S. Friedrichs, J. D. Chodera, R. J. Radmer, C. M. Bruns, J. P. Ku, K. A. Beauchamp, T. J. Lane, L.-P. Wang, D. Shukla, T. Tye, M. Houston, T. Stich, C. Klein, M. R. Shirts, and V. S. Pande. \"OpenMM 4: A Reusable, Extensible, Hardware Independent Library for High Performance Molecular Simulation.\" J. Chem. Theor. Comput. 9(1): 461-469. (2013). If using _energy_minimize_openbabel(), please cite: .. [1] O'Boyle, N.M.; Banck, M.; James, C.A.; Morley, C.; Vandermeersch, T.; Hutchison, G.R. \"Open Babel: An open chemical toolbox.\" (2011) J. Cheminf. 3, 33 .. [2] Open Babel, version X.X.X http://openbabel.org, (installed Month Year) If using the 'MMFF94' force field please also cite the following: .. [3] T.A. Halgren, \"Merck molecular force field. I. Basis, form, scope, parameterization, and performance of MMFF94.\" (1996) J. Comput. Chem. 17, 490-519 .. [4] T.A. Halgren, \"Merck molecular force field. II. MMFF94 van der Waals and electrostatic parameters for intermolecular interactions.\" (1996) J. Comput. Chem. 17, 520-552 .. [5] T.A. Halgren, \"Merck molecular force field. III. Molecular geometries and vibrational frequencies for MMFF94.\" (1996) J. Comput. Chem. 17, 553-586 .. [6] T.A. Halgren and R.B. Nachbar, \"Merck molecular force field. IV. Conformational energies and geometries for MMFF94.\" (1996) J. Comput. Chem. 17, 587-615 .. [7] T.A. Halgren, \"Merck molecular force field. V. Extension of MMFF94 using experimental data, additional computational data, and empirical rules.\" (1996) J. Comput. Chem. 17, 616-641 If using the 'MMFF94s' force field please cite the above along with: .. [8] T.A. Halgren, \"MMFF VI. MMFF94s option for energy minimization studies.\" (1999) J. Comput. Chem. 20, 720-729 If using the 'UFF' force field please cite the following: .. [3] Rappe, A.K., Casewit, C.J., Colwell, K.S., Goddard, W.A. III, Skiff, W.M. \"UFF, a full periodic table force field for molecular mechanics and molecular dynamics simulations.\" (1992) J. Am. Chem. Soc. 114, 10024-10039 If using the 'GAFF' force field please cite the following: .. [3] Wang, J., Wolf, R.M., Caldwell, J.W., Kollman, P.A., Case, D.A. \"Development and testing of a general AMBER force field\" (2004) J. Comput. Chem. 25, 1157-1174 If using the 'Ghemical' force field please cite the following: .. [3] T. Hassinen and M. Perakyla, \"New energy terms for reduced protein models implemented in an off-lattice force field\" (2001) J. Comput. Chem. 22, 1229-1242", "response": "def energy_minimize(self, forcefield='UFF', steps=1000, **kwargs):\n        \"\"\"Perform an energy minimization on a Compound\n\n        Default beahvior utilizes Open Babel (http://openbabel.org/docs/dev/)\n        to perform an energy minimization/geometry optimization on a\n        Compound by applying a generic force field\n\n        Can also utilize OpenMM (http://openmm.org/) to energy minimize\n        after atomtyping a Compound using\n        Foyer (https://github.com/mosdef-hub/foyer) to apply a forcefield\n        XML file that contains valid SMARTS strings.\n\n        This function is primarily intended to be used on smaller components,\n        with sizes on the order of 10's to 100's of particles, as the energy\n        minimization scales poorly with the number of particles.\n\n        Parameters\n        ----------\n        steps : int, optional, default=1000\n            The number of optimization iterations\n        forcefield : str, optional, default='UFF'\n            The generic force field to apply to the Compound for minimization.\n            Valid options are 'MMFF94', 'MMFF94s', ''UFF', 'GAFF', and 'Ghemical'.\n            Please refer to the Open Babel documentation (http://open-babel.\n            readthedocs.io/en/latest/Forcefields/Overview.html) when considering\n            your choice of force field.\n            Utilizing OpenMM for energy minimization requires a forcefield\n            XML file with valid SMARTS strings. Please refer to (http://docs.\n            openmm.org/7.0.0/userguide/application.html#creating-force-fields)\n            for more information.\n\n\n        Keyword Arguments\n        ------------\n        algorithm : str, optional, default='cg'\n            The energy minimization algorithm.  Valid options are 'steep',\n            'cg', and 'md', corresponding to steepest descent, conjugate\n            gradient, and equilibrium molecular dynamics respectively.\n            For _energy_minimize_openbabel\n        scale_bonds : float, optional, default=1\n            Scales the bond force constant (1 is completely on).\n            For _energy_minimize_openmm\n        scale_angles : float, optional, default=1\n            Scales the angle force constant (1 is completely on)\n            For _energy_minimize_openmm\n        scale_torsions : float, optional, default=1\n            Scales the torsional force constants (1 is completely on)\n            For _energy_minimize_openmm\n            Note: Only Ryckaert-Bellemans style torsions are currently supported \n        scale_nonbonded : float, optional, default=1\n            Scales epsilon (1 is completely on)\n            For _energy_minimize_openmm\n\n        References\n        ----------\n        If using _energy_minimize_openmm(), please cite:\n        .. [1] P. Eastman, M. S. Friedrichs, J. D. Chodera, R. J. Radmer,\n               C. M. Bruns, J. P. Ku, K. A. Beauchamp, T. J. Lane,\n               L.-P. Wang, D. Shukla, T. Tye, M. Houston, T. Stich,\n               C. Klein, M. R. Shirts, and V. S. Pande.\n               \"OpenMM 4: A Reusable, Extensible, Hardware Independent\n               Library for High Performance Molecular Simulation.\"\n               J. Chem. Theor. Comput. 9(1): 461-469. (2013).\n\n\n        If using _energy_minimize_openbabel(), please cite:\n        .. [1] O'Boyle, N.M.; Banck, M.; James, C.A.; Morley, C.;\n               Vandermeersch, T.; Hutchison, G.R. \"Open Babel: An open\n               chemical toolbox.\" (2011) J. Cheminf. 3, 33\n\n        .. [2] Open Babel, version X.X.X http://openbabel.org, (installed\n               Month Year)\n\n        If using the 'MMFF94' force field please also cite the following:\n        .. [3] T.A. Halgren, \"Merck molecular force field. I. Basis, form,\n               scope, parameterization, and performance of MMFF94.\" (1996)\n               J. Comput. Chem. 17, 490-519\n        .. [4] T.A. Halgren, \"Merck molecular force field. II. MMFF94 van der\n               Waals and electrostatic parameters for intermolecular\n               interactions.\" (1996) J. Comput. Chem. 17, 520-552\n        .. [5] T.A. Halgren, \"Merck molecular force field. III. Molecular\n               geometries and vibrational frequencies for MMFF94.\" (1996)\n               J. Comput. Chem. 17, 553-586\n        .. [6] T.A. Halgren and R.B. Nachbar, \"Merck molecular force field.\n               IV. Conformational energies and geometries for MMFF94.\" (1996)\n               J. Comput. Chem. 17, 587-615\n        .. [7] T.A. Halgren, \"Merck molecular force field. V. Extension of\n               MMFF94 using experimental data, additional computational data,\n               and empirical rules.\" (1996) J. Comput. Chem. 17, 616-641\n\n        If using the 'MMFF94s' force field please cite the above along with:\n        .. [8] T.A. Halgren, \"MMFF VI. MMFF94s option for energy minimization\n               studies.\" (1999) J. Comput. Chem. 20, 720-729\n\n        If using the 'UFF' force field please cite the following:\n        .. [3] Rappe, A.K., Casewit, C.J., Colwell, K.S., Goddard, W.A. III,\n               Skiff, W.M. \"UFF, a full periodic table force field for\n               molecular mechanics and molecular dynamics simulations.\" (1992)\n               J. Am. Chem. Soc. 114, 10024-10039\n\n        If using the 'GAFF' force field please cite the following:\n        .. [3] Wang, J., Wolf, R.M., Caldwell, J.W., Kollman, P.A., Case, D.A.\n               \"Development and testing of a general AMBER force field\" (2004)\n               J. Comput. Chem. 25, 1157-1174\n\n        If using the 'Ghemical' force field please cite the following:\n        .. [3] T. Hassinen and M. Perakyla, \"New energy terms for reduced\n               protein models implemented in an off-lattice force field\" (2001)\n               J. Comput. Chem. 22, 1229-1242\n\n\n\n        \"\"\"\n        tmp_dir = tempfile.mkdtemp()\n        original = clone(self)\n        self._kick()\n        self.save(os.path.join(tmp_dir, 'un-minimized.mol2'))\n        extension = os.path.splitext(forcefield)[-1]\n        openbabel_ffs = ['MMFF94', 'MMFF94s', 'UFF', 'GAFF', 'Ghemical']\n        if forcefield in openbabel_ffs:\n            self._energy_minimize_openbabel(tmp_dir, forcefield=forcefield,\n                                            steps=steps, **kwargs)\n        elif extension == '.xml':\n            self._energy_minimize_openmm(tmp_dir, forcefield_files=forcefield,\n                                         forcefield_name=None,\n                                         steps=steps, **kwargs)\n        else:\n            self._energy_minimize_openmm(tmp_dir, forcefield_files=None,\n                                         forcefield_name=forcefield,\n                                         steps=steps, **kwargs)\n\n        self.update_coordinates(os.path.join(tmp_dir, 'minimized.pdb'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming energy minimization using OpenMM.", "response": "def _energy_minimize_openmm(\n            self,\n            tmp_dir,\n            forcefield_files=None,\n            forcefield_name=None,\n            steps=1000,\n            scale_bonds=1,\n            scale_angles=1,\n            scale_torsions=1,\n            scale_nonbonded=1):\n        \"\"\" Perform energy minimization using OpenMM\n\n        Converts an mBuild Compound to a Parmed Structure,\n        applies a forcefield using Foyer, and creates an OpenMM System.\n\n        Parameters\n        ----------\n        forcefield_files : str or list of str, optional, default=None\n            Forcefield files to load \n        forcefield_name : str, optional, default=None\n            Apply a named forcefield to the output file using the `foyer`\n            package, e.g. 'oplsaa'. Forcefields listed here:\n            https://github.com/mosdef-hub/foyer/tree/master/foyer/forcefields\n        steps : int, optional, default=1000\n            Number of energy minimization iterations\n        scale_bonds : float, optional, default=1\n            Scales the bond force constant (1 is completely on)\n        scale_angles : float, optiona, default=1\n            Scales the angle force constant (1 is completely on)\n        scale_torsions : float, optional, default=1\n            Scales the torsional force constants (1 is completely on)\n        scale_nonbonded : float, optional, default=1\n            Scales epsilon (1 is completely on)\n\n\n        Notes\n        -----\n        Assumes a particular organization for the force groups\n        (HarmonicBondForce, HarmonicAngleForce, RBTorsionForce, NonBondedForce)\n\n        References\n        ----------\n\n        .. [1] P. Eastman, M. S. Friedrichs, J. D. Chodera, R. J. Radmer,\n               C. M. Bruns, J. P. Ku, K. A. Beauchamp, T. J. Lane,\n               L.-P. Wang, D. Shukla, T. Tye, M. Houston, T. Stich,\n               C. Klein, M. R. Shirts, and V. S. Pande.\n               \"OpenMM 4: A Reusable, Extensible, Hardware Independent\n               Library for High Performance Molecular Simulation.\"\n               J. Chem. Theor. Comput. 9(1): 461-469. (2013).\n\n\n\n        \"\"\"\n        foyer = import_('foyer')\n\n        to_parmed = self.to_parmed()\n        ff = foyer.Forcefield(forcefield_files=forcefield_files, name=forcefield_name)\n        to_parmed = ff.apply(to_parmed)\n\n        from simtk.openmm.app.simulation import Simulation\n        from simtk.openmm.app.pdbreporter import PDBReporter\n        from simtk.openmm.openmm import LangevinIntegrator\n        import simtk.unit as u\n\n        system = to_parmed.createSystem()\n        integrator = LangevinIntegrator(298 * u.kelvin, 1 / u.picosecond,\n                                        0.002 * u.picoseconds)\n        simulation = Simulation(to_parmed.topology, system, integrator)\n\n        for force in system.getForces():\n            if type(force).__name__ == \"HarmonicBondForce\":\n                for bond_index in range(force.getNumBonds()):\n                    atom1, atom2, r0, k = force.getBondParameters(bond_index)\n                    force.setBondParameters(bond_index,\n                                            atom1, atom2,\n                                            r0, k * scale_bonds)\n                force.updateParametersInContext(simulation.context)\n\n            elif type(force).__name__ == \"HarmonicAngleForce\":\n                for angle_index in range(force.getNumAngles()):\n                    atom1, atom2, atom3, r0, k = force.getAngleParameters(\n                        angle_index)\n                    force.setAngleParameters(angle_index,\n                                             atom1, atom2, atom3,\n                                             r0, k * scale_angles)\n                force.updateParametersInContext(simulation.context)\n\n            elif type(force).__name__ == \"RBTorsionForce\":\n                for torsion_index in range(force.getNumTorsions()):\n                    atom1, atom2, atom3, atom4, c0, c1, c2, c3, c4, c5 = force.getTorsionParameters(\n                        torsion_index)\n                    force.setTorsionParameters(\n                        torsion_index,\n                        atom1,\n                        atom2,\n                        atom3,\n                        atom4,\n                        c0 * scale_torsions,\n                        c1 * scale_torsions,\n                        c2 * scale_torsions,\n                        c3 * scale_torsions,\n                        c4 * scale_torsions,\n                        c5 * scale_torsions)\n                force.updateParametersInContext(simulation.context)\n\n            elif type(force).__name__ == \"NonbondedForce\":\n                for nb_index in range(force.getNumParticles()):\n                    charge, sigma, epsilon = force.getParticleParameters(\n                        nb_index)\n                    force.setParticleParameters(nb_index,\n                                                charge, sigma,\n                                                epsilon * scale_nonbonded)\n                force.updateParametersInContext(simulation.context)\n\n            elif type(force).__name__ == \"CMMotionRemover\":\n                pass\n\n            else:\n                warn(\n                    'OpenMM Force {} is '\n                    'not currently supported in _energy_minimize_openmm. '\n                    'This Force will not be updated!'.format(\n                        type(force).__name__))\n\n        simulation.context.setPositions(to_parmed.positions)\n        simulation.minimizeEnergy(maxIterations=steps)\n        reporter = PDBReporter(os.path.join(tmp_dir, 'minimized.pdb'), 1)\n        reporter.report(\n            simulation,\n            simulation.context.getState(\n                getPositions=True))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _energy_minimize_openbabel(self, tmp_dir, steps=1000, algorithm='cg',\n                                   forcefield='UFF'):\n        \"\"\"Perform an energy minimization on a Compound\n\n        Utilizes Open Babel (http://openbabel.org/docs/dev/) to perform an\n        energy minimization/geometry optimization on a Compound by applying\n        a generic force field.\n\n        This function is primarily intended to be used on smaller components,\n        with sizes on the order of 10's to 100's of particles, as the energy\n        minimization scales poorly with the number of particles.\n\n        Parameters\n        ----------\n        steps : int, optionl, default=1000\n            The number of optimization iterations\n        algorithm : str, optional, default='cg'\n            The energy minimization algorithm.  Valid options are 'steep',\n            'cg', and 'md', corresponding to steepest descent, conjugate\n            gradient, and equilibrium molecular dynamics respectively.\n        forcefield : str, optional, default='UFF'\n            The generic force field to apply to the Compound for minimization.\n            Valid options are 'MMFF94', 'MMFF94s', ''UFF', 'GAFF', and 'Ghemical'.\n            Please refer to the Open Babel documentation (http://open-babel.\n            readthedocs.io/en/latest/Forcefields/Overview.html) when considering\n            your choice of force field.\n\n        References\n        ----------\n        .. [1] O'Boyle, N.M.; Banck, M.; James, C.A.; Morley, C.;\n               Vandermeersch, T.; Hutchison, G.R. \"Open Babel: An open\n               chemical toolbox.\" (2011) J. Cheminf. 3, 33\n        .. [2] Open Babel, version X.X.X http://openbabel.org, (installed\n               Month Year)\n\n        If using the 'MMFF94' force field please also cite the following:\n        .. [3] T.A. Halgren, \"Merck molecular force field. I. Basis, form,\n               scope, parameterization, and performance of MMFF94.\" (1996)\n               J. Comput. Chem. 17, 490-519\n        .. [4] T.A. Halgren, \"Merck molecular force field. II. MMFF94 van der\n               Waals and electrostatic parameters for intermolecular\n               interactions.\" (1996) J. Comput. Chem. 17, 520-552\n        .. [5] T.A. Halgren, \"Merck molecular force field. III. Molecular\n               geometries and vibrational frequencies for MMFF94.\" (1996)\n               J. Comput. Chem. 17, 553-586\n        .. [6] T.A. Halgren and R.B. Nachbar, \"Merck molecular force field.\n               IV. Conformational energies and geometries for MMFF94.\" (1996)\n               J. Comput. Chem. 17, 587-615\n        .. [7] T.A. Halgren, \"Merck molecular force field. V. Extension of\n               MMFF94 using experimental data, additional computational data,\n               and empirical rules.\" (1996) J. Comput. Chem. 17, 616-641\n\n        If using the 'MMFF94s' force field please cite the above along with:\n        .. [8] T.A. Halgren, \"MMFF VI. MMFF94s option for energy minimization\n               studies.\" (1999) J. Comput. Chem. 20, 720-729\n\n        If using the 'UFF' force field please cite the following:\n        .. [3] Rappe, A.K., Casewit, C.J., Colwell, K.S., Goddard, W.A. III,\n               Skiff, W.M. \"UFF, a full periodic table force field for\n               molecular mechanics and molecular dynamics simulations.\" (1992)\n               J. Am. Chem. Soc. 114, 10024-10039\n\n        If using the 'GAFF' force field please cite the following:\n        .. [3] Wang, J., Wolf, R.M., Caldwell, J.W., Kollman, P.A., Case, D.A.\n               \"Development and testing of a general AMBER force field\" (2004)\n               J. Comput. Chem. 25, 1157-1174\n\n        If using the 'Ghemical' force field please cite the following:\n        .. [3] T. Hassinen and M. Perakyla, \"New energy terms for reduced\n               protein models implemented in an off-lattice force field\" (2001)\n               J. Comput. Chem. 22, 1229-1242\n        \"\"\"\n\n        openbabel = import_('openbabel')\n\n        for particle in self.particles():\n            try:\n                get_by_symbol(particle.name)\n            except KeyError:\n                raise MBuildError(\"Element name {} not recognized. Cannot \"\n                                  \"perform minimization.\"\n                                  \"\".format(particle.name))\n\n        obConversion = openbabel.OBConversion()\n        obConversion.SetInAndOutFormats(\"mol2\", \"pdb\")\n        mol = openbabel.OBMol()\n\n        obConversion.ReadFile(mol, os.path.join(tmp_dir, \"un-minimized.mol2\"))\n\n        ff = openbabel.OBForceField.FindForceField(forcefield)\n        if ff is None:\n            raise MBuildError(\"Force field '{}' not supported for energy \"\n                              \"minimization. Valid force fields are 'MMFF94', \"\n                              \"'MMFF94s', 'UFF', 'GAFF', and 'Ghemical'.\"\n                              \"\".format(forcefield))\n        warn(\n            \"Performing energy minimization using the Open Babel package. Please \"\n            \"refer to the documentation to find the appropriate citations for \"\n            \"Open Babel and the {} force field\".format(forcefield))\n        ff.Setup(mol)\n        if algorithm == 'steep':\n            ff.SteepestDescent(steps)\n        elif algorithm == 'md':\n            ff.MolecularDynamicsTakeNSteps(steps, 300)\n        elif algorithm == 'cg':\n            ff.ConjugateGradients(steps)\n        else:\n            raise MBuildError(\"Invalid minimization algorithm. Valid options \"\n                              \"are 'steep', 'cg', and 'md'.\")\n        ff.UpdateCoordinates(mol)\n\n        obConversion.WriteFile(mol, os.path.join(tmp_dir, 'minimized.pdb'))", "response": "Perform an energy minimization on a Compound using Open Babel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the Compound to a file.", "response": "def save(self, filename, show_ports=False, forcefield_name=None,\n             forcefield_files=None, forcefield_debug=False, box=None,\n             overwrite=False, residues=None, references_file=None,\n             combining_rule='lorentz', foyerkwargs={}, **kwargs):\n        \"\"\"Save the Compound to a file.\n\n        Parameters\n        ----------\n        filename : str\n            Filesystem path in which to save the trajectory. The extension or\n            prefix will be parsed and control the format. Supported\n            extensions are: 'hoomdxml', 'gsd', 'gro', 'top', 'lammps', 'lmp'\n        show_ports : bool, optional, default=False\n            Save ports contained within the compound.\n        forcefield_files : str, optional, default=None\n            Apply a forcefield to the output file using a forcefield provided\n            by the `foyer` package.\n        forcefield_name : str, optional, default=None\n            Apply a named forcefield to the output file using the `foyer`\n            package, e.g. 'oplsaa'. Forcefields listed here:\n            https://github.com/mosdef-hub/foyer/tree/master/foyer/forcefields\n        forcefield_debug : bool, optional, default=False\n            Choose level of verbosity when applying a forcefield through `foyer`.\n            Specifically, when missing atom types in the forcefield xml file,\n            determine if the warning is condensed or verbose.\n        box : mb.Box, optional, default=self.boundingbox (with buffer)\n            Box information to be written to the output file. If 'None', a\n            bounding box is used with 0.25nm buffers at each face to avoid\n            overlapping atoms.\n        overwrite : bool, optional, default=False\n            Overwrite if the filename already exists\n        residues : str of list of str\n            Labels of residues in the Compound. Residues are assigned by\n            checking against Compound.name.\n        references_file : str, optional, default=None\n            Specify a filename to write references for the forcefield that is\n            to be applied. References are written in BiBTeX format.\n        combining_rule : str, optional, default='lorentz'\n            Specify the combining rule for nonbonded interactions. Only relevant\n            when the `foyer` package is used to apply a forcefield. Valid\n            options are 'lorentz' and 'geometric', specifying Lorentz-Berthelot\n            and geometric combining rules respectively.\n        \n\n        Other Parameters\n        ----------------\n        foyerkwargs : dict, optional\n            Specify keyword arguments when applying the foyer Forcefield\n        ref_distance : float, optional, default=1.0\n            Normalization factor used when saving to .gsd and .hoomdxml formats\n            for converting distance values to reduced units.\n        ref_energy : float, optional, default=1.0\n            Normalization factor used when saving to .gsd and .hoomdxml formats\n            for converting energy values to reduced units.\n        ref_mass : float, optional, default=1.0\n            Normalization factor used when saving to .gsd and .hoomdxml formats\n            for converting mass values to reduced units.\n        atom_style: str, default='full'\n            Defines the style of atoms to be saved in a LAMMPS data file. The following atom\n            styles are currently supported: 'full', 'atomic', 'charge', 'molecular'\n            see http://lammps.sandia.gov/doc/atom_style.html for more\n            information on atom styles.\n\n        See Also\n        --------\n        formats.gsdwrite.write_gsd : Write to GSD format\n        formats.hoomdxml.write_hoomdxml : Write to Hoomd XML format\n        formats.lammpsdata.write_lammpsdata : Write to LAMMPS data format\n\n        \"\"\"\n        extension = os.path.splitext(filename)[-1]\n        if extension == '.xyz':\n            traj = self.to_trajectory(show_ports=show_ports)\n            traj.save(filename)\n            return\n\n        # Savers supported by mbuild.formats\n        savers = {'.hoomdxml': write_hoomdxml,\n                  '.gsd': write_gsd,\n                  '.lammps': write_lammpsdata,\n                  '.lmp': write_lammpsdata}\n\n        try:\n            saver = savers[extension]\n        except KeyError:\n            saver = None\n\n        if os.path.exists(filename) and not overwrite:\n            raise IOError('{0} exists; not overwriting'.format(filename))\n\n        structure = self.to_parmed(box=box, residues=residues,\n                                   show_ports=show_ports)\n        # Apply a force field with foyer if specified\n        if forcefield_name or forcefield_files:\n            foyer = import_('foyer')\n            ff = foyer.Forcefield(forcefield_files=forcefield_files,\n                                  name=forcefield_name, debug=forcefield_debug)\n            structure = ff.apply(structure, references_file=references_file,\n                    **foyerkwargs)\n            structure.combining_rule = combining_rule\n\n        total_charge = sum([atom.charge for atom in structure])\n        if round(total_charge, 4) != 0.0:\n            warn('System is not charge neutral. Total charge is {}.'\n                 ''.format(total_charge))\n\n        # Provide a warning if rigid_ids are not sequential from 0\n        if self.contains_rigid:\n            unique_rigid_ids = sorted(set([\n                p.rigid_id for p in self.rigid_particles()]))\n            if max(unique_rigid_ids) != len(unique_rigid_ids) - 1:\n                warn(\"Unique rigid body IDs are not sequential starting from zero.\")\n\n        if saver:  # mBuild supported saver.\n            if extension in ['.gsd', '.hoomdxml']:\n                kwargs['rigid_bodies'] = [\n                        p.rigid_id for p in self.particles()]\n            saver(filename=filename, structure=structure, **kwargs)\n        else:  # ParmEd supported saver.\n            structure.save(filename, overwrite=overwrite, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef translate(self, by):\n        new_positions = _translate(self.xyz_with_ports, by)\n        self.xyz_with_ports = new_positions", "response": "Translate the Compound by a vector"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrotate the Compound around an arbitrary vector.", "response": "def rotate(self, theta, around):\n        \"\"\"Rotate Compound around an arbitrary vector.\n\n        Parameters\n        ----------\n        theta : float\n            The angle by which to rotate the Compound, in radians.\n        around : np.ndarray, shape=(3,), dtype=float\n            The vector about which to rotate the Compound.\n\n        \"\"\"\n        new_positions = _rotate(self.xyz_with_ports, theta, around)\n        self.xyz_with_ports = new_positions"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrotates the Compound in place around an arbitrary vector.", "response": "def spin(self, theta, around):\n        \"\"\"Rotate Compound in place around an arbitrary vector.\n\n        Parameters\n        ----------\n        theta : float\n            The angle by which to rotate the Compound, in radians.\n        around : np.ndarray, shape=(3,), dtype=float\n            The axis about which to spin the Compound.\n\n        \"\"\"\n        around = np.asarray(around).reshape(3)\n        center_pos = self.center\n        self.translate(-center_pos)\n        self.rotate(theta, around)\n        self.translate(center_pos)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_trajectory(self, traj, frame=-1, coords_only=False):\n        if coords_only:\n            if traj.n_atoms != self.n_particles:\n                raise ValueError('Number of atoms in {traj} does not match'\n                                 ' {self}'.format(**locals()))\n            atoms_particles = zip(traj.topology.atoms,\n                                  self.particles(include_ports=False))\n            if None in self._particles(include_ports=False):\n                raise ValueError('Some particles are None')\n            for mdtraj_atom, particle in atoms_particles:\n                particle.pos = traj.xyz[frame, mdtraj_atom.index]\n            return\n\n        atom_mapping = dict()\n        for chain in traj.topology.chains:\n            if traj.topology.n_chains > 1:\n                chain_compound = Compound()\n                self.add(chain_compound, 'chain[$]')\n            else:\n                chain_compound = self\n            for res in chain.residues:\n                for atom in res.atoms:\n                    new_atom = Particle(name=str(atom.name),\n                                        pos=traj.xyz[frame, atom.index])\n                    chain_compound.add(\n                        new_atom, label='{0}[$]'.format(\n                            atom.name))\n                    atom_mapping[atom] = new_atom\n\n        for mdtraj_atom1, mdtraj_atom2 in traj.topology.bonds:\n            atom1 = atom_mapping[mdtraj_atom1]\n            atom2 = atom_mapping[mdtraj_atom2]\n            self.add_bond((atom1, atom2))\n\n        if np.any(traj.unitcell_lengths) and np.any(traj.unitcell_lengths[0]):\n            self.periodicity = traj.unitcell_lengths[0]\n        else:\n            self.periodicity = np.array([0., 0., 0.])", "response": "Extract atoms and bonds from a md. Trajectory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_trajectory(self, show_ports=False, chains=None,\n                      residues=None, box=None):\n        \"\"\"Convert to an md.Trajectory and flatten the compound.\n\n        Parameters\n        ----------\n        show_ports : bool, optional, default=False\n            Include all port atoms when converting to trajectory.\n        chains : mb.Compound or list of mb.Compound\n            Chain types to add to the topology\n        residues : str of list of str\n            Labels of residues in the Compound. Residues are assigned by\n            checking against Compound.name.\n        box : mb.Box, optional, default=self.boundingbox (with buffer)\n            Box information to be used when converting to a `Trajectory`.\n            If 'None', a bounding box is used with a 0.5nm buffer in each\n            dimension. to avoid overlapping atoms, unless `self.periodicity`\n            is not None, in which case those values are used for the\n            box lengths.\n\n        Returns\n        -------\n        trajectory : md.Trajectory\n\n        See also\n        --------\n        _to_topology\n\n        \"\"\"\n        atom_list = [particle for particle in self.particles(show_ports)]\n\n        top = self._to_topology(atom_list, chains, residues)\n\n        # Coordinates.\n        xyz = np.ndarray(shape=(1, top.n_atoms, 3), dtype='float')\n        for idx, atom in enumerate(atom_list):\n            xyz[0, idx] = atom.pos\n\n        # Unitcell information.\n        unitcell_angles = [90.0, 90.0, 90.0]\n        if box is None:\n            unitcell_lengths = np.empty(3)\n            for dim, val in enumerate(self.periodicity):\n                if val:\n                    unitcell_lengths[dim] = val\n                else:\n                    unitcell_lengths[dim] = self.boundingbox.lengths[dim] + 0.5\n        else:\n            unitcell_lengths = box.lengths\n            unitcell_angles = box.angles\n\n        return md.Trajectory(xyz, top, unitcell_lengths=unitcell_lengths,\n                             unitcell_angles=unitcell_angles)", "response": "Convert the Compound to a md. Trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_topology(self, atom_list, chains=None, residues=None):\n        from mdtraj.core.topology import Topology\n\n        if isinstance(chains, string_types):\n            chains = [chains]\n        if isinstance(chains, (list, set)):\n            chains = tuple(chains)\n\n        if isinstance(residues, string_types):\n            residues = [residues]\n        if isinstance(residues, (list, set)):\n            residues = tuple(residues)\n        top = Topology()\n        atom_mapping = {}\n\n        default_chain = top.add_chain()\n        default_residue = top.add_residue('RES', default_chain)\n\n        compound_residue_map = dict()\n        atom_residue_map = dict()\n        compound_chain_map = dict()\n        atom_chain_map = dict()\n\n        for atom in atom_list:\n            # Chains\n            if chains:\n                if atom.name in chains:\n                    current_chain = top.add_chain()\n                    compound_chain_map[atom] = current_chain\n                else:\n                    for parent in atom.ancestors():\n                        if chains and parent.name in chains:\n                            if parent not in compound_chain_map:\n                                current_chain = top.add_chain()\n                                compound_chain_map[parent] = current_chain\n                                current_residue = top.add_residue(\n                                    'RES', current_chain)\n                            break\n                    else:\n                        current_chain = default_chain\n            else:\n                current_chain = default_chain\n            atom_chain_map[atom] = current_chain\n\n            # Residues\n            if residues:\n                if atom.name in residues:\n                    current_residue = top.add_residue(atom.name, current_chain)\n                    compound_residue_map[atom] = current_residue\n                else:\n                    for parent in atom.ancestors():\n                        if residues and parent.name in residues:\n                            if parent not in compound_residue_map:\n                                current_residue = top.add_residue(\n                                    parent.name, current_chain)\n                                compound_residue_map[parent] = current_residue\n                            break\n                    else:\n                        current_residue = default_residue\n            else:\n                if chains:\n                    try:  # Grab the default residue from the custom chain.\n                        current_residue = next(current_chain.residues)\n                    except StopIteration:  # Add the residue to the current chain\n                        current_residue = top.add_residue('RES', current_chain)\n                else:  # Grab the default chain's default residue\n                    current_residue = default_residue\n            atom_residue_map[atom] = current_residue\n\n            # Add the actual atoms\n            try:\n                elem = get_by_symbol(atom.name)\n            except KeyError:\n                elem = get_by_symbol(\"VS\")\n            at = top.add_atom(atom.name, elem, atom_residue_map[atom])\n            at.charge = atom.charge\n            atom_mapping[atom] = at\n\n        # Remove empty default residues.\n        chains_to_remove = [\n            chain for chain in top.chains if chain.n_atoms == 0]\n        residues_to_remove = [res for res in top.residues if res.n_atoms == 0]\n        for chain in chains_to_remove:\n            top._chains.remove(chain)\n        for res in residues_to_remove:\n            for chain in top.chains:\n                try:\n                    chain._residues.remove(res)\n                except ValueError:  # Already gone.\n                    pass\n\n        for atom1, atom2 in self.bonds():\n            # Ensure that both atoms are part of the compound. This becomes an\n            # issue if you try to convert a sub-compound to a topology which is\n            # bonded to a different subcompound.\n            if all(a in atom_mapping.keys() for a in [atom1, atom2]):\n                top.add_bond(atom_mapping[atom1], atom_mapping[atom2])\n        return top", "response": "Create a mdtraj. Topology object from a list of atoms and chains."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_parmed(self, structure, coords_only=False):\n        if coords_only:\n            if len(structure.atoms) != self.n_particles:\n                raise ValueError(\n                    'Number of atoms in {structure} does not match'\n                    ' {self}'.format(\n                        **locals()))\n            atoms_particles = zip(structure.atoms,\n                                  self.particles(include_ports=False))\n            if None in self._particles(include_ports=False):\n                raise ValueError('Some particles are None')\n            for parmed_atom, particle in atoms_particles:\n                particle.pos = np.array([parmed_atom.xx,\n                                         parmed_atom.xy,\n                                         parmed_atom.xz]) / 10\n            return\n\n        atom_mapping = dict()\n        chain_id = None\n        chains = defaultdict(list)\n        for residue in structure.residues:\n            chains[residue.chain].append(residue)\n\n        for chain, residues in chains.items():\n            if len(chains) > 1:\n                chain_compound = Compound()\n                self.add(chain_compound, chain_id)\n            else:\n                chain_compound = self\n            for residue in residues:\n                for atom in residue.atoms:\n                    pos = np.array([atom.xx, atom.xy, atom.xz]) / 10\n                    new_atom = Particle(name=str(atom.name), pos=pos)\n                    chain_compound.add(\n                        new_atom, label='{0}[$]'.format(\n                            atom.name))\n                    atom_mapping[atom] = new_atom\n\n        for bond in structure.bonds:\n            atom1 = atom_mapping[bond.atom1]\n            atom2 = atom_mapping[bond.atom2]\n            self.add_bond((atom1, atom2))\n\n        if structure.box is not None:\n            # Convert from A to nm\n            self.periodicity = 0.1 * structure.box[0:3]\n        else:\n            self.periodicity = np.array([0., 0., 0.])", "response": "Extract atoms and bonds from a structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_parmed(self, box=None, title='', residues=None, show_ports=False):\n        structure = pmd.Structure()\n        structure.title = title if title else self.name\n        atom_mapping = {}  # For creating bonds below\n        guessed_elements = set()\n\n        if isinstance(residues, string_types):\n            residues = [residues]\n        if isinstance(residues, (list, set)):\n            residues = tuple(residues)\n\n        default_residue = pmd.Residue('RES')\n        port_residue = pmd.Residue('PRT')\n        compound_residue_map = dict()\n        atom_residue_map = dict()\n\n        for atom in self.particles(include_ports=show_ports):\n            if atom.port_particle:\n                current_residue = port_residue\n                atom_residue_map[atom] = current_residue\n\n                if current_residue not in structure.residues:\n                    structure.residues.append(current_residue)\n\n                pmd_atom = pmd.Atom(atomic_number=0, name='VS',\n                                    mass=0, charge=0)\n                pmd_atom.xx, pmd_atom.xy, pmd_atom.xz = atom.pos * 10  # Angstroms\n\n            else:\n                if residues and atom.name in residues:\n                    current_residue = pmd.Residue(atom.name)\n                    atom_residue_map[atom] = current_residue\n                    compound_residue_map[atom] = current_residue\n                elif residues:\n                    for parent in atom.ancestors():\n                        if residues and parent.name in residues:\n                            if parent not in compound_residue_map:\n                                current_residue = pmd.Residue(parent.name)\n                                compound_residue_map[parent] = current_residue\n                            atom_residue_map[atom] = current_residue\n                            break\n                    else:  # Did not find specified residues in ancestors.\n                        current_residue = default_residue\n                        atom_residue_map[atom] = current_residue\n                else:\n                    current_residue = default_residue\n                    atom_residue_map[atom] = current_residue\n\n                if current_residue not in structure.residues:\n                    structure.residues.append(current_residue)\n\n                atomic_number = None\n                name = ''.join(char for char in atom.name if not char.isdigit())\n                try:\n                    atomic_number = AtomicNum[atom.name]\n                except KeyError:\n                    element = element_by_name(atom.name)\n                    if name not in guessed_elements:\n                        warn(\n                            'Guessing that \"{}\" is element: \"{}\"'.format(\n                                atom, element))\n                        guessed_elements.add(name)\n                else:\n                    element = atom.name\n\n                atomic_number = atomic_number or AtomicNum[element]\n                mass = Mass[element]\n                pmd_atom = pmd.Atom(atomic_number=atomic_number, name=atom.name,\n                                    mass=mass, charge=atom.charge)\n                pmd_atom.xx, pmd_atom.xy, pmd_atom.xz = atom.pos * 10  # Angstroms\n\n            residue = atom_residue_map[atom]\n            structure.add_atom(pmd_atom, resname=residue.name,\n                               resnum=residue.idx)\n\n            atom_mapping[atom] = pmd_atom\n\n        structure.residues.claim()\n\n        for atom1, atom2 in self.bonds():\n            bond = pmd.Bond(atom_mapping[atom1], atom_mapping[atom2])\n            structure.bonds.append(bond)\n        # pad box with .25nm buffers\n        if box is None:\n            box = self.boundingbox\n            box_vec_max = box.maxs.tolist()\n            box_vec_min = box.mins.tolist()\n            for dim, val in enumerate(self.periodicity):\n                if val:\n                    box_vec_max[dim] = val\n                    box_vec_min[dim] = 0.0\n                if not val:\n                    box_vec_max[dim] += 0.25\n                    box_vec_min[dim] -= 0.25\n            box.mins = np.asarray(box_vec_min)\n            box.maxs = np.asarray(box_vec_max)\n\n        box_vector = np.empty(6)\n        if box.angles is not None:\n            box_vector[3:6] = box.angles\n        else:\n            box_vector[3] = box_vector[4] = box_vector[5] = 90.0\n        for dim in range(3):\n            box_vector[dim] = box.lengths[dim] * 10\n        structure.box = box_vector\n        return structure", "response": "Create a ParmEd Structure object from a Compound."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_networkx(self, names_only=False):\n        nx = import_('networkx')\n\n        nodes = list()\n        edges = list()\n        if names_only:\n            nodes.append(self.name)\n        else:\n            nodes.append(self)\n        nodes, edges = self._iterate_children(nodes, edges, names_only=names_only)\n\n        graph = nx.DiGraph()\n        graph.add_nodes_from(nodes)\n        graph.add_edges_from(edges)\n        return graph", "response": "Create a NetworkX graph representing the hierarchy of a Compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_intermol(self, molecule_types=None):\n        from intermol.atom import Atom as InterMolAtom\n        from intermol.molecule import Molecule\n        from intermol.system import System\n        import simtk.unit as u\n\n        if isinstance(molecule_types, list):\n            molecule_types = tuple(molecule_types)\n        elif molecule_types is None:\n            molecule_types = (type(self),)\n        intermol_system = System()\n\n        last_molecule_compound = None\n        for atom_index, atom in enumerate(self.particles()):\n            for parent in atom.ancestors():\n                # Don't want inheritance via isinstance().\n                if type(parent) in molecule_types:\n                    # Check if we have encountered this molecule type before.\n                    if parent.name not in intermol_system.molecule_types:\n                        self._add_intermol_molecule_type(\n                            intermol_system, parent)\n                    if parent != last_molecule_compound:\n                        last_molecule_compound = parent\n                        last_molecule = Molecule(name=parent.name)\n                        intermol_system.add_molecule(last_molecule)\n                    break\n            else:\n                # Should never happen if molecule_types only contains\n                # type(self)\n                raise ValueError(\n                    'Found an atom {} that is not part of any of '\n                    'the specified molecule types {}'.format(\n                        atom, molecule_types))\n\n            # Add the actual intermol atoms.\n            intermol_atom = InterMolAtom(atom_index + 1, name=atom.name,\n                                         residue_index=1, residue_name='RES')\n            intermol_atom.position = atom.pos * u.nanometers\n            last_molecule.add_atom(intermol_atom)\n        return intermol_system", "response": "Create an InterMol system from a Compound."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new molecule type for the parent and add bonds.", "response": "def _add_intermol_molecule_type(intermol_system, parent):\n        \"\"\"Create a molecule type for the parent and add bonds. \"\"\"\n        from intermol.moleculetype import MoleculeType\n        from intermol.forces.bond import Bond as InterMolBond\n\n        molecule_type = MoleculeType(name=parent.name)\n        intermol_system.add_molecule_type(molecule_type)\n\n        for index, parent_atom in enumerate(parent.particles()):\n            parent_atom.index = index + 1\n\n        for atom1, atom2 in parent.bonds():\n            intermol_bond = InterMolBond(atom1.index, atom2.index)\n            molecule_type.bonds.add(intermol_bond)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that a Port label exists in a Compound.", "response": "def assert_port_exists(port_name, compound):\n    \"\"\"Ensure that a Port label exists in a Compound.  \"\"\"\n    if port_name in compound.labels:\n        return True\n    else:\n        from mbuild.port import Port\n        available_ports = [name for name in compound.labels\n                           if isinstance(compound.labels[name], Port)]\n        compound_name = compound.__class__.__name__\n        raise ValueError(\"No port named '{port_name}' in {compound_name}'s\"\n                         \" labels. Labeled Ports in {compound_name} are:\"\n                         \" {available_ports}\".format(**locals()))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cleave_interface(self, bulk_silica, tile_x, tile_y, thickness):\n        O_buffer = self._O_buffer\n        tile_z = int(math.ceil((thickness + 2*O_buffer) / bulk_silica.periodicity[2]))\n        bulk = mb.recipes.TiledCompound(bulk_silica, n_tiles=(tile_x, tile_y, tile_z))\n\n        interface = mb.Compound(periodicity=(bulk.periodicity[0],\n                                             bulk.periodicity[1],\n                                             0.0))\n        for i, particle in enumerate(bulk.particles()):\n            if ((particle.name == 'Si' and O_buffer < particle.pos[2] < (thickness + O_buffer)) or\n                    (particle.name == 'O' and particle.pos[2] < (thickness + 2*O_buffer))):\n                interface_particle = mb.Compound(name=particle.name, pos=particle.pos)\n                interface.add(interface_particle, particle.name + \"_{}\".format(i))\n        self.add(interface)", "response": "Carve interface from bulk silica."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving stray atoms and surface pieces.", "response": "def _strip_stray_atoms(self):\n        \"\"\"Remove stray atoms and surface pieces. \"\"\"\n        components = self.bond_graph.connected_components()\n        major_component = max(components, key=len)\n        for atom in list(self.particles()):\n            if atom not in major_component:\n                self.remove(atom)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bridge_dangling_Os(self, oh_density, thickness):\n\n        area = self.periodicity[0] * self.periodicity[1]\n        target = int(oh_density * area)\n\n        dangling_Os = [atom for atom in self.particles()\n                       if atom.name == 'O' and\n                       atom.pos[2] > thickness and\n                       len(self.bond_graph.neighbors(atom)) == 1]\n\n        n_bridges = int((len(dangling_Os) - target) / 2)\n\n        for _ in range(n_bridges):\n            bridged = False\n            while not bridged:\n                O1 = random.choice(dangling_Os)\n                Si1 = self.bond_graph.neighbors(O1)[0]\n                for O2 in dangling_Os:\n                    if O2 == O1:\n                        continue\n                    Si2 = self.bond_graph.neighbors(O2)[0]\n                    if Si1 == Si2:\n                        continue\n                    if any(neigh in self.bond_graph.neighbors(Si2)\n                           for neigh in self.bond_graph.neighbors(Si1)):\n                        continue\n                    r = self.min_periodic_distance(Si1.pos, Si2.pos)\n                    if r < 0.45:\n                        bridged = True\n                        self.add_bond((O1, Si2))\n                        dangling_Os.remove(O1)\n                        dangling_Os.remove(O2)\n                        self.remove(O2)\n                        break", "response": "Form Si - O - Si bridges to yield desired density of reactive surface sites."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _identify_surface_sites(self, thickness):\n        for atom in self.particles():\n            if len(self.bond_graph.neighbors(atom)) == 1:\n                if atom.name == 'O' and atom.pos[2] > thickness:\n                    atom.name = 'OS'\n                    port = mb.Port(anchor=atom)\n                    port.spin(np.pi/2, [1, 0, 0])\n                    port.translate(np.array([0.0, 0.0, 0.1]))\n                    self.add(port, \"port_{}\".format(len(self.referenced_ports())))", "response": "Label surface sites and add ports above them."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _adjust_stoichiometry(self):\n        num_O = len(list(self.particles_by_name('O')))\n        num_Si = len(list(self.particles_by_name('Si')))\n        n_deletions = num_O - 2*num_Si\n\n        bottom_Os = [atom for atom in self.particles()\n                     if atom.name == 'O' and\n                        atom.pos[2] < self._O_buffer and\n                        len(self.bond_graph.neighbors(atom)) == 1]\n\n        for _ in range(n_deletions):\n            O1 = random.choice(bottom_Os)\n            bottom_Os.remove(O1)\n            self.remove(O1)", "response": "Remove O s from underside of surface to yield a 2 : 1 Si : O ratio."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fill_box(compound, n_compounds=None, box=None, density=None, overlap=0.2,\n             seed=12345, edge=0.2, compound_ratio=None,\n             aspect_ratio=None, fix_orientation=False, temp_file=None):\n    \"\"\"Fill a box with a compound using packmol.\n\n    Two arguments of `n_compounds, box, and density` must be specified.\n\n    If `n_compounds` and `box` are not None, the specified number of\n    n_compounds will be inserted into a box of the specified size.\n\n    If `n_compounds` and `density` are not None, the corresponding box\n    size will be calculated internally. In this case, `n_compounds`\n    must be an int and not a list of int.\n\n    If `box` and `density` are not None, the corresponding number of\n    compounds will be calculated internally.\n\n    For the cases in which `box` is not specified but generated internally,\n    the default behavior is to calculate a cubic box. Optionally,\n    `aspect_ratio` can be passed to generate a non-cubic box.\n\n    Parameters\n    ----------\n    compound : mb.Compound or list of mb.Compound\n        Compound or list of compounds to be put in box.\n    n_compounds : int or list of int\n        Number of compounds to be put in box.\n    box : mb.Box\n        Box to be filled by compounds.\n    density : float, units kg/m^3, default=None\n        Target density for the system in macroscale units. If not None, one of\n        `n_compounds` or `box`, but not both, must be specified.\n    overlap : float, units nm, default=0.2\n        Minimum separation between atoms of different molecules.\n    seed : int, default=12345\n        Random seed to be passed to PACKMOL.\n    edge : float, units nm, default=0.2\n        Buffer at the edge of the box to not place molecules. This is necessary\n        in some systems because PACKMOL does not account for periodic boundary\n        conditions in its optimization.\n    compound_ratio : list, default=None\n        Ratio of number of each compound to be put in box. Only used in the\n        case of `density` and `box` having been specified, `n_compounds` not\n        specified, and more than one `compound`.\n    aspect_ratio : list of float\n        If a non-cubic box is desired, the ratio of box lengths in the x, y,\n        and z directions.\n    fix_orientation : bool or list of bools\n        Specify that compounds should not be rotated when filling the box,\n        default=False.\n    temp_file : str, default=None\n        File name to write PACKMOL's raw output to.\n\n    Returns\n    -------\n    filled : mb.Compound\n\n    \"\"\"\n    _check_packmol(PACKMOL)\n\n    arg_count = 3 - [n_compounds, box, density].count(None)\n    if arg_count != 2:\n        msg = (\"Exactly 2 of `n_compounds`, `box`, and `density` \"\n               \"must be specified. {} were given.\".format(arg_count))\n        raise ValueError(msg)\n\n    if box is not None:\n        box = _validate_box(box)\n    if not isinstance(compound, (list, set)):\n        compound = [compound]\n    if n_compounds is not None and not isinstance(n_compounds, (list, set)):\n        n_compounds = [n_compounds]\n    if not isinstance(fix_orientation, (list, set)):\n        fix_orientation = [fix_orientation]*len(compound)\n\n    if compound is not None and n_compounds is not None:\n        if len(compound) != len(n_compounds):\n            msg = (\"`compound` and `n_compounds` must be of equal length.\")\n            raise ValueError(msg)\n\n    if compound is not None:\n        if len(compound) != len(fix_orientation):\n            msg = (\"`compound`, `n_compounds`, and `fix_orientation` must be of equal length.\")\n            raise ValueError(msg)\n\n    if density is not None:\n        if box is None and n_compounds is not None:\n            total_mass = np.sum([n*np.sum([a.mass for a in c.to_parmed().atoms])\n                                for c, n in zip(compound, n_compounds)])\n            # Conversion from (amu/(kg/m^3))**(1/3) to nm\n            L = (total_mass/density)**(1/3)*1.1841763\n            if aspect_ratio is None:\n                box = _validate_box(Box(3*[L]))\n            else:\n                L *= np.prod(aspect_ratio) ** (-1/3)\n                box = _validate_box(Box([val*L for val in aspect_ratio]))\n        if n_compounds is None and box is not None:\n            if len(compound) == 1:\n                compound_mass = np.sum([a.mass for a in compound[0].to_parmed().atoms])\n                # Conversion from kg/m^3 / amu * nm^3 to dimensionless units\n                n_compounds = [int(density/compound_mass*np.prod(box.lengths)*.60224)]\n            else:\n                if compound_ratio is None:\n                    msg = (\"Determing `n_compounds` from `density` and `box` \"\n                           \"for systems with more than one compound type requires\"\n                           \"`compound_ratio`\")\n                    raise ValueError(msg)\n                if len(compound) != len(compound_ratio):\n                    msg = (\"Length of `compound_ratio` must equal length of \"\n                           \"`compound`\")\n                    raise ValueError(msg)\n                prototype_mass = 0\n                for c, r in zip(compound, compound_ratio):\n                    prototype_mass += r * np.sum([a.mass for a in c.to_parmed().atoms])\n                # Conversion from kg/m^3 / amu * nm^3 to dimensionless units\n                n_prototypes = int(density/prototype_mass*np.prod(box.lengths)*.60224)\n                n_compounds = list()\n                for c in compound_ratio:\n                    n_compounds.append(int(n_prototypes * c))\n\n    # In angstroms for packmol.\n    box_mins = box.mins * 10\n    box_maxs = box.maxs * 10\n    overlap *= 10\n\n    # Apply edge buffer\n    box_maxs -= edge * 10\n\n    # Build the input file for each compound and call packmol.\n    filled_xyz = _new_xyz_file()\n\n    # create a list to contain the file handles for the compound temp files\n    compound_xyz_list = list()\n    try:\n        input_text = PACKMOL_HEADER.format(overlap, filled_xyz.name, seed)\n        for comp, m_compounds, rotate in zip(compound, n_compounds, fix_orientation):\n            m_compounds = int(m_compounds)\n\n            compound_xyz = _new_xyz_file()\n            compound_xyz_list.append(compound_xyz)\n\n            comp.save(compound_xyz.name, overwrite=True)\n            input_text += PACKMOL_BOX.format(compound_xyz.name, m_compounds,\n                                             box_mins[0], box_mins[1],\n                                             box_mins[2], box_maxs[0],\n                                             box_maxs[1], box_maxs[2],\n                                             PACKMOL_CONSTRAIN if rotate else \"\")\n\n        _run_packmol(input_text, filled_xyz, temp_file)\n        # Create the topology and update the coordinates.\n        filled = Compound()\n        filled = _create_topology(filled, compound, n_compounds)\n        filled.update_coordinates(filled_xyz.name)\n        filled.periodicity = np.asarray(box.lengths, dtype=np.float32)\n\n    finally:\n        for file_handle in compound_xyz_list:\n            file_handle.close()\n            os.unlink(file_handle.name)\n        filled_xyz.close()\n        os.unlink(filled_xyz.name)\n    return filled", "response": "Fill a box with a compound using packmol."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_region(compound, n_compounds, region, overlap=0.2,\n                seed=12345, edge=0.2, fix_orientation=False, temp_file=None):\n    \"\"\"Fill a region of a box with a compound using packmol.\n\n    Parameters\n    ----------\n    compound : mb.Compound or list of mb.Compound\n        Compound or list of compounds to be put in region.\n    n_compounds : int or list of int\n        Number of compounds to be put in region.\n    region : mb.Box or list of mb.Box\n        Region to be filled by compounds.\n    overlap : float, units nm, default=0.2\n        Minimum separation between atoms of different molecules.\n    seed : int, default=12345\n        Random seed to be passed to PACKMOL.\n    edge : float, units nm, default=0.2\n        Buffer at the edge of the region to not place molecules. This is\n        necessary in some systems because PACKMOL does not account for\n        periodic boundary conditions in its optimization.\n    fix_orientation : bool or list of bools\n        Specify that compounds should not be rotated when filling the box,\n        default=False.\n    temp_file : str, default=None\n        File name to write PACKMOL's raw output to.\n\n    Returns\n    -------\n    filled : mb.Compound\n\n    If using mulitple regions and compounds, the nth value in each\n    list are used in order.\n    For example, if the third compound will be put in the third\n    region using the third value in n_compounds.\n    \"\"\"\n    _check_packmol(PACKMOL)\n\n    if not isinstance(compound, (list, set)):\n        compound = [compound]\n    if not isinstance(n_compounds, (list, set)):\n        n_compounds = [n_compounds]\n    if not isinstance(fix_orientation, (list, set)):\n        fix_orientation = [fix_orientation]*len(compound)\n\n    if compound is not None and n_compounds is not None:\n        if len(compound) != len(n_compounds):\n            msg = (\"`compound` and `n_compounds` must be of equal length.\")\n            raise ValueError(msg)\n    if compound is not None:\n        if len(compound) != len(fix_orientation):\n            msg = (\"`compound`, `n_compounds`, and `fix_orientation` must be of equal length.\")\n            raise ValueError(msg)\n\n    # See if region is a single region or list\n    if isinstance(region, Box):  # Cannot iterate over boxes\n        region = [region]\n    elif not any(isinstance(reg, (list, set, Box)) for reg in region):\n        region = [region]\n    region = [_validate_box(reg) for reg in region]\n\n    # In angstroms for packmol.\n    overlap *= 10\n\n    # Build the input file and call packmol.\n    filled_xyz = _new_xyz_file()\n\n    # List to hold file handles for the temporary compounds\n    compound_xyz_list = list()\n    try:\n        input_text = PACKMOL_HEADER.format(overlap, filled_xyz.name, seed)\n\n        for comp, m_compounds, reg, rotate in zip(compound, n_compounds, region, fix_orientation):\n            m_compounds = int(m_compounds)\n\n            compound_xyz = _new_xyz_file()\n            compound_xyz_list.append(compound_xyz)\n\n            comp.save(compound_xyz.name, overwrite=True)\n            reg_mins = reg.mins * 10\n            reg_maxs = reg.maxs * 10\n            reg_maxs -= edge * 10  # Apply edge buffer\n            input_text += PACKMOL_BOX.format(compound_xyz.name, m_compounds,\n                                             reg_mins[0], reg_mins[1],\n                                             reg_mins[2], reg_maxs[0],\n                                             reg_maxs[1], reg_maxs[2],\n                                            PACKMOL_CONSTRAIN if rotate else \"\")\n\n        _run_packmol(input_text, filled_xyz, temp_file)\n\n        # Create the topology and update the coordinates.\n        filled = Compound()\n        filled = _create_topology(filled, compound, n_compounds)\n        filled.update_coordinates(filled_xyz.name)\n    finally:\n        for file_handle in compound_xyz_list:\n            file_handle.close()\n            os.unlink(file_handle.name)\n        filled_xyz.close()\n        os.unlink(filled_xyz.name)\n    return filled", "response": "Fill a region of a box with a compound."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fill_sphere(compound, sphere, n_compounds=None, density=None, overlap=0.2,\n                seed=12345, edge=0.2, compound_ratio=None,\n                fix_orientation=False, temp_file=None):\n    \"\"\"Fill a sphere with a compound using packmol.\n\n    One argument of `n_compounds and density` must be specified.\n\n    If `n_compounds` is not None, the specified number of\n    n_compounds will be inserted into a sphere of the specified size.\n\n    If `density` is not None, the corresponding number of\n    compounds will be calculated internally.\n\n    Parameters\n    ----------\n    compound : mb.Compound or list of mb.Compound\n        Compound or list of compounds to be put in box.\n    sphere : list, units nm\n        Sphere coordinates in the form [x_center, y_center, z_center, radius]\n    n_compounds : int or list of int\n        Number of compounds to be put in box.\n    density : float, units kg/m^3, default=None\n        Target density for the sphere in macroscale units.\n    overlap : float, units nm, default=0.2\n        Minimum separation between atoms of different molecules.\n    seed : int, default=12345\n        Random seed to be passed to PACKMOL.\n    edge : float, units nm, default=0.2\n        Buffer at the edge of the sphere to not place molecules. This is necessary\n        in some systems because PACKMOL does not account for periodic boundary\n        conditions in its optimization.\n    compound_ratio : list, default=None\n        Ratio of number of each compound to be put in sphere. Only used in the\n        case of `density` having been specified, `n_compounds` not specified, \n        and more than one `compound`.\n    fix_orientation : bool or list of bools\n        Specify that compounds should not be rotated when filling the sphere,\n        default=False.\n    temp_file : str, default=None\n        File name to write PACKMOL's raw output to.\n\n    Returns\n    -------\n    filled : mb.Compound\n\n    \"\"\"\n    _check_packmol(PACKMOL)\n\n    arg_count = 2 - [n_compounds, density].count(None)\n    if arg_count != 1:\n        msg = (\"Exactly 1 of `n_compounds` and `density` \"\n               \"must be specified. {} were given.\".format(arg_count))\n        raise ValueError(msg)\n\n    if isinstance(sphere, (list, set, tuple)):\n        if len(sphere) != 4:\n            msg = (\"`sphere` must be a list of len 4\")\n    else:\n        msg = (\"`sphere` must be a list\")\n        raise ValueError(msg)\n\n    if not isinstance(compound, (list, set)):\n        compound = [compound]\n    if n_compounds is not None and not isinstance(n_compounds, (list, set)):\n        n_compounds = [n_compounds]\n    if not isinstance(fix_orientation, (list, set)):\n        fix_orientation = [fix_orientation]*len(compound)\n\n    if compound is not None and n_compounds is not None:\n        if len(compound) != len(n_compounds):\n            msg = (\"`compound` and `n_compounds` must be of equal length.\")\n            raise ValueError(msg)\n\n    if compound is not None:\n        if len(compound) != len(fix_orientation):\n            msg = (\"`compound`, `n_compounds`, and `fix_orientation` must be of equal length.\")\n            raise ValueError(msg)\n\n    for coord in sphere[:3]:\n        if coord < sphere[3]:\n            msg = (\"`sphere` center coordinates must be greater than radius.\")\n            raise ValueError(msg)\n\n    # Apply edge buffer\n    radius = sphere[3] - edge\n\n    if density is not None:\n        if n_compounds is None:\n            if len(compound) == 1:\n                compound_mass = np.sum([a.mass for a in compound[0].to_parmed().atoms])\n                # Conversion from kg/m^3 / amu * nm^3 to dimensionless units\n                n_compounds = [int(density/compound_mass*(4/3*np.pi*radius**3)*.60224)]\n            else:\n                if compound_ratio is None:\n                    msg = (\"Determing `n_compounds` from `density` \"\n                           \"for systems with more than one compound type requires\"\n                           \"`compound_ratio`\")\n                    raise ValueError(msg)\n                if len(compound) != len(compound_ratio):\n                    msg = (\"Length of `compound_ratio` must equal length of \"\n                           \"`compound`\")\n                    raise ValueError(msg)\n                prototype_mass = 0\n                for c, r in zip(compound, compound_ratio):\n                    prototype_mass += r * np.sum([a.mass for a in c.to_parmed().atoms])\n                # Conversion from kg/m^3 / amu * nm^3 to dimensionless units\n                n_prototypes = int(density/prototype_mass*(4/3*np.pi*radius**3)*.60224)\n                n_compounds = list()\n                for c in compound_ratio:\n                    n_compounds.append(int(n_prototypes * c))\n\n    # In angstroms for packmol.\n    sphere = np.multiply(sphere, 10)\n    radius *= 10\n    overlap *= 10\n\n    # Build the input file for each compound and call packmol.\n    filled_xyz = _new_xyz_file()\n\n    # List to hold file handles for the temporary compounds\n    compound_xyz_list = list()\n    try:\n        input_text = PACKMOL_HEADER.format(overlap, filled_xyz.name, seed)\n        for comp, m_compounds, rotate in zip(compound, n_compounds, fix_orientation):\n            m_compounds = int(m_compounds)\n\n            compound_xyz = _new_xyz_file()\n            compound_xyz_list.append(compound_xyz)\n\n            comp.save(compound_xyz.name, overwrite=True)\n            input_text += PACKMOL_SPHERE.format(compound_xyz.name, m_compounds,\n                                                sphere[0], sphere[1],\n                                                sphere[2], radius,\n                                                PACKMOL_CONSTRAIN if rotate else \"\")\n        print(input_text)\n        _run_packmol(input_text, filled_xyz, temp_file)\n\n        # Create the topology and update the coordinates.\n        filled = Compound()\n        filled = _create_topology(filled, compound, n_compounds)\n        filled.update_coordinates(filled_xyz.name)\n    finally:\n        for file_handle in compound_xyz_list:\n            file_handle.close()\n            os.unlink(file_handle.name)\n        filled_xyz.close()\n        os.unlink(filled_xyz.name)\n    return filled", "response": "Fill a single sphere with a compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef solvate(solute, solvent, n_solvent, box, overlap=0.2,\n            seed=12345, edge=0.2, fix_orientation=False, temp_file=None):\n    \"\"\"Solvate a compound in a box of solvent using packmol.\n\n    Parameters\n    ----------\n    solute : mb.Compound\n        Compound to be placed in a box and solvated.\n    solvent : mb.Compound\n        Compound to solvate the box.\n    n_solvent : int\n        Number of solvents to be put in box.\n    box : mb.Box\n        Box to be filled by compounds.\n    overlap : float, units nm, default=0.2\n        Minimum separation between atoms of different molecules.\n    seed : int, default=12345\n        Random seed to be passed to PACKMOL.\n    edge : float, units nm, default=0.2\n        Buffer at the edge of the box to not place molecules. This is necessary\n        in some systems because PACKMOL does not account for periodic boundary\n        conditions in its optimization.\n    fix_orientation : bool\n        Specify if solvent should not be rotated when filling box,\n        default=False.\n    temp_file : str, default=None\n        File name to write PACKMOL's raw output to.\n\n    Returns\n    -------\n    solvated : mb.Compound\n\n    \"\"\"\n    _check_packmol(PACKMOL)\n\n    box = _validate_box(box)\n    if not isinstance(solvent, (list, set)):\n        solvent = [solvent]\n    if not isinstance(n_solvent, (list, set)):\n        n_solvent = [n_solvent]\n    if not isinstance(fix_orientation, (list, set)):\n        fix_orientation = [fix_orientation] * len(solvent)\n\n    if len(solvent) != len(n_solvent):\n        msg = (\"`n_solvent` and `n_solvent` must be of equal length.\")\n        raise ValueError(msg)\n\n    # In angstroms for packmol.\n    box_mins = box.mins * 10\n    box_maxs = box.maxs * 10\n    overlap *= 10\n    center_solute = (box_maxs + box_mins) / 2\n\n    # Apply edge buffer\n    box_maxs -= edge * 10\n\n    # Build the input file for each compound and call packmol.\n    solvated_xyz = _new_xyz_file()\n    solute_xyz = _new_xyz_file()\n\n    # generate list of temp files for the solvents\n    solvent_xyz_list = list()\n    try:\n        solute.save(solute_xyz.name, overwrite=True)\n        input_text = (PACKMOL_HEADER.format(overlap, solvated_xyz.name, seed) +\n                      PACKMOL_SOLUTE.format(solute_xyz.name, *center_solute))\n\n        for solv, m_solvent, rotate in zip(solvent, n_solvent, fix_orientation):\n            m_solvent = int(m_solvent)\n\n            solvent_xyz = _new_xyz_file()\n            solvent_xyz_list.append(solvent_xyz)\n\n            solv.save(solvent_xyz.name, overwrite=True)\n            input_text += PACKMOL_BOX.format(solvent_xyz.name, m_solvent,\n                                             box_mins[0], box_mins[1],\n                                             box_mins[2], box_maxs[0],\n                                             box_maxs[1], box_maxs[2],\n                                             PACKMOL_CONSTRAIN if rotate else \"\")\n        _run_packmol(input_text, solvated_xyz, temp_file)\n\n        # Create the topology and update the coordinates.\n        solvated = Compound()\n        solvated.add(solute)\n        solvated = _create_topology(solvated, solvent, n_solvent)\n        solvated.update_coordinates(solvated_xyz.name)\n\n    finally:\n        for file_handle in solvent_xyz_list:\n            file_handle.close()\n            os.unlink(file_handle.name)\n        solvated_xyz.close()\n        solute_xyz.close()\n        os.unlink(solvated_xyz.name)\n        os.unlink(solute_xyz.name)\n    return solvated", "response": "Solvate a compound in a box of solvent using packmol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new mBuild compound with new coordinates.", "response": "def _create_topology(container, comp_to_add, n_compounds):\n    \"\"\"Return updated mBuild compound with new coordinates.\n\n    Parameters\n    ----------\n    container : mb.Compound, required\n        Compound containing the updated system generated by PACKMOL.\n    comp_to_add : mb.Compound or list of mb.Compounds, required\n        Compound(s) to add to the container.\n    container : int or list of int, required\n        Amount of comp_to_add to container.\n\n    Return\n    ------\n    container : mb.Compound\n        Compound with added compounds from PACKMOL.\n    \"\"\"\n\n    for comp, m_compound in zip(comp_to_add, n_compounds):\n            for _ in range(m_compound):\n                container.add(clone(comp))\n    return container"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a GSD file from a ParmEd structure to a file.", "response": "def write_gsd(structure, filename, ref_distance=1.0, ref_mass=1.0,\n              ref_energy=1.0, rigid_bodies=None, shift_coords=True,\n              write_special_pairs=True):\n    \"\"\"Output a GSD file (HOOMD v2 default data format).\n\n    Parameters\n    ----------\n    structure : parmed.Structure\n        ParmEd Structure object\n    filename : str\n        Path of the output file.\n    ref_distance : float, optional, default=1.0\n        Reference distance for conversion to reduced units\n    ref_mass : float, optional, default=1.0\n        Reference mass for conversion to reduced units\n    ref_energy : float, optional, default=1.0\n        Reference energy for conversion to reduced units\n    rigid_bodies : list of int, optional, default=None\n        List of rigid body information. An integer value is required for\n        each atom corresponding to the index of the rigid body the particle\n        is to be associated with. A value of None indicates the atom is not\n        part of a rigid body.\n    shift_coords : bool, optional, default=True\n        Shift coordinates from (0, L) to (-L/2, L/2) if necessary.\n    write_special_pairs : bool, optional, default=True\n        Writes out special pair information necessary to correctly use the OPLS fudged 1,4 interactions\n        in HOOMD.\n\n    Notes\n    -----\n    Force field parameters are not written to the GSD file and must be included\n    manually into a HOOMD input script. Work on a HOOMD plugin is underway to\n    read force field parameters from a Foyer XML file.\n\n    \"\"\"\n\n    import_('gsd')\n    import gsd.hoomd\n\n    xyz = np.array([[atom.xx, atom.xy, atom.xz] for atom in structure.atoms])\n    if shift_coords:\n        xyz = coord_shift(xyz, structure.box[:3])\n\n    gsd_file = gsd.hoomd.Snapshot()\n\n    gsd_file.configuration.step = 0\n    gsd_file.configuration.dimensions = 3\n\n    # Write box information\n    if np.allclose(structure.box[3:6], np.array([90, 90, 90])):\n        gsd_file.configuration.box = np.hstack((structure.box[:3] / ref_distance,\n                                                np.zeros(3)))\n    else:\n        a, b, c = structure.box[0:3] / ref_distance\n        alpha, beta, gamma = np.radians(structure.box[3:6])\n\n        lx = a\n        xy = b * np.cos(gamma)\n        xz = c * np.cos(beta)\n        ly = np.sqrt(b**2 - xy**2)\n        yz = (b*c*np.cos(alpha) - xy*xz) / ly\n        lz = np.sqrt(c**2 - xz**2 - yz**2)\n\n        gsd_file.configuration.box = np.array([lx, ly, lz, xy, xz, yz])\n\n    _write_particle_information(gsd_file, structure, xyz, ref_distance,\n            ref_mass, ref_energy, rigid_bodies)\n    if write_special_pairs:\n        _write_pair_information(gsd_file, structure)\n    if structure.bonds:\n        _write_bond_information(gsd_file, structure)\n    if structure.angles:\n        _write_angle_information(gsd_file, structure)\n    if structure.rb_torsions:\n        _write_dihedral_information(gsd_file, structure)\n\n\n    gsd.hoomd.create(filename, gsd_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_particle_information(gsd_file, structure, xyz, ref_distance,\n        ref_mass, ref_energy, rigid_bodies):\n    \"\"\"Write out the particle information.\n\n    \"\"\"\n\n    gsd_file.particles.N = len(structure.atoms)\n    gsd_file.particles.position = xyz / ref_distance\n\n    types = [atom.name if atom.type == '' else atom.type\n             for atom in structure.atoms]\n\n    unique_types = list(set(types))\n    unique_types.sort(key=natural_sort)\n    gsd_file.particles.types = unique_types\n\n    typeids = np.array([unique_types.index(t) for t in types])\n    gsd_file.particles.typeid = typeids\n\n    masses = np.array([atom.mass for atom in structure.atoms])\n    masses[masses==0] = 1.0\n    gsd_file.particles.mass = masses / ref_mass\n\n    charges = np.array([atom.charge for atom in structure.atoms])\n    e0 = 2.39725e-4\n    '''\n    Permittivity of free space = 2.39725e-4 e^2/((kcal/mol)(angstrom)),\n    where e is the elementary charge\n    '''\n    charge_factor = (4.0*np.pi*e0*ref_distance*ref_energy)**0.5\n    gsd_file.particles.charge = charges / charge_factor\n\n    if rigid_bodies:\n        rigid_bodies = [-1 if body is None else body for body in rigid_bodies]\n    gsd_file.particles.body = rigid_bodies", "response": "Write out the particle information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_pair_information(gsd_file, structure):\n    pair_types = []\n    pair_typeid = []\n    pairs = []\n    for ai in structure.atoms:\n        for aj in ai.dihedral_partners:\n            #make sure we don't double add\n            if ai.idx > aj.idx:\n                ps = '-'.join(sorted([ai.type, aj.type], key=natural_sort))\n                if ps not in pair_types:\n                    pair_types.append(ps)\n                pair_typeid.append(pair_types.index(ps))\n                pairs.append((ai.idx, aj.idx))\n    gsd_file.pairs.types = pair_types\n    gsd_file.pairs.typeid = pair_typeid\n    gsd_file.pairs.group = pairs\n    gsd_file.pairs.N = len(pairs)", "response": "Write the special pairs in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_bond_information(gsd_file, structure):\n\n    gsd_file.bonds.N = len(structure.bonds)\n\n    unique_bond_types = set()\n    for bond in structure.bonds:\n        t1, t2 = bond.atom1.type, bond.atom2.type\n        if t1 == '' or t2 == '':\n            t1, t2 = bond.atom1.name, bond.atom2.name\n        t1, t2 = sorted([t1, t2], key=natural_sort)\n        try:\n            bond_type = ('-'.join((t1, t2)))\n        except AttributeError: # no forcefield applied, bond.type is None\n            bond_type = ('-'.join((t1, t2)), 0.0, 0.0)\n        unique_bond_types.add(bond_type)\n    unique_bond_types = sorted(list(unique_bond_types), key=natural_sort)\n    gsd_file.bonds.types = unique_bond_types\n\n    bond_typeids = []\n    bond_groups = []\n    for bond in structure.bonds:\n        t1, t2 = bond.atom1.type, bond.atom2.type\n        if t1 == '' or t2 == '':\n            t1, t2 = bond.atom1.name, bond.atom2.name\n        t1, t2 = sorted([t1, t2], key=natural_sort)\n        try:\n            bond_type = ('-'.join((t1, t2)))\n        except AttributeError: # no forcefield applied, bond.type is None\n            bond_type = ('-'.join((t1, t2)), 0.0, 0.0)\n        bond_typeids.append(unique_bond_types.index(bond_type))\n        bond_groups.append((bond.atom1.idx, bond.atom2.idx))\n\n    gsd_file.bonds.typeid = bond_typeids\n    gsd_file.bonds.group = bond_groups", "response": "Write the bonds in the system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the angles in the system.", "response": "def _write_angle_information(gsd_file, structure):\n    \"\"\"Write the angles in the system.\n\n    Parameters\n    ----------\n    gsd_file :\n        The file object of the GSD file being written\n    structure : parmed.Structure\n        Parmed structure object holding system information\n\n    \"\"\"\n\n    gsd_file.angles.N = len(structure.angles)\n\n    unique_angle_types = set()\n    for angle in structure.angles:\n        t1, t2, t3 = angle.atom1.type, angle.atom2.type, angle.atom3.type\n        t1, t3 = sorted([t1, t3], key=natural_sort)\n        angle_type = ('-'.join((t1, t2, t3)))\n        unique_angle_types.add(angle_type)\n    unique_angle_types = sorted(list(unique_angle_types), key=natural_sort)\n    gsd_file.angles.types = unique_angle_types\n\n    angle_typeids = []\n    angle_groups = []\n    for angle in structure.angles:\n        t1, t2, t3 = angle.atom1.type, angle.atom2.type, angle.atom3.type\n        t1, t3 = sorted([t1, t3], key=natural_sort)\n        angle_type = ('-'.join((t1, t2, t3)))\n        angle_typeids.append(unique_angle_types.index(angle_type))\n        angle_groups.append((angle.atom1.idx, angle.atom2.idx,\n                             angle.atom3.idx))\n\n    gsd_file.angles.typeid = angle_typeids\n    gsd_file.angles.group = angle_groups"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_dihedral_information(gsd_file, structure):\n\n    gsd_file.dihedrals.N = len(structure.rb_torsions)\n\n    unique_dihedral_types = set()\n    for dihedral in structure.rb_torsions:\n        t1, t2 = dihedral.atom1.type, dihedral.atom2.type\n        t3, t4 = dihedral.atom3.type, dihedral.atom4.type\n        if [t2, t3] == sorted([t2, t3], key=natural_sort):\n            dihedral_type = ('-'.join((t1, t2, t3, t4)))\n        else:\n            dihedral_type = ('-'.join((t4, t3, t2, t1)))\n        unique_dihedral_types.add(dihedral_type)\n    unique_dihedral_types = sorted(list(unique_dihedral_types), key=natural_sort)\n    gsd_file.dihedrals.types = unique_dihedral_types\n\n    dihedral_typeids = []\n    dihedral_groups = []\n    for dihedral in structure.rb_torsions:\n        t1, t2 = dihedral.atom1.type, dihedral.atom2.type\n        t3, t4 = dihedral.atom3.type, dihedral.atom4.type\n        if [t2, t3] == sorted([t2, t3], key=natural_sort):\n            dihedral_type = ('-'.join((t1, t2, t3, t4)))\n        else:\n            dihedral_type = ('-'.join((t4, t3, t2, t1)))\n        dihedral_typeids.append(unique_dihedral_types.index(dihedral_type))\n        dihedral_groups.append((dihedral.atom1.idx, dihedral.atom2.idx,\n                                dihedral.atom3.idx, dihedral.atom4.idx))\n\n    gsd_file.dihedrals.typeid = dihedral_typeids\n    gsd_file.dihedrals.group = dihedral_groups", "response": "Write the dihedrals in the system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_(module):\n    try:\n        return importlib.import_module(module)\n    except ImportError as e:\n        try:\n            message = MESSAGES[module]\n        except KeyError:\n            message = 'The code at {filename}:{line_number} requires the ' + module + ' package'\n            e = ImportError('No module named %s' % module)\n\n        frame, filename, line_number, function_name, lines, index = \\\n            inspect.getouterframes(inspect.currentframe())[1]\n\n        m = message.format(filename=os.path.basename(filename), line_number=line_number)\n        m = textwrap.dedent(m)\n\n        bar = '\\033[91m' + '#' * max(len(line) for line in m.split(os.linesep)) + '\\033[0m'\n\n        print('', file=sys.stderr)\n        print(bar, file=sys.stderr)\n        print(m, file=sys.stderr)\n        print(bar, file=sys.stderr)\n        raise DelayImportError(m)", "response": "Imports a module and returns the object that represents it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the full path to one of the reference files shipped for utils.", "response": "def get_fn(name):\n    \"\"\"Get the full path to one of the reference files shipped for utils.\n\n    In the source distribution, these files are in ``mbuild/utils/reference``,\n    but on installation, they're moved to somewhere in the user's python\n    site-packages directory.\n\n    Parameters\n    ----------\n    name : str\n        Name of the file to load (with respect to the reference/ folder).\n\n    \"\"\"\n    fn = resource_filename('mbuild', os.path.join('utils', 'reference', name))\n    if not os.path.exists(fn):\n        raise IOError('Sorry! {} does not exists.'.format(fn))\n    return fn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new compound that is equivalent to the given from_positions and to_positions.", "response": "def force_overlap(move_this, from_positions, to_positions, add_bond=True):\n    \"\"\"Computes an affine transformation that maps the from_positions to the\n    respective to_positions, and applies this transformation to the compound.\n\n    Parameters\n    ----------\n    move_this : mb.Compound\n        The Compound to be moved.\n    from_positions : np.ndarray, shape=(n, 3), dtype=float\n        Original positions.\n    to_positions : np.ndarray, shape=(n, 3), dtype=float\n        New positions.\n    add_bond : bool, optional, default=True\n        If `from_positions` and `to_positions` are `Ports`, create a bond\n        between the two anchor atoms.\n\n    \"\"\"\n    from mbuild.port import Port\n    T = None\n    if isinstance(from_positions, (list, tuple)) and isinstance(to_positions, (list, tuple)):\n        equivalence_pairs = zip(from_positions, to_positions)\n    elif isinstance(from_positions, Port) and isinstance(to_positions, Port):\n        equivalence_pairs, T = _choose_correct_port(from_positions, to_positions)\n        from_positions.used = True\n        to_positions.used = True\n    else:\n        equivalence_pairs = [(from_positions, to_positions)]\n\n    if not T:\n        T = _create_equivalence_transform(equivalence_pairs)\n    atom_positions = move_this.xyz_with_ports\n    atom_positions = T.apply_to(atom_positions)\n    move_this.xyz_with_ports = atom_positions\n\n    if add_bond:\n        if isinstance(from_positions, Port) and isinstance(to_positions, Port):\n            if not from_positions.anchor or not to_positions.anchor:\n                warn(\"Attempting to form bond from port that has no anchor\")\n            else:\n                from_positions.anchor.parent.add_bond((from_positions.anchor, to_positions.anchor))\n                to_positions.anchor.parent.add_bond((from_positions.anchor, to_positions.anchor))\n                from_positions.anchor.parent.remove(from_positions)\n                to_positions.anchor.parent.remove(to_positions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the angle in radians between two vectors.", "response": "def angle(u, v, w=None):\n    \"\"\"Returns the angle in radians between two vectors. \"\"\"\n    if w is not None:\n        u = u - v\n        v = w - v\n    c = np.dot(u, v) / norm(u) / norm(v)\n    return np.arccos(np.clip(c, -1, 1))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute an equivalence transformation that transforms this compound to another compound s coordinate system.", "response": "def _create_equivalence_transform(equiv):\n    \"\"\"Compute an equivalence transformation that transforms this compound\n    to another compound's coordinate system.\n\n    Parameters\n    ----------\n    equiv : np.ndarray, shape=(n, 3), dtype=float\n        Array of equivalent points.\n\n    Returns\n    -------\n    T : CoordinateTransform\n        Transform that maps this point cloud to the other point cloud's\n        coordinates system.\n\n    \"\"\"\n    from mbuild.compound import Compound\n    self_points = np.array([])\n    self_points.shape = (0, 3)\n    other_points = np.array([])\n    other_points.shape = (0, 3)\n\n    for pair in equiv:\n        if not isinstance(pair, tuple) or len(pair) != 2:\n            raise ValueError('Equivalence pair not a 2-tuple')\n        if not (isinstance(pair[0], Compound) and isinstance(pair[1], Compound)):\n            raise ValueError('Equivalence pair type mismatch: pair[0] is a {0} '\n                             'and pair[1] is a {1}'.format(type(pair[0]),\n                                                           type(pair[1])))\n\n        # TODO: vstack is slow, replace with list concatenation\n        if not pair[0].children:\n            self_points = np.vstack([self_points, pair[0].pos])\n            other_points = np.vstack([other_points, pair[1].pos])\n        else:\n            for atom0 in pair[0]._particles(include_ports=True):\n                self_points = np.vstack([self_points, atom0.pos])\n            for atom1 in pair[1]._particles(include_ports=True):\n                other_points = np.vstack([other_points, atom1.pos])\n    T = RigidTransform(self_points, other_points)\n    return T"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef equivalence_transform(compound, from_positions, to_positions, add_bond=True):\n    warn('The `equivalence_transform` function is being phased out in favor of'\n         ' `force_overlap`.', DeprecationWarning)\n    from mbuild.port import Port\n    T = None\n    if isinstance(from_positions, (list, tuple)) and isinstance(to_positions, (list, tuple)):\n        equivalence_pairs = zip(from_positions, to_positions)\n    elif isinstance(from_positions, Port) and isinstance(to_positions, Port):\n        equivalence_pairs, T = _choose_correct_port(from_positions, to_positions)\n        from_positions.used = True\n        to_positions.used = True\n    else:\n        equivalence_pairs = [(from_positions, to_positions)]\n\n    if not T:\n        T = _create_equivalence_transform(equivalence_pairs)\n    atom_positions = compound.xyz_with_ports\n    atom_positions = T.apply_to(atom_positions)\n    compound.xyz_with_ports = atom_positions\n\n    if add_bond:\n        if isinstance(from_positions, Port) and isinstance(to_positions, Port):\n            if not from_positions.anchor or not to_positions.anchor:\n                # TODO: I think warnings is undefined here\n                warn(\"Attempting to form bond from port that has no anchor\")\n            else:\n                from_positions.anchor.parent.add_bond((from_positions.anchor, to_positions.anchor))\n                to_positions.anchor.parent.add_bond((from_positions.anchor, to_positions.anchor))", "response": "This function computes an affine transformation that maps the from_positions to the\n    respective to_positions and applies this transformation to the compound."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect the correct port when using an equivalence transform on two Ports.", "response": "def _choose_correct_port(from_port, to_port):\n    \"\"\"Chooses the direction when using an equivalence transform on two Ports.\n\n    Each Port object actually contains 2 sets of 4 atoms, either of which can be\n    used to make a connection with an equivalence transform. This function\n    chooses the set of 4 atoms that makes the anchor atoms not overlap which is\n    the intended behavior for most use-cases.\n\n    TODO: -Increase robustness for cases where the anchors are a different\n           distance from their respective ports.\n          -Provide options in `force_overlap` to override this behavior.\n\n    Parameters\n    ----------\n    from_port : mb.Port\n    to_port : mb.Port\n\n    Returns\n    -------\n    equivalence_pairs : tuple of Ports, shape=(2,)\n        Technically, a tuple of the Ports' sub-Compounds ('up' or 'down')\n        that are used to make the correct connection between components.\n\n    \"\"\"\n    # First we try matching the two 'up' ports.\n    T1 = _create_equivalence_transform([(from_port['up'], to_port['up'])])\n    new_position = T1.apply_to(np.array(from_port.anchor.pos, ndmin=2))\n\n    dist_between_anchors_up_up = norm(new_position[0] - to_port.anchor.pos)\n\n    # Then matching a 'down' with an 'up' port.\n    T2 = _create_equivalence_transform([(from_port['down'], to_port['up'])])\n    new_position = T2.apply_to(np.array(from_port.anchor.pos, ndmin=2))\n\n    # Determine which transform places the anchors further away from each other.\n    dist_between_anchors_down_up = norm(new_position[0] - to_port.anchor.pos)\n    difference_between_distances = dist_between_anchors_down_up - dist_between_anchors_up_up\n\n    if difference_between_distances > 0:\n        correct_port = from_port['down']\n        T = T2\n    else:\n        correct_port = from_port['up']\n        T = T1\n    return [(correct_port, to_port['up'])], T"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef translate(compound, pos):\n    atom_positions = compound.xyz_with_ports\n    atom_positions = Translation(pos).apply_to(atom_positions)\n    compound.xyz_with_ports = atom_positions", "response": "Translate a compound by a vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntranslate a compound to a coordinate.", "response": "def translate_to(compound, pos):\n    \"\"\"Translate a compound to a coordinate.\n\n    Parameters\n    ----------\n    compound : mb.Compound\n        The compound being translated.\n    pos : np.ndarray, shape=(3,), dtype=float\n        The coordinate to translate the compound to.\n\n    \"\"\"\n    atom_positions = compound.xyz_with_ports\n    atom_positions -= compound.center\n    atom_positions = Translation(pos).apply_to(atom_positions)\n    compound.xyz_with_ports = atom_positions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _translate_to(coordinates, to):\n    coordinates -= np.mean(coordinates, axis=0)\n    return Translation(to).apply_to(coordinates)", "response": "Translate a set of coordinates to a location."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _rotate(coordinates, theta, around):\n    around = np.asarray(around).reshape(3)\n    if np.array_equal(around, np.zeros(3)):\n        raise ValueError('Cannot rotate around a zero vector')\n    return Rotation(theta, around).apply_to(coordinates)", "response": "Rotate a set of coordinates around an arbitrary vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotate(compound, theta, around):\n    around = np.asarray(around).reshape(3)\n    if np.array_equal(around, np.zeros(3)):\n        raise ValueError('Cannot rotate around a zero vector')\n    atom_positions = compound.xyz_with_ports\n    atom_positions = Rotation(theta, around).apply_to(atom_positions)\n    compound.xyz_with_ports = atom_positions", "response": "Rotates a compound around an arbitrary vector."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spin(compound, theta, around):\n    around = np.asarray(around).reshape(3)\n    if np.array_equal(around, np.zeros(3)):\n        raise ValueError('Cannot spin around a zero vector')\n    center_pos = compound.center\n    translate(compound, -center_pos)\n    rotate(compound, theta, around)\n    translate(compound, center_pos)", "response": "Rotate a compound in place around an arbitrary vector."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrotate a set of coordinates in place around an arbitrary vector.", "response": "def _spin(coordinates, theta, around):\n    \"\"\"Rotate a set of coordinates in place around an arbitrary vector.\n\n    Parameters\n    ----------\n    coordinates : np.ndarray, shape=(n,3), dtype=float\n        The coordinates being spun.\n    theta : float\n        The angle by which to spin the coordinates, in radians.\n    around : np.ndarray, shape=(3,), dtype=float\n        The axis about which to spin the coordinates.\n\n    \"\"\"\n    around = np.asarray(around).reshape(3)\n    if np.array_equal(around, np.zeros(3)):\n        raise ValueError('Cannot spin around a zero vector')\n    center_pos = np.mean(coordinates, axis=0)\n    coordinates -= center_pos\n    coordinates = _rotate(coordinates, theta, around)\n    coordinates += center_pos\n    return coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef x_axis_transform(compound, new_origin=None,\n                     point_on_x_axis=None,\n                     point_on_xy_plane=None):\n    \"\"\"Move a compound such that the x-axis lies on specified points.\n\n    Parameters\n    ----------\n    compound : mb.Compound\n        The compound to move.\n    new_origin : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 0.0]\n        Where to place the new origin of the coordinate system.\n    point_on_x_axis : mb.Compound or list-like of size 3, optional, default=[1.0, 0.0, 0.0]\n        A point on the new x-axis.\n    point_on_xy_plane : mb.Compound, or list-like of size 3, optional, default=[1.0, 0.0, 0.0]\n        A point on the new xy-plane.\n\n    \"\"\"\n    import mbuild as mb\n\n    if new_origin is None:\n        new_origin = np.array([0, 0, 0])\n    elif isinstance(new_origin, mb.Compound):\n        new_origin = new_origin.pos\n    elif isinstance(new_origin, (tuple, list,np.ndarray)):\n        new_origin = np.asarray(new_origin)\n    else:\n        raise TypeError('x_axis_transform, y_axis_transform, and z_axis_transform only accept'\n                        ' mb.Compounds, list-like of length 3 or None for the new_origin'\n                        ' parameter. User passed type: {}.'.format(type(new_origin)))\n    if point_on_x_axis is None:\n        point_on_x_axis = np.array([1.0, 0.0, 0.0])\n    elif isinstance(point_on_x_axis, mb.Compound):\n        point_on_x_axis = point_on_x_axis.pos\n    elif isinstance(point_on_x_axis, (list, tuple, np.ndarray)):\n        point_on_x_axis = np.asarray(point_on_x_axis)\n    else:\n        raise TypeError('x_axis_transform, y_axis_transform, and z_axis_transform only accept'\n                        ' mb.Compounds, list-like of size 3, or None for the point_on_x_axis'\n                        ' parameter. User passed type: {}.'.format(type(point_on_x_axis)))    \n    if point_on_xy_plane is None:\n        point_on_xy_plane = np.array([1.0, 1.0, 0.0])\n    elif isinstance(point_on_xy_plane, mb.Compound):\n        point_on_xy_plane = point_on_xy_plane.pos\n    elif isinstance(point_on_xy_plane, (list, tuple, np.ndarray)):\n        point_on_xy_plane = np.asarray(point_on_xy_plane)\n    else:\n        raise TypeError('x_axis_transform, y_axis_transform, and z_axis_transform only accept'\n                        ' mb.Compounds, list-like of size 3, or None for the point_on_xy_plane'\n                        ' parameter. User passed type: {}.'.format(type(point_on_xy_plane)))\n\n    atom_positions = compound.xyz_with_ports\n    transform = AxisTransform(new_origin=new_origin,\n                              point_on_x_axis=point_on_x_axis,\n                              point_on_xy_plane=point_on_xy_plane)\n    atom_positions = transform.apply_to(atom_positions)\n    compound.xyz_with_ports = atom_positions", "response": "Move a compound such that the x - axis lies on specified points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove a compound such that the y - axis lies on specified points.", "response": "def y_axis_transform(compound, new_origin=None,\n                     point_on_y_axis=None,\n                     point_on_xy_plane=None):\n    \"\"\"Move a compound such that the y-axis lies on specified points.\n\n    Parameters\n    ----------\n    compound : mb.Compound\n        The compound to move.\n    new_origin : mb.Compound or like-like of size 3, optional, default=[0.0, 0.0, 0.0]\n        Where to place the new origin of the coordinate system.\n    point_on_y_axis : mb.Compound or list-like of size 3, optional, default=[0.0, 1.0, 0.0]\n        A point on the new y-axis.\n    point_on_xy_plane : mb.Compound or list-like of size 3, optional, default=[0.0, 1.0, 0.0]\n        A point on the new xy-plane.\n\n    \"\"\"\n    x_axis_transform(compound, new_origin=new_origin,\n                     point_on_x_axis=point_on_y_axis,\n                     point_on_xy_plane=point_on_xy_plane)\n    rotate_around_z(compound, np.pi / 2)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef z_axis_transform(compound, new_origin=None,\n                     point_on_z_axis=None,\n                     point_on_zx_plane=None):\n    \"\"\"Move a compound such that the z-axis lies on specified points.\n\n    Parameters\n    ----------\n    compound : mb.Compound\n        The compound to move.\n    new_origin : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 0.0]\n        Where to place the new origin of the coordinate system.\n    point_on_z_axis : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 1.0]\n        A point on the new z-axis.\n    point_on_zx_plane : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 1.0]\n        A point on the new xz-plane.\n\n    \"\"\"\n    x_axis_transform(compound, new_origin=new_origin,\n                     point_on_x_axis=point_on_z_axis,\n                     point_on_xy_plane=point_on_zx_plane)\n    rotate_around_y(compound, np.pi * 3 / 2)", "response": "Move a compound such that the z - axis lies on specified points."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_to(self, A):\n        if A.ndim == 1:\n            A = np.expand_dims(A, axis=0)\n        rows, cols = A.shape\n        A_new = np.hstack([A, np.ones((rows, 1))])\n\n        A_new = np.transpose(self.T.dot(np.transpose(A_new)))\n        return A_new[:, 0:cols]", "response": "Apply the coordinate transformation to points in A."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a LAMMPS data file in the full atom style format.", "response": "def write_lammpsdata(structure, filename, atom_style='full'):\n    \"\"\"Output a LAMMPS data file.\n    \n    Outputs a LAMMPS data file in the 'full' atom style format. Assumes use\n    of 'real' units. See http://lammps.sandia.gov/doc/atom_style.html for\n    more information on atom styles.\n\n    Parameters\n    ----------\n    structure : parmed.Structure\n        ParmEd structure object\n    filename : str\n        Path of the output file\n    atom_style: str\n        Defines the style of atoms to be saved in a LAMMPS data file. The following atom\n        styles are currently supported: 'full', 'atomic', 'charge', 'molecular'\n        see http://lammps.sandia.gov/doc/atom_style.html for more\n        information on atom styles.\n\n    Notes\n    -----\n    See http://lammps.sandia.gov/doc/2001/data_format.html for a full description\n    of the LAMMPS data format. Currently the following sections are supported (in\n    addition to the header): *Masses*, *Nonbond Coeffs*, *Bond Coeffs*, *Angle\n    Coeffs*, *Dihedral Coeffs*, *Atoms*, *Bonds*, *Angles*, *Dihedrals*\n\n    Some of this function has beed adopted from `mdtraj`'s support of the LAMMPSTRJ\n    trajectory format. See https://github.com/mdtraj/mdtraj/blob/master/mdtraj/formats/lammpstrj.py for details.\n\n    \"\"\"\n\n    if atom_style not in ['atomic', 'charge', 'molecular', 'full']:\n        raise ValueError('Atom style \"{}\" is invalid or is not currently supported'.format(atom_style))\n\n    xyz = np.array([[atom.xx,atom.xy,atom.xz] for atom in structure.atoms])\n\n    forcefield = True\n    if structure[0].type == '':\n        forcefield = False\n\n    # Internally use nm\n    box = Box(lengths=np.array([0.1 * val for val in structure.box[0:3]]),\n              angles=structure.box[3:6])\n\n    if forcefield:\n        types = [atom.type for atom in structure.atoms]\n    else:\n        types = [atom.name for atom in structure.atoms]\n\n    unique_types = list(set(types))\n    unique_types.sort(key=natural_sort)\n\n    charges = [atom.charge for atom in structure.atoms]\n\n    bonds = [[bond.atom1.idx+1, bond.atom2.idx+1] for bond in structure.bonds]\n    angles = [[angle.atom1.idx+1,\n               angle.atom2.idx+1,\n               angle.atom3.idx+1] for angle in structure.angles]\n    dihedrals = [[dihedral.atom1.idx+1,\n                  dihedral.atom2.idx+1,\n                  dihedral.atom3.idx+1,\n                  dihedral.atom4.idx+1] for dihedral in structure.rb_torsions]\n\n    if bonds:\n        if len(structure.bond_types) == 0:\n            bond_types = np.ones(len(bonds),dtype=int)\n        else:\n            unique_bond_types = dict(enumerate(set([(round(bond.type.k,3),\n                                                     round(bond.type.req,3)) for bond in structure.bonds])))\n            unique_bond_types = OrderedDict([(y,x+1) for x,y in unique_bond_types.items()])\n            bond_types = [unique_bond_types[(round(bond.type.k,3),\n                                             round(bond.type.req,3))] for bond in structure.bonds]\n\n    if angles:\n        unique_angle_types = dict(enumerate(set([(round(angle.type.k,3),\n                                                  round(angle.type.theteq,3)) for angle in structure.angles])))\n        unique_angle_types = OrderedDict([(y,x+1) for x,y in unique_angle_types.items()])\n        angle_types = [unique_angle_types[(round(angle.type.k,3),\n                                           round(angle.type.theteq,3))] for angle in structure.angles]\n\n    if dihedrals:\n        unique_dihedral_types = dict(enumerate(set([(round(dihedral.type.c0,3),\n                                                     round(dihedral.type.c1,3),\n                                                     round(dihedral.type.c2,3),\n                                                     round(dihedral.type.c3,3),\n                                                     round(dihedral.type.c4,3),\n                                                     round(dihedral.type.c5,3),\n                                                     round(dihedral.type.scee,1),\n                                                     round(dihedral.type.scnb,1)) for dihedral in structure.rb_torsions])))\n        unique_dihedral_types = OrderedDict([(y,x+1) for x,y in unique_dihedral_types.items()])\n        dihedral_types = [unique_dihedral_types[(round(dihedral.type.c0,3),\n                                                 round(dihedral.type.c1,3),\n                                                 round(dihedral.type.c2,3),\n                                                 round(dihedral.type.c3,3),\n                                                 round(dihedral.type.c4,3),\n                                                 round(dihedral.type.c5,3),\n                                                 round(dihedral.type.scee,1),\n                                                 round(dihedral.type.scnb,1))] for dihedral in structure.rb_torsions]\n\n    with open(filename, 'w') as data:\n        data.write(filename+' - created by mBuild\\n\\n')\n        data.write('{:d} atoms\\n'.format(len(structure.atoms)))\n        if atom_style in ['full', 'molecular']:\n            data.write('{:d} bonds\\n'.format(len(bonds)))\n            data.write('{:d} angles\\n'.format(len(angles)))\n            data.write('{:d} dihedrals\\n\\n'.format(len(dihedrals)))\n\n        data.write('{:d} atom types\\n'.format(len(set(types))))\n        if atom_style in ['full', 'molecular']:\n            if bonds:\n                data.write('{:d} bond types\\n'.format(len(set(bond_types))))\n            if angles:\n                data.write('{:d} angle types\\n'.format(len(set(angle_types))))\n            if dihedrals:\n                data.write('{:d} dihedral types\\n'.format(len(set(dihedral_types))))\n\n        data.write('\\n')\n        # Box data\n        if np.allclose(box.angles, np.array([90, 90, 90])):\n            for i,dim in enumerate(['x','y','z']):\n                data.write('{0:.6f} {1:.6f} {2}lo {2}hi\\n'.format(\n                    10.0 * box.mins[i],\n                    10.0 * box.maxs[i],\n                    dim))\n        else:\n            a, b, c = 10.0 * box.lengths\n            alpha, beta, gamma = np.radians(box.angles)\n\n            lx = a\n            xy = b * np.cos(gamma)\n            xz = c * np.cos(beta)\n            ly = np.sqrt(b**2 - xy**2)\n            yz = (b*c*np.cos(alpha) - xy*xz) / ly\n            lz = np.sqrt(c**2 - xz**2 - yz**2)\n\n            xlo, ylo, zlo = 10.0 * box.mins\n            xhi = xlo + lx\n            yhi = ylo + ly\n            zhi = zlo + lz\n\n            xlo_bound = xlo + np.min([0.0, xy, xz, xy+xz])\n            xhi_bound = xhi + np.max([0.0, xy, xz, xy+xz])\n            ylo_bound = ylo + np.min([0.0, yz])\n            yhi_bound = yhi + np.max([0.0, yz])\n            zlo_bound = zlo\n            zhi_bound = zhi\n\n            data.write('{0:.6f} {1:.6f} xlo xhi\\n'.format(\n                xlo_bound, xhi_bound))\n            data.write('{0:.6f} {1:.6f} ylo yhi\\n'.format(\n                ylo_bound, yhi_bound))\n            data.write('{0:.6f} {1:.6f} zlo zhi\\n'.format(\n                zlo_bound, zhi_bound))\n            data.write('{0:.6f} {1:.6f} {2:6f} xy xz yz\\n'.format(\n                xy, xz, yz))\n\n        # Mass data\n        masses = [atom.mass for atom in structure.atoms]\n        mass_dict = dict([(unique_types.index(atom_type)+1,mass) for atom_type,mass in zip(types,masses)])\n\n        data.write('\\nMasses\\n\\n')\n        for atom_type,mass in mass_dict.items():\n            data.write('{:d}\\t{:.6f}\\t# {}\\n'.format(atom_type,mass,unique_types[atom_type-1]))\n\n        if forcefield:\n            # Pair coefficients\n            epsilons = [atom.epsilon for atom in structure.atoms]\n            sigmas = [atom.sigma for atom in structure.atoms]\n            epsilon_dict = dict([(unique_types.index(atom_type)+1,epsilon) for atom_type,epsilon in zip(types,epsilons)])\n            sigma_dict = dict([(unique_types.index(atom_type)+1,sigma) for atom_type,sigma in zip(types,sigmas)])\n            data.write('\\nPair Coeffs # lj\\n\\n')\n            for idx,epsilon in epsilon_dict.items():\n                data.write('{}\\t{:.5f}\\t{:.5f}\\n'.format(idx,epsilon,sigma_dict[idx]))\n\n            # Bond coefficients\n            if bonds:\n                data.write('\\nBond Coeffs # harmonic\\n\\n')\n                for params,idx in unique_bond_types.items():\n                    data.write('{}\\t{}\\t{}\\n'.format(idx,*params))\n\n            # Angle coefficients\n            if angles:\n                data.write('\\nAngle Coeffs # harmonic\\n\\n')\n                for params,idx in unique_angle_types.items():\n                    data.write('{}\\t{}\\t{:.5f}\\n'.format(idx,*params))\n\n            # Dihedral coefficients\n            if dihedrals:\n                data.write('\\nDihedral Coeffs # opls\\n\\n')\n                for params,idx in unique_dihedral_types.items():\n                    opls_coeffs = RB_to_OPLS(params[0],\n                                             params[1],\n                                             params[2],\n                                             params[3],\n                                             params[4],\n                                             params[5])\n                    data.write('{}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\n'.format(idx,*opls_coeffs))\n\n        # Atom data\n        data.write('\\nAtoms\\n\\n')\n        if atom_style == 'atomic':\n            atom_line = '{index:d}\\t{type_index:d}\\t{x:.6f}\\t{y:.6f}\\t{z:.6f}\\n'\n        elif atom_style == 'charge':\n            atom_line = '{index:d}\\t{type_index:d}\\t{charge:.6f}\\t{x:.6f}\\t{y:.6f}\\t{z:.6f}\\n'\n        elif atom_style == 'molecular':\n            atom_line = '{index:d}\\t{zero:d}\\t{type_index:d}\\t{x:.6f}\\t{y:.6f}\\t{z:.6f}\\n'\n        elif atom_style == 'full':\n            atom_line ='{index:d}\\t{zero:d}\\t{type_index:d}\\t{charge:.6f}\\t{x:.6f}\\t{y:.6f}\\t{z:.6f}\\n'\n\n        for i,coords in enumerate(xyz):\n            data.write(atom_line.format(\n                index=i+1,type_index=unique_types.index(types[i])+1,\n                zero=0,charge=charges[i],\n                x=coords[0],y=coords[1],z=coords[2]))\n\n        if atom_style in ['full', 'molecular']:\n            # Bond data\n            if bonds:\n                data.write('\\nBonds\\n\\n')\n                for i,bond in enumerate(bonds):\n                    data.write('{:d}\\t{:d}\\t{:d}\\t{:d}\\n'.format(\n                        i+1,bond_types[i],bond[0],bond[1]))\n\n            # Angle data\n            if angles:\n                data.write('\\nAngles\\n\\n')\n                for i,angle in enumerate(angles):\n                    data.write('{:d}\\t{:d}\\t{:d}\\t{:d}\\t{:d}\\n'.format(\n                        i+1,angle_types[i],angle[0],angle[1],angle[2]))\n\n            # Dihedral data\n            if dihedrals:\n                data.write('\\nDihedrals\\n\\n')\n                for i,dihedral in enumerate(dihedrals):\n                    data.write('{:d}\\t{:d}\\t{:d}\\t{:d}\\t{:d}\\t{:d}\\n'.format(\n                        i+1,dihedral_types[i],dihedral[0],\n                        dihedral[1],dihedral[2],dihedral[3]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new tile with a label indicating its tiling position.", "response": "def _add_tile(self, new_tile, ijk):\n        \"\"\"Add a tile with a label indicating its tiling position. \"\"\"\n        tile_label = \"{0}_{1}\".format(self.name, '-'.join(str(d) for d in ijk))\n        self.add(new_tile, label=tile_label, inherit_periodicity=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding labels for all the ports to the parent.", "response": "def _hoist_ports(self, new_tile):\n        \"\"\"Add labels for all the ports to the parent (TiledCompound). \"\"\"\n        for port in new_tile.children:\n            if isinstance(port, Port):\n                self.add(port, containment=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_particle_image(self, query, match, all_particles):\n        _, idxs = self.particle_kdtree.query(query.pos, k=10)\n\n        neighbors = all_particles[idxs]\n\n        for particle in neighbors:\n            if particle.index == match.index:\n                return particle\n        raise MBuildError('Unable to find matching particle image while'\n                          ' stitching bonds.')", "response": "Find the particle image with the same index as match in a neighboring tile."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts Ryckaert - Bellemans type dihedrals to OPLS type.", "response": "def RB_to_OPLS(c0, c1, c2, c3, c4, c5):\n    \"\"\"Converts Ryckaert-Bellemans type dihedrals to OPLS type.\n\n    Parameters\n    ----------\n    c0, c1, c2, c3, c4, c5 : Ryckaert-Belleman coefficients (in kcal/mol)\n\n    Returns\n    -------\n    opls_coeffs : np.array, shape=(4,)\n        Array containing the OPLS dihedrals coeffs f1, f2, f3, and f4\n        (in kcal/mol)\n\n    \"\"\"\n\n    f1 = (-1.5 * c3) - (2 * c1)\n    f2 = c0 + c1 + c3\n    f3 = -0.5 * c3\n    f4 = -0.25 * c4\n    return np.array([f1, f2, f3, f4])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a HOOMD XML file for a given structure.", "response": "def write_hoomdxml(structure, filename, ref_distance=1.0, ref_mass=1.0,\n                   ref_energy=1.0, rigid_bodies=None, shift_coords=True,\n                   auto_scale=False):\n    \"\"\"Output a HOOMD XML file.\n\n    Parameters\n    ----------\n    structure : parmed.Structure\n        ParmEd structure object\n    filename : str\n        Path of the output file.\n    ref_distance : float, optional, default=1.0, units=nanometers\n        Reference distance for conversion to reduced units\n    ref_mass : float, optional, default=1.0, units=amu\n        Reference mass for conversion to reduced units\n    ref_energy : float, optional, default=1.0, units=kJ/mol\n        Reference energy for conversion to reduced units\n    rigid_bodies : list\n        List of rigid body information. An integer value is required\n        for each particle corresponding to the number of the rigid body with\n        which the particle should be included. A value of None indicates the\n        particle is not part of any rigid body.\n    shift_coords : bool, optional, default=True\n        Shift coordinates from (0, L) to (-L/2, L/2) if necessary.\n    auto_scale : bool, optional, default=False\n        Automatically use largest sigma value as ref_distance, largest mass value\n        as ref_mass and largest epsilon value as ref_energy.\n\n    Returns\n    -------\n    ReferenceValues : namedtuple\n        Values used in scaling\n\n    Example\n    -------\n    ref_values = ethane.save(filename='ethane-opls.hoomdxml', forcefield_name='oplsaa', auto_scale=True)\n    print(ref_values.mass, ref_values.distance, ref_values.energy)\n\n\n    Notes\n    -----\n    The following elements are always written:\n\n    * **position** : particle positions\n    * **type** : particle types\n    * **mass** : particle masses (default 1.0)\n    * **charge** : particle charges\n\n    The following elements may be written if applicable:\n\n    * **pair_coeffs** : Pair coefficients for each particle type (assumes a 12-6 LJ pair style). The following information is written for each particle type:\n\n                        * type : particle type\n                        * epsilon : LJ epsilon\n                        * sigma : LJ sigma\n\n    * **bond_coeffs** : Coefficients for each bond type (assumes a harmonic bond style). The following information is written for each bond type:\n\n                        * type : bond type\n                        * k : force constant (units of energy/distance^2)\n                        * r0 : bond rest length (units of distance)\n\n    * **bond** : system bonds\n    * **angle_coeffs** : Coefficients for each angle type (assumes a harmonic angle style). The following information is written for each angle type:\n\n                         * type : angle type\n                         * k : force constant (units of energy/radians^2)\n                         * theta : rest angle (units of radians)\n\n    * **angle** : system angles\n    * **dihedral_coeffs** : Coefficients for each dihedral type (assumes an OPLS dihedral style). The following information is written for each dihedral type:\n\n                            * type : dihedral type\n                            * k1, k2, k3, k4 : force coefficients (units of energy)\n\n    * **dihedral** : system dihedrals\n\n    * **body** : ID of the rigid body to which each particle belongs\n\n    \"\"\"\n    ref_distance *= 10  # Parmed unit hack\n    ref_energy /= 4.184  # Parmed unit hack\n    forcefield = True\n    if structure[0].type == '':\n        forcefield = False\n    if auto_scale and forcefield:\n        ref_mass = max([atom.mass for atom in structure.atoms])\n        pair_coeffs = list(set((atom.type,\n                                atom.epsilon,\n                                atom.sigma) for atom in structure.atoms))\n        ref_energy = max(pair_coeffs, key=operator.itemgetter(1))[1]\n        ref_distance = max(pair_coeffs, key=operator.itemgetter(2))[2]\n\n    xyz = np.array([[atom.xx, atom.xy, atom.xz] for atom in structure.atoms])\n    if shift_coords:\n        xyz = coord_shift(xyz, structure.box[:3])\n\n    with open(filename, 'w') as xml_file:\n        xml_file.write('<?xml version=\"1.2\" encoding=\"UTF-8\"?>\\n')\n        xml_file.write('<hoomd_xml version=\"1.2\">\\n')\n        xml_file.write('<!-- ref_distance (nm) ref_mass (amu) ref_energy (kJ/mol) -->\\n')\n        xml_file.write('<!-- {} {} {} -->\\n'.format(ref_distance, ref_mass, ref_energy))\n        xml_file.write('<configuration time_step=\"0\">\\n')\n        _write_box_information(xml_file, structure, ref_distance)\n        _write_particle_information(xml_file, structure, xyz, forcefield,\n                ref_distance, ref_mass, ref_energy)\n        _write_bond_information(xml_file, structure, ref_distance, ref_energy)\n        _write_angle_information(xml_file, structure, ref_energy)\n        _write_dihedral_information(xml_file, structure, ref_energy)\n        _write_rigid_information(xml_file, rigid_bodies)\n        xml_file.write('</configuration>\\n')\n        xml_file.write('</hoomd_xml>')\n\n    ReferenceValues = namedtuple(\"ref_values\", [\"distance\", \"mass\", \"energy\"])\n\n    return ReferenceValues(ref_distance, ref_mass, ref_energy)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_particle_information(xml_file, structure, xyz, forcefield,\n        ref_distance, ref_mass, ref_energy):\n    \"\"\"Write out the particle information.\n\n    Parameters\n    ----------\n    xml_file : file object\n        The file object of the hoomdxml file being written\n    structure : parmed.Structure\n        Parmed structure object\n    xyz : np.ndarray, shape=(n,3), dtype=float\n        The particle positions to be written.\n    forcefield : bool\n        If True, write the particle \"type\". Write the particle \"name\" otherwise.\n    ref_distance : float, default=1.0\n        Reference distance for conversion to reduced units\n    ref_mass : float, default=1.0\n        Reference mass for conversion to reduced units\n    ref_energy : float, default=1.0\n        Reference energy for conversion to reduced units\n\n    \"\"\"\n\n    xml_file.write('<position units=\"sigma\" num=\"{}\">\\n'.format(xyz.shape[0]))\n    for pos in xyz:\n        xml_file.write('{}\\t{}\\t{}\\n'.format(*pos/ref_distance))\n    xml_file.write('</position>\\n')\n    if forcefield:\n        types = [atom.type for atom in structure.atoms]\n    else:\n        types = [atom.name for atom in structure.atoms]\n\n    xml_file.write('<type>\\n')\n    for atom_type in types:\n        xml_file.write('{}\\n'.format(atom_type))\n    xml_file.write('</type>\\n')\n\n    masses = [atom.mass for atom in structure.atoms]\n    xml_file.write('<mass>\\n')\n    for mass in masses:\n        if mass == 0:\n            mass = 1.0\n        xml_file.write('{}\\n'.format(mass/ref_mass))\n    xml_file.write('</mass>\\n')\n\n    charges = [atom.charge for atom in structure.atoms]\n    xml_file.write('<charge>\\n')\n    e0 = 5.72956500956023e-4 # e^2-mol/kJ-nm, permittivity of free space\n    charge_factor = (4.0 * np.pi * e0 * ref_distance * ref_energy)**0.5\n    for charge in charges:\n        xml_file.write('{}\\n'.format(charge/charge_factor))\n    xml_file.write('</charge>\\n')\n    if forcefield:\n        pair_coeffs = list(set((atom.type,\n                                atom.epsilon,\n                                atom.sigma) for atom in structure.atoms))\n        pair_coeffs.sort(key=lambda pair_type: pair_type[0])\n        xml_file.write('<pair_coeffs>\\n')\n        for param_set in pair_coeffs:\n            xml_file.write('{}\\t{:.4f}\\t{:.4f}\\n'.format(\n                param_set[0], param_set[1]/ref_energy,\n                param_set[2]/ref_distance))\n        xml_file.write('</pair_coeffs>\\n')", "response": "Writes out the particle information for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the bonds in the system to xml_file.", "response": "def _write_bond_information(xml_file, structure, ref_distance, ref_energy):\n    \"\"\"Write the bonds in the system.\n\n    Parameters\n    ----------\n    xml_file : file object\n        The file object of the hoomdxml file being written\n    structure : parmed.Structure\n        Parmed structure object\n    ref_distance : float, default=1.0\n        Reference distance for conversion to reduced units\n    ref_energy : float, default=1.0\n        Reference energy for conversion to reduced units\n\n    \"\"\"\n\n    unique_bond_types = set()\n    xml_file.write('<bond>\\n')\n    for bond in structure.bonds:\n        t1, t2 = bond.atom1.type, bond.atom2.type\n        if t1 == '' or t2 == '':\n            t1, t2 = bond.atom1.name, bond.atom2.name\n        t1, t2 = sorted([t1, t2])\n        try:\n            bond_type = ('-'.join((t1, t2)), bond.type.k, bond.type.req)\n        except AttributeError:  # no forcefield applied, bond.type is None\n            bond_type = ('-'.join((t1, t2)), 0.0, 0.0)\n        unique_bond_types.add(bond_type)\n        xml_file.write('{} {} {}\\n'.format(\n            bond_type[0], bond.atom1.idx, bond.atom2.idx))\n    xml_file.write('</bond>\\n')\n    xml_file.write('<bond_coeffs>\\n')\n    xml_file.write('<!-- type k r_eq -->\\n')\n    for bond_type, k, req in unique_bond_types:\n        xml_file.write('{} {} {}\\n'.format(bond_type,\n            k * 2.0 / ref_energy * ref_distance**2.0, req/ref_distance))\n    xml_file.write('</bond_coeffs>\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_angle_information(xml_file, structure, ref_energy):\n\n    unique_angle_types = set()\n    xml_file.write('<angle>\\n')\n    for angle in structure.angles:\n        t1, t2, t3 = angle.atom1.type, angle.atom2.type, angle.atom3.type\n        t1, t3 = sorted([t1, t3])\n        angle_type = ('-'.join((t1, t2, t3)), angle.type.k, angle.type.theteq)\n        unique_angle_types.add(angle_type)\n        xml_file.write('{} {} {} {}\\n'.format(\n            angle_type[0], angle.atom1.idx, angle.atom2.idx, angle.atom3.idx))\n    xml_file.write('</angle>\\n')\n    xml_file.write('<angle_coeffs>\\n')\n    xml_file.write('<!-- type k theta_eq -->\\n')\n    for angle_type, k, teq in unique_angle_types:\n        xml_file.write('{} {} {}\\n'.format(angle_type,\n            k * 2.0 / ref_energy, radians(teq)))\n    xml_file.write('</angle_coeffs>\\n')", "response": "Writes the angles in the system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting dihedrals in the system.", "response": "def _write_dihedral_information(xml_file, structure, ref_energy):\n    \"\"\"Write dihedrals in the system.\n\n    Parameters\n    ----------\n    xml_file : file object\n        The file object of the hoomdxml file being written\n    structure : parmed.Structure\n        Parmed structure object\n    ref_energy : float, default=1.0\n        Reference energy for conversion to reduced units\n\n    \"\"\"\n\n    unique_dihedral_types = set()\n    xml_file.write('<dihedral>\\n')\n    for dihedral in structure.rb_torsions:\n        t1, t2 = dihedral.atom1.type, dihedral.atom2.type,\n        t3, t4 = dihedral.atom3.type, dihedral.atom4.type\n        if [t2, t3] == sorted([t2, t3]):\n            types_in_dihedral = '-'.join((t1, t2, t3, t4))\n        else:\n            types_in_dihedral = '-'.join((t4, t3, t2, t1))\n        dihedral_type = (types_in_dihedral, dihedral.type.c0,\n        dihedral.type.c1, dihedral.type.c2, dihedral.type.c3, dihedral.type.c4,\n        dihedral.type.c5, dihedral.type.scee, dihedral.type.scnb)\n        unique_dihedral_types.add(dihedral_type)\n        xml_file.write('{} {} {} {} {}\\n'.format(\n            dihedral_type[0], dihedral.atom1.idx, dihedral.atom2.idx,\n            dihedral.atom3.idx, dihedral.atom4.idx))\n    xml_file.write('</dihedral>\\n')\n    xml_file.write('<dihedral_coeffs>\\n')\n    xml_file.write('<!-- type k1 k2 k3 k4 -->\\n')\n    for dihedral_type, c0, c1, c2, c3, c4, c5, scee, scnb in unique_dihedral_types:\n        opls_coeffs = RB_to_OPLS(c0, c1, c2, c3, c4, c5)\n        opls_coeffs /= ref_energy\n        xml_file.write('{} {:.5f} {:.5f} {:.5f} {:.5f}\\n'.format(\n            dihedral_type, *opls_coeffs))\n    xml_file.write('</dihedral_coeffs>\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_rigid_information(xml_file, rigid_bodies):\n\n    if not all(body is None for body in rigid_bodies):\n        xml_file.write('<body>\\n')\n        for body in rigid_bodies:\n            if body is None:\n                body = -1\n            xml_file.write('{}\\n'.format(int(body)))\n        xml_file.write('</body>\\n')", "response": "Write rigid body information to the hoomdxml file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write_box_information(xml_file, structure, ref_distance):\n    if np.allclose(structure.box[3:6], np.array([90, 90, 90])):\n        box_str = '<box units=\"sigma\"  Lx=\"{}\" Ly=\"{}\" Lz=\"{}\"/>\\n'\n        xml_file.write(box_str.format(*structure.box[:3] / ref_distance))\n    else:\n        a, b, c = structure.box[0:3] / ref_distance\n        alpha, beta, gamma = np.radians(structure.box[3:6])\n\n        lx = a\n        xy = b * np.cos(gamma)\n        xz = c * np.cos(beta)\n        ly = np.sqrt(b**2 - xy**2)\n        yz = (b*c*np.cos(alpha) - xy*xz) / ly\n        lz = np.sqrt(c**2 - xz**2 - yz**2)\n        box_str = '<box units=\"sigma\"  Lx=\"{}\" Ly=\"{}\" Lz=\"{}\" xy=\"{}\" xz=\"{}\" yz=\"{}\"/>\\n'\n        xml_file.write(box_str.format(lx, ly, lz, xy, xz, yz))", "response": "Write the box information to the xml file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of labels used to access the Port", "response": "def access_labels(self):\n        \"\"\"List of labels used to access the Port\n\n        Returns\n        -------\n        list of str\n            Strings that can be used to access this Port relative to self.root\n        \"\"\"\n        access_labels = []\n        for referrer in self.referrers:\n            referrer_labels = [key for key, val in self.root.labels.items()\n                               if val == referrer]\n            port_labels = [key for key, val in referrer.labels.items()\n                           if val == self]\n            if referrer is self.root:\n                for label in port_labels:\n                    access_labels.append(\"['{}']\".format(label))\n            for label in itertools.product(referrer_labels, port_labels):\n                access_labels.append(\"['{}']\".format(\"']['\".join(label)))\n\n        return access_labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signed_area(coords):\r\n    xs, ys = map(list, zip(*coords))\r\n    xs.append(xs[1])\r\n    ys.append(ys[1])\r\n    return sum(xs[i]*(ys[i+1]-ys[i-1]) for i in range(1, len(coords)))/2.0", "response": "Return the signed area enclosed by a ring using the linear time\r\n    algorithm. A value < 0 indicates a counter - clockwise oriented ring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning this Record as a dictionary using the field names as keys", "response": "def as_dict(self):\r\n        \"\"\"\r\n        Returns this Record as a dictionary using the field names as keys\r\n        :return: dict\r\n        \"\"\"\r\n        return dict((f, self[i]) for f, i in self.__field_positions.items())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_shp(self, shapefile_name):\r\n        shp_ext = 'shp'\r\n        try:\r\n            self.shp = open(\"%s.%s\" % (shapefile_name, shp_ext), \"rb\")\r\n        except IOError:\r\n            try:\r\n                self.shp = open(\"%s.%s\" % (shapefile_name, shp_ext.upper()), \"rb\")\r\n            except IOError:\r\n                pass", "response": "Loads the shapefile with the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_shx(self, shapefile_name):\r\n        shx_ext = 'shx'\r\n        try:\r\n            self.shx = open(\"%s.%s\" % (shapefile_name, shx_ext), \"rb\")\r\n        except IOError:\r\n            try:\r\n                self.shx = open(\"%s.%s\" % (shapefile_name, shx_ext.upper()), \"rb\")\r\n            except IOError:\r\n                pass", "response": "Loads the. shx extension of the class instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_dbf(self, shapefile_name):\r\n        dbf_ext = 'dbf'\r\n        try:\r\n            self.dbf = open(\"%s.%s\" % (shapefile_name, dbf_ext), \"rb\")\r\n        except IOError:\r\n            try:\r\n                self.dbf = open(\"%s.%s\" % (shapefile_name, dbf_ext.upper()), \"rb\")\r\n            except IOError:\r\n                pass", "response": "Loads the DBF file with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __getFileObj(self, f):\r\n        if not f:\r\n            raise ShapefileException(\"Shapefile Reader requires a shapefile or file-like object.\")\r\n        if self.shp and self.shpLength is None:\r\n            self.load()\r\n        if self.dbf and len(self.fields) == 0:\r\n            self.load()\r\n        return f", "response": "Checks to see if the requested shapefile file object is available. If not a ShapefileException is raised."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproviding list - like handling of a record index with a clearerange error message.", "response": "def __restrictIndex(self, i):\r\n        \"\"\"Provides list-like handling of a record index with a clearer\r\n        error message if the index is out of bounds.\"\"\"\r\n        if self.numRecords:\r\n            rmax = self.numRecords - 1\r\n            if abs(i) > rmax:\r\n                raise IndexError(\"Shape or Record index out of range.\")\r\n            if i < 0: i = range(self.numRecords)[i]\r\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __shpHeader(self):\r\n        if not self.shp:\r\n            raise ShapefileException(\"Shapefile Reader requires a shapefile or file-like object. (no shp file found\")\r\n        shp = self.shp\r\n        # File length (16-bit word * 2 = bytes)\r\n        shp.seek(24)\r\n        self.shpLength = unpack(\">i\", shp.read(4))[0] * 2\r\n        # Shape type\r\n        shp.seek(32)\r\n        self.shapeType= unpack(\"<i\", shp.read(4))[0]\r\n        # The shapefile's bounding box (lower left, upper right)\r\n        self.bbox = _Array('d', unpack(\"<4d\", shp.read(32)))\r\n        # Elevation\r\n        self.zbox = _Array('d', unpack(\"<2d\", shp.read(16)))\r\n        # Measure\r\n        self.mbox = []\r\n        for m in _Array('d', unpack(\"<2d\", shp.read(16))):\r\n            # Measure values less than -10e38 are nodata values according to the spec\r\n            if m > NODATA:\r\n                self.mbox.append(m)\r\n            else:\r\n                self.mbox.append(None)", "response": "Reads the header information from a. shp or. shx file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the header info and geometry for a single shape.", "response": "def __shape(self):\r\n        \"\"\"Returns the header info and geometry for a single shape.\"\"\"\r\n        f = self.__getFileObj(self.shp)\r\n        record = Shape()\r\n        nParts = nPoints = zmin = zmax = mmin = mmax = None\r\n        (recNum, recLength) = unpack(\">2i\", f.read(8))\r\n        # Determine the start of the next record\r\n        next = f.tell() + (2 * recLength)\r\n        shapeType = unpack(\"<i\", f.read(4))[0]\r\n        record.shapeType = shapeType\r\n        # For Null shapes create an empty points list for consistency\r\n        if shapeType == 0:\r\n            record.points = []\r\n        # All shape types capable of having a bounding box\r\n        elif shapeType in (3,5,8,13,15,18,23,25,28,31):\r\n            record.bbox = _Array('d', unpack(\"<4d\", f.read(32)))\r\n        # Shape types with parts\r\n        if shapeType in (3,5,13,15,23,25,31):\r\n            nParts = unpack(\"<i\", f.read(4))[0]\r\n        # Shape types with points\r\n        if shapeType in (3,5,8,13,15,18,23,25,28,31):\r\n            nPoints = unpack(\"<i\", f.read(4))[0]\r\n        # Read parts\r\n        if nParts:\r\n            record.parts = _Array('i', unpack(\"<%si\" % nParts, f.read(nParts * 4)))\r\n        # Read part types for Multipatch - 31\r\n        if shapeType == 31:\r\n            record.partTypes = _Array('i', unpack(\"<%si\" % nParts, f.read(nParts * 4)))\r\n        # Read points - produces a list of [x,y] values\r\n        if nPoints:\r\n            flat = unpack(\"<%sd\" % (2 * nPoints), f.read(16*nPoints))\r\n            record.points = list(izip(*(iter(flat),) * 2))\r\n        # Read z extremes and values\r\n        if shapeType in (13,15,18,31):\r\n            (zmin, zmax) = unpack(\"<2d\", f.read(16))\r\n            record.z = _Array('d', unpack(\"<%sd\" % nPoints, f.read(nPoints * 8)))\r\n        # Read m extremes and values\r\n        if shapeType in (13,15,18,23,25,28,31):\r\n            if next - f.tell() >= 16:\r\n                (mmin, mmax) = unpack(\"<2d\", f.read(16))\r\n            # Measure values less than -10e38 are nodata values according to the spec\r\n            if next - f.tell() >= nPoints * 8:\r\n                record.m = []\r\n                for m in _Array('d', unpack(\"<%sd\" % nPoints, f.read(nPoints * 8))):\r\n                    if m > NODATA:\r\n                        record.m.append(m)\r\n                    else:\r\n                        record.m.append(None)\r\n            else:\r\n                record.m = [None for _ in range(nPoints)]\r\n        # Read a single point\r\n        if shapeType in (1,11,21):\r\n            record.points = [_Array('d', unpack(\"<2d\", f.read(16)))]\r\n        # Read a single Z value\r\n        if shapeType == 11:\r\n            record.z = list(unpack(\"<d\", f.read(8)))\r\n        # Read a single M value\r\n        if shapeType in (21,11):\r\n            if next - f.tell() >= 8:\r\n                (m,) = unpack(\"<d\", f.read(8))\r\n            else:\r\n                m = NODATA\r\n            # Measure values less than -10e38 are nodata values according to the spec\r\n            if m > NODATA:\r\n                record.m = [m]\r\n            else:\r\n                record.m = [None]\r\n        # Seek to the end of this record as defined by the record header because\r\n        # the shapefile spec doesn't require the actual content to meet the header\r\n        # definition.  Probably allowed for lazy feature deletion. \r\n        f.seek(next)\r\n        return record"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __shapeIndex(self, i=None):\r\n        shx = self.shx\r\n        if not shx:\r\n            return None\r\n        if not self._offsets:\r\n            # File length (16-bit word * 2 = bytes) - header length\r\n            shx.seek(24)\r\n            shxRecordLength = (unpack(\">i\", shx.read(4))[0] * 2) - 100\r\n            numRecords = shxRecordLength // 8\r\n            # Jump to the first record.\r\n            shx.seek(100)\r\n            shxRecords = _Array('i')\r\n            # Each offset consists of two nrs, only the first one matters\r\n            shxRecords.fromfile(shx, 2 * numRecords)\r\n            if sys.byteorder != 'big':\r\n                 shxRecords.byteswap()\r\n            self._offsets = [2 * el for el in shxRecords[::2]]\r\n        if not i == None:\r\n            return self._offsets[i]", "response": "Returns the offset in a. shp file for a shape based on information\r\n            in the. shx index file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shape(self, i=0):\r\n        shp = self.__getFileObj(self.shp)\r\n        i = self.__restrictIndex(i)\r\n        offset = self.__shapeIndex(i)\r\n        if not offset:\r\n            # Shx index not available so iterate the full list.\r\n            for j,k in enumerate(self.iterShapes()):\r\n                if j == i:\r\n                    return k\r\n        shp.seek(offset)\r\n        return self.__shape()", "response": "Returns a shape object for a shape in the geometry\r\n record file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iterShapes(self):\r\n        shp = self.__getFileObj(self.shp)\r\n        shp.seek(0,2)\r\n        self.shpLength = shp.tell()\r\n        shp.seek(100)\r\n        while shp.tell() < self.shpLength:\r\n            yield self.__shape()", "response": "Serves up shapes in a shapefile as an iterator. Useful when handling large shapefiles."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __dbfHeader(self):\r\n        if not self.dbf:\r\n            raise ShapefileException(\"Shapefile Reader requires a shapefile or file-like object. (no dbf file found)\")\r\n        dbf = self.dbf\r\n        # read relevant header parts\r\n        self.numRecords, self.__dbfHdrLength, self.__recordLength = \\\r\n                unpack(\"<xxxxLHH20x\", dbf.read(32))\r\n        # read fields\r\n        numFields = (self.__dbfHdrLength - 33) // 32\r\n        for field in range(numFields):\r\n            fieldDesc = list(unpack(\"<11sc4xBB14x\", dbf.read(32)))\r\n            name = 0\r\n            idx = 0\r\n            if b\"\\x00\" in fieldDesc[name]:\r\n                idx = fieldDesc[name].index(b\"\\x00\")\r\n            else:\r\n                idx = len(fieldDesc[name]) - 1\r\n            fieldDesc[name] = fieldDesc[name][:idx]\r\n            fieldDesc[name] = u(fieldDesc[name], self.encoding, self.encodingErrors)\r\n            fieldDesc[name] = fieldDesc[name].lstrip()\r\n            fieldDesc[1] = u(fieldDesc[1], 'ascii')\r\n            self.fields.append(fieldDesc)\r\n        terminator = dbf.read(1)\r\n        if terminator != b\"\\r\":\r\n            raise ShapefileException(\"Shapefile dbf header lacks expected terminator. (likely corrupt?)\")\r\n        self.fields.insert(0, ('DeletionFlag', 'C', 1, 0))\r\n        fmt,fmtSize = self.__recordFmt()\r\n        self.__recStruct = Struct(fmt)\r\n\r\n        # Store the field positions\r\n        self.__fieldposition_lookup = dict((f[0], i) for i, f in enumerate(self.fields[1:]))", "response": "Reads a single DBF header."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __record(self, oid=None):\r\n        f = self.__getFileObj(self.dbf)\r\n        recordContents = self.__recStruct.unpack(f.read(self.__recStruct.size))\r\n        if recordContents[0] != b' ':\r\n            # deleted record\r\n            return None\r\n        record = []\r\n        for (name, typ, size, deci), value in zip(self.fields, recordContents):\r\n            if name == 'DeletionFlag':\r\n                continue\r\n            elif typ in (\"N\",\"F\"):\r\n                # numeric or float: number stored as a string, right justified, and padded with blanks to the width of the field. \r\n                value = value.split(b'\\0')[0]\r\n                value = value.replace(b'*', b'')  # QGIS NULL is all '*' chars\r\n                if value == b'':\r\n                    value = None\r\n                elif deci:\r\n                    try:\r\n                        value = float(value)\r\n                    except ValueError:\r\n                        #not parseable as float, set to None\r\n                        value = None\r\n                else:\r\n                    # force to int\r\n                    try:\r\n                        # first try to force directly to int.\r\n                        # forcing a large int to float and back to int\r\n                        # will lose information and result in wrong nr.\r\n                        value = int(value) \r\n                    except ValueError:\r\n                        # forcing directly to int failed, so was probably a float.\r\n                        try:\r\n                            value = int(float(value))\r\n                        except ValueError:\r\n                            #not parseable as int, set to None\r\n                            value = None\r\n            elif typ == 'D':\r\n                # date: 8 bytes - date stored as a string in the format YYYYMMDD.\r\n                if value.count(b'0') == len(value):  # QGIS NULL is all '0' chars\r\n                    value = None\r\n                else:\r\n                    try:\r\n                        y, m, d = int(value[:4]), int(value[4:6]), int(value[6:8])\r\n                        value = date(y, m, d)\r\n                    except:\r\n                        value = value.strip()\r\n            elif typ == 'L':\r\n                # logical: 1 byte - initialized to 0x20 (space) otherwise T or F.\r\n                if value == b\" \":\r\n                    value = None # space means missing or not yet set\r\n                else:\r\n                    if value in b'YyTt1':\r\n                        value = True\r\n                    elif value in b'NnFf0':\r\n                        value = False\r\n                    else:\r\n                        value = None # unknown value is set to missing\r\n            else:\r\n                # anything else is forced to string/unicode\r\n                value = u(value, self.encoding, self.encodingErrors)\r\n                value = value.strip()\r\n            record.append(value)\r\n\r\n        return _Record(self.__fieldposition_lookup, record, oid)", "response": "Reads and returns a dbf record row as a list of values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a specific dbf record based on the supplied index.", "response": "def record(self, i=0):\r\n        \"\"\"Returns a specific dbf record based on the supplied index.\"\"\"\r\n        f = self.__getFileObj(self.dbf)\r\n        if self.numRecords is None:\r\n            self.__dbfHeader()\r\n        i = self.__restrictIndex(i)\r\n        recSize = self.__recStruct.size\r\n        f.seek(0)\r\n        f.seek(self.__dbfHdrLength + (i * recSize))\r\n        return self.__record(oid=i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserves up records in a dbf file as an iterator. Useful for large shapefiles or dbf files.", "response": "def iterRecords(self):\r\n        \"\"\"Serves up records in a dbf file as an iterator.\r\n        Useful for large shapefiles or dbf files.\"\"\"\r\n        if self.numRecords is None:\r\n            self.__dbfHeader()\r\n        f = self.__getFileObj(self.dbf)\r\n        f.seek(self.__dbfHdrLength)\r\n        for i in xrange(self.numRecords):\r\n            r = self.__record()\r\n            if r:\r\n                yield r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shapeRecord(self, i=0):\r\n        i = self.__restrictIndex(i)\r\n        return ShapeRecord(shape=self.shape(i), record=self.record(i))", "response": "Returns a combination geometry and attribute record for the\r\n        supplied record index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shapeRecords(self):\r\n        return ShapeRecords([ShapeRecord(shape=rec[0], record=rec[1]) \\\r\n                                for rec in zip(self.shapes(), self.records())])", "response": "Returns a list of combination geometry or attribute records for\r\n        all records in a shapefile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iterShapeRecords(self):\r\n        for shape, record in izip(self.iterShapes(), self.iterRecords()):\r\n            yield ShapeRecord(shape=shape, record=record)", "response": "Returns a generator of combination geometry and attribute records for all records in a shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\r\n        # Check if any of the files have already been closed\r\n        shp_open = self.shp and not (hasattr(self.shp, 'closed') and self.shp.closed)\r\n        shx_open = self.shx and not (hasattr(self.shx, 'closed') and self.shx.closed)\r\n        dbf_open = self.dbf and not (hasattr(self.dbf, 'closed') and self.dbf.closed)\r\n            \r\n        # Balance if already not balanced\r\n        if self.shp and shp_open and self.dbf and dbf_open:\r\n            if self.autoBalance:\r\n                self.balance()\r\n            if self.recNum != self.shpNum:\r\n                raise ShapefileException(\"When saving both the dbf and shp file, \"\r\n                                         \"the number of records (%s) must correspond \"\r\n                                         \"with the number of shapes (%s)\" % (self.recNum, self.shpNum))\r\n        # Fill in the blank headers\r\n        if self.shp and shp_open:\r\n            self.__shapefileHeader(self.shp, headerType='shp')\r\n        if self.shx and shx_open:\r\n            self.__shapefileHeader(self.shx, headerType='shx')\r\n\r\n        # Update the dbf header with final length etc\r\n        if self.dbf and dbf_open:\r\n            self.__dbfHeader()\r\n\r\n        # Close files, if target is a filepath\r\n        if self.target:\r\n            for attribute in (self.shp, self.shx, self.dbf):\r\n                if hasattr(attribute, 'close'):\r\n                    try:\r\n                        attribute.close()\r\n                    except IOError:\r\n                        pass", "response": "Close the file and all of its attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __shpFileLength(self):\r\n        # Remember starting position\r\n        start = self.shp.tell()\r\n        # Calculate size of all shapes\r\n        self.shp.seek(0,2)\r\n        size = self.shp.tell()\r\n        # Calculate size as 16-bit words\r\n        size //= 2\r\n        # Return to start\r\n        self.shp.seek(start)\r\n        return size", "response": "Calculates the length of the shp file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the specified header type to the specified file - like object.", "response": "def __shapefileHeader(self, fileObj, headerType='shp'):\r\n        \"\"\"Writes the specified header type to the specified file-like object.\r\n        Several of the shapefile formats are so similar that a single generic\r\n        method to read or write them is warranted.\"\"\"\r\n        f = self.__getFileObj(fileObj)\r\n        f.seek(0)\r\n        # File code, Unused bytes\r\n        f.write(pack(\">6i\", 9994,0,0,0,0,0))\r\n        # File length (Bytes / 2 = 16-bit words)\r\n        if headerType == 'shp':\r\n            f.write(pack(\">i\", self.__shpFileLength()))\r\n        elif headerType == 'shx':\r\n            f.write(pack('>i', ((100 + (self.shpNum * 8)) // 2)))\r\n        # Version, Shape type\r\n        if self.shapeType is None:\r\n            self.shapeType = NULL\r\n        f.write(pack(\"<2i\", 1000, self.shapeType))\r\n        # The shapefile's bounding box (lower left, upper right)\r\n        if self.shapeType != 0:\r\n            try:\r\n                bbox = self.bbox()\r\n                if bbox is None:\r\n                    # The bbox is initialized with None, so this would mean the shapefile contains no valid geometries.\r\n                    # In such cases of empty shapefiles, ESRI spec says the bbox values are 'unspecified'.\r\n                    # Not sure what that means, so for now just setting to 0s, which is the same behavior as in previous versions.\r\n                    # This would also make sense since the Z and M bounds are similarly set to 0 for non-Z/M type shapefiles.\r\n                    bbox = [0,0,0,0] \r\n                f.write(pack(\"<4d\", *bbox))\r\n            except error:\r\n                raise ShapefileException(\"Failed to write shapefile bounding box. Floats required.\")\r\n        else:\r\n            f.write(pack(\"<4d\", 0,0,0,0))\r\n        # Elevation\r\n        if self.shapeType in (11,13,15,18):\r\n            # Z values are present in Z type\r\n            zbox = self.zbox()\r\n        else:\r\n            # As per the ESRI shapefile spec, the zbox for non-Z type shapefiles are set to 0s\r\n            zbox = [0,0]\r\n        # Measure\r\n        if self.shapeType in (11,13,15,18,21,23,25,28,31):\r\n            # M values are present in M or Z type\r\n            mbox = self.mbox()\r\n        else:\r\n            # As per the ESRI shapefile spec, the mbox for non-M type shapefiles are set to 0s\r\n            mbox = [0,0]\r\n        # Try writing\r\n        try:\r\n            f.write(pack(\"<4d\", zbox[0], zbox[1], mbox[0], mbox[1]))\r\n        except error:\r\n            raise ShapefileException(\"Failed to write shapefile elevation and measure values. Floats required.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the dbf header and field descriptors.", "response": "def __dbfHeader(self):\r\n        \"\"\"Writes the dbf header and field descriptors.\"\"\"\r\n        f = self.__getFileObj(self.dbf)\r\n        f.seek(0)\r\n        version = 3\r\n        year, month, day = time.localtime()[:3]\r\n        year -= 1900\r\n        # Remove deletion flag placeholder from fields\r\n        for field in self.fields:\r\n            if field[0].startswith(\"Deletion\"):\r\n                self.fields.remove(field)\r\n        numRecs = self.recNum\r\n        numFields = len(self.fields)\r\n        headerLength = numFields * 32 + 33\r\n        if headerLength >= 65535:\r\n            raise ShapefileException(\r\n                    \"Shapefile dbf header length exceeds maximum length.\")\r\n        recordLength = sum([int(field[2]) for field in self.fields]) + 1\r\n        header = pack('<BBBBLHH20x', version, year, month, day, numRecs,\r\n                headerLength, recordLength)\r\n        f.write(header)\r\n        # Field descriptors\r\n        for field in self.fields:\r\n            name, fieldType, size, decimal = field\r\n            name = b(name, self.encoding, self.encodingErrors)\r\n            name = name.replace(b' ', b'_')\r\n            name = name.ljust(11).replace(b' ', b'\\x00')\r\n            fieldType = b(fieldType, 'ascii')\r\n            size = int(size)\r\n            fld = pack('<11sc4xBB14x', name, fieldType, size, decimal)\r\n            f.write(fld)\r\n        # Terminator\r\n        f.write(b'\\r')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __shxRecord(self, offset, length):\r\n         f = self.__getFileObj(self.shx)\r\n         f.write(pack(\">i\", offset // 2))\r\n         f.write(pack(\">i\", length))", "response": "Writes the shx records."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record(self, *recordList, **recordDict):\r\n        # Balance if already not balanced\r\n        if self.autoBalance and self.recNum > self.shpNum:\r\n            self.balance()\r\n            \r\n        record = []\r\n        fieldCount = len(self.fields)\r\n        # Compensate for deletion flag\r\n        if self.fields[0][0].startswith(\"Deletion\"): fieldCount -= 1\r\n        if recordList:\r\n            record = [recordList[i] for i in range(fieldCount)]\r\n        elif recordDict:\r\n            for field in self.fields:\r\n                if field[0] in recordDict:\r\n                    val = recordDict[field[0]]\r\n                    if val is None:\r\n                        record.append(\"\")\r\n                    else:\r\n                        record.append(val)\r\n        else:\r\n            # Blank fields for empty record\r\n            record = [\"\" for i in range(fieldCount)]\r\n        self.__dbfRecord(record)", "response": "Creates a new dbf attribute record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __dbfRecord(self, record):\r\n        f = self.__getFileObj(self.dbf)\r\n        if self.recNum == 0:\r\n            # first records, so all fields should be set\r\n            # allowing us to write the dbf header\r\n            # cannot change the fields after this point\r\n            self.__dbfHeader()\r\n        # begin\r\n        self.recNum += 1\r\n        if not self.fields[0][0].startswith(\"Deletion\"):\r\n            f.write(b' ') # deletion flag\r\n        for (fieldName, fieldType, size, deci), value in zip(self.fields, record):\r\n            fieldType = fieldType.upper()\r\n            size = int(size)\r\n            if fieldType in (\"N\",\"F\"):\r\n                # numeric or float: number stored as a string, right justified, and padded with blanks to the width of the field.\r\n                if value in MISSING:\r\n                    value = b\"*\"*size # QGIS NULL\r\n                elif not deci:\r\n                    # force to int\r\n                    try:\r\n                        # first try to force directly to int.\r\n                        # forcing a large int to float and back to int\r\n                        # will lose information and result in wrong nr.\r\n                        value = int(value) \r\n                    except ValueError:\r\n                        # forcing directly to int failed, so was probably a float.\r\n                        value = int(float(value))\r\n                    value = format(value, \"d\")[:size].rjust(size) # caps the size if exceeds the field size\r\n                else:\r\n                    value = float(value)\r\n                    value = format(value, \".%sf\"%deci)[:size].rjust(size) # caps the size if exceeds the field size\r\n            elif fieldType == \"D\":\r\n                # date: 8 bytes - date stored as a string in the format YYYYMMDD.\r\n                if isinstance(value, date):\r\n                    value = '{:04d}{:02d}{:02d}'.format(value.year, value.month, value.day)\r\n                elif isinstance(value, list) and len(value) == 3:\r\n                    value = '{:04d}{:02d}{:02d}'.format(*value)\r\n                elif value in MISSING:\r\n                    value = b'0' * 8 # QGIS NULL for date type\r\n                elif is_string(value) and len(value) == 8:\r\n                    pass # value is already a date string\r\n                else:\r\n                    raise ShapefileException(\"Date values must be either a datetime.date object, a list, a YYYYMMDD string, or a missing value.\")\r\n            elif fieldType == 'L':\r\n                # logical: 1 byte - initialized to 0x20 (space) otherwise T or F.\r\n                if value in MISSING:\r\n                    value = b' ' # missing is set to space\r\n                elif value in [True,1]:\r\n                    value = b'T'\r\n                elif value in [False,0]:\r\n                    value = b'F'\r\n                else:\r\n                    value = b' ' # unknown is set to space\r\n            else:\r\n                # anything else is forced to string, truncated to the length of the field\r\n                value = b(value, self.encoding, self.encodingErrors)[:size].ljust(size)\r\n            if not isinstance(value, bytes):\r\n                # just in case some of the numeric format() and date strftime() results are still in unicode (Python 3 only)\r\n                value = b(value, 'ascii', self.encodingErrors) # should be default ascii encoding\r\n            if len(value) != size:\r\n                raise ShapefileException(\r\n                    \"Shapefile Writer unable to pack incorrect sized value\"\r\n                    \" (size %d) into field '%s' (size %d).\" % (len(value), fieldName, size))\r\n            f.write(value)", "response": "Writes the dbf records."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds corresponding empty attributes or null geometry records depending on which type of record was created.", "response": "def balance(self):\r\n        \"\"\"Adds corresponding empty attributes or null geometry records depending\r\n        on which type of record was created to make sure all three files\r\n        are in synch.\"\"\"\r\n        while self.recNum > self.shpNum:\r\n            self.null()\r\n        while self.recNum < self.shpNum:\r\n            self.record()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef point(self, x, y):\r\n        shapeType = POINT\r\n        pointShape = Shape(shapeType)\r\n        pointShape.points.append([x, y])\r\n        self.shape(pointShape)", "response": "Creates a POINT shape with the given x and y coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pointm(self, x, y, m=None):\r\n        shapeType = POINTM\r\n        pointShape = Shape(shapeType)\r\n        pointShape.points.append([x, y, m])\r\n        self.shape(pointShape)", "response": "Creates a POINTM shape.\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a POINTZ shape.", "response": "def pointz(self, x, y, z=0, m=None):\r\n        \"\"\"Creates a POINTZ shape.\r\n        If the z (elevation) value is not set, it defaults to 0.\r\n        If the m (measure) value is not set, it defaults to NoData.\"\"\"\r\n        shapeType = POINTZ\r\n        pointShape = Shape(shapeType)\r\n        pointShape.points.append([x, y, z, m])\r\n        self.shape(pointShape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multipoint(self, points):\r\n        shapeType = MULTIPOINT\r\n        points = [points] # nest the points inside a list to be compatible with the generic shapeparts method\r\n        self._shapeparts(parts=points, shapeType=shapeType)", "response": "Creates a MULTIPOINT shape.\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multipointm(self, points):\r\n        shapeType = MULTIPOINTM\r\n        points = [points] # nest the points inside a list to be compatible with the generic shapeparts method\r\n        self._shapeparts(parts=points, shapeType=shapeType)", "response": "Creates a MULTIPOINTM shape.\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a MULTIPOINTZ shape. Points is a list of xyzm values.", "response": "def multipointz(self, points):\r\n        \"\"\"Creates a MULTIPOINTZ shape.\r\n        Points is a list of xyzm values.\r\n        If the z (elevation) value is not included, it defaults to 0.\r\n        If the m (measure) value is not included, it defaults to None (NoData).\"\"\"\r\n        shapeType = MULTIPOINTZ\r\n        points = [points] # nest the points inside a list to be compatible with the generic shapeparts method\r\n        self._shapeparts(parts=points, shapeType=shapeType)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a POLYLINE shape.", "response": "def line(self, lines):\r\n        \"\"\"Creates a POLYLINE shape.\r\n        Lines is a collection of lines, each made up of a list of xy values.\"\"\"\r\n        shapeType = POLYLINE\r\n        self._shapeparts(parts=lines, shapeType=shapeType)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a POLYLINEM shape. Lines is a collection of lines each made up of a list of xym values.", "response": "def linem(self, lines):\r\n        \"\"\"Creates a POLYLINEM shape.\r\n        Lines is a collection of lines, each made up of a list of xym values.\r\n        If the m (measure) value is not included, it defaults to None (NoData).\"\"\"\r\n        shapeType = POLYLINEM\r\n        self._shapeparts(parts=lines, shapeType=shapeType)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a POLYLINEZ shape.", "response": "def linez(self, lines):\r\n        \"\"\"Creates a POLYLINEZ shape.\r\n        Lines is a collection of lines, each made up of a list of xyzm values.\r\n        If the z (elevation) value is not included, it defaults to 0.\r\n        If the m (measure) value is not included, it defaults to None (NoData).\"\"\"\r\n        shapeType = POLYLINEZ\r\n        self._shapeparts(parts=lines, shapeType=shapeType)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef poly(self, polys):\r\n        shapeType = POLYGON\r\n        self._shapeparts(parts=polys, shapeType=shapeType)", "response": "Creates a POLYGON shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef polym(self, polys):\r\n        shapeType = POLYGONM\r\n        self._shapeparts(parts=polys, shapeType=shapeType)", "response": "Creates a POLYGONM shape."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a POLYGONZ shape.", "response": "def polyz(self, polys):\r\n        \"\"\"Creates a POLYGONZ shape.\r\n        Polys is a collection of polygons, each made up of a list of xyzm values.\r\n        Note that for ordinary polygons the coordinates must run in a clockwise direction.\r\n        If some of the polygons are holes, these must run in a counterclockwise direction.\r\n        If the z (elevation) value is not included, it defaults to 0.\r\n        If the m (measure) value is not included, it defaults to None (NoData).\"\"\"\r\n        shapeType = POLYGONZ\r\n        self._shapeparts(parts=polys, shapeType=shapeType)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multipatch(self, parts, partTypes):\r\n        shapeType = MULTIPATCH\r\n        polyShape = Shape(shapeType)\r\n        polyShape.parts = []\r\n        polyShape.points = []\r\n        for part in parts:\r\n            # set part index position\r\n            polyShape.parts.append(len(polyShape.points))\r\n            # add points\r\n            for point in part:\r\n                # Ensure point is list\r\n                if not isinstance(point, list):\r\n                    point = list(point)\r\n                polyShape.points.append(point)\r\n        polyShape.partTypes = partTypes\r\n        # write the shape\r\n        self.shape(polyShape)", "response": "Creates a MULTIPATCH shape with the given set of parts and part types."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _shapeparts(self, parts, shapeType):\r\n        polyShape = Shape(shapeType)\r\n        polyShape.parts = []\r\n        polyShape.points = []\r\n        for part in parts:\r\n            # set part index position\r\n            polyShape.parts.append(len(polyShape.points))\r\n            # add points\r\n            for point in part:\r\n                # Ensure point is list\r\n                if not isinstance(point, list):\r\n                    point = list(point)\r\n                polyShape.points.append(point)\r\n        # write the shape\r\n        self.shape(polyShape)", "response": "Internal method for adding a shape that has multiple collections of points"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a dbf field descriptor to the shapefile.", "response": "def field(self, name, fieldType=\"C\", size=\"50\", decimal=0):\r\n        \"\"\"Adds a dbf field descriptor to the shapefile.\"\"\"\r\n        if fieldType == \"D\":\r\n            size = \"8\"\r\n            decimal = 0\r\n        elif fieldType == \"L\":\r\n            size = \"1\"\r\n            decimal = 0\r\n        if len(self.fields) >= 2046:\r\n            raise ShapefileException(\r\n                \"Shapefile Writer reached maximum number of fields: 2046.\")\r\n        self.fields.append((name, fieldType, size, decimal))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a seasonal - trend decomposition of observed time series data.", "response": "def decompose(df, period=365, lo_frac=0.6, lo_delta=0.01):\n    \"\"\"Create a seasonal-trend (with Loess, aka \"STL\") decomposition of observed time series data.\n\n    This implementation is modeled after the ``statsmodels.tsa.seasonal_decompose`` method \n    but substitutes a Lowess regression for a convolution in its trend estimation.\n\n    This is an additive model, Y[t] = T[t] + S[t] + e[t]        \n\n    For more details on lo_frac and lo_delta, see: \n    `statsmodels.nonparametric.smoothers_lowess.lowess()`\n\n    Args:\n        df (pandas.Dataframe): Time series of observed counts. This DataFrame must be continuous (no \n            gaps or missing data), and include a ``pandas.DatetimeIndex``.  \n        period (int, optional): Most significant periodicity in the observed time series, in units of\n            1 observation. Ex: to accomodate strong annual periodicity within years of daily \n            observations, ``period=365``. \n        lo_frac (float, optional): Fraction of data to use in fitting Lowess regression. \n        lo_delta (float, optional): Fractional distance within which to use linear-interpolation \n            instead of weighted regression. Using non-zero ``lo_delta`` significantly decreases \n            computation time.\n\n    Returns:\n        `statsmodels.tsa.seasonal.DecomposeResult`: An object with DataFrame attributes for the \n            seasonal, trend, and residual components, as well as the average seasonal cycle. \n\n    \"\"\"\n    # use some existing pieces of statsmodels    \n    lowess = sm.nonparametric.lowess\n    _pandas_wrapper, _ = _maybe_get_pandas_wrapper_freq(df)\n\n    # get plain np array\n    observed = np.asanyarray(df).squeeze()\n\n    # calc trend, remove from observation\n    trend = lowess(observed, [x for x in range(len(observed))], \n                   frac=lo_frac, \n                   delta=lo_delta * len(observed),\n                   return_sorted=False)\n    detrended = observed - trend\n\n    # period must not be larger than size of series to avoid introducing NaNs\n    period = min(period, len(observed))\n\n    # calc one-period seasonality, remove tiled array from detrended\n    period_averages = np.array([pd_nanmean(detrended[i::period]) for i in range(period)])\n    # 0-center the period avgs\n    period_averages -= np.mean(period_averages)\n    seasonal = np.tile(period_averages, len(observed) // period + 1)[:len(observed)]    \n    resid = detrended - seasonal\n\n    # convert the arrays back to appropriate dataframes, stuff them back into \n    #  the statsmodel object\n    results = list(map(_pandas_wrapper, [seasonal, trend, resid, observed]))    \n    dr = DecomposeResult(seasonal=results[0],\n                         trend=results[1],\n                         resid=results[2], \n                         observed=results[3],\n                         period_averages=period_averages)\n    return dr"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nforecasts the given decomposition of observed time Series using the given forecasting function.", "response": "def forecast(stl, fc_func, steps=10, seasonal=False, **fc_func_kwargs):\n    \"\"\"Forecast the given decomposition ``stl`` forward by ``steps`` steps using the forecasting \n    function ``fc_func``, optionally including the calculated seasonality. \n\n    This is an additive model, Y[t] = T[t] + S[t] + e[t]    \n\n    Args:\n        stl (a modified statsmodels.tsa.seasonal.DecomposeResult): STL decomposition of observed time \n            series created using the ``stldecompose.decompose()`` method. \n        fc_func (function): Function which takes an array of observations and returns a single\n            valued forecast for the next point.\n        steps (int, optional): Number of forward steps to include in the forecast\n        seasonal (bool, optional): Include seasonal component in forecast\n        fc_func_kwargs: keyword arguments\n            All remaining arguments are passed to the forecasting function ``fc_func``\n\n    Returns:\n        forecast_frame (pd.Dataframe): A ``pandas.Dataframe`` containing forecast values and a \n            DatetimeIndex matching the observed index.  \n    \"\"\"\n    # container for forecast values\n    forecast_array = np.array([])\n\n    # forecast trend\n    # unpack precalculated trend array stl frame\n    trend_array = stl.trend\n\n    # iteratively forecast trend (\"seasonally adjusted\") component\n    # note: this loop can be slow\n    for step in range(steps):\n        # make this prediction on all available data\n        pred = fc_func(np.append(trend_array, forecast_array), **fc_func_kwargs)\n        # add this prediction to current array\n        forecast_array = np.append(forecast_array, pred)\n    col_name = fc_func.__name__\n\n    # forecast start and index are determined by observed data \n    observed_timedelta = stl.observed.index[-1] - stl.observed.index[-2]\n    forecast_idx_start = stl.observed.index[-1] + observed_timedelta\n    forecast_idx = pd.date_range(start=forecast_idx_start, \n                                 periods=steps,\n                                 freq=pd.tseries.frequencies.to_offset(observed_timedelta))\n\n    # (optionally) forecast seasonal & combine \n    if seasonal:\n        # track index and value of max correlation\n        seasonal_ix = 0\n        max_correlation = -np.inf\n        # loop over indexes=length of period avgs\n        detrended_array = np.asanyarray(stl.observed - stl.trend).squeeze()\n        for i, x in enumerate(stl.period_averages):\n            # work slices backward from end of detrended observations\n            if i == 0:\n                # slicing w/ [x:-0] doesn't work\n                detrended_slice = detrended_array[-len(stl.period_averages):]        \n            else:\n                detrended_slice = detrended_array[-(len(stl.period_averages) + i):-i]\n            # calculate corr b/w period_avgs and detrend_slice\n            this_correlation = np.correlate(detrended_slice, stl.period_averages)[0]\n            if this_correlation > max_correlation:\n                # update ix and max correlation\n                max_correlation = this_correlation\n                seasonal_ix = i\n        # roll seasonal signal to matching phase\n        rolled_period_averages = np.roll(stl.period_averages, -seasonal_ix)\n        # tile as many time as needed to reach \"steps\", then truncate\n        tiled_averages = np.tile(rolled_period_averages, \n                                 (steps // len(stl.period_averages) + 1))[:steps]\n        # add seasonal values to previous forecast\n        forecast_array += tiled_averages                \n        col_name += '+seasonal'\n\n    # combine data array with index into named dataframe\n    forecast_frame = pd.DataFrame(data=forecast_array, index=forecast_idx)\n    forecast_frame.columns = [col_name]  \n    return forecast_frame"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the mean forecast for the next point in the n - value series.", "response": "def mean(data, n=3, **kwargs):\n    \"\"\"The mean forecast for the next point is the mean value of the previous ``n`` points in \n    the series.\n\n    Args:\n        data (np.array): Observed data, presumed to be ordered in time.\n        n (int): period over which to calculate the mean \n\n    Returns:\n        float: a single-valued forecast for the next value in the series.\n    \"\"\"\n    # don't start averaging until we've seen n points\n    if len(data[-n:]) < n:\n        forecast = np.nan\n    else:\n        # nb: we'll keep the forecast as a float\n        forecast = np.mean(data[-n:])\n    return forecast"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drift(data, n=3, **kwargs):\n    yi = data[-n]\n    yf = data[-1]\n    slope = (yf - yi) / (n - 1)\n    forecast = yf + slope\n    return forecast", "response": "This function calculates the drift forecast for the next point in the series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exchange_token(self, code):\n        url = '%s%s/oauth2/token' % (self.scheme, self.host)\n        options = {\n            'grant_type': 'authorization_code',\n            'redirect_uri': self._redirect_uri(),\n            'client_id': self.options.get('client_id'),\n            'client_secret': self.options.get('client_secret'),\n            'code': code,\n        }\n        options.update({\n            'verify_ssl': self.options.get('verify_ssl', True),\n            'proxies': self.options.get('proxies', None)\n        })\n        self.token = wrapped_resource(\n            make_request('post', url, options))\n        self.access_token = self.token.access_token\n        return self.token", "response": "Request an access token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the authorization URL so the user can authorize the app.", "response": "def _authorization_code_flow(self):\n        \"\"\"Build the the auth URL so the user can authorize the app.\"\"\"\n        options = {\n            'scope': getattr(self, 'scope', 'non-expiring'),\n            'client_id': self.options.get('client_id'),\n            'response_type': 'code',\n            'redirect_uri': self._redirect_uri()\n        }\n        url = '%s%s/connect' % (self.scheme, self.host)\n        self._authorize_url = '%s?%s' % (url, urlencode(options))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a refresh token obtain a new access token.", "response": "def _refresh_token_flow(self):\n        \"\"\"Given a refresh token, obtain a new access token.\"\"\"\n        url = '%s%s/oauth2/token' % (self.scheme, self.host)\n        options = {\n            'grant_type': 'refresh_token',\n            'client_id': self.options.get('client_id'),\n            'client_secret': self.options.get('client_secret'),\n            'refresh_token': self.options.get('refresh_token')\n        }\n        options.update({\n            'verify_ssl': self.options.get('verify_ssl', True),\n            'proxies': self.options.get('proxies', None)\n        })\n        self.token = wrapped_resource(\n            make_request('post', url, options))\n        self.access_token = self.token.access_token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _request(self, method, resource, **kwargs):\n        url = self._resolve_resource_name(resource)\n\n        if hasattr(self, 'access_token'):\n            kwargs.update(dict(oauth_token=self.access_token))\n        if hasattr(self, 'client_id'):\n            kwargs.update(dict(client_id=self.client_id))\n\n        kwargs.update({\n            'verify_ssl': self.options.get('verify_ssl', True),\n            'proxies': self.options.get('proxies', None)\n        })\n        return wrapped_resource(make_request(method, url, kwargs))", "response": "This method is used to make a request to the aracacore API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _resolve_resource_name(self, name):\n        if name[:4] == 'http':  # already a url\n            return name\n        name = name.rstrip('/').lstrip('/')\n        return '%s%s/%s' % (self.scheme, self.host, name)", "response": "Convert a resource name ( e. g. tracks to a URI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning any file objects from the provided dict.", "response": "def extract_files_from_dict(d):\n    \"\"\"Return any file objects from the provided dict.\n\n    >>> extract_files_from_dict({\n    ... 'oauth_token': 'foo',\n    ... 'track': {\n    ...   'title': 'bar',\n    ...   'asset_data': open('setup.py', 'rb')\n    ...  }})  # doctest:+ELLIPSIS\n    {'track': {'asset_data': <...}}\n    \"\"\"\n    files = {}\n    for key, value in six.iteritems(d):\n        if isinstance(value, dict):\n            files[key] = extract_files_from_dict(value)\n        elif is_file_like(value):\n            files[key] = value\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the provided dict with any file objects removed.", "response": "def remove_files_from_dict(d):\n    \"\"\"Return the provided dict with any file objects removed.\n\n    >>> remove_files_from_dict({\n    ...   'oauth_token': 'foo',\n    ...   'track': {\n    ...       'title': 'bar',\n    ...       'asset_data': open('setup.py', 'rb')\n    ...   }\n    ... }) == {'track': {'title': 'bar'}, 'oauth_token': 'foo'}\n    ... # doctest:+ELLIPSIS\n    True\n    \"\"\"\n    file_free = {}\n    for key, value in six.iteritems(d):\n        if isinstance(value, dict):\n            file_free[key] = remove_files_from_dict(value)\n        elif not is_file_like(value):\n            if hasattr(value, '__iter__'):\n                file_free[key] = value\n            else:\n                if hasattr(value, 'encode'):\n                    file_free[key] = value.encode('utf-8')\n                else:\n                    file_free[key] = str(value)\n    return file_free"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform a nested dict into a namespaced query string.", "response": "def namespaced_query_string(d, prefix=\"\"):\n    \"\"\"Transform a nested dict into a string with namespaced query params.\n\n    >>> namespaced_query_string({\n    ...  'oauth_token': 'foo',\n    ...  'track': {'title': 'bar', 'sharing': 'private'}}) == {\n    ...      'track[sharing]': 'private',\n    ...      'oauth_token': 'foo',\n    ...      'track[title]': 'bar'}  # doctest:+ELLIPSIS\n    True\n    \"\"\"\n    qs = {}\n    prefixed = lambda k: prefix and \"%s[%s]\" % (prefix, k) or k\n    for key, value in six.iteritems(d):\n        if isinstance(value, dict):\n            qs.update(namespaced_query_string(value, prefix=key))\n        else:\n            qs[prefixed(key)] = value\n    return qs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes an HTTP request.", "response": "def make_request(method, url, params):\n    \"\"\"Make an HTTP request, formatting params as required.\"\"\"\n    empty = []\n\n    # TODO\n    # del params[key]\n    # without list\n    for key, value in six.iteritems(params):\n        if value is None:\n            empty.append(key)\n    for key in empty:\n        del params[key]\n\n    # allow caller to disable automatic following of redirects\n    allow_redirects = params.get('allow_redirects', True)\n\n    kwargs = {\n        'allow_redirects': allow_redirects,\n        'headers': {\n            'User-Agent': soundcloud.USER_AGENT\n        }\n    }\n    # options, not params\n    if 'verify_ssl' in params:\n        if params['verify_ssl'] is False:\n            kwargs['verify'] = params['verify_ssl']\n        del params['verify_ssl']\n    if 'proxies' in params:\n        kwargs['proxies'] = params['proxies']\n        del params['proxies']\n    if 'allow_redirects' in params:\n        del params['allow_redirects']\n\n    params = hashconversions.to_params(params)\n    files = namespaced_query_string(extract_files_from_dict(params))\n    data = namespaced_query_string(remove_files_from_dict(params))\n\n    request_func = getattr(requests, method, None)\n    if request_func is None:\n        raise TypeError('Unknown method: %s' % (method,))\n\n    if method == 'get':\n        kwargs['headers']['Accept'] = 'application/json'\n        qs = urlencode(data)\n        if '?' in url:\n            url_qs = '%s&%s' % (url, qs)\n        else:\n            url_qs = '%s?%s' % (url, qs)\n        result = request_func(url_qs, **kwargs)\n    else:\n        kwargs['data'] = data\n        if files:\n            kwargs['files'] = files\n        result = request_func(url, **kwargs)\n\n    # if redirects are disabled, don't raise for 301 / 302\n    if result.status_code in (301, 302):\n        if allow_redirects:\n            result.raise_for_status()\n    else:\n        result.raise_for_status()\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a response wrapped in the appropriate wrapper type.", "response": "def wrapped_resource(response):\n    \"\"\"Return a response wrapped in the appropriate wrapper type.\n\n    Lists will be returned as a ```ResourceList``` instance,\n    dicts will be returned as a ```Resource``` instance.\n    \"\"\"\n    # decode response text, assuming utf-8 if unset\n    response_content = response.content.decode(response.encoding or 'utf-8')\n\n    try:\n        content = json.loads(response_content)\n    except ValueError:\n        # not JSON\n        content = response_content\n    if isinstance(content, list):\n        result = ResourceList(content)\n    else:\n        result = Resource(content)\n        if hasattr(result, 'collection'):\n            result.collection = ResourceList(result.collection)\n    result.raw_data = response_content\n\n    for attr in ('encoding', 'url', 'status_code', 'reason'):\n        setattr(result, attr, getattr(response, attr))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a set of key value parameters into a dictionary suitable for passing into requests.", "response": "def normalize_param(key, value):\n    \"\"\"Convert a set of key, value parameters into a dictionary suitable for\n    passing into requests. This will convert lists into the syntax required\n    by SoundCloud. Heavily lifted from HTTParty.\n\n    >>> normalize_param('playlist', {\n    ...  'title': 'foo',\n    ...  'sharing': 'private',\n    ...  'tracks': [\n    ...    {id: 1234}, {id: 4567}\n    ...  ]}) == {\n    ...     u'playlist[tracks][][<built-in function id>]': [1234, 4567],\n    ...     u'playlist[sharing]': 'private',\n    ...     u'playlist[title]': 'foo'}  # doctest:+ELLIPSIS\n    True\n\n    >>> normalize_param('oauth_token', 'foo')\n    {'oauth_token': 'foo'}\n\n    >>> normalize_param('playlist[tracks]', [1234, 4567]) == {\n    ...     u'playlist[tracks][]': [1234, 4567]}\n    True\n    \"\"\"\n    params = {}\n    stack = []\n    if isinstance(value, list):\n        normalized = [normalize_param(u\"{0[key]}[]\".format(dict(key=key)), e) for e in value]\n        keys = [item for sublist in tuple(h.keys() for h in normalized) for item in sublist]\n\n        lists = {}\n        if len(keys) != len(set(keys)):\n            duplicates = [x for x, y in collections.Counter(keys).items() if y > 1]\n            for dup in duplicates:\n                lists[dup] = [h[dup] for h in normalized]\n                for h in normalized:\n                    del h[dup]\n\n        params.update(dict((k, v) for d in normalized for (k, v) in d.items()))\n        params.update(lists)\n    elif isinstance(value, dict):\n        stack.append([key, value])\n    else:\n        params.update({key: value})\n\n    for (parent, hash) in stack:\n        for (key, value) in six.iteritems(hash):\n            if isinstance(value, dict):\n                stack.append([u\"{0[parent]}[{0[key]}]\".format(dict(parent=parent, key=key)), value])\n            else:\n                params.update(normalize_param(u\"{0[parent]}[{0[key]}]\".format(dict(parent=parent, key=key)), value))\n\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening the smart object as binary IO.", "response": "def open(self, external_dir=None):\n        \"\"\"\n        Open the smart object as binary IO.\n\n        :param external_dir: Path to the directory of the external file.\n\n        Example::\n\n            with layer.smart_object.open() as f:\n                data = f.read()\n        \"\"\"\n        if self.kind == 'data':\n            with io.BytesIO(self._data.data) as f:\n                yield f\n        elif self.kind == 'external':\n            filepath = self._data.linked_file[b'fullPath'].value\n            filepath = filepath.replace('\\x00', '').replace('file://', '')\n            if not os.path.exists(filepath):\n                filepath = self._data.linked_file[b'relPath'].value\n                filepath = filepath.replace('\\x00', '')\n                if external_dir is not None:\n                    filepath = os.path.join(external_dir, filepath)\n            if not os.path.exists(filepath):\n                raise FileNotFoundError(filepath)\n            with open(filepath, 'rb') as f:\n                yield f\n        else:\n            raise NotImplementedError('alias is not supported.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data(self):\n        if self.kind == 'data':\n            return self._data.data\n        else:\n            with self.open() as f:\n                return f.read()", "response": "Embedded file content or empty string"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the size of the object.", "response": "def filesize(self):\n        \"\"\"File size of the object.\"\"\"\n        if self.kind == 'data':\n            return len(self._data.data)\n        return self._data.filesize"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the smart object to a file.", "response": "def save(self, filename=None):\n        \"\"\"\n        Save the smart object to a file.\n\n        :param filename: File name to export. If None, use the embedded name.\n        \"\"\"\n        if filename is None:\n            filename = self.filename\n        with open(filename, 'wb') as f:\n            f.write(self.data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _traverse(element, condition=None):\n        if condition is None or condition(element):\n            yield element\n        if isinstance(element, DictElement):\n            for child in element.values():\n                for _ in BaseElement._traverse(child, condition):\n                    yield _\n        elif isinstance(element, ListElement):\n            for child in element:\n                for _ in BaseElement._traverse(child, condition):\n                    yield _\n        elif attr.has(element.__class__):\n            for field in attr.fields(element.__class__):\n                child = getattr(element, field.name)\n                for _ in BaseElement._traverse(child, condition):\n                    yield _", "response": "Traverses the hierarchy of the elements in the specified element and yields the base class names that are found in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new PSD document.", "response": "def new(cls, mode, size, color=0, depth=8, **kwargs):\n        \"\"\"\n        Create a new PSD document.\n\n        :param mode: The color mode to use for the new image.\n        :param size: A tuple containing (width, height) in pixels.\n        :param color: What color to use for the image. Default is black.\n        :return: A :py:class:`~psd_tools.api.psd_image.PSDImage` object.\n        \"\"\"\n        header = cls._make_header(mode, size, depth)\n        image_data = ImageData.new(header, color=color, **kwargs)\n        # TODO: Add default metadata.\n        return cls(PSD(\n            header=header,\n            image_data=image_data,\n            image_resources=ImageResources.new(),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new PSD document from PIL Image object.", "response": "def frompil(cls, image, compression=Compression.PACK_BITS):\n        \"\"\"\n        Create a new PSD document from PIL Image.\n\n        :param image: PIL Image object.\n        :param compression: ImageData compression option. See\n            :py:class:`~psd_tools.constants.Compression`.\n        :return: A :py:class:`~psd_tools.api.psd_image.PSDImage` object.\n        \"\"\"\n        header = cls._make_header(image.mode, image.size)\n        # TODO: Add default metadata.\n        # TODO: Perhaps make this smart object.\n        image_data = ImageData(compression=compression)\n        image_data.set_data([channel.tobytes() for channel in image.split()],\n                            header)\n        return cls(PSD(\n            header=header,\n            image_data=image_data,\n            image_resources=ImageResources.new(),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a PSD document.", "response": "def open(cls, fp):\n        \"\"\"\n        Open a PSD document.\n\n        :param fp: filename or file-like object.\n        :return: A :py:class:`~psd_tools.api.psd_image.PSDImage` object.\n        \"\"\"\n        if hasattr(fp, 'read'):\n            self = cls(PSD.read(fp))\n        else:\n            with open(fp, 'rb') as f:\n                self = cls(PSD.read(f))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, fp, mode='wb'):\n        if hasattr(fp, 'write'):\n            self._record.write(fp)\n        else:\n            with open(fp, mode) as f:\n                self._record.write(f)", "response": "Save the PSD file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting PIL Image. :return: :py:class:`PIL.Image`, or `None` if the composed image is not available.", "response": "def topil(self, **kwargs):\n        \"\"\"\n        Get PIL Image.\n\n        :return: :py:class:`PIL.Image`, or `None` if the composed image is not\n            available.\n        \"\"\"\n        if self.has_preview():\n            return pil_io.convert_image_data_to_pil(self._record, **kwargs)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomposing the PSD image.", "response": "def compose(self, force=False, bbox=None, **kwargs):\n        \"\"\"\n        Compose the PSD image.\n\n        See :py:func:`~psd_tools.compose` for available extra arguments.\n\n        :param bbox: Viewport tuple (left, top, right, bottom).\n        :return: :py:class:`PIL.Image`, or `None` if there is no pixel.\n        \"\"\"\n        from psd_tools.api.composer import compose\n        image = None\n        if not force or len(self) == 0:\n            image = self.topil(**kwargs)\n        if image is None:\n            image = compose(\n                self, bbox=bbox or self.viewbox, force=force, **kwargs\n            )\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bbox(self):\n        bbox = super(PSDImage, self).bbox\n        if bbox == (0, 0, 0, 0):\n            bbox = self.viewbox\n        return bbox", "response": "Minimal bounding box that contains all the visible layers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef viewbox(self):\n        return self.left, self.top, self.right, self.bottom", "response": "Return bounding box of the viewport."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a thumbnail image in PIL. Image.", "response": "def thumbnail(self):\n        \"\"\"\n        Returns a thumbnail image in PIL.Image. When the file does not\n        contain an embedded thumbnail image, returns None.\n        \"\"\"\n        if 'THUMBNAIL_RESOURCE' in self.image_resources:\n            return pil_io.convert_thumbnail_to_pil(\n                self.image_resources.get_data('THUMBNAIL_RESOURCE')\n            )\n        elif 'THUMBNAIL_RESOURCE_PS4' in self.image_resources:\n            return pil_io.convert_thumbnail_to_pil(\n                self.image_resources.get_data('THUMBNAIL_RESOURCE_PS4'), 'BGR'\n            )\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_pattern(self, pattern_id):\n        for key in ('PATTERNS1', 'PATTERNS2', 'PATTERNS3'):\n            if key in self.tagged_blocks:\n                data = self.tagged_blocks.get_data(key)\n                for pattern in data:\n                    if pattern.pattern_id == pattern_id:\n                        return pattern\n        return None", "response": "Get pattern item by id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the layer structure.", "response": "def _init(self):\n        \"\"\"Initialize layer structure.\"\"\"\n        group_stack = [self]\n        clip_stack = []\n        last_layer = None\n\n        for record, channels in self._record._iter_layers():\n            current_group = group_stack[-1]\n\n            blocks = record.tagged_blocks\n            end_of_group = False\n            divider = blocks.get_data('SECTION_DIVIDER_SETTING', None)\n            divider = blocks.get_data('NESTED_SECTION_DIVIDER_SETTING',\n                                      divider)\n            if divider is not None:\n                if divider.kind == SectionDivider.BOUNDING_SECTION_DIVIDER:\n                    layer = Group(self, None, None, current_group)\n                    group_stack.append(layer)\n                elif divider.kind in (SectionDivider.OPEN_FOLDER,\n                                      SectionDivider.CLOSED_FOLDER):\n                    layer = group_stack.pop()\n                    assert layer is not self\n\n                    layer._record = record\n                    layer._channels = channels\n                    for key in (\n                        'ARTBOARD_DATA1', 'ARTBOARD_DATA2', 'ARTBOARD_DATA3'\n                    ):\n                        if key in blocks:\n                            layer = Artboard._move(layer)\n                    end_of_group = True\n            elif (\n                'TYPE_TOOL_OBJECT_SETTING' in blocks or\n                'TYPE_TOOL_INFO' in blocks\n            ):\n                layer = TypeLayer(self, record, channels, current_group)\n            elif (\n                record.flags.pixel_data_irrelevant and (\n                    'VECTOR_ORIGINATION_DATA' in blocks or\n                    'VECTOR_MASK_SETTING1' in blocks or\n                    'VECTOR_MASK_SETTING2' in blocks or\n                    'VECTOR_STROKE_DATA' in blocks or\n                    'VECTOR_STROKE_CONTENT_DATA' in blocks\n                )\n            ):\n                layer = ShapeLayer(self, record, channels, current_group)\n            elif (\n                'SMART_OBJECT_LAYER_DATA1' in blocks or\n                'SMART_OBJECT_LAYER_DATA2' in blocks or\n                'PLACED_LAYER1' in blocks or\n                'PLACED_LAYER2' in blocks\n            ):\n                layer = SmartObjectLayer(self, record, channels,\n                                         current_group)\n            else:\n                layer = None\n                for key in adjustments.TYPES.keys():\n                    if key in blocks:\n                        layer = adjustments.TYPES[key](\n                            self, record, channels, current_group\n                        )\n                        break\n                # If nothing applies, this is a pixel layer.\n                if layer is None:\n                    layer = PixelLayer(\n                        self, record, channels, current_group\n                    )\n\n            if record.clipping == Clipping.NON_BASE:\n                clip_stack.append(layer)\n            else:\n                if clip_stack:\n                    last_layer._clip_layers = clip_stack\n                clip_stack = []\n                if not end_of_group:\n                    current_group._layers.append(layer)\n                last_layer = layer\n\n        if clip_stack and last_layer:\n            last_layer._clip_layers = clip_stack"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts PIL mode to ColorMode.", "response": "def get_color_mode(mode):\n    \"\"\"Convert PIL mode to ColorMode.\"\"\"\n    name = mode.upper()\n    name = name.rstrip('A')  # Trim alpha.\n    name = {'1': 'BITMAP', 'L': 'GRAYSCALE'}.get(name, name)\n    return getattr(ColorMode, name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pil_mode(value, alpha=False):\n    name = {\n        'GRAYSCALE': 'L',\n        'BITMAP': '1',\n        'DUOTONE': 'L',\n        'INDEXED': 'P',\n    }.get(value, value)\n    if alpha and name in ('L', 'RGB'):\n        name += 'A'\n    return name", "response": "Get PIL mode from ColorMode."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert ImageData to PIL Image.", "response": "def convert_image_data_to_pil(psd, apply_icc=True, **kwargs):\n    \"\"\"Convert ImageData to PIL Image.\n\n    .. note:: Image resources contain extra alpha channels in these keys:\n        `ALPHA_NAMES_UNICODE`, `ALPHA_NAMES_PASCAL`, `ALPHA_IDENTIFIERS`.\n    \"\"\"\n    from PIL import Image, ImageOps\n    header = psd.header\n    size = (header.width, header.height)\n    channels = []\n    for channel_data in psd.image_data.get_data(header):\n        channels.append(_create_channel(size, channel_data, header.depth))\n    alpha = _get_alpha_use(psd)\n    mode = get_pil_mode(header.color_mode.name)\n    if mode == 'P':\n        image = Image.merge('L', channels[:get_pil_channels(mode)])\n        image.putpalette(psd.color_mode_data.interleave())\n    elif mode == 'MULTICHANNEL':\n        image = channels[0]  # Multi-channel mode is a collection of alpha.\n    else:\n        image = Image.merge(mode, channels[:get_pil_channels(mode)])\n    if mode == 'CMYK':\n        image = image.point(lambda x: 255 - x)\n    if apply_icc and 'ICC_PROFILE' in psd.image_resources:\n        image = _apply_icc(image, psd.image_resources.get_data('ICC_PROFILE'))\n    if alpha and mode in ('L', 'RGB'):\n        image.putalpha(channels[-1])\n    return _remove_white_background(image)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_layer_to_pil(layer, apply_icc=True, **kwargs):\n    from PIL import Image\n    header = layer._psd._record.header\n    if header.color_mode == ColorMode.BITMAP:\n        raise NotImplementedError\n    width, height = layer.width, layer.height\n    channels, alpha = [], None\n    for ci, cd in zip(layer._record.channel_info, layer._channels):\n        if ci.id in (ChannelID.USER_LAYER_MASK,\n                     ChannelID.REAL_USER_LAYER_MASK):\n            continue\n        channel = cd.get_data(width, height, header.depth, header.version)\n        channel_image = _create_channel(\n            (width, height), channel, header.depth\n        )\n        if ci.id == ChannelID.TRANSPARENCY_MASK:\n            alpha = channel_image\n        else:\n            channels.append(channel_image)\n    mode = get_pil_mode(header.color_mode.name)\n    channels = _check_channels(channels, header.color_mode)\n    image = Image.merge(mode, channels)\n    if mode == 'CMYK':\n        image = image.point(lambda x: 255 - x)\n    if alpha is not None:\n        if mode in ('RGB', 'L'):\n            image.putalpha(alpha)\n        else:\n            logger.debug('Alpha channel is not supported in %s' % (mode))\n\n    if apply_icc and 'ICC_PROFILE' in layer._psd.image_resources:\n        image = _apply_icc(\n            image, layer._psd.image_resources.get_data('ICC_PROFILE')\n        )\n    return image", "response": "Convert a Layer to PIL Image."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Mask to PIL Image.", "response": "def convert_mask_to_pil(mask, real=True):\n    \"\"\"Convert Mask to PIL Image.\"\"\"\n    from PIL import Image\n    header = mask._layer._psd._record.header\n    channel_ids = [ci.id for ci in mask._layer._record.channel_info]\n    if real and mask._has_real():\n        width = mask._data.real_right - mask._data.real_left\n        height = mask._data.real_bottom - mask._data.real_top\n        channel = mask._layer._channels[\n            channel_ids.index(ChannelID.REAL_USER_LAYER_MASK)\n        ]\n    else:\n        width = mask._data.right - mask._data.left\n        height = mask._data.bottom - mask._data.top\n        channel = mask._layer._channels[\n            channel_ids.index(ChannelID.USER_LAYER_MASK)\n        ]\n    data = channel.get_data(width, height, header.depth, header.version)\n    return _create_channel((width, height), data, header.depth)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_pattern_to_pil(pattern, version=1):\n    from PIL import Image\n    mode = get_pil_mode(pattern.image_mode.name, False)\n    # The order is different here.\n    size = pattern.data.rectangle[3], pattern.data.rectangle[2]\n    channels = [\n        _create_channel(size, c.get_data(version), c.pixel_depth).convert('L')\n        for c in pattern.data.channels if c.is_written\n    ]\n    if len(channels) == len(mode) + 1:\n        mode += 'A'  # TODO: Perhaps doesn't work for some modes.\n    if mode == 'P':\n        image = channels[0]\n        image.putpalette([x for rgb in pattern.color_table for x in rgb])\n    else:\n        image = Image.merge(mode, channels)\n    if mode == 'CMYK':\n        image = image.point(lambda x: 255 - x)\n    return image", "response": "Convert Pattern to PIL Image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_thumbnail_to_pil(thumbnail, mode='RGB'):\n    from PIL import Image\n    if thumbnail.fmt == 0:\n        size = (thumbnail.width, thumbnail.height)\n        stride = thumbnail.widthbytes\n        return Image.frombytes('RGBX', size, thumbnail.data, 'raw', mode,\n                                stride)\n    elif thumbnail.fmt == 1:\n        return Image.open(io.BytesIO(thumbnail.data))\n    else:\n        raise ValueError('Unknown thumbnail format %d' % (thumbnail.fmt))", "response": "Convert a thumbnail resource to PIL. Image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_icc(image, icc_profile):\n    from io import BytesIO\n    try:\n        from PIL import ImageCms\n    except ImportError:\n        logger.debug(\n            'ICC profile found but not supported. Install little-cms.'\n        )\n        return image\n\n    if image.mode not in ('RGB',):\n        logger.debug('%s ICC profile is not supported.' % image.mode)\n        return image\n\n    try:\n        in_profile = ImageCms.ImageCmsProfile(BytesIO(icc_profile))\n        out_profile = ImageCms.createProfile('sRGB')\n        return ImageCms.profileToProfile(image, in_profile, out_profile)\n    except ImageCms.PyCMSError as e:\n        logger.warning('PyCMSError: %s' % (e))\n\n    return image", "response": "Apply ICC Color profile to image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_white_background(image):\n    from PIL import ImageMath, Image\n    if image.mode == \"RGBA\":\n        bands = image.split()\n        a = bands[3]\n        rgb = [\n            ImageMath.eval(\n                'convert('\n                'float(x + a - 255) * 255.0 / float(max(a, 1)) * '\n                'float(min(a, 1)) + float(x) * float(1 - min(a, 1))'\n                ', \"L\")',\n                x=x, a=a\n            )\n            for x in bands[:3]\n        ]\n        return Image.merge(bands=rgb + [a], mode=\"RGBA\")\n\n    return image", "response": "Remove white background in the preview image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bbox(self):\n        return self.left, self.top, self.right, self.bottom", "response": "get the bounding box of the hierarchy"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompresses raw data. :param data: raw data bytes to write. :param compression: compression type, see :py:class:`.Compression`. :param width: width. :param height: height. :param depth: bit depth of the pixel. :param version: psd file version. :return: compressed data bytes.", "response": "def compress(data, compression, width, height, depth, version=1):\n    \"\"\"Compress raw data.\n\n    :param data: raw data bytes to write.\n    :param compression: compression type, see :py:class:`.Compression`.\n    :param width: width.\n    :param height: height.\n    :param depth: bit depth of the pixel.\n    :param version: psd file version.\n    :return: compressed data bytes.\n    \"\"\"\n    if compression == Compression.RAW:\n        result = data\n    elif compression == Compression.PACK_BITS:\n        result = encode_packbits(data, width, height, depth, version)\n    elif compression == Compression.ZIP:\n        result = zlib.compress(data)\n    else:\n        encoded = encode_prediction(data, width, height, depth)\n        result = zlib.compress(encoded)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decompress(data, compression, width, height, depth, version=1):\n    length = width * height * depth // 8\n\n    result = None\n    if compression == Compression.RAW:\n        result = data[:length]\n    elif compression == Compression.PACK_BITS:\n        result = decode_packbits(data, height, version)\n    elif compression == Compression.ZIP:\n        result = zlib.decompress(data)\n    else:\n        decompressed = zlib.decompress(data)\n        result = decode_prediction(decompressed, width, height, depth)\n\n    assert len(result) == length, 'len=%d, expected=%d' % (\n        len(result), length\n    )\n\n    return result", "response": "Decompress raw data.\n\n    :param data: compressed data bytes.\n    :param compression: compression type,\n            see :py:class:`~psd_tools.constants.Compression`.\n    :param width: width.\n    :param height: height.\n    :param depth: bit depth of the pixel.\n    :param version: psd file version.\n    :return: decompressed data bytes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_bbox(layers):\n    if not hasattr(layers, '__iter__'):\n        layers = [layers]\n    bboxes = [\n        layer.bbox for layer in layers\n        if layer.is_visible() and not layer.bbox == (0, 0, 0, 0)\n    ]\n    if len(bboxes) == 0:  # Empty bounding box.\n        return (0, 0, 0, 0)\n    lefts, tops, rights, bottoms = zip(*bboxes)\n    return (min(lefts), min(tops), max(rights), max(bottoms))", "response": "Extract the bounding box for a list of layers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomposing a list of layers to a single image.", "response": "def compose(layers, bbox=None, layer_filter=None, color=None, **kwargs):\n    \"\"\"\n    Compose layers to a single :py:class:`PIL.Image`.\n    If the layers do not have visible pixels, the function returns `None`.\n\n    Example::\n\n        image = compose([layer1, layer2])\n\n    In order to skip some layers, pass `layer_filter` function which\n    should take `layer` as an argument and return `True` to keep the layer\n    or return `False` to skip::\n\n        image = compose(\n            layers,\n            layer_filter=lambda x: x.is_visible() and x.kind == 'type'\n        )\n\n    By default, visible layers are composed.\n\n    .. note:: This function is experimental and does not guarantee\n        Photoshop-quality rendering.\n\n        Currently the following are ignored:\n\n         - Adjustments layers\n         - Layer effects\n         - Blending mode (all blending modes become normal)\n\n        Shape drawing is inaccurate if the PSD file is not saved with\n        maximum compatibility.\n\n    :param layers: a layer, or an iterable of layers.\n    :param bbox: (left, top, bottom, right) tuple that specifies a region to\n        compose. By default, all the visible area is composed. The origin\n        is at the top-left corner of the PSD document.\n    :param layer_filter: a callable that takes a layer and returns `bool`.\n    :param color: background color in `int` or `tuple`.\n    :return: :py:class:`PIL.Image` or `None`.\n    \"\"\"\n    from PIL import Image\n\n    if not hasattr(layers, '__iter__'):\n        layers = [layers]\n\n    def _default_filter(layer):\n        return layer.is_visible()\n\n    layer_filter = layer_filter or _default_filter\n    valid_layers = [x for x in layers if layer_filter(x)]\n    if len(valid_layers) == 0:\n        return None\n\n    if bbox is None:\n        bbox = extract_bbox(valid_layers)\n        if bbox == (0, 0, 0, 0):\n            return None\n\n    # Alpha must be forced to correctly blend.\n    mode = get_pil_mode(valid_layers[0]._psd.color_mode, True)\n    result = Image.new(\n        mode, (bbox[2] - bbox[0], bbox[3] - bbox[1]),\n        color=color if color is not None else 'white',\n    )\n    result.putalpha(0)\n\n    for layer in valid_layers:\n        if intersect(layer.bbox, bbox) == (0, 0, 0, 0):\n            continue\n\n        image = layer.compose(**kwargs)\n        if image is None:\n            continue\n\n        logger.debug('Composing %s' % layer)\n        offset = (layer.left - bbox[0], layer.top - bbox[1])\n        result = _blend(result, image, offset)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compose_layer(layer, force=False, **kwargs):\n    from PIL import Image, ImageChops\n    assert layer.bbox != (0, 0, 0, 0), 'Layer bbox is (0, 0, 0, 0)'\n\n    image = layer.topil(**kwargs)\n    if image is None or force:\n        texture = create_fill(layer)\n        if texture is not None:\n            image = texture\n\n    if image is None:\n        return image\n\n    # TODO: Group should have the following too.\n\n    # Apply mask.\n    if layer.has_mask() and not layer.mask.disabled:\n        mask_bbox = layer.mask.bbox\n        if (\n            (mask_bbox[2] - mask_bbox[0]) > 0 and\n            (mask_bbox[3] - mask_bbox[1]) > 0\n        ):\n            color = layer.mask.background_color\n            offset = (mask_bbox[0] - layer.left, mask_bbox[1] - layer.top)\n            mask = Image.new('L', image.size, color=color)\n            mask.paste(layer.mask.topil(), offset)\n            if image.mode.endswith('A'):\n                # What should we do here? There are two alpha channels.\n                pass\n            image.putalpha(mask)\n    elif layer.has_vector_mask() and (force or not layer.has_pixels()):\n        mask = draw_vector_mask(layer)\n        # TODO: Stroke drawing.\n        texture = image\n        image = Image.new(image.mode, image.size, 'white')\n        image.paste(texture, mask=mask)\n\n    # Apply layer fill effects.\n    apply_effect(layer, image)\n\n    # Clip layers.\n    if layer.has_clip_layers():\n        clip_box = extract_bbox(layer.clip_layers)\n        inter_box = intersect(layer.bbox, clip_box)\n        if inter_box != (0, 0, 0, 0):\n            clip_image = compose(layer.clip_layers, bbox=layer.bbox)\n            mask = image.getchannel('A')\n            if clip_image.mode.endswith('A'):\n                mask = ImageChops.multiply(clip_image.getchannel('A'), mask)\n            clip_image.putalpha(mask)\n            image = _blend(image, clip_image, (0, 0))\n\n    # Apply opacity.\n    if layer.opacity < 255:\n        opacity = layer.opacity\n        if image.mode.endswith('A'):\n            opacity = opacity / 255.\n            channels = list(image.split())\n            channels[-1] = channels[-1].point(lambda x: int(x * opacity))\n            image = Image.merge(image.mode, channels)\n        else:\n            image.putalpha(opacity)\n\n    return image", "response": "Compose a single layer with pixels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_effect(layer, image):\n    for effect in layer.effects:\n        if effect.__class__.__name__ == 'PatternOverlay':\n            draw_pattern_fill(image, layer._psd, effect.value)\n\n    for effect in layer.effects:\n        if effect.__class__.__name__ == 'GradientOverlay':\n            draw_gradient_fill(image, effect.value)\n\n    for effect in layer.effects:\n        if effect.__class__.__name__ == 'ColorOverlay':\n            draw_solid_color_fill(image, effect.value)", "response": "Apply effects to the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_symbol(path, width, height, command='C'):\n    if len(path) == 0:\n        return\n\n    # Initial point.\n    yield 'M'\n    yield path[0].anchor[1] * width\n    yield path[0].anchor[0] * height\n    yield command\n\n    # Closed path or open path\n    points = (zip(path, path[1:] + path[0:1]) if path.is_closed()\n              else zip(path, path[1:]))\n\n    # Rest of the points.\n    for p1, p2 in points:\n        yield p1.leaving[1] * width\n        yield p1.leaving[0] * height\n        yield p2.preceding[1] * width\n        yield p2.preceding[0] * height\n        yield p2.anchor[1] * width\n        yield p2.anchor[0] * height\n\n    if path.is_closed():\n        yield 'Z'", "response": "Generate a sequence of symbols that can be used to generate SVG path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw pattern fill on the image.", "response": "def draw_pattern_fill(image, psd, setting, blend=True):\n    \"\"\"\n    Draw pattern fill on the image.\n\n    :param image: Image to be filled.\n    :param psd: :py:class:`PSDImage`.\n    :param setting: Descriptor containing pattern fill.\n    :param blend: Blend the fill or ignore. Effects blend.\n    \"\"\"\n    from PIL import Image\n    pattern_id = setting[b'Ptrn'][b'Idnt'].value.rstrip('\\x00')\n    pattern = psd._get_pattern(pattern_id)\n    if not pattern:\n        logger.error('Pattern not found: %s' % (pattern_id))\n        return None\n    panel = convert_pattern_to_pil(pattern, psd._record.header.version)\n\n    scale = setting.get(b'Scl ', 100) / 100.\n    if scale != 1.:\n        panel = panel.resize((\n            int(panel.width * scale),\n            int(panel.height * scale)\n        ))\n\n    opacity = int(setting.get(b'Opct', 100) / 100. * 255)\n    if opacity != 255:\n        panel.putalpha(opacity)\n\n    pattern_image = Image.new(image.mode, image.size)\n    mask = image.getchannel('A') if blend else Image.new('L', image.size, 255)\n\n    for left in range(0, pattern_image.width, panel.width):\n        for top in range(0, pattern_image.height, panel.height):\n            panel_mask = mask.crop(\n                (left, top, left + panel.width, top + panel.height)\n            )\n            pattern_image.paste(panel, (left, top), panel_mask)\n\n    if blend:\n        image.paste(_blend(image, pattern_image, (0, 0)))\n    else:\n        image.paste(pattern_image)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_linear_gradient(width, height, angle=90.):\n    import numpy as np\n    X, Y = np.meshgrid(np.linspace(0, 1, width), np.linspace(0, 1, height))\n    theta = np.radians(angle % 360)\n    c, s = np.cos(theta), np.sin(theta)\n    if 0 <= theta and theta < 0.5 * np.pi:\n        Z = np.abs(c * X + s * Y)\n    elif 0.5 * np.pi <= theta and theta < np.pi:\n        Z = np.abs(c * (X - width) + s * Y)\n    elif np.pi <= theta and theta < 1.5 * np.pi:\n        Z = np.abs(c * (X - width) + s * (Y - height))\n    elif 1.5 * np.pi <= theta and theta < 2.0 * np.pi:\n        Z = np.abs(c * X + s * (Y - height))\n    return (Z - Z.min()) / (Z.max() - Z.min())", "response": "Generates index map for linear gradients."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the bounding box of the knots in relative coordinates.", "response": "def bbox(self):\n        \"\"\"\n        Bounding box tuple (left, top, right, bottom) in relative coordinates,\n        where top-left corner is (0., 0.) and bottom-right corner is (1., 1.).\n\n        :return: `tuple`\n        \"\"\"\n        from itertools import chain\n        knots = [\n            (knot.anchor[1], knot.anchor[0])\n            for knot in chain.from_iterable(self.paths)\n        ]\n        if len(knots) == 0:\n            return (0., 0., 1., 1.)\n        x, y = zip(*knots)\n        return (min(x), min(y), max(x), max(y))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncap type one of butt round square.", "response": "def line_cap_type(self):\n        \"\"\"Cap type, one of `butt`, `round`, `square`.\"\"\"\n        key = self._data.get(b'strokeStyleLineCapType').enum\n        return self.STROKE_STYLE_LINE_CAP_TYPES.get(key, str(key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\njoins type one of miter round bevel.", "response": "def line_join_type(self):\n        \"\"\"Join type, one of `miter`, `round`, `bevel`.\"\"\"\n        key = self._data.get(b'strokeStyleLineJoinType').enum\n        return self.STROKE_STYLE_LINE_JOIN_TYPES.get(key, str(key))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef line_alignment(self):\n        key = self._data.get(b'strokeStyleLineAlignment').enum\n        return self.STROKE_STYLE_LINE_ALIGNMENTS.get(key, str(key))", "response": "Alignment one of inner outer center."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the bounding box of the live shape.", "response": "def bbox(self):\n        \"\"\"\n        Bounding box of the live shape.\n\n        :return: :py:class:`~psd_tools.psd.descriptor.Descriptor`\n        \"\"\"\n        bbox = self._data.get(b'keyOriginShapeBBox')\n        if bbox:\n            return (\n                bbox.get(b'Left').value,\n                bbox.get(b'Top ').value,\n                bbox.get(b'Rght').value,\n                bbox.get(b'Btom').value,\n            )\n        return (0, 0, 0, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the layer has associated pixels.", "response": "def has_pixels(self):\n        \"\"\"\n        Returns True if the layer has associated pixels. When this is True,\n        `topil` method returns :py:class:`PIL.Image`.\n\n        :return: `bool`\n        \"\"\"\n        return any(\n            ci.id >= 0 and cd.data and len(cd.data) > 0\n            for ci, cd in zip(self._record.channel_info, self._channels)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mask(self):\n        if not hasattr(self, \"_mask\"):\n            self._mask = Mask(self) if self.has_mask() else None\n        return self._mask", "response": "Returns the mask associated with this layer or None if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns vector mask associated with this layer.", "response": "def vector_mask(self):\n        \"\"\"\n        Returns vector mask associated with this layer.\n\n        :return: :py:class:`~psd_tools.api.shape.VectorMask` or `None`\n        \"\"\"\n        if not hasattr(self, '_vector_mask'):\n            self._vector_mask = None\n            blocks = self.tagged_blocks\n            for key in ('VECTOR_MASK_SETTING1', 'VECTOR_MASK_SETTING2'):\n                if key in blocks:\n                    self._vector_mask = VectorMask(blocks.get_data(key))\n        return self._vector_mask"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef origination(self):\n        if not hasattr(self, '_origination'):\n            data = self.tagged_blocks.get_data('VECTOR_ORIGINATION_DATA', {})\n            self._origination = [\n                Origination.create(x) for x\n                in data.get(b'keyDescriptorList', [])\n                if not data.get(b'keyShapeInvalidated')\n            ]\n        return self._origination", "response": "Property for a list of live shapes or a line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compose(self, *args, **kwargs):\n        from psd_tools.api.composer import compose_layer\n        if self.bbox == (0, 0, 0, 0):\n            return None\n        return compose_layer(self, *args, **kwargs)", "response": "Compose the layer and masks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef effects(self):\n        if not hasattr(self, '_effects'):\n            self._effects = Effects(self)\n        return self._effects", "response": "Return a new object of type Effects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bbox(self):\n        if not hasattr(self, '_bbox'):\n            self._bbox = extract_bbox(self)\n        return self._bbox", "response": "get the bounding box of the current image"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compose(self, **kwargs):\n        from psd_tools.api.composer import compose\n        return compose(self, **kwargs)", "response": "Compose layer and masks and clipping layers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator to iterate over all descendant layers.", "response": "def descendants(self, include_clip=True):\n        \"\"\"\n        Return a generator to iterate over all descendant layers.\n\n        Example::\n\n            # Iterate over all layers\n            for layer in psd.descendants():\n                print(layer)\n\n            # Iterate over all layers in reverse order\n            for layer in reversed(list(psd.descendants())):\n                print(layer)\n\n        :param include_clip: include clipping layers.\n        \"\"\"\n        for layer in self:\n            yield layer\n            if layer.is_group():\n                for child in layer.descendants(include_clip):\n                    yield child\n            if include_clip and hasattr(layer, 'clip_layers'):\n                for clip_layer in layer.clip_layers:\n                    yield clip_layer"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the bounding box of the current object.", "response": "def bbox(self):\n        \"\"\"(left, top, right, bottom) tuple.\"\"\"\n        if not hasattr(self, '_bbox'):\n            data = None\n            for key in ('ARTBOARD_DATA1', 'ARTBOARD_DATA2', 'ARTBOARD_DATA3'):\n                if key in self.tagged_blocks:\n                    data = self.tagged_blocks.get_data(key)\n            assert data is not None\n            rect = data.get(b'artboardRect')\n            self._bbox = (\n                int(rect.get(b'Left')),\n                int(rect.get(b'Top ')),\n                int(rect.get(b'Rght')),\n                int(rect.get(b'Btom')),\n            )\n        return self._bbox"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompose the artboard. See :py:func:`~psd_tools.compose` for available extra arguments. :param bbox: Viewport tuple (left, top, right, bottom). :return: :py:class:`PIL.Image`, or `None` if there is no pixel.", "response": "def compose(self, bbox=None, **kwargs):\n        \"\"\"\n        Compose the artboard.\n\n        See :py:func:`~psd_tools.compose` for available extra arguments.\n\n        :param bbox: Viewport tuple (left, top, right, bottom).\n        :return: :py:class:`PIL.Image`, or `None` if there is no pixel.\n        \"\"\"\n        from psd_tools.api.composer import compose\n        return compose(self, bbox=bbox or self.bbox, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the associated smart object.", "response": "def smart_object(self):\n        \"\"\"\n        Associated smart object.\n\n        :return: :py:class:`~psd_tools.api.smart_object.SmartObject`.\n        \"\"\"\n        if not hasattr(self, '_smart_object'):\n            self._smart_object = SmartObject(self)\n        return self._smart_object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bbox(self):\n        if not hasattr(self, '_bbox'):\n            if self.has_pixels():\n                self._bbox = (\n                    self._record.left,\n                    self._record.top,\n                    self._record.right,\n                    self._record.bottom,\n                )\n            elif self.has_origination() and not any(\n                x.invalidated for x in self.origination\n            ):\n                lefts, tops, rights, bottoms = zip(*[\n                    x.bbox for x in self.origination\n                ])\n                self._bbox = (\n                    int(min(lefts)), int(min(tops)),\n                    int(max(rights)), int(max(bottoms))\n                )\n            elif self.has_vector_mask():\n                bbox = self.vector_mask.bbox\n                self._bbox = (\n                    int(bbox[0] * self._psd.width),\n                    int(bbox[1] * self._psd.height),\n                    int(bbox[2] * self._psd.width),\n                    int(bbox[3] * self._psd.height),\n                )\n            else:\n                self._bbox = (0, 0, 0, 0)\n        return self._bbox", "response": "Returns the bounding box of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading data from fp according to fmt.", "response": "def read_fmt(fmt, fp):\n    \"\"\"\n    Reads data from ``fp`` according to ``fmt``.\n    \"\"\"\n    fmt = str(\">\" + fmt)\n    fmt_size = struct.calcsize(fmt)\n    data = fp.read(fmt_size)\n    assert len(data) == fmt_size, 'read=%d, expected=%d' % (\n        len(data), fmt_size\n    )\n    return struct.unpack(fmt, data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_fmt(fp, fmt, *args):\n    fmt = str(\">\" + fmt)\n    fmt_size = struct.calcsize(fmt)\n    written = write_bytes(fp, struct.pack(fmt, *args))\n    assert written == fmt_size, 'written=%d, expected=%d' % (\n        written, fmt_size\n    )\n    return written", "response": "Writes data to fp according to fmt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_bytes(fp, data):\n    pos = fp.tell()\n    fp.write(data)\n    written = fp.tell() - pos\n    assert written == len(data), 'written=%d, expected=%d' % (\n        written, len(data)\n    )\n    return written", "response": "Writes data to the file object and returns the number of bytes written."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a block of data with a length marker at the beginning.", "response": "def read_length_block(fp, fmt='I', padding=1):\n    \"\"\"\n    Read a block of data with a length marker at the beginning.\n\n    :param fp: file-like\n    :param fmt: format of the length marker\n    :return: bytes object\n    \"\"\"\n    length = read_fmt(fmt, fp)[0]\n    data = fp.read(length)\n    assert len(data) == length, (len(data), length)\n    read_padding(fp, length, padding)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a block of data with a length marker at the beginning.", "response": "def write_length_block(fp, writer, fmt='I', padding=1, **kwargs):\n    \"\"\"\n    Writes a block of data with a length marker at the beginning.\n\n    Example::\n\n        with io.BytesIO() as fp:\n            write_length_block(fp, lambda f: f.write(b'\\x00\\x00'))\n\n    :param fp: file-like\n    :param writer: function object that takes file-like object as an argument\n    :param fmt: format of the length marker\n    :param padding: divisor for padding not included in length marker\n    :return: written byte size\n    \"\"\"\n    length_position = reserve_position(fp, fmt)\n    written = writer(fp, **kwargs)\n    written += write_position(fp, length_position, written, fmt)\n    written += write_padding(fp, written, padding)\n    return written"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreserve the current position for write.", "response": "def reserve_position(fp, fmt='I'):\n    \"\"\"\n    Reserves the current position for write.\n\n    Use with `write_position`.\n\n    :param fp: file-like object\n    :param fmt: format of the reserved position\n    :return: the position\n    \"\"\"\n    position = fp.tell()\n    fp.seek(struct.calcsize(str('>' + fmt)), 1)\n    return position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a value to the specified position.", "response": "def write_position(fp, position, value, fmt='I'):\n    \"\"\"\n    Writes a value to the specified position.\n\n    :param fp: file-like object\n    :param position: position of the value marker\n    :param value: value to write\n    :param fmt: format of the value\n    :return: written byte size\n    \"\"\"\n    current_position = fp.tell()\n    fp.seek(position)\n    written = write_bytes(fp, struct.pack(str('>' + fmt), value))\n    fp.seek(current_position)\n    return written"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_padding(fp, size, divisor=2):\n    remainder = size % divisor\n    if remainder:\n        return fp.read(divisor - remainder)\n    return b''", "response": "Read padding bytes for the given byte size."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_padding(fp, size, divisor=2):\n    remainder = size % divisor\n    if remainder:\n        return write_bytes(fp, struct.pack('%dx' % (divisor - remainder)))\n    return 0", "response": "Writes padding bytes given the currently written size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_readable(fp, size=1):\n    read_size = len(fp.read(size))\n    fp.seek(-read_size, 1)\n    return read_size == size", "response": "Check if the file - like object is readable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_pascal_string(fp, encoding='macroman', padding=2):\n    start_pos = fp.tell()\n    # read_length_block doesn't work for a byte.\n    length = read_fmt('B', fp)[0]\n    data = fp.read(length)\n    assert len(data) == length, (len(data), length)\n    read_padding(fp, fp.tell() - start_pos, padding)\n    return data.decode(encoding)", "response": "Reads pascal string (length + bytes).\n\n    :param fp: file-like object\n    :param encoding: string encoding\n    :param padding: padding size\n    :return: str"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_be_array(fmt, count, fp):\n    arr = array.array(str(fmt))\n    if hasattr(arr, 'frombytes'):\n        arr.frombytes(fp.read(count * arr.itemsize))\n    else:\n        arr.fromstring(fp.read(count * arr.itemsize))\n    return fix_byteorder(arr)", "response": "Reads an array from a file with big - endian data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading an array from bytestring with big - endian data.", "response": "def be_array_from_bytes(fmt, data):\n    \"\"\"\n    Reads an array from bytestring with big-endian data.\n    \"\"\"\n    arr = array.array(str(fmt), data)\n    return fix_byteorder(arr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef be_array_to_bytes(arr):\n    data = fix_byteorder(arr)\n    if hasattr(arr, 'tobytes'):\n        return data.tobytes()\n    else:\n        return data.tostring()", "response": "Writes an array to bytestring with big - endian data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an empty dict and a register decorator.", "response": "def new_registry(attribute=None):\n    \"\"\"\n    Returns an empty dict and a @register decorator.\n    \"\"\"\n    registry = {}\n\n    def register(key):\n        def decorator(func):\n            registry[key] = func\n            if attribute:\n                setattr(func, attribute, key)\n            return func\n        return decorator\n\n    return registry, register"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop():\n    global global_server\n    if global_server is not None:\n        ioloop = global_server.ioloop\n        def stop_ioloop():\n            ioloop.stop()\n            ioloop.close()\n        global_server.ioloop.add_callback(stop_ioloop)\n        global_server = None", "response": "Stop the server invalidating any viewer URLs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister callback to run in the server event loop thread.", "response": "def defer_callback(callback, *args, **kwargs):\n    \"\"\"Register `callback` to run in the server event loop thread.\"\"\"\n    start()\n    global_server.ioloop.add_callback(lambda: callback(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes a list of successive downsampling factors for a given size.", "response": "def compute_near_isotropic_downsampling_scales(size,\n                                               voxel_size,\n                                               dimensions_to_downsample,\n                                               max_scales=DEFAULT_MAX_DOWNSAMPLING_SCALES,\n                                               max_downsampling=DEFAULT_MAX_DOWNSAMPLING,\n                                               max_downsampled_size=DEFAULT_MAX_DOWNSAMPLED_SIZE):\n    \"\"\"Compute a list of successive downsampling factors.\"\"\"\n\n    num_dims = len(voxel_size)\n    cur_scale = np.ones((num_dims, ), dtype=int)\n    scales = [tuple(cur_scale)]\n    while (len(scales) < max_scales and (np.prod(cur_scale) < max_downsampling) and\n           (size / cur_scale).max() > max_downsampled_size):\n        # Find dimension with smallest voxelsize.\n        cur_voxel_size = cur_scale * voxel_size\n        smallest_cur_voxel_size_dim = dimensions_to_downsample[np.argmin(cur_voxel_size[\n            dimensions_to_downsample])]\n        cur_scale[smallest_cur_voxel_size_dim] *= 2\n        target_voxel_size = cur_voxel_size[smallest_cur_voxel_size_dim] * 2\n        for d in dimensions_to_downsample:\n            if d == smallest_cur_voxel_size_dim:\n                continue\n            d_voxel_size = cur_voxel_size[d]\n            if abs(d_voxel_size - target_voxel_size) > abs(d_voxel_size * 2 - target_voxel_size):\n                cur_scale[d] *= 2\n        scales.append(tuple(cur_scale))\n    return scales"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_two_dimensional_near_isotropic_downsampling_scales(\n        size,\n        voxel_size,\n        max_scales=float('inf'),\n        max_downsampling=DEFAULT_MAX_DOWNSAMPLING,\n        max_downsampled_size=DEFAULT_MAX_DOWNSAMPLED_SIZE):\n    \"\"\"Compute a list of successive downsampling factors for 2-d tiles.\"\"\"\n\n    max_scales = min(max_scales, 10)\n\n    # First compute a set of 2-d downsamplings for XY, XZ, and YZ with a high\n    # number of max_scales, and ignoring other criteria.\n    scales_transpose = [\n        compute_near_isotropic_downsampling_scales(\n            size=size,\n            voxel_size=voxel_size,\n            dimensions_to_downsample=dimensions_to_downsample,\n            max_scales=max_scales,\n            max_downsampling=float('inf'),\n            max_downsampled_size=0, ) for dimensions_to_downsample in [[0, 1], [0, 2], [1, 2]]\n    ]\n\n    # Truncate all list of scales to the same length, once the stopping criteria\n    # is reached for all values of dimensions_to_downsample.\n    scales = [((1, ) * 3, ) * 3]\n    size = np.array(size)\n\n    def scale_satisfies_criteria(scale):\n        return np.prod(scale) < max_downsampling and (size / scale).max() > max_downsampled_size\n\n    for i in range(1, max_scales):\n        cur_scales = tuple(scales_transpose[d][i] for d in range(3))\n        if all(not scale_satisfies_criteria(scale) for scale in cur_scales):\n            break\n        scales.append(cur_scales)\n    return scales", "response": "Compute a list of successive downsampling factors for 2 - d tiles."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _on_state_changed(self):\n        raw_state, generation = self.state.raw_state_and_generation\n        if generation != self._last_generation:\n            self._last_generation = generation\n            self._send_update(raw_state, generation)", "response": "Invoked when the viewer state changes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a future that maps the result of future by func.", "response": "def future_then_immediate(future, func):\n    \"\"\"Returns a future that maps the result of `future` by `func`.\n\n    If `future` succeeds, sets the result of the returned future to `func(future.result())`.  If\n    `future` fails or `func` raises an exception, the exception is stored in the returned future.\n\n    If `future` has not yet finished, `func` is invoked by the same thread that finishes it.\n    Otherwise, it is invoked immediately in the same thread that calls `future_then_immediate`.\n    \"\"\"\n    result = concurrent.futures.Future()\n\n    def on_done(f):\n        try:\n            result.set_result(func(f.result()))\n        except Exception as e:\n            result.set_exception(e)\n\n    future.add_done_callback(on_done)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_on_new_thread(func, daemon=True):\n    f = concurrent.futures.Future()\n    def wrapper():\n        if not f.set_running_or_notify_cancel():\n            return\n        try:\n            f.set_result(func())\n        except Exception as e:\n            f.set_exception(e)\n    t = threading.Thread(target=wrapper)\n    t.daemon = daemon\n    t.start()\n    return f", "response": "Calls func from a new thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef downsample_with_averaging(array, factor):\n    factor = tuple(factor)\n    output_shape = tuple(int(math.ceil(s / f)) for s, f in zip(array.shape, factor))\n    temp = np.zeros(output_shape, dtype=np.float32)\n    counts = np.zeros(output_shape, np.int)\n    for offset in np.ndindex(factor):\n        part = array[tuple(np.s_[o::f] for o, f in zip(offset, factor))]\n        indexing_expr = tuple(np.s_[:s] for s in part.shape)\n        temp[indexing_expr] += part\n        counts[indexing_expr] += 1\n    return np.cast[array.dtype](temp / counts)", "response": "Downsample x by factor using averaging.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds and returns the root of the set containing obj.", "response": "def _get_representative(self, obj):\n        \"\"\"Finds and returns the root of the set containing `obj`.\"\"\"\n\n        if obj not in self._parents:\n            self._parents[obj] = obj\n            self._weights[obj] = 1\n            self._prev_next[obj] = [obj, obj]\n            self._min_values[obj] = obj\n            return obj\n\n        path = [obj]\n        root = self._parents[obj]\n        while root != path[-1]:\n            path.append(root)\n            root = self._parents[root]\n\n        # compress the path and return\n        for ancestor in path:\n            self._parents[ancestor] = root\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef union(self, *args):\n        if self._readonly:\n            raise AttributeError\n\n        if len(args) == 0:\n            return None\n        if len(args) == 1:\n            return self[args[0]]\n        for a, b in zip(args[:-1], args[1:]):\n            result = self._union_pair(a, b)\n        return result", "response": "Unions the equivalence classes containing the elements in args."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield the members of the equivalence class containing x.", "response": "def members(self, x):\n        \"\"\"Yields the members of the equivalence class containing `x`.\"\"\"\n        if x not in self._parents:\n            yield x\n            return\n        cur_x = x\n        while True:\n            yield cur_x\n            cur_x = self._prev_next[cur_x][1]\n            if cur_x == x:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sets(self):\n        sets = {}\n        for x in self._parents:\n            sets.setdefault(self[x], set()).add(x)\n        return frozenset(frozenset(v) for v in six.viewvalues(sets))", "response": "Returns the equivalence classes as a set of sets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_json(self):\n        sets = self.sets()\n        return sorted(sorted(x) for x in sets)", "response": "Returns the equivalence classes a sorted list of sorted lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the equivalence class containing x.", "response": "def delete_set(self, x):\n        \"\"\"Removes the equivalence class containing `x`.\"\"\"\n        if x not in self._parents:\n            return\n        members = list(self.members(x))\n        for v in members:\n            del self._parents[v]\n            del self._weights[v]\n            del self._prev_next[v]\n            del self._min_values[v]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isolate_element(self, x):\n        members = list(self.members(x))\n        self.delete_set(x)\n        self.union(*(v for v in members if v != x))", "response": "Isolates x from its equivalence class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_edge_from_heap(self, segment_ids):\n        self._initialize_heap()\n        key = normalize_edge(segment_ids)\n        if key in self.edge_map:\n            self.edge_map[key][0] = None\n            self.num_valid_edges -= 1", "response": "Removes an edge from the heap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef txn(self, overwrite=False, lock=True):\n        if lock:\n            self._lock.acquire()\n        try:\n            new_state, existing_generation = self.state_and_generation\n            new_state = copy.deepcopy(new_state)\n            yield new_state\n            if overwrite:\n                existing_generation = None\n            self.set_state(new_state, existing_generation=existing_generation)\n        finally:\n            if lock:\n                self._lock.release()", "response": "Context manager for a state modification transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invalidate(self):\n        with self._mesh_generator_lock:\n            self._mesh_generator_pending = None\n            self._mesh_generator = None\n        self._dispatch_changed_callbacks()", "response": "Mark the data invalidated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_md(p, *vsheets):\n    'pipe tables compatible with org-mode'\n    with p.open_text(mode='w') as fp:\n        for vs in vsheets:\n            if len(vsheets) > 1:\n                fp.write('# %s\\n\\n' % vs.name)\n            fp.write('|' + '|'.join('%-*s' % (col.width or options.default_width, markdown_escape(col.name)) for col in vs.visibleCols) + '|\\n')\n            fp.write('|' + '+'.join(markdown_colhdr(col) for col in vs.visibleCols) + '|\\n')\n\n            for row in Progress(vs.rows, 'saving'):\n                fp.write('|' + '|'.join('%-*s' % (col.width or options.default_width, markdown_escape(col.getDisplayValue(row))) for col in vs.visibleCols) + '|\\n')\n            fp.write('\\n')\n\n    status('%s save finished' % p)", "response": "pipe tables compatible with org - mode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexpanding all visible columns of containers to the given depth", "response": "def expand_cols_deep(sheet, cols, row, depth=0):  # depth == 0 means drill all the way\n    'expand all visible columns of containers to the given depth (0=fully)'\n    ret = []\n    for col in cols:\n        newcols = _addExpandedColumns(col, row, sheet.columns.index(col))\n        if depth != 1:  # countdown not yet complete, or negative (indefinite)\n            ret.extend(expand_cols_deep(sheet, newcols, row, depth-1))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn Sheet object of appropriate type for given sources in args.", "response": "def load_pyobj(name, pyobj):\n    'Return Sheet object of appropriate type for given sources in `args`.'\n    if isinstance(pyobj, list) or isinstance(pyobj, tuple):\n        if getattr(pyobj, '_fields', None):  # list of namedtuple\n            return SheetNamedTuple(name, pyobj)\n        else:\n            return SheetList(name, pyobj)\n    elif isinstance(pyobj, dict):\n        return SheetDict(name, pyobj)\n    elif isinstance(pyobj, object):\n        return SheetObject(name, pyobj)\n    else:\n        error(\"cannot load '%s' as pyobj\" % type(pyobj).__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getPublicAttrs(obj):\n    'Return all public attributes (not methods or `_`-prefixed) on object.'\n    return [k for k in dir(obj) if not k.startswith('_') and not callable(getattr(obj, k))]", "response": "Return all public attributes not methods or _ - prefixed on object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn columns for each public attribute on an object.", "response": "def PyobjColumns(obj):\n    'Return columns for each public attribute on an object.'\n    return [ColumnAttr(k, type(getattr(obj, k))) for k in getPublicAttrs(obj)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DictKeyColumns(d):\n    'Return a list of Column objects from dictionary keys.'\n    return [ColumnItem(k, k, type=deduceType(d[k])) for k in d.keys()]", "response": "Return a list of Column objects from dictionary keys."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SheetList(name, src, **kwargs):\n    'Creates a Sheet from a list of homogenous dicts or namedtuples.'\n\n    if not src:\n        status('no content in ' + name)\n        return\n\n    if isinstance(src[0], dict):\n        return ListOfDictSheet(name, source=src, **kwargs)\n    elif isinstance(src[0], tuple):\n        if getattr(src[0], '_fields', None):  # looks like a namedtuple\n            return ListOfNamedTupleSheet(name, source=src, **kwargs)\n\n    # simple list\n    return ListOfPyobjSheet(name, source=src, **kwargs)", "response": "Creates a Sheet from a list of homogenous dicts or namedtuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating histrow for each row and then reverse - sort by length.", "response": "def reload(self):\n        'Generate histrow for each row and then reverse-sort by length.'\n        self.rows = []\n\n#        if len(self.origCols) == 1 and self.origCols[0].type in (int, float, currency):\n#            self.numericBinning()\n#        else:\n        self.discreteBinning()\n\n        # automatically add cache to all columns now that everything is binned\n        for c in self.nonKeyVisibleCols:\n            c._cachedValues = collections.OrderedDict()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetects available clipboard util and return cmdline to copy data to the system clipboard.", "response": "def detect_command(cmdlist):\n    '''Detect available clipboard util and return cmdline to copy data to the system clipboard.\n    cmddict is list of (platform, progname, argstr).'''\n\n    for platform, command, args in cmdlist:\n        if platform is None or sys.platform == platform:\n            path = shutil.which(command)\n            if path:\n                return ' '.join([path, args])\n\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef saveToClipboard(sheet, rows, filetype=None):\n    'copy rows from sheet to system clipboard'\n    filetype = filetype or options.save_filetype\n    vs = copy(sheet)\n    vs.rows = rows\n    status('copying rows to clipboard')\n    clipboard().save(vs, filetype)", "response": "copy rows from sheet to system clipboard"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy a cell to the system clipboard.", "response": "def copy(self, value):\n        'Copy a cell to the system clipboard.'\n\n        with tempfile.NamedTemporaryFile() as temp:\n            with open(temp.name, 'w', encoding=options.encoding) as fp:\n                fp.write(str(value))\n\n            p = subprocess.Popen(\n                self.command,\n                stdin=open(temp.name, 'r', encoding=options.encoding),\n                stdout=subprocess.DEVNULL)\n            p.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying rows to the system clipboard.", "response": "def save(self, vs, filetype):\n        'Copy rows to the system clipboard.'\n\n        # use NTF to generate filename and delete file on context exit\n        with tempfile.NamedTemporaryFile(suffix='.'+filetype) as temp:\n            saveSheets(temp.name, vs)\n            sync(1)\n            p = subprocess.Popen(\n                self.command,\n                stdin=open(temp.name, 'r', encoding=options.encoding),\n                stdout=subprocess.DEVNULL,\n                close_fds=True)\n            p.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef amendPrevious(self, targethash):\n        'amend targethash with current index, then rebase newer commits on top'\n\n        prevBranch = loggit_all('rev-parse', '--symbolic-full-name', '--abbrev-ref', 'HEAD').strip()\n\n        ret = loggit_all('commit', '-m', 'MERGE '+targethash) # commit index to viewed branch\n        newChanges = loggit_all('rev-parse', 'HEAD').strip()\n\n        ret += loggit_all('stash', 'save', '--keep-index') # stash everything else\n        with GitUndo('stash', 'pop'):\n            tmpBranch = randomBranchName()\n            ret += loggit_all('checkout', '-b', tmpBranch) # create/switch to tmp branch\n            with GitUndo('checkout', prevBranch), GitUndo('branch', '-D', tmpBranch):\n                ret += loggit_all('reset', '--hard', targethash) # tmpbranch now at targethash\n                ret += loggit_all('cherry-pick', '-n', newChanges)  # pick new change from original branch\n                ret += loggit_all('commit', '--amend', '--no-edit')  # recommit to fix targethash (which will change)\n                ret += loggit_all('rebase', '--onto', tmpBranch, 'HEAD@{1}', prevBranch)  # replay the rest\n\n        return ret.splitlines()", "response": "amend targethash with current index then rebase newer commits on top"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_html(p, *vsheets):\n    'Save vsheets as HTML tables in a single file'\n\n    with open(p.resolve(), 'w', encoding='ascii', errors='xmlcharrefreplace') as fp:\n        for sheet in vsheets:\n\n            fp.write('<h2 class=\"sheetname\">%s</h2>\\n'.format(sheetname=html.escape(sheet.name)))\n\n            fp.write('<table id=\"{sheetname}\">\\n'.format(sheetname=html.escape(sheet.name)))\n\n            # headers\n            fp.write('<tr>')\n            for col in sheet.visibleCols:\n                contents = html.escape(col.name)\n                fp.write('<th>{colname}</th>'.format(colname=contents))\n            fp.write('</tr>\\n')\n\n            # rows\n            for r in Progress(sheet.rows, 'saving'):\n                fp.write('<tr>')\n                for col in sheet.visibleCols:\n                    fp.write('<td>')\n                    fp.write(html.escape(col.getDisplayValue(r)))\n                    fp.write('</td>')\n                fp.write('</tr>\\n')\n\n            fp.write('</table>')\n            status('%s save finished' % p)", "response": "Save vsheets as HTML tables in a single file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tsv_trdict(vs):\n    'returns string.translate dictionary for replacing tabs and newlines'\n    if options.safety_first:\n        delim = options.get('delimiter', vs)\n        return {ord(delim): options.get('tsv_safe_tab', vs), # \\t\n            10: options.get('tsv_safe_newline', vs),  # \\n\n            13: options.get('tsv_safe_newline', vs),  # \\r\n            }\n    return {}", "response": "returns string. translate dictionary for replacing tabs and newlines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_tsv_header(p, vs):\n    'Write tsv header for Sheet `vs` to Path `p`.'\n    trdict = tsv_trdict(vs)\n    delim = options.delimiter\n\n    with p.open_text(mode='w') as fp:\n        colhdr = delim.join(col.name.translate(trdict) for col in vs.visibleCols) + '\\n'\n        if colhdr.strip():  # is anything but whitespace\n            fp.write(colhdr)", "response": "Write tsv header for Sheet vs to Path p."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite sheet to file fn as TSV.", "response": "def save_tsv(p, vs):\n    'Write sheet to file `fn` as TSV.'\n    delim = options.get('delimiter', vs)\n    trdict = tsv_trdict(vs)\n\n    save_tsv_header(p, vs)\n\n    with p.open_text(mode='a') as fp:\n        for dispvals in genAllValues(vs.rows, vs.visibleCols, trdict, format=True):\n            fp.write(delim.join(dispvals))\n            fp.write('\\n')\n\n    status('%s save finished' % p)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nappending row to vs. source creating file with correct headers if necessary. For internal use only.", "response": "def append_tsv_row(vs, row):\n    'Append `row` to vs.source, creating file with correct headers if necessary. For internal use only.'\n    if not vs.source.exists():\n        with contextlib.suppress(FileExistsError):\n            parentdir = vs.source.parent.resolve()\n            if parentdir:\n                os.makedirs(parentdir)\n\n        save_tsv_header(vs.source, vs)\n\n    with vs.source.open_text(mode='a') as fp:\n        fp.write('\\t'.join(col.getDisplayValue(row) for col in vs.visibleCols) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reload_sync(self):\n        'Perform synchronous loading of TSV file, discarding header lines.'\n        header_lines = options.get('header', self)\n        delim = options.get('delimiter', self)\n\n        with self.source.open_text() as fp:\n            # get one line anyway to determine number of columns\n            lines = list(getlines(fp, int(header_lines) or 1))\n            headers = [L.split(delim) for L in lines]\n\n            if header_lines <= 0:\n                self.columns = [ColumnItem('', i) for i in range(len(headers[0]))]\n            else:\n                self.columns = [\n                    ColumnItem('\\\\n'.join(x), i)\n                        for i, x in enumerate(zip(*headers[:header_lines]))\n                    ]\n\n            lines = lines[header_lines:]  # in case of header_lines == 0\n            self._rowtype = namedlist('tsvobj', [c.name for c in self.columns])\n\n            self.recalc()\n            self.rows = []\n\n            with Progress(total=self.source.filesize) as prog:\n                for L in itertools.chain(lines, getlines(fp)):\n                    row = L.split(delim)\n                    ncols = self._rowtype.length()  # current number of cols\n                    if len(row) > ncols:\n                        # add unnamed columns to the type not found in the header\n                        newcols = [ColumnItem('', len(row)+i, width=8) for i in range(len(row)-ncols)]\n                        self._rowtype = namedlist(self._rowtype.__name__, list(self._rowtype._fields) + ['_' for c in newcols])\n                        for c in newcols:\n                            self.addColumn(c)\n                    elif len(row) < ncols:\n                        # extend rows that are missing entries\n                        row.extend([None]*(ncols-len(row)))\n\n                    self.addRow(self._rowtype(row))\n                    prog.addProgress(len(L))", "response": "Perform synchronous loading of TSV file discarding header lines."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_csv(vs):\n    'Convert from CSV, first handling header row specially.'\n    with vs.source.open_text() as fp:\n        for i in range(options.skip):\n            wrappedNext(fp)  # discard initial lines\n\n        if options.safety_first:\n            rdr = csv.reader(removeNulls(fp), **csvoptions())\n        else:\n            rdr = csv.reader(fp, **csvoptions())\n\n        vs.rows = []\n\n        # headers first, to setup columns before adding rows\n        headers = [wrappedNext(rdr) for i in range(int(options.header))]\n\n        if headers:\n            # columns ideally reflect the max number of fields over all rows\n            vs.columns = ArrayNamedColumns('\\\\n'.join(x) for x in zip(*headers))\n        else:\n            r = wrappedNext(rdr)\n            vs.addRow(r)\n            vs.columns = ArrayColumns(len(vs.rows[0]))\n\n        if not vs.columns:\n            vs.columns = [ColumnItem(0)]\n\n        vs.recalc()  # make columns usable\n        with Progress(total=vs.source.filesize) as prog:\n            try:\n                samplelen = 0\n                for i in range(options_num_first_rows):  # for progress below\n                    row = wrappedNext(rdr)\n                    vs.addRow(row)\n                    samplelen += sum(len(x) for x in row)\n\n                samplelen //= options_num_first_rows  # avg len of first n rows\n\n                while True:\n                    vs.addRow(wrappedNext(rdr))\n                    prog.addProgress(samplelen)\n            except StopIteration:\n                pass  # as expected\n\n    vs.recalc()\n    return vs", "response": "Convert from CSV first handling header row specially."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_csv(p, sheet):\n    'Save as single CSV file, handling column names as first line.'\n    with p.open_text(mode='w') as fp:\n        cw = csv.writer(fp, **csvoptions())\n        colnames = [col.name for col in sheet.visibleCols]\n        if ''.join(colnames):\n            cw.writerow(colnames)\n        for r in Progress(sheet.rows, 'saving'):\n            cw.writerow([col.getDisplayValue(r) for col in sheet.visibleCols])", "response": "Save as single CSV file handling column names as first line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef currency_multiplier(src_currency, dest_currency):\n    'returns equivalent value in USD for an amt of currency_code'\n    if src_currency == 'USD':\n        return 1.0\n    usd_mult = currency_rates()[src_currency]\n    if dest_currency == 'USD':\n        return usd_mult\n    return usd_mult/currency_rates()[dest_currency]", "response": "returns equivalent value in USD for an amt of currency_code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef moveVisibleCol(sheet, fromVisColIdx, toVisColIdx):\n    'Move visible column to another visible index in sheet.'\n    toVisColIdx = min(max(toVisColIdx, 0), sheet.nVisibleCols)\n    fromColIdx = sheet.columns.index(sheet.visibleCols[fromVisColIdx])\n    toColIdx = sheet.columns.index(sheet.visibleCols[toVisColIdx])\n    moveListItem(sheet.columns, fromColIdx, toColIdx)\n    return toVisColIdx", "response": "Move visible column to another visible index in sheet."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef moveListItem(L, fromidx, toidx):\n    \"Move element within list `L` and return element's new index.\"\n    r = L.pop(fromidx)\n    L.insert(toidx, r)\n    return toidx", "response": "Move element within list L and return element s new index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef urlcache(url, cachesecs=24*60*60):\n    'Returns Path object to local cache of url contents.'\n    p = Path(os.path.join(options.visidata_dir, 'cache', urllib.parse.quote(url, safe='')))\n    if p.exists():\n        secs = time.time() - p.stat().st_mtime\n        if secs < cachesecs:\n            return p\n\n    if not p.parent.exists():\n        os.makedirs(p.parent.resolve(), exist_ok=True)\n\n    assert p.parent.is_dir(), p.parent\n\n    req = urllib.request.Request(url, headers={'User-Agent': __version_info__})\n    with urllib.request.urlopen(req) as fp:\n        ret = fp.read().decode('utf-8').strip()\n        with p.open_text(mode='w') as fpout:\n            fpout.write(ret)\n\n    return p", "response": "Returns Path object to local cache of url contents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling null cells in col with the previous non - null value", "response": "def fillNullValues(col, rows):\n    'Fill null cells in col with the previous non-null value'\n    lastval = None\n    nullfunc = isNullFunc()\n    n = 0\n    rowsToFill = list(rows)\n    for r in Progress(col.sheet.rows, 'filling'):  # loop over all rows\n        try:\n            val = col.getValue(r)\n        except Exception as e:\n            val = e\n\n        if nullfunc(val) and r in rowsToFill:\n            if lastval:\n                col.setValue(r, lastval)\n                n += 1\n        else:\n            lastval = val\n\n    col.recalc()\n    status(\"filled %d values\" % n)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef saveSheets(fn, *vsheets, confirm_overwrite=False):\n    'Save sheet `vs` with given filename `fn`.'\n    givenpath = Path(fn)\n\n    # determine filetype to save as\n    filetype = ''\n    basename, ext = os.path.splitext(fn)\n    if ext:\n        filetype = ext[1:]\n\n    filetype = filetype or options.save_filetype\n\n    if len(vsheets) > 1:\n        if not fn.endswith('/'):  # forcibly specify save individual files into directory by ending path with /\n            savefunc = getGlobals().get('multisave_' + filetype, None)\n            if savefunc:\n                # use specific multisave function\n                return savefunc(givenpath, *vsheets)\n\n        # more than one sheet; either no specific multisave for save filetype, or path ends with /\n\n        # save as individual files in the givenpath directory\n        if not givenpath.exists():\n            try:\n                os.makedirs(givenpath.resolve(), exist_ok=True)\n            except FileExistsError:\n                pass\n\n        assert givenpath.is_dir(), filetype + ' cannot save multiple sheets to non-dir'\n\n        # get save function to call\n        savefunc = getGlobals().get('save_' + filetype) or fail('no function save_'+filetype)\n\n        if givenpath.exists():\n            if confirm_overwrite:\n                confirm('%s already exists. overwrite? ' % fn)\n\n        status('saving %s sheets to %s' % (len(vsheets), givenpath.fqpn))\n        for vs in vsheets:\n            p = Path(os.path.join(givenpath.fqpn, vs.name+'.'+filetype))\n            savefunc(p, vs)\n    else:\n        # get save function to call\n        savefunc = getGlobals().get('save_' + filetype) or fail('no function save_'+filetype)\n\n        if givenpath.exists():\n            if confirm_overwrite:\n                confirm('%s already exists. overwrite? ' % fn)\n\n        status('saving to %s as %s' % (givenpath.fqpn, filetype))\n        savefunc(givenpath, vsheets[0])", "response": "Save sheet vs with given filename fn."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef openSource(p, filetype=None):\n    'calls open_ext(Path) or openurl_scheme(UrlPath, filetype)'\n    if not filetype:\n        filetype = options.filetype\n    if isinstance(p, str):\n        if '://' in p:\n            return openSource(UrlPath(p), filetype)  # convert to Path and recurse\n        elif p == '-':\n            return openSource(PathFd('-', vd().stdin), filetype)\n        else:\n            return openSource(Path(p), filetype)  # convert to Path and recurse\n    elif isinstance(p, UrlPath):\n        openfunc = 'openurl_' + p.scheme\n        return getGlobals()[openfunc](p, filetype=filetype)\n    elif isinstance(p, Path):\n        if not filetype:\n            filetype = p.suffix or 'txt'\n\n        if os.path.isdir(p.resolve()):\n            filetype = 'dir'\n\n        openfunc = 'open_' + filetype.lower()\n        if openfunc not in getGlobals():\n            warning('no %s function' % openfunc)\n            filetype = 'txt'\n            openfunc = 'open_txt'\n        vs = getGlobals()[openfunc](p)\n    else:  # some other object\n        status('unknown object type %s' % type(p))\n        vs = None\n\n    if vs:\n        status('opening %s as %s' % (p.name, filetype))\n\n    return vs", "response": "calls open_ext or openurl_scheme or open_txt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate sheet from. txt file at Path p checking whether it is TSV.", "response": "def open_txt(p):\n    'Create sheet from `.txt` file at Path `p`, checking whether it is TSV.'\n    with p.open_text() as fp:\n        if options.delimiter in next(fp):    # peek at the first line\n            return open_tsv(p)  # TSV often have .txt extension\n        return TextSheet(p.name, p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loadInternalSheet(klass, p, **kwargs):\n    'Load internal sheet of given klass.  Internal sheets are always tsv.'\n    vs = klass(p.name, source=p, **kwargs)\n    options._set('encoding', 'utf8', vs)\n    if p.exists():\n        vd.sheets.insert(0, vs)\n        vs.reload.__wrapped__(vs)\n        vd.sheets.pop(0)\n    return vs", "response": "Load internal sheet of given klass. Internal sheets are always tsv."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nliking namedtuple but editable", "response": "def namedlist(objname, fieldnames):\n    'like namedtuple but editable'\n    class NamedListTemplate(list):\n        __name__ = objname\n        _fields = fieldnames\n\n        def __init__(self, L=None, **kwargs):\n            if L is None:\n                L = [None]*len(fieldnames)\n            super().__init__(L)\n            for k, v in kwargs.items():\n                setattr(self, k, v)\n\n        @classmethod\n        def length(cls):\n            return len(cls._fields)\n\n    for i, attrname in enumerate(fieldnames):\n        # create property getter/setter for each field\n        setattr(NamedListTemplate, attrname, property(operator.itemgetter(i), itemsetter(i)))\n\n    return NamedListTemplate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_by_ip(cls, ip):\n        'Returns Host instance for the given ip address.'\n        ret = cls.hosts_by_ip.get(ip)\n        if ret is None:\n            ret = cls.hosts_by_ip[ip] = [Host(ip)]\n        return ret", "response": "Returns Host instance for the given ip address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef combineColumns(cols):\n    'Return Column object formed by joining fields in given columns.'\n    return Column(\"+\".join(c.name for c in cols),\n                  getter=lambda col,row,cols=cols,ch=' ': ch.join(c.getDisplayValue(row) for c in cols))", "response": "Return Column object formed by joining fields in given columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancelThread(*threads, exception=EscapeException):\n    'Raise exception on another thread.'\n    for t in threads:\n        ctypes.pythonapi.PyThreadState_SetAsyncExc(ctypes.c_long(t.ident), ctypes.py_object(exception))", "response": "Raise exception on another thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn entire output of git command.", "response": "def git_all(*args, git=maybeloggit, **kwargs):\n    'Return entire output of git command.'\n\n    try:\n        cmd = git(*args, _err_to_out=True, _decode_errors='replace', **kwargs)\n        out = cmd.stdout\n    except sh.ErrorReturnCode as e:\n        status('exit_code=%s' % e.exit_code)\n        out = e.stdout\n\n    out = out.decode('utf-8')\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tuple of status adds and deletions.", "response": "def git_status(self, r):\n        '''return tuple of (status, adds, dels).\n        status like !! ??\n        adds and dels are lists of additions and deletions.\n        '''\n        ret = self._cachedStatus.get(r.filename, None) if r else None\n        return ret if ret else [\"//\", None, None]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef columnize(rows):\n    'Generate (i,j) indexes for fixed-width columns found in rows'\n\n    ## find all character columns that are not spaces\n    allNonspaces = set()\n    allNonspaces.add(max(len(r) for r in rows)+1)\n    for r in rows:\n        for i, ch in enumerate(r):\n            if not ch.isspace():\n                allNonspaces.add(i)\n\n    colstart = 0\n    prev = 0\n\n    # collapse fields\n    for i in allNonspaces:\n        if i > prev+1:\n            yield colstart, prev+1\n            colstart = i\n        prev = i", "response": "Generate ( i j ) indexes for fixed - width columns found in rows"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scaleY(self, canvasY):\n        'returns plotter y coordinate, with y-axis inverted'\n        plotterY = super().scaleY(canvasY)\n        return (self.plotviewBox.ymax-plotterY+4)", "response": "returns plotter y coordinate with y - axis inverted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving pathname shell variables and ~userdir", "response": "def resolve(self):\n        'Resolve pathname shell variables and ~userdir'\n        return os.path.expandvars(os.path.expanduser(self.fqpn))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iterline(x1, y1, x2, y2):\n    'Yields (x, y) coords of line from (x1, y1) to (x2, y2)'\n    xdiff = abs(x2-x1)\n    ydiff = abs(y2-y1)\n    xdir = 1 if x1 <= x2 else -1\n    ydir = 1 if y1 <= y2 else -1\n\n    r = math.ceil(max(xdiff, ydiff))\n    if r == 0:  # point, not line\n        yield x1, y1\n    else:\n        x, y = math.floor(x1), math.floor(y1)\n        i = 0\n        while i < r:\n            x += xdir * xdiff / r\n            y += ydir * ydiff / r\n\n            yield x, y\n            i += 1", "response": "Yields ( x y ) coords of line from ( x1 y1 to x2 y2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resetCanvasDimensions(self, windowHeight, windowWidth):\n        'sets total available canvas dimensions to (windowHeight, windowWidth) (in char cells)'\n        self.plotwidth = windowWidth*2\n        self.plotheight = (windowHeight-1)*4  # exclude status line\n\n        # pixels[y][x] = { attr: list(rows), ... }\n        self.pixels = [[defaultdict(list) for x in range(self.plotwidth)] for y in range(self.plotheight)]", "response": "sets total available canvas dimensions to ( windowHeight windowWidth in char cells )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getPixelAttrRandom(self, x, y):\n        'weighted-random choice of attr at this pixel.'\n        c = list(attr for attr, rows in self.pixels[y][x].items()\n                         for r in rows if attr and attr not in self.hiddenAttrs)\n        return random.choice(c) if c else 0", "response": "weighted - random choice of attr at this pixel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getPixelAttrMost(self, x, y):\n        'most common attr at this pixel.'\n        r = self.pixels[y][x]\n        c = sorted((len(rows), attr, rows) for attr, rows in list(r.items()) if attr and attr not in self.hiddenAttrs)\n        if not c:\n            return 0\n        _, attr, rows = c[-1]\n        if isinstance(self.source, BaseSheet) and anySelected(self.source, rows):\n            attr = CursesAttr(attr, 8).update_attr(colors.color_graph_selected, 10).attr\n        return attr", "response": "most common attr at this pixel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning list of deduped rows within bbox", "response": "def rowsWithin(self, bbox):\n        'return list of deduped rows within bbox'\n        ret = {}\n        for y in range(bbox.ymin, bbox.ymax+1):\n            for x in range(bbox.xmin, bbox.xmax+1):\n                for attr, rows in self.pixels[y][x].items():\n                    if attr not in self.hiddenAttrs:\n                        for r in rows:\n                            ret[id(r)] = r\n        return list(ret.values())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear everything in preparation for a fresh reload", "response": "def reset(self):\n        'clear everything in preparation for a fresh reload()'\n        self.polylines.clear()\n        self.legends.clear()\n        self.plotAttrs.clear()\n        self.unusedAttrs = list(colors[colorname.translate(str.maketrans('_', ' '))].attr for colorname in options.plot_colors.split())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting width based on diagonal corner p", "response": "def setCursorSize(self, p):\n        'sets width based on diagonal corner p'\n        self.cursorBox = BoundingBox(self.cursorBox.xmin, self.cursorBox.ymin, p.x, p.y)\n        self.cursorBox.w = max(self.cursorBox.w, self.canvasCharWidth)\n        self.cursorBox.h = max(self.cursorBox.h, self.canvasCharHeight)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd lines for ( x y ) vertexes of a polygon", "response": "def polyline(self, vertexes, attr=0, row=None):\n        'adds lines for (x,y) vertexes of a polygon'\n        self.polylines.append((vertexes, attr, row))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds lines for ( x y ) vertexes of a polygon", "response": "def polygon(self, vertexes, attr=0, row=None):\n        'adds lines for (x,y) vertexes of a polygon'\n        self.polylines.append((vertexes + [vertexes[0]], attr, row))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef qcurve(self, vertexes, attr=0, row=None):\n        'quadratic curve from vertexes[0] to vertexes[2] with control point at vertexes[1]'\n        assert len(vertexes) == 3, len(vertexes)\n        x1, y1 = vertexes[0]\n        x2, y2 = vertexes[1]\n        x3, y3 = vertexes[2]\n\n        self.point(x1, y1, attr, row)\n        self._recursive_bezier(x1, y1, x2, y2, x3, y3, attr, row)\n        self.point(x3, y3, attr, row)", "response": "This method will generate a quadratic curve from vertexes [ 0 1 ) to vertexes [ 2 ] with control point at vertexes [ 1 ] with control point at vertexes [ 2 ] with control point at vertexes [ 1 ]."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fixPoint(self, plotterPoint, canvasPoint):\n        'adjust visibleBox.xymin so that canvasPoint is plotted at plotterPoint'\n        self.visibleBox.xmin = canvasPoint.x - self.canvasW(plotterPoint.x-self.plotviewBox.xmin)\n        self.visibleBox.ymin = canvasPoint.y - self.canvasH(plotterPoint.y-self.plotviewBox.ymin)\n        self.refresh()", "response": "adjust visibleBox. xymin so that canvasPoint is plotted at plotterPoint"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting visible area to bbox maintaining aspectRatio if applicable", "response": "def zoomTo(self, bbox):\n        'set visible area to bbox, maintaining aspectRatio if applicable'\n        self.fixPoint(self.plotviewBox.xymin, bbox.xymin)\n        self.zoomlevel=max(bbox.w/self.canvasBox.w, bbox.h/self.canvasBox.h)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns plotter x coordinate", "response": "def scaleX(self, x):\n        'returns plotter x coordinate'\n        return round(self.plotviewBox.xmin+(x-self.visibleBox.xmin)*self.xScaler)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scaleY(self, y):\n        'returns plotter y coordinate'\n        return round(self.plotviewBox.ymin+(y-self.visibleBox.ymin)*self.yScaler)", "response": "returns plotter y coordinate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, h, w):\n        'resets plotter, cancels previous render threads, spawns a new render'\n        self.needsRefresh = False\n        cancelThread(*(t for t in self.currentThreads if t.name == 'plotAll_async'))\n        self.labels.clear()\n        self.resetCanvasDimensions(h, w)\n        self.render_async()", "response": "resets plotter cancels previous render threads spawns a new render"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_sync(self):\n        'plots points and lines and text onto the Plotter'\n\n        self.setZoom()\n        bb = self.visibleBox\n        xmin, ymin, xmax, ymax = bb.xmin, bb.ymin, bb.xmax, bb.ymax\n        xfactor, yfactor = self.xScaler, self.yScaler\n        plotxmin, plotymin = self.plotviewBox.xmin, self.plotviewBox.ymin\n\n        for vertexes, attr, row in Progress(self.polylines, 'rendering'):\n            if len(vertexes) == 1:  # single point\n                x1, y1 = vertexes[0]\n                x1, y1 = float(x1), float(y1)\n                if xmin <= x1 <= xmax and ymin <= y1 <= ymax:\n                    x = plotxmin+(x1-xmin)*xfactor\n                    y = plotymin+(y1-ymin)*yfactor\n                    self.plotpixel(round(x), round(y), attr, row)\n                continue\n\n            prev_x, prev_y = vertexes[0]\n            for x, y in vertexes[1:]:\n                r = clipline(prev_x, prev_y, x, y, xmin, ymin, xmax, ymax)\n                if r:\n                    x1, y1, x2, y2 = r\n                    x1 = plotxmin+float(x1-xmin)*xfactor\n                    y1 = plotymin+float(y1-ymin)*yfactor\n                    x2 = plotxmin+float(x2-xmin)*xfactor\n                    y2 = plotymin+float(y2-ymin)*yfactor\n                    self.plotline(x1, y1, x2, y2, attr, row)\n                prev_x, prev_y = x, y\n\n        for x, y, text, attr, row in Progress(self.gridlabels, 'labeling'):\n            self.plotlabel(self.scaleX(x), self.scaleY(y), text, attr, row)", "response": "plots points and lines and text onto the Plotter"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef moveToNextRow(vs, func, reverse=False):\n    'Move cursor to next (prev if reverse) row for which func returns True.  Returns False if no row meets the criteria.'\n    rng = range(vs.cursorRowIndex-1, -1, -1) if reverse else range(vs.cursorRowIndex+1, vs.nRows)\n\n    for i in rng:\n        try:\n            if func(vs.rows[i]):\n                vs.cursorRowIndex = i\n                return True\n        except Exception:\n            pass\n\n    return False", "response": "Move cursor to next row for which func returns True. Returns False if no row meets the criteria."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngoes to first visible column after the cursor matching colregex.", "response": "def nextColRegex(sheet, colregex):\n    'Go to first visible column after the cursor matching `colregex`.'\n    pivot = sheet.cursorVisibleColIndex\n    for i in itertools.chain(range(pivot+1, len(sheet.visibleCols)), range(0, pivot+1)):\n        c = sheet.visibleCols[i]\n        if re.search(colregex, c.name, regex_flags()):\n            return i\n\n    fail('no column name matches /%s/' % colregex)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching for the regex in the specified sheet.", "response": "def searchRegex(vd, sheet, moveCursor=False, reverse=False, **kwargs):\n        'Set row index if moveCursor, otherwise return list of row indexes.'\n        def findMatchingColumn(sheet, row, columns, func):\n            'Find column for which func matches the displayed value in this row'\n            for c in columns:\n                if func(c.getDisplayValue(row)):\n                    return c\n\n        vd.searchContext.update(kwargs)\n\n        regex = kwargs.get(\"regex\")\n        if regex:\n            vd.searchContext[\"regex\"] = re.compile(regex, regex_flags()) or error('invalid regex: %s' % regex)\n\n        regex = vd.searchContext.get(\"regex\") or fail(\"no regex\")\n\n        columns = vd.searchContext.get(\"columns\")\n        if columns == \"cursorCol\":\n            columns = [sheet.cursorCol]\n        elif columns == \"visibleCols\":\n            columns = tuple(sheet.visibleCols)\n        elif isinstance(columns, Column):\n            columns = [columns]\n\n        if not columns:\n            error('bad columns')\n\n        searchBackward = vd.searchContext.get(\"backward\")\n        if reverse:\n            searchBackward = not searchBackward\n\n        matchingRowIndexes = 0\n        for r in rotate_range(len(sheet.rows), sheet.cursorRowIndex, reverse=searchBackward):\n            c = findMatchingColumn(sheet, sheet.rows[r], columns, regex.search)\n            if c:\n                if moveCursor:\n                    sheet.cursorRowIndex = r\n                    sheet.cursorVisibleColIndex = sheet.visibleCols.index(c)\n                    return\n                else:\n                    matchingRowIndexes += 1\n                    yield r\n\n        status('%s matches for /%s/' % (matchingRowIndexes, regex.pattern))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef currency(s=''):\n    'dirty float (strip non-numeric characters)'\n    if isinstance(s, str):\n        s = ''.join(ch for ch in s if ch in floatchars)\n    return float(s) if s else TypedWrapper(float, None)", "response": "dirty float ( strip non - numeric characters )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chooseMany(choices):\n    'Return list of `choices` elements (if list) or values (if dict).'\n    if isinstance(choices, dict):\n        choosed = input('/'.join(choices.keys()) + ': ', completer=CompleteKey(choices)).split()\n        return [choices[c] for c in choosed]\n    else:\n        return input('/'.join(str(x) for x in choices) + ': ', completer=CompleteKey(choices)).split()", "response": "Return list of choices elements ( if list or values ( if dict )."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction decorator to make calls to func spawn a separate thread if available.", "response": "def asyncthread(func):\n    'Function decorator, to make calls to `func()` spawn a separate thread if available.'\n    @functools.wraps(func)\n    def _execAsync(*args, **kwargs):\n        return vd().execAsync(func, *args, **kwargs)\n    return _execAsync"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wrapply(func, *args, **kwargs):\n    'Like apply(), but which wraps Exceptions and passes through Wrappers (if first arg)'\n    val = args[0]\n    if val is None:\n        return TypedWrapper(func, None)\n    elif isinstance(val, TypedExceptionWrapper):\n        tew = copy(val)\n        tew.forwarded = True\n        return tew\n    elif isinstance(val, TypedWrapper):\n        return val\n    elif isinstance(val, Exception):\n        return TypedWrapper(func, *args)\n\n    try:\n        return func(*args, **kwargs)\n    except Exception as e:\n        e.stacktrace = stacktrace()\n        return TypedExceptionWrapper(func, *args, exception=e)", "response": "Like apply but which wraps Exceptions and passes through Wrappers ( if first arg )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning dotted attr ( like a. b. c from obj or default if any of the components are missing.", "response": "def getattrdeep(obj, attr, *default):\n    'Return dotted attr (like \"a.b.c\") from obj, or default if any of the components are missing.'\n    attrs = attr.split('.')\n    if default:\n        getattr_default = lambda o,a,d=default[0]: getattr(o, a, d)\n    else:\n        getattr_default = lambda o,a: getattr(o, a)\n\n    for a in attrs[:-1]:\n        obj = getattr_default(obj, a)\n\n    return getattr_default(obj, attrs[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setattrdeep(obj, attr, val):\n    'Set dotted attr (like \"a.b.c\") on obj to val.'\n    attrs = attr.split('.')\n\n    for a in attrs[:-1]:\n        obj = getattr(obj, a)\n    setattr(obj, attrs[-1], val)", "response": "Set dotted attr ( like a. b. c on obj to val."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns clipped string and width in terminal display characters.", "response": "def clipstr(s, dispw):\n    '''Return clipped string and width in terminal display characters.\n\n    Note: width may differ from len(s) if East Asian chars are 'fullwidth'.'''\n    w = 0\n    ret = ''\n    ambig_width = options.disp_ambig_width\n    for c in s:\n        if c != ' ' and unicodedata.category(c) in ('Cc', 'Zs', 'Zl'):  # control char, space, line sep\n            c = options.disp_oddspace\n\n        if c:\n            c = c[0]  # multi-char disp_oddspace just uses the first char\n            ret += c\n            eaw = unicodedata.east_asian_width(c)\n            if eaw == 'A':  # ambiguous\n                w += ambig_width\n            elif eaw in 'WF': # wide/full\n                w += 2\n            elif not unicodedata.combining(c):\n                w += 1\n\n        if w > dispw-len(options.disp_truncator)+1:\n            ret = ret[:-2] + options.disp_truncator  # replace final char with ellipsis\n            w += len(options.disp_truncator)\n            break\n\n    return ret, w"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clipdraw(scr, y, x, s, attr, w=None, rtl=False):\n    'Draw string `s` at (y,x)-(y,x+w), clipping with ellipsis char.  if rtl, draw inside (x-w, x).  Returns width drawn (max of w).'\n    if not scr:\n        return 0\n    _, windowWidth = scr.getmaxyx()\n    dispw = 0\n    try:\n        if w is None:\n            w = windowWidth-1\n        w = min(w, (x-1) if rtl else (windowWidth-x-1))\n        if w <= 0:  # no room anyway\n            return 0\n\n        # convert to string just before drawing\n        clipped, dispw = clipstr(str(s), w)\n        if rtl:\n            # clearing whole area (w) has negative display effects; clearing just dispw area is useless\n#            scr.addstr(y, x-dispw-1, disp_column_fill*dispw, attr)\n            scr.addstr(y, x-dispw-1, clipped, attr)\n        else:\n            scr.addstr(y, x, disp_column_fill*w, attr)  # clear whole area before displaying\n            scr.addstr(y, x, clipped, attr)\n    except Exception as e:\n        pass\n#        raise type(e)('%s [clip_draw y=%s x=%s dispw=%s w=%s clippedlen=%s]' % (e, y, x, dispw, w, len(clipped))\n#                ).with_traceback(sys.exc_info()[2])\n\n    return dispw", "response": "Draw string s at y x + w clipping with ellipsis char. Returns width drawn ( max of w."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef editText(scr, y, x, w, i=0, attr=curses.A_NORMAL, value='', fillchar=' ', truncchar='-', unprintablechar='.', completer=lambda text,idx: None, history=[], display=True):\n    'A better curses line editing widget.'\n    ESC='^['\n    ENTER='^J'\n    TAB='^I'\n\n    def until_get_wch():\n        'Ignores get_wch timeouts'\n        ret = None\n        while not ret:\n            try:\n                ret = scr.get_wch()\n            except curses.error:\n                pass\n\n        return ret\n\n    def splice(v, i, s):\n        'Insert `s` into string `v` at `i` (such that v[i] == s[0]).'\n        return v if i < 0 else v[:i] + s + v[i:]\n\n    def clean_printable(s):\n        'Escape unprintable characters.'\n        return ''.join(c if c.isprintable() else ('<%04X>' % ord(c)) for c in str(s))\n\n    def delchar(s, i, remove=1):\n        'Delete `remove` characters from str `s` beginning at position `i`.'\n        return s if i < 0 else s[:i] + s[i+remove:]\n\n    class CompleteState:\n        def __init__(self, completer_func):\n            self.comps_idx = -1\n            self.completer_func = completer_func\n            self.former_i = None\n            self.just_completed = False\n\n        def complete(self, v, i, state_incr):\n            self.just_completed = True\n            self.comps_idx += state_incr\n\n            if self.former_i is None:\n                self.former_i = i\n            try:\n                r = self.completer_func(v[:self.former_i], self.comps_idx)\n            except Exception as e:\n                # beep/flash; how to report exception?\n                return v, i\n\n            if not r:\n                # beep/flash to indicate no matches?\n                return v, i\n\n            v = r + v[i:]\n            return v, len(v)\n\n        def reset(self):\n            if self.just_completed:\n                self.just_completed = False\n            else:\n                self.former_i = None\n                self.comps_idx = -1\n\n    class HistoryState:\n        def __init__(self, history):\n            self.history = history\n            self.hist_idx = None\n            self.prev_val = None\n\n        def up(self, v, i):\n            if self.hist_idx is None:\n                self.hist_idx = len(self.history)\n                self.prev_val = v\n            if self.hist_idx > 0:\n                self.hist_idx -= 1\n                v = self.history[self.hist_idx]\n            i = len(v)\n            return v, i\n\n        def down(self, v, i):\n            if self.hist_idx is None:\n                return v, i\n            elif self.hist_idx < len(self.history)-1:\n                self.hist_idx += 1\n                v = self.history[self.hist_idx]\n            else:\n                v = self.prev_val\n                self.hist_idx = None\n            i = len(v)\n            return v, i\n\n    history_state = HistoryState(history)\n    complete_state = CompleteState(completer)\n    insert_mode = True\n    first_action = True\n    v = str(value)  # value under edit\n\n    # i = 0  # index into v, initial value can be passed in as argument as of 1.2\n    if i != 0:\n        first_action = False\n\n    left_truncchar = right_truncchar = truncchar\n\n    def rfind_nonword(s, a, b):\n        if not s:\n            return 0\n\n        while not s[b].isalnum() and b >= a:  # first skip non-word chars\n            b -= 1\n        while s[b].isalnum() and b >= a:\n            b -= 1\n        return b\n\n    while True:\n        if display:\n            dispval = clean_printable(v)\n        else:\n            dispval = '*' * len(v)\n\n        dispi = i  # the onscreen offset within the field where v[i] is displayed\n        if len(dispval) < w:  # entire value fits\n            dispval += fillchar*(w-len(dispval)-1)\n        elif i == len(dispval):  # cursor after value (will append)\n            dispi = w-1\n            dispval = left_truncchar + dispval[len(dispval)-w+2:] + fillchar\n        elif i >= len(dispval)-w//2:  # cursor within halfwidth of end\n            dispi = w-(len(dispval)-i)\n            dispval = left_truncchar + dispval[len(dispval)-w+1:]\n        elif i <= w//2:  # cursor within halfwidth of beginning\n            dispval = dispval[:w-1] + right_truncchar\n        else:\n            dispi = w//2  # visual cursor stays right in the middle\n            k = 1 if w%2==0 else 0  # odd widths have one character more\n            dispval = left_truncchar + dispval[i-w//2+1:i+w//2-k] + right_truncchar\n\n        prew = clipdraw(scr, y, x, dispval[:dispi], attr, w)\n        clipdraw(scr, y, x+prew, dispval[dispi:], attr, w-prew+1)\n        scr.move(y, x+prew)\n        ch = vd().getkeystroke(scr)\n        if ch == '':                               continue\n        elif ch == 'KEY_IC':                       insert_mode = not insert_mode\n        elif ch == '^A' or ch == 'KEY_HOME':       i = 0\n        elif ch == '^B' or ch == 'KEY_LEFT':       i -= 1\n        elif ch in ('^C', '^Q', ESC):              raise EscapeException(ch)\n        elif ch == '^D' or ch == 'KEY_DC':         v = delchar(v, i)\n        elif ch == '^E' or ch == 'KEY_END':        i = len(v)\n        elif ch == '^F' or ch == 'KEY_RIGHT':      i += 1\n        elif ch in ('^H', 'KEY_BACKSPACE', '^?'):  i -= 1; v = delchar(v, i)\n        elif ch == TAB:                            v, i = complete_state.complete(v, i, +1)\n        elif ch == 'KEY_BTAB':                     v, i = complete_state.complete(v, i, -1)\n        elif ch == ENTER:                          break\n        elif ch == '^K':                           v = v[:i]  # ^Kill to end-of-line\n        elif ch == '^O':                           v = launchExternalEditor(v)\n        elif ch == '^R':                           v = str(value)  # ^Reload initial value\n        elif ch == '^T':                           v = delchar(splice(v, i-2, v[i-1]), i)  # swap chars\n        elif ch == '^U':                           v = v[i:]; i = 0  # clear to beginning\n        elif ch == '^V':                           v = splice(v, i, until_get_wch()); i += 1  # literal character\n        elif ch == '^W':                           j = rfind_nonword(v, 0, i-1); v = v[:j+1] + v[i:]; i = j+1  # erase word\n        elif ch == '^Z':                           suspend()\n        elif history and ch == 'KEY_UP':           v, i = history_state.up(v, i)\n        elif history and ch == 'KEY_DOWN':         v, i = history_state.down(v, i)\n        elif ch.startswith('KEY_'):                pass\n        else:\n            if first_action:\n                v = ''\n            if insert_mode:\n                v = splice(v, i, ch)\n            else:\n                v = v[:i] + ch + v[i+1:]\n\n            i += 1\n\n        if i < 0: i = 0\n        if i > len(v): i = len(v)\n        first_action = False\n        complete_state.reset()\n\n    return v", "response": "A better curses line editing widget."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(*sheetlist):\n    'Main entry point; launches vdtui with the given sheets already pushed (last one is visible)'\n\n    # reduce ESC timeout to 25ms. http://en.chys.info/2009/09/esdelay-ncurses/\n    os.putenv('ESCDELAY', '25')\n\n    ret = wrapper(cursesMain, sheetlist)\n    if ret:\n        print(ret)", "response": "Main entry point ; launches vdtui with the given sheets already pushed ( last one is visible )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cursesMain(_scr, sheetlist):\n    'Populate VisiData object with sheets from a given list.'\n\n    colors.setup()\n\n    for vs in sheetlist:\n        vd().push(vs)  # first push does a reload\n\n    status('Ctrl+H opens help')\n    return vd().run(_scr)", "response": "Populate VisiData object with sheets from a given list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter(self, obj=None):\n        'Iterate through all keys considering context of obj. If obj is None, uses the context of the top sheet.'\n        if obj is None and vd:\n            obj = vd.sheet\n\n        for o in self._mappings(obj):\n            for k in self.keys():\n                for o2 in self[k]:\n                    if o == o2:\n                        yield (k, o), self[k][o2]", "response": "Iterate through all keys considering context of obj. If obj is None uses the context of the top sheet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get(self, k, obj=None):\n        'Return Option object for k in context of obj. Cache result until any set().'\n        opt = self._cache.get((k, obj), None)\n        if opt is None:\n            opt = self._opts._get(k, obj)\n            self._cache[(k, obj or vd.sheet)] = opt\n        return opt", "response": "Return Option object for k in context of obj. Cache result until any set()."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd status message to be shown until next action.", "response": "def status(self, *args, priority=0):\n        'Add status message to be shown until next action.'\n        k = (priority, args)\n        self.statuses[k] = self.statuses.get(k, 0) + 1\n\n        if self.statusHistory:\n            prevpri, prevargs, prevn = self.statusHistory[-1]\n            if prevpri == priority and prevargs == args:\n                self.statusHistory[-1][2] += 1\n                return True\n\n        self.statusHistory.append([priority, args, 1])\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling all functions registered with addHook for the given hookname.", "response": "def callHook(self, hookname, *args, **kwargs):\n        'Call all functions registered with `addHook` for the given hookname.'\n        r = []\n        for f in self.hooks[hookname]:\n            try:\n                r.append(f(*args, **kwargs))\n            except Exception as e:\n                exceptionCaught(e)\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute func(*args ** kwargs in a separate thread.", "response": "def execAsync(self, func, *args, **kwargs):\n        'Execute `func(*args, **kwargs)` in a separate thread.'\n\n        thread = threading.Thread(target=self.toplevelTryFunc, daemon=True, args=(func,)+args, kwargs=kwargs)\n        self.addThread(thread)\n\n        if self.sheets:\n            currentSheet = self.sheets[0]\n            currentSheet.currentThreads.append(thread)\n        else:\n            currentSheet = None\n\n        thread.sheet = currentSheet\n        thread.start()\n\n        return thread"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toplevelTryFunc(func, *args, **kwargs):\n        'Thread entry-point for `func(*args, **kwargs)` with try/except wrapper'\n        t = threading.current_thread()\n        t.name = func.__name__\n        ret = None\n        try:\n            ret = func(*args, **kwargs)\n        except EscapeException as e:  # user aborted\n            t.status += 'aborted by user'\n            status('%s aborted' % t.name, priority=2)\n        except Exception as e:\n            t.exception = e\n            exceptionCaught(e)\n\n        if t.sheet:\n            t.sheet.currentThreads.remove(t)\n        return ret", "response": "Thread entry - point for func(*args ** kwargs with try / except wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmarks terminated threads with endTime.", "response": "def checkForFinishedThreads(self):\n        'Mark terminated threads with endTime.'\n        for t in self.unfinishedThreads:\n            if not t.is_alive():\n                t.endTime = time.process_time()\n                if getattr(t, 'status', None) is None:\n                    t.status = 'ended'"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for all but expectedThreads async threads to finish.", "response": "def sync(self, expectedThreads=0):\n        'Wait for all but expectedThreads async threads to finish.'\n        while len(self.unfinishedThreads) > expectedThreads:\n            time.sleep(.3)\n            self.checkForFinishedThreads()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef editText(self, y, x, w, record=True, **kwargs):\n        'Wrap global editText with `preedit` and `postedit` hooks.'\n        v = self.callHook('preedit') if record else None\n        if not v or v[0] is None:\n            with EnableCursor():\n                v = editText(self.scr, y, x, w, **kwargs)\n        else:\n            v = v[0]\n\n        if kwargs.get('display', True):\n            status('\"%s\"' % v)\n            self.callHook('postedit', v) if record else None\n        return v", "response": "Wrap global editText with preedit and postedit hooks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef input(self, prompt, type='', defaultLast=False, **kwargs):\n        'Get user input, with history of `type`, defaulting to last history item if no input and defaultLast is True.'\n        if type:\n            histlist = list(self.lastInputs[type].keys())\n            ret = self._inputLine(prompt, history=histlist, **kwargs)\n            if ret:\n                self.lastInputs[type][ret] = ret\n            elif defaultLast:\n                histlist or fail(\"no previous input\")\n                ret = histlist[-1]\n        else:\n            ret = self._inputLine(prompt, **kwargs)\n\n        return ret", "response": "Get user input with history of type defaulting to last history item if no input and defaultLast is True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd prompt to bottom of screen and get line of input from user.", "response": "def _inputLine(self, prompt, **kwargs):\n        'Add prompt to bottom of screen and get line of input from user.'\n        self.inInput = True\n        rstatuslen = self.drawRightStatus(self.scr, self.sheets[0])\n        attr = 0\n        promptlen = clipdraw(self.scr, self.windowHeight-1, 0, prompt, attr, w=self.windowWidth-rstatuslen-1)\n        ret = self.editText(self.windowHeight-1, promptlen, self.windowWidth-promptlen-rstatuslen-2,\n                            attr=colors.color_edit_cell,\n                            unprintablechar=options.disp_unprintable,\n                            truncchar=options.disp_truncator,\n                            **kwargs)\n        self.inInput = False\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getkeystroke(self, scr, vs=None):\n        'Get keystroke and display it on status bar.'\n        k = None\n        try:\n            k = scr.get_wch()\n            self.drawRightStatus(scr, vs or self.sheets[0]) # continue to display progress %\n        except curses.error:\n            return ''  # curses timeout\n\n        if isinstance(k, str):\n            if ord(k) >= 32 and ord(k) != 127:  # 127 == DEL or ^?\n                return k\n            k = ord(k)\n        return curses.keyname(k).decode('utf-8')", "response": "Get keystroke and display it on status bar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaintaining list of most recent errors and return most recent one.", "response": "def exceptionCaught(self, exc=None, **kwargs):\n        'Maintain list of most recent errors and return most recent one.'\n        if isinstance(exc, ExpectedException):  # already reported, don't log\n            return\n        self.lastErrors.append(stacktrace())\n        if kwargs.get('status', True):\n            status(self.lastErrors[-1][-1], priority=2)  # last line of latest error\n        if options.debug:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef drawLeftStatus(self, scr, vs):\n        'Draw left side of status bar.'\n        cattr = CursesAttr(colors.color_status)\n        attr = cattr.attr\n        error_attr = cattr.update_attr(colors.color_error, 1).attr\n        warn_attr = cattr.update_attr(colors.color_warning, 2).attr\n        sep = options.disp_status_sep\n\n        try:\n            lstatus = vs.leftStatus()\n            maxwidth = options.disp_lstatus_max\n            if maxwidth > 0:\n                lstatus = middleTruncate(lstatus, maxwidth//2)\n\n            y = self.windowHeight-1\n            x = clipdraw(scr, y, 0, lstatus, attr)\n            self.onMouse(scr, y, 0, 1, x,\n                            BUTTON1_PRESSED='sheets',\n                            BUTTON3_PRESSED='rename-sheet',\n                            BUTTON3_CLICKED='rename-sheet')\n\n            one = False\n            for (pri, msgparts), n in sorted(self.statuses.items(), key=lambda k: -k[0][0]):\n                if x > self.windowWidth:\n                    break\n                if one:  # any messages already:\n                    x += clipdraw(scr, y, x, sep, attr, self.windowWidth)\n                one = True\n                msg = composeStatus(msgparts, n)\n\n                if pri == 3: msgattr = error_attr\n                elif pri == 2: msgattr = warn_attr\n                elif pri == 1: msgattr = warn_attr\n                else: msgattr = attr\n                x += clipdraw(scr, y, x, msg, msgattr, self.windowWidth)\n        except Exception as e:\n            self.exceptionCaught(e)", "response": "Draw left side of status bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws right side of status bar. Return length displayed.", "response": "def drawRightStatus(self, scr, vs):\n        'Draw right side of status bar.  Return length displayed.'\n        rightx = self.windowWidth-1\n\n        ret = 0\n        for rstatcolor in self.callHook('rstatus', vs):\n            if rstatcolor:\n                try:\n                    rstatus, coloropt = rstatcolor\n                    rstatus = ' '+rstatus\n                    attr = colors.get_color(coloropt).attr\n                    statuslen = clipdraw(scr, self.windowHeight-1, rightx, rstatus, attr, rtl=True)\n                    rightx -= statuslen\n                    ret += statuslen\n                except Exception as e:\n                    self.exceptionCaught(e)\n\n        if scr:\n            curses.doupdate()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomposing right side of status bar.", "response": "def rightStatus(self, sheet):\n        'Compose right side of status bar.'\n        if sheet.currentThreads:\n            gerund = (' '+sheet.progresses[0].gerund) if sheet.progresses else ''\n            status = '%9d  %2d%%%s' % (len(sheet), sheet.progressPct, gerund)\n        else:\n            status = '%9d %s' % (len(sheet), sheet.rowtype)\n        return status, 'color_status'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, scr):\n        'Manage execution of keystrokes and subsequent redrawing of screen.'\n        global sheet\n        scr.timeout(int(options.curses_timeout))\n        with suppress(curses.error):\n            curses.curs_set(0)\n\n        self.scr = scr\n        numTimeouts = 0\n\n        self.keystrokes = ''\n        while True:\n            if not self.sheets:\n                # if no more sheets, exit\n                return\n\n            sheet = self.sheets[0]\n            threading.current_thread().sheet = sheet\n\n            try:\n                sheet.draw(scr)\n            except Exception as e:\n                self.exceptionCaught(e)\n\n            self.drawLeftStatus(scr, sheet)\n            self.drawRightStatus(scr, sheet)  # visible during this getkeystroke\n\n            keystroke = self.getkeystroke(scr, sheet)\n\n            if keystroke:  # wait until next keystroke to clear statuses and previous keystrokes\n                numTimeouts = 0\n                if not self.prefixWaiting:\n                    self.keystrokes = ''\n\n                self.statuses.clear()\n\n                if keystroke == 'KEY_MOUSE':\n                    self.keystrokes = ''\n                    clicktype = ''\n                    try:\n                        devid, x, y, z, bstate = curses.getmouse()\n                        sheet.mouseX, sheet.mouseY = x, y\n                        if bstate & curses.BUTTON_CTRL:\n                            clicktype += \"CTRL-\"\n                            bstate &= ~curses.BUTTON_CTRL\n                        if bstate & curses.BUTTON_ALT:\n                            clicktype += \"ALT-\"\n                            bstate &= ~curses.BUTTON_ALT\n                        if bstate & curses.BUTTON_SHIFT:\n                            clicktype += \"SHIFT-\"\n                            bstate &= ~curses.BUTTON_SHIFT\n\n                        keystroke = clicktype + curses.mouseEvents.get(bstate, str(bstate))\n\n                        f = self.getMouse(scr, x, y, keystroke)\n                        if f:\n                            if isinstance(f, str):\n                                for cmd in f.split():\n                                    sheet.exec_keystrokes(cmd)\n                            else:\n                                f(y, x, keystroke)\n\n                            self.keystrokes = keystroke\n                            keystroke = ''\n                    except curses.error:\n                        pass\n                    except Exception as e:\n                        exceptionCaught(e)\n\n                self.keystrokes += keystroke\n\n            self.drawRightStatus(scr, sheet)  # visible for commands that wait for input\n\n            if not keystroke:  # timeout instead of keypress\n                pass\n            elif keystroke == '^Q':\n                return self.lastErrors and '\\n'.join(self.lastErrors[-1])\n            elif bindkeys._get(self.keystrokes):\n                sheet.exec_keystrokes(self.keystrokes)\n                self.prefixWaiting = False\n            elif keystroke in self.allPrefixes:\n                self.keystrokes = ''.join(sorted(set(self.keystrokes)))  # prefix order/quantity does not matter\n                self.prefixWaiting = True\n            else:\n                status('no command for \"%s\"' % (self.keystrokes))\n                self.prefixWaiting = False\n\n            self.checkForFinishedThreads()\n            self.callHook('predraw')\n            catchapply(sheet.checkCursor)\n\n            # no idle redraw unless background threads are running\n            time.sleep(0)  # yield to other threads which may not have started yet\n            if vd.unfinishedThreads:\n                scr.timeout(options.curses_timeout)\n            else:\n                numTimeouts += 1\n                if numTimeouts > 1:\n                    scr.timeout(-1)\n                else:\n                    scr.timeout(options.curses_timeout)", "response": "Manage execution of keystrokes and subsequent redrawing of screen."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push(self, vs):\n        'Move given sheet `vs` to index 0 of list `sheets`.'\n        if vs:\n            vs.vd = self\n            if vs in self.sheets:\n                self.sheets.remove(vs)\n                self.sheets.insert(0, vs)\n            elif not vs.loaded:\n                self.sheets.insert(0, vs)\n                vs.reload()\n                vs.recalc()  # set up Columns\n            else:\n                self.sheets.insert(0, vs)\n\n            if vs.precious and vs not in vs.vd.allSheets:\n                vs.vd.allSheets[vs] = vs.name\n            return vs", "response": "Move given sheet vs to index 0 of list sheets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exec_command(self, cmd, args='', vdglobals=None, keystrokes=None):\n        \"Execute `cmd` tuple with `vdglobals` as globals and this sheet's attributes as locals.  Returns True if user cancelled.\"\n        global sheet\n        sheet = vd.sheets[0]\n\n        if not cmd:\n            debug('no command \"%s\"' % keystrokes)\n            return True\n\n        if isinstance(cmd, CommandLog):\n            cmd.replay()\n            return False\n\n        escaped = False\n        err = ''\n\n        if vdglobals is None:\n            vdglobals = getGlobals()\n\n        if not self.vd:\n            self.vd = vd()\n\n        self.sheet = self\n\n        try:\n            self.vd.callHook('preexec', self, cmd, '', keystrokes)\n            exec(cmd.execstr, vdglobals, LazyMap(self))\n        except EscapeException as e:  # user aborted\n            status('aborted')\n            escaped = True\n        except Exception as e:\n            debug(cmd.execstr)\n            err = self.vd.exceptionCaught(e)\n            escaped = True\n\n        try:\n            self.vd.callHook('postexec', self.vd.sheets[0] if self.vd.sheets else None, escaped, err)\n        except Exception:\n            self.vd.exceptionCaught(e)\n\n        catchapply(self.checkCursor)\n\n        self.vd.refresh()\n        return escaped", "response": "Execute cmd tuple with vdglobals as globals and this sheet s attributes as locals. Returns True if user cancelled."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef colorize(self, col, row, value=None):\n        'Returns curses attribute for the given col/row/value'\n\n#        colorstack = tuple(c.coloropt for c in self.getColorizers() if wrapply(c.func, self, col, row, value))\n\n        colorstack = []\n        for colorizer in self.getColorizers():\n            try:\n                r = colorizer.func(self, col, row, value)\n                if r:\n                    colorstack.append(colorizer.coloropt if colorizer.coloropt else r)\n            except Exception as e:\n                exceptionCaught(e)\n\n        return colors.resolve_colors(tuple(colorstack))", "response": "Returns curses attribute for the given col / row / value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef column(self, colregex):\n        'Return first column whose Column.name matches colregex.'\n        for c in self.columns:\n            if re.search(colregex, c.name, regex_flags()):\n                return c", "response": "Return first column whose Column. name matches colregex."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reload(self):\n        'Loads rows and/or columns.  Override in subclass.'\n        self.rows = []\n        for r in self.iterload():\n            self.addRow(r)", "response": "Loads rows and / or columns. Override in subclass."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes rows for which func ( row ) is true. Returns number of deleted rows.", "response": "def deleteBy(self, func):\n        'Delete rows for which func(row) is true.  Returns number of deleted rows.'\n        oldrows = copy(self.rows)\n        oldidx = self.cursorRowIndex\n        ndeleted = 0\n\n        row = None   # row to re-place cursor after\n        while oldidx < len(oldrows):\n            if not func(oldrows[oldidx]):\n                row = self.rows[oldidx]\n                break\n            oldidx += 1\n\n        self.rows.clear()\n        for r in Progress(oldrows, 'deleting'):\n            if not func(r):\n                self.rows.append(r)\n                if r is row:\n                    self.cursorRowIndex = len(self.rows)-1\n            else:\n                ndeleted += 1\n\n        status('deleted %s %s' % (ndeleted, self.rowtype))\n        return ndeleted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete all selected rows.", "response": "def deleteSelected(self):\n        'Delete all selected rows.'\n        ndeleted = self.deleteBy(self.isSelected)\n        nselected = len(self._selectedRows)\n        self._selectedRows.clear()\n        if ndeleted != nselected:\n            error('expected %s' % nselected)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting of rows onscreen.", "response": "def visibleRows(self):  # onscreen rows\n        'List of rows onscreen. '\n        return self.rows[self.topRowIndex:self.topRowIndex+self.nVisibleRows]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visibleCols(self):  # non-hidden cols\n        'List of `Column` which are not hidden.'\n        return self.keyCols + [c for c in self.columns if not c.hidden and not c.keycol]", "response": "List of Column which are not hidden."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef statusLine(self):\n        'String of row and column stats.'\n        rowinfo = 'row %d/%d (%d selected)' % (self.cursorRowIndex, self.nRows, len(self._selectedRows))\n        colinfo = 'col %d/%d (%d visible)' % (self.cursorColIndex, self.nCols, len(self.visibleCols))\n        return '%s  %s' % (rowinfo, colinfo)", "response": "String of row and column stats."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toggle(self, rows):\n        'Toggle selection of given `rows`.'\n        for r in Progress(rows, 'toggling', total=len(self.rows)):\n            if not self.unselectRow(r):\n                self.selectRow(r)", "response": "Toggle selection of given rows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unselectRow(self, row):\n        'Unselect given row, return True if selected; else return False. O(log n)'\n        if id(row) in self._selectedRows:\n            del self._selectedRows[id(row)]\n            return True\n        else:\n            return False", "response": "Unselect given row return True if selected ; else return False. O ( log n )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbulks select given rows. Don t show progress if progress = False ; don t show status if status = False.", "response": "def select(self, rows, status=True, progress=True):\n        \"Bulk select given rows. Don't show progress if progress=False; don't show status if status=False.\"\n        before = len(self._selectedRows)\n        if options.bulk_select_clear:\n            self._selectedRows.clear()\n        for r in (Progress(rows, 'selecting') if progress else rows):\n            self.selectRow(r)\n        if status:\n            if options.bulk_select_clear:\n                msg = 'selected %s %s%s' % (len(self._selectedRows), self.rowtype, ' instead' if before > 0 else '')\n            else:\n                msg = 'selected %s%s %s' % (len(self._selectedRows)-before, ' more' if before > 0 else '', self.rowtype)\n            vd.status(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef selectByIdx(self, rowIdxs):\n        'Select given row indexes, without progress bar.'\n        self.select((self.rows[i] for i in rowIdxs), progress=False)", "response": "Select given row indexes without progress bar."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unselectByIdx(self, rowIdxs):\n        'Unselect given row indexes, without progress bar.'\n        self.unselect((self.rows[i] for i in rowIdxs), progress=False)", "response": "Unselect given row indexes without progress bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates only rows for which the given func returns True.", "response": "def gatherBy(self, func):\n        'Generate only rows for which the given func returns True.'\n        for i in rotate_range(len(self.rows), self.cursorRowIndex):\n            try:\n                r = self.rows[i]\n                if func(r):\n                    yield r\n            except Exception:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef selectedRows(self):\n        'List of selected rows in sheet order. [O(nRows*log(nSelected))]'\n        if len(self._selectedRows) <= 1:\n            return list(self._selectedRows.values())\n        return [r for r in self.rows if id(r) in self._selectedRows]", "response": "List of selected rows in sheet order. [ O ( nRows * log ( nSelected )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nredraws page one screen to the left.", "response": "def pageLeft(self):\n        '''Redraw page one screen to the left.\n\n        Note: keep the column cursor in the same general relative position:\n\n         - if it is on the furthest right column, then it should stay on the\n           furthest right column if possible\n\n         - likewise on the left or in the middle\n\n        So really both the `leftIndex` and the `cursorIndex` should move in\n        tandem until things are correct.'''\n\n        targetIdx = self.leftVisibleColIndex  # for rightmost column\n        firstNonKeyVisibleColIndex = self.visibleCols.index(self.nonKeyVisibleCols[0])\n        while self.rightVisibleColIndex != targetIdx and self.leftVisibleColIndex > firstNonKeyVisibleColIndex:\n            self.cursorVisibleColIndex -= 1\n            self.leftVisibleColIndex -= 1\n            self.calcColLayout()  # recompute rightVisibleColIndex\n\n        # in case that rightmost column is last column, try to squeeze maximum real estate from screen\n        if self.rightVisibleColIndex == self.nVisibleCols-1:\n            # try to move further left while right column is still full width\n            while self.leftVisibleColIndex > 0:\n                rightcol = self.visibleCols[self.rightVisibleColIndex]\n                if rightcol.width > self.visibleColLayout[self.rightVisibleColIndex][1]:\n                    # went too far\n                    self.cursorVisibleColIndex += 1\n                    self.leftVisibleColIndex += 1\n                    break\n                else:\n                    self.cursorVisibleColIndex -= 1\n                    self.leftVisibleColIndex -= 1\n                    self.calcColLayout()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addColumn(self, col, index=None):\n        'Insert column at given index or after all columns.'\n        if col:\n            if index is None:\n                index = len(self.columns)\n            col.sheet = self\n            self.columns.insert(index, col)\n            return col", "response": "Insert column at given index or after all columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rowkey(self, row):\n        'returns a tuple of the key for the given row'\n        return tuple(c.getTypedValueOrException(row) for c in self.keyCols)", "response": "returns a tuple of the key for the given row"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkCursor(self):\n        'Keep cursor in bounds of data and screen.'\n        # keep cursor within actual available rowset\n        if self.nRows == 0 or self.cursorRowIndex <= 0:\n            self.cursorRowIndex = 0\n        elif self.cursorRowIndex >= self.nRows:\n            self.cursorRowIndex = self.nRows-1\n\n        if self.cursorVisibleColIndex <= 0:\n            self.cursorVisibleColIndex = 0\n        elif self.cursorVisibleColIndex >= self.nVisibleCols:\n            self.cursorVisibleColIndex = self.nVisibleCols-1\n\n        if self.topRowIndex <= 0:\n            self.topRowIndex = 0\n        elif self.topRowIndex > self.nRows-1:\n            self.topRowIndex = self.nRows-1\n\n        # (x,y) is relative cell within screen viewport\n        x = self.cursorVisibleColIndex - self.leftVisibleColIndex\n        y = self.cursorRowIndex - self.topRowIndex + 1  # header\n\n        # check bounds, scroll if necessary\n        if y < 1:\n            self.topRowIndex = self.cursorRowIndex\n        elif y > self.nVisibleRows:\n            self.topRowIndex = self.cursorRowIndex-self.nVisibleRows+1\n\n        if x <= 0:\n            self.leftVisibleColIndex = self.cursorVisibleColIndex\n        else:\n            while True:\n                if self.leftVisibleColIndex == self.cursorVisibleColIndex:  # not much more we can do\n                    break\n                self.calcColLayout()\n                mincolidx, maxcolidx = min(self.visibleColLayout.keys()), max(self.visibleColLayout.keys())\n                if self.cursorVisibleColIndex < mincolidx:\n                    self.leftVisibleColIndex -= max((self.cursorVisibleColIndex - mincolid)//2, 1)\n                    continue\n                elif self.cursorVisibleColIndex > maxcolidx:\n                    self.leftVisibleColIndex += max((maxcolidx - self.cursorVisibleColIndex)//2, 1)\n                    continue\n\n                cur_x, cur_w = self.visibleColLayout[self.cursorVisibleColIndex]\n                if cur_x+cur_w < self.vd.windowWidth:  # current columns fit entirely on screen\n                    break\n                self.leftVisibleColIndex += 1", "response": "Keep cursor in bounds of data and screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calcColLayout(self):\n        'Set right-most visible column, based on calculation.'\n        minColWidth = len(options.disp_more_left)+len(options.disp_more_right)\n        sepColWidth = len(options.disp_column_sep)\n        winWidth = self.vd.windowWidth\n        self.visibleColLayout = {}\n        x = 0\n        vcolidx = 0\n        for vcolidx in range(0, self.nVisibleCols):\n            col = self.visibleCols[vcolidx]\n            if col.width is None and len(self.visibleRows) > 0:\n                # handle delayed column width-finding\n                col.width = col.getMaxWidth(self.visibleRows)+minColWidth\n                if vcolidx != self.nVisibleCols-1:  # let last column fill up the max width\n                    col.width = min(col.width, options.default_width)\n            width = col.width if col.width is not None else options.default_width\n            if col in self.keyCols:\n                width = max(width, 1)  # keycols must all be visible\n            if col in self.keyCols or vcolidx >= self.leftVisibleColIndex:  # visible columns\n                self.visibleColLayout[vcolidx] = [x, min(width, winWidth-x)]\n                x += width+sepColWidth\n            if x > winWidth-1:\n                break\n\n        self.rightVisibleColIndex = vcolidx", "response": "Set right - most visible column based on calculation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompose and draw column header for given vcolidx.", "response": "def drawColHeader(self, scr, y, vcolidx):\n        'Compose and draw column header for given vcolidx.'\n        col = self.visibleCols[vcolidx]\n\n        # hdrattr highlights whole column header\n        # sepattr is for header separators and indicators\n        sepattr = colors.color_column_sep\n\n        hdrattr = self.colorize(col, None)\n        if vcolidx == self.cursorVisibleColIndex:\n            hdrattr = hdrattr.update_attr(colors.color_current_hdr, 2)\n\n        C = options.disp_column_sep\n        if (self.keyCols and col is self.keyCols[-1]) or vcolidx == self.rightVisibleColIndex:\n            C = options.disp_keycol_sep\n\n        x, colwidth = self.visibleColLayout[vcolidx]\n\n        # ANameTC\n        T = getType(col.type).icon\n        if T is None:  # still allow icon to be explicitly non-displayed ''\n            T = '?'\n        N = ' ' + col.name  # save room at front for LeftMore\n        if len(N) > colwidth-1:\n            N = N[:colwidth-len(options.disp_truncator)] + options.disp_truncator\n        clipdraw(scr, y, x, N, hdrattr.attr, colwidth)\n        clipdraw(scr, y, x+colwidth-len(T), T, hdrattr.attr, len(T))\n        vd.onMouse(scr, y, x, 1, colwidth, BUTTON3_RELEASED='rename-col')\n\n        if vcolidx == self.leftVisibleColIndex and col not in self.keyCols and self.nonKeyVisibleCols.index(col) > 0:\n            A = options.disp_more_left\n            scr.addstr(y, x, A, sepattr)\n\n        if C and x+colwidth+len(C) < self.vd.windowWidth:\n            scr.addstr(y, x+colwidth, C, sepattr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndraws entire screen onto the scr curses object.", "response": "def draw(self, scr):\n        'Draw entire screen onto the `scr` curses object.'\n        numHeaderRows = 1\n        scr.erase()  # clear screen before every re-draw\n\n        vd().refresh()\n\n        if not self.columns:\n            return\n\n        color_current_row = CursesAttr(colors.color_current_row, 5)\n        disp_column_sep = options.disp_column_sep\n\n        rowattrs = {}  # [rowidx] -> attr\n        colattrs = {}  # [colidx] -> attr\n        isNull = isNullFunc()\n\n        self.rowLayout = {}\n        self.calcColLayout()\n        vcolidx = 0\n        rows = list(self.rows[self.topRowIndex:self.topRowIndex+self.nVisibleRows])\n        for vcolidx, colinfo in sorted(self.visibleColLayout.items()):\n            x, colwidth = colinfo\n            col = self.visibleCols[vcolidx]\n\n            if x < self.vd.windowWidth:  # only draw inside window\n                headerRow = 0\n                self.drawColHeader(scr, headerRow, vcolidx)\n\n                y = headerRow + numHeaderRows\n                for rowidx in range(0, min(len(rows), self.nVisibleRows)):\n                    dispRowIdx = self.topRowIndex + rowidx\n                    if dispRowIdx >= self.nRows:\n                        break\n\n                    self.rowLayout[dispRowIdx] = y\n\n                    row = rows[rowidx]\n                    cellval = col.getCell(row, colwidth-1)\n                    try:\n                        if isNull(cellval.value):\n                            cellval.note = options.disp_note_none\n                            cellval.notecolor = 'color_note_type'\n                    except TypeError:\n                        pass\n\n                    attr = self.colorize(col, row, cellval)\n\n                    # sepattr is the attr between cell/columns\n                    rowattr = rowattrs.get(rowidx)\n                    if rowattr is None:\n                        rowattr = rowattrs[rowidx] = self.colorize(None, row)\n                    sepattr = rowattr\n\n                    # must apply current row here, because this colorization requires cursorRowIndex\n                    if dispRowIdx == self.cursorRowIndex:\n                        attr = attr.update_attr(color_current_row)\n                        sepattr = sepattr.update_attr(color_current_row)\n\n                    note = getattr(cellval, 'note', None)\n                    if note:\n                        noteattr = attr.update_attr(colors.get_color(cellval.notecolor), 10)\n                        clipdraw(scr, y, x+colwidth-len(note), note, noteattr.attr, len(note))\n\n                    clipdraw(scr, y, x, disp_column_fill+cellval.display, attr.attr, colwidth-(1 if note else 0))\n                    vd.onMouse(scr, y, x, 1, colwidth, BUTTON3_RELEASED='edit-cell')\n\n                    sepchars = disp_column_sep\n                    if (self.keyCols and col is self.keyCols[-1]) or vcolidx == self.rightVisibleColIndex:\n                        sepchars = options.disp_keycol_sep\n\n                    if x+colwidth+len(sepchars) <= self.vd.windowWidth:\n                       scr.addstr(y, x+colwidth, sepchars, sepattr.attr)\n\n                    y += 1\n\n        if vcolidx+1 < self.nVisibleCols:\n            scr.addstr(headerRow, self.vd.windowWidth-2, options.disp_more_right, colors.color_column_sep)\n\n        catchapply(self.checkCursor)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall editText at its place on the screen. Returns the new value properly typed", "response": "def editCell(self, vcolidx=None, rowidx=None, **kwargs):\n        'Call `editText` at its place on the screen.  Returns the new value, properly typed'\n\n        if vcolidx is None:\n            vcolidx = self.cursorVisibleColIndex\n        x, w = self.visibleColLayout.get(vcolidx, (0, 0))\n\n        col = self.visibleCols[vcolidx]\n        if rowidx is None:\n            rowidx = self.cursorRowIndex\n        if rowidx < 0:  # header\n            y = 0\n            currentValue = col.name\n        else:\n            y = self.rowLayout.get(rowidx, 0)\n            currentValue = col.getDisplayValue(self.rows[self.cursorRowIndex])\n\n        editargs = dict(value=currentValue,\n                        fillchar=options.disp_edit_fill,\n                        truncchar=options.disp_truncator)\n        editargs.update(kwargs)  # update with user-specified args\n        r = self.vd.editText(y, x, w, **editargs)\n        if rowidx >= 0:  # if not header\n            r = col.type(r)  # convert input to column type, let exceptions be raised\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting column cache attach to sheet and reify name", "response": "def recalc(self, sheet=None):\n        'reset column cache, attach to sheet, and reify name'\n        if self._cachedValues:\n            self._cachedValues.clear()\n        if sheet:\n            self.sheet = sheet\n        self.name = self._name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format(self, typedval):\n        'Return displayable string of `typedval` according to `Column.fmtstr`'\n        if typedval is None:\n            return None\n\n        if isinstance(typedval, (list, tuple)):\n            return '[%s]' % len(typedval)\n        if isinstance(typedval, dict):\n            return '{%s}' % len(typedval)\n        if isinstance(typedval, bytes):\n            typedval = typedval.decode(options.encoding, options.encoding_errors)\n\n        return getType(self.type).formatter(self.fmtstr, typedval)", "response": "Return displayable string of typedval according to Column. fmtstr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getValueRows(self, rows):\n        'Generate (val, row) for the given `rows` at this Column, excluding errors and nulls.'\n        f = isNullFunc()\n\n        for r in Progress(rows, 'calculating'):\n            try:\n                v = self.getTypedValue(r)\n                if not f(v):\n                    yield v, r\n            except Exception:\n                pass", "response": "Generate ( val row ) for the given rows at this Column excluding errors and nulls."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the properly - typed value for the given row at this column.", "response": "def getTypedValue(self, row):\n        'Returns the properly-typed value for the given row at this column.'\n        return wrapply(self.type, wrapply(self.getValue, row))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the properly - typed value for the given row at this column or an Exception object.", "response": "def getTypedValueOrException(self, row):\n        'Returns the properly-typed value for the given row at this column, or an Exception object.'\n        return wrapply(self.type, wrapply(self.getValue, row))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the properly - typed value for the given row at this column. Returns the type s default value if either the getter or the type conversion fails.", "response": "def getTypedValueNoExceptions(self, row):\n        '''Returns the properly-typed value for the given row at this column.\n           Returns the type's default value if either the getter or the type conversion fails.'''\n        return wrapply(self.type, wrapply(self.getValue, row))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getCell(self, row, width=None):\n        'Return DisplayWrapper for displayable cell value.'\n        cellval = wrapply(self.getValue, row)\n        typedval = wrapply(self.type, cellval)\n\n        if isinstance(typedval, TypedWrapper):\n            if isinstance(cellval, TypedExceptionWrapper):  # calc failed\n                exc = cellval.exception\n                if cellval.forwarded:\n                    dispval = str(cellval)  # traceback.format_exception_only(type(exc), exc)[-1].strip()\n                else:\n                    dispval = options.disp_error_val\n                return DisplayWrapper(cellval.val, error=exc.stacktrace,\n                                        display=dispval,\n                                        note=options.note_getter_exc,\n                                        notecolor='color_error')\n            elif typedval.val is None:  # early out for strict None\n                return DisplayWrapper(None, display='',  # force empty display for None\n                                            note=options.disp_note_none,\n                                            notecolor='color_note_type')\n            elif isinstance(typedval, TypedExceptionWrapper):  # calc succeeded, type failed\n                return DisplayWrapper(typedval.val, display=str(cellval),\n                                            error=typedval.exception.stacktrace,\n                                            note=options.note_type_exc,\n                                            notecolor='color_warning')\n            else:\n                return DisplayWrapper(typedval.val, display=str(typedval.val),\n                                            note=options.note_type_exc,\n                                            notecolor='color_warning')\n\n        elif isinstance(typedval, threading.Thread):\n            return DisplayWrapper(None,\n                                display=options.disp_pending,\n                                note=options.note_pending,\n                                notecolor='color_note_pending')\n\n        dw = DisplayWrapper(cellval)\n\n        try:\n            dw.display = self.format(typedval) or ''\n\n            if width and isNumeric(self):\n                dw.display = dw.display.rjust(width-1)\n\n            # annotate cells with raw value type in anytype columns, except for strings\n            if self.type is anytype and type(cellval) is not str:\n                typedesc = typemap.get(type(cellval), None)\n                dw.note = typedesc.icon if typedesc else options.note_unknown_type\n                dw.notecolor = 'color_note_type'\n\n        except Exception as e:  # formatting failure\n            e.stacktrace = stacktrace()\n            dw.error = e\n            try:\n                dw.display = str(cellval)\n            except Exception as e:\n                dw.display = str(e)\n            dw.note = options.note_format_exc\n            dw.notecolor = 'color_warning'\n\n        return dw", "response": "Return DisplayWrapper for displayable cell value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setValueSafe(self, row, value):\n        'setValue and ignore exceptions'\n        try:\n            return self.setValue(row, value)\n        except Exception as e:\n            exceptionCaught(e)", "response": "setValue and ignore exceptions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setValues(self, rows, *values):\n        'Set our column value for given list of rows to `value`.'\n        for r, v in zip(rows, itertools.cycle(values)):\n            self.setValueSafe(r, v)\n        self.recalc()\n        return status('set %d cells to %d values' % (len(rows), len(values)))", "response": "Set our column value for given list of rows to value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset values on this column for rows coerced to the column type. Will stop on first exception in type().", "response": "def setValuesTyped(self, rows, *values):\n        'Set values on this column for rows, coerced to the column type.  will stop on first exception in type().'\n        for r, v in zip(rows, itertools.cycle(self.type(val) for val in values)):\n            self.setValueSafe(r, v)\n        self.recalc()\n        return status('set %d cells to %d values' % (len(rows), len(values)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getMaxWidth(self, rows):\n        'Return the maximum length of any cell in column or its header.'\n        w = 0\n        if len(rows) > 0:\n            w = max(max(len(self.getDisplayValue(r)) for r in rows), len(self.name))+2\n        return max(w, len(self.name))", "response": "Return the maximum length of any cell in column or its header."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges column width to either given width.", "response": "def toggleWidth(self, width):\n        'Change column width to either given `width` or default value.'\n        if self.width != width:\n            self.width = width\n        else:\n            self.width = int(options.default_width)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the curses attribute for the colorstack a list of color option names sorted highest - precedence color first.", "response": "def resolve_colors(self, colorstack):\n        'Returns the curses attribute for the colorstack, a list of color option names sorted highest-precedence color first.'\n        attr = CursesAttr()\n        for coloropt in colorstack:\n            c = self.get_color(coloropt)\n            attr = attr.update_attr(c)\n        return attr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefines aggregator name that calls func ( col rows )", "response": "def _defaggr(name, type, func):\n    'Define aggregator `name` that calls func(col, rows)'\n    func.type=type\n    func.__name__ = name\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef aggregator(name, func, *args, type=None):\n    'Define simple aggregator `name` that calls func(values)'\n    def _func(col, rows):  # wrap builtins so they can have a .type\n        vals = list(col.getValues(rows))\n        try:\n            return func(vals, *args)\n        except Exception as e:\n            if len(vals) == 0:\n                return None\n            return e\n\n    aggregators[name] = _defaggr(name, type, _func)", "response": "Define simple aggregator name that calls func ( values )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the percentile of a list of values.", "response": "def _percentile(N, percent, key=lambda x:x):\n    \"\"\"\n    Find the percentile of a list of values.\n\n    @parameter N - is a list of values. Note N MUST BE already sorted.\n    @parameter percent - a float value from 0.0 to 1.0.\n    @parameter key - optional key function to compute value from each element of N.\n\n    @return - the percentile of the values\n    \"\"\"\n    if not N:\n        return None\n    k = (len(N)-1) * percent\n    f = math.floor(k)\n    c = math.ceil(k)\n    if f == c:\n        return key(N[int(k)])\n    d0 = key(N[int(f)]) * (c-k)\n    d1 = key(N[int(c)]) * (k-f)\n    return d0+d1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addAggregators(cols, aggrnames):\n    'add aggregator for each aggrname to each of cols'\n    for aggrname in aggrnames:\n        aggrs = aggregators.get(aggrname)\n        aggrs = aggrs if isinstance(aggrs, list) else [aggrs]\n        for aggr in aggrs:\n            for c in cols:\n                if not hasattr(c, 'aggregators'):\n                    c.aggregators = []\n                if aggr and aggr not in c.aggregators:\n                    c.aggregators += [aggr]", "response": "add aggregator for each aggrname to each of cols"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef indexMatch(L, func):\n    'returns the smallest i for which func(L[i]) is true'\n    for i, x in enumerate(L):\n        if func(x):\n            return i", "response": "returns the smallest i for which func ( L [ i ] is true"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef removeSheet(self, vs):\n        'Remove all traces of sheets named vs.name from the cmdlog.'\n        self.rows = [r for r in self.rows if r.sheet != vs.name]\n        status('removed \"%s\" from cmdlog' % vs.name)", "response": "Remove all traces of sheets named vs. name from the cmdlog."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the sheet / row / col to the values in the replay row. return sheet", "response": "def moveToReplayContext(self, r):\n        'set the sheet/row/col to the values in the replay row.  return sheet'\n        if not r.sheet:\n#            assert not r.col and not r.row\n            return self  # any old sheet should do, row/column don't matter\n\n        try:\n            sheetidx = int(r.sheet)\n            vs = vd().sheets[sheetidx]\n        except ValueError:\n            vs = vd().getSheet(r.sheet) or error('no sheet named %s' % r.sheet)\n\n        if r.row:\n            try:\n                rowidx = int(r.row)\n            except ValueError:\n                rowidx = indexMatch(vs.rows, lambda r,vs=vs,k=r.row: keystr(vs.rowkey(r)) == k)\n\n            if rowidx is None:\n                error('no \"%s\" row' % r.row)\n\n            if options.replay_movement:\n                while vs.cursorRowIndex != rowidx:\n                    vs.cursorRowIndex += 1 if (rowidx - vs.cursorRowIndex) > 0 else -1\n                    while not self.delay(0.5):\n                        pass\n            else:\n                vs.cursorRowIndex = rowidx\n\n        if r.col:\n            try:\n                vcolidx = int(r.col)\n            except ValueError:\n                vcolidx = indexMatch(vs.visibleCols, lambda c,name=r.col: name == c.name)\n\n            if vcolidx is None:\n                error('no \"%s\" column' % r.col)\n\n            if options.replay_movement:\n                while vs.cursorVisibleColIndex != vcolidx:\n                    vs.cursorVisibleColIndex += 1 if (vcolidx - vs.cursorVisibleColIndex) > 0 else -1\n                    while not self.delay(0.5):\n                        pass\n\n                assert vs.cursorVisibleColIndex == vcolidx\n            else:\n                vs.cursorVisibleColIndex = vcolidx\n        return vs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delay(self, factor=1):\n        'returns True if delay satisfied'\n        acquired = CommandLog.semaphore.acquire(timeout=options.replay_wait*factor if not self.paused else None)\n        return acquired or not self.paused", "response": "returns True if delay satisfied"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplaying the command in one given row.", "response": "def replayOne(self, r):\n        'Replay the command in one given row.'\n        CommandLog.currentReplayRow = r\n\n        longname = getattr(r, 'longname', None)\n        if longname == 'set-option':\n            try:\n                options.set(r.row, r.input, options._opts.getobj(r.col))\n                escaped = False\n            except Exception as e:\n                exceptionCaught(e)\n                escaped = True\n        else:\n            vs = self.moveToReplayContext(r)\n\n            vd().keystrokes = r.keystrokes\n            # <=v1.2 used keystrokes in longname column; getCommand fetches both\n            escaped = vs.exec_command(vs.getCommand(longname if longname else r.keystrokes), keystrokes=r.keystrokes)\n\n        CommandLog.currentReplayRow = None\n\n        if escaped:  # escape during replay aborts replay\n            warning('replay aborted')\n        return escaped"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplays all commands in log.", "response": "def replay_sync(self, live=False):\n        'Replay all commands in log.'\n        self.cursorRowIndex = 0\n        CommandLog.currentReplay = self\n        with Progress(total=len(self.rows)) as prog:\n            while self.cursorRowIndex < len(self.rows):\n                if CommandLog.currentReplay is None:\n                    status('replay canceled')\n                    return\n\n                vd().statuses.clear()\n                try:\n                    if self.replayOne(self.cursorRow):\n                        self.cancel()\n                        return\n                except Exception as e:\n                    self.cancel()\n                    exceptionCaught(e)\n                    status('replay canceled')\n                    return\n\n                self.cursorRowIndex += 1\n                prog.addProgress(1)\n\n                sync(1 if live else 0)  # expect this thread also if playing live\n                while not self.delay():\n                    pass\n\n        status('replay complete')\n        CommandLog.currentReplay = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset user input on last command if not already set.", "response": "def setLastArgs(self, args):\n        'Set user input on last command, if not already set.'\n        # only set if not already set (second input usually confirmation)\n        if self.currentActiveRow is not None:\n            if not self.currentActiveRow.input:\n                self.currentActiveRow.input = args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload service account credentials from JSON file or path.", "response": "def get_service_account_credentials(private_key):\n    \"\"\"DEPRECATED: Load service account credentials from key data or key path.\"\"\"\n\n    import google.auth.transport.requests\n    from google.oauth2.service_account import Credentials\n\n    is_path = os.path.isfile(private_key)\n\n    try:\n        if is_path:\n            with open(private_key) as f:\n                json_key = json.loads(f.read())\n        else:\n            # ugly hack: 'private_key' field has new lines inside,\n            # they break json parser, but we need to preserve them\n            json_key = json.loads(private_key.replace(\"\\n\", \"   \"))\n            json_key[\"private_key\"] = json_key[\"private_key\"].replace(\n                \"   \", \"\\n\"\n            )\n\n        json_key[\"private_key\"] = bytes(json_key[\"private_key\"], \"UTF-8\")\n        credentials = Credentials.from_service_account_info(json_key)\n        credentials = credentials.with_scopes(SCOPES)\n\n        # Refresh the token before trying to use it.\n        request = google.auth.transport.requests.Request()\n        credentials.refresh(request)\n\n        return credentials, json_key.get(\"project_id\")\n    except (KeyError, ValueError, TypeError, AttributeError):\n        raise pandas_gbq.exceptions.InvalidPrivateKeyFormat(\n            \"Detected private_key as {}. \".format(\n                \"path\" if is_path else \"contents\"\n            )\n            + \"Private key is missing or invalid. It should be service \"\n            \"account private key JSON (file path or string contents) \"\n            'with at least two keys: \"client_email\" and \"private_key\". '\n            \"Can be obtained from: https://console.developers.google.\"\n            \"com/permissions/serviceaccounts\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a file - like object of CSV - encoded rows.", "response": "def encode_chunk(dataframe):\n    \"\"\"Return a file-like object of CSV-encoded rows.\n\n    Args:\n      dataframe (pandas.DataFrame): A chunk of a dataframe to encode\n    \"\"\"\n    csv_buffer = six.StringIO()\n    dataframe.to_csv(\n        csv_buffer,\n        index=False,\n        header=False,\n        encoding=\"utf-8\",\n        float_format=\"%.15g\",\n        date_format=\"%Y-%m-%d %H:%M:%S.%f\",\n    )\n\n    # Convert to a BytesIO buffer so that unicode text is properly handled.\n    # See: https://github.com/pydata/pandas-gbq/issues/106\n    body = csv_buffer.getvalue()\n    if isinstance(body, bytes):\n        body = body.decode(\"utf-8\")\n    body = body.encode(\"utf-8\")\n    return six.BytesIO(body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bqschema_to_nullsafe_dtypes(schema_fields):\n    # If you update this mapping, also update the table at\n    # `docs/source/reading.rst`.\n    dtype_map = {\n        \"FLOAT\": np.dtype(float),\n        # pandas doesn't support timezone-aware dtype in DataFrame/Series\n        # constructors. It's more idiomatic to localize after construction.\n        # https://github.com/pandas-dev/pandas/issues/25843\n        \"TIMESTAMP\": \"datetime64[ns]\",\n        \"TIME\": \"datetime64[ns]\",\n        \"DATE\": \"datetime64[ns]\",\n        \"DATETIME\": \"datetime64[ns]\",\n    }\n\n    dtypes = {}\n    for field in schema_fields:\n        name = str(field[\"name\"])\n        if field[\"mode\"].upper() == \"REPEATED\":\n            continue\n\n        dtype = dtype_map.get(field[\"type\"].upper())\n        if dtype:\n            dtypes[name] = dtype\n\n    return dtypes", "response": "Specify explicit dtypes based on BigQuery schema."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncasting any columns in an empty dataframe to correct types.", "response": "def _cast_empty_df_dtypes(schema_fields, df):\n    \"\"\"Cast any columns in an empty dataframe to correct type.\n\n    In an empty dataframe, pandas cannot choose a dtype unless one is\n    explicitly provided. The _bqschema_to_nullsafe_dtypes() function only\n    provides dtypes when the dtype safely handles null values. This means\n    that empty int64 and boolean columns are incorrectly classified as\n    ``object``.\n    \"\"\"\n    if not df.empty:\n        raise ValueError(\n            \"DataFrame must be empty in order to cast non-nullsafe dtypes\"\n        )\n\n    dtype_map = {\"BOOLEAN\": bool, \"INTEGER\": np.int64}\n\n    for field in schema_fields:\n        column = str(field[\"name\"])\n        if field[\"mode\"].upper() == \"REPEATED\":\n            continue\n\n        dtype = dtype_map.get(field[\"type\"].upper())\n        if dtype:\n            df[column] = df[column].astype(dtype)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlocalizes any TIMESTAMP columns to tz - aware type.", "response": "def _localize_df(schema_fields, df):\n    \"\"\"Localize any TIMESTAMP columns to tz-aware type.\n\n    In pandas versions before 0.24.0, DatetimeTZDtype cannot be used as the\n    dtype in Series/DataFrame construction, so localize those columns after\n    the DataFrame is constructed.\n    \"\"\"\n    for field in schema_fields:\n        column = str(field[\"name\"])\n        if field[\"mode\"].upper() == \"REPEATED\":\n            continue\n\n        if field[\"type\"].upper() == \"TIMESTAMP\" and df[column].dt.tz is None:\n            df[column] = df[column].dt.tz_localize(\"UTC\")\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_gbq(\n    query,\n    project_id=None,\n    index_col=None,\n    col_order=None,\n    reauth=False,\n    auth_local_webserver=False,\n    dialect=None,\n    location=None,\n    configuration=None,\n    credentials=None,\n    use_bqstorage_api=False,\n    verbose=None,\n    private_key=None,\n):\n    r\"\"\"Load data from Google BigQuery using google-cloud-python\n\n    The main method a user calls to execute a Query in Google BigQuery\n    and read results into a pandas DataFrame.\n\n    This method uses the Google Cloud client library to make requests to\n    Google BigQuery, documented `here\n    <https://google-cloud-python.readthedocs.io/en/latest/bigquery/usage.html>`__.\n\n    See the :ref:`How to authenticate with Google BigQuery <authentication>`\n    guide for authentication instructions.\n\n    Parameters\n    ----------\n    query : str\n        SQL-Like Query to return data values.\n    project_id : str, optional\n        Google BigQuery Account project ID. Optional when available from\n        the environment.\n    index_col : str, optional\n        Name of result column to use for index in results DataFrame.\n    col_order : list(str), optional\n        List of BigQuery column names in the desired order for results\n        DataFrame.\n    reauth : boolean, default False\n        Force Google BigQuery to re-authenticate the user. This is useful\n        if multiple accounts are used.\n    auth_local_webserver : boolean, default False\n        Use the `local webserver flow`_ instead of the `console flow`_\n        when getting user credentials.\n\n        .. _local webserver flow:\n            http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n        .. _console flow:\n            http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n        .. versionadded:: 0.2.0\n    dialect : str, default 'standard'\n        Note: The default value changed to 'standard' in version 0.10.0.\n\n        SQL syntax dialect to use. Value can be one of:\n\n        ``'legacy'``\n            Use BigQuery's legacy SQL dialect. For more information see\n            `BigQuery Legacy SQL Reference\n            <https://cloud.google.com/bigquery/docs/reference/legacy-sql>`__.\n        ``'standard'``\n            Use BigQuery's standard SQL, which is\n            compliant with the SQL 2011 standard. For more information\n            see `BigQuery Standard SQL Reference\n            <https://cloud.google.com/bigquery/docs/reference/standard-sql/>`__.\n    location : str, optional\n        Location where the query job should run. See the `BigQuery locations\n        documentation\n        <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n        list of available locations. The location must match that of any\n        datasets used in the query.\n\n        .. versionadded:: 0.5.0\n    configuration : dict, optional\n        Query config parameters for job processing.\n        For example:\n\n            configuration = {'query': {'useQueryCache': False}}\n\n        For more information see `BigQuery REST API Reference\n        <https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query>`__.\n    credentials : google.auth.credentials.Credentials, optional\n        Credentials for accessing Google APIs. Use this parameter to override\n        default credentials, such as to use Compute Engine\n        :class:`google.auth.compute_engine.Credentials` or Service Account\n        :class:`google.oauth2.service_account.Credentials` directly.\n\n        .. versionadded:: 0.8.0\n    use_bqstorage_api : bool, default False\n        Use the `BigQuery Storage API\n        <https://cloud.google.com/bigquery/docs/reference/storage/>`__ to\n        download query results quickly, but at an increased cost. To use this\n        API, first `enable it in the Cloud Console\n        <https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com>`__.\n        You must also have the `bigquery.readsessions.create\n        <https://cloud.google.com/bigquery/docs/access-control#roles>`__\n        permission on the project you are billing queries to.\n\n        **Note:** Due to a `known issue in the ``google-cloud-bigquery``\n        package\n        <https://github.com/googleapis/google-cloud-python/pull/7633>`__\n        (fixed in version 1.11.0), you must write your query results to a\n        destination table. To do this with ``read_gbq``, supply a\n        ``configuration`` dictionary.\n\n        This feature requires the ``google-cloud-bigquery-storage`` and\n        ``fastavro`` packages.\n\n        .. versionadded:: 0.10.0\n    verbose : None, deprecated\n        Deprecated in Pandas-GBQ 0.4.0. Use the `logging module\n        to adjust verbosity instead\n        <https://pandas-gbq.readthedocs.io/en/latest/intro.html#logging>`__.\n    private_key : str, deprecated\n        Deprecated in pandas-gbq version 0.8.0. Use the ``credentials``\n        parameter and\n        :func:`google.oauth2.service_account.Credentials.from_service_account_info`\n        or\n        :func:`google.oauth2.service_account.Credentials.from_service_account_file`\n        instead.\n\n        Service account private key in JSON format. Can be file path\n        or string contents. This is useful for remote server\n        authentication (eg. Jupyter/IPython notebook on remote host).\n\n    Returns\n    -------\n    df: DataFrame\n        DataFrame representing results of query.\n    \"\"\"\n    global context\n\n    if dialect is None:\n        dialect = context.dialect\n\n    if dialect is None:\n        dialect = \"standard\"\n\n    _test_google_api_imports()\n\n    if verbose is not None and SHOW_VERBOSE_DEPRECATION:\n        warnings.warn(\n            \"verbose is deprecated and will be removed in \"\n            \"a future version. Set logging level in order to vary \"\n            \"verbosity\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if private_key is not None and SHOW_PRIVATE_KEY_DEPRECATION:\n        warnings.warn(\n            PRIVATE_KEY_DEPRECATION_MESSAGE, FutureWarning, stacklevel=2\n        )\n\n    if dialect not in (\"legacy\", \"standard\"):\n        raise ValueError(\"'{0}' is not valid for dialect\".format(dialect))\n\n    connector = GbqConnector(\n        project_id,\n        reauth=reauth,\n        dialect=dialect,\n        auth_local_webserver=auth_local_webserver,\n        location=location,\n        credentials=credentials,\n        private_key=private_key,\n        use_bqstorage_api=use_bqstorage_api,\n    )\n\n    final_df = connector.run_query(query, configuration=configuration)\n\n    # Reindex the DataFrame on the provided column\n    if index_col is not None:\n        if index_col in final_df.columns:\n            final_df.set_index(index_col, inplace=True)\n        else:\n            raise InvalidIndexColumn(\n                'Index column \"{0}\" does not exist in DataFrame.'.format(\n                    index_col\n                )\n            )\n\n    # Change the order of columns in the DataFrame based on provided list\n    if col_order is not None:\n        if sorted(col_order) == sorted(final_df.columns):\n            final_df = final_df[col_order]\n        else:\n            raise InvalidColumnOrder(\n                \"Column order does not match this DataFrame.\"\n            )\n\n    connector.log_elapsed_seconds(\n        \"Total time taken\",\n        datetime.now().strftime(\"s.\\nFinished at %Y-%m-%d %H:%M:%S.\"),\n    )\n\n    return final_df", "response": "r Reads data from Google BigQuery using google - cloud - python."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a dataframe to a Google BigQuery table.", "response": "def to_gbq(\n    dataframe,\n    destination_table,\n    project_id=None,\n    chunksize=None,\n    reauth=False,\n    if_exists=\"fail\",\n    auth_local_webserver=False,\n    table_schema=None,\n    location=None,\n    progress_bar=True,\n    credentials=None,\n    verbose=None,\n    private_key=None,\n):\n    \"\"\"Write a DataFrame to a Google BigQuery table.\n\n    The main method a user calls to export pandas DataFrame contents to\n    Google BigQuery table.\n\n    This method uses the Google Cloud client library to make requests to\n    Google BigQuery, documented `here\n    <https://google-cloud-python.readthedocs.io/en/latest/bigquery/usage.html>`__.\n\n    See the :ref:`How to authenticate with Google BigQuery <authentication>`\n    guide for authentication instructions.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        DataFrame to be written to a Google BigQuery table.\n    destination_table : str\n        Name of table to be written, in the form ``dataset.tablename``.\n    project_id : str, optional\n        Google BigQuery Account project ID. Optional when available from\n        the environment.\n    chunksize : int, optional\n        Number of rows to be inserted in each chunk from the dataframe.\n        Set to ``None`` to load the whole dataframe at once.\n    reauth : bool, default False\n        Force Google BigQuery to re-authenticate the user. This is useful\n        if multiple accounts are used.\n    if_exists : str, default 'fail'\n        Behavior when the destination table exists. Value can be one of:\n\n        ``'fail'``\n            If table exists, do nothing.\n        ``'replace'``\n            If table exists, drop it, recreate it, and insert data.\n        ``'append'``\n            If table exists, insert data. Create if does not exist.\n    auth_local_webserver : bool, default False\n        Use the `local webserver flow`_ instead of the `console flow`_\n        when getting user credentials.\n\n        .. _local webserver flow:\n            http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n        .. _console flow:\n            http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n\n        .. versionadded:: 0.2.0\n    table_schema : list of dicts, optional\n        List of BigQuery table fields to which according DataFrame\n        columns conform to, e.g. ``[{'name': 'col1', 'type':\n        'STRING'},...]``.\n\n        - If ``table_schema`` is provided, it may contain all or a subset of\n          DataFrame columns. If a subset is provided, the rest will be\n          inferred from the DataFrame dtypes.\n        - If ``table_schema`` is **not** provided, it will be\n          generated according to dtypes of DataFrame columns. See\n          `Inferring the Table Schema\n          <https://pandas-gbq.readthedocs.io/en/latest/writing.html#writing-schema>`__.\n          for a description of the schema inference.\n\n        See `BigQuery API documentation on valid column names\n        <https://cloud.google.com/bigquery/docs/schemas#column_names`>__.\n\n        .. versionadded:: 0.3.1\n    location : str, optional\n        Location where the load job should run. See the `BigQuery locations\n        documentation\n        <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n        list of available locations. The location must match that of the\n        target dataset.\n\n        .. versionadded:: 0.5.0\n    progress_bar : bool, default True\n        Use the library `tqdm` to show the progress bar for the upload,\n        chunk by chunk.\n\n        .. versionadded:: 0.5.0\n    credentials : google.auth.credentials.Credentials, optional\n        Credentials for accessing Google APIs. Use this parameter to override\n        default credentials, such as to use Compute Engine\n        :class:`google.auth.compute_engine.Credentials` or Service Account\n        :class:`google.oauth2.service_account.Credentials` directly.\n\n        .. versionadded:: 0.8.0\n    verbose : bool, deprecated\n        Deprecated in Pandas-GBQ 0.4.0. Use the `logging module\n        to adjust verbosity instead\n        <https://pandas-gbq.readthedocs.io/en/latest/intro.html#logging>`__.\n    private_key : str, deprecated\n        Deprecated in pandas-gbq version 0.8.0. Use the ``credentials``\n        parameter and\n        :func:`google.oauth2.service_account.Credentials.from_service_account_info`\n        or\n        :func:`google.oauth2.service_account.Credentials.from_service_account_file`\n        instead.\n\n        Service account private key in JSON format. Can be file path\n        or string contents. This is useful for remote server\n        authentication (eg. Jupyter/IPython notebook on remote host).\n    \"\"\"\n\n    _test_google_api_imports()\n    from pandas_gbq import schema\n\n    if verbose is not None and SHOW_VERBOSE_DEPRECATION:\n        warnings.warn(\n            \"verbose is deprecated and will be removed in \"\n            \"a future version. Set logging level in order to vary \"\n            \"verbosity\",\n            FutureWarning,\n            stacklevel=1,\n        )\n\n    if private_key is not None and SHOW_PRIVATE_KEY_DEPRECATION:\n        warnings.warn(\n            PRIVATE_KEY_DEPRECATION_MESSAGE, FutureWarning, stacklevel=2\n        )\n\n    if if_exists not in (\"fail\", \"replace\", \"append\"):\n        raise ValueError(\"'{0}' is not valid for if_exists\".format(if_exists))\n\n    if \".\" not in destination_table:\n        raise NotFoundException(\n            \"Invalid Table Name. Should be of the form 'datasetId.tableId' \"\n        )\n\n    connector = GbqConnector(\n        project_id,\n        reauth=reauth,\n        auth_local_webserver=auth_local_webserver,\n        location=location,\n        credentials=credentials,\n        private_key=private_key,\n    )\n    dataset_id, table_id = destination_table.rsplit(\".\", 1)\n\n    table = _Table(\n        project_id,\n        dataset_id,\n        location=location,\n        credentials=connector.credentials,\n    )\n\n    default_schema = _generate_bq_schema(dataframe)\n    if not table_schema:\n        table_schema = default_schema\n    else:\n        table_schema = schema.update_schema(\n            default_schema, dict(fields=table_schema)\n        )\n\n    # If table exists, check if_exists parameter\n    if table.exists(table_id):\n        if if_exists == \"fail\":\n            raise TableCreationError(\n                \"Could not create the table because it \"\n                \"already exists. \"\n                \"Change the if_exists parameter to \"\n                \"'append' or 'replace' data.\"\n            )\n        elif if_exists == \"replace\":\n            connector.delete_and_recreate_table(\n                dataset_id, table_id, table_schema\n            )\n        elif if_exists == \"append\":\n            if not connector.schema_is_subset(\n                dataset_id, table_id, table_schema\n            ):\n                raise InvalidSchema(\n                    \"Please verify that the structure and \"\n                    \"data types in the DataFrame match the \"\n                    \"schema of the destination table.\"\n                )\n    else:\n        table.create(table_id, table_schema)\n\n    if dataframe.empty:\n        # Create the table (if needed), but don't try to run a load job with an\n        # empty file. See: https://github.com/pydata/pandas-gbq/issues/237\n        return\n\n    connector.load_data(\n        dataframe,\n        dataset_id,\n        table_id,\n        chunksize=chunksize,\n        schema=table_schema,\n        progress_bar=progress_bar,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndeprecate - Generate a BigQuery schema from a DataFrame.", "response": "def generate_bq_schema(df, default_type=\"STRING\"):\n    \"\"\"DEPRECATED: Given a passed df, generate the associated Google BigQuery\n    schema.\n\n    Parameters\n    ----------\n    df : DataFrame\n    default_type : string\n        The default big query type in case the type of the column\n        does not exist in the schema.\n    \"\"\"\n    # deprecation TimeSeries, #11121\n    warnings.warn(\n        \"generate_bq_schema is deprecated and will be removed in \"\n        \"a future version\",\n        FutureWarning,\n        stacklevel=2,\n    )\n\n    return _generate_bq_schema(df, default_type=default_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the schema of the table.", "response": "def schema(self, dataset_id, table_id):\n        \"\"\"Retrieve the schema of the table\n\n        Obtain from BigQuery the field names and field types\n        for the table defined by the parameters\n\n        Parameters\n        ----------\n        dataset_id : str\n            Name of the BigQuery dataset for the table\n        table_id : str\n            Name of the BigQuery table\n\n        Returns\n        -------\n        list of dicts\n            Fields representing the schema\n        \"\"\"\n        table_ref = self.client.dataset(dataset_id).table(table_id)\n\n        try:\n            table = self.client.get_table(table_ref)\n            remote_schema = table.schema\n\n            remote_fields = [\n                field_remote.to_api_repr() for field_remote in remote_schema\n            ]\n            for field in remote_fields:\n                field[\"type\"] = field[\"type\"].upper()\n                field[\"mode\"] = field[\"mode\"].upper()\n\n            return remote_fields\n        except self.http_error as ex:\n            self.process_http_error(ex)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a sanitized version of the schema for comparisons.", "response": "def _clean_schema_fields(self, fields):\n        \"\"\"Return a sanitized version of the schema for comparisons.\"\"\"\n        fields_sorted = sorted(fields, key=lambda field: field[\"name\"])\n        # Ignore mode and description when comparing schemas.\n        return [\n            {\"name\": field[\"name\"], \"type\": field[\"type\"]}\n            for field in fields_sorted\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_schema(self, dataset_id, table_id, schema):\n\n        fields_remote = self._clean_schema_fields(\n            self.schema(dataset_id, table_id)\n        )\n        fields_local = self._clean_schema_fields(schema[\"fields\"])\n\n        return fields_remote == fields_local", "response": "Verify that the schema of the table identified in dataset_id and table_id are exactly the same as the schema passed in."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schema_is_subset(self, dataset_id, table_id, schema):\n\n        fields_remote = self._clean_schema_fields(\n            self.schema(dataset_id, table_id)\n        )\n        fields_local = self._clean_schema_fields(schema[\"fields\"])\n\n        return all(field in fields_remote for field in fields_local)", "response": "Indicate whether the schema to be uploaded is a subset of the fields in the BigQuery table with the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a table exists in Google BigQuery", "response": "def exists(self, table_id):\n        \"\"\" Check if a table exists in Google BigQuery\n\n        Parameters\n        ----------\n        table : str\n            Name of table to be verified\n\n        Returns\n        -------\n        boolean\n            true if table exists, otherwise false\n        \"\"\"\n        from google.api_core.exceptions import NotFound\n\n        table_ref = self.client.dataset(self.dataset_id).table(table_id)\n        try:\n            self.client.get_table(table_ref)\n            return True\n        except NotFound:\n            return False\n        except self.http_error as ex:\n            self.process_http_error(ex)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, table_id, schema):\n        from google.cloud.bigquery import SchemaField\n        from google.cloud.bigquery import Table\n\n        if self.exists(table_id):\n            raise TableCreationError(\n                \"Table {0} already \" \"exists\".format(table_id)\n            )\n\n        if not _Dataset(self.project_id, credentials=self.credentials).exists(\n            self.dataset_id\n        ):\n            _Dataset(\n                self.project_id,\n                credentials=self.credentials,\n                location=self.location,\n            ).create(self.dataset_id)\n\n        table_ref = self.client.dataset(self.dataset_id).table(table_id)\n        table = Table(table_ref)\n\n        # Manually create the schema objects, adding NULLABLE mode\n        # as a workaround for\n        # https://github.com/GoogleCloudPlatform/google-cloud-python/issues/4456\n        for field in schema[\"fields\"]:\n            if \"mode\" not in field:\n                field[\"mode\"] = \"NULLABLE\"\n\n        table.schema = [\n            SchemaField.from_api_repr(field) for field in schema[\"fields\"]\n        ]\n\n        try:\n            self.client.create_table(table)\n        except self.http_error as ex:\n            self.process_http_error(ex)", "response": "Create a new table in Google BigQuery given a table and schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, table_id):\n        from google.api_core.exceptions import NotFound\n\n        if not self.exists(table_id):\n            raise NotFoundException(\"Table does not exist\")\n\n        table_ref = self.client.dataset(self.dataset_id).table(table_id)\n        try:\n            self.client.delete_table(table_ref)\n        except NotFound:\n            # Ignore 404 error which may occur if table already deleted\n            pass\n        except self.http_error as ex:\n            self.process_http_error(ex)", "response": "Delete a table in Google BigQuery"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a dataset exists in Google BigQuery", "response": "def exists(self, dataset_id):\n        \"\"\" Check if a dataset exists in Google BigQuery\n\n        Parameters\n        ----------\n        dataset_id : str\n            Name of dataset to be verified\n\n        Returns\n        -------\n        boolean\n            true if dataset exists, otherwise false\n        \"\"\"\n        from google.api_core.exceptions import NotFound\n\n        try:\n            self.client.get_dataset(self.client.dataset(dataset_id))\n            return True\n        except NotFound:\n            return False\n        except self.http_error as ex:\n            self.process_http_error(ex)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a dataset in Google BigQuery", "response": "def create(self, dataset_id):\n        \"\"\" Create a dataset in Google BigQuery\n\n        Parameters\n        ----------\n        dataset : str\n            Name of dataset to be written\n        \"\"\"\n        from google.cloud.bigquery import Dataset\n\n        if self.exists(dataset_id):\n            raise DatasetCreationError(\n                \"Dataset {0} already \" \"exists\".format(dataset_id)\n            )\n\n        dataset = Dataset(self.client.dataset(dataset_id))\n\n        if self.location is not None:\n            dataset.location = self.location\n\n        try:\n            self.client.create_dataset(dataset)\n        except self.http_error as ex:\n            self.process_http_error(ex)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an old BigQuery schema update it with a new one.", "response": "def update_schema(schema_old, schema_new):\n    \"\"\"\n    Given an old BigQuery schema, update it with a new one.\n\n    Where a field name is the same, the new will replace the old. Any\n    new fields not present in the old schema will be added.\n\n    Arguments:\n        schema_old: the old schema to update\n        schema_new: the new schema which will overwrite/extend the old\n    \"\"\"\n    old_fields = schema_old[\"fields\"]\n    new_fields = schema_new[\"fields\"]\n    output_fields = list(old_fields)\n\n    field_indices = {field[\"name\"]: i for i, field in enumerate(output_fields)}\n\n    for field in new_fields:\n        name = field[\"name\"]\n        if name in field_indices:\n            # replace old field with new field of same name\n            output_fields[field_indices[name]] = field\n        else:\n            # add new field\n            output_fields.append(field)\n\n    return {\"fields\": output_fields}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean(self):\n        if self.user:\n            self.username = self.user.username\n        elif not self.username:\n            raise ValidationError({\n                'username': _NOT_BLANK_MESSAGE,\n                'user': _NOT_BLANK_MESSAGE\n            })", "response": "Checks that the user has a username and sets it automatically sets username"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting groupname automatically sets groupname automatically sets groupname automatically sets groupname automatically sets groupname automatically sets groupname automatically sets groupname automatically sets groupname automatically sets groupname", "response": "def clean(self):\n        \"\"\"\n        automatically sets groupname\n        \"\"\"\n        super().clean()\n        if self.group:\n            self.groupname = self.group.name\n        elif not self.groupname:\n            raise ValidationError({\n                'groupname': _NOT_BLANK_MESSAGE,\n                'group': _NOT_BLANK_MESSAGE\n            })"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the default group to False", "response": "def set_default(self):\n        \"\"\"\n        ensures there's only 1 default group\n        (logic overridable via custom models)\n        \"\"\"\n        queryset = self.get_default_queryset()\n        if queryset.exists():\n            queryset.update(default=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a queryset of all the default items for this one.", "response": "def get_default_queryset(self):\n        \"\"\"\n        looks for default groups excluding the current one\n        overridable by openwisp-radius and other 3rd party apps\n        \"\"\"\n        return self.__class__.objects.exclude(pk=self.pk) \\\n                                     .filter(default=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_user(self, request):\n        try:\n            return User.objects.get(username=request.data.get('username'),\n                                    is_active=True)\n        except User.DoesNotExist:\n            return None", "response": "Returns the user that is currently active"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the password value supplied is a valid user password or a valid user token.", "response": "def authenticate_user(self, request, user):\n        \"\"\"\n        returns ``True`` if the password value supplied is\n        a valid user password or a valid user token\n        can be overridden to implement more complex checks\n        \"\"\"\n        return user.check_password(request.data.get('password')) or \\\n               self.check_user_token(request, user)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_user_token(self, request, user):\n        if not app_settings.REST_USER_TOKEN_ENABLED:\n            return False\n        try:\n            token = Token.objects.get(\n                user=user,\n                key=request.data.get('password')\n            )\n        except Token.DoesNotExist:\n            token = None\n        else:\n            if app_settings.DISPOSABLE_USER_TOKEN:\n                token.delete()\n        finally:\n            return token is not None", "response": "Checks if a user has a valid token."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new resource in the response body.", "response": "def post(self, request, *args, **kwargs):\n        \"\"\"\n        Sets the response data to None in order to instruct\n        FreeRADIUS to avoid processing the response body\n        \"\"\"\n        response = self.create(request, *args, **kwargs)\n        response.data = None\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, request, *args, **kwargs):\n        if not request.GET.get('cp'):\n            return HttpResponse(_('missing cp GET param'), status=400)\n        self.authorize(request, *args, **kwargs)\n        return HttpResponseRedirect(self.get_redirect_url(request))", "response": "Redirect user to captive page with the social auth token in querystring\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef authorize(self, request, *args, **kwargs):\n        user = request.user\n        if not user.is_authenticated or not user.socialaccount_set.exists():\n            raise PermissionDenied()", "response": "authorize - checks if the user is authorized to access this object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_redirect_url(self, request):\n        cp = request.GET.get('cp')\n        user = request.user\n        Token.objects.filter(user=user).delete()\n        token = Token.objects.create(user=user)\n        return '{0}?username={1}&token={2}'.format(cp, user.username, token.key)", "response": "Returns the URL to redirect to the user s captive page."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating the data dict.", "response": "def validate(self, data):\n        \"\"\"\n        We need to set some timestamps according to the accounting packet type\n        * update_time: set everytime a Interim-Update / Stop packet is received\n        * stop_time: set everytime a Stop packet is received\n        * session_time: calculated if not present in the accounting packet\n        :param data: accounting packet\n        :return: Dict accounting packet\n        \"\"\"\n        time = timezone.now()\n        status_type = data.pop('status_type')\n        if status_type == 'Interim-Update':\n            data['update_time'] = time\n        if status_type == 'Stop':\n            data['update_time'] = time\n            data['stop_time'] = time\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_install_requires():\n    requirements = []\n    for line in open('requirements.txt').readlines():\n        # skip to next iteration if comment or empty line\n        if line.startswith('#') or line == '' or line.startswith('http') or line.startswith('git'):\n            continue\n        # add line to requirements\n        requirements.append(line)\n    return requirements", "response": "parse requirements. txt ignore links exclude comments"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fields(self, request, obj=None):\n        fields = self.fields[:]\n        if not obj:\n            fields.remove('value')\n        return fields", "response": "get the fields that should be displayed when adding a new item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_inline_instances(self, request, obj=None):\n        inlines = super().get_inline_instances(request, obj)\n        if obj:\n            usergroup = RadiusUserGroupInline(self.model,\n                                              self.admin_site)\n            inlines.append(usergroup)\n        return inlines", "response": "Adds RadiusGroupInline only for existing objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a stable ID for a Context given its parent and its character offsets relative to the parent.", "response": "def construct_stable_id(\n    parent_context,\n    polymorphic_type,\n    relative_char_offset_start,\n    relative_char_offset_end,\n):\n    \"\"\"\n    Contruct a stable ID for a Context given its parent and its character\n    offsets relative to the parent.\n    \"\"\"\n    doc_id, _, parent_doc_char_start, _ = split_stable_id(parent_context.stable_id)\n    start = parent_doc_char_start + relative_char_offset_start\n    end = parent_doc_char_start + relative_char_offset_end\n    return f\"{doc_id}::{polymorphic_type}:{start}:{end}\""}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_stable_id(stable_id):\n    split1 = stable_id.split(\"::\")\n    if len(split1) == 2:\n        split2 = split1[1].split(\":\")\n        if len(split2) == 3:\n            return split1[0], split2[0], int(split2[1]), int(split2[2])\n    raise ValueError(f\"Malformed stable_id:\\t{stable_id}\")", "response": "Split a stable id into a sequence of two elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vizlib_unary_features(span):\n    if not span.sentence.is_visual():\n        return\n\n    for f in get_visual_aligned_lemmas(span):\n        yield f\"ALIGNED_{f}\", DEF_VALUE\n\n    for page in set(span.get_attrib_tokens(\"page\")):\n        yield f\"PAGE_[{page}]\", DEF_VALUE", "response": "Yields the unary features for a single span"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vizlib_binary_features(span1, span2):\n    if same_page((span1, span2)):\n        yield \"SAME_PAGE\", DEF_VALUE\n\n        if is_horz_aligned((span1, span2)):\n            yield \"HORZ_ALIGNED\", DEF_VALUE\n\n        if is_vert_aligned((span1, span2)):\n            yield \"VERT_ALIGNED\", DEF_VALUE\n\n        if is_vert_aligned_left((span1, span2)):\n            yield \"VERT_ALIGNED_LEFT\", DEF_VALUE\n\n        if is_vert_aligned_right((span1, span2)):\n            yield \"VERT_ALIGNED_RIGHT\", DEF_VALUE\n\n        if is_vert_aligned_center((span1, span2)):\n            yield \"VERT_ALIGNED_CENTER\", DEF_VALUE", "response": "Generates the visual - related features for a pair of spans."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(self, doc):\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionNgrams.apply() must be of type Document\"\n            )\n\n        for sentence in doc.sentences:\n            for ts in Ngrams.apply(self, sentence):\n                yield ts", "response": "Generate MentionNgrams from a Document by parsing all of its Sentences."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates MentionFigures from a Document by parsing all of its Figures.", "response": "def apply(self, doc):\n        \"\"\"\n        Generate MentionFigures from a Document by parsing all of its Figures.\n\n        :param doc: The ``Document`` to parse.\n        :type doc: ``Document``\n        :raises TypeError: If the input doc is not of type ``Document``.\n        \"\"\"\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionFigures.apply() must be of type Document\"\n            )\n\n        for figure in doc.figures:\n            if self.types is None or any(\n                figure.url.lower().endswith(type) for type in self.types\n            ):\n                yield TemporaryFigureMention(figure)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply(self, doc):\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionSentences.apply() must be of type Document\"\n            )\n\n        for sentence in doc.sentences:\n            yield TemporarySpanMention(\n                char_start=0, char_end=len(sentence.text) - 1, sentence=sentence\n            )", "response": "Generates MentionSentences from a Document by parsing all of its Sentences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply(self, doc):\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionParagraphs.apply() must be of type Document\"\n            )\n\n        for paragraph in doc.paragraphs:\n            yield TemporaryParagraphMention(paragraph)", "response": "Generates MentionParagraphs from a Document by parsing all of its Paragraphs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates MentionCaptions from a Document by parsing all of its Captions.", "response": "def apply(self, doc):\n        \"\"\"\n        Generate MentionCaptions from a Document by parsing all of its Captions.\n\n        :param doc: The ``Document`` to parse.\n        :type doc: ``Document``\n        :raises TypeError: If the input doc is not of type ``Document``.\n        \"\"\"\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionCaptions.apply() must be of type Document\"\n            )\n\n        for caption in doc.captions:\n            yield TemporaryCaptionMention(caption)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply(self, doc):\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionCells.apply() must be of type Document\"\n            )\n\n        for cell in doc.cells:\n            yield TemporaryCellMention(cell)", "response": "Generates MentionCells from a Document by parsing all of its Cells."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply(self, doc):\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionTables.apply() must be of type Document\"\n            )\n\n        for table in doc.tables:\n            yield TemporaryTableMention(table)", "response": "Generates MentionTables from a Document by parsing all of its Tables."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates MentionSections from a Document by parsing all of its Sections.", "response": "def apply(self, doc):\n        \"\"\"\n        Generate MentionSections from a Document by parsing all of its Sections.\n\n        :param doc: The ``Document`` to parse.\n        :type doc: ``Document``\n        :raises TypeError: If the input doc is not of type ``Document``.\n        \"\"\"\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionSections.apply() must be of type Document\"\n            )\n\n        for section in doc.sections:\n            yield TemporarySectionMention(section)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply(self, docs, clear=True, parallelism=None, progress_bar=True):\n        super(MentionExtractor, self).apply(\n            docs, clear=clear, parallelism=parallelism, progress_bar=progress_bar\n        )", "response": "Run the MentionExtractor.\n\n        :Example: To extract mentions from a set of training documents using\n            4 cores::\n\n                mention_extractor.apply(train_docs, parallelism=4)\n\n        :param docs: Set of documents to extract from.\n        :param clear: Whether or not to clear the existing Mentions\n            beforehand.\n        :type clear: bool\n        :param parallelism: How many threads to use for extraction. This will\n            override the parallelism value used to initialize the\n            MentionExtractor if it is provided.\n        :type parallelism: int\n        :param progress_bar: Whether or not to display a progress bar. The\n            progress bar is measured per document.\n        :type progress_bar: bool"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self):\n\n        # Create set of candidate_subclasses associated with each mention_subclass\n        cand_subclasses = set()\n        for mentions, tablename in [\n            (_[1][0], _[1][1]) for _ in candidate_subclasses.values()\n        ]:\n            for mention in mentions:\n                if mention in self.mention_classes:\n                    cand_subclasses.add(tablename)\n\n        # First, clear all the Mentions. This will cascade and remove the\n        # mention_subclasses and corresponding candidate_subclasses.\n        for mention_class in self.mention_classes:\n            logger.info(f\"Clearing table: {mention_class.__tablename__}\")\n            self.session.query(Mention).filter_by(\n                type=mention_class.__tablename__\n            ).delete(synchronize_session=\"fetch\")\n\n        # Next, clear the Candidates. This is done manually because we have\n        # no cascading relationship from candidate_subclass to Candidate.\n        for cand_subclass in cand_subclasses:\n            logger.info(f\"Cascading to clear table: {cand_subclass}\")\n            self.session.query(Candidate).filter_by(type=cand_subclass).delete(\n                synchronize_session=\"fetch\"\n            )", "response": "Delete Mentions of each class in the extractor from the given split."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_all(self):\n        logger.info(\"Clearing ALL Mentions.\")\n        self.session.query(Mention).delete(synchronize_session=\"fetch\")\n\n        # With no Mentions, there should be no Candidates also\n        self.session.query(Candidate).delete(synchronize_session=\"fetch\")\n        logger.info(\"Cleared ALL Mentions (and Candidates).\")", "response": "Delete all Mentions from given split the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of lists of the Mentions associated with this extractor.", "response": "def get_mentions(self, docs=None, sort=False):\n        \"\"\"Return a list of lists of the mentions associated with this extractor.\n\n        Each list of the return will contain the Mentions for one of the\n        mention classes associated with the MentionExtractor.\n\n        :param docs: If provided, return Mentions from these documents. Else,\n            return all Mentions.\n        :param sort: If sort is True, then return all Mentions sorted by stable_id.\n        :type sort: bool\n        :return: Mentions for each mention_class.\n        :rtype: List of lists.\n        \"\"\"\n        result = []\n        if docs:\n            docs = docs if isinstance(docs, (list, tuple)) else [docs]\n            # Get cands from all splits\n            for mention_class in self.mention_classes:\n                mentions = (\n                    self.session.query(mention_class)\n                    .filter(mention_class.document_id.in_([doc.id for doc in docs]))\n                    .order_by(mention_class.id)\n                    .all()\n                )\n                if sort:\n                    mentions = sorted(mentions, key=lambda x: x[0].get_stable_id())\n                result.append(mentions)\n        else:\n            for mention_class in self.mention_classes:\n                mentions = (\n                    self.session.query(mention_class).order_by(mention_class.id).all()\n                )\n                if sort:\n                    mentions = sorted(mentions, key=lambda x: x[0].get_stable_id())\n                result.append(mentions)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply(self, doc, clear, **kwargs):\n\n        # Reattach doc with the current session or DetachedInstanceError happens\n        doc = self.session.merge(doc)\n        # Iterate over each mention class\n        for i, mention_class in enumerate(self.mention_classes):\n            tc_to_insert = defaultdict(list)\n            # Generate TemporaryContexts that are children of the context using\n            # the mention_space and filtered by the Matcher\n            self.child_context_set.clear()\n            for tc in self.matchers[i].apply(self.mention_spaces[i].apply(doc)):\n                rec = tc._load_id_or_insert(self.session)\n                if rec:\n                    tc_to_insert[tc._get_table()].append(rec)\n                self.child_context_set.add(tc)\n\n            # Bulk insert temporary contexts\n            for table, records in tc_to_insert.items():\n                stmt = insert(table.__table__).values(records)\n                self.session.execute(stmt)\n\n            # Generates and persists mentions\n            mention_args = {\"document_id\": doc.id}\n            for child_context in self.child_context_set:\n                # Assemble mention arguments\n                for arg_name in mention_class.__argnames__:\n                    mention_args[arg_name + \"_id\"] = child_context.id\n\n                # Checking for existence\n                if not clear:\n                    q = select([mention_class.id])\n                    for key, value in list(mention_args.items()):\n                        q = q.where(getattr(mention_class, key) == value)\n                    mention_id = self.session.execute(q).first()\n                    if mention_id is not None:\n                        continue\n\n                # Add Mention to session\n                yield mention_class(**mention_args)", "response": "Extract mentions from the given document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the document and return a generator of tokenized text.", "response": "def parse(self, contents):\n        \"\"\"Parse the document.\n\n        :param contents: The text contents of the document.\n        :rtype: a *generator* of tokenized text.\n        \"\"\"\n        i = 0\n        for text in contents.split(self.delim):\n            if not len(text.strip()):\n                continue\n            words = text.split()\n            char_offsets = [0] + [\n                int(_) for _ in np.cumsum([len(x) + 1 for x in words])[:-1]\n            ]\n            text = \" \".join(words)\n            yield {\n                \"text\": text,\n                \"words\": words,\n                \"pos_tags\": [\"\"] * len(words),\n                \"ner_tags\": [\"\"] * len(words),\n                \"lemmas\": [\"\"] * len(words),\n                \"dep_parents\": [0] * len(words),\n                \"dep_labels\": [\"\"] * len(words),\n                \"char_offsets\": char_offsets,\n                \"abs_char_offsets\": char_offsets,\n            }\n            i += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms a CoreNLP object with dep_path and dep_parent attributes into an XMLTree.", "response": "def corenlp_to_xmltree(s, prune_root=True):\n    \"\"\"\n    Transforms an object with CoreNLP dep_path and dep_parent attributes into\n    an XMLTree. Will include elements of any array having the same dimensiion\n    as dep_* as node attributes. Also adds special word_idx attribute\n    corresponding to original sequence order in sentence.\n    \"\"\"\n    # Convert input object to dictionary\n    s = get_as_dict(s)\n\n    # Use the dep_parents array as a guide: ensure it is present and a list of\n    # ints\n    if not (\"dep_parents\" in s and isinstance(s[\"dep_parents\"], list)):\n        raise ValueError(\n            \"Input CoreNLP object must have a 'dep_parents' attribute which is a list\"\n        )\n    try:\n        dep_parents = list(map(int, s[\"dep_parents\"]))\n    except Exception:\n        raise ValueError(\"'dep_parents' attribute must be a list of ints\")\n\n    # Also ensure that we are using CoreNLP-native indexing\n    # (root=0, 1-base word indexes)!\n    b = min(dep_parents)\n    if b != 0:\n        dep_parents = list(map(lambda j: j - b, dep_parents))\n\n    # Parse recursively\n    root = corenlp_to_xmltree_sub(s, dep_parents, 0)\n\n    # Often the return tree will have several roots, where one is the actual\n    # root and the rest are just singletons not included in the dep tree\n    # parse...\n    # We optionally remove these singletons and then collapse the root if only\n    # one child left.\n    if prune_root:\n        for c in root:\n            if len(c) == 0:\n                root.remove(c)\n        if len(root) == 1:\n            root = root.findall(\"./*\")[0]\n    return XMLTree(root, words=s[\"words\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields the unary features for a single span.", "response": "def strlib_unary_features(span):\n    \"\"\"\n    Structural-related features for a single span\n    \"\"\"\n    if not span.sentence.is_structural():\n        return\n\n    yield f\"TAG_{get_tag(span)}\", DEF_VALUE\n\n    for attr in get_attributes(span):\n        yield f\"HTML_ATTR_{attr}\", DEF_VALUE\n\n    yield f\"PARENT_TAG_{get_parent_tag(span)}\", DEF_VALUE\n\n    prev_tags = get_prev_sibling_tags(span)\n    if len(prev_tags):\n        yield f\"PREV_SIB_TAG_{prev_tags[-1]}\", DEF_VALUE\n        yield f\"NODE_POS_{len(prev_tags) + 1}\", DEF_VALUE\n    else:\n        yield \"FIRST_NODE\", DEF_VALUE\n\n    next_tags = get_next_sibling_tags(span)\n    if len(next_tags):\n        yield f\"NEXT_SIB_TAG_{next_tags[0]}\", DEF_VALUE\n    else:\n        yield \"LAST_NODE\", DEF_VALUE\n\n    yield f\"ANCESTOR_CLASS_[{' '.join(get_ancestor_class_names(span))}]\", DEF_VALUE\n\n    yield f\"ANCESTOR_TAG_[{' '.join(get_ancestor_tag_names(span))}]\", DEF_VALUE\n\n    yield f\"ANCESTOR_ID_[{' '.join(get_ancestor_id_names(span))}]\", DEF_VALUE"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the ngrams between two binary - Mentions of a binary - Mention Candidate.", "response": "def get_between_ngrams(c, attrib=\"words\", n_min=1, n_max=1, lower=True):\n    \"\"\"Return the ngrams *between* two unary Mentions of a binary-Mention Candidate.\n\n    Get the ngrams *between* two unary Mentions of a binary-Mention Candidate,\n    where both share the same sentence Context.\n\n    :param c: The binary-Mention Candidate to evaluate.\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param lower: If 'True', all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    if len(c) != 2:\n        raise ValueError(\"Only applicable to binary Candidates\")\n    span0 = _to_span(c[0])\n    span1 = _to_span(c[1])\n    if span0.sentence != span1.sentence:\n        raise ValueError(\n            \"Only applicable to Candidates where both spans are \\\n                          from the same immediate Context.\"\n        )\n    distance = abs(span0.get_word_start_index() - span1.get_word_start_index())\n    if span0.get_word_start_index() < span1.get_word_start_index():\n        for ngram in get_right_ngrams(\n            span0,\n            window=distance - 1,\n            attrib=attrib,\n            n_min=n_min,\n            n_max=n_max,\n            lower=lower,\n        ):\n            yield ngram\n    else:  # span0.get_word_start_index() > span1.get_word_start_index()\n        for ngram in get_right_ngrams(\n            span1,\n            window=distance - 1,\n            attrib=attrib,\n            n_min=n_min,\n            n_max=n_max,\n            lower=lower,\n        ):\n            yield ngram"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_left_ngrams(mention, window=3, attrib=\"words\", n_min=1, n_max=1, lower=True):\n    span = _to_span(mention)\n    i = span.get_word_start_index()\n    for ngram in tokens_to_ngrams(\n        getattr(span.sentence, attrib)[max(0, i - window) : i],\n        n_min=n_min,\n        n_max=n_max,\n        lower=lower,\n    ):\n        yield ngram", "response": "Returns the ngrams from the sentence Context that are the left of the given Mention."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the ngrams within a window to the right of the Mention.", "response": "def get_right_ngrams(mention, window=3, attrib=\"words\", n_min=1, n_max=1, lower=True):\n    \"\"\"Get the ngrams within a window to the *right* from the sentence Context.\n\n    For higher-arity Candidates, defaults to the *last* argument.\n\n    :param mention: The Mention to evaluate. If a candidate is given, default\n        to its last Mention.\n    :param window: The number of tokens to the left of the first argument to\n        return\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param lower: If True, all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    span = _to_span(mention, idx=-1)\n    i = span.get_word_end_index()\n    for ngram in tokens_to_ngrams(\n        getattr(span.sentence, attrib)[i + 1 : i + 1 + window],\n        n_min=n_min,\n        n_max=n_max,\n        lower=lower,\n    ):\n        yield ngram"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_node(type, name, content):\n    if type == \"doc\":\n        return f\"<html>{content}</html>\"\n    if type == \"section\":\n        return f\"<section name='{name}'>{content}</section>\"\n    if type == \"text\":\n        return f\"<p name='{name}'>{content}</p>\"\n    if type == \"figure\":\n        return f\"<img name='{name}' src='{content}'/>\"", "response": "Wrap up content in to a html node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert raw content to list of strutured tuples where each tuple contains the content of the n - tuple type and the content of the n - tuple type contains the n - tuple type.", "response": "def column_constructor(text, name=None, type=\"text\", delim=None):\n    \"\"\"\n    Converts raw content to a list of strutured tuple where each tuple contains\n        (type, name, content).\n\n    :param text: content to be converted ()\n    :type path: str\n    :param type: content name (default: None)\n    :type path: str\n    :param type: content type (default: text)\n    :type path: str\n    :param delim: delimiter to split the content\n    :type path: str\n    :return: A list of tuple where each tuple contains\n        (content type, content name, content)\n    \"\"\"\n    if delim is None:\n        return [(type, name, text)]\n    return [(type, name, content) for content in text.split(delim)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_span(x, idx=0):\n    if isinstance(x, Candidate):\n        return x[idx].context\n    elif isinstance(x, Mention):\n        return x.context\n    elif isinstance(x, TemporarySpanMention):\n        return x\n    else:\n        raise ValueError(f\"{type(x)} is an invalid argument type\")", "response": "Convert a Candidate Mention or Span to a span."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a Candidate Mention or Span to a list of spans.", "response": "def _to_spans(x):\n    \"\"\"Convert a Candidate, Mention, or Span to a list of spans.\"\"\"\n    if isinstance(x, Candidate):\n        return [_to_span(m) for m in x]\n    elif isinstance(x, Mention):\n        return [x.context]\n    elif isinstance(x, TemporarySpanMention):\n        return [x]\n    else:\n        raise ValueError(f\"{type(x)} is an invalid argument type\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of candidates that are matched by a particular LF.", "response": "def get_matches(lf, candidate_set, match_values=[1, -1]):\n    \"\"\"Return a list of candidates that are matched by a particular LF.\n\n    A simple helper function to see how many matches (non-zero by default) an\n    LF gets.\n\n    :param lf: The labeling function to apply to the candidate_set\n    :param candidate_set: The set of candidates to evaluate\n    :param match_values: An option list of the values to consider as matched.\n        [1, -1] by default.\n    :rtype: a list of candidates\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    matches = []\n    for c in candidate_set:\n        label = lf(c)\n        if label in match_values:\n            matches.append(c)\n    logger.info(f\"{len(matches)} matches\")\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _preprocess_data(self, X, Y=None, idxs=None, train=False):\n\n        C, F = X\n\n        if Y is not None:\n            Y = np.array(Y).astype(np.float32)\n\n        if idxs is None:\n            if Y is not None:\n                return (\n                    [\n                        [\n                            F.indices[F.indptr[i] : F.indptr[i + 1]],\n                            F.data[F.indptr[i] : F.indptr[i + 1]],\n                        ]\n                        for i in range(len(C))\n                    ],\n                    Y,\n                )\n            else:\n                return [\n                    [\n                        F.indices[F.indptr[i] : F.indptr[i + 1]],\n                        F.data[F.indptr[i] : F.indptr[i + 1]],\n                    ]\n                    for i in range(len(C))\n                ]\n        if Y is not None:\n            return (\n                [\n                    [\n                        F.indices[F.indptr[i] : F.indptr[i + 1]],\n                        F.data[F.indptr[i] : F.indptr[i + 1]],\n                    ]\n                    for i in idxs\n                ],\n                Y[idxs],\n            )\n        else:\n            return [\n                [\n                    F.indices[F.indptr[i] : F.indptr[i + 1]],\n                    F.data[F.indptr[i] : F.indptr[i + 1]],\n                ]\n                for i in idxs\n            ]", "response": "Preprocess the data for the pair with candidates and corresponding features."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _collate(self, batch):\n\n        Y_batch = None\n        if isinstance(batch[0], tuple):\n            batch, Y_batch = list(zip(*batch))\n            Y_batch = self._cuda(torch.Tensor(Y_batch))\n\n        f_batch, v_batch = list(zip(*batch))\n\n        f_batch, _ = pad_batch(f_batch, 0)\n        v_batch, _ = pad_batch(v_batch, 0, type=\"float\")\n\n        f_batch = self._cuda(f_batch)\n        v_batch = self._cuda(v_batch)\n\n        if Y_batch is not None:\n            return [f_batch, v_batch], Y_batch\n        else:\n            return [f_batch, v_batch]", "response": "Takes a list of features feature weights and returns a list of torch. Tensor with the input data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the model argument.", "response": "def _update_settings(self, X):\n        \"\"\"\n        Update the model argument.\n\n        :param X: The input data of the model.\n        :type X: list of (candidate, features) pair\n        \"\"\"\n\n        self.logger.info(\"Loading default parameters for Sparse Logistic Regression\")\n        config = get_config()[\"learning\"][\"SparseLogisticRegression\"]\n\n        for key in config.keys():\n            if key not in self.settings:\n                self.settings[key] = config[key]\n\n        # Add one feature for padding vector (all 0s)\n        self.settings[\"input_dim\"] = X[1].shape[1] + 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, docs=None, split=0, parallelism=None, progress_bar=True):\n        self.apply(\n            docs=docs,\n            split=split,\n            train=True,\n            clear=False,\n            parallelism=parallelism,\n            progress_bar=progress_bar,\n        )", "response": "Update the features of the specified candidates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply(\n        self,\n        docs=None,\n        split=0,\n        train=False,\n        clear=True,\n        parallelism=None,\n        progress_bar=True,\n    ):\n        \"\"\"Apply features to the specified candidates.\n\n        :param docs: If provided, apply features to all the candidates in these\n            documents.\n        :param split: If docs is None, apply features to the candidates in this\n            particular split.\n        :type split: int\n        :param train: Whether or not to update the global key set of features\n            and the features of candidates.\n        :type train: bool\n        :param clear: Whether or not to clear the features table before\n            applying features.\n        :type clear: bool\n        :param parallelism: How many threads to use for extraction. This will\n            override the parallelism value used to initialize the Featurizer if\n            it is provided.\n        :type parallelism: int\n        :param progress_bar: Whether or not to display a progress bar. The\n            progress bar is measured per document.\n        :type progress_bar: bool\n        \"\"\"\n        if docs:\n            # Call apply on the specified docs for all splits\n            split = ALL_SPLITS\n            super(Featurizer, self).apply(\n                docs,\n                split=split,\n                train=train,\n                clear=clear,\n                parallelism=parallelism,\n                progress_bar=progress_bar,\n            )\n            # Needed to sync the bulk operations\n            self.session.commit()\n        else:\n            # Only grab the docs containing candidates from the given split.\n            split_docs = get_docs_from_split(\n                self.session, self.candidate_classes, split\n            )\n            super(Featurizer, self).apply(\n                split_docs,\n                split=split,\n                train=train,\n                clear=clear,\n                parallelism=parallelism,\n                progress_bar=progress_bar,\n            )\n            # Needed to sync the bulk operations\n            self.session.commit()", "response": "Applies features to the specified candidates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef drop_keys(self, keys, candidate_classes=None):\n        # Make sure keys is iterable\n        keys = keys if isinstance(keys, (list, tuple)) else [keys]\n\n        # Make sure candidate_classes is iterable\n        if candidate_classes:\n            candidate_classes = (\n                candidate_classes\n                if isinstance(candidate_classes, (list, tuple))\n                else [candidate_classes]\n            )\n\n            # Ensure only candidate classes associated with the featurizer\n            # are used.\n            candidate_classes = [\n                _.__tablename__\n                for _ in candidate_classes\n                if _ in self.candidate_classes\n            ]\n\n            if len(candidate_classes) == 0:\n                logger.warning(\n                    \"You didn't specify valid candidate classes for this featurizer.\"\n                )\n                return\n        # If unspecified, just use all candidate classes\n        else:\n            candidate_classes = [_.__tablename__ for _ in self.candidate_classes]\n\n        # build dict for use by utils\n        key_map = dict()\n        for key in keys:\n            key_map[key] = set(candidate_classes)\n\n        drop_keys(self.session, FeatureKey, key_map)", "response": "Drop the specified keys from FeatureKeys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear(self, train=False, split=0):\n        # Clear Features for the candidates in the split passed in.\n        logger.info(f\"Clearing Features (split {split})\")\n\n        sub_query = (\n            self.session.query(Candidate.id).filter(Candidate.split == split).subquery()\n        )\n        query = self.session.query(Feature).filter(Feature.candidate_id.in_(sub_query))\n        query.delete(synchronize_session=\"fetch\")\n\n        # Delete all old annotation keys\n        if train:\n            logger.debug(f\"Clearing all FeatureKeys from {self.candidate_classes}...\")\n            drop_all_keys(self.session, FeatureKey, self.candidate_classes)", "response": "Delete all features and annotation keys from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_all(self):\n        logger.info(\"Clearing ALL Features and FeatureKeys.\")\n        self.session.query(Feature).delete(synchronize_session=\"fetch\")\n        self.session.query(FeatureKey).delete(synchronize_session=\"fetch\")", "response": "Delete all Features and FeatureKeys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply(self, doc, split, train, **kwargs):\n        logger.debug(f\"Document: {doc}\")\n\n        # Get all the candidates in this doc that will be featurized\n        cands_list = get_cands_list_from_split(\n            self.session, self.candidate_classes, doc, split\n        )\n\n        feature_map = dict()\n\n        # Make a flat list of all candidates from the list of list of\n        # candidates. This helps reduce the number of queries needed to update.\n        all_cands = itertools.chain.from_iterable(cands_list)\n        records = list(\n            get_mapping(self.session, Feature, all_cands, get_all_feats, feature_map)\n        )\n        batch_upsert_records(self.session, Feature, records)\n\n        # Insert all Feature Keys\n        if train:\n            upsert_keys(self.session, FeatureKey, feature_map)\n\n        # This return + yield makes a completely empty generator\n        return\n        yield", "response": "Extract candidates from the given document and split into a single set of FeatureKeys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _merge(x, y):\n    merged = {**x, **y}\n\n    xkeys = x.keys()\n\n    for key in xkeys:\n        if isinstance(x[key], dict) and key in y:\n            merged[key] = _merge(x[key], y[key])\n\n    return merged", "response": "Merge two nested dictionaries. Overwrite values in x with values in y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for settings file in root of project and its parents.", "response": "def get_config(path=os.getcwd()):\n    \"\"\"Search for settings file in root of project and its parents.\"\"\"\n    config = default\n    tries = 0\n    current_dir = path\n    while current_dir and tries < MAX_CONFIG_SEARCH_DEPTH:\n        potential_path = os.path.join(current_dir, \".fonduer-config.yaml\")\n        if os.path.exists(potential_path):\n            with open(potential_path, \"r\") as f:\n                config = _merge(config, yaml.safe_load(f))\n            logger.debug(f\"Loading Fonduer config from {potential_path}.\")\n            break\n\n        new_dir = os.path.split(current_dir)[0]\n        if current_dir == new_dir:\n            logger.debug(\"Unable to find config file. Using defaults.\")\n            break\n        current_dir = new_dir\n        tries += 1\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_id_or_insert(self, session):\n        if self.id is None:\n            stable_id = self.get_stable_id()\n            # Check if exists\n            id = session.execute(\n                select([Context.id]).where(Context.stable_id == stable_id)\n            ).first()\n\n            # If not, insert\n            if id is None:\n                self.id = session.execute(\n                    Context.__table__.insert(),\n                    {\"type\": self._get_table().__tablename__, \"stable_id\": stable_id},\n                ).inserted_primary_key[0]\n                insert_args = self._get_insert_args()\n                insert_args[\"id\"] = self.id\n                return insert_args\n            else:\n                self.id = id[0]", "response": "Load the id of the temporary context if it exists or insert the args."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _collate(self, batch):\n\n        if isinstance(batch[0], tuple):\n            return [self._cuda(torch.Tensor(samples)) for samples in list(zip(*batch))]\n        else:\n            return self._cuda(torch.Tensor(batch))", "response": "Put each data field into a tensor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tablelib_unary_features(span):\n    if not span.sentence.is_tabular():\n        return\n    sentence = span.sentence\n    for attrib in settings[\"featurization\"][\"table\"][\"unary_features\"][\"attrib\"]:\n        for ngram in get_cell_ngrams(\n            span,\n            n_max=settings[\"featurization\"][\"table\"][\"unary_features\"][\n                \"get_cell_ngrams\"\n            ][\"max\"],\n            attrib=attrib,\n        ):\n            yield f\"CELL_{attrib.upper()}_[{ngram}]\", DEF_VALUE\n        for row_num in range(sentence.row_start, sentence.row_end + 1):\n            yield f\"ROW_NUM_[{row_num}]\", DEF_VALUE\n        for col_num in range(sentence.col_start, sentence.col_end + 1):\n            yield f\"COL_NUM_[{col_num}]\", DEF_VALUE\n        # NOTE: These two features could be accounted for by HTML_ATTR in\n        # structural features\n        yield f\"ROW_SPAN_[{num_rows(sentence)}]\", DEF_VALUE\n        yield f\"COL_SPAN_[{num_cols(sentence)}]\", DEF_VALUE\n        for axis in [\"row\", \"col\"]:\n            for ngram in get_head_ngrams(\n                span,\n                axis,\n                n_max=settings[\"featurization\"][\"table\"][\"unary_features\"][\n                    \"get_head_ngrams\"\n                ][\"max\"],\n                attrib=attrib,\n            ):\n                yield f\"{axis.upper()}_HEAD_{attrib.upper()}_[{ngram}]\", DEF_VALUE\n        for ngram in get_row_ngrams(\n            span,\n            n_max=settings[\"featurization\"][\"table\"][\"unary_features\"][\n                \"get_row_ngrams\"\n            ][\"max\"],\n            attrib=attrib,\n        ):\n            yield f\"ROW_{attrib.upper()}_[{ngram}]\", DEF_VALUE\n        for ngram in get_col_ngrams(\n            span,\n            n_max=settings[\"featurization\"][\"table\"][\"unary_features\"][\n                \"get_col_ngrams\"\n            ][\"max\"],\n            attrib=attrib,\n        ):\n            yield f\"COL_{attrib.upper()}_[{ngram}]\", DEF_VALUE", "response": "Return an iterator over the distinct table - related features for a single span."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield the set of binary features that are used for a pair of spans.", "response": "def tablelib_binary_features(span1, span2):\n    \"\"\"\n    Table-/structure-related features for a pair of spans\n    \"\"\"\n    binary_features = settings[\"featurization\"][\"table\"][\"binary_features\"]\n    if span1.sentence.is_tabular() and span2.sentence.is_tabular():\n        if span1.sentence.table == span2.sentence.table:\n            yield \"SAME_TABLE\", DEF_VALUE\n            if span1.sentence.cell is not None and span2.sentence.cell is not None:\n                row_diff = min_row_diff(\n                    span1.sentence,\n                    span2.sentence,\n                    absolute=binary_features[\"min_row_diff\"][\"absolute\"],\n                )\n                col_diff = min_col_diff(\n                    span1.sentence,\n                    span2.sentence,\n                    absolute=binary_features[\"min_col_diff\"][\"absolute\"],\n                )\n                yield f\"SAME_TABLE_ROW_DIFF_[{row_diff}]\", DEF_VALUE\n                yield f\"SAME_TABLE_COL_DIFF_[{col_diff}]\", DEF_VALUE\n                yield (\n                    f\"SAME_TABLE_MANHATTAN_DIST_[{abs(row_diff) + abs(col_diff)}]\"\n                ), DEF_VALUE\n                if span1.sentence.cell == span2.sentence.cell:\n                    yield \"SAME_CELL\", DEF_VALUE\n                    yield (\n                        f\"WORD_DIFF_[\"\n                        f\"{span1.get_word_start_index() - span2.get_word_start_index()}\"\n                        f\"]\"\n                    ), DEF_VALUE\n                    yield (\n                        f\"CHAR_DIFF_[{span1.char_start - span2.char_start}]\"\n                    ), DEF_VALUE\n                    if span1.sentence == span2.sentence:\n                        yield \"SAME_SENTENCE\", DEF_VALUE\n        else:\n            if span1.sentence.cell is not None and span2.sentence.cell is not None:\n                yield \"DIFF_TABLE\", DEF_VALUE\n                row_diff = min_row_diff(\n                    span1.sentence,\n                    span2.sentence,\n                    absolute=binary_features[\"min_row_diff\"][\"absolute\"],\n                )\n                col_diff = min_col_diff(\n                    span1.sentence,\n                    span2.sentence,\n                    absolute=binary_features[\"min_col_diff\"][\"absolute\"],\n                )\n                yield f\"DIFF_TABLE_ROW_DIFF_[{row_diff}]\", DEF_VALUE\n                yield f\"DIFF_TABLE_COL_DIFF_[{col_diff}]\", DEF_VALUE\n                yield (\n                    f\"DIFF_TABLE_MANHATTAN_DIST_[{abs(row_diff) + abs(col_diff)}]\"\n                ), DEF_VALUE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply the parser to get the set of available LFs.", "response": "def apply(\n        self, doc_loader, pdf_path=None, clear=True, parallelism=None, progress_bar=True\n    ):\n        \"\"\"Run the Parser.\n\n        :param doc_loader: An iteratable of ``Documents`` to parse. Typically,\n            one of Fonduer's document preprocessors.\n        :param pdf_path: The path to the PDF documents, if any. This path will\n            override the one used in initialization, if provided.\n        :param clear: Whether or not to clear the labels table before applying\n            these LFs.\n        :type clear: bool\n        :param parallelism: How many threads to use for extraction. This will\n            override the parallelism value used to initialize the Labeler if\n            it is provided.\n        :type parallelism: int\n        :param progress_bar: Whether or not to display a progress bar. The\n            progress bar is measured per document.\n        :type progress_bar: bool\n        \"\"\"\n        super(Parser, self).apply(\n            doc_loader,\n            pdf_path=pdf_path,\n            clear=clear,\n            parallelism=parallelism,\n            progress_bar=progress_bar,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_last_documents(self):\n        return (\n            self.session.query(Document)\n            .filter(Document.name.in_(self.last_docs))\n            .order_by(Document.name)\n            .all()\n        )", "response": "Return the most recently parsed list of Documents ordered by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all the parsed Documents in the database ordered by name.", "response": "def get_documents(self):\n        \"\"\"Return all the parsed ``Documents`` in the database.\n\n        :rtype: A list of all ``Documents`` in the database ordered by name.\n        \"\"\"\n        return self.session.query(Document).order_by(Document.name).all()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _valid_pdf(self, path, filename):\n        # If path is file, but not PDF.\n        if os.path.isfile(path) and path.lower().endswith(\".pdf\"):\n            return True\n        else:\n            full_path = os.path.join(path, filename)\n            if os.path.isfile(full_path) and full_path.lower().endswith(\".pdf\"):\n                return True\n            elif os.path.isfile(os.path.join(path, filename + \".pdf\")):\n                return True\n            elif os.path.isfile(os.path.join(path, filename + \".PDF\")):\n                return True\n\n        return False", "response": "Verify that the file exists and has a PDF extension."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the figure node and return the state necessary to place it in the global state necessary to place the node in context .", "response": "def _parse_figure(self, node, state):\n        \"\"\"Parse the figure node.\n\n        :param node: The lxml img node to parse\n        :param state: The global state necessary to place the node in context\n            of the document as a whole.\n        \"\"\"\n        if node.tag not in [\"img\", \"figure\"]:\n            return state\n\n        # Process the Figure\n        stable_id = (\n            f\"{state['document'].name}\"\n            f\"::\"\n            f\"{'figure'}\"\n            f\":\"\n            f\"{state['figure']['idx']}\"\n        )\n\n        # Set name for Figure\n        name = node.attrib[\"name\"] if \"name\" in node.attrib else None\n\n        # img within a Figure get's processed in the parent Figure\n        if node.tag == \"img\" and isinstance(state[\"parent\"][node], Figure):\n            return state\n\n        # NOTE: We currently do NOT support nested figures.\n        parts = {}\n        parent = state[\"parent\"][node]\n        if isinstance(parent, Section):\n            parts[\"section\"] = parent\n        elif isinstance(parent, Cell):\n            parts[\"section\"] = parent.table.section\n            parts[\"cell\"] = parent\n        else:\n            logger.warning(f\"Figure is nested within {state['parent'][node]}\")\n            return state\n\n        parts[\"document\"] = state[\"document\"]\n        parts[\"stable_id\"] = stable_id\n        parts[\"name\"] = name\n        parts[\"position\"] = state[\"figure\"][\"idx\"]\n\n        # If processing a raw img\n        if node.tag == \"img\":\n            # Create the Figure entry in the DB\n            parts[\"url\"] = node.get(\"src\")\n            state[\"context\"][node] = Figure(**parts)\n        elif node.tag == \"figure\":\n            # Pull the image from a child img node, if one exists\n            imgs = [child for child in node if child.tag == \"img\"]\n\n            if len(imgs) > 1:\n                logger.warning(\"Figure contains multiple images.\")\n                # Right now we don't support multiple URLs in the Figure context\n                # As a workaround, just ignore the outer Figure and allow processing\n                # of the individual images. We ignore the accompanying figcaption\n                # by marking it as visited.\n                captions = [child for child in node if child.tag == \"figcaption\"]\n                state[\"visited\"].update(captions)\n                return state\n\n            img = imgs[0]\n            state[\"visited\"].add(img)\n\n            # Create the Figure entry in the DB\n            parts[\"url\"] = img.get(\"src\")\n            state[\"context\"][node] = Figure(**parts)\n\n        state[\"figure\"][\"idx\"] += 1\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the Sentences of the node.", "response": "def _parse_sentence(self, paragraph, node, state):\n        \"\"\"Parse the Sentences of the node.\n\n        :param node: The lxml node to parse\n        :param state: The global state necessary to place the node in context\n            of the document as a whole.\n        \"\"\"\n        text = state[\"paragraph\"][\"text\"]\n        field = state[\"paragraph\"][\"field\"]\n\n        # Set name for Sentence\n        name = node.attrib[\"name\"] if \"name\" in node.attrib else None\n\n        # Lingual Parse\n        document = state[\"document\"]\n        for parts in self.tokenize_and_split_sentences(text):\n            parts[\"document\"] = document\n            # NOTE: Why do we overwrite this from the spacy parse?\n            parts[\"position\"] = state[\"sentence\"][\"idx\"]\n            abs_sentence_offset_end = (\n                state[\"sentence\"][\"abs_offset\"]\n                + parts[\"char_offsets\"][-1]\n                + len(parts[\"words\"][-1])\n            )\n            parts[\"stable_id\"] = construct_stable_id(\n                document,\n                \"sentence\",\n                state[\"sentence\"][\"abs_offset\"],\n                abs_sentence_offset_end,\n            )\n            parts[\"name\"] = name\n            state[\"sentence\"][\"abs_offset\"] = abs_sentence_offset_end\n            if self.structural:\n                context_node = node.getparent() if field == \"tail\" else node\n                tree = lxml.etree.ElementTree(state[\"root\"])\n                parts[\"xpath\"] = tree.getpath(context_node)\n                parts[\"html_tag\"] = context_node.tag\n                parts[\"html_attrs\"] = [\n                    \"=\".join(x) for x in list(context_node.attrib.items())\n                ]\n\n                # Extending html style attribute with the styles\n                # from inline style class for the element.\n                cur_style_index = None\n                for index, attr in enumerate(parts[\"html_attrs\"]):\n                    if attr.find(\"style\") >= 0:\n                        cur_style_index = index\n                        break\n                head = state[\"root\"].find(\"head\")\n                styles = None\n                if head is not None:\n                    styles = head.find(\"style\")\n                if styles is not None:\n                    for x in list(context_node.attrib.items()):\n                        if x[0] == \"class\":\n                            exp = r\"(.\" + x[1] + r\")([\\n\\s\\r]*)\\{(.*?)\\}\"\n                            r = re.compile(exp, re.DOTALL)\n                            if r.search(styles.text) is not None:\n                                if cur_style_index is not None:\n                                    parts[\"html_attrs\"][cur_style_index] += (\n                                        r.search(styles.text)\n                                        .group(3)\n                                        .replace(\"\\r\", \"\")\n                                        .replace(\"\\n\", \"\")\n                                        .replace(\"\\t\", \"\")\n                                    )\n                                else:\n                                    parts[\"html_attrs\"].extend(\n                                        [\n                                            \"style=\"\n                                            + re.sub(\n                                                r\"\\s{1,}\",\n                                                \" \",\n                                                r.search(styles.text)\n                                                .group(3)\n                                                .replace(\"\\r\", \"\")\n                                                .replace(\"\\n\", \"\")\n                                                .replace(\"\\t\", \"\")\n                                                .strip(),\n                                            )\n                                        ]\n                                    )\n                            break\n            if self.tabular:\n                parts[\"position\"] = state[\"sentence\"][\"idx\"]\n\n                # If tabular, consider own Context first in case a Cell\n                # was just created. Otherwise, defer to the parent.\n                parent = paragraph\n                if isinstance(parent, Paragraph):\n                    parts[\"section\"] = parent.section\n                    parts[\"paragraph\"] = parent\n                    if parent.cell:\n                        parts[\"table\"] = parent.cell.table\n                        parts[\"cell\"] = parent.cell\n                        parts[\"row_start\"] = parent.cell.row_start\n                        parts[\"row_end\"] = parent.cell.row_end\n                        parts[\"col_start\"] = parent.cell.col_start\n                        parts[\"col_end\"] = parent.cell.col_end\n                else:\n                    raise NotImplementedError(\"Sentence parent must be Paragraph.\")\n            yield Sentence(**parts)\n            state[\"sentence\"][\"idx\"] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single paragraph of the node.", "response": "def _parse_paragraph(self, node, state):\n        \"\"\"Parse a Paragraph of the node.\n\n        :param node: The lxml node to parse\n        :param state: The global state necessary to place the node in context\n            of the document as a whole.\n        \"\"\"\n\n        # Both Paragraphs will share the same parent\n        parent = (\n            state[\"context\"][node]\n            if node in state[\"context\"]\n            else state[\"parent\"][node]\n        )\n        # Set name for Paragraph\n        name = node.attrib[\"name\"] if \"name\" in node.attrib else None\n\n        for field in [\"text\", \"tail\"]:\n            text = getattr(node, field)\n            text = text.strip() if text and self.strip else text\n\n            # Skip if \"\" or None\n            if not text:\n                continue\n\n            # Run RegEx replacements\n            for (rgx, replace) in self.replacements:\n                text = rgx.sub(replace, text)\n\n            # Process the Paragraph\n            stable_id = (\n                f\"{state['document'].name}\"\n                f\"::\"\n                f\"{'paragraph'}\"\n                f\":\"\n                f\"{state['paragraph']['idx']}\"\n            )\n            parts = {}\n            parts[\"stable_id\"] = stable_id\n            parts[\"name\"] = name\n            parts[\"document\"] = state[\"document\"]\n            parts[\"position\"] = state[\"paragraph\"][\"idx\"]\n            if isinstance(parent, Caption):\n                if parent.table:\n                    parts[\"section\"] = parent.table.section\n                elif parent.figure:\n                    parts[\"section\"] = parent.figure.section\n                parts[\"caption\"] = parent\n            elif isinstance(parent, Cell):\n                parts[\"section\"] = parent.table.section\n                parts[\"cell\"] = parent\n            elif isinstance(parent, Section):\n                parts[\"section\"] = parent\n            elif isinstance(parent, Figure):  # occurs with text in the tail of an img\n                parts[\"section\"] = parent.section\n            elif isinstance(parent, Table):  # occurs with text in the tail of a table\n                parts[\"section\"] = parent.section\n            else:\n                raise NotImplementedError(\n                    f\"Para '{text}' parent must be Section, Caption, or Cell, \"\n                    f\"not {parent}\"\n                )\n\n            # Create the entry in the DB\n            paragraph = Paragraph(**parts)\n\n            state[\"paragraph\"][\"idx\"] += 1\n\n            state[\"paragraph\"][\"text\"] = text\n            state[\"paragraph\"][\"field\"] = field\n\n            yield from self._parse_sentence(paragraph, node, state)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a section of the node.", "response": "def _parse_section(self, node, state):\n        \"\"\"Parse a Section of the node.\n\n        Note that this implementation currently creates a Section at the\n        beginning of the document and creates Section based on tag of node.\n\n        :param node: The lxml node to parse\n        :param state: The global state necessary to place the node in context\n            of the document as a whole.\n        \"\"\"\n        if node.tag not in [\"html\", \"section\"]:\n            return state\n\n        # Add a Section\n        stable_id = (\n            f\"{state['document'].name}\"\n            f\"::\"\n            f\"{'section'}\"\n            f\":\"\n            f\"{state['section']['idx']}\"\n        )\n\n        # Set name for Section\n        name = node.attrib[\"name\"] if \"name\" in node.attrib else None\n\n        state[\"context\"][node] = Section(\n            document=state[\"document\"],\n            name=name,\n            stable_id=stable_id,\n            position=state[\"section\"][\"idx\"],\n        )\n        state[\"section\"][\"idx\"] += 1\n\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a Caption of the node.", "response": "def _parse_caption(self, node, state):\n        \"\"\"Parse a Caption of the node.\n\n        :param node: The lxml node to parse\n        :param state: The global state necessary to place the node in context\n            of the document as a whole.\n        \"\"\"\n        if node.tag not in [\"caption\", \"figcaption\"]:  # captions used in Tables\n            return state\n\n        # Add a Caption\n        parent = state[\"parent\"][node]\n        stable_id = (\n            f\"{state['document'].name}\"\n            f\"::\"\n            f\"{'caption'}\"\n            f\":\"\n            f\"{state['caption']['idx']}\"\n        )\n\n        # Set name for Section\n        name = node.attrib[\"name\"] if \"name\" in node.attrib else None\n\n        if isinstance(parent, Table):\n            state[\"context\"][node] = Caption(\n                document=state[\"document\"],\n                table=parent,\n                figure=None,\n                stable_id=stable_id,\n                name=name,\n                position=state[\"caption\"][\"idx\"],\n            )\n        elif isinstance(parent, Figure):\n            state[\"context\"][node] = Caption(\n                document=state[\"document\"],\n                table=None,\n                figure=parent,\n                stable_id=stable_id,\n                name=name,\n                position=state[\"caption\"][\"idx\"],\n            )\n        else:\n            raise NotImplementedError(\"Caption must be a child of Table or Figure.\")\n        state[\"caption\"][\"idx\"] += 1\n\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the node and yield Sentences", "response": "def _parse_node(self, node, state):\n        \"\"\"Entry point for parsing all node types.\n\n        :param node: The lxml HTML node to parse\n        :param state: The global state necessary to place the node in context\n            of the document as a whole.\n        :rtype: a *generator* of Sentences\n        \"\"\"\n        # Processing on entry of node\n        state = self._parse_section(node, state)\n\n        state = self._parse_figure(node, state)\n\n        if self.tabular:\n            state = self._parse_table(node, state)\n\n        state = self._parse_caption(node, state)\n\n        yield from self._parse_paragraph(node, state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, document, text):\n        stack = []\n\n        root = lxml.html.fromstring(text)\n\n        # flattens children of node that are in the 'flatten' list\n        if self.flatten:\n            lxml.etree.strip_tags(root, self.flatten)\n        # Assign the text, which was stripped of the 'flatten'-tags, to the document\n        document.text = lxml.etree.tostring(root, encoding=\"unicode\")\n\n        # This dictionary contain the global state necessary to parse a\n        # document and each context element. This reflects the relationships\n        # defined in parser/models. This contains the state necessary to create\n        # the respective Contexts within the document.\n        state = {\n            \"visited\": set(),\n            \"parent\": {},  # map of parent[child] = node used to discover child\n            \"context\": {},  # track the Context of each node (context['td'] = Cell)\n            \"root\": root,\n            \"document\": document,\n            \"section\": {\"idx\": 0},\n            \"paragraph\": {\"idx\": 0},\n            \"figure\": {\"idx\": 0},\n            \"caption\": {\"idx\": 0},\n            \"table\": {\"idx\": 0},\n            \"sentence\": {\"idx\": 0, \"abs_offset\": 0},\n        }\n        # NOTE: Currently the helper functions directly manipulate the state\n        # rather than returning a modified copy.\n\n        # Iterative Depth-First Search\n        stack.append(root)\n        state[\"parent\"][root] = document\n        state[\"context\"][root] = document\n\n        tokenized_sentences = []\n        while stack:\n            node = stack.pop()\n            if node not in state[\"visited\"]:\n                state[\"visited\"].add(node)  # mark as visited\n\n                # Process\n                if self.lingual:\n                    tokenized_sentences += [y for y in self._parse_node(node, state)]\n                else:\n                    yield from self._parse_node(node, state)\n\n                # NOTE: This reversed() order is to ensure that the iterative\n                # DFS matches the order that would be produced by a recursive\n                # DFS implementation.\n                for child in reversed(node):\n                    # Skip nodes that are comments or blacklisted\n                    if child.tag is lxml.etree.Comment or (\n                        self.blacklist and child.tag in self.blacklist\n                    ):\n                        continue\n\n                    stack.append(child)\n\n                    # store the parent of the node, which is either the parent\n                    # Context, or if the parent did not create a Context, then\n                    # use the node's parent Context.\n                    state[\"parent\"][child] = (\n                        state[\"context\"][node]\n                        if node in state[\"context\"]\n                        else state[\"parent\"][node]\n                    )\n\n        if self.lingual:\n            yield from self.enrich_tokenized_sentences_with_nlp(tokenized_sentences)", "response": "This function parses the provided text into a dictionary of state necessary to create a new document."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_logging(\n    log_dir=tempfile.gettempdir(),\n    format=\"[%(asctime)s][%(levelname)s] %(name)s:%(lineno)s - %(message)s\",\n    level=logging.INFO,\n):\n    \"\"\"Configures logging to output to the provided log_dir.\n\n    Will use a nested directory whose name is the current timestamp.\n\n    :param log_dir: The directory to store logs in.\n    :type log_dir: str\n    :param format: The logging format string to use.\n    :type format: str\n    :param level: The logging level to use, e.g., logging.INFO.\n    \"\"\"\n\n    if not Meta.log_path:\n        # Generate a new directory using the log_dir, if it doesn't exist\n        dt = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        log_path = os.path.join(log_dir, dt)\n        if not os.path.exists(log_path):\n            os.makedirs(log_path)\n\n        # Configure the logger using the provided path\n        logging.basicConfig(\n            format=format,\n            level=level,\n            handlers=[\n                logging.FileHandler(os.path.join(log_path, \"fonduer.log\")),\n                logging.StreamHandler(),\n            ],\n        )\n\n        # Notify user of log location\n        logger.info(f\"Setting logging directory to: {log_path}\")\n        Meta.log_path = log_path\n    else:\n        logger.info(\n            f\"Logging was already initialized to use {Meta.log_path}.  \"\n            \"To configure logging manually, call fonduer.init_logging before \"\n            \"initialiting Meta.\"\n        )", "response": "Initializes logging to output to the provided log_dir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the unique Meta class.", "response": "def init(cls, conn_string=None):\n        \"\"\"Return the unique Meta class.\"\"\"\n        if conn_string:\n            _update_meta(conn_string)\n            # We initialize the engine within the models module because models'\n            # schema can depend on which data types are supported by the engine\n            Meta.Session = new_sessionmaker()\n            Meta.engine = Meta.Session.kw[\"bind\"]\n            logger.info(\n                f\"Connecting user:{Meta.DBUSER} \"\n                f\"to {Meta.DBHOST}:{Meta.DBPORT}/{Meta.DBNAME}\"\n            )\n            Meta._init_db()\n\n        if not Meta.log_path:\n            init_logging()\n\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the storage schema.", "response": "def _init_db(cls):\n        \"\"\" Initialize the storage schema.\n\n        This call must be performed after all classes that extend\n        Base are declared to ensure the storage schema is initialized.\n        \"\"\"\n        # This list of import defines which SQLAlchemy classes will be\n        # initialized when Meta.init() is called. If a sqlalchemy class is not\n        # imported before the call to create_all(), it will not be created.\n        import fonduer.candidates.models  # noqa\n        import fonduer.features.models  # noqa\n        import fonduer.learning.models  # noqa\n        import fonduer.parser.models  # noqa\n        import fonduer.supervision.models  # noqa\n        import fonduer.utils.models  # noqa\n\n        logger.info(\"Initializing the storage schema\")\n        Meta.Base.metadata.create_all(Meta.engine)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_custom_boundary(doc):\n    if doc.user_data == {}:\n        raise AttributeError(\"A list of Sentence is not attached to doc.user_data.\")\n    # Set every token.is_sent_start False because they are all True by default\n    for token_nr, token in enumerate(doc):\n        doc[token_nr].is_sent_start = False\n    # Set token.is_sent_start True when it is the first token of a Sentence\n    token_nr = 0\n    for sentence in doc.user_data:\n        doc[token_nr].is_sent_start = True\n        token_nr += len(sentence.words)\n    return doc", "response": "Set the sentence boundaries based on the already separated sentences."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if spaCy language model is installed.", "response": "def model_installed(name):\n        \"\"\"Check if spaCy language model is installed.\n\n        From https://github.com/explosion/spaCy/blob/master/spacy/util.py\n\n        :param name:\n        :return:\n        \"\"\"\n        data_path = util.get_data_path()\n        if not data_path or not data_path.exists():\n            raise IOError(f\"Can't find spaCy data path: {data_path}\")\n        if name in {d.name for d in data_path.iterdir()}:\n            return True\n        if Spacy.is_package(name):  # installed as package\n            return True\n        if Path(name).exists():  # path to model data directory\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_lang_model(self):\n        if self.lang in self.languages:\n            if not Spacy.model_installed(self.lang):\n                download(self.lang)\n            model = spacy.load(self.lang)\n        elif self.lang in self.alpha_languages:\n            language_module = importlib.import_module(f\"spacy.lang.{self.lang}\")\n            language_method = getattr(language_module, self.alpha_languages[self.lang])\n            model = language_method()\n        self.model = model", "response": "Load spaCy language model or download if model is available and not installed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enrich_sentences_with_NLP(self, all_sentences):\n        if not self.has_NLP_support():\n            raise NotImplementedError(\n                f\"Language {self.lang} not available in spacy beyond tokenization\"\n            )\n\n        if len(all_sentences) == 0:\n            return  # Nothing to parse\n\n        if self.model.has_pipe(\"sentencizer\"):\n            self.model.remove_pipe(\"sentencizer\")\n            self.logger.debug(\n                f\"Removed sentencizer ('sentencizer') from model. \"\n                f\"Now in pipeline: {self.model.pipe_names}\"\n            )\n\n        if self.model.has_pipe(\"sentence_boundary_detector\"):\n            self.model.remove_pipe(name=\"sentence_boundary_detector\")\n        self.model.add_pipe(\n            set_custom_boundary, before=\"parser\", name=\"sentence_boundary_detector\"\n        )\n\n        sentence_batches = self._split_sentences_by_char_limit(\n            all_sentences, self.model.max_length\n        )\n\n        # TODO: We could do this in parallel. Test speedup in the future\n        for sentence_batch in sentence_batches:\n            custom_tokenizer = TokenPreservingTokenizer(self.model.vocab)\n            # we circumvent redundant tokenization by using a custom\n            # tokenizer that directly uses the already separated words\n            # of each sentence as tokens\n            doc = custom_tokenizer(sentence_batch)\n            doc.user_data = sentence_batch\n            for name, proc in self.model.pipeline:  # iterate over components in order\n                doc = proc(doc)\n\n            try:\n                assert doc.is_parsed\n            except Exception:\n                self.logger.exception(f\"{doc} was not parsed\")\n\n            for sent, current_sentence_obj in zip(doc.sents, sentence_batch):\n                parts = defaultdict(list)\n\n                for i, token in enumerate(sent):\n                    parts[\"lemmas\"].append(token.lemma_)\n                    parts[\"pos_tags\"].append(token.tag_)\n                    parts[\"ner_tags\"].append(\n                        token.ent_type_ if token.ent_type_ else \"O\"\n                    )\n                    head_idx = (\n                        0 if token.head is token else token.head.i - sent[0].i + 1\n                    )\n                    parts[\"dep_parents\"].append(head_idx)\n                    parts[\"dep_labels\"].append(token.dep_)\n                current_sentence_obj.pos_tags = parts[\"pos_tags\"]\n                current_sentence_obj.lemmas = parts[\"lemmas\"]\n                current_sentence_obj.ner_tags = parts[\"ner_tags\"]\n                current_sentence_obj.dep_parents = parts[\"dep_parents\"]\n                current_sentence_obj.dep_labels = parts[\"dep_labels\"]\n                yield current_sentence_obj", "response": "Enrich a list of fonduer Sentence objects with NLP features. We merge the text of all Sentences with NLP features. We parse the text of all Sentences and then parse the texts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit input text into sentences that match CoreNLP s default format and are not yet processed.", "response": "def split_sentences(self, text):\n        \"\"\"\n        Split input text into sentences that match CoreNLP's default format,\n        but are not yet processed.\n\n        :param text: The text of the parent paragraph of the sentences\n        :return:\n        \"\"\"\n\n        if self.model.has_pipe(\"sentence_boundary_detector\"):\n            self.model.remove_pipe(name=\"sentence_boundary_detector\")\n\n        if not self.model.has_pipe(\"sentencizer\"):\n            sentencizer = self.model.create_pipe(\"sentencizer\")  # add sentencizer\n            self.model.add_pipe(sentencizer)\n        try:\n            doc = self.model(text, disable=[\"parser\", \"tagger\", \"ner\"])\n        except ValueError:\n            # temporary increase character limit of spacy\n            # 'Probably save' according to spacy, as no parser or NER is used\n            previous_max_length = self.model.max_length\n            self.model.max_length = 100_000_000\n            self.logger.warning(\n                f\"Temporarily increased spacy maximum \"\n                f\"character limit to {self.model.max_length} to split sentences.\"\n            )\n            doc = self.model(text, disable=[\"parser\", \"tagger\", \"ner\"])\n            self.model.max_length = previous_max_length\n            self.logger.warning(\n                f\"Spacy maximum \"\n                f\"character limit set back to {self.model.max_length}.\"\n            )\n\n        doc.is_parsed = True\n        position = 0\n        for sent in doc.sents:\n            parts = defaultdict(list)\n            text = sent.text\n\n            for i, token in enumerate(sent):\n                parts[\"words\"].append(str(token))\n                parts[\"lemmas\"].append(token.lemma_)\n                parts[\"pos_tags\"].append(token.pos_)\n                parts[\"ner_tags\"].append(\"\")  # placeholder for later NLP parsing\n                parts[\"char_offsets\"].append(token.idx)\n                parts[\"abs_char_offsets\"].append(token.idx)\n                parts[\"dep_parents\"].append(0)  # placeholder for later NLP parsing\n                parts[\"dep_labels\"].append(\"\")  # placeholder for later NLP parsing\n\n            # make char_offsets relative to start of sentence\n            parts[\"char_offsets\"] = [\n                p - parts[\"char_offsets\"][0] for p in parts[\"char_offsets\"]\n            ]\n            parts[\"position\"] = position\n            parts[\"text\"] = text\n\n            position += 1\n\n            yield parts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setup_model_loss(self, lr):\n        # Setup loss\n        if not hasattr(self, \"loss\"):\n            self.loss = SoftCrossEntropyLoss()\n\n        # Setup optimizer\n        if not hasattr(self, \"optimizer\"):\n            self.optimizer = optim.Adam(self.parameters(), lr=lr)", "response": "Setup loss and optimizer for PyTorch model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef train(\n        self,\n        X_train,\n        Y_train,\n        n_epochs=25,\n        lr=0.01,\n        batch_size=256,\n        shuffle=True,\n        X_dev=None,\n        Y_dev=None,\n        print_freq=5,\n        dev_ckpt=True,\n        dev_ckpt_delay=0.75,\n        b=0.5,\n        pos_label=1,\n        seed=1234,\n        host_device=\"CPU\",\n    ):\n        \"\"\"\n        Generic training procedure for PyTorch model\n\n        :param X_train: The training data which is a (list of Candidate objects,\n            a sparse matrix of corresponding features) pair.\n        :type X_train: pair\n        :param Y_train: Array of marginal probabilities for each Candidate.\n        :type Y_train: list or numpy.array\n        :param n_epochs: Number of training epochs.\n        :type n_epochs: int\n        :param lr: Learning rate.\n        :type lr: float\n        :param batch_size: Batch size for learning model.\n        :type batch_size: int\n        :param shuffle: If True, shuffle training data every epoch.\n        :type shuffle: bool\n        :param X_dev: Candidates for evaluation, same format as X_train.\n        :param Y_dev: Labels for evaluation, same format as Y_train.\n        :param print_freq: number of epochs at which to print status, and if present,\n            evaluate the dev set (X_dev, Y_dev).\n        :type print_freq: int\n        :param dev_ckpt: If True, save a checkpoint whenever highest score\n            on (X_dev, Y_dev) reached. Note: currently only evaluates at\n            every @print_freq epochs.\n        :param dev_ckpt_delay: Start dev checkpointing after this portion\n            of n_epochs.\n        :type dev_ckpt_delay: float\n        :param b: Decision boundary *for binary setting only*.\n        :type b: float\n        :param pos_label: Positive class index *for binary setting only*. Default: 1\n        :type pos_label: int\n        :param seed: Random seed\n        :type seed: int\n        :param host_device: Host device\n        :type host_device: str\n        \"\"\"\n\n        # Update training parameters\n        self.settings.update(\n            {\n                \"n_epochs\": n_epochs,\n                \"lr\": lr,\n                \"batch_size\": batch_size,\n                \"shuffle\": shuffle,\n                \"seed\": 1234,\n                \"host_device\": host_device,\n            }\n        )\n\n        # Set random seed\n        self._set_random_seed(self.settings[\"seed\"])\n\n        self._check_input(X_train)\n        verbose = print_freq > 0\n\n        # Update cardinality of the model with training marginals\n        self.cardinality = Y_train.shape[1]\n\n        # Make sure marginals are in [0,1] (v.s e.g. [-1, 1])\n        if not np.all(Y_train.sum(axis=1) - 1 < 1e-10):\n            raise ValueError(\"Y_train must be row-stochastic (rows sum to 1).\")\n        if not np.all(Y_train >= 0):\n            raise ValueError(\"Y_train must have values in [0,1].\")\n\n        # Remove unlabeled examples\n        diffs = Y_train.max(axis=1) - Y_train.min(axis=1)\n        train_idxs = np.where(diffs > 1e-6)[0]\n\n        self._update_settings(X_train)\n\n        _X_train, _Y_train = self._preprocess_data(\n            X_train, Y_train, idxs=train_idxs, train=True\n        )\n\n        train_dataloader = DataLoader(\n            MultiModalDataset(_X_train, _Y_train),\n            batch_size=self.settings[\"batch_size\"],\n            collate_fn=self._collate_fn(),\n            shuffle=self.settings[\"shuffle\"],\n        )\n\n        if X_dev is not None:\n            _X_dev, _Y_dev = self._preprocess_data(X_dev, Y_dev)\n\n        if self.settings[\"host_device\"] in self._gpu:\n            if not torch.cuda.is_available():\n                self.settings[\"host_device\"] = \"CPU\"\n                self.logger.info(\"GPU is not available, switching to CPU...\")\n            else:\n                self.logger.info(\"Using GPU...\")\n\n        self.logger.info(f\"Settings: {self.settings}\")\n\n        # Build network\n        self._build_model()\n        self._setup_model_loss(self.settings[\"lr\"])\n\n        # Set up GPU if necessary\n        if self.settings[\"host_device\"] in self._gpu:\n            nn.Module.cuda(self)\n\n        # Run mini-batch SGD\n        n = len(_X_train)\n        if self.settings[\"batch_size\"] > n:\n            self.logger.info(f\"Switching batch size to {n} for training.\")\n        batch_size = min(self.settings[\"batch_size\"], n)\n\n        if verbose:\n            st = time()\n            self.logger.info(f\"[{self.name}] Training model\")\n            self.logger.info(\n                f\"[{self.name}] \"\n                f\"n_train={n} \"\n                f\"#epochs={self.settings['n_epochs']} \"\n                f\"batch size={batch_size}\"\n            )\n\n        dev_score_opt = 0.0\n\n        for epoch in range(self.settings[\"n_epochs\"]):\n            iteration_losses = []\n\n            nn.Module.train(self, True)\n\n            for X_batch, Y_batch in train_dataloader:\n                # zero gradients for each batch\n                self.optimizer.zero_grad()\n\n                output = self._calc_logits(X_batch)\n\n                loss = self.loss(output, Y_batch)\n\n                # Compute gradient\n                loss.backward()\n\n                # Update the parameters\n                self.optimizer.step()\n\n                iteration_losses.append(self._non_cuda(loss))\n\n            # Print training stats and optionally checkpoint model\n            if (\n                verbose and (epoch + 1) % print_freq == 0\n            ) or epoch + 1 == self.settings[\"n_epochs\"]:\n                # Log the training loss into tensorboard\n                self.tensorboard_logger.add_scalar(\"loss\", loss.item(), epoch + 1)\n\n                msg = (\n                    f\"[{self.name}] \"\n                    f\"Epoch {epoch + 1} ({time() - st:.2f}s)\\t\"\n                    f\"Average loss={torch.stack(iteration_losses).mean():.6f}\"\n                )\n                if X_dev is not None:\n                    scores = self.score(_X_dev, _Y_dev, b=b, pos_label=pos_label)\n\n                    score = scores[\"accuracy\"] if self.cardinality > 2 else scores[\"f1\"]\n                    score_label = \"Acc.\" if self.cardinality > 2 else \"F1\"\n                    msg += f\"\\tDev {score_label}={100.0 * score:.2f}\"\n\n                    # Log the evaulation score on dev set into tensorboard\n                    for metric in scores.keys():\n                        self.tensorboard_logger.add_scalar(\n                            metric, scores[metric], epoch + 1\n                        )\n\n                self.logger.info(msg)\n\n                # Save checkpoint\n                model_file = f\"checkpoint_epoch_{epoch + 1}.pt\"\n                self.save(model_file=model_file, save_dir=self.settings[\"log_dir\"])\n\n                # If best score on dev set so far and dev checkpointing is\n                # active, save best checkpoint\n                if (\n                    X_dev is not None\n                    and dev_ckpt\n                    and epoch > dev_ckpt_delay * self.settings[\"n_epochs\"]\n                    and score > dev_score_opt\n                ):\n                    dev_score_opt = score\n                    self.logger.info(\n                        f\"Saving best checkpoint \"\n                        f'{self.settings[\"log_dir\"]}/{model_file}.'\n                    )\n                    copyfile(\n                        f'{self.settings[\"log_dir\"]}/{model_file}',\n                        f'{self.settings[\"log_dir\"]}/best_model.pt',\n                    )\n\n                if (X_dev is None or dev_ckpt is False) and epoch + 1 == self.settings[\n                    \"n_epochs\"\n                ]:\n                    self.logger.info(\n                        f\"Saving final model as best checkpoint \"\n                        f'{self.settings[\"log_dir\"]}/{model_file}.'\n                    )\n                    copyfile(\n                        f'{self.settings[\"log_dir\"]}/{model_file}',\n                        f'{self.settings[\"log_dir\"]}/best_model.pt',\n                    )\n\n        # Conclude training\n        if verbose:\n            self.logger.info(f\"[{self.name}] Training done ({time() - st:.2f}s)\")\n\n        # Load the best checkpoint (i.e. best on dev set)\n        self.logger.info(\"Loading best checkpoint\")\n        self.load(model_file=\"best_model.pt\", save_dir=self.settings[\"log_dir\"])", "response": "Train the PyTorch model for the given Candidate Classes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the marginals for the given candidates X.", "response": "def marginals(self, X):\n        \"\"\"\n        Compute the marginals for the given candidates X.\n        Note: split into batches to avoid OOM errors.\n\n        :param X: The input data which is a (list of Candidate objects, a sparse\n            matrix of corresponding features) pair or a list of\n            (Candidate, features) pairs.\n        :type X: pair or list\n        \"\"\"\n\n        nn.Module.train(self, False)\n\n        if self._check_input(X):\n            X = self._preprocess_data(X)\n\n        dataloader = DataLoader(\n            MultiModalDataset(X),\n            batch_size=self.settings[\"batch_size\"],\n            collate_fn=self._collate_fn(),\n            shuffle=False,\n        )\n\n        marginals = torch.Tensor([])\n\n        for X_batch in dataloader:\n            marginal = self._non_cuda(self._calc_logits(X_batch))\n            marginals = torch.cat((marginals, marginal), 0)\n\n        return F.softmax(marginals, dim=-1).detach().numpy()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_marginals(self, session, X, training=False):\n\n        save_marginals(session, X, self.marginals(X), training=training)", "response": "Saves the predicted marginal probabilities for the Candidates X."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(self, X, b=0.5, pos_label=1, return_probs=False):\n\n        if self._check_input(X):\n            X = self._preprocess_data(X)\n\n        Y_prob = self.marginals(X)\n\n        if self.cardinality > 2:\n            Y_pred = Y_prob.argmax(axis=1) + 1\n            if return_probs:\n                return Y_pred, Y_prob\n            else:\n                return Y_pred\n\n        if pos_label not in [1, 2]:\n            raise ValueError(\"pos_label must have values in {1,2}.\")\n        self.logger.info(f\"Using positive label class {pos_label} with threshold {b}\")\n\n        Y_pred = np.array(\n            [pos_label if p[pos_label - 1] > b else 3 - pos_label for p in Y_prob]\n        )\n        if return_probs:\n            return Y_pred, Y_prob\n        else:\n            return Y_pred", "response": "Predict the class for X based on predicted marginal probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the summary scores of the given set of test classes.", "response": "def score(\n        self, X_test, Y_test, b=0.5, pos_label=1, set_unlabeled_as_neg=True, beta=1\n    ):\n        \"\"\"\n        Returns the summary scores:\n            * For binary: precision, recall, F-beta score, ROC-AUC score\n            * For categorical: accuracy\n\n        :param X_test: The input test candidates.\n        :type X_test: pair with candidates and corresponding features\n        :param Y_test: The input test labels.\n        :type Y_test: list of labels\n        :param b: Decision boundary *for binary setting only*.\n        :type b: float\n        :param pos_label: Positive class index *for binary setting only*. Default: 1\n        :type pos_label: int\n        :param set_unlabeled_as_neg: Whether to map 0 labels -> -1,\n            *for binary setting only*\n        :type set_unlabeled_as_neg: bool\n        :param beta: For F-beta score; by default beta = 1 => F-1 score.\n        :type beta: int\n        \"\"\"\n\n        if self._check_input(X_test):\n            X_test, Y_test = self._preprocess_data(X_test, Y_test)\n\n        Y_pred, Y_prob = self.predict(\n            X_test, b=b, pos_label=pos_label, return_probs=True\n        )\n\n        # Convert Y_test to dense numpy array\n        try:\n            Y_test = np.array(Y_test.todense()).reshape(-1)\n        except Exception:\n            Y_test = np.array(Y_test)\n\n        scores = {}\n\n        # Compute P/R/F1 for binary settings\n        if self.cardinality == 2:\n            # Either remap or filter out unlabeled (0-valued) test labels\n            if set_unlabeled_as_neg:\n                Y_test[Y_test == 0] = 3 - pos_label\n            else:\n                Y_pred = Y_pred[Y_test != 0]\n                Y_test = Y_test[Y_test != 0]\n\n            # Compute and return precision, recall, and F1 score\n\n            pred_pos = np.where(Y_pred == pos_label, True, False)\n            gt_pos = np.where(Y_test == pos_label, True, False)\n            TP = np.sum(pred_pos * gt_pos)\n            FP = np.sum(pred_pos * np.logical_not(gt_pos))\n            FN = np.sum(np.logical_not(pred_pos) * gt_pos)\n\n            prec = TP / (TP + FP) if TP + FP > 0 else 0.0\n            rec = TP / (TP + FN) if TP + FN > 0 else 0.0\n            fbeta = (\n                (1 + beta ** 2) * (prec * rec) / ((beta ** 2 * prec) + rec)\n                if (beta ** 2 * prec) + rec > 0\n                else 0.0\n            )\n            scores[\"precision\"] = prec\n            scores[\"recall\"] = rec\n            scores[f\"f{beta}\"] = fbeta\n\n            roc_auc = roc_auc_score(Y_test, Y_prob[:, pos_label - 1])\n            scores[\"roc_auc\"] = roc_auc\n\n        # Compute accuracy for all settings\n        acc = np.where([Y_pred == Y_test])[0].shape[0] / float(Y_test.shape[0])\n        scores[\"accuracy\"] = acc\n\n        return scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the HTML tag of the Mention s parent.", "response": "def get_parent_tag(mention):\n    \"\"\"Return the HTML tag of the Mention's parent.\n\n    These may be tags such as 'p', 'h2', 'table', 'div', etc.\n    If a candidate is passed in, only the tag of its first Mention is returned.\n\n    :param mention: The Mention to evaluate\n    :rtype: string\n    \"\"\"\n    span = _to_span(mention)\n    i = _get_node(span.sentence)\n    return str(i.getparent().tag) if i.getparent() is not None else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the HTML tag of the Mention s previous siblings.", "response": "def get_prev_sibling_tags(mention):\n    \"\"\"Return the HTML tag of the Mention's previous siblings.\n\n    Previous siblings are Mentions which are at the same level in the HTML tree\n    as the given mention, but are declared before the given mention. If a\n    candidate is passed in, only the previous siblings of its first Mention are\n    considered in the calculation.\n\n    :param mention: The Mention to evaluate\n    :rtype: list of strings\n    \"\"\"\n    span = _to_span(mention)\n    prev_sibling_tags = []\n    i = _get_node(span.sentence)\n    while i.getprevious() is not None:\n        prev_sibling_tags.insert(0, str(i.getprevious().tag))\n        i = i.getprevious()\n    return prev_sibling_tags"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the HTML tag of the Mention s next siblings.", "response": "def get_next_sibling_tags(mention):\n    \"\"\"Return the HTML tag of the Mention's next siblings.\n\n    Next siblings are Mentions which are at the same level in the HTML tree as\n    the given mention, but are declared after the given mention.\n    If a candidate is passed in, only the next siblings of its last Mention\n    are considered in the calculation.\n\n    :param mention: The Mention to evaluate\n    :rtype: list of strings\n    \"\"\"\n    span = _to_span(mention)\n    next_sibling_tags = []\n    i = _get_node(span.sentence)\n    while i.getnext() is not None:\n        next_sibling_tags.append(str(i.getnext().tag))\n        i = i.getnext()\n    return next_sibling_tags"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the HTML classes of the Mention s ancestors.", "response": "def get_ancestor_class_names(mention):\n    \"\"\"Return the HTML classes of the Mention's ancestors.\n\n    If a candidate is passed in, only the ancestors of its first Mention are\n    returned.\n\n    :param mention: The Mention to evaluate\n    :rtype: list of strings\n    \"\"\"\n    span = _to_span(mention)\n    class_names = []\n    i = _get_node(span.sentence)\n    while i is not None:\n        class_names.insert(0, str(i.get(\"class\")))\n        i = i.getparent()\n    return class_names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the HTML tag of the Mention s ancestors.", "response": "def get_ancestor_tag_names(mention):\n    \"\"\"Return the HTML tag of the Mention's ancestors.\n\n    For example, ['html', 'body', 'p'].\n    If a candidate is passed in, only the ancestors of its first Mention are returned.\n\n    :param mention: The Mention to evaluate\n    :rtype: list of strings\n    \"\"\"\n    span = _to_span(mention)\n    tag_names = []\n    i = _get_node(span.sentence)\n    while i is not None:\n        tag_names.insert(0, str(i.tag))\n        i = i.getparent()\n    return tag_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ancestor_id_names(mention):\n    span = _to_span(mention)\n    id_names = []\n    i = _get_node(span.sentence)\n    while i is not None:\n        id_names.insert(0, str(i.get(\"id\")))\n        i = i.getparent()\n    return id_names", "response": "Return the HTML id s of the Mention s ancestors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef common_ancestor(c):\n    span1 = _to_span(c[0])\n    span2 = _to_span(c[1])\n    ancestor1 = np.array(span1.sentence.xpath.split(\"/\"))\n    ancestor2 = np.array(span2.sentence.xpath.split(\"/\"))\n    min_len = min(ancestor1.size, ancestor2.size)\n    return list(ancestor1[: np.argmin(ancestor1[:min_len] == ancestor2[:min_len])])", "response": "Return the path to the root that is shared between a binary - Mention Candidate."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nforwards function. :param x: Input sequence tensor. :type x: torch.Tensor of shape (batch_size * length) :param x_mask: Use use_cuda or not. :type x_mask: torch.Tensor of shape (batch_size * length) :param state_word: Initial state of LSTM. :type state_word: torch.Tensor (see init_hidden() for more information) :return: Output of LSTM layer, either after mean pooling or attention. :rtype: torch.Tensor with shape (batch_size, num_directions * hidden_size) if num_classes > 0 otherwise with shape (batch_size, num_classes)", "response": "def forward(self, x, x_mask, state_word):\n        \"\"\"Forward function.\n\n        :param x: Input sequence tensor.\n        :type x: torch.Tensor of shape (batch_size * length)\n        :param x_mask: Use use_cuda or not.\n        :type x_mask: torch.Tensor of shape (batch_size * length)\n        :param state_word: Initial state of LSTM.\n        :type state_word: torch.Tensor (see init_hidden() for more information)\n        :return: Output of LSTM layer, either after mean pooling or attention.\n        :rtype: torch.Tensor with shape (batch_size, num_directions * hidden_size)\n            if num_classes > 0 otherwise with shape (batch_size, num_classes)\n        \"\"\"\n\n        x_emb = self.drop(self.lookup(x))\n        output_word, state_word = self.word_lstm(x_emb, state_word)\n        output_word = self.drop(output_word)\n        if self.attention:\n            \"\"\"\n            An attention layer where the attention weight is\n            a = T' . tanh(Wx + b)\n            where x is the input, b is the bias.\n            \"\"\"\n            word_squish = torch.tanh(self.attn_linear_w_1(output_word))\n            word_attn = self.attn_linear_w_2(word_squish)\n            word_attn.data.masked_fill_(x_mask.data.unsqueeze(dim=2), float(\"-inf\"))\n            word_attn_norm = torch.sigmoid(word_attn.squeeze(2))\n            word_attn_vectors = torch.bmm(\n                output_word.transpose(1, 2), word_attn_norm.unsqueeze(2)\n            ).squeeze(2)\n            output = (\n                self.linear(word_attn_vectors)\n                if self.final_linear\n                else word_attn_vectors\n            )\n        else:\n            \"\"\"\n            Mean pooling\n            \"\"\"\n            x_lens = x_mask.data.eq(0).long().sum(dim=1)\n            if self.use_cuda:\n                weights = torch.ones(x.size()).cuda() / x_lens.unsqueeze(1).float()\n            else:\n                weights = torch.ones(x.size()) / x_lens.unsqueeze(1).float()\n            weights.data.masked_fill_(x_mask.data.unsqueeze(dim=2), 0.0)\n            word_vectors = torch.bmm(\n                output_word.transpose(1, 2), weights.unsqueeze(2)\n            ).squeeze(2)\n            output = self.linear(word_vectors) if self.final_linear else word_vectors\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_hidden(self, batch_size):\n\n        b = 2 if self.bidirectional else 1\n        if self.use_cuda:\n            return (\n                torch.zeros(self.num_layers * b, batch_size, self.lstm_hidden).cuda(),\n                torch.zeros(self.num_layers * b, batch_size, self.lstm_hidden).cuda(),\n            )\n        else:\n            return (\n                torch.zeros(self.num_layers * b, batch_size, self.lstm_hidden),\n                torch.zeros(self.num_layers * b, batch_size, self.lstm_hidden),\n            )", "response": "Initiate the initial state of LSTM records."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_scalar(self, name, value, step):\n        self.writer.add_scalar(name, value, step)", "response": "Log a scalar variable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mention_to_tokens(mention, token_type=\"words\", lowercase=False):\n\n    tokens = mention.context.sentence.__dict__[token_type]\n    return [w.lower() if lowercase else w for w in tokens]", "response": "Extract tokens from the mention object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mark_sentence(s, args):\n\n    marks = sorted([y for m in args for y in mark(*m)], reverse=True)\n    x = list(s)\n    for k, v in marks:\n        x.insert(k, v)\n    return x", "response": "Insert markers around relation arguments in word sequence s."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npad the batch into matrix .", "response": "def pad_batch(batch, max_len=0, type=\"int\"):\n    \"\"\"Pad the batch into matrix\n\n    :param batch: The data for padding.\n    :type batch: list of word index sequences\n    :param max_len: Max length of sequence of padding.\n    :type max_len: int\n    :param type: mask value type.\n    :type type: str\n    :return: The padded matrix and correspoing mask matrix.\n    :rtype: pair of torch.Tensors with shape (batch_size, max_sent_len)\n    \"\"\"\n\n    batch_size = len(batch)\n    max_sent_len = int(np.max([len(x) for x in batch]))\n    if max_len > 0 and max_len < max_sent_len:\n        max_sent_len = max_len\n    if type == \"float\":\n        idx_matrix = np.zeros((batch_size, max_sent_len), dtype=np.float32)\n    else:\n        idx_matrix = np.zeros((batch_size, max_sent_len), dtype=np.int)\n\n    for idx1, i in enumerate(batch):\n        for idx2, j in enumerate(i):\n            if idx2 >= max_sent_len:\n                break\n            idx_matrix[idx1, idx2] = j\n    idx_matrix = torch.tensor(idx_matrix)\n    mask_matrix = torch.tensor(torch.eq(idx_matrix.data, 0))\n    return idx_matrix, mask_matrix"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a stable id.", "response": "def get_stable_id(self):\n        \"\"\"\n        Return a stable id.\n\n        :rtype: string\n        \"\"\"\n        return construct_stable_id(\n            self.sentence,\n            self._get_polymorphic_identity(),\n            self.char_start,\n            self.char_end,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _char_to_word_index(self, ci):\n        i = None\n        for i, co in enumerate(self.sentence.char_offsets):\n            if ci == co:\n                return i\n            elif ci < co:\n                return i - 1\n        return i", "response": "Return the index of the word this character is in."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the tokens of the attribute a.", "response": "def get_attrib_tokens(self, a=\"words\"):\n        \"\"\"Get the tokens of sentence attribute *a*.\n\n        Intuitively, like calling::\n\n            span.a\n\n\n        :param a: The attribute to get tokens for.\n        :type a: str\n        :return: The tokens of sentence attribute defined by *a* for the span.\n        :rtype: list\n        \"\"\"\n        return self.sentence.__getattribute__(a)[\n            self.get_word_start_index() : self.get_word_end_index() + 1\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the span of the sentence attribute a.", "response": "def get_attrib_span(self, a, sep=\" \"):\n        \"\"\"Get the span of sentence attribute *a*.\n\n        Intuitively, like calling::\n\n            sep.join(span.a)\n\n        :param a: The attribute to get a span for.\n        :type a: str\n        :param sep: The separator to use for the join.\n        :type sep: str\n        :return: The joined tokens, or text if a=\"words\".\n        :rtype: str\n        \"\"\"\n        # NOTE: Special behavior for words currently (due to correspondence\n        # with char_offsets)\n        if a == \"words\":\n            return self.sentence.text[self.char_start : self.char_end + 1]\n        else:\n            return sep.join(self.get_attrib_tokens(a))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a set of Document objects for all files in the directory.", "response": "def _generate(self):\n        \"\"\"Parses a file or directory of files into a set of ``Document`` objects.\"\"\"\n        doc_count = 0\n        for fp in self.all_files:\n            for doc in self._get_docs_for_path(fp):\n                yield doc\n                doc_count += 1\n                if doc_count >= self.max_docs:\n                    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if all the components of c are horizontally aligned.", "response": "def is_horz_aligned(c):\n    \"\"\"Return True if all the components of c are horizontally aligned.\n\n    Horizontal alignment means that the bounding boxes of each Mention of c\n    shares a similar y-axis value in the visual rendering of the document.\n\n    :param c: The candidate to evaluate\n    :rtype: boolean\n    \"\"\"\n    return all(\n        [\n            _to_span(c[i]).sentence.is_visual()\n            and bbox_horz_aligned(\n                bbox_from_span(_to_span(c[i])), bbox_from_span(_to_span(c[0]))\n            )\n            for i in range(len(c))\n        ]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning true if all the components of c are vertically aligned.", "response": "def is_vert_aligned(c):\n    \"\"\"Return true if all the components of c are vertically aligned.\n\n    Vertical alignment means that the bounding boxes of each Mention of c\n    shares a similar x-axis value in the visual rendering of the document.\n\n    :param c: The candidate to evaluate\n    :rtype: boolean\n    \"\"\"\n    return all(\n        [\n            _to_span(c[i]).sentence.is_visual()\n            and bbox_vert_aligned(\n                bbox_from_span(_to_span(c[i])), bbox_from_span(_to_span(c[0]))\n            )\n            for i in range(len(c))\n        ]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if all components are vertically aligned on their left border.", "response": "def is_vert_aligned_left(c):\n    \"\"\"Return true if all components are vertically aligned on their left border.\n\n    Vertical alignment means that the bounding boxes of each Mention of c\n    shares a similar x-axis value in the visual rendering of the document. In\n    this function the similarity of the x-axis value is based on the left\n    border of their bounding boxes.\n\n    :param c: The candidate to evaluate\n    :rtype: boolean\n    \"\"\"\n    return all(\n        [\n            _to_span(c[i]).sentence.is_visual()\n            and bbox_vert_aligned_left(\n                bbox_from_span(_to_span(c[i])), bbox_from_span(_to_span(c[0]))\n            )\n            for i in range(len(c))\n        ]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_vert_aligned_right(c):\n    return all(\n        [\n            _to_span(c[i]).sentence.is_visual()\n            and bbox_vert_aligned_right(\n                bbox_from_span(_to_span(c[i])), bbox_from_span(_to_span(c[0]))\n            )\n            for i in range(len(c))\n        ]\n    )", "response": "Return true if all components vertically aligned on their right border."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if all the components are vertically aligned on their center.", "response": "def is_vert_aligned_center(c):\n    \"\"\"Return true if all the components are vertically aligned on their center.\n\n    Vertical alignment means that the bounding boxes of each Mention of c\n    shares a similar x-axis value in the visual rendering of the document. In\n    this function the similarity of the x-axis value is based on the center of\n    their bounding boxes.\n\n    :param c: The candidate to evaluate\n    :rtype: boolean\n    \"\"\"\n    return all(\n        [\n            _to_span(c[i]).sentence.is_visual()\n            and bbox_vert_aligned_center(\n                bbox_from_span(_to_span(c[i])), bbox_from_span(_to_span(c[0]))\n            )\n            for i in range(len(c))\n        ]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef same_page(c):\n    return all(\n        [\n            _to_span(c[i]).sentence.is_visual()\n            and bbox_from_span(_to_span(c[i])).page\n            == bbox_from_span(_to_span(c[0])).page\n            for i in range(len(c))\n        ]\n    )", "response": "Return true if all the components of c are on the same page of the document."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_horz_ngrams(\n    mention, attrib=\"words\", n_min=1, n_max=1, lower=True, from_sentence=True\n):\n    \"\"\"Return all ngrams which are visually horizontally aligned with the Mention.\n\n    Note that if a candidate is passed in, all of its Mentions will be searched.\n\n    :param mention: The Mention to evaluate\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param lower: If True, all ngrams will be returned in lower case\n    :param from_sentence: If True, returns ngrams from any horizontally aligned\n        Sentences, rather than just horizontally aligned ngrams themselves.\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in _get_direction_ngrams(\n            \"horz\", span, attrib, n_min, n_max, lower, from_sentence\n        ):\n            yield ngram", "response": "Returns all ngrams which are visually horizontally aligned with the Mention."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the vertical percentile of the page the Mention is located in.", "response": "def get_page_vert_percentile(\n    mention, page_width=DEFAULT_WIDTH, page_height=DEFAULT_HEIGHT\n):\n    \"\"\"Return which percentile from the TOP in the page the Mention is located in.\n\n    Percentile is calculated where the top of the page is 0.0, and the bottom\n    of the page is 1.0. For example, a Mention in at the top 1/4 of the page\n    will have a percentile of 0.25.\n\n    Page width and height are based on pt values::\n\n        Letter      612x792\n        Tabloid     792x1224\n        Ledger      1224x792\n        Legal       612x1008\n        Statement   396x612\n        Executive   540x720\n        A0          2384x3371\n        A1          1685x2384\n        A2          1190x1684\n        A3          842x1190\n        A4          595x842\n        A4Small     595x842\n        A5          420x595\n        B4          729x1032\n        B5          516x729\n        Folio       612x936\n        Quarto      610x780\n        10x14       720x1008\n\n    and should match the source documents. Letter size is used by default.\n\n    Note that if a candidate is passed in, only the vertical percentil of its\n    first Mention is returned.\n\n    :param mention: The Mention to evaluate\n    :param page_width: The width of the page. Default to Letter paper width.\n    :param page_height: The heigh of the page. Default to Letter paper height.\n    :rtype: float in [0.0, 1.0]\n    \"\"\"\n    span = _to_span(mention)\n    return bbox_from_span(span).top / page_height"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the horizontal percentile of the page the Mention is located in.", "response": "def get_page_horz_percentile(\n    mention, page_width=DEFAULT_WIDTH, page_height=DEFAULT_HEIGHT\n):\n    \"\"\"Return which percentile from the LEFT in the page the Mention is located in.\n\n    Percentile is calculated where the left of the page is 0.0, and the right\n    of the page is 1.0.\n\n    Page width and height are based on pt values::\n\n        Letter      612x792\n        Tabloid     792x1224\n        Ledger      1224x792\n        Legal       612x1008\n        Statement   396x612\n        Executive   540x720\n        A0          2384x3371\n        A1          1685x2384\n        A2          1190x1684\n        A3          842x1190\n        A4          595x842\n        A4Small     595x842\n        A5          420x595\n        B4          729x1032\n        B5          516x729\n        Folio       612x936\n        Quarto      610x780\n        10x14       720x1008\n\n    and should match the source documents. Letter size is used by default.\n\n    Note that if a candidate is passed in, only the vertical percentile of its\n    first Mention is returned.\n\n    :param c: The Mention to evaluate\n    :param page_width: The width of the page. Default to Letter paper width.\n    :param page_height: The heigh of the page. Default to Letter paper height.\n    :rtype: float in [0.0, 1.0]\n    \"\"\"\n    span = _to_span(mention)\n    return bbox_from_span(span).left / page_width"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_visual_aligned_lemmas(mention):\n    spans = _to_spans(mention)\n    for span in spans:\n        sentence = span.sentence\n        doc = sentence.document\n        # cache features for the entire document\n        _preprocess_visual_features(doc)\n\n        for aligned_lemma in sentence._aligned_lemmas:\n            yield aligned_lemma", "response": "Returns a generator of the lemmas aligned visually with the Mention."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef camel_to_under(name):\n    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()", "response": "Converts camel - case string to lowercase string separated by underscores."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_as_dict(x):\n    if isinstance(x, dict):\n        return x\n    else:\n        try:\n            return x._asdict()\n        except AttributeError:\n            return x.__dict__", "response": "Return an object as a dictionary of its attributes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(\n        self, doc_loader, clear=True, parallelism=None, progress_bar=True, **kwargs\n    ):\n        \"\"\"\n        Apply the given UDF to the set of objects returned by the doc_loader, either\n        single or multi-threaded, and optionally calling clear() first.\n        \"\"\"\n        # Clear everything downstream of this UDF if requested\n        if clear:\n            self.clear(**kwargs)\n\n        # Track the last documents parsed by apply\n        self.last_docs = set(doc.name for doc in doc_loader)\n\n        # Execute the UDF\n        self.logger.info(\"Running UDF...\")\n\n        # Setup progress bar\n        if progress_bar:\n            self.logger.debug(\"Setting up progress bar...\")\n            if hasattr(doc_loader, \"__len__\"):\n                self.pb = tqdm(total=len(doc_loader))\n            else:\n                self.logger.error(\"Could not determine size of progress bar\")\n\n        # Use the parallelism of the class if none is provided to apply\n        parallelism = parallelism if parallelism else self.parallelism\n        if parallelism < 2:\n            self._apply_st(doc_loader, clear=clear, **kwargs)\n        else:\n            self._apply_mt(doc_loader, parallelism, clear=clear, **kwargs)\n\n        # Close progress bar\n        if self.pb is not None:\n            self.logger.debug(\"Closing progress bar...\")\n            self.pb.close()", "response": "Apply the given UDF to the set of objects returned by the given doc_loader."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the UDF single - threaded optionally with progress bar", "response": "def _apply_st(self, doc_loader, **kwargs):\n        \"\"\"Run the UDF single-threaded, optionally with progress bar\"\"\"\n        udf = self.udf_class(**self.udf_init_kwargs)\n\n        # Run single-thread\n        for doc in doc_loader:\n            if self.pb is not None:\n                self.pb.update(1)\n\n            udf.session.add_all(y for y in udf.apply(doc, **kwargs))\n\n        # Commit session and close progress bar if applicable\n        udf.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_mt(self, doc_loader, parallelism, **kwargs):\n        if not Meta.postgres:\n            raise ValueError(\"Fonduer must use PostgreSQL as a database backend.\")\n\n        def fill_input_queue(in_queue, doc_loader, terminal_signal):\n            for doc in doc_loader:\n                in_queue.put(doc)\n            in_queue.put(terminal_signal)\n\n        # Create an input queue to feed documents to UDF workers\n        manager = Manager()\n        in_queue = manager.Queue()\n        # Use an output queue to track multiprocess progress\n        out_queue = JoinableQueue()\n\n        total_count = len(doc_loader)\n\n        # Start UDF Processes\n        for i in range(parallelism):\n            udf = self.udf_class(\n                in_queue=in_queue,\n                out_queue=out_queue,\n                worker_id=i,\n                **self.udf_init_kwargs,\n            )\n            udf.apply_kwargs = kwargs\n            self.udfs.append(udf)\n\n        # Start the UDF processes, and then join on their completion\n        for udf in self.udfs:\n            udf.start()\n\n        # Fill input queue with documents\n        terminal_signal = UDF.QUEUE_CLOSED\n        in_queue_filler = Process(\n            target=fill_input_queue, args=(in_queue, doc_loader, terminal_signal)\n        )\n        in_queue_filler.start()\n\n        count_parsed = 0\n        while count_parsed < total_count:\n            y = out_queue.get()\n            # Update progress bar whenever an item has been processed\n            if y == UDF.TASK_DONE:\n                count_parsed += 1\n                if self.pb is not None:\n                    self.pb.update(1)\n            else:\n                raise ValueError(\"Got non-sentinal output.\")\n\n        in_queue_filler.join()\n        in_queue.put(UDF.QUEUE_CLOSED)\n\n        for udf in self.udfs:\n            udf.join()\n\n        # Terminate and flush the processes\n        for udf in self.udfs:\n            udf.terminate()\n        self.udfs = []", "response": "Run the UDF multi - threaded using python multiprocessing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if all Mentions in the given candidate are from the same Document.", "response": "def same_document(c):\n    \"\"\"Return True if all Mentions in the given candidate are from the same Document.\n\n    :param c: The candidate whose Mentions are being compared\n    :rtype: boolean\n    \"\"\"\n    return all(\n        _to_span(c[i]).sentence.document is not None\n        and _to_span(c[i]).sentence.document == _to_span(c[0]).sentence.document\n        for i in range(len(c))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if all Mentions in the given candidate are from the same Table.", "response": "def same_table(c):\n    \"\"\"Return True if all Mentions in the given candidate are from the same Table.\n\n    :param c: The candidate whose Mentions are being compared\n    :rtype: boolean\n    \"\"\"\n    return all(\n        _to_span(c[i]).sentence.is_tabular()\n        and _to_span(c[i]).sentence.table == _to_span(c[0]).sentence.table\n        for i in range(len(c))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef same_row(c):\n    return same_table(c) and all(\n        is_row_aligned(_to_span(c[i]).sentence, _to_span(c[0]).sentence)\n        for i in range(len(c))\n    )", "response": "Return True if all Mentions in the given candidate are from the same Row."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if all Mentions in the given candidate are from the same Col.", "response": "def same_col(c):\n    \"\"\"Return True if all Mentions in the given candidate are from the same Col.\n\n    :param c: The candidate whose Mentions are being compared\n    :rtype: boolean\n    \"\"\"\n    return same_table(c) and all(\n        is_col_aligned(_to_span(c[i]).sentence, _to_span(c[0]).sentence)\n        for i in range(len(c))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_tabular_aligned(c):\n    return same_table(c) and (\n        is_col_aligned(_to_span(c[i]).sentence, _to_span(c[0]).sentence)\n        or is_row_aligned(_to_span(c[i]).sentence, _to_span(c[0]).sentence)\n        for i in range(len(c))\n    )", "response": "Return True if all Mentions in the given candidate are from the same Row or Col."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if all Mentions in the given candidate are from the same Cell.", "response": "def same_cell(c):\n    \"\"\"Return True if all Mentions in the given candidate are from the same Cell.\n\n    :param c: The candidate whose Mentions are being compared\n    :rtype: boolean\n    \"\"\"\n    return all(\n        _to_span(c[i]).sentence.cell is not None\n        and _to_span(c[i]).sentence.cell == _to_span(c[0]).sentence.cell\n        for i in range(len(c))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if all Mentions in the given candidate are from the same Sentence.", "response": "def same_sentence(c):\n    \"\"\"Return True if all Mentions in the given candidate are from the same Sentence.\n\n    :param c: The candidate whose Mentions are being compared\n    :rtype: boolean\n    \"\"\"\n    return all(\n        _to_span(c[i]).sentence is not None\n        and _to_span(c[i]).sentence == _to_span(c[0]).sentence\n        for i in range(len(c))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_max_col_num(mention):\n    span = _to_span(mention, idx=-1)\n    if span.sentence.is_tabular():\n        return span.sentence.cell.col_end\n    else:\n        return None", "response": "Returns the largest column number that a Mention occupies."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_min_col_num(mention):\n    span = _to_span(mention)\n    if span.sentence.is_tabular():\n        return span.sentence.cell.col_start\n    else:\n        return None", "response": "Returns the lowest column number that a Mention occupies."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the lowest row number that a Mention occupies.", "response": "def get_min_row_num(mention):\n    \"\"\"Return the lowest row number that a Mention occupies.\n\n    :param mention: The Mention to evaluate. If a candidate is given, default\n        to its first Mention.\n    :rtype: integer or None\n    \"\"\"\n    span = _to_span(mention)\n    if span.sentence.is_tabular():\n        return span.sentence.cell.row_start\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the ngrams that are in the Sentence of the given Mention.", "response": "def get_sentence_ngrams(mention, attrib=\"words\", n_min=1, n_max=1, lower=True):\n    \"\"\"Get the ngrams that are in the Sentence of the given Mention, not including itself.\n\n    Note that if a candidate is passed in, all of its Mentions will be\n    searched.\n\n    :param mention: The Mention whose Sentence is being searched\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param lower: If True, all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in get_left_ngrams(\n            span, window=100, attrib=attrib, n_min=n_min, n_max=n_max, lower=lower\n        ):\n            yield ngram\n        for ngram in get_right_ngrams(\n            span, window=100, attrib=attrib, n_min=n_min, n_max=n_max, lower=lower\n        ):\n            yield ngram"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_neighbor_sentence_ngrams(\n    mention, d=1, attrib=\"words\", n_min=1, n_max=1, lower=True\n):\n    \"\"\"Get the ngrams that are in the neighoring Sentences of the given Mention.\n\n    Note that if a candidate is passed in, all of its Mentions will be searched.\n\n    :param mention: The Mention whose neighbor Sentences are being searched\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param lower: If True, all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in chain.from_iterable(\n            [\n                tokens_to_ngrams(\n                    getattr(sentence, attrib), n_min=n_min, n_max=n_max, lower=lower\n                )\n                for sentence in span.sentence.document.sentences\n                if abs(sentence.position - span.sentence.position) <= d\n                and sentence != span.sentence\n            ]\n        ):\n            yield ngram", "response": "Returns the ngrams that are in the neighoring Sentences of the given Mention."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cell_ngrams(mention, attrib=\"words\", n_min=1, n_max=1, lower=True):\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in get_sentence_ngrams(\n            span, attrib=attrib, n_min=n_min, n_max=n_max, lower=lower\n        ):\n            yield ngram\n        if span.sentence.is_tabular():\n            for ngram in chain.from_iterable(\n                [\n                    tokens_to_ngrams(\n                        getattr(sentence, attrib), n_min=n_min, n_max=n_max, lower=lower\n                    )\n                    for sentence in _get_table_cells(span.sentence.table)[\n                        span.sentence.cell\n                    ]\n                    if sentence != span.sentence\n                ]\n            ):\n                yield ngram", "response": "Get the ngrams that are in the Cell of the given Mention."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_neighbor_cell_ngrams(\n    mention, dist=1, directions=False, attrib=\"words\", n_min=1, n_max=1, lower=True\n):\n    \"\"\"\n    Get the ngrams from all Cells that are within a given Cell distance in one\n    direction from the given Mention.\n\n    Note that if a candidate is passed in, all of its Mentions will be\n    searched. If `directions=True``, each ngram will be returned with a\n    direction in {'UP', 'DOWN', 'LEFT', 'RIGHT'}.\n\n    :param mention: The Mention whose neighbor Cells are being searched\n    :param dist: The Cell distance within which a neighbor Cell must be to be\n        considered\n    :param directions: A Boolean expressing whether or not to return the\n        direction of each ngram\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param lower: If True, all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams (or (ngram, direction) tuples if directions=True)\n    \"\"\"\n    # TODO: Fix this to be more efficient (optimize with SQL query)\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in get_sentence_ngrams(\n            span, attrib=attrib, n_min=n_min, n_max=n_max, lower=lower\n        ):\n            yield ngram\n        if span.sentence.is_tabular():\n            root_cell = span.sentence.cell\n            for sentence in chain.from_iterable(\n                [\n                    _get_aligned_sentences(root_cell, \"row\"),\n                    _get_aligned_sentences(root_cell, \"col\"),\n                ]\n            ):\n                row_diff = min_row_diff(sentence, root_cell, absolute=False)\n                col_diff = min_col_diff(sentence, root_cell, absolute=False)\n                if (\n                    (row_diff or col_diff)\n                    and not (row_diff and col_diff)\n                    and abs(row_diff) + abs(col_diff) <= dist\n                ):\n                    if directions:\n                        direction = \"\"\n                        if col_diff == 0:\n                            if 0 < row_diff and row_diff <= dist:\n                                direction = \"UP\"\n                            elif 0 > row_diff and row_diff >= -dist:\n                                direction = \"DOWN\"\n                        elif row_diff == 0:\n                            if 0 < col_diff and col_diff <= dist:\n                                direction = \"RIGHT\"\n                            elif 0 > col_diff and col_diff >= -dist:\n                                direction = \"LEFT\"\n                        for ngram in tokens_to_ngrams(\n                            getattr(sentence, attrib),\n                            n_min=n_min,\n                            n_max=n_max,\n                            lower=lower,\n                        ):\n                            yield (ngram, direction)\n                    else:\n                        for ngram in tokens_to_ngrams(\n                            getattr(sentence, attrib),\n                            n_min=n_min,\n                            n_max=n_max,\n                            lower=lower,\n                        ):\n                            yield ngram", "response": "Get the ngrams from all Cells that are within a given distance in one cell direction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the ngrams from all Cells that are in the same column as the given Mention.", "response": "def get_col_ngrams(\n    mention, attrib=\"words\", n_min=1, n_max=1, spread=[0, 0], lower=True\n):\n    \"\"\"Get the ngrams from all Cells that are in the same column as the given Mention.\n\n    Note that if a candidate is passed in, all of its Mentions will be searched.\n\n    :param mention: The Mention whose column Cells are being searched\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param spread: The number of cols left and right to also consider \"aligned\".\n    :param lower: If True, all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in _get_axis_ngrams(\n            span,\n            axis=\"col\",\n            attrib=attrib,\n            n_min=n_min,\n            n_max=n_max,\n            spread=spread,\n            lower=lower,\n        ):\n            yield ngram"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the ngrams from all Cells in the same row or column as the given Mention.", "response": "def get_aligned_ngrams(\n    mention, attrib=\"words\", n_min=1, n_max=1, spread=[0, 0], lower=True\n):\n    \"\"\"Get the ngrams from all Cells in the same row or column as the given Mention.\n\n    Note that if a candidate is passed in, all of its Mentions will be\n    searched.\n\n    :param mention: The Mention whose row and column Cells are being searched\n    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n    :param n_min: The minimum n of the ngrams that should be returned\n    :param n_max: The maximum n of the ngrams that should be returned\n    :param spread: The number of rows/cols above/below/left/right to also\n        consider \"aligned\".\n    :param lower: If True, all ngrams will be returned in lower case\n    :rtype: a *generator* of ngrams\n    \"\"\"\n    spans = _to_spans(mention)\n    for span in spans:\n        for ngram in get_row_ngrams(\n            span, attrib=attrib, n_min=n_min, n_max=n_max, spread=spread, lower=lower\n        ):\n            yield ngram\n        for ngram in get_col_ngrams(\n            span, attrib=attrib, n_min=n_min, n_max=n_max, spread=spread, lower=lower\n        ):\n            yield ngram"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_head_ngrams(mention, axis=None, attrib=\"words\", n_min=1, n_max=1, lower=True):\n    spans = _to_spans(mention)\n    axes = (axis,) if axis else (\"row\", \"col\")\n    for span in spans:\n        if span.sentence.is_tabular():\n            for axis in axes:\n                if getattr(span.sentence, _other_axis(axis) + \"_start\") == 0:\n                    return\n                for sentence in getattr(\n                    _get_head_cell(span.sentence.cell, axis), \"sentences\", []\n                ):\n                    for ngram in tokens_to_ngrams(\n                        getattr(sentence, attrib), n_min=n_min, n_max=n_max, lower=lower\n                    ):\n                        yield ngram", "response": "Get the ngrams from the head of the Mention."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forward(self, input, target):\n\n        n, k = input.shape\n        losses = input.new_zeros(n)\n\n        for i in range(k):\n            cls_idx = input.new_full((n,), i, dtype=torch.long)\n            loss = F.cross_entropy(input, cls_idx, reduction=\"none\")\n            if self.weight is not None:\n                loss = loss * self.weight[i]\n            losses += target[:, i].float() * loss\n\n        if self.reduction == \"mean\":\n            losses = losses.mean()\n        elif self.reduction == \"sum\":\n            losses = losses.sum()\n        elif self.reduction != \"none\":\n            raise ValueError(f\"Unrecognized reduction: {self.reduction}\")\n\n        return losses", "response": "Calculate the loss of the cluster classifiers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning true if the vertical center point of either span is within the vertical range of the other", "response": "def bbox_horz_aligned(box1, box2):\n    \"\"\"\n    Returns true if the vertical center point of either span is within the\n    vertical range of the other\n    \"\"\"\n    if not (box1 and box2):\n        return False\n    # NEW: any overlap counts\n    #    return box1.top <= box2.bottom and box2.top <= box1.bottom\n    box1_top = box1.top + 1.5\n    box2_top = box2.top + 1.5\n    box1_bottom = box1.bottom - 1.5\n    box2_bottom = box2.bottom - 1.5\n    return not (box1_top > box2_bottom or box2_top > box1_bottom)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bbox_vert_aligned(box1, box2):\n    if not (box1 and box2):\n        return False\n    # NEW: any overlap counts\n    #    return box1.left <= box2.right and box2.left <= box1.right\n    box1_left = box1.left + 1.5\n    box2_left = box2.left + 1.5\n    box1_right = box1.right - 1.5\n    box2_right = box2.right - 1.5\n    return not (box1_left > box2_right or box2_left > box1_right)", "response": "Returns true if the horizontal center point of either span is within the\n    horizontal range of the other\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the left boundary of both boxes is within 2 pts", "response": "def bbox_vert_aligned_left(box1, box2):\n    \"\"\"\n    Returns true if the left boundary of both boxes is within 2 pts\n    \"\"\"\n    if not (box1 and box2):\n        return False\n    return abs(box1.left - box2.left) <= 2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bbox_vert_aligned_right(box1, box2):\n    if not (box1 and box2):\n        return False\n    return abs(box1.right - box2.right) <= 2", "response": "Returns True if the right boundary of both boxes is within 2 pts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bbox_vert_aligned_center(box1, box2):\n    if not (box1 and box2):\n        return False\n    return abs(((box1.right + box1.left) / 2.0) - ((box2.right + box2.left) / 2.0)) <= 5", "response": "Returns true if the center of both boxes is within 5 pts"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nforward function. :param X: The input (batch) of the model contains word sequences for lstm, features and feature weights. :type X: For word sequences: a list of torch.Tensor pair (word sequence and word mask) of shape (batch_size, sequence_length). For features: torch.Tensor of shape (batch_size, sparse_feature_size). For feature weights: torch.Tensor of shape (batch_size, sparse_feature_size). :return: The output of LSTM layer. :rtype: torch.Tensor of shape (batch_size, num_classes)", "response": "def forward(self, X):\n        \"\"\"Forward function.\n\n        :param X: The input (batch) of the model contains word sequences for lstm,\n            features and feature weights.\n        :type X: For word sequences: a list of torch.Tensor pair (word sequence\n            and word mask) of shape (batch_size, sequence_length).\n            For features: torch.Tensor of shape (batch_size, sparse_feature_size).\n            For feature weights: torch.Tensor of shape\n            (batch_size, sparse_feature_size).\n        :return: The output of LSTM layer.\n        :rtype: torch.Tensor of shape (batch_size, num_classes)\n        \"\"\"\n\n        s = X[:-2]\n        f = X[-2]\n        w = X[-1]\n\n        batch_size = len(f)\n\n        # Generate lstm weight indices\n        x_idx = self._cuda(\n            torch.as_tensor(np.arange(1, self.settings[\"lstm_dim\"] + 1)).repeat(\n                batch_size, 1\n            )\n        )\n\n        outputs = self._cuda(torch.Tensor([]))\n\n        # Calculate textual features from LSTMs\n        for i in range(len(s)):\n            state_word = self.lstms[0].init_hidden(batch_size)\n            output = self.lstms[0].forward(s[i][0], s[i][1], state_word)\n            outputs = torch.cat((outputs, output), 1)\n\n        # Concatenate textual features with multi-modal features\n        feaures = torch.cat((x_idx, f), 1)\n        weights = torch.cat((outputs, w), 1)\n\n        return self.sparse_linear(feaures, weights)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _preprocess_data(self, X, Y=None, idxs=None, train=False):\n\n        C, F = X\n\n        if Y is not None:\n            Y = np.array(Y).astype(np.float32)\n\n        # Create word dictionary for LSTM\n        if not hasattr(self, \"word_dict\"):\n            self.word_dict = SymbolTable()\n            arity = len(C[0])\n            # Add paddings into word dictionary\n            for i in range(arity):\n                list(map(self.word_dict.get, [\"~~[[\" + str(i), str(i) + \"]]~~\"]))\n\n        # Make sequence input for LSTM from candidates\n        seq_data = []\n        for candidate in C:\n            cand_idx = []\n            for i in range(len(candidate)):\n                # Add mark for each mention in the original sentence\n                args = [\n                    (\n                        candidate[i].context.get_word_start_index(),\n                        candidate[i].context.get_word_end_index(),\n                        i,\n                    )\n                ]\n                s = mark_sentence(mention_to_tokens(candidate[i]), args)\n                f = self.word_dict.get if train else self.word_dict.lookup\n                cand_idx.append(list(map(f, s)))\n            seq_data.append(cand_idx)\n\n        # Generate proprcessed the input\n        if idxs is None:\n            if Y is not None:\n                return (\n                    [\n                        [\n                            seq_data[i],\n                            F.indices[F.indptr[i] : F.indptr[i + 1]],\n                            F.data[F.indptr[i] : F.indptr[i + 1]],\n                        ]\n                        for i in range(len(C))\n                    ],\n                    Y,\n                )\n            else:\n                return [\n                    [\n                        seq_data[i],\n                        F.indices[F.indptr[i] : F.indptr[i + 1]],\n                        F.data[F.indptr[i] : F.indptr[i + 1]],\n                    ]\n                    for i in range(len(C))\n                ]\n        if Y is not None:\n            return (\n                [\n                    [\n                        seq_data[i],\n                        F.indices[F.indptr[i] : F.indptr[i + 1]],\n                        F.data[F.indptr[i] : F.indptr[i + 1]],\n                    ]\n                    for i in idxs\n                ],\n                Y[idxs],\n            )\n        else:\n            return [\n                [\n                    seq_data[i],\n                    F.indices[F.indptr[i] : F.indptr[i + 1]],\n                    F.data[F.indptr[i] : F.indptr[i + 1]],\n                ]\n                for i in idxs\n            ]", "response": "Preprocess the data for the LSTM."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a list of tuples and returns a list of torch. Tensor with the data.", "response": "def _collate(self, batch):\n        \"\"\"\n        Puts each data field into a tensor.\n\n        :param batch: The input data batch.\n        :type batch: list of (word sequences, features, feature_weights) tuples\n        :return: Preprocessed data.\n        :rtype: list of torch.Tensor with torch.Tensor (Optional)\n        \"\"\"\n\n        Y_batch = None\n        if isinstance(batch[0], tuple):\n            batch, Y_batch = list(zip(*batch))\n            Y_batch = self._cuda(torch.Tensor(Y_batch))\n\n        batch, f_batch, v_batch = list(zip(*batch))\n\n        f_batch, _ = pad_batch(f_batch, 0)\n        v_batch, _ = pad_batch(v_batch, 0, type=\"float\")\n\n        f_batch = self._cuda(f_batch)\n        v_batch = self._cuda(v_batch)\n\n        X_batch = []\n\n        for samples in list(zip(*batch)):\n            x, x_mask = pad_batch(samples, max_len=self.settings[\"max_sentence_length\"])\n            X_batch.append((self._cuda(x), self._cuda(x_mask)))\n        X_batch.extend([f_batch, v_batch])\n\n        if Y_batch is not None:\n            return X_batch, Y_batch\n        else:\n            return X_batch"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_settings(self, X):\n\n        self.logger.info(\"Loading default parameters for Sparse LSTM\")\n        config = get_config()[\"learning\"][\"SparseLSTM\"]\n\n        for key in config.keys():\n            if key not in self.settings:\n                self.settings[key] = config[key]\n\n        self.settings[\"relation_arity\"] = len(X[0][0])\n        self.settings[\"lstm_dim\"] = (\n            len(X[0][0])\n            * self.settings[\"hidden_dim\"]\n            * (2 if self.settings[\"bidirectional\"] else 1)\n        )\n\n        # Add one feature for padding vector (all 0s)\n        self.settings[\"input_dim\"] = (\n            X[1].shape[1]\n            + len(X[0][0])\n            * self.settings[\"hidden_dim\"]\n            * (2 if self.settings[\"bidirectional\"] else 1)\n            + 1\n        )", "response": "Update the model argument."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_opts(self):\n        for opt in self.opts.keys():\n            if opt not in self.__dict__:\n                raise Exception(f\"Unsupported option: {opt}\")", "response": "Checks for unsupported opts raises error"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef f(self, m):\n        if len(self.children) == 0:\n            return self._f(m)\n        elif len(self.children) == 1:\n            return self._f(m) and self.children[0].f(m)\n        else:\n            raise Exception(\n                f\"{self.__name__} does not support more than one child Matcher\"\n            )", "response": "Returns the recursively composed version of filter function f."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the Matcher to a generator of mentions. Optionally only takes the longest match.", "response": "def apply(self, mentions):\n        \"\"\"\n        Apply the Matcher to a **generator** of mentions.\n        Optionally only takes the longest match (NOTE: assumes this is the\n        *first* match)\n        \"\"\"\n        seen_spans = set()\n        for m in mentions:\n            if self.f(m) and (\n                not self.longest_match_only\n                or not any([self._is_subspan(m, s) for s in seen_spans])\n            ):\n                if self.longest_match_only:\n                    seen_spans.add(self._get_span(m))\n                yield m"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_subspan(self, m, span):\n        return (\n            m.sentence.id == span[0]\n            and m.char_start >= span[1]\n            and m.char_end <= span[2]\n        )", "response": "Tests if mention m is subspan of span."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a tuple that identifies a span for the specific mention class that m belongs to.", "response": "def _get_span(self, m):\n        \"\"\"\n        Gets a tuple that identifies a span for the specific mention class\n        that m belongs to.\n        \"\"\"\n        return (m.sentence.id, m.char_start, m.char_end)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests if mention m exists and is a subspan of span", "response": "def _is_subspan(self, m, span):\n        \"\"\"Tests if mention m does exist\"\"\"\n        return m.figure.document.id == span[0] and m.figure.position == span[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_span(self, m):\n        return (m.figure.document.id, m.figure.position)", "response": "Returns a tuple that identifies a figure for the specific mention class\n        that m belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nforward function. :param X: The input (batch) of the model contains word sequences for lstm and features. :type X: For word sequences: a list of torch.Tensor pair (word sequence and word mask) of shape (batch_size, sequence_length). For features: torch.Tensor of shape (batch_size, feature_size). :return: The output of LSTM layer. :rtype: torch.Tensor of shape (batch_size, num_classes)", "response": "def forward(self, X):\n        \"\"\"Forward function.\n\n        :param X: The input (batch) of the model contains word sequences for lstm\n            and features.\n        :type X: For word sequences: a list of torch.Tensor pair (word sequence\n            and word mask) of shape (batch_size, sequence_length).\n            For features: torch.Tensor of shape (batch_size, feature_size).\n        :return: The output of LSTM layer.\n        :rtype: torch.Tensor of shape (batch_size, num_classes)\n        \"\"\"\n\n        s = X[:-1]\n        f = X[-1]\n\n        batch_size = len(f)\n\n        outputs = self._cuda(torch.Tensor([]))\n\n        # Calculate textual features from LSTMs\n        for i in range(len(s)):\n            state_word = self.lstms[0].init_hidden(batch_size)\n            output = self.lstms[0].forward(s[i][0], s[i][1], state_word)\n            outputs = torch.cat((outputs, output), 1)\n\n        # Concatenate textual features with multi-modal features\n        outputs = torch.cat((outputs, f), 1)\n\n        return self.linear(outputs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and returns a Candidate subclass with provided argument names table_name and cardinality.", "response": "def candidate_subclass(\n    class_name, args, table_name=None, cardinality=None, values=None\n):\n    \"\"\"\n    Creates and returns a Candidate subclass with provided argument names,\n    which are Context type. Creates the table in DB if does not exist yet.\n\n    Import using:\n\n    .. code-block:: python\n\n        from fonduer.candidates.models import candidate_subclass\n\n    :param class_name: The name of the class, should be \"camel case\" e.g.\n        NewCandidate\n    :param args: A list of names of constituent arguments, which refer to the\n        Contexts--representing mentions--that comprise the candidate\n    :param table_name: The name of the corresponding table in DB; if not\n        provided, is converted from camel case by default, e.g. new_candidate\n    :param cardinality: The cardinality of the variable corresponding to the\n        Candidate. By default is 2 i.e. is a binary value, e.g. is or is not\n        a true mention.\n    \"\"\"\n    if table_name is None:\n        table_name = camel_to_under(class_name)\n\n    # If cardinality and values are None, default to binary classification\n    if cardinality is None and values is None:\n        values = [True, False]\n        cardinality = 2\n\n    # Else use values if present, and validate proper input\n    elif values is not None:\n        if cardinality is not None and len(values) != cardinality:\n            raise ValueError(\"Number of values must match cardinality.\")\n        if None in values:\n            raise ValueError(\"`None` is a protected value.\")\n        # Note that bools are instances of ints in Python...\n        if any([isinstance(v, int) and not isinstance(v, bool) for v in values]):\n            raise ValueError(\n                (\n                    \"Default usage of values is consecutive integers.\"\n                    \"Leave values unset if trying to define values as integers.\"\n                )\n            )\n        cardinality = len(values)\n\n    # If cardinality is specified but not values, fill in with ints\n    elif cardinality is not None:\n        values = list(range(cardinality))\n\n    class_spec = (args, table_name, cardinality, values)\n    if class_name in candidate_subclasses:\n        if class_spec == candidate_subclasses[class_name][1]:\n            return candidate_subclasses[class_name][0]\n        else:\n            raise ValueError(\n                f\"Candidate subclass {class_name} \"\n                f\"already exists in memory with incompatible \"\n                f\"specification: {candidate_subclasses[class_name][1]}\"\n            )\n    else:\n        # Set the class attributes == the columns in the database\n        class_attribs = {\n            # Declares name for storage table\n            \"__tablename__\": table_name,\n            # Connects candidate_subclass records to generic Candidate records\n            \"id\": Column(\n                Integer,\n                ForeignKey(\"candidate.id\", ondelete=\"CASCADE\"),\n                primary_key=True,\n            ),\n            # Store values & cardinality information in the class only\n            \"values\": values,\n            \"cardinality\": cardinality,\n            # Polymorphism information for SQLAlchemy\n            \"__mapper_args__\": {\"polymorphic_identity\": table_name},\n            # Helper method to get argument names\n            \"__argnames__\": [_.__tablename__ for _ in args],\n            \"mentions\": args,\n        }\n        class_attribs[\"document_id\"] = Column(\n            Integer, ForeignKey(\"document.id\", ondelete=\"CASCADE\")\n        )\n        class_attribs[\"document\"] = relationship(\n            \"Document\",\n            backref=backref(table_name + \"s\", cascade=\"all, delete-orphan\"),\n            foreign_keys=class_attribs[\"document_id\"],\n        )\n\n        # Create named arguments, i.e. the entity mentions comprising the\n        # relation mention.\n        unique_args = []\n        for arg in args:\n            # Primary arguments are constituent Contexts, and their ids\n            class_attribs[arg.__tablename__ + \"_id\"] = Column(\n                Integer, ForeignKey(arg.__tablename__ + \".id\", ondelete=\"CASCADE\")\n            )\n            class_attribs[arg.__tablename__] = relationship(\n                arg.__name__,\n                backref=backref(\n                    table_name + \"_\" + arg.__tablename__ + \"s\",\n                    cascade_backrefs=False,\n                    cascade=\"all, delete-orphan\",\n                ),\n                cascade_backrefs=False,\n                foreign_keys=class_attribs[arg.__tablename__ + \"_id\"],\n            )\n            unique_args.append(class_attribs[arg.__tablename__ + \"_id\"])\n\n        # Add unique constraints to the arguments\n        class_attribs[\"__table_args__\"] = (UniqueConstraint(*unique_args),)\n\n        # Create class\n        C = type(class_name, (Candidate,), class_attribs)\n\n        # Create table in DB\n        if not Meta.engine.dialect.has_table(Meta.engine, table_name):\n            C.__table__.create(bind=Meta.engine)\n\n        candidate_subclasses[class_name] = C, class_spec\n\n        return C"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_stable_id(self):\n        doc_id, _, parent_doc_char_start, _ = split_stable_id(self.sentence.stable_id)\n        return (\n            f\"{self.sentence.document.name}\"\n            f\"::\"\n            f\"{self._get_polymorphic_identity()}\"\n            f\":\"\n            f\"{parent_doc_char_start + self.char_start}\"\n            f\":\"\n            f\"{parent_doc_char_start + self.char_end}\"\n            f\":\"\n            f\"{self.expander_key}\"\n            f\":\"\n            f\"{self.position}\"\n        )", "response": "Return a stable id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_attrib_span(self, a, sep=\" \"):\n        if a == \"words\":\n            return self.text\n        else:\n            return sep.join(self.get_attrib_tokens(a))", "response": "Get the span of the sentence attribute a."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the CandidateExtractor. :Example: To extract candidates from a set of training documents using 4 cores:: candidate_extractor.apply(train_docs, split=0, parallelism=4) :param docs: Set of documents to extract from. :param split: Which split to assign the extracted Candidates to. :type split: int :param clear: Whether or not to clear the existing Candidates beforehand. :type clear: bool :param parallelism: How many threads to use for extraction. This will override the parallelism value used to initialize the CandidateExtractor if it is provided. :type parallelism: int :param progress_bar: Whether or not to display a progress bar. The progress bar is measured per document. :type progress_bar: bool", "response": "def apply(self, docs, split=0, clear=True, parallelism=None, progress_bar=True):\n        \"\"\"Run the CandidateExtractor.\n\n        :Example: To extract candidates from a set of training documents using\n            4 cores::\n\n                candidate_extractor.apply(train_docs, split=0, parallelism=4)\n\n        :param docs: Set of documents to extract from.\n        :param split: Which split to assign the extracted Candidates to.\n        :type split: int\n        :param clear: Whether or not to clear the existing Candidates\n            beforehand.\n        :type clear: bool\n        :param parallelism: How many threads to use for extraction. This will\n            override the parallelism value used to initialize the\n            CandidateExtractor if it is provided.\n        :type parallelism: int\n        :param progress_bar: Whether or not to display a progress bar. The\n            progress bar is measured per document.\n        :type progress_bar: bool\n        \"\"\"\n        super(CandidateExtractor, self).apply(\n            docs,\n            split=split,\n            clear=clear,\n            parallelism=parallelism,\n            progress_bar=progress_bar,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self, split):\n        for candidate_class in self.candidate_classes:\n            logger.info(\n                f\"Clearing table {candidate_class.__tablename__} (split {split})\"\n            )\n            self.session.query(Candidate).filter(\n                Candidate.type == candidate_class.__tablename__\n            ).filter(Candidate.split == split).delete(synchronize_session=\"fetch\")", "response": "Delete Candidates of each class initialized with the\n        CandidateExtractor from given split the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_all(self, split):\n        logger.info(\"Clearing ALL Candidates.\")\n        self.session.query(Candidate).filter(Candidate.split == split).delete(\n            synchronize_session=\"fetch\"\n        )", "response": "Delete ALL Candidates from given split the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_candidates(self, docs=None, split=0, sort=False):\n        result = []\n        if docs:\n            docs = docs if isinstance(docs, (list, tuple)) else [docs]\n            # Get cands from all splits\n            for candidate_class in self.candidate_classes:\n                cands = (\n                    self.session.query(candidate_class)\n                    .filter(candidate_class.document_id.in_([doc.id for doc in docs]))\n                    .order_by(candidate_class.id)\n                    .all()\n                )\n                if sort:\n                    cands = sorted(\n                        cands,\n                        key=lambda x: \" \".join(\n                            [x[i][0].get_stable_id() for i in range(len(x))]\n                        ),\n                    )\n                result.append(cands)\n        else:\n            for candidate_class in self.candidate_classes:\n                # Filter by candidate_ids in a particular split\n                sub_query = (\n                    self.session.query(Candidate.id)\n                    .filter(Candidate.split == split)\n                    .subquery()\n                )\n                cands = (\n                    self.session.query(candidate_class)\n                    .filter(candidate_class.id.in_(sub_query))\n                    .order_by(candidate_class.id)\n                    .all()\n                )\n                if sort:\n                    cands = sorted(\n                        cands,\n                        key=lambda x: \" \".join(\n                            [x[i][0].get_stable_id() for i in range(len(x))]\n                        ),\n                    )\n                result.append(cands)\n        return result", "response": "Return a list of the candidates associated with this CandidateExtractor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply(self, context, clear, split, **kwargs):\n        logger.debug(f\"Document: {context}\")\n        # Iterate over each candidate class\n        for i, candidate_class in enumerate(self.candidate_classes):\n            logger.debug(f\"  Relation: {candidate_class.__name__}\")\n            # Generates and persists candidates\n            candidate_args = {\"split\": split}\n            candidate_args[\"document_id\"] = context.id\n            cands = product(\n                *[\n                    enumerate(\n                        self.session.query(mention)\n                        .filter(mention.document_id == context.id)\n                        .order_by(mention.id)\n                        .all()\n                    )\n                    for mention in candidate_class.mentions\n                ]\n            )\n            for cand in cands:\n\n                # Apply throttler if one was given.\n                # Accepts a tuple of Mention objects\n                # (throttler returns whether or not proposed candidate\n                # passes throttling condition)\n                if self.throttlers[i]:\n                    if not self.throttlers[i](\n                        tuple(cand[j][1] for j in range(self.arities[i]))\n                    ):\n                        continue\n\n                # TODO: Make this work for higher-order relations\n                if self.arities[i] == 2:\n                    ai, a = (cand[0][0], cand[0][1].context)\n                    bi, b = (cand[1][0], cand[1][1].context)\n\n                    # Check for self-joins, \"nested\" joins (joins from context to\n                    # its subcontext), and flipped duplicate \"symmetric\" relations\n                    if not self.self_relations and a == b:\n                        logger.debug(f\"Skipping self-joined candidate {cand}\")\n                        continue\n                    if not self.nested_relations and (a in b or b in a):\n                        logger.debug(f\"Skipping nested candidate {cand}\")\n                        continue\n                    if not self.symmetric_relations and ai > bi:\n                        logger.debug(f\"Skipping symmetric candidate {cand}\")\n                        continue\n\n                # Assemble candidate arguments\n                for j, arg_name in enumerate(candidate_class.__argnames__):\n                    candidate_args[arg_name + \"_id\"] = cand[j][1].id\n\n                # Checking for existence\n                if not clear:\n                    q = select([candidate_class.id])\n                    for key, value in list(candidate_args.items()):\n                        q = q.where(getattr(candidate_class, key) == value)\n                    candidate_id = self.session.execute(q).first()\n                    if candidate_id is not None:\n                        continue\n\n                # Add Candidate to session\n                yield candidate_class(**candidate_args)", "response": "Extract candidates from the given Context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward(self, x, w):\n\n        if self.bias is None:\n            return (w.unsqueeze(2) * self.weight(x)).sum(dim=1)\n        else:\n            return (w.unsqueeze(2) * self.weight(x)).sum(dim=1) + self.bias", "response": "Forward function.\n\n        :param x: Feature indices.\n        :type x: torch.Tensor of shape (batch_size * length)\n        :param w: Feature weights.\n        :type w: torch.Tensor of shape (batch_size * length)\n        :return: Output of linear layer.\n        :rtype: torch.Tensor of shape (batch_size, num_classes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the marginal probabilities for a set of Candidates to db.", "response": "def save_marginals(session, X, marginals, training=True):\n    \"\"\"Save marginal probabilities for a set of Candidates to db.\n\n    :param X: A list of arbitrary objects with candidate ids accessible via a\n        .id attrib\n    :param marginals: A dense M x K matrix of marginal probabilities, where\n        K is the cardinality of the candidates, OR a M-dim list/array if K=2.\n    :param training: If True, these are training marginals / labels; else they\n        are saved as end model predictions.\n\n    Note: The marginals for k=0 are not stored, only for k = 1,...,K\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    # Make sure that we are working with a numpy array\n    try:\n        shape = marginals.shape\n    except Exception:\n        marginals = np.array(marginals)\n        shape = marginals.shape\n\n    # Handle binary input as M x 1-dim array; assume elements represent\n    # poksitive (k=1) class values\n    if len(shape) == 1:\n        marginals = np.vstack([1 - marginals, marginals]).T\n\n    # Only add values for classes k=1,...,K\n    marginal_tuples = []\n    for i in range(shape[0]):\n        for k in range(1, shape[1] if len(shape) > 1 else 2):\n            if marginals[i, k] > 0:\n                marginal_tuples.append((i, k, marginals[i, k]))\n\n    # NOTE: This will delete all existing marginals of type `training`\n    session.query(Marginal).filter(Marginal.training == training).delete(\n        synchronize_session=\"fetch\"\n    )\n\n    # Prepare bulk INSERT query\n    q = Marginal.__table__.insert()\n\n    # Prepare values\n    insert_vals = []\n    for i, k, p in marginal_tuples:\n        cid = X[i].id\n        insert_vals.append(\n            {\n                \"candidate_id\": cid,\n                \"training\": training,\n                \"value\": k,\n                # We cast p in case its a numpy type, which psycopg2 does not handle\n                \"probability\": float(p),\n            }\n        )\n\n    # Execute update\n    session.execute(q, insert_vals)\n    session.commit()\n    logger.info(f\"Saved {len(marginals)} marginals\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a generator function which accepts an xml root and a list of indexes for a mention and will generate relation features for this entity.", "response": "def compile_entity_feature_generator():\n    \"\"\"\n    Given optional arguments, returns a generator function which accepts an xml\n    root and a list of indexes for a mention, and will generate relation\n    features for this entity.\n    \"\"\"\n\n    BASIC_ATTRIBS_REL = [\"lemma\", \"dep_label\"]\n\n    m = Mention(0)\n\n    # Basic relation feature templates\n    temps = [\n        [Indicator(m, a) for a in BASIC_ATTRIBS_REL],\n        Indicator(m, \"dep_label,lemma\"),\n        # The *first element on the* path to the root: ngram lemmas along it\n        Ngrams(Parents(m, 3), \"lemma\", (1, 3)),\n        Ngrams(Children(m), \"lemma\", (1, 3)),\n        # The siblings of the mention\n        [LeftNgrams(LeftSiblings(m), a) for a in BASIC_ATTRIBS_REL],\n        [RightNgrams(RightSiblings(m), a) for a in BASIC_ATTRIBS_REL],\n    ]\n\n    # return generator function\n    return Compile(temps).apply_mention"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ddlib_feats(span, context, idxs):\n\n    if span.stable_id not in unary_ddlib_feats:\n        unary_ddlib_feats[span.stable_id] = set()\n\n        for seq_feat in _get_seq_features(context, idxs):\n            unary_ddlib_feats[span.stable_id].add(seq_feat)\n\n        for window_feat in _get_window_features(context, idxs):\n            unary_ddlib_feats[span.stable_id].add(window_feat)\n\n    for f in unary_ddlib_feats[span.stable_id]:\n        yield f", "response": "Get the features from the ddlib file for a given span."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the corresponding values for the key_table.", "response": "def _get_cand_values(candidate, key_table):\n    \"\"\"Get the corresponding values for the key_table.\"\"\"\n    # NOTE: Import just before checking to avoid circular imports.\n    from fonduer.features.models import FeatureKey\n    from fonduer.supervision.models import GoldLabelKey, LabelKey\n\n    if key_table == FeatureKey:\n        return candidate.features\n    elif key_table == LabelKey:\n        return candidate.labels\n    elif key_table == GoldLabelKey:\n        return candidate.gold_labels\n    else:\n        raise ValueError(f\"{key_table} is not a valid key table.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _batch_postgres_query(table, records):\n    if not records:\n        return\n\n    POSTGRESQL_MAX = 0x3FFFFFFF\n\n    # Create preamble and measure its length\n    preamble = (\n        \"INSERT INTO \"\n        + table.__tablename__\n        + \" (\"\n        + \", \".join(records[0].keys())\n        + \") VALUES (\"\n        + \", \".join([\"?\"] * len(records[0].keys()))\n        + \")\\n\"\n    )\n    start = 0\n    end = 0\n    total_len = len(preamble)\n    while end < len(records):\n        record_len = sum([len(str(v)) for v in records[end].values()])\n\n        # Pre-increment to include the end element in the slice\n        end += 1\n\n        if total_len + record_len >= POSTGRESQL_MAX:\n            logger.debug(f\"Splitting query due to length ({total_len} chars).\")\n            yield records[start:end]\n            start = end\n            # Reset the total query length\n            total_len = len(preamble)\n        else:\n            total_len += record_len\n\n    yield records[start:end]", "response": "Batch the query for the given table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sparse_matrix_keys(session, key_table):\n    return session.query(key_table).order_by(key_table.name).all()", "response": "Return a list of keys for the sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbatches upsert records into postgresql database.", "response": "def batch_upsert_records(session, table, records):\n    \"\"\"Batch upsert records into postgresql database.\"\"\"\n    if not records:\n        return\n    for record_batch in _batch_postgres_query(table, records):\n        stmt = insert(table.__table__)\n        stmt = stmt.on_conflict_do_update(\n            constraint=table.__table__.primary_key,\n            set_={\n                \"keys\": stmt.excluded.get(\"keys\"),\n                \"values\": stmt.excluded.get(\"values\"),\n            },\n        )\n        session.execute(stmt, record_batch)\n        session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload sparse matrix of GoldLabels for each candidate_class.", "response": "def get_sparse_matrix(session, key_table, cand_lists, key=None):\n    \"\"\"Load sparse matrix of GoldLabels for each candidate_class.\"\"\"\n    result = []\n    cand_lists = cand_lists if isinstance(cand_lists, (list, tuple)) else [cand_lists]\n\n    for cand_list in cand_lists:\n        if len(cand_list) == 0:\n            raise ValueError(\"cand_lists contain empty cand_list.\")\n        candidate_class = cand_list[0].__tablename__\n\n        # Keys are used as a global index\n        if key:\n            keys_map = {key: 0}\n            key_size = len(keys_map)\n        else:\n            all_keys = get_sparse_matrix_keys(session, key_table)\n            key_size = len(all_keys)\n            keys_map = {}\n            for (i, k) in enumerate(all_keys):\n                if candidate_class in k.candidate_classes:\n                    keys_map[k.name] = i\n\n        indptr = [0]\n        indices = []\n        data = []\n        for cand in cand_list:\n            values = _get_cand_values(cand, key_table)\n            if values:\n                for cand_key, cand_value in zip(values[0].keys, values[0].values):\n                    if cand_key in keys_map:\n                        indices.append(keys_map[cand_key])\n                        data.append(cand_value)\n\n            indptr.append(len(indices))\n\n        result.append(\n            csr_matrix((data, indices, indptr), shape=(len(cand_list), key_size))\n        )\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_docs_from_split(session, candidate_classes, split):\n    # Only grab the docs containing candidates from the given split.\n    sub_query = session.query(Candidate.id).filter(Candidate.split == split).subquery()\n    split_docs = set()\n    for candidate_class in candidate_classes:\n        split_docs.update(\n            cand.document\n            for cand in session.query(candidate_class)\n            .filter(candidate_class.id.in_(sub_query))\n            .all()\n        )\n    return split_docs", "response": "Return a list of documents that contain the candidates in the split."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a mapping of keys and values for each candidate in the database table.", "response": "def get_mapping(session, table, candidates, generator, key_map):\n    \"\"\"Generate map of keys and values for the candidate from the generator.\n\n    :param session: The database session.\n    :param table: The table we will be inserting into (i.e. Feature or Label).\n    :param candidates: The candidates to get mappings for.\n    :param generator: A generator yielding (candidate_id, key, value) tuples.\n    :param key_map: A mutable dict which values will be added to as {key:\n        [relations]}.\n    :type key_map: Dict\n    :return: Generator of dictionaries of {\"candidate_id\": _, \"keys\": _, \"values\": _}\n    :rtype: generator of dict\n    \"\"\"\n    for cand in candidates:\n        # Grab the old values currently in the DB\n        try:\n            temp = session.query(table).filter(table.candidate_id == cand.id).one()\n            cand_map = dict(zip(temp.keys, temp.values))\n        except NoResultFound:\n            cand_map = {}\n\n        map_args = {\"candidate_id\": cand.id}\n        for cid, key, value in generator(cand):\n            if value == 0:\n                continue\n            cand_map[key] = value\n\n        # Assemble label arguments\n        map_args[\"keys\"] = [*cand_map.keys()]\n        map_args[\"values\"] = [*cand_map.values()]\n\n        # Update key_map by adding the candidate class for each key\n        for key in map_args[\"keys\"]:\n            try:\n                key_map[key].add(cand.__class__.__tablename__)\n            except KeyError:\n                key_map[key] = {cand.__class__.__tablename__}\n        yield map_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cands_list_from_split(session, candidate_classes, doc, split):\n    cands = []\n    if split == ALL_SPLITS:\n        # Get cands from all splits\n        for candidate_class in candidate_classes:\n            cands.append(\n                session.query(candidate_class)\n                .filter(candidate_class.document_id == doc.id)\n                .all()\n            )\n    else:\n        # Get cands from the specified split\n        for candidate_class in candidate_classes:\n            cands.append(\n                session.query(candidate_class)\n                .filter(candidate_class.document_id == doc.id)\n                .filter(candidate_class.split == split)\n                .all()\n            )\n    return cands", "response": "Return the list of list of candidates from this document based on the split."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_all_keys(session, key_table, candidate_classes):\n    if not candidate_classes:\n        return\n\n    candidate_classes = set([c.__tablename__ for c in candidate_classes])\n\n    # Select all rows that contain ANY of the candidate_classes\n    all_rows = (\n        session.query(key_table)\n        .filter(\n            key_table.candidate_classes.overlap(cast(candidate_classes, ARRAY(String)))\n        )\n        .all()\n    )\n    to_delete = set()\n    to_update = []\n\n    # All candidate classes will be the same for all keys, so just look at one\n    for row in all_rows:\n        # Remove the selected candidate_classes. If empty, mark for deletion.\n        row.candidate_classes = list(\n            set(row.candidate_classes) - set(candidate_classes)\n        )\n        if len(row.candidate_classes) == 0:\n            to_delete.add(row.name)\n        else:\n            to_update.append(\n                {\"name\": row.name, \"candidate_classes\": row.candidate_classes}\n            )\n\n    # Perform all deletes\n    if to_delete:\n        query = session.query(key_table).filter(key_table.name.in_(to_delete))\n        query.delete(synchronize_session=\"fetch\")\n\n    # Perform all updates\n    if to_update:\n        for batch in _batch_postgres_query(key_table, to_update):\n            stmt = insert(key_table.__table__)\n            stmt = stmt.on_conflict_do_update(\n                constraint=key_table.__table__.primary_key,\n                set_={\n                    \"name\": stmt.excluded.get(\"name\"),\n                    \"candidate_classes\": stmt.excluded.get(\"candidate_classes\"),\n                },\n            )\n            session.execute(stmt, batch)\n            session.commit()", "response": "Bulk drop annotation keys for all the candidate_classes in the table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_keys(session, key_table, keys):\n    # Do nothing if empty\n    if not keys:\n        return\n\n    for key_batch in _batch_postgres_query(\n        key_table, [{\"name\": k[0], \"candidate_classes\": k[1]} for k in keys.items()]\n    ):\n        all_rows = (\n            session.query(key_table)\n            .filter(key_table.name.in_([key[\"name\"] for key in key_batch]))\n            .all()\n        )\n\n        to_delete = set()\n        to_update = []\n\n        # All candidate classes will be the same for all keys, so just look at one\n        candidate_classes = key_batch[0][\"candidate_classes\"]\n        for row in all_rows:\n            # Remove the selected candidate_classes. If empty, mark for deletion.\n            row.candidate_classes = list(\n                set(row.candidate_classes) - set(candidate_classes)\n            )\n            if len(row.candidate_classes) == 0:\n                to_delete.add(row.name)\n            else:\n                to_update.append(\n                    {\"name\": row.name, \"candidate_classes\": row.candidate_classes}\n                )\n\n        # Perform all deletes\n        if to_delete:\n            query = session.query(key_table).filter(key_table.name.in_(to_delete))\n            query.delete(synchronize_session=\"fetch\")\n\n        # Perform all updates\n        if to_update:\n            stmt = insert(key_table.__table__)\n            stmt = stmt.on_conflict_do_update(\n                constraint=key_table.__table__.primary_key,\n                set_={\n                    \"name\": stmt.excluded.get(\"name\"),\n                    \"candidate_classes\": stmt.excluded.get(\"candidate_classes\"),\n                },\n            )\n            session.execute(stmt, to_update)\n            session.commit()", "response": "Bulk drop annotation keys to the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upsert_keys(session, key_table, keys):\n    # Do nothing if empty\n    if not keys:\n        return\n\n    for key_batch in _batch_postgres_query(\n        key_table, [{\"name\": k[0], \"candidate_classes\": k[1]} for k in keys.items()]\n    ):\n        stmt = insert(key_table.__table__)\n        stmt = stmt.on_conflict_do_update(\n            constraint=key_table.__table__.primary_key,\n            set_={\n                \"name\": stmt.excluded.get(\"name\"),\n                \"candidate_classes\": stmt.excluded.get(\"candidate_classes\"),\n            },\n        )\n        while True:\n            try:\n                session.execute(stmt, key_batch)\n                session.commit()\n                break\n            except Exception as e:\n                logger.debug(e)", "response": "Bulk add annotation keys to the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a pdf file into an image object.", "response": "def pdf_to_img(pdf_file, page_num, pdf_dim=None):\n    \"\"\"\n    Converts pdf file into image\n    :param pdf_file: path to the pdf file\n    :param page_num: page number to convert (index starting at 1)\n    :return: wand image object\n    \"\"\"\n    if not pdf_dim:\n        pdf_dim = get_pdf_dim(pdf_file)\n    page_width, page_height = pdf_dim\n    img = Image(filename=f\"{pdf_file}[{page_num - 1}]\")\n    img.resize(page_width, page_height)\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying each of the bounding boxes passed in boxes on images of the pdf_file.", "response": "def display_boxes(self, pdf_file, boxes, alternate_colors=False):\n        \"\"\"\n        Displays each of the bounding boxes passed in 'boxes' on images of the pdf\n        pointed to by pdf_file\n        boxes is a list of 5-tuples (page, top, left, bottom, right)\n        \"\"\"\n        imgs = []\n        colors = [Color(\"blue\"), Color(\"red\")]\n        boxes_per_page = defaultdict(int)\n        boxes_by_page = defaultdict(list)\n        for i, (page, top, left, bottom, right) in enumerate(boxes):\n            boxes_per_page[page] += 1\n            boxes_by_page[page].append((top, left, bottom, right))\n        for i, page_num in enumerate(boxes_per_page.keys()):\n            img = pdf_to_img(pdf_file, page_num)\n            draw = Drawing()\n            draw.fill_color = Color(\"rgba(0, 0, 0, 0.0)\")\n            for j, (top, left, bottom, right) in enumerate(boxes_by_page[page_num]):\n                draw.stroke_color = colors[j % 2] if alternate_colors else colors[0]\n                draw.rectangle(left=left, top=top, right=right, bottom=bottom)\n            draw(img)\n            imgs.append(img)\n        return imgs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying the bounding boxes corresponding to candidates on an image of the pdf file.", "response": "def display_candidates(self, candidates, pdf_file=None):\n        \"\"\"\n        Displays the bounding boxes corresponding to candidates on an image of the pdf\n        boxes is a list of 5-tuples (page, top, left, bottom, right)\n        \"\"\"\n        if not pdf_file:\n            pdf_file = os.path.join(\n                self.pdf_path, candidates[0][0].context.sentence.document.name\n            )\n            if os.path.isfile(pdf_file + \".pdf\"):\n                pdf_file += \".pdf\"\n            elif os.path.isfile(pdf_file + \".PDF\"):\n                pdf_file += \".PDF\"\n            else:\n                logger.error(\"display_candidates failed: pdf file missing.\")\n        boxes = [\n            get_box(mention.context) for c in candidates for mention in c.get_mentions()\n        ]\n        imgs = self.display_boxes(pdf_file, boxes, alternate_colors=True)\n        return display(*imgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, docs=None, split=0, lfs=None, parallelism=None, progress_bar=True):\n        if lfs is None:\n            raise ValueError(\"Please provide a list of lists of labeling functions.\")\n\n        if len(lfs) != len(self.candidate_classes):\n            raise ValueError(\"Please provide LFs for each candidate class.\")\n\n        self.apply(\n            docs=docs,\n            split=split,\n            lfs=lfs,\n            train=True,\n            clear=False,\n            parallelism=parallelism,\n            progress_bar=progress_bar,\n        )", "response": "Update the labels of the specified candidates based on the provided LFs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(\n        self,\n        docs=None,\n        split=0,\n        train=False,\n        lfs=None,\n        clear=True,\n        parallelism=None,\n        progress_bar=True,\n    ):\n        \"\"\"Apply the labels of the specified candidates based on the provided LFs.\n\n        :param docs: If provided, apply the LFs to all the candidates in these\n            documents.\n        :param split: If docs is None, apply the LFs to the candidates in this\n            particular split.\n        :type split: int\n        :param train: Whether or not to update the global key set of labels and\n            the labels of candidates.\n        :type train: bool\n        :param lfs: A list of lists of labeling functions to apply. Each list\n            should correspond with the candidate_classes used to initialize the\n            Labeler.\n        :type lfs: list of lists\n        :param clear: Whether or not to clear the labels table before applying\n            these LFs.\n        :type clear: bool\n        :param parallelism: How many threads to use for extraction. This will\n            override the parallelism value used to initialize the Labeler if\n            it is provided.\n        :type parallelism: int\n        :param progress_bar: Whether or not to display a progress bar. The\n            progress bar is measured per document.\n        :type progress_bar: bool\n\n        :raises ValueError: If labeling functions are not provided for each\n            candidate class.\n        \"\"\"\n        if lfs is None:\n            raise ValueError(\"Please provide a list of labeling functions.\")\n\n        if len(lfs) != len(self.candidate_classes):\n            raise ValueError(\"Please provide LFs for each candidate class.\")\n\n        self.lfs = lfs\n        if docs:\n            # Call apply on the specified docs for all splits\n            split = ALL_SPLITS\n            super(Labeler, self).apply(\n                docs,\n                split=split,\n                train=train,\n                lfs=self.lfs,\n                clear=clear,\n                parallelism=parallelism,\n                progress_bar=progress_bar,\n            )\n            # Needed to sync the bulk operations\n            self.session.commit()\n        else:\n            # Only grab the docs containing candidates from the given split.\n            split_docs = get_docs_from_split(\n                self.session, self.candidate_classes, split\n            )\n            super(Labeler, self).apply(\n                split_docs,\n                split=split,\n                train=train,\n                lfs=self.lfs,\n                clear=clear,\n                parallelism=parallelism,\n                progress_bar=progress_bar,\n            )\n            # Needed to sync the bulk operations\n            self.session.commit()", "response": "Applies the labeling functions to the specified candidates and sets the global key set of the labels table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the LabelKeys of each class in the specified split.", "response": "def clear(self, train, split, lfs=None):\n        \"\"\"Delete Labels of each class from the database.\n\n        :param train: Whether or not to clear the LabelKeys.\n        :type train: bool\n        :param split: Which split of candidates to clear labels from.\n        :type split: int\n        :param lfs: This parameter is ignored.\n        \"\"\"\n        # Clear Labels for the candidates in the split passed in.\n        logger.info(f\"Clearing Labels (split {split})\")\n\n        sub_query = (\n            self.session.query(Candidate.id).filter(Candidate.split == split).subquery()\n        )\n        query = self.session.query(Label).filter(Label.candidate_id.in_(sub_query))\n        query.delete(synchronize_session=\"fetch\")\n\n        # Delete all old annotation keys\n        if train:\n            logger.debug(f\"Clearing all LabelKeys from {self.candidate_classes}...\")\n            drop_all_keys(self.session, LabelKey, self.candidate_classes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete all Labels and LabelKeys.", "response": "def clear_all(self):\n        \"\"\"Delete all Labels.\"\"\"\n        logger.info(\"Clearing ALL Labels and LabelKeys.\")\n        self.session.query(Label).delete(synchronize_session=\"fetch\")\n        self.session.query(LabelKey).delete(synchronize_session=\"fetch\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_gold_labels(self, cand_lists, annotator=None):\n        return get_sparse_matrix(self.session, GoldLabelKey, cand_lists, key=annotator)", "response": "Load a sparse matrix of GoldLabels for each candidate_class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts lfs into a generator of id name and labels.", "response": "def _f_gen(self, c):\n        \"\"\"Convert lfs into a generator of id, name, and labels.\n\n        In particular, catch verbose values and convert to integer ones.\n        \"\"\"\n        lf_idx = self.candidate_classes.index(c.__class__)\n        labels = lambda c: [(c.id, lf.__name__, lf(c)) for lf in self.lfs[lf_idx]]\n        for cid, lf_key, label in labels(c):\n            # Note: We assume if the LF output is an int, it is already\n            # mapped correctly\n            if isinstance(label, int):\n                yield cid, lf_key, label\n            # None is a protected LF output value corresponding to 0,\n            # representing LF abstaining\n            elif label is None:\n                yield cid, lf_key, 0\n            elif label in c.values:\n                if c.cardinality > 2:\n                    yield cid, lf_key, c.values.index(label) + 1\n                # Note: Would be nice to not special-case here, but for\n                # consistency we leave binary LF range as {-1,0,1}\n                else:\n                    val = 1 if c.values.index(label) == 0 else -1\n                    yield cid, lf_key, val\n            else:\n                raise ValueError(\n                    f\"Can't parse label value {label} for candidate values {c.values}\"\n                )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply(self, doc, split, train, lfs, **kwargs):\n        logger.debug(f\"Document: {doc}\")\n\n        if lfs is None:\n            raise ValueError(\"Must provide lfs kwarg.\")\n\n        self.lfs = lfs\n\n        # Get all the candidates in this doc that will be labeled\n        cands_list = get_cands_list_from_split(\n            self.session, self.candidate_classes, doc, split\n        )\n\n        label_map = dict()\n        for cands in cands_list:\n            records = list(\n                get_mapping(self.session, Label, cands, self._f_gen, label_map)\n            )\n            batch_upsert_records(self.session, Label, records)\n\n        # Insert all Label Keys\n        if train:\n            upsert_keys(self.session, LabelKey, label_map)\n\n        # This return + yield makes a completely empty generator\n        return\n        yield", "response": "Extract candidates from the given document and split and insert them into the given Context."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_duration(line):\n    DAYS, SECS = {'D': 1, 'W': 7}, {'S': 1, 'M': 60, 'H': 3600}\n    sign, i = 1, 0\n    if line[i] in '-+':\n        if line[i] == '-':\n            sign = -1\n        i += 1\n    if line[i] != 'P':\n        raise parse.ParseError()\n    i += 1\n    days, secs = 0, 0\n    while i < len(line):\n        if line[i] == 'T':\n            i += 1\n            if i == len(line):\n                break\n        j = i\n        while line[j].isdigit():\n            j += 1\n        if i == j:\n            raise parse.ParseError()\n        val = int(line[i:j])\n        if line[j] in DAYS:\n            days += val * DAYS[line[j]]\n            DAYS.pop(line[j])\n        elif line[j] in SECS:\n            secs += val * SECS[line[j]]\n            SECS.pop(line[j])\n        else:\n            raise parse.ParseError()\n        i = j + 1\n    return timedelta(sign * days, sign * secs)", "response": "Parse a string in the DURATION property format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef timedelta_to_duration(dt):\n    days, secs = dt.days, dt.seconds\n    res = 'P'\n    if days // 7:\n        res += str(days // 7) + 'W'\n        days %= 7\n    if days:\n        res += str(days) + 'D'\n    if secs:\n        res += 'T'\n        if secs // 3600:\n            res += str(secs // 3600) + 'H'\n            secs %= 3600\n        if secs // 60:\n            res += str(secs // 60) + 'M'\n            secs %= 60\n        if secs:\n            res += str(secs) + 'S'\n    return res", "response": "Return a string according to the DURATION property format\n    from a timedelta object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clone(self):\n        clone = copy.copy(self)\n        clone._unused = clone._unused.clone()\n        return clone", "response": "Returns an exact copy of self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over every event that is included in the timespan between start and stop.", "response": "def included(self, start, stop):\n        \"\"\"Iterates (in chronological order) over every event that is included\n        in the timespan between `start` and `stop`\n\n        Args:\n            start : (Arrow object)\n            stop : (Arrow object)\n        \"\"\"\n        for event in self:\n            if (start <= event.begin <= stop # if start is between the bonds\n            and start <= event.end <= stop): # and stop is between the bonds\n                yield event"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over every event that has an intersection with the timespan between start and stop.", "response": "def overlapping(self, start, stop):\n        \"\"\"Iterates (in chronological order) over every event that has an intersection\n        with the timespan between `start` and `stop`\n\n        Args:\n            start : (Arrow object)\n            stop : (Arrow object)\n        \"\"\"\n        for event in self:\n            if ((start <= event.begin <= stop # if start is between the bonds\n            or start <= event.end <= stop) # or stop is between the bonds\n            or event.begin <= start and event.end >= stop): # or event is a superset of [start,stop]\n                yield event"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating over all events that occuring during the specified instant.", "response": "def at(self, instant):\n        \"\"\"Iterates (in chronological order) over all events that are occuring during `instant`.\n\n        Args:\n            instant (Arrow object)\n        \"\"\"\n\n        for event in self:\n            if event.begin <= instant <= event.end:\n                yield event"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on(self, day, strict=False):\n        day_start, day_stop = day.floor('day').span('day')\n        if strict:\n            return self.included(day_start, day_stop)\n        else:\n            return self.overlapping(day_start, day_stop)", "response": "Iterates over all events that occurs on a given day."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over all events that occurs today.", "response": "def today(self, strict=False):\n        \"\"\"Iterates (in chronological order) over all events that occurs today\n\n        Args:\n            strict (bool): if True events will be returned only if they are\\\n            strictly *included* in `day`.\n        \"\"\"\n        return self.on(arrow.now(), strict=strict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef end(self):\n\n        if self._duration:  # if end is duration defined\n            # return the beginning + duration\n            return self.begin + self._duration\n        elif self._end_time:  # if end is time defined\n            if self.all_day:\n                return self._end_time\n            else:\n                return self._end_time\n        elif self._begin:  # if end is not defined\n            if self.all_day:\n                return self._begin + timedelta(days=1)\n            else:\n                # instant event\n                return self._begin\n        else:\n            return None", "response": "Get or set the end of the event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef duration(self):\n        if self._duration:\n            return self._duration\n        elif self.end:\n            # because of the clever getter for end, this also takes care of all_day events\n            return self.end - self.begin\n        else:\n            # event has neither start, nor end, nor duration\n            return None", "response": "Get or set the duration of the event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming self to an all - day event.", "response": "def make_all_day(self):\n        \"\"\"Transforms self to an all-day event.\n\n        The event will span all the days from the begin to the end day.\n        \"\"\"\n        if self.all_day:\n            # Do nothing if we already are a all day event\n            return\n\n        begin_day = self.begin.floor('day')\n        end_day = self.end.floor('day')\n\n        self._begin = begin_day\n\n        # for a one day event, we don't need a _end_time\n        if begin_day == end_day:\n            self._end_time = None\n        else:\n            self._end_time = end_day + timedelta(days=1)\n\n        self._duration = None\n        self._begin_precision = 'day'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join(self, other, *args, **kwarg):\n        event = Event(*args, **kwarg)\n        if self.intersects(other):\n            if self.starts_within(other):\n                event.begin = other.begin\n            else:\n                event.begin = self.begin\n\n            if self.ends_within(other):\n                event.end = other.end\n            else:\n                event.end = self.end\n\n            return event\n        raise ValueError('Cannot join {} with {}: they don\\'t intersect.'.format(self, other))", "response": "Create a new Event instance which covers the time range of two intersecting events\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timezone(calendar, vtimezones):\n    for vtimezone in vtimezones:\n        remove_x(vtimezone)  # Remove non standard lines from the block\n        fake_file = StringIO()\n        fake_file.write(str(vtimezone))  # Represent the block as a string\n        fake_file.seek(0)\n        timezones = tzical(fake_file)  # tzical does not like strings\n        # timezones is a tzical object and could contain multiple timezones\n        for key in timezones.keys():\n            calendar._timezones[key] = timezones.get(key)", "response": "Takes a list of VTIMEZONE blocks and parses them and adds them to the calendar. _timezones dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an exact deep copy of self", "response": "def clone(self):\n        \"\"\"\n        Returns:\n            Calendar: an exact deep copy of self\n        \"\"\"\n        clone = copy.copy(self)\n        clone._unused = clone._unused.clone()\n        clone.events = copy.copy(self.events)\n        clone.todos = copy.copy(self.todos)\n        clone._timezones = copy.copy(self._timezones)\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget or set the end of the todo.", "response": "def due(self):\n        \"\"\"Get or set the end of the todo.\n\n        |  Will return an :class:`Arrow` object.\n        |  May be set to anything that :func:`Arrow.get` understands.\n        |  If set to a non null value, removes any already\n            existing duration.\n        |  Setting to None will have unexpected behavior if\n            begin is not None.\n        |  Must not be set to an inferior value than self.begin.\n        \"\"\"\n\n        if self._duration:\n            # if due is duration defined return the beginning + duration\n            return self.begin + self._duration\n        elif self._due_time:\n            # if due is time defined\n            return self._due_time\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef duration(self):\n        if self._duration:\n            return self._duration\n        elif self.due:\n            return self.due - self.begin\n        else:\n            # todo has neither due, nor start and duration\n            return None", "response": "Get or set the duration of the todo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a deep copy of the current object", "response": "def clone(self):\n        \"\"\"\n        Returns:\n            Todo: an exact copy of self\"\"\"\n        clone = copy.copy(self)\n        clone._unused = clone._unused.clone()\n        clone.alarms = copy.copy(self.alarms)\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef login(self, email, password, android_id):\n        self._email = email\n        self._android_id = android_id\n        res = gpsoauth.perform_master_login(self._email, password, self._android_id)\n        if 'Token' not in res:\n            raise exception.LoginException(res.get('Error'), res.get('ErrorDetail'))\n        self._master_token = res['Token']\n\n        self.refresh()\n        return True", "response": "Authenticate to Google with the provided credentials."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(self, email, master_token, android_id):\n        self._email = email\n        self._android_id = android_id\n        self._master_token = master_token\n\n        self.refresh()\n        return True", "response": "Authenticate to Google with the provided master token and android_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef refresh(self):\n        res = gpsoauth.perform_oauth(\n            self._email, self._master_token, self._android_id,\n            service=self._scopes,\n            app='com.google.android.keep',\n            client_sig='38918a453d07199354f8b19af05ec6562ced5788'\n        )\n        if 'Auth' not in res:\n            if 'Token' not in res:\n                raise exception.LoginException(res.get('Error'))\n\n        self._auth_token = res['Auth']\n        return self._auth_token", "response": "Refresh the OAuth token."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging out of the account.", "response": "def logout(self):\n        \"\"\"Log out of the account.\"\"\"\n        self._master_token = None\n        self._auth_token = None\n        self._email = None\n        self._android_id = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, **req_kwargs):\n        i = 0\n        while True:\n            response = self._send(**req_kwargs).json()\n            if 'error' not in response:\n                break\n\n            error = response['error']\n            if error['code'] != 401:\n                raise exception.APIException(error['code'], error)\n\n            if i >= self.RETRY_CNT:\n                raise exception.APIException(error['code'], error)\n\n            logger.info('Refreshing access token')\n            self._auth.refresh()\n            i += 1\n\n        return response", "response": "Send an authenticated request to a Google API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _send(self, **req_kwargs):\n        auth_token = self._auth.getAuthToken()\n        if auth_token is None:\n            raise exception.LoginException('Not logged in')\n\n        req_kwargs.setdefault('headers', {\n            'Authorization': 'OAuth ' + auth_token\n        })\n\n        return self._session.request(**req_kwargs)", "response": "Send an authenticated request to a Google API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, blob):\n        return self._send(\n            url=self._base_url + blob.parent.server_id + '/' + blob.server_id + '?s=0',\n            method='GET',\n            allow_redirects=False\n        ).headers.get('Location')", "response": "Get the canonical link to a media blob."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self):\n        params = {}\n        return self.send(\n            url=self._base_url + 'create',\n            method='POST',\n            json=params\n        )", "response": "Create a new reminder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate to Google with the provided credentials & sync.", "response": "def login(self, username, password, state=None, sync=True):\n        \"\"\"Authenticate to Google with the provided credentials & sync.\n\n        Args:\n            email (str): The account to use.\n            password (str): The account password.\n            state (dict): Serialized state to load.\n\n        Raises:\n            LoginException: If there was a problem logging in.\n        \"\"\"\n        auth = APIAuth(self.OAUTH_SCOPES)\n\n        ret = auth.login(username, password, get_mac())\n        if ret:\n            self.load(auth, state, sync)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resume(self, email, master_token, state=None, sync=True):\n        auth = APIAuth(self.OAUTH_SCOPES)\n\n        ret = auth.load(email, master_token, android_id=get_mac())\n        if ret:\n            self.load(auth, state, sync)\n\n        return ret", "response": "Authenticate to Google with the provided master token & sync."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nauthenticates to Google with a prepared authentication object & sync.", "response": "def load(self, auth, state=None, sync=True):\n        \"\"\"Authenticate to Google with a prepared authentication object & sync.\n        Args:\n            auth (APIAuth): Authentication object.\n            state (dict): Serialized state to load.\n\n        Raises:\n            LoginException: If there was a problem logging in.\n        \"\"\"\n        self._keep_api.setAuth(auth)\n        self._reminders_api.setAuth(auth)\n        self._media_api.setAuth(auth)\n        if state is not None:\n            self.restore(state)\n        if sync:\n            self.sync(True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(self):\n        # Find all nodes manually, as the Keep object isn't aware of new ListItems\n        # until they've been synced to the server.\n        nodes = []\n        for node in self.all():\n            nodes.append(node)\n            for child in node.children:\n                nodes.append(child)\n        return {\n            'keep_version': self._keep_version,\n            'labels': [label.save(False) for label in self.labels()],\n            'nodes': [node.save(False) for node in nodes]\n        }", "response": "Serialize note data.\n\n        Args:\n            state (dict): Serialized state to load."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restore(self, state):\n        self._clear()\n        self._parseUserInfo({'labels': state['labels']})\n        self._parseNodes(state['nodes'])\n        self._keep_version = state['keep_version']", "response": "Unserialize saved note data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a note with the given ID.", "response": "def get(self, node_id):\n        \"\"\"Get a note with the given ID.\n\n        Args:\n            node_id (str): The note ID.\n\n        Returns:\n            gkeepapi.node.TopLevelNode: The Note or None if not found.\n        \"\"\"\n        return \\\n            self._nodes[_node.Root.ID].get(node_id) or \\\n            self._nodes[_node.Root.ID].get(self._sid_map.get(node_id))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a top level node to the cache.", "response": "def add(self, node):\n        \"\"\"Register a top level node (and its children) for syncing up to the server. There's no need to call this for nodes created by\n        :py:meth:`createNote` or :py:meth:`createList` as they are automatically added.\n\n            LoginException: If :py:meth:`login` has not been called.\n        Args:\n            node (gkeepapi.node.Node): The node to sync.\n\n        Raises:\n            Invalid: If the parent node is not found.\n        \"\"\"\n        if node.parent_id != _node.Root.ID:\n            raise exception.InvalidException('Not a top level node')\n\n        self._nodes[node.id] = node\n        self._nodes[node.parent_id].append(node, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find(self, query=None, func=None, labels=None, colors=None, pinned=None, archived=None, trashed=False): # pylint: disable=too-many-arguments\n        if labels is not None:\n            labels = [i.id if isinstance(i, _node.Label) else i for i in labels]\n\n        return (node for node in self.all() if\n            (query is None or (\n                (isinstance(query, six.string_types) and (query in node.title or query in node.text)) or\n                (isinstance(query, Pattern) and (\n                    query.search(node.title) or query.search(node.text)\n                ))\n            )) and\n            (func is None or func(node)) and \\\n            (labels is None or \\\n             (not labels and not node.labels.all()) or \\\n             (any((node.labels.get(i) is not None for i in labels)))\n            ) and \\\n            (colors is None or node.color in colors) and \\\n            (pinned is None or node.pinned == pinned) and \\\n            (archived is None or node.archived == archived) and \\\n            (trashed is None or node.trashed == trashed)\n        )", "response": "Find Notes based on the specified criteria."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef createNote(self, title=None, text=None):\n        node = _node.Note()\n        if title is not None:\n            node.title = title\n        if text is not None:\n            node.text = text\n        self.add(node)\n        return node", "response": "Create a new managed note."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new list and populate it with the given items.", "response": "def createList(self, title=None, items=None):\n        \"\"\"Create a new list and populate it. Any changes to the note will be uploaded when :py:meth:`sync` is called.\n\n        Args:\n            title (str): The title of the list.\n            items (List[(str, bool)]): A list of tuples. Each tuple represents the text and checked status of the listitem.\n\n        Returns:\n            gkeepapi.node.List: The new list.\n        \"\"\"\n        if items is None:\n            items = []\n\n        node = _node.List()\n        if title is not None:\n            node.title = title\n        for text, checked in items:\n            node.add(text, checked)\n        self.add(node)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef createLabel(self, name):\n        if self.findLabel(name):\n            raise exception.LabelException('Label exists')\n        node = _node.Label()\n        node.name = name\n        self._labels[node.id] = node # pylint: disable=protected-access\n        return node", "response": "Create a new label."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind a label with the given name.", "response": "def findLabel(self, query, create=False):\n        \"\"\"Find a label with the given name.\n\n        Args:\n            name (Union[_sre.SRE_Pattern, str]): A str or regular expression to match against the name.\n            create (bool): Whether to create the label if it doesn't exist (only if name is a str).\n\n        Returns:\n            Union[gkeepapi.node.Label, None]: The label.\n        \"\"\"\n        if isinstance(query, six.string_types):\n            query = query.lower()\n\n        for label in self._labels.values():\n            if (isinstance(query, six.string_types) and query == label.name.lower()) or \\\n                (isinstance(query, Pattern) and query.search(label.name)):\n                return label\n\n        return self.createLabel(query) if create and isinstance(query, six.string_types) else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a label. Args: label_id (str): Label id.", "response": "def deleteLabel(self, label_id):\n        \"\"\"Deletes a label.\n\n        Args:\n            label_id (str): Label id.\n        \"\"\"\n        if label_id not in self._labels:\n            return\n\n        label = self._labels[label_id]\n        label.delete()\n        for node in self.all():\n            node.labels.remove(label)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sync(self, resync=False):\n        if resync:\n            self._clear()\n\n        while True:\n            logger.debug('Starting reminder sync: %s', self._reminder_version)\n            changes = self._reminders_api.list()\n\n            if 'task' in changes:\n                self._parseTasks(changes['task'])\n\n            self._reminder_version = changes['storageVersion']\n            logger.debug('Finishing sync: %s', self._reminder_version)\n            history = self._reminders_api.history(self._reminder_version)\n            if self._reminder_version == history['highestStorageVersion']:\n                break\n\n        while True:\n            logger.debug('Starting keep sync: %s', self._keep_version)\n\n            labels_updated = any((i.dirty for i in self._labels.values()))\n            changes = self._keep_api.changes(\n                target_version=self._keep_version,\n                nodes=[i.save() for i in self._findDirtyNodes()],\n                labels=[i.save() for i in self._labels.values()] if labels_updated else None,\n            )\n\n            if changes.get('forceFullResync'):\n                raise exception.ResyncRequiredException('Full resync required')\n\n            if changes.get('upgradeRecommended'):\n                raise exception.UpgradeRecommendedException('Upgrade recommended')\n\n            if 'userInfo' in changes:\n                self._parseUserInfo(changes['userInfo'])\n\n            if 'nodes' in changes:\n                self._parseNodes(changes['nodes'])\n\n            self._keep_version = changes['toVersion']\n            logger.debug('Finishing sync: %s', self._keep_version)\n            if not changes['truncated']:\n                break\n\n        if _node.DEBUG:\n            self._clean()", "response": "Syncs the local keep tree with the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _clean(self):\n        found_ids = {}\n        nodes = [self._nodes[_node.Root.ID]]\n        while nodes:\n            node = nodes.pop()\n            found_ids[node.id] = None\n            nodes = nodes + node.children\n\n        for node_id in self._nodes:\n            if node_id in found_ids:\n                continue\n            logger.error('Dangling node: %s', node_id)\n\n        for node_id in found_ids:\n            if node_id in self._nodes:\n                continue\n            logger.error('Unregistered node: %s', node_id)", "response": "Recursively check that all nodes are reachable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_json(raw):\n    ncls = None\n    _type = raw.get('type')\n    try:\n        ncls = _type_map[NodeType(_type)]\n    except (KeyError, ValueError) as e:\n        logger.warning('Unknown node type: %s', _type)\n        if DEBUG:\n            raise_from(exception.ParseException('Parse error for %s' % (_type), raw), e)\n        return None\n    node = ncls()\n    node.load(raw)\n\n    return node", "response": "Helper to construct a node from a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, raw):\n        try:\n            self._load(raw)\n        except (KeyError, ValueError) as e:\n            raise_from(exception.ParseException('Parse error in %s' % (type(self)), raw), e)", "response": "Deserialize from raw representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes into raw representation. Clears the dirty bit by default.", "response": "def save(self, clean=True):\n        \"\"\"Serialize into raw representation. Clears the dirty bit by default.\n\n        Args:\n            clean (bool): Whether to clear the dirty bit.\n\n        Returns:\n            dict: Raw.\n        \"\"\"\n        ret = {}\n        if clean:\n            self._dirty = False\n        else:\n            ret['_dirty'] = self._dirty\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_json(cls, raw):\n        bcls = None\n        if 'webLink' in raw:\n            bcls = WebLink\n        elif 'topicCategory' in raw:\n            bcls = Category\n        elif 'taskAssist' in raw:\n            bcls = TaskAssist\n        elif 'context' in raw:\n            bcls = Context\n\n        if bcls is None:\n            logger.warning('Unknown annotation type: %s', raw.keys())\n            return None\n        annotation = bcls()\n        annotation.load(raw)\n\n        return annotation", "response": "Helper to construct an annotation from a dict representation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef links(self):\n        return [annotation for annotation in self._annotations.values()\n            if isinstance(annotation, WebLink)\n        ]", "response": "Get all links.\n\n        Returns:\n            list[gkeepapi.node.WebLink]: A list of links."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append(self, annotation):\n        self._annotations[annotation.id] = annotation\n        self._dirty = True\n        return annotation", "response": "Add an annotation.\n\n        Args:\n            annotation (gkeepapi.node.Annotation): An Annotation object.\n\n        Returns:\n            gkeepapi.node.Annotation: The Annotation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove an annotation. Args: annotation (gkeepapi.node.Annotation): An Annotation object. Returns: gkeepapi.node.Annotation: The Annotation.", "response": "def remove(self, annotation):\n        \"\"\"Removes an annotation.\n\n        Args:\n            annotation (gkeepapi.node.Annotation): An Annotation object.\n\n        Returns:\n            gkeepapi.node.Annotation: The Annotation.\n        \"\"\"\n        if annotation.id in self._annotations:\n            del self._annotations[annotation.id]\n        self._dirty = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, email):\n        if email not in self._collaborators:\n            self._collaborators[email] = ShareRequestValue.Add\n        self._dirty = True", "response": "Add a collaborator.\n\n        Args:\n            str : Collaborator email address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a Collaborator from the set.", "response": "def remove(self, email):\n        \"\"\"Remove a Collaborator.\n\n        Args:\n            str : Collaborator email address.\n        \"\"\"\n        if email in self._collaborators:\n            if self._collaborators[email] == ShareRequestValue.Add:\n                del self._collaborators[email]\n            else:\n                self._collaborators[email] = ShareRequestValue.Remove\n        self._dirty = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all(self):\n        return [email for email, action in self._collaborators.items() if action in [RoleValue.Owner, RoleValue.User, ShareRequestValue.Add]]", "response": "Get all collaborators.\n\n        Returns:\n            List[str]: Collaborators."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, label):\n        self._labels[label.id] = label\n        self._dirty = True", "response": "Add a label.\n\n        Args:\n            label (gkeepapi.node.Label): The Label object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a label. Args: label (gkeepapi.node.Label): The Label object.", "response": "def remove(self, label):\n        \"\"\"Remove a label.\n\n        Args:\n            label (gkeepapi.node.Label): The Label object.\n        \"\"\"\n        if label.id in self._labels:\n            self._labels[label.id] = None\n        self._dirty = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmarking the node as dirty.", "response": "def touch(self, edited=False):\n        \"\"\"Mark the node as dirty.\n\n        Args:\n            edited (bool): Whether to set the edited time.\n        \"\"\"\n        self._dirty = True\n        dt = datetime.datetime.utcnow()\n        self.timestamps.updated = dt\n        if edited:\n            self.timestamps.edited = dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if this item is in the trashed state False otherwise.", "response": "def trashed(self):\n        \"\"\"Get the trashed state.\n\n        Returns:\n            bool: Whether this item is trashed.\n        \"\"\"\n        return self.timestamps.trashed is not None and self.timestamps.trashed > NodeTimestamps.int_to_dt(0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if this item is in the deleted state False otherwise.", "response": "def deleted(self):\n        \"\"\"Get the deleted state.\n\n        Returns:\n            bool: Whether this item is deleted.\n        \"\"\"\n        return self.timestamps.deleted is not None and self.timestamps.deleted > NodeTimestamps.int_to_dt(0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef text(self, value):\n        self._text = value\n        self.timestamps.edited = datetime.datetime.utcnow()\n        self.touch(True)", "response": "Set the text value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append(self, node, dirty=True):\n        self._children[node.id] = node\n        node.parent = self\n        if dirty:\n            self.touch()\n\n        return node", "response": "Add a new child node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, node, dirty=True):\n        if node.id in self._children:\n            self._children[node.id].parent = None\n            del self._children[node.id]\n        if dirty:\n            self.touch()", "response": "Removes the given child node from the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(self, text, checked=False, sort=None):\n        node = ListItem(parent_id=self.id, parent_server_id=self.server_id)\n        node.checked = checked\n        node.text = text\n        if sort is not None:\n            node.sort = sort\n        self.append(node, True)\n        self.touch(True)\n        return node", "response": "Add a new item to the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef items_sort(cls, items):\n        class t(tuple):\n            \"\"\"Tuple with element-based sorting\"\"\"\n            def __cmp__(self, other):\n                for a, b in six.moves.zip_longest(self, other):\n                    if a != b:\n                        if a is None:\n                            return 1\n                        if b is None:\n                            return -1\n                        return a - b\n                return 0\n\n            def __lt__(self, other):\n                return self.__cmp__(other) < 0\n            def __gt_(self, other):\n                return self.__cmp__(other) > 0\n            def __le__(self, other):\n                return self.__cmp__(other) <= 0\n            def __ge_(self, other):\n                return self.__cmp__(other) >= 0\n            def __eq__(self, other):\n                return self.__cmp__(other) == 0\n            def __ne__(self, other):\n                return self.__cmp__(other) != 0\n\n        def key_func(x):\n            if x.indented:\n                return t((int(x.parent_item.sort), int(x.sort)))\n            return t((int(x.sort), ))\n\n        return sorted(items, key=key_func, reverse=True)", "response": "Sort list items taking into account parent items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, text, checked=False, sort=None):\n        if self.parent is None:\n            raise exception.InvalidException('Item has no parent')\n        node = self.parent.add(text, checked, sort)\n        self.indent(node)\n        return node", "response": "Add a new sub item to the list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindent an item. Does nothing if the target has subitems.", "response": "def indent(self, node, dirty=True):\n        \"\"\"Indent an item. Does nothing if the target has subitems.\n\n        Args:\n            node (gkeepapi.node.ListItem): Item to indent.\n            dirty (bool): Whether this node should be marked dirty.\n        \"\"\"\n        if node.subitems:\n            return\n\n        self._subitems[node.id] = node\n        node.super_list_item_id = self.id\n        node.parent_item = self\n        if dirty:\n            node.touch(True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_prompt_code(response):\n        num_code = response.find(\"div\", {\"jsname\": \"EKvSSd\"})\n        if num_code:\n            print(\"numerical code for prompt: {}\".format(num_code.string))", "response": "Check if there is an additional numerical code on the response page that needs to be selected. Print it if it s there."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_short_module_name(module_name, obj_name):\n    scope = {}\n    try:\n        # Find out what the real object is supposed to be.\n        exec('from %s import %s' % (module_name, obj_name), scope, scope)\n        real_obj = scope[obj_name]\n    except Exception:\n        return module_name\n\n    parts = module_name.split('.')\n    short_name = module_name\n    for i in range(len(parts) - 1, 0, -1):\n        short_name = '.'.join(parts[:i])\n        scope = {}\n        try:\n            exec('from %s import %s' % (short_name, obj_name), scope, scope)\n            # Ensure shortened object is the same as what we expect.\n            assert real_obj is scope[obj_name]\n        except Exception:  # libraries can throw all sorts of exceptions...\n            # get the last working module name\n            short_name = '.'.join(parts[:(i + 1)])\n            break\n    return short_name", "response": "Get the shortest possible module name for a given object name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_object_names_from_docs(filename):\n    text = split_code_and_text_blocks(filename)[1]\n    text = '\\n'.join(t[1] for t in text if t[0] == 'text')\n    regex = re.compile(r':(?:'\n                       r'func(?:tion)?|'\n                       r'meth(?:od)?|'\n                       r'attr(?:ibute)?|'\n                       r'obj(?:ect)?|'\n                       r'class):`(\\S*)`'\n                       )\n    return [(x, x) for x in re.findall(regex, text)]", "response": "Extract object names from the text blocks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a codeobj summary by identifying and resolving used names.", "response": "def identify_names(filename):\n    \"\"\"Builds a codeobj summary by identifying and resolving used names.\"\"\"\n    node, _ = parse_source_file(filename)\n    if node is None:\n        return {}\n\n    # Get matches from the code (AST)\n    finder = NameFinder()\n    finder.visit(node)\n    names = list(finder.get_mapping())\n    names += extract_object_names_from_docs(filename)\n\n    example_code_obj = collections.OrderedDict()\n    for name, full_name in names:\n        if name in example_code_obj:\n            continue  # if someone puts it in the docstring and code\n        # name is as written in file (e.g. np.asarray)\n        # full_name includes resolved import path (e.g. numpy.asarray)\n        splitted = full_name.rsplit('.', 1)\n        if len(splitted) == 1:\n            # module without attribute. This is not useful for\n            # backreferences\n            continue\n\n        module, attribute = splitted\n        # get shortened module name\n        module_short = get_short_module_name(module, attribute)\n        cobj = {'name': attribute, 'module': module,\n                'module_short': module_short}\n        example_code_obj[name] = cobj\n    return example_code_obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scan_used_functions(example_file, gallery_conf):\n    example_code_obj = identify_names(example_file)\n    if example_code_obj:\n        codeobj_fname = example_file[:-3] + '_codeobj.pickle.new'\n        with open(codeobj_fname, 'wb') as fid:\n            pickle.dump(example_code_obj, fid, pickle.HIGHEST_PROTOCOL)\n        _replace_md5(codeobj_fname)\n\n    backrefs = set('{module_short}.{name}'.format(**entry)\n                   for entry in example_code_obj.values()\n                   if entry['module'].startswith(gallery_conf['doc_module']))\n\n    return backrefs", "response": "save variables so we can later add links to the documentation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a div that will place a thumbnail in a gallery", "response": "def _thumbnail_div(target_dir, src_dir, fname, snippet, is_backref=False,\n                   check=True):\n    \"\"\"Generates RST to place a thumbnail in a gallery\"\"\"\n    thumb, _ = _find_image_ext(\n        os.path.join(target_dir, 'images', 'thumb',\n                     'sphx_glr_%s_thumb.png' % fname[:-3]))\n    if check and not os.path.isfile(thumb):\n        # This means we have done something wrong in creating our thumbnail!\n        raise RuntimeError('Could not find internal sphinx-gallery thumbnail '\n                           'file:\\n%s' % (thumb,))\n    thumb = os.path.relpath(thumb, src_dir)\n    full_dir = os.path.relpath(target_dir, src_dir)\n\n    # Inside rst files forward slash defines paths\n    thumb = thumb.replace(os.sep, \"/\")\n\n    ref_name = os.path.join(full_dir, fname).replace(os.path.sep, '_')\n\n    template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE\n    return template.format(snippet=escape(snippet),\n                           thumbnail=thumb, ref_name=ref_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_backreferences(seen_backrefs, gallery_conf,\n                         target_dir, fname, snippet):\n    \"\"\"Writes down back reference files, which include a thumbnail list\n    of examples using a certain module\"\"\"\n    if gallery_conf['backreferences_dir'] is None:\n        return\n\n    example_file = os.path.join(target_dir, fname)\n    backrefs = scan_used_functions(example_file, gallery_conf)\n    for backref in backrefs:\n        include_path = os.path.join(gallery_conf['src_dir'],\n                                    gallery_conf['backreferences_dir'],\n                                    '%s.examples.new' % backref)\n        seen = backref in seen_backrefs\n        with codecs.open(include_path, 'a' if seen else 'w',\n                         encoding='utf-8') as ex_file:\n            if not seen:\n                heading = '\\n\\nExamples using ``%s``' % backref\n                ex_file.write(heading + '\\n')\n                ex_file.write('^' * len(heading) + '\\n')\n            ex_file.write(_thumbnail_div(target_dir, gallery_conf['src_dir'],\n                                         fname, snippet, is_backref=True))\n            seen_backrefs.add(backref)", "response": "Writes down back reference files which include a thumbnail list\n    of examples using a certain module"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef finalize_backreferences(seen_backrefs, gallery_conf):\n    logger = sphinx_compatibility.getLogger('sphinx-gallery')\n    if gallery_conf['backreferences_dir'] is None:\n        return\n\n    for backref in seen_backrefs:\n        path = os.path.join(gallery_conf['src_dir'],\n                            gallery_conf['backreferences_dir'],\n                            '%s.examples.new' % backref)\n        if os.path.isfile(path):\n            _replace_md5(path)\n        else:\n            level = gallery_conf['log_level'].get('backreference_missing',\n                                                  'warning')\n            func = getattr(logger, level)\n            func('Could not find backreferences file: %s' % (path,))\n            func('The backreferences are likely to be erroneous '\n                 'due to file system case insensitivity.')", "response": "Replace backref files only if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary with the elements of a Jupyter notebook", "response": "def jupyter_notebook_skeleton():\n    \"\"\"Returns a dictionary with the elements of a Jupyter notebook\"\"\"\n    py_version = sys.version_info\n    notebook_skeleton = {\n        \"cells\": [],\n        \"metadata\": {\n            \"kernelspec\": {\n                \"display_name\": \"Python \" + str(py_version[0]),\n                \"language\": \"python\",\n                \"name\": \"python\" + str(py_version[0])\n            },\n            \"language_info\": {\n                \"codemirror_mode\": {\n                    \"name\": \"ipython\",\n                    \"version\": py_version[0]\n                },\n                \"file_extension\": \".py\",\n                \"mimetype\": \"text/x-python\",\n                \"name\": \"python\",\n                \"nbconvert_exporter\": \"python\",\n                \"pygments_lexer\": \"ipython\" + str(py_version[0]),\n                \"version\": '{0}.{1}.{2}'.format(*sys.version_info[:3])\n            }\n        },\n        \"nbformat\": 4,\n        \"nbformat_minor\": 0\n    }\n    return notebook_skeleton"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the RST text from the examples docstrigs and comments into markdown text for the Jupyter notebooks", "response": "def rst2md(text):\n    \"\"\"Converts the RST text from the examples docstrigs and comments\n    into markdown text for the Jupyter notebooks\"\"\"\n\n    top_heading = re.compile(r'^=+$\\s^([\\w\\s-]+)^=+$', flags=re.M)\n    text = re.sub(top_heading, r'# \\1', text)\n\n    math_eq = re.compile(r'^\\.\\. math::((?:.+)?(?:\\n+^  .+)*)', flags=re.M)\n    text = re.sub(math_eq,\n                  lambda match: r'\\begin{{align}}{0}\\end{{align}}'.format(\n                      match.group(1).strip()),\n                  text)\n    inline_math = re.compile(r':math:`(.+?)`', re.DOTALL)\n    text = re.sub(inline_math, r'$\\1$', text)\n\n    directives = ('warning', 'note')\n    for directive in directives:\n        directive_re = re.compile(r'^\\.\\. %s::((?:.+)?(?:\\n+^  .+)*)'\n                                  % directive, flags=re.M)\n        text = re.sub(directive_re,\n                      partial(directive_fun, directive=directive), text)\n\n    links = re.compile(r'^ *\\.\\. _.*:.*$\\n', flags=re.M)\n    text = re.sub(links, '', text)\n\n    refs = re.compile(r':ref:`')\n    text = re.sub(refs, '`', text)\n\n    contents = re.compile(r'^\\s*\\.\\. contents::.*$(\\n +:\\S+: *$)*\\n',\n                          flags=re.M)\n    text = re.sub(contents, '', text)\n\n    images = re.compile(\n        r'^\\.\\. image::(.*$)(?:\\n *:alt:(.*$)\\n)?(?: +:\\S+:.*$\\n)*',\n        flags=re.M)\n    text = re.sub(\n        images, lambda match: '![{1}]({0})\\n'.format(\n            match.group(1).strip(), (match.group(2) or '').strip()), text)\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a Jupyter notebook file cell - by - cell.", "response": "def jupyter_notebook(script_blocks, gallery_conf):\n    \"\"\"Generate a Jupyter notebook file cell-by-cell\n\n    Parameters\n    ----------\n    script_blocks : list\n        Script execution cells.\n    gallery_conf : dict\n        The sphinx-gallery configuration dictionary.\n    \"\"\"\n    first_cell = gallery_conf.get(\"first_notebook_cell\", \"%matplotlib inline\")\n    work_notebook = jupyter_notebook_skeleton()\n    if first_cell is not None:\n        add_code_cell(work_notebook, first_cell)\n    fill_notebook(work_notebook, script_blocks)\n\n    return work_notebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_code_cell(work_notebook, code):\n\n    code_cell = {\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {\"collapsed\": False},\n        \"outputs\": [],\n        \"source\": [code.strip()]\n    }\n    work_notebook[\"cells\"].append(code_cell)", "response": "Adds a code cell to the notebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill_notebook(work_notebook, script_blocks):\n\n    for blabel, bcontent, lineno in script_blocks:\n        if blabel == 'code':\n            add_code_cell(work_notebook, bcontent)\n        else:\n            add_markdown_cell(work_notebook, bcontent + '\\n')", "response": "Writes the Jupyter notebook cells with the contents of the script_blocks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_notebook(work_notebook, write_file):\n    with open(write_file, 'w') as out_nb:\n        json.dump(work_notebook, out_nb, indent=2)", "response": "Saves the Jupyter work_notebook to write_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef python_to_jupyter_cli(args=None, namespace=None):\n    from . import gen_gallery  # To avoid circular import\n    parser = argparse.ArgumentParser(\n        description='Sphinx-Gallery Notebook converter')\n    parser.add_argument('python_src_file', nargs='+',\n                        help='Input Python file script to convert. '\n                        'Supports multiple files and shell wildcards'\n                        ' (e.g. *.py)')\n    args = parser.parse_args(args, namespace)\n\n    for src_file in args.python_src_file:\n        file_conf, blocks = split_code_and_text_blocks(src_file)\n        print('Converting {0}'.format(src_file))\n        gallery_conf = copy.deepcopy(gen_gallery.DEFAULT_GALLERY_CONF)\n        example_nb = jupyter_notebook(blocks, gallery_conf)\n        save_notebook(example_nb, replace_py_ipynb(src_file))", "response": "Exposes the jupyter notebook renderer to the command line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn n random Gaussian mixtures each of length m*.", "response": "def layers(n, m):\n    \"\"\"\n    Return *n* random Gaussian mixtures, each of length *m*.\n    \"\"\"\n    def bump(a):\n        x = 1 / (.1 + np.random.random())\n        y = 2 * np.random.random() - .3\n        z = 13 / (.1 + np.random.random())\n        for i in range(m):\n            w = (i / float(m) - y) * z\n            a[i] += x * np.exp(-w * w)\n    a = np.zeros((m, n))\n    for i in range(n):\n        for j in range(12):\n            bump(a[:, i])\n    return np.abs(a)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscraping Matplotlib images. Parameters ---------- block : tuple A tuple containing the (label, content, line_number) of the block. block_vars : dict Dict of block variables. gallery_conf : dict Contains the configuration of Sphinx-Gallery **kwargs : dict Additional keyword arguments to pass to :meth:`~matplotlib.figure.Figure.savefig`, e.g. ``format='svg'``. The ``format`` kwarg in particular is used to set the file extension of the output file (currently only 'png' and 'svg' are supported). Returns ------- rst : str The ReSTructuredText that will be rendered to HTML containing the images. This is often produced by :func:`figure_rst`.", "response": "def matplotlib_scraper(block, block_vars, gallery_conf, **kwargs):\n    \"\"\"Scrape Matplotlib images.\n\n    Parameters\n    ----------\n    block : tuple\n        A tuple containing the (label, content, line_number) of the block.\n    block_vars : dict\n        Dict of block variables.\n    gallery_conf : dict\n        Contains the configuration of Sphinx-Gallery\n    **kwargs : dict\n        Additional keyword arguments to pass to\n        :meth:`~matplotlib.figure.Figure.savefig`, e.g. ``format='svg'``.\n        The ``format`` kwarg in particular is used to set the file extension\n        of the output file (currently only 'png' and 'svg' are supported).\n\n    Returns\n    -------\n    rst : str\n        The ReSTructuredText that will be rendered to HTML containing\n        the images. This is often produced by :func:`figure_rst`.\n    \"\"\"\n    matplotlib, plt = _import_matplotlib()\n    image_path_iterator = block_vars['image_path_iterator']\n    image_paths = list()\n    for fig_num, image_path in zip(plt.get_fignums(), image_path_iterator):\n        if 'format' in kwargs:\n            image_path = '%s.%s' % (os.path.splitext(image_path)[0],\n                                    kwargs['format'])\n        # Set the fig_num figure as the current figure as we can't\n        # save a figure that's not the current figure.\n        fig = plt.figure(fig_num)\n        to_rgba = matplotlib.colors.colorConverter.to_rgba\n        for attr in ['facecolor', 'edgecolor']:\n            fig_attr = getattr(fig, 'get_' + attr)()\n            default_attr = matplotlib.rcParams['figure.' + attr]\n            if to_rgba(fig_attr) != to_rgba(default_attr) and \\\n                    attr not in kwargs:\n                kwargs[attr] = fig_attr\n        fig.savefig(image_path, **kwargs)\n        image_paths.append(image_path)\n    plt.close('all')\n    return figure_rst(image_paths, gallery_conf['src_dir'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscraping Mayavi images. Parameters ---------- block : tuple A tuple containing the (label, content, line_number) of the block. block_vars : dict Dict of block variables. gallery_conf : dict Contains the configuration of Sphinx-Gallery Returns ------- rst : str The ReSTructuredText that will be rendered to HTML containing the images. This is often produced by :func:`figure_rst`.", "response": "def mayavi_scraper(block, block_vars, gallery_conf):\n    \"\"\"Scrape Mayavi images.\n\n    Parameters\n    ----------\n    block : tuple\n        A tuple containing the (label, content, line_number) of the block.\n    block_vars : dict\n        Dict of block variables.\n    gallery_conf : dict\n        Contains the configuration of Sphinx-Gallery\n\n    Returns\n    -------\n    rst : str\n        The ReSTructuredText that will be rendered to HTML containing\n        the images. This is often produced by :func:`figure_rst`.\n    \"\"\"\n    from mayavi import mlab\n    image_path_iterator = block_vars['image_path_iterator']\n    image_paths = list()\n    e = mlab.get_engine()\n    for scene, image_path in zip(e.scenes, image_path_iterator):\n        mlab.savefig(image_path, figure=scene)\n        # make sure the image is not too large\n        scale_image(image_path, image_path, 850, 999)\n        image_paths.append(image_path)\n    mlab.close(all=True)\n    return figure_rst(image_paths, gallery_conf['src_dir'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_image_ext(path, number=None):\n    if number is not None:\n        path = path.format(number)\n    path = os.path.splitext(path)[0]\n    for ext in _KNOWN_IMG_EXTS:\n        this_path = '%s.%s' % (path, ext)\n        if os.path.isfile(this_path):\n            break\n    else:\n        ext = 'png'\n    return ('%s.%s' % (path, ext), ext)", "response": "Find an image tolerant of different file extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_figures(block, block_vars, gallery_conf):\n    image_path_iterator = block_vars['image_path_iterator']\n    all_rst = u''\n    prev_count = len(image_path_iterator)\n    for scraper in gallery_conf['image_scrapers']:\n        rst = scraper(block, block_vars, gallery_conf)\n        if not isinstance(rst, basestring):\n            raise TypeError('rst from scraper %r was not a string, '\n                            'got type %s:\\n%r'\n                            % (scraper, type(rst), rst))\n        n_new = len(image_path_iterator) - prev_count\n        for ii in range(n_new):\n            current_path, _ = _find_image_ext(\n                image_path_iterator.paths[prev_count + ii])\n            if not os.path.isfile(current_path):\n                raise RuntimeError('Scraper %s did not produce expected image:'\n                                   '\\n%s' % (scraper, current_path))\n        all_rst += rst\n    return all_rst", "response": "Save all open figures of the example code - block."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate RST code for a list of PNG filenames.", "response": "def figure_rst(figure_list, sources_dir):\n    \"\"\"Generate RST for a list of PNG filenames.\n\n    Depending on whether we have one or more figures, we use a\n    single rst call to 'image' or a horizontal list.\n\n    Parameters\n    ----------\n    figure_list : list\n        List of strings of the figures' absolute paths.\n    sources_dir : str\n        absolute path of Sphinx documentation sources\n\n    Returns\n    -------\n    images_rst : str\n        rst code to embed the images in the document\n    \"\"\"\n\n    figure_paths = [os.path.relpath(figure_path, sources_dir)\n                    .replace(os.sep, '/').lstrip('/')\n                    for figure_path in figure_list]\n    images_rst = \"\"\n    if len(figure_paths) == 1:\n        figure_name = figure_paths[0]\n        images_rst = SINGLE_IMAGE % figure_name\n    elif len(figure_paths) > 1:\n        images_rst = HLIST_HEADER\n        for figure_name in figure_paths:\n            images_rst += HLIST_IMAGE_TEMPLATE % figure_name\n\n    return images_rst"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring all files in file_list into a zip file with the name of the python archive.", "response": "def python_zip(file_list, gallery_path, extension='.py'):\n    \"\"\"Stores all files in file_list into an zip file\n\n    Parameters\n    ----------\n    file_list : list\n        Holds all the file names to be included in zip file\n    gallery_path : str\n        path to where the zipfile is stored\n    extension : str\n        '.py' or '.ipynb' In order to deal with downloads of python\n        sources and jupyter notebooks the file extension from files in\n        file_list will be removed and replace with the value of this\n        variable while generating the zip file\n    Returns\n    -------\n    zipname : str\n        zip file name, written as `target_dir_{python,jupyter}.zip`\n        depending on the extension\n    \"\"\"\n    zipname = os.path.basename(os.path.normpath(gallery_path))\n    zipname += '_python' if extension == '.py' else '_jupyter'\n    zipname = os.path.join(gallery_path, zipname + '.zip')\n    zipname_new = zipname + '.new'\n    with zipfile.ZipFile(zipname_new, mode='w') as zipf:\n        for fname in file_list:\n            file_src = os.path.splitext(fname)[0] + extension\n            zipf.write(file_src, os.path.relpath(file_src, gallery_path))\n    _replace_md5(zipname_new)\n    return zipname"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of python source files that are not downloaded from the target_dir.", "response": "def list_downloadable_sources(target_dir):\n    \"\"\"Returns a list of python source files is target_dir\n\n    Parameters\n    ----------\n    target_dir : str\n        path to the directory where python source file are\n    Returns\n    -------\n    list\n        list of paths to all Python source files in `target_dir`\n    \"\"\"\n    return [os.path.join(target_dir, fname)\n            for fname in os.listdir(target_dir)\n            if fname.endswith('.py')]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_zipfiles(gallery_dir):\n\n    listdir = list_downloadable_sources(gallery_dir)\n    for directory in sorted(os.listdir(gallery_dir)):\n        if os.path.isdir(os.path.join(gallery_dir, directory)):\n            target_dir = os.path.join(gallery_dir, directory)\n            listdir.extend(list_downloadable_sources(target_dir))\n\n    py_zipfile = python_zip(listdir, gallery_dir)\n    jy_zipfile = python_zip(listdir, gallery_dir, \".ipynb\")\n\n    def rst_path(filepath):\n        return filepath.replace(os.sep, '/')\n\n    dw_rst = CODE_ZIP_DOWNLOAD.format(os.path.basename(py_zipfile),\n                                      rst_path(py_zipfile),\n                                      os.path.basename(jy_zipfile),\n                                      rst_path(jy_zipfile))\n    return dw_rst", "response": "Generates a zipfile for all Python source files and Jupyter notebooks in the gallery_dir and returns the contents of the generated files."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns reStructuredText code block from code string", "response": "def codestr2rst(codestr, lang='python', lineno=None):\n    \"\"\"Return reStructuredText code block from code string\"\"\"\n    if lineno is not None:\n        if LooseVersion(sphinx.__version__) >= '1.3':\n            # Sphinx only starts numbering from the first non-empty line.\n            blank_lines = codestr.count('\\n', 0, -len(codestr.lstrip()))\n            lineno = '   :lineno-start: {0}\\n'.format(lineno + blank_lines)\n        else:\n            lineno = '   :linenos:\\n'\n    else:\n        lineno = ''\n    code_directive = \"\\n.. code-block:: {0}\\n{1}\\n\".format(lang, lineno)\n    indented_block = indent(codestr, ' ' * 4)\n    return code_directive + indented_block"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_intro_and_title(filename, docstring):\n\n    # lstrip is just in case docstring has a '\\n\\n' at the beginning\n    paragraphs = docstring.lstrip().split('\\n\\n')\n    # remove comments and other syntax like `.. _link:`\n    paragraphs = [p for p in paragraphs\n                  if not p.startswith('.. ') and len(p) > 0]\n    if len(paragraphs) == 0:\n        raise ValueError(\n            \"Example docstring should have a header for the example title. \"\n            \"Please check the example file:\\n {}\\n\".format(filename))\n    # Title is the first paragraph with any ReSTructuredText title chars\n    # removed, i.e. lines that consist of (all the same) 7-bit non-ASCII chars.\n    # This conditional is not perfect but should hopefully be good enough.\n    title_paragraph = paragraphs[0]\n    match = re.search(r'([\\w ]+)', title_paragraph)\n\n    if match is None:\n        raise ValueError(\n            'Could not find a title in first paragraph:\\n{}'.format(\n                title_paragraph))\n    title = match.group(1).strip()\n    # Use the title if no other paragraphs are provided\n    intro_paragraph = title if len(paragraphs) < 2 else paragraphs[1]\n    # Concatenate all lines of the first paragraph and truncate at 95 chars\n    intro = re.sub('\\n', ' ', intro_paragraph)\n    if len(intro) > 95:\n        intro = intro[:95] + '...'\n\n    return intro, title", "response": "Extract the intro and title from the module - level docstring. max : 95 char"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether the md5sum of the source file is the same as the one on disk", "response": "def md5sum_is_current(src_file):\n    \"\"\"Checks whether src_file has the same md5 hash as the one on disk\"\"\"\n\n    src_md5 = get_md5sum(src_file)\n\n    src_md5_file = src_file + '.md5'\n    if os.path.exists(src_md5_file):\n        with open(src_md5_file, 'r') as file_checksum:\n            ref_md5 = file_checksum.read()\n\n        return src_md5 == ref_md5\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_thumbnail(image_path_template, src_file, file_conf, gallery_conf):\n    # read specification of the figure to display as thumbnail from main text\n    thumbnail_number = file_conf.get('thumbnail_number', 1)\n    if not isinstance(thumbnail_number, int):\n        raise TypeError(\n            'sphinx_gallery_thumbnail_number setting is not a number, '\n            'got %r' % (thumbnail_number,))\n    thumbnail_image_path, ext = _find_image_ext(image_path_template,\n                                                thumbnail_number)\n\n    thumb_dir = os.path.join(os.path.dirname(thumbnail_image_path), 'thumb')\n    if not os.path.exists(thumb_dir):\n        os.makedirs(thumb_dir)\n\n    base_image_name = os.path.splitext(os.path.basename(src_file))[0]\n    thumb_file = os.path.join(thumb_dir,\n                              'sphx_glr_%s_thumb.%s' % (base_image_name, ext))\n\n    if src_file in gallery_conf['failing_examples']:\n        img = os.path.join(glr_path_static(), 'broken_example.png')\n    elif os.path.exists(thumbnail_image_path):\n        img = thumbnail_image_path\n    elif not os.path.exists(thumb_file):\n        # create something to replace the thumbnail\n        img = os.path.join(glr_path_static(), 'no_image.png')\n        img = gallery_conf.get(\"default_thumb_file\", img)\n    else:\n        return\n    if ext == 'svg':\n        copyfile(img, thumb_file)\n    else:\n        scale_image(img, thumb_file, *gallery_conf[\"thumbnail_size\"])", "response": "Generate and Save the thumbnail image"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_dir_rst(src_dir, target_dir, gallery_conf, seen_backrefs):\n\n    head_ref = os.path.relpath(target_dir, gallery_conf['src_dir'])\n    fhindex = \"\"\"\\n\\n.. _sphx_glr_{0}:\\n\\n\"\"\".format(\n        head_ref.replace(os.path.sep, '_'))\n\n    with codecs.open(os.path.join(src_dir, 'README.txt'), 'r',\n                     encoding='utf-8') as fid:\n        fhindex += fid.read()\n    # Add empty lines to avoid bug in issue #165\n    fhindex += \"\\n\\n\"\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    # get filenames\n    listdir = [fname for fname in os.listdir(src_dir)\n               if fname.endswith('.py')]\n    # limit which to look at based on regex (similar to filename_pattern)\n    listdir = [fname for fname in listdir\n               if re.search(gallery_conf['ignore_pattern'],\n                            os.path.normpath(os.path.join(src_dir, fname)))\n               is None]\n    # sort them\n    sorted_listdir = sorted(\n        listdir, key=gallery_conf['within_subsection_order'](src_dir))\n    entries_text = []\n    computation_times = []\n    build_target_dir = os.path.relpath(target_dir, gallery_conf['src_dir'])\n    iterator = sphinx_compatibility.status_iterator(\n        sorted_listdir,\n        'generating gallery for %s... ' % build_target_dir,\n        length=len(sorted_listdir))\n    clean_modules(gallery_conf, src_dir)  # fix gh-316\n    for fname in iterator:\n        intro, time_elapsed = generate_file_rst(\n            fname, target_dir, src_dir, gallery_conf)\n        clean_modules(gallery_conf, fname)\n        src_file = os.path.normpath(os.path.join(src_dir, fname))\n        computation_times.append((time_elapsed, src_file))\n        this_entry = _thumbnail_div(target_dir, gallery_conf['src_dir'],\n                                    fname, intro) + \"\"\"\n\n.. toctree::\n   :hidden:\n\n   /%s\\n\"\"\" % os.path.join(build_target_dir, fname[:-3]).replace(os.sep, '/')\n        entries_text.append(this_entry)\n\n        if gallery_conf['backreferences_dir']:\n            write_backreferences(seen_backrefs, gallery_conf,\n                                 target_dir, fname, intro)\n\n    for entry_text in entries_text:\n        fhindex += entry_text\n\n    # clear at the end of the section\n    fhindex += \"\"\".. raw:: html\\n\n    <div style='clear:both'></div>\\n\\n\"\"\"\n\n    return fhindex, computation_times", "response": "Generate the reStructuredText for a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _memory_usage(func, gallery_conf):\n    if gallery_conf['show_memory']:\n        from memory_profiler import memory_usage\n        assert callable(func)\n        mem, out = memory_usage(func, max_usage=True, retval=True,\n                                multiprocess=True)\n        mem = mem[0]\n    else:\n        out = func()\n        mem = 0\n    return out, mem", "response": "Get memory usage of a function call."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the base amount of memory used by running a Python process.", "response": "def _get_memory_base(gallery_conf):\n    \"\"\"Get the base amount of memory used by running a Python process.\"\"\"\n    if not gallery_conf['show_memory']:\n        memory_base = 0\n    else:\n        # There might be a cleaner way to do this at some point\n        from memory_profiler import memory_usage\n        sleep, timeout = (1, 2) if sys.platform == 'win32' else (0.5, 1)\n        proc = subprocess.Popen(\n            [sys.executable, '-c',\n             'import time, sys; time.sleep(%s); sys.exit(0)' % sleep],\n            close_fds=True)\n        memories = memory_usage(proc, interval=1e-3, timeout=timeout)\n        kwargs = dict(timeout=timeout) if sys.version_info >= (3, 5) else {}\n        proc.communicate(**kwargs)\n        # On OSX sometimes the last entry can be None\n        memories = [mem for mem in memories if mem is not None] + [0.]\n        memory_base = max(memories)\n    return memory_base"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the code block of the example file and returns the code block that was executed.", "response": "def execute_code_block(compiler, block, example_globals,\n                       script_vars, gallery_conf):\n    \"\"\"Executes the code block of the example file\"\"\"\n    blabel, bcontent, lineno = block\n    # If example is not suitable to run, skip executing its blocks\n    if not script_vars['execute_script'] or blabel == 'text':\n        script_vars['memory_delta'].append(0)\n        return ''\n\n    cwd = os.getcwd()\n    # Redirect output to stdout and\n    orig_stdout = sys.stdout\n    src_file = script_vars['src_file']\n\n    # First cd in the original example dir, so that any file\n    # created by the example get created in this directory\n\n    my_stdout = MixedEncodingStringIO()\n    os.chdir(os.path.dirname(src_file))\n\n    sys_path = copy.deepcopy(sys.path)\n    sys.path.append(os.getcwd())\n    sys.stdout = LoggingTee(my_stdout, logger, src_file)\n\n    try:\n        dont_inherit = 1\n        code_ast = compile(bcontent, src_file, 'exec',\n                           ast.PyCF_ONLY_AST | compiler.flags, dont_inherit)\n        ast.increment_lineno(code_ast, lineno - 1)\n        # don't use unicode_literals at the top of this file or you get\n        # nasty errors here on Py2.7\n        _, mem = _memory_usage(_exec_once(\n            compiler(code_ast, src_file, 'exec'), example_globals),\n            gallery_conf)\n    except Exception:\n        sys.stdout.flush()\n        sys.stdout = orig_stdout\n        except_rst = handle_exception(sys.exc_info(), src_file, script_vars,\n                                      gallery_conf)\n        # python2.7: Code was read in bytes needs decoding to utf-8\n        # unless future unicode_literals is imported in source which\n        # make ast output unicode strings\n        if hasattr(except_rst, 'decode') and not \\\n                isinstance(except_rst, unicode):\n            except_rst = except_rst.decode('utf-8')\n\n        code_output = u\"\\n{0}\\n\\n\\n\\n\".format(except_rst)\n        # still call this even though we won't use the images so that\n        # figures are closed\n        save_figures(block, script_vars, gallery_conf)\n        mem = 0\n    else:\n        sys.stdout.flush()\n        sys.stdout = orig_stdout\n        sys.path = sys_path\n        os.chdir(cwd)\n\n        my_stdout = my_stdout.getvalue().strip().expandtabs()\n        if my_stdout:\n            stdout = CODE_OUTPUT.format(indent(my_stdout, u' ' * 4))\n        else:\n            stdout = ''\n        images_rst = save_figures(block, script_vars, gallery_conf)\n        code_output = u\"\\n{0}\\n\\n{1}\\n\\n\".format(images_rst, stdout)\n\n    finally:\n        os.chdir(cwd)\n        sys.path = sys_path\n        sys.stdout = orig_stdout\n    script_vars['memory_delta'].append(mem)\n\n    return code_output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates if script has to be executed according to gallery configuration", "response": "def executable_script(src_file, gallery_conf):\n    \"\"\"Validate if script has to be run according to gallery configuration\n\n    Parameters\n    ----------\n    src_file : str\n        path to python script\n\n    gallery_conf : dict\n        Contains the configuration of Sphinx-Gallery\n\n    Returns\n    -------\n    bool\n        True if script has to be executed\n    \"\"\"\n\n    filename_pattern = gallery_conf.get('filename_pattern')\n    execute = re.search(filename_pattern, src_file) and gallery_conf[\n        'plot_gallery']\n    return execute"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_script(script_blocks, script_vars, gallery_conf):\n\n    example_globals = {\n        # A lot of examples contains 'print(__doc__)' for example in\n        # scikit-learn so that running the example prints some useful\n        # information. Because the docstring has been separated from\n        # the code blocks in sphinx-gallery, __doc__ is actually\n        # __builtin__.__doc__ in the execution context and we do not\n        # want to print it\n        '__doc__': '',\n        # Examples may contain if __name__ == '__main__' guards\n        # for in example scikit-learn if the example uses multiprocessing\n        '__name__': '__main__',\n        # Don't ever support __file__: Issues #166 #212\n    }\n\n    argv_orig = sys.argv[:]\n    if script_vars['execute_script']:\n        # We want to run the example without arguments. See\n        # https://github.com/sphinx-gallery/sphinx-gallery/pull/252\n        # for more details.\n        sys.argv[0] = script_vars['src_file']\n        sys.argv[1:] = []\n\n    t_start = time()\n    gc.collect()\n    _, memory_start = _memory_usage(lambda: None, gallery_conf)\n    compiler = codeop.Compile()\n    # include at least one entry to avoid max() ever failing\n    script_vars['memory_delta'] = [memory_start]\n    output_blocks = [execute_code_block(compiler, block,\n                                        example_globals,\n                                        script_vars, gallery_conf)\n                     for block in script_blocks]\n    time_elapsed = time() - t_start\n    script_vars['memory_delta'] = (  # actually turn it into a delta now\n        max(script_vars['memory_delta']) - memory_start)\n\n    sys.argv = argv_orig\n\n    # Write md5 checksum if the example was meant to run (no-plot\n    # shall not cache md5sum) and has built correctly\n    if script_vars['execute_script']:\n        with open(script_vars['target_file'] + '.md5', 'w') as file_checksum:\n            file_checksum.write(get_md5sum(script_vars['target_file']))\n        gallery_conf['passing_examples'].append(script_vars['src_file'])\n\n    return output_blocks, time_elapsed", "response": "Execute and capture output from python script already in block structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the rst file for a given example.", "response": "def generate_file_rst(fname, target_dir, src_dir, gallery_conf):\n    \"\"\"Generate the rst file for a given example.\n\n    Parameters\n    ----------\n    fname : str\n        Filename of python script\n    target_dir : str\n        Absolute path to directory in documentation where examples are saved\n    src_dir : str\n        Absolute path to directory where source examples are stored\n    gallery_conf : dict\n        Contains the configuration of Sphinx-Gallery\n\n    Returns\n    -------\n    intro: str\n        The introduction of the example\n    time_elapsed : float\n        seconds required to run the script\n    \"\"\"\n    src_file = os.path.normpath(os.path.join(src_dir, fname))\n    target_file = os.path.join(target_dir, fname)\n    _replace_md5(src_file, target_file, 'copy')\n\n    intro, _ = extract_intro_and_title(fname,\n                                       get_docstring_and_rest(src_file)[0])\n\n    executable = executable_script(src_file, gallery_conf)\n\n    if md5sum_is_current(target_file):\n        if executable:\n            gallery_conf['stale_examples'].append(target_file)\n        return intro, 0\n\n    image_dir = os.path.join(target_dir, 'images')\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n\n    base_image_name = os.path.splitext(fname)[0]\n    image_fname = 'sphx_glr_' + base_image_name + '_{0:03}.png'\n    image_path_template = os.path.join(image_dir, image_fname)\n\n    script_vars = {\n        'execute_script': executable,\n        'image_path_iterator': ImagePathIterator(image_path_template),\n        'src_file': src_file,\n        'target_file': target_file}\n\n    file_conf, script_blocks = split_code_and_text_blocks(src_file)\n    output_blocks, time_elapsed = execute_script(script_blocks,\n                                                 script_vars,\n                                                 gallery_conf)\n\n    logger.debug(\"%s ran in : %.2g seconds\\n\", src_file, time_elapsed)\n\n    example_rst = rst_blocks(script_blocks, output_blocks,\n                             file_conf, gallery_conf)\n    memory_used = gallery_conf['memory_base'] + script_vars['memory_delta']\n    if not executable:\n        time_elapsed = memory_used = 0.  # don't let the output change\n    save_rst_example(example_rst, target_file, time_elapsed, memory_used,\n                     gallery_conf)\n\n    save_thumbnail(image_path_template, src_file, file_conf, gallery_conf)\n\n    example_nb = jupyter_notebook(script_blocks, gallery_conf)\n    ipy_fname = replace_py_ipynb(target_file) + '.new'\n    save_notebook(example_nb, ipy_fname)\n    _replace_md5(ipy_fname)\n\n    return intro, time_elapsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the rst string containing the script prose code and output", "response": "def rst_blocks(script_blocks, output_blocks, file_conf, gallery_conf):\n    \"\"\"Generates the rst string containing the script prose, code and output\n\n    Parameters\n    ----------\n    script_blocks : list\n        (label, content, line_number)\n        List where each element is a tuple with the label ('text' or 'code'),\n        the corresponding content string of block and the leading line number\n    output_blocks : list\n        List of strings where each element is the restructured text\n        representation of the output of each block\n    file_conf : dict\n        File-specific settings given in source file comments as:\n        ``# sphinx_gallery_<name> = <value>``\n    gallery_conf : dict\n        Contains the configuration of Sphinx-Gallery\n\n    Returns\n    -------\n    out : str\n        rst notebook\n    \"\"\"\n\n    # A simple example has two blocks: one for the\n    # example introduction/explanation and one for the code\n    is_example_notebook_like = len(script_blocks) > 2\n    example_rst = u\"\"  # there can be unicode content\n    for (blabel, bcontent, lineno), code_output in \\\n            zip(script_blocks, output_blocks):\n        if blabel == 'code':\n\n            if not file_conf.get('line_numbers',\n                                 gallery_conf.get('line_numbers', False)):\n                lineno = None\n\n            code_rst = codestr2rst(bcontent, lang=gallery_conf['lang'],\n                                   lineno=lineno) + '\\n'\n            if is_example_notebook_like:\n                example_rst += code_rst\n                example_rst += code_output\n            else:\n                example_rst += code_output\n                if 'sphx-glr-script-out' in code_output:\n                    # Add some vertical space after output\n                    example_rst += \"\\n\\n|\\n\\n\"\n                example_rst += code_rst\n        else:\n            block_separator = '\\n\\n' if not bcontent.endswith('\\n') else '\\n'\n            example_rst += bcontent + block_separator\n    return example_rst"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_rst_example(example_rst, example_file, time_elapsed,\n                     memory_used, gallery_conf):\n    \"\"\"Saves the rst notebook to example_file including header & footer\n\n    Parameters\n    ----------\n    example_rst : str\n        rst containing the executed file content\n    example_file : str\n        Filename with full path of python example file in documentation folder\n    time_elapsed : float\n        Time elapsed in seconds while executing file\n    memory_used : float\n        Additional memory used during the run.\n    gallery_conf : dict\n        Sphinx-Gallery configuration dictionary\n    \"\"\"\n\n    ref_fname = os.path.relpath(example_file, gallery_conf['src_dir'])\n    ref_fname = ref_fname.replace(os.path.sep, \"_\")\n\n    binder_conf = check_binder_conf(gallery_conf.get('binder'))\n\n    binder_text = (\" or run this example in your browser via Binder\"\n                   if len(binder_conf) else \"\")\n    example_rst = (\".. note::\\n\"\n                   \"    :class: sphx-glr-download-link-note\\n\\n\"\n                   \"    Click :ref:`here <sphx_glr_download_{0}>` \"\n                   \"to download the full example code{1}\\n\"\n                   \".. rst-class:: sphx-glr-example-title\\n\\n\"\n                   \".. _sphx_glr_{0}:\\n\\n\"\n                   ).format(ref_fname, binder_text) + example_rst\n\n    if time_elapsed >= gallery_conf[\"min_reported_time\"]:\n        time_m, time_s = divmod(time_elapsed, 60)\n        example_rst += TIMING_CONTENT.format(time_m, time_s)\n    if gallery_conf['show_memory']:\n        example_rst += (\"**Estimated memory usage:** {0: .0f} MB\\n\\n\"\n                        .format(memory_used))\n\n    # Generate a binder URL if specified\n    binder_badge_rst = ''\n    if len(binder_conf) > 0:\n        binder_badge_rst += gen_binder_rst(example_file, binder_conf,\n                                           gallery_conf)\n\n    fname = os.path.basename(example_file)\n    example_rst += CODE_DOWNLOAD.format(fname,\n                                        replace_py_ipynb(fname),\n                                        binder_badge_rst,\n                                        ref_fname)\n    example_rst += SPHX_GLR_SIG\n\n    write_file_new = re.sub(r'\\.py$', '.rst.new', example_file)\n    with codecs.open(write_file_new, 'w', encoding=\"utf-8\") as f:\n        f.write(example_rst)\n    # in case it wasn't in our pattern, only replace the file if it's\n    # still stale.\n    _replace_md5(write_file_new)", "response": "Saves the rst notebook to example_file including header & footer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_data(url):\n    if urllib_parse.urlparse(url).scheme in ('http', 'https'):\n        resp = urllib_request.urlopen(url)\n        encoding = resp.headers.get('content-encoding', 'plain')\n        data = resp.read()\n        if encoding == 'plain':\n            data = data.decode('utf-8')\n        elif encoding == 'gzip':\n            data = BytesIO(data)\n            data = gzip.GzipFile(fileobj=data).read().decode('utf-8')\n        else:\n            raise RuntimeError('unknown encoding')\n    else:\n        with codecs.open(url, mode='r', encoding='utf-8') as fid:\n            data = fid.read()\n\n    return data", "response": "Helper function to get data over http or local file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the Sphinx index for documentation options.", "response": "def parse_sphinx_docopts(index):\n    \"\"\"\n    Parse the Sphinx index for documentation options.\n\n    Parameters\n    ----------\n    index : str\n        The Sphinx index page\n\n    Returns\n    -------\n    docopts : dict\n        The documentation options from the page.\n    \"\"\"\n\n    pos = index.find('var DOCUMENTATION_OPTIONS')\n    if pos < 0:\n        raise ValueError('Documentation options could not be found in index.')\n    pos = index.find('{', pos)\n    if pos < 0:\n        raise ValueError('Documentation options could not be found in index.')\n    endpos = index.find('};', pos)\n    if endpos < 0:\n        raise ValueError('Documentation options could not be found in index.')\n    block = index[pos + 1:endpos].strip()\n    docopts = {}\n    for line in block.splitlines():\n        key, value = line.split(':', 1)\n        key = key.strip().strip('\"')\n\n        value = value.strip()\n        if value[-1] == ',':\n            value = value[:-1].rstrip()\n        if value[0] in '\"\\'':\n            value = value[1:-1]\n        elif value == 'false':\n            value = False\n        elif value == 'true':\n            value = True\n        else:\n            try:\n                value = int(value)\n            except ValueError:\n                # In Sphinx 1.7.5, URL_ROOT is a JavaScript fragment.\n                # Ignoring this entry since URL_ROOT is not used\n                # elsewhere.\n                # https://github.com/sphinx-gallery/sphinx-gallery/issues/382\n                continue\n\n        docopts[key] = value\n\n    return docopts"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nembeds hyperlinks to documentation into example code", "response": "def embed_code_links(app, exception):\n    \"\"\"Embed hyperlinks to documentation into example code\"\"\"\n    if exception is not None:\n        return\n\n    # No need to waste time embedding hyperlinks when not running the examples\n    # XXX: also at the time of writing this fixes make html-noplot\n    # for some reason I don't fully understand\n    if not app.builder.config.plot_gallery:\n        return\n\n    # XXX: Whitelist of builders for which it makes sense to embed\n    # hyperlinks inside the example html. Note that the link embedding\n    # require searchindex.js to exist for the links to the local doc\n    # and there does not seem to be a good way of knowing which\n    # builders creates a searchindex.js.\n    if app.builder.name not in ['html', 'readthedocs']:\n        return\n\n    logger.info('embedding documentation hyperlinks...', color='white')\n\n    gallery_conf = app.config.sphinx_gallery_conf\n\n    gallery_dirs = gallery_conf['gallery_dirs']\n    if not isinstance(gallery_dirs, list):\n        gallery_dirs = [gallery_dirs]\n\n    for gallery_dir in gallery_dirs:\n        _embed_code_links(app, gallery_conf, gallery_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_link(self, cobj):\n\n        fullname = cobj['module_short'] + '.' + cobj['name']\n        try:\n            value = self._searchindex['objects'][cobj['module_short']]\n            match = value[cobj['name']]\n        except KeyError:\n            link = False\n        else:\n            fname_idx = match[0]\n            objname_idx = str(match[1])\n            anchor = match[3]\n\n            fname = self._searchindex['filenames'][fname_idx]\n            # In 1.5+ Sphinx seems to have changed from .rst.html to only\n            # .html extension in converted files. Find this from the options.\n            ext = self._docopts.get('FILE_SUFFIX', '.rst.html')\n            fname = os.path.splitext(fname)[0] + ext\n            if self._is_windows:\n                fname = fname.replace('/', '\\\\')\n                link = os.path.join(self.doc_url, fname)\n            else:\n                link = posixpath.join(self.doc_url, fname)\n\n            if anchor == '':\n                anchor = fullname\n            elif anchor == '-':\n                anchor = (self._searchindex['objnames'][objname_idx][1] + '-' +\n                          fullname)\n\n            link = link + '#' + anchor\n\n        return link", "response": "Get a valid link for the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve(self, cobj, this_url):\n        full_name = cobj['module_short'] + '.' + cobj['name']\n        link = self._link_cache.get(full_name, None)\n        if link is None:\n            # we don't have it cached\n            link = self._get_link(cobj)\n            # cache it for the future\n            self._link_cache[full_name] = link\n\n        if link is False or link is None:\n            # failed to resolve\n            return None\n\n        if self.relative:\n            link = os.path.relpath(link, start=this_url)\n            if self._is_windows:\n                # replace '\\' with '/' so it on the web\n                link = link.replace('\\\\', '/')\n\n            # for some reason, the relative link goes one directory too high up\n            link = link[3:]\n\n        return link", "response": "Resolves the link to the documentation returns None if not found"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn absolute path to packaged static files", "response": "def glr_path_static():\n    \"\"\"Returns path to packaged static files\"\"\"\n    return os.path.abspath(os.path.join(os.path.dirname(__file__), '_static'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_binder_url(fpath, binder_conf, gallery_conf):\n    # Build the URL\n    fpath_prefix = binder_conf.get('filepath_prefix')\n    link_base = binder_conf.get('notebooks_dir')\n\n    # We want to keep the relative path to sub-folders\n    relative_link = os.path.relpath(fpath, gallery_conf['src_dir'])\n    path_link = os.path.join(\n        link_base, replace_py_ipynb(relative_link))\n\n    # In case our website is hosted in a sub-folder\n    if fpath_prefix is not None:\n        path_link = '/'.join([fpath_prefix.strip('/'), path_link])\n\n    # Make sure we have the right slashes (in case we're on Windows)\n    path_link = path_link.replace(os.path.sep, '/')\n\n    # Create the URL\n    binder_url = binder_conf['binderhub_url']\n    binder_url = '/'.join([binder_conf['binderhub_url'],\n                           'v2', 'gh',\n                           binder_conf['org'],\n                           binder_conf['repo'],\n                           binder_conf['branch']])\n\n    if binder_conf.get('use_jupyter_lab', False) is True:\n        binder_url += '?urlpath=lab/tree/{}'.format(path_link)\n    else:\n        binder_url += '?filepath={}'.format(path_link)\n    return binder_url", "response": "Generate a Binder URL according to the configuration in conf. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_binder_rst(fpath, binder_conf, gallery_conf):\n    binder_conf = check_binder_conf(binder_conf)\n    binder_url = gen_binder_url(fpath, binder_conf, gallery_conf)\n\n    rst = (\n        \"\\n\"\n        \"  .. container:: binder-badge\\n\\n\"\n        \"    .. image:: https://mybinder.org/badge_logo.svg\\n\"\n        \"      :target: {}\\n\"\n        \"      :width: 150 px\\n\").format(binder_url)\n    return rst", "response": "Generate the RST + link for the Binder badge."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies all Binder requirements and notebooks files.", "response": "def copy_binder_files(app, exception):\n    \"\"\"Copy all Binder requirements and notebooks files.\"\"\"\n    if exception is not None:\n        return\n\n    if app.builder.name not in ['html', 'readthedocs']:\n        return\n\n    gallery_conf = app.config.sphinx_gallery_conf\n    binder_conf = check_binder_conf(gallery_conf.get('binder'))\n\n    if not len(binder_conf) > 0:\n        return\n\n    logger.info('copying binder requirements...', color='white')\n    _copy_binder_reqs(app, binder_conf)\n    _copy_binder_notebooks(app)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy the Binder requirements files to a Binder folder in the docs.", "response": "def _copy_binder_reqs(app, binder_conf):\n    \"\"\"Copy Binder requirements files to a \"binder\" folder in the docs.\"\"\"\n    path_reqs = binder_conf.get('dependencies')\n    for path in path_reqs:\n        if not os.path.exists(os.path.join(app.srcdir, path)):\n            raise ValueError((\"Couldn't find the Binder requirements file: {}, \"\n                              \"did you specify the path correctly?\".format(path)))\n\n    binder_folder = os.path.join(app.outdir, 'binder')\n    if not os.path.isdir(binder_folder):\n        os.makedirs(binder_folder)\n\n    # Copy over the requirements to the output directory\n    for path in path_reqs:\n        shutil.copy(os.path.join(app.srcdir, path), binder_folder)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a list of files in contents remove all files named ipynb or directories named images and return the result.", "response": "def _remove_ipynb_files(path, contents):\n    \"\"\"Given a list of files in `contents`, remove all files named `ipynb` or\n    directories named `images` and return the result.\n\n    Used with the `shutil` \"ignore\" keyword to filter out non-ipynb files.\"\"\"\n    contents_return = []\n    for entry in contents:\n        if entry.endswith('.ipynb'):\n            # Don't include ipynb files\n            pass\n        elif (entry != \"images\") and os.path.isdir(os.path.join(path, entry)):\n            # Don't include folders not called \"images\"\n            pass\n        else:\n            # Keep everything else\n            contents_return.append(entry)\n    return contents_return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy Jupyter notebooks to the binder notebooks directory.", "response": "def _copy_binder_notebooks(app):\n    \"\"\"Copy Jupyter notebooks to the binder notebooks directory.\n\n    Copy each output gallery directory structure but only including the\n    Jupyter notebook files.\"\"\"\n\n    gallery_conf = app.config.sphinx_gallery_conf\n    gallery_dirs = gallery_conf.get('gallery_dirs')\n    binder_conf = gallery_conf.get('binder')\n    notebooks_dir = os.path.join(app.outdir, binder_conf.get('notebooks_dir'))\n    shutil.rmtree(notebooks_dir, ignore_errors=True)\n    os.makedirs(notebooks_dir)\n\n    if not isinstance(gallery_dirs, (list, tuple)):\n        gallery_dirs = [gallery_dirs]\n\n    iterator = sphinx_compatibility.status_iterator(\n        gallery_dirs, 'copying binder notebooks...', length=len(gallery_dirs))\n\n    for i_folder in iterator:\n        shutil.copytree(os.path.join(app.srcdir, i_folder),\n                        os.path.join(notebooks_dir, i_folder),\n                        ignore=_remove_ipynb_files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks to make sure that the Binder configuration is correct.", "response": "def check_binder_conf(binder_conf):\n    \"\"\"Check to make sure that the Binder configuration is correct.\"\"\"\n    # Grab the configuration and return None if it's not configured\n    binder_conf = {} if binder_conf is None else binder_conf\n    if not isinstance(binder_conf, dict):\n        raise ValueError('`binder_conf` must be a dictionary or None.')\n    if len(binder_conf) == 0:\n        return binder_conf\n\n    if binder_conf.get('url') and not binder_conf.get('binderhub_url'):\n        logger.warning(\n            'Found old BinderHub URL keyword (\"url\"). Please update your '\n            'configuration to use the new keyword (\"binderhub_url\"). \"url\" will be '\n            'deprecated in sphinx-gallery v0.4')\n        binder_conf['binderhub_url'] = binderhub_conf.get('url')\n\n    # Ensure all fields are populated\n    req_values = ['binderhub_url', 'org', 'repo', 'branch', 'dependencies']\n    optional_values = ['filepath_prefix', 'notebooks_dir', 'use_jupyter_lab']\n    missing_values = []\n    for val in req_values:\n        if binder_conf.get(val) is None:\n            missing_values.append(val)\n\n    if len(missing_values) > 0:\n        raise ValueError('binder_conf is missing values for: {}'.format(\n            missing_values))\n\n    for key in binder_conf.keys():\n        if key not in (req_values + optional_values):\n            raise ValueError(\"Unknown Binder config key: {}\".format(key))\n\n    # Ensure we have http in the URL\n    if not any(binder_conf['binderhub_url'].startswith(ii)\n               for ii in ['http://', 'https://']):\n        raise ValueError('did not supply a valid url, '\n                         'gave binderhub_url: {}'.format(binder_conf['binderhub_url']))\n\n    # Ensure we have at least one dependency file\n    # Need at least one of these three files\n    required_reqs_files = ['requirements.txt', 'environment.yml', 'Dockerfile']\n    path_reqs = binder_conf['dependencies']\n    if isinstance(path_reqs, basestring):\n        path_reqs = [path_reqs]\n        binder_conf['dependencies'] = path_reqs\n    elif not isinstance(path_reqs, (list, tuple)):\n        raise ValueError(\"`dependencies` value should be a list of strings. \"\n                         \"Got type {}.\".format(type(path_reqs)))\n\n    binder_conf['notebooks_dir'] = binder_conf.get('notebooks_dir',\n                                                   'notebooks')\n    path_reqs_filenames = [os.path.basename(ii) for ii in path_reqs]\n    if not any(ii in path_reqs_filenames for ii in required_reqs_files):\n        raise ValueError(\n            'Did not find one of `requirements.txt` or `environment.yml` '\n            'in the \"dependencies\" section of the binder configuration '\n            'for sphinx-gallery. A path to at least one of these files '\n            'must exist in your Binder dependencies.')\n    return binder_conf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_source_file(filename):\n\n    # can't use codecs.open(filename, 'r', 'utf-8') here b/c ast doesn't\n    # work with unicode strings in Python2.7 \"SyntaxError: encoding\n    # declaration in Unicode string\" In python 2.7 the string can't be\n    # encoded and have information about its encoding. That is particularly\n    # problematic since source files include in their header information\n    # about the file encoding.\n    # Minimal example to fail: ast.parse(u'# -*- coding: utf-8 -*-')\n\n    with open(filename, 'rb') as fid:\n        content = fid.read()\n    # change from Windows format to UNIX for uniformity\n    content = content.replace(b'\\r\\n', b'\\n')\n\n    try:\n        node = ast.parse(content)\n        return node, content.decode('utf-8')\n    except SyntaxError:\n        return None, content.decode('utf-8')", "response": "Parse a source file into an AST node and its content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nseparate filename content between docstring and rest", "response": "def get_docstring_and_rest(filename):\n    \"\"\"Separate ``filename`` content between docstring and the rest\n\n    Strongly inspired from ast.get_docstring.\n\n    Returns\n    -------\n    docstring : str\n        docstring of ``filename``\n    rest : str\n        ``filename`` content without the docstring\n    \"\"\"\n    node, content = parse_source_file(filename)\n\n    if node is None:\n        return SYNTAX_ERROR_DOCSTRING, content, 1\n\n    if not isinstance(node, ast.Module):\n        raise TypeError(\"This function only supports modules. \"\n                        \"You provided {0}\".format(node.__class__.__name__))\n    if not (node.body and isinstance(node.body[0], ast.Expr) and\n            isinstance(node.body[0].value, ast.Str)):\n        raise ValueError(('Could not find docstring in file \"{0}\". '\n                          'A docstring is required by sphinx-gallery '\n                          'unless the file is ignored by \"ignore_pattern\"')\n                         .format(filename))\n\n    if LooseVersion(sys.version) >= LooseVersion('3.7'):\n        docstring = ast.get_docstring(node)\n        assert docstring is not None  # should be guaranteed above\n        # This is just for backward compat\n        if len(node.body[0].value.s) and node.body[0].value.s[0] == '\\n':\n            # just for strict backward compat here\n            docstring = '\\n' + docstring\n        ts = tokenize.tokenize(BytesIO(content.encode()).readline)\n        # find the first string according to the tokenizer and get its end row\n        for tk in ts:\n            if tk.exact_type == 3:\n                lineno, _ = tk.end\n                break\n        else:\n            lineno = 0\n    else:\n        # this block can be removed when python 3.6 support is dropped\n        docstring_node = node.body[0]\n        docstring = docstring_node.value.s\n        # python2.7: Code was read in bytes needs decoding to utf-8\n        # unless future unicode_literals is imported in source which\n        # make ast output unicode strings\n        if hasattr(docstring, 'decode') and not isinstance(docstring, unicode):\n            docstring = docstring.decode('utf-8')\n        lineno = docstring_node.lineno  # The last line of the string.\n\n    # This get the content of the file after the docstring last line\n    # Note: 'maxsplit' argument is not a keyword argument in python2\n    rest = '\\n'.join(content.split('\\n')[lineno:])\n    lineno += 1\n    return docstring, rest, lineno"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the file - specific config from the docstring.", "response": "def extract_file_config(content):\n    \"\"\"\n    Pull out the file-specific config specified in the docstring.\n    \"\"\"\n\n    prop_pat = re.compile(\n        r\"^\\s*#\\s*sphinx_gallery_([A-Za-z0-9_]+)\\s*=\\s*(.+)\\s*$\",\n        re.MULTILINE)\n    file_conf = {}\n    for match in re.finditer(prop_pat, content):\n        name = match.group(1)\n        value = match.group(2)\n        try:\n            value = ast.literal_eval(value)\n        except (SyntaxError, ValueError):\n            logger.warning(\n                'Sphinx-gallery option %s was passed invalid value %s',\n                name, value)\n        else:\n            file_conf[name] = value\n    return file_conf"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns list with source file separated into code and text blocks.", "response": "def split_code_and_text_blocks(source_file):\n    \"\"\"Return list with source file separated into code and text blocks.\n\n    Returns\n    -------\n    file_conf : dict\n        File-specific settings given in source file comments as:\n        ``# sphinx_gallery_<name> = <value>``\n    blocks : list\n        (label, content, line_number)\n        List where each element is a tuple with the label ('text' or 'code'),\n        the corresponding content string of block and the leading line number\n    \"\"\"\n    docstring, rest_of_content, lineno = get_docstring_and_rest(source_file)\n    blocks = [('text', docstring, 1)]\n\n    file_conf = extract_file_config(rest_of_content)\n\n    pattern = re.compile(\n        r'(?P<header_line>^#{20,}.*)\\s(?P<text_content>(?:^#.*\\s)*)',\n        flags=re.M)\n    sub_pat = re.compile('^#', flags=re.M)\n\n    pos_so_far = 0\n    for match in re.finditer(pattern, rest_of_content):\n        code_block_content = rest_of_content[pos_so_far:match.start()]\n        if code_block_content.strip():\n            blocks.append(('code', code_block_content, lineno))\n        lineno += code_block_content.count('\\n')\n\n        lineno += 1  # Ignored header line of hashes.\n        text_content = match.group('text_content')\n        text_block_content = dedent(re.sub(sub_pat, '', text_content)).lstrip()\n        if text_block_content.strip():\n            blocks.append(('text', text_block_content, lineno))\n        lineno += text_content.count('\\n')\n\n        pos_so_far = match.end()\n\n    remaining_content = rest_of_content[pos_so_far:]\n    if remaining_content.strip():\n        blocks.append(('code', remaining_content, lineno))\n\n    return file_conf, blocks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_config(app):\n    try:\n        plot_gallery = eval(app.builder.config.plot_gallery)\n    except TypeError:\n        plot_gallery = bool(app.builder.config.plot_gallery)\n    src_dir = app.builder.srcdir\n    abort_on_example_error = app.builder.config.abort_on_example_error\n    lang = app.builder.config.highlight_language\n    gallery_conf = _complete_gallery_conf(\n        app.config.sphinx_gallery_conf, src_dir, plot_gallery,\n        abort_on_example_error, lang, app.builder.name)\n\n    # this assures I can call the config in other places\n    app.config.sphinx_gallery_conf = gallery_conf\n    app.config.html_static_path.append(glr_path_static())\n    return gallery_conf", "response": "Process the Sphinx Gallery configuration"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of subsections of a gallery", "response": "def get_subsections(srcdir, examples_dir, sortkey):\n    \"\"\"Return the list of subsections of a gallery\n\n    Parameters\n    ----------\n    srcdir : str\n        absolute path to directory containing conf.py\n    examples_dir : str\n        path to the examples directory relative to conf.py\n    sortkey : callable\n        The sort key to use.\n\n    Returns\n    -------\n    out : list\n        sorted list of gallery subsection folder names\n\n    \"\"\"\n    subfolders = [subfolder for subfolder in os.listdir(examples_dir)\n                  if os.path.exists(os.path.join(\n                      examples_dir, subfolder, 'README.txt'))]\n    base_examples_dir_path = os.path.relpath(examples_dir, srcdir)\n    subfolders_with_path = [os.path.join(base_examples_dir_path, item)\n                            for item in subfolders]\n    sorted_subfolders = sorted(subfolders_with_path, key=sortkey)\n\n    return [subfolders[i] for i in [subfolders_with_path.index(item)\n                                    for item in sorted_subfolders]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates necessary folders for sphinx_gallery files", "response": "def _prepare_sphx_glr_dirs(gallery_conf, srcdir):\n    \"\"\"Creates necessary folders for sphinx_gallery files \"\"\"\n    examples_dirs = gallery_conf['examples_dirs']\n    gallery_dirs = gallery_conf['gallery_dirs']\n\n    if not isinstance(examples_dirs, list):\n        examples_dirs = [examples_dirs]\n\n    if not isinstance(gallery_dirs, list):\n        gallery_dirs = [gallery_dirs]\n\n    if bool(gallery_conf['backreferences_dir']):\n        backreferences_dir = os.path.join(\n            srcdir, gallery_conf['backreferences_dir'])\n        if not os.path.exists(backreferences_dir):\n            os.makedirs(backreferences_dir)\n\n    return list(zip(examples_dirs, gallery_dirs))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_gallery_rst(app):\n    logger.info('generating gallery...', color='white')\n    gallery_conf = parse_config(app)\n\n    seen_backrefs = set()\n\n    computation_times = []\n    workdirs = _prepare_sphx_glr_dirs(gallery_conf,\n                                      app.builder.srcdir)\n\n    # Check for duplicate filenames to make sure linking works as expected\n    examples_dirs = [ex_dir for ex_dir, _ in workdirs]\n    files = collect_gallery_files(examples_dirs)\n    check_duplicate_filenames(files)\n\n    for examples_dir, gallery_dir in workdirs:\n\n        examples_dir = os.path.join(app.builder.srcdir, examples_dir)\n        gallery_dir = os.path.join(app.builder.srcdir, gallery_dir)\n\n        if not os.path.exists(os.path.join(examples_dir, 'README.txt')):\n            raise FileNotFoundError(\"Main example directory {0} does not \"\n                                    \"have a README.txt file. Please write \"\n                                    \"one to introduce your gallery.\"\n                                    .format(examples_dir))\n\n        # Here we don't use an os.walk, but we recurse only twice: flat is\n        # better than nested.\n\n        this_fhindex, this_computation_times = generate_dir_rst(\n            examples_dir, gallery_dir, gallery_conf, seen_backrefs)\n\n        computation_times += this_computation_times\n        write_computation_times(gallery_conf, gallery_dir,\n                                this_computation_times)\n\n        # we create an index.rst with all examples\n        index_rst_new = os.path.join(gallery_dir, 'index.rst.new')\n        with codecs.open(index_rst_new, 'w', encoding='utf-8') as fhindex:\n            # :orphan: to suppress \"not included in TOCTREE\" sphinx warnings\n            fhindex.write(\":orphan:\\n\\n\" + this_fhindex)\n\n            for subsection in get_subsections(\n                    app.builder.srcdir, examples_dir,\n                    gallery_conf['subsection_order']):\n                src_dir = os.path.join(examples_dir, subsection)\n                target_dir = os.path.join(gallery_dir, subsection)\n                this_fhindex, this_computation_times = \\\n                    generate_dir_rst(src_dir, target_dir, gallery_conf,\n                                     seen_backrefs)\n                fhindex.write(this_fhindex)\n                computation_times += this_computation_times\n                write_computation_times(gallery_conf, target_dir,\n                                        this_computation_times)\n\n            if gallery_conf['download_all_examples']:\n                download_fhindex = generate_zipfiles(gallery_dir)\n                fhindex.write(download_fhindex)\n\n            fhindex.write(SPHX_GLR_SIG)\n        _replace_md5(index_rst_new)\n    finalize_backreferences(seen_backrefs, gallery_conf)\n\n    if gallery_conf['plot_gallery']:\n        logger.info(\"computation time summary:\", color='white')\n        for time_elapsed, fname in sorted(computation_times, reverse=True):\n            fname = os.path.relpath(fname,\n                                    os.path.normpath(gallery_conf['src_dir']))\n            if time_elapsed is not None:\n                if time_elapsed >= gallery_conf['min_reported_time']:\n                    logger.info(\"    - %s: %.2g sec\", fname, time_elapsed)\n            else:\n                logger.info(\"    - %s: not run\", fname)\n        # Also create a junit.xml file, useful e.g. on CircleCI\n        write_junit_xml(gallery_conf, app.builder.outdir, computation_times)", "response": "Generate the main examples gallery reStructuredText"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a number of seconds to a more readable representation.", "response": "def _sec_to_readable(t):\n    \"\"\"Convert a number of seconds to a more readable representation.\"\"\"\n    # This will only work for < 1 day execution time\n    # And we reserve 2 digits for minutes because presumably\n    # there aren't many > 99 minute scripts, but occasionally some\n    # > 9 minute ones\n    t = datetime(1, 1, 1) + timedelta(seconds=t)\n    t = '{0:02d}:{1:02d}.{2:03d}'.format(\n        t.hour * 60 + t.minute, t.second,\n        int(round(t.microsecond / 1000.)))\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef touch_empty_backreferences(app, what, name, obj, options, lines):\n\n    if not bool(app.config.sphinx_gallery_conf['backreferences_dir']):\n        return\n\n    examples_path = os.path.join(app.srcdir,\n                                 app.config.sphinx_gallery_conf[\n                                     \"backreferences_dir\"],\n                                 \"%s.examples\" % name)\n\n    if not os.path.exists(examples_path):\n        # touch file\n        open(examples_path, 'w').close()", "response": "Generate empty back - reference example files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit the failures into two lists.", "response": "def _parse_failures(gallery_conf):\n    \"\"\"Split the failures.\"\"\"\n    failing_examples = set(gallery_conf['failing_examples'].keys())\n    expected_failing_examples = set(\n        os.path.normpath(os.path.join(gallery_conf['src_dir'], path))\n        for path in gallery_conf['expected_failing_examples'])\n    failing_as_expected = failing_examples.intersection(\n        expected_failing_examples)\n    failing_unexpectedly = failing_examples.difference(\n        expected_failing_examples)\n    passing_unexpectedly = expected_failing_examples.difference(\n        failing_examples)\n    # filter from examples actually run\n    passing_unexpectedly = [\n        src_file for src_file in passing_unexpectedly\n        if re.search(gallery_conf.get('filename_pattern'), src_file)]\n    return failing_as_expected, failing_unexpectedly, passing_unexpectedly"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef summarize_failing_examples(app, exception):\n    if exception is not None:\n        return\n\n    # Under no-plot Examples are not run so nothing to summarize\n    if not app.config.sphinx_gallery_conf['plot_gallery']:\n        logger.info('Sphinx-gallery gallery_conf[\"plot_gallery\"] was '\n                    'False, so no examples were executed.', color='brown')\n        return\n\n    gallery_conf = app.config.sphinx_gallery_conf\n    failing_as_expected, failing_unexpectedly, passing_unexpectedly = \\\n        _parse_failures(gallery_conf)\n\n    if failing_as_expected:\n        logger.info(\"Examples failing as expected:\", color='brown')\n        for fail_example in failing_as_expected:\n            logger.info('%s failed leaving traceback:', fail_example,\n                        color='brown')\n            logger.info(gallery_conf['failing_examples'][fail_example],\n                        color='brown')\n\n    fail_msgs = []\n    if failing_unexpectedly:\n        fail_msgs.append(red(\"Unexpected failing examples:\"))\n        for fail_example in failing_unexpectedly:\n            fail_msgs.append(fail_example + ' failed leaving traceback:\\n' +\n                             gallery_conf['failing_examples'][fail_example] +\n                             '\\n')\n\n    if passing_unexpectedly:\n        fail_msgs.append(red(\"Examples expected to fail, but not failing:\\n\") +\n                         \"Please remove these examples from\\n\" +\n                         \"sphinx_gallery_conf['expected_failing_examples']\\n\" +\n                         \"in your conf.py file\"\n                         \"\\n\".join(passing_unexpectedly))\n\n    # standard message\n    n_good = len(gallery_conf['passing_examples'])\n    n_tot = len(gallery_conf['failing_examples']) + n_good\n    n_stale = len(gallery_conf['stale_examples'])\n    logger.info('\\nSphinx-gallery successfully executed %d out of %d '\n                'file%s subselected by:\\n\\n'\n                '    gallery_conf[\"filename_pattern\"] = %r\\n'\n                '    gallery_conf[\"ignore_pattern\"]   = %r\\n'\n                '\\nafter excluding %d file%s that had previously been run '\n                '(based on MD5).\\n'\n                % (n_good, n_tot, 's' if n_tot != 1 else '',\n                   gallery_conf['filename_pattern'],\n                   gallery_conf['ignore_pattern'],\n                   n_stale, 's' if n_stale != 1 else '',\n                   ),\n                color='brown')\n\n    if fail_msgs:\n        raise ValueError(\"Here is a summary of the problems encountered when \"\n                         \"running the examples\\n\\n\" + \"\\n\".join(fail_msgs) +\n                         \"\\n\" + \"-\" * 79)", "response": "Collects the list of falling examples and prints them with a traceback. Raises ValueError if there are no failing examples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncollect python files from the gallery example directories.", "response": "def collect_gallery_files(examples_dirs):\n    \"\"\"Collect python files from the gallery example directories.\"\"\"\n    files = []\n    for example_dir in examples_dirs:\n        for root, dirnames, filenames in os.walk(example_dir):\n            for filename in filenames:\n                if filename.endswith('.py'):\n                    files.append(os.path.join(root, filename))\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for duplicate filenames across gallery directories.", "response": "def check_duplicate_filenames(files):\n    \"\"\"Check for duplicate filenames across gallery directories.\"\"\"\n    # Check whether we'll have duplicates\n    used_names = set()\n    dup_names = list()\n\n    for this_file in files:\n        this_fname = os.path.basename(this_file)\n        if this_fname in used_names:\n            dup_names.append(this_file)\n        else:\n            used_names.add(this_fname)\n\n    if len(dup_names) > 0:\n        logger.warning(\n            'Duplicate file name(s) found. Having duplicate file names will '\n            'break some links. List of files: {}'.format(sorted(dup_names),))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(app):\n    sphinx_compatibility._app = app\n\n    app.add_config_value('sphinx_gallery_conf', DEFAULT_GALLERY_CONF, 'html')\n    for key in ['plot_gallery', 'abort_on_example_error']:\n        app.add_config_value(key, get_default_config_value(key), 'html')\n\n    try:\n        app.add_css_file('gallery.css')\n    except AttributeError:  # Sphinx < 1.8\n        app.add_stylesheet('gallery.css')\n\n    # Sphinx < 1.6 calls it `_extensions`, >= 1.6 is `extensions`.\n    extensions_attr = '_extensions' if hasattr(\n        app, '_extensions') else 'extensions'\n    if 'sphinx.ext.autodoc' in getattr(app, extensions_attr):\n        app.connect('autodoc-process-docstring', touch_empty_backreferences)\n\n    app.connect('builder-inited', generate_gallery_rst)\n    app.connect('build-finished', copy_binder_files)\n    app.connect('build-finished', summarize_failing_examples)\n    app.connect('build-finished', embed_code_links)\n    metadata = {'parallel_read_safe': True,\n                'parallel_write_safe': False,\n                'version': _sg_version}\n    return metadata", "response": "Setup sphinx - gallery sphinx extension"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_py_ipynb(fname):\n    fname_prefix, extension = os.path.splitext(fname)\n    allowed_extension = '.py'\n    if extension != allowed_extension:\n        raise ValueError(\n            \"Unrecognized file extension, expected %s, got %s\"\n            % (allowed_extension, extension))\n    new_extension = '.ipynb'\n    return '{}{}'.format(fname_prefix, new_extension)", "response": "Replace. py extension in filename by. ipynb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn md5sum of file", "response": "def get_md5sum(src_file):\n    \"\"\"Returns md5sum of file\"\"\"\n    with open(src_file, 'rb') as src_data:\n        src_content = src_data.read()\n        return hashlib.md5(src_content).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the zabbix server IP address and port from a zabbix agent config file.", "response": "def _load_from_config(self, config_file):\n        \"\"\"Load zabbix server IP address and port from zabbix agent config\n        file.\n\n        If ServerActive variable is not found in the file, it will\n        use the default: 127.0.0.1:10051\n\n        :type config_file: str\n        :param use_config: Path to zabbix_agentd.conf file to load settings\n            from. If value is `True` then default config path will used:\n            /etc/zabbix/zabbix_agentd.conf\n        \"\"\"\n\n        if config_file and isinstance(config_file, bool):\n            config_file = '/etc/zabbix/zabbix_agentd.conf'\n\n        logger.debug(\"Used config: %s\", config_file)\n\n        #  This is workaround for config wile without sections\n        with open(config_file, 'r') as f:\n            config_file_data = \"[root]\\n\" + f.read()\n\n        params = {}\n\n        try:\n            # python2\n            args = inspect.getargspec(\n                configparser.RawConfigParser.__init__).args\n        except ValueError:\n            # python3\n            args = inspect.getfullargspec(\n                configparser.RawConfigParser.__init__).kwonlyargs\n\n        if 'strict' in args:\n            params['strict'] = True\n\n        config_file_fp = StringIO(config_file_data)\n        config = configparser.RawConfigParser(**params)\n        config.readfp(config_file_fp)\n        # Prefer ServerActive, then try Server and fallback to defaults\n        if config.has_option('root', 'ServerActive'):\n            zabbix_serveractives = config.get('root', 'ServerActive')\n        elif config.has_option('root', 'Server'):\n            zabbix_serveractives = config.get('root', 'Server')\n        else:\n            zabbix_serveractives = '127.0.0.1:10051'\n\n        result = []\n        for serverport in zabbix_serveractives.split(','):\n            if ':' not in serverport:\n                serverport = \"%s:%s\" % (serverport.strip(), 10051)\n            server, port = serverport.split(':')\n            serverport = (server, int(port))\n            result.append(serverport)\n        logger.debug(\"Loaded params: %s\", result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _receive(self, sock, count):\n\n        buf = b''\n\n        while len(buf) < count:\n            chunk = sock.recv(count - len(buf))\n            if not chunk:\n                break\n            buf += chunk\n\n        return buf", "response": "Reads socket to receive data from zabbix server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_messages(self, metrics):\n\n        messages = []\n\n        # Fill the list of messages\n        for m in metrics:\n            messages.append(str(m))\n\n        logger.debug('Messages: %s', messages)\n\n        return messages", "response": "Create a list of zabbix messages from a list of ZabbixMetrics."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a formatted zabbix request from a list of messages.", "response": "def _create_request(self, messages):\n        \"\"\"Create a formatted request to zabbix from a list of messages.\n\n        :type messages: list\n        :param messages: List of zabbix messages\n\n        :rtype: list\n        :return: Formatted zabbix request\n        \"\"\"\n\n        msg = ','.join(messages)\n        request = '{{\"request\":\"sender data\",\"data\":[{msg}]}}'.format(msg=msg)\n        request = request.encode(\"utf-8\")\n        logger.debug('Request: %s', request)\n\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_packet(self, request):\n\n        data_len = struct.pack('<Q', len(request))\n        packet = b'ZBXD\\x01' + data_len + request\n\n        def ord23(x):\n            if not isinstance(x, int):\n                return ord(x)\n            else:\n                return x\n\n        logger.debug('Packet [str]: %s', packet)\n        logger.debug('Packet [hex]: %s',\n                     ':'.join(hex(ord23(x))[2:] for x in packet))\n        return packet", "response": "Create a formatted packet from a request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread from self. socket. and returns response.", "response": "def _get_response(self, connection):\n        \"\"\"Get response from zabbix server, reads from self.socket.\n\n        :type connection: :class:`socket._socketobject`\n        :param connection: Socket to read.\n\n        :rtype: dict\n        :return: Response from zabbix server or False in case of error.\n        \"\"\"\n\n        response_header = self._receive(connection, 13)\n        logger.debug('Response header: %s', response_header)\n\n        if (not response_header.startswith(b'ZBXD\\x01') or\n                len(response_header) != 13):\n            logger.debug('Zabbix return not valid response.')\n            result = False\n        else:\n            response_len = struct.unpack('<Q', response_header[5:])[0]\n            response_body = connection.recv(response_len)\n            result = json.loads(response_body.decode(\"utf-8\"))\n            logger.debug('Data received: %s', result)\n\n        try:\n            connection.close()\n        except Exception as err:\n            pass\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _chunk_send(self, metrics):\n        messages = self._create_messages(metrics)\n        request = self._create_request(messages)\n        packet = self._create_packet(request)\n\n        for host_addr in self.zabbix_uri:\n            logger.debug('Sending data to %s', host_addr)\n\n            # create socket object\n            connection_ = socket.socket()\n            if self.socket_wrapper:\n                connection = self.socket_wrapper(connection_)\n            else:\n                connection = connection_\n\n            connection.settimeout(self.timeout)\n\n            try:\n                # server and port must be tuple\n                connection.connect(host_addr)\n                connection.sendall(packet)\n            except socket.timeout:\n                logger.error('Sending failed: Connection to %s timed out after'\n                             '%d seconds', host_addr, self.timeout)\n                connection.close()\n                raise socket.timeout\n            except Exception as err:\n                # In case of error we should close connection, otherwise\n                # we will close it after data will be received.\n                logger.warn('Sending failed: %s', getattr(err, 'msg', str(err)))\n                connection.close()\n                raise Exception(err)\n\n            response = self._get_response(connection)\n            logger.debug('%s response: %s', host_addr, response)\n\n            if response and response.get('response') != 'success':\n                logger.debug('Response error: %s}', response)\n                raise Exception(response)\n\n        return response", "response": "Send one chunk metrics to the Zabbix server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend the metrics to the zabbix server.", "response": "def send(self, metrics):\n        \"\"\"Send the metrics to zabbix server.\n\n        :type metrics: list\n        :param metrics: List of :class:`zabbix.sender.ZabbixMetric` to send\n            to Zabbix\n\n        :rtype: :class:`pyzabbix.sender.ZabbixResponse`\n        :return: Parsed response from Zabbix Server\n        \"\"\"\n        result = ZabbixResponse()\n        for m in range(0, len(metrics), self.chunk_size):\n            result.parse(self._chunk_send(metrics[m:m + self.chunk_size]))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs-in to zabbix server.", "response": "def _login(self, user='', password=''):\n        \"\"\"Do login to zabbix server.\n\n        :type user: str\n        :param user: Zabbix user\n\n        :type password: str\n        :param password: Zabbix user password\n        \"\"\"\n\n        logger.debug(\"ZabbixAPI.login({0},{1})\".format(user, password))\n\n        self.auth = None\n\n        if self.use_authenticate:\n            self.auth = self.user.authenticate(user=user, password=password)\n        else:\n            self.auth = self.user.login(user=user, password=password)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _logout(self):\n\n        if self.auth:\n            logger.debug(\"ZabbixAPI.logout()\")\n\n            if self.user.logout():\n                self.auth = None", "response": "Do logout from zabbix server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_request(self, method, params=None):\n\n        request_json = {\n            'jsonrpc': '2.0',\n            'method': method,\n            'params': params or {},\n            'id': '1',\n        }\n\n        # apiinfo.version and user.login doesn't require auth token\n        if self.auth and (method not in ('apiinfo.version', 'user.login')):\n            request_json['auth'] = self.auth\n\n        logger.debug(\n            'urllib2.Request({0}, {1})'.format(\n                self.url,\n                json.dumps(request_json)))\n\n        data = json.dumps(request_json)\n        if not isinstance(data, bytes):\n            data = data.encode(\"utf-8\")\n\n        req = urllib2.Request(self.url, data)\n        req.get_method = lambda: 'POST'\n        req.add_header('Content-Type', 'application/json-rpc')\n\n        try:\n            res = urlopen(req)\n            res_str = res.read().decode('utf-8')\n            res_json = json.loads(res_str)\n        except ValueError as e:\n            raise ZabbixAPIException(\"Unable to parse json: %s\" % e.message)\n\n        res_str = json.dumps(res_json, indent=4, separators=(',', ': '))\n        logger.debug(\"Response Body: %s\", res_str)\n\n        if 'error' in res_json:\n            err = res_json['error'].copy()\n            err.update({'json': str(request_json)})\n            raise ZabbixAPIException(err)\n\n        return res_json", "response": "Make a request to Zabbix API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_id(self, item_type, item=None, with_id=False, hostid=None, **args):\n\n        result = None\n        name = args.get('name', False)\n\n        type_ = '{item_type}.get'.format(item_type=item_type)\n\n        item_filter_name = {\n            'mediatype': 'description',\n            'trigger': 'description',\n            'triggerprototype': 'description',\n            'user': 'alias',\n            'usermacro': 'macro',\n        }\n\n        item_id_name = {\n            'discoveryrule': 'item',\n            'graphprototype': 'graph',\n            'hostgroup': 'group',\n            'itemprototype': 'item',\n            'map': 'selement',\n            'triggerprototype': 'trigger',\n            'usergroup': 'usrgrp',\n            'usermacro': 'hostmacro',\n        }\n\n        filter_ = {\n            'filter': {\n                item_filter_name.get(item_type, 'name'): item,\n            },\n            'output': 'extend'}\n\n        if hostid:\n            filter_['filter'].update({'hostid': hostid})\n\n        if args.get('templateids'):\n            if item_type == 'usermacro':\n                filter_['hostids'] = args['templateids']\n            else:\n                filter_['templateids'] = args['templateids']\n\n        if args.get('app_name'):\n            filter_['application'] = args['app_name']\n\n        logger.debug(\n            'do_request( \"{type}\", {filter} )'.format(\n                type=type_,\n                filter=filter_))\n        response = self.do_request(type_, filter_)['result']\n\n        if response:\n            item_id_str = item_id_name.get(item_type, item_type)\n            item_id = '{item}id'.format(item=item_id_str)\n            result = []\n            for obj in response:\n                # Check if object not belong current template\n                if args.get('templateids'):\n                    if (not obj.get('templateid') in (\"0\", None) or\n                            not len(obj.get('templateids', [])) == 0):\n                        continue\n\n                if name:\n                    o = obj.get(item_filter_name.get(item_type, 'name'))\n                    result.append(o)\n                elif with_id:\n                    result.append({item_id: int(obj.get(item_id))})\n                else:\n                    result.append(int(obj.get(item_id)))\n\n            list_types = (list, type(None))\n            if not isinstance(item, list_types):\n                result = result[0]\n\n        return result", "response": "Return id or list of zabbix objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an HTML color string to an RGB tuple", "response": "def HTMLColorToRGB(colorstring):\n    \"\"\" convert #RRGGBB to an (R, G, B) tuple \"\"\"\n    colorstring = colorstring.strip()\n    if colorstring[0] == '#':\n        colorstring = colorstring[1:]\n    if len(colorstring) != 6:\n        raise ValueError(\n            \"input #{0} is not in #RRGGBB format\".format(colorstring))\n    r, g, b = colorstring[:2], colorstring[2:4], colorstring[4:]\n    r, g, b = [int(n, 16) for n in (r, g, b)]\n    return (r, g, b)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_optimized_y_move_down_x_SOL(y_dist):\n\n    # Optimization to move N lines and go to SOL in one command. Note that some terminals \n    # may not support this so we might have to remove this optimization or make it optional \n    # if that winds up mattering for terminals we care about. If we had to remove we'd\n    # want to rework things such that we used \"\\x1b[{0}B\" but also we would want to change\n    # our interface to this function so we didn't guarantee x=0 since caller might ultimate \n    # want it in a different place and we don't want to output two x moves. Could pass in \n    # desired x, or return current x from here. \n          \n    string = \"\\x1b[{0}E\".format(y_dist)  # ANSI code to move down N lines and move x to SOL\n\n    # Would a sequence of 1 or more \\n chars be cheaper? If so we'll output that instead\n    if y_dist < len(string):\n        string = '\\n' * y_dist\n        \n    return string", "response": "Generate an optimized y_dist - distance string to move down x_dist of the current term."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_ANSI_to_move_cursor(cur_x, cur_y, target_x, target_y):\n\n\n    \"\"\"\n        **SIZE - this code (in concert with its caller) implements what I would call \"local optimizations\"\n        to try to minimize the number and size of cursor movements outputted. It does not attempt \"global\n        optimizations\" which I think are rarely going to be worthwhile. See the DESIGN NOTE on global\n        optimizations in this file for more details \n    \"\"\"        \n\n\n    string = \"\"\n\n    if cur_y < target_y:    # MOVE DOWN\n        y_dist = target_y - cur_y\n\n        # See if we can optimize moving x and y together\n        if cur_x == target_x: \n        \n            # Need to move in y only\n            if target_x != 0: \n                # Already in correct x position which is NOT SOL. Just output code to move cursor \n                # down. No special optimization is possible since \\n would take us to SOL and then \n                # we'd also need to output a move for x. \n                return \"\\x1b[{0}B\".format(y_dist)  # ANSI code to move down N lines\n            else:\n                # Already in correct x position which is SOL. Output efficient code to move down.\n                return generate_optimized_y_move_down_x_SOL(y_dist)\n        else:\n        \n            # Need to move in x and y\n            if target_x != 0: \n                # x move is going to be required so we'll move y efficiently and as a side\n                # effect, x will become 0. Code below will move x to the right place\n                string += generate_optimized_y_move_down_x_SOL(y_dist)\n                cur_x = 0\n            else:\n                # Output move down that brings x to SOL. Then we're done.\n                return generate_optimized_y_move_down_x_SOL(y_dist)\n                \n    elif cur_y > target_y:  # MOVE UP\n        if target_x == 0:        \n            # We want to move up and be at the SOL. That can be achieved with one command so we're\n            # done and we return it. However note that some terminals may not support this so we\n            # might have to remove this optimization or make it optional if that winds up mattering for terminals we care about.  \n            return \"\\x1b[{0}F\".format(cur_y - target_y)     # ANSI code to move up N lines and move x to SOL\n        else:\n            string += \"\\x1b[{0}A\".format(cur_y - target_y)  # ANSI code to move up N lines \n\n    if cur_x < target_x:    # MOVE RIGHT\n        # **SIZE - Note that when the bgcolor is specified (not None) and not overdrawing another drawing (as in an animation case)\n        # an optimization could be performed to draw spaces rather than output cursor advances. This would use less\n        # size when advancing less than 3 columns since the min escape sequence here is len 4. Not implementing this now\n        # \\t (tab) could also be a cheap way to move forward, but not clear we can determine how far it goes or if that would\n        # be consistent, nor whether it is ever destructive.\n        string += \"\\x1b[{0}C\".format(target_x - cur_x)  # ANSI code to move cursor right N columns\n    elif cur_x > target_x:  # MOVE LEFT\n        # **SIZE - potential optimizations: \\b (backspace) could be a cheaper way to move backwards when there is only a short\n        # way to go. However, not sure if it is ever destructive so not bothering with it now.    \n        # If we need to move to x=0, \\r could be a cheap way to get there. However not entirely clear whether some terminals\n        # will move to next line as well, and might sometimes be destructive. Not going to research this so not doing it now. \n        string += \"\\x1b[{0}D\".format(cur_x - target_x)  # ANSI code to move cursor left N columns \n\n    return string", "response": "Generates ANSI code to move the cursor to target_x and target_y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_ANSI_from_pixels(pixels, width, height, bgcolor_rgba, current_ansi_colors = None, current_cursor_pos = None, get_pixel_func = None, is_overdraw = False, x_offset = 0):\n\n    if get_pixel_func is None:\n        get_pixel_func = lambda pixels, x, y: (\" \", pixels[x, y])      # just treat pixels as 2D array \n\n    # Compute ANSI bg color and strings we'll use to reset colors when moving to next line   \n    if bgcolor_rgba is not None:\n        bgcolor_ANSI = getANSIcolor_for_rgb(bgcolor_rgba)\n        # Reset cur bg color to bgcolor because \\n will fill the new line with this color\n        bgcolor_ANSI_string = getANSIbgstring_for_ANSIcolor(bgcolor_ANSI)\n    else:\n        bgcolor_ANSI = None        \n        # Reset cur bg color default because \\n will fill the new line with this color (possibly only if BCE supported by terminal)\n        bgcolor_ANSI_string = \"\\x1b[49m\"     # reset bg to default (if we want to support terminals that can't handle this will need to instead use 0m which clears fg too and then when using this reset prior_fg_color to None too\n    \n    # Do we know the current ANSI colors that have been set? \n    if current_ansi_colors is not None:    \n        string = \"\"\n        prior_fg_color = current_ansi_colors['fg']       # Value of None is OK - means default\n        prior_bg_color = current_ansi_colors['bg']       # Value of None is OK - means default\n    else:\n        # We don't know the current colors so output a reset to terminal defaults - we want to be in a known state\n        # **SIZE - could suppress outputting this here, and remember that we have unknown (not same as default)\n        # colors. Then when we need to output we can take this into account. If we wind up setting both fg and bg colors\n        # for output (as for a non-space) then we'd never need to output the reset.  \n        # I'm not going to implement this now since the better thing to do for repeated calls is to pass current_ansi_colors\n        # so we'd never get to this case.\n        string = \"\\x1b[0m\"          # removes all attributes (formatting and colors) to start in a known state\n        prior_fg_color = None       # this is an ANSI color not rgba. None means default.\n        prior_bg_color = None       # this is an ANSI color not rgba. None means default.\n    \n    # Do we know the cursor pos?\n    if current_cursor_pos is not None:\n        cursor_x = current_cursor_pos['x']\n        cursor_y = current_cursor_pos['y']\n    else:\n        cursor_x = 0\n        cursor_y = 0\n\n    for h in range(height):  \n        for w in range(width):\n\n            draw_char, rgba = get_pixel_func(pixels, w, h)\n\n            # Handle fully or partially transparent pixels - but not if it is the special \"erase\" character (None)\n            skip_pixel = False\n            if draw_char is not None:\n                alpha = rgba[3]\n                if alpha == 0:\n                    skip_pixel = True       # skip any full transparent pixel. Note that we don't output a bgcolor space (in specified or default cases). Why? In overdraw mode, that would be wrong since whatever is already drawn should show through. In non-overdraw, assumption is that any line we're drawing on has already been filled with bgcolor so lets not do extra output. If this was an issue in practice, could make it an option.                        \n                elif alpha != 255 and bgcolor_rgba is not None:\n                    rgba = alpha_blend(rgba, bgcolor_rgba)  # non-opaque so blend with specified bgcolor\n                        \n            if not skip_pixel:\n\n                this_pixel_str = \"\"\n\n                # Throw away alpha channel - can still have non-fully-opaque alpha value here if\n                # bgcolor was partially transparent or if no bgcolor and not fully transparent\n                # Could make argument to use threshold to decide if throw away (e.g. >50% transparent) \n                # vs. consider opaque (e.g. <50% transparent) but at least for now we just throw it away \n                # which means we treat the pixel as fully opaque.\n                rgb = rgba[:3]   \n                \n                # If we've got the special \"erase\" character turn it into outputting a space using the bgcolor\n                # which if None will just be a reset to default bg which is what we want\n                if draw_char is None:\n                    draw_char = \" \"\n                    color = bgcolor_ANSI\n                else:\n                    # Convert from RGB to ansi color, using closest color. Conceivably we could optionally support\n                    # dithering to spread the color error. Problematic when dealing with transparency (see cmt in dither_image_to_web_palette())\n                    # or unknown/default bgcolor, and currently not worthwhile since either easy (img2txt) or more correct (graphics) to do \n                    # dithering upstream. \n                    color = getANSIcolor_for_rgb(rgb)\n                \n                    # Optimization - if we're drawing a space and the color is the same as a specified bg color\n                    # then just skip this pixel. We need to make this check here because the conversion to ANSI above can \n                    # cause colors that didn't match to now match\n                    # We cannot do this optimization in overdraw mode because we cannot assume that the bg color\n                    # is already drawn at this location. We could presumably pass in the known state of the screen\n                    # and thus have this knoweldge if the optimization was worthwhile. \n                    if not is_overdraw and (draw_char == \" \") and (color == bgcolor_ANSI):\n                        skip_pixel = True\n\n                if not skip_pixel:\n      \n                    if len(draw_char) > 1:\n                        raise ValueError(\"Not allowing multicharacter draw strings\")\n\n                    # If we are not at the cursor location where we need to draw (happens if we skip pixels or lines)\n                    # then output ANSI sequence to move cursor there.\n                    # This is how we implement transparency - we don't draw spaces, we skip via cursor moves\n                    # We take the x_offset (if any) into account here\n                    ofsetted_w = x_offset + w\n                    if (cursor_x != ofsetted_w) or (cursor_y != h):\n                        string += generate_ANSI_to_move_cursor(cursor_x, cursor_y, ofsetted_w, h)\n                        cursor_x = ofsetted_w\n                        cursor_y = h\n          \n                    # Generate the ANSI sequences to set the colors the way we want them\n                    if draw_char == \" \":\n                \n                        # **SIZE - If we are willing to assume terminals that support ECH (Erase Character) as specified\n                        #   in here http://vt100.net/docs/vt220-rm/chapter4.html we could replace long runs of same-color\n                        #   spaces with single ECH codes. Seems like it is only correct to do this if BCE is supported\n                        #   though (http://superuser.com/questions/249898/how-can-i-prevent-os-x-terminal-app-from-overriding-vim-colours-on-a-remote-syst)\n                        #   else \"erase\" would draw the _default_ background color not the currently set background color\n                        #   Note that if we implement this by accumulating spaces (as opposed to lookahead), need to output that \n                        #   before any different output be that a color change, or if we need to output a \\n (if line ended \n                        #   in same-color spaces in non-overdraw)\n                                        \n                        # We are supposed to output a space, so we're going to need to change the background color.\n                        # No, we can't output an \"upper ascii\" character that fills the entire foreground - all terminals\n                        # don't display such characters the same way, if at all. e.g. Mac terminal outputs ? for \"upper ascii\" chars\n                        # Since we're outputting a space we can leave the prior fg color intact as it won't be used      \n                        string += generate_ANSI_to_set_fg_bg_colors(prior_fg_color, prior_bg_color, prior_fg_color, color)\n                        prior_bg_color = color\n                    \n                    else:\n                        # We're supposed to output a non-space character, so we're going to need to change the foreground color\n                        # and make sure the bg is set appropriately\n                        string += generate_ANSI_to_set_fg_bg_colors(prior_fg_color, prior_bg_color, color, bgcolor_ANSI)\n                        prior_fg_color = color\n                        prior_bg_color = bgcolor_ANSI\n\n                    # Actually output the character\n                    string += draw_char                               \n\n                    cursor_x = cursor_x + 1\n \n        # Handle end of line - unless last line which is NOP because we don't want to do anything to the _line after_ our drawing\n        # and outputting \\n would establish it and fill it\n        if (h + 1) != height:\n                      \n            # Move to next line. If this establishes a new line in the terminal then it fills the _newly established line_\n            # up to EOL with current bg color. Filling with the current bg color vs. default might be dependent on terminal's\n            # support for BCE (Background Color Erase) - I'm not sure.\n            # If cursor had been moved up and this just goes back down to an existing line, no filling occurs\n            # In overdraw mode, we are going to assume we don't need to establish/fill a new line (which could be untrue\n            # if we are overdrawing some lines but going further down too - if that becomes important can allow passing\n            # in how many lines we can go down before hitting that). Next time we need to draw in overdraw mode we'll\n            # move the cursor down as needed.\n            if not is_overdraw:\n\n                # If not already desired color, reset bg color so \\n fills with it\n                # NOTE: it would be ideal to optionally dither the background color if it is not perfectly resolvable\n                # in the palette we have to work with. However, we can't actually do this in the general case because\n                # we don't know the width of the terminal (which can be different at display-time) and because we\n                # don't always know the bg color (\"default\" is not known by us, and not known by anybody until display-time) \n                if prior_bg_color != bgcolor_ANSI:\n                    string += bgcolor_ANSI_string;    \n                    prior_bg_color = bgcolor_ANSI       \n         \n                # If the cursor is not at the correct y, move it there before outputting the newline\n                # In current use this will only occur if current_cursor_pos includes a y offset and\n                # the first line was entirely transparent. We pass 0/0 for cur/target x because no need\n                # to adjust x as it will be changed by the \\n \n                if (cursor_y != h):\n                    string += generate_ANSI_to_move_cursor(0, cursor_y, 0, h)\n                    cursor_y = h\n              \n                string += \"\\n\"      \n                cursor_y += 1\n                cursor_x = 0        # we are assuming UNIX-style \\n behavior - if it were windows we'd have to output \\r to get cursor_x to 0\n\n    return string, {'fg': prior_fg_color, 'bg': prior_bg_color}, { 'x': cursor_x, 'y': cursor_y }", "response": "Generates ANSI codes from a set of pixels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_head(self) -> Commit:\n        head_commit = self.repo.head.commit\n        return Commit(head_commit, self.path, self.main_branch)", "response": "Get the head commit."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a generator of all the commits in the repo.", "response": "def get_list_commits(self, branch: str = None,\n                         reverse_order: bool = True) \\\n            -> Generator[Commit, None, None]:\n        \"\"\"\n        Return a generator of commits of all the commits in the repo.\n\n        :return: Generator[Commit], the generator of all the commits in the\n            repo\n        \"\"\"\n        for commit in self.repo.iter_commits(branch, reverse=reverse_order):\n            yield self.get_commit_from_gitpython(commit)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the specified commit.", "response": "def get_commit(self, commit_id: str) -> Commit:\n        \"\"\"\n        Get the specified commit.\n\n        :param str commit_id: hash of the commit to analyze\n        :return: Commit\n        \"\"\"\n        return Commit(self.repo.commit(commit_id), self.path, self.main_branch)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a PyDriller commit object from a GitPython commit object.", "response": "def get_commit_from_gitpython(self, commit: GitCommit) -> Commit:\n        \"\"\"\n        Build a PyDriller commit object from a GitPython commit object.\n        This is internal of PyDriller, I don't think users generally will need\n        it.\n\n        :param GitCommit commit: GitPython commit\n        :return: Commit commit: PyDriller commit\n        \"\"\"\n        return Commit(commit, self.path, self.main_branch)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkout(self, _hash: str) -> None:\n        with self.lock:\n            self._delete_tmp_branch()\n            self.git.checkout('-f', _hash, b='_PD')", "response": "Checkout the repo at the speficied commit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nobtaining the list of the files in the archive.", "response": "def files(self) -> List[str]:\n        \"\"\"\n        Obtain the list of the files (excluding .git directory).\n\n        :return: List[str], the list of the files\n        \"\"\"\n        _all = []\n        for path, _, files in os.walk(str(self.path)):\n            if '.git' in path:\n                continue\n            for name in files:\n                _all.append(os.path.join(path, name))\n        return _all"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self) -> None:\n        with self.lock:\n            self.git.checkout('-f', self.main_branch)\n            self._delete_tmp_branch()", "response": "Reset the state of the repo."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain the tagged commit.", "response": "def get_commit_from_tag(self, tag: str) -> Commit:\n        \"\"\"\n        Obtain the tagged commit.\n\n        :param str tag: the tag\n        :return: Commit commit: the commit the tag referred to\n        \"\"\"\n        try:\n            selected_tag = self.repo.tags[tag]\n            return self.get_commit(selected_tag.commit.hexsha)\n        except (IndexError, AttributeError):\n            logger.debug('Tag %s not found', tag)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_diff(self, diff: str) -> Dict[str, List[Tuple[int, str]]]:\n        lines = diff.split('\\n')\n        modified_lines = {'added': [], 'deleted': []}\n\n        count_deletions = 0\n        count_additions = 0\n\n        for line in lines:\n            line = line.rstrip()\n            count_deletions += 1\n            count_additions += 1\n\n            if line.startswith('@@'):\n                count_deletions, count_additions = self._get_line_numbers(line)\n\n            if line.startswith('-'):\n                modified_lines['deleted'].append((count_deletions, line[1:]))\n                count_additions -= 1\n\n            if line.startswith('+'):\n                modified_lines['added'].append((count_additions, line[1:]))\n                count_deletions -= 1\n\n            if line == r'\\ No newline at end of file':\n                count_deletions -= 1\n                count_additions -= 1\n\n        return modified_lines", "response": "Given a diff returns a dictionary with the added and deleted lines."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_commits_last_modified_lines(self, commit: Commit,\n                                        modification: Modification = None) \\\n            -> Set[str]:\n        \"\"\"\n        Given the Commit object, returns the set of commits that last\n        \"touched\" the lines that are modified in the files included in the\n        commit. It applies SZZ.\n        The algorithm works as follow: (for every file in the commit)\n\n        1- obtain the diff\n\n        2- obtain the list of deleted lines\n\n        3- blame the file and obtain the commits were those lines were added\n\n        Can also be passed as parameter a single Modification, in this case\n        only this file\n        will be analyzed.\n\n        :param Commit commit: the commit to analyze\n        :param Modification modification: single modification to analyze\n        :return: the set containing all the bug inducing commits\n        \"\"\"\n        buggy_commits = set()\n\n        if modification is not None:\n            modifications = [modification]\n        else:\n            modifications = commit.modifications\n\n        for mod in modifications:\n            path = mod.new_path\n            if mod.change_type == ModificationType.RENAME or \\\n                    mod.change_type == ModificationType.DELETE:\n                path = mod.old_path\n\n            deleted_lines = self.parse_diff(mod.diff)['deleted']\n            try:\n                blame = self.git.blame(commit.hash + '^',\n                                       '--', path).split('\\n')\n                for num_line, line in deleted_lines:\n                    if not self._useless_line(line.strip()):\n                        buggy_commit = blame[num_line - 1].split(' ')[\n                            0].replace('^', '')\n                        buggy_commits.add(self.get_commit(buggy_commit).hash)\n            except GitCommandError:\n                logger.debug(\n                    \"Could not found file %s in commit %s. Probably a double \"\n                    \"rename!\", mod.filename, commit.hash)\n        return buggy_commits", "response": "Given the Commit object returns the set of commits that last\n        touched the lines that are modified in the files included in the commit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_commits_modified_file(self, filepath: str) -> List[str]:\n        path = str(Path(filepath))\n\n        commits = []\n        try:\n            commits = self.git.log(\"--follow\", \"--format=%H\", path).split('\\n')\n        except GitCommandError:\n            logger.debug(\"Could not find information of file %s\", path)\n\n        return commits", "response": "Given a filepath returns all the commits that modified this file\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traverse_commits(self) -> Generator[Commit, None, None]:\n\n        if isinstance(self._path_to_repo, str):\n            self._path_to_repo = [self._path_to_repo]\n\n        for path_repo in self._path_to_repo:\n            # if it is a remote repo, clone it first in a temporary folder!\n            if self._isremote(path_repo):\n                tmp_folder = tempfile.TemporaryDirectory()\n                path_repo = self._clone_remote_repos(tmp_folder.name,\n                                                     path_repo)\n\n            git_repo = GitRepository(path_repo)\n\n            self._sanity_check_filters(git_repo)\n            self._check_timezones()\n\n            logger.info('Analyzing git repository in %s', git_repo.path)\n\n            if self._filepath is not None:\n                self._filepath_commits = git_repo.get_commits_modified_file(\n                    self._filepath)\n\n            for commit in git_repo.get_list_commits(self._only_in_branch,\n                                                    not self._reversed_order):\n                logger.info('Commit #%s in %s from %s', commit.hash,\n                            commit.committer_date,\n                            commit.author.name)\n\n                if self._is_commit_filtered(commit):\n                    logger.info('Commit #%s filtered', commit.hash)\n                    continue\n\n                yield commit", "response": "Traverse commits in the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef added(self) -> int:\n        added = 0\n        for line in self.diff.replace('\\r', '').split(\"\\n\"):\n            if line.startswith('+') and not line.startswith('+++'):\n                added += 1\n        return added", "response": "Return the total number of added lines in the file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef removed(self):\n        removed = 0\n        for line in self.diff.replace('\\r', '').split(\"\\n\"):\n            if line.startswith('-') and not line.startswith('---'):\n                removed += 1\n        return removed", "response": "Return the total number of deleted lines in the file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef author(self) -> Developer:\n        return Developer(self._c_object.author.name,\n                         self._c_object.author.email)", "response": "Return the author of the commit as a Developer object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the committer of the commit as a Developer object.", "response": "def committer(self) -> Developer:\n        \"\"\"\n        Return the committer of the commit as a Developer object.\n\n        :return: committer\n        \"\"\"\n        return Developer(self._c_object.committer.name,\n                         self._c_object.committer.email)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parents(self) -> List[str]:\n        parents = []\n        for p in self._c_object.parents:\n            parents.append(p.hexsha)\n        return parents", "response": "Return the list of parents of this object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of modified files.", "response": "def modifications(self) -> List[Modification]:\n        \"\"\"\n        Return a list of modified files.\n\n        :return: List[Modification] modifications\n        \"\"\"\n        if self._modifications is None:\n            self._modifications = self._get_modifications()\n\n        return self._modifications"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef branches(self) -> Set[str]:\n        if self._branches is None:\n            self._branches = self._get_branches()\n\n        return self._branches", "response": "Return the set of branches that contain the commit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_files(payload, dirname=None, indent=4, tabs=False, header=False):\n    if dirname is None:\n        dirname = os.getcwd()\n\n    for config in payload['config']:\n        path = config['file']\n        if not os.path.isabs(path):\n            path = os.path.join(dirname, path)\n\n        # make directories that need to be made for the config to be built\n        dirpath = os.path.dirname(path)\n        if not os.path.exists(dirpath):\n            os.makedirs(dirpath)\n\n        # build then create the nginx config file using the json payload\n        parsed = config['parsed']\n        output = build(parsed, indent=indent, tabs=tabs, header=header)\n        output = output.rstrip() + '\\n'\n        with codecs.open(path, 'w', encoding='utf-8') as fp:\n            fp.write(output)", "response": "Builds the nginx config files for a given payload."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding 3 - tuples of token lineno quoted", "response": "def _lex_file_object(file_obj):\n    \"\"\"\n    Generates token tuples from an nginx config file object\n\n    Yields 3-tuples like (token, lineno, quoted)\n    \"\"\"\n    token = ''  # the token buffer\n    token_line = 0  # the line the token starts on\n    next_token_is_directive = True\n\n    it = itertools.chain.from_iterable(file_obj)\n    it = _iterescape(it)  # treat escaped characters differently\n    it = _iterlinecount(it)  # count the number of newline characters\n\n    for char, line in it:\n        # handle whitespace\n        if char.isspace():\n            # if token complete yield it and reset token buffer\n            if token:\n                yield (token, token_line, False)\n                if next_token_is_directive and token in EXTERNAL_LEXERS:\n                    for custom_lexer_token in EXTERNAL_LEXERS[token](it, token):\n                        yield custom_lexer_token\n                        next_token_is_directive = True\n                else:\n                    next_token_is_directive = False\n                token = ''\n\n            # disregard until char isn't a whitespace character\n            while char.isspace():\n                char, line = next(it)\n\n        # if starting comment\n        if not token and char == '#':\n            while not char.endswith('\\n'):\n                token = token + char\n                char, _ = next(it)\n            yield (token, line, False)\n            token = ''\n            continue\n\n        if not token:\n            token_line = line\n\n        # handle parameter expansion syntax (ex: \"${var[@]}\")\n        if token and token[-1] == '$' and char == '{':\n            next_token_is_directive = False\n            while token[-1] != '}' and not char.isspace():\n                token += char\n                char, line = next(it)\n\n        # if a quote is found, add the whole string to the token buffer\n        if char in ('\"', \"'\"):\n            # if a quote is inside a token, treat it like any other char\n            if token:\n                token += char\n                continue\n\n            quote = char\n            char, line = next(it)\n            while char != quote:\n                token += quote if char == '\\\\' + quote else char\n                char, line = next(it)\n\n            yield (token, token_line, True)  # True because this is in quotes\n\n            # handle quoted external directives\n            if next_token_is_directive and token in EXTERNAL_LEXERS:\n                for custom_lexer_token in EXTERNAL_LEXERS[token](it, token):\n                    yield custom_lexer_token\n                    next_token_is_directive = True\n            else:\n                next_token_is_directive = False\n\n            token = ''\n            continue\n\n        # handle special characters that are treated like full tokens\n        if char in ('{', '}', ';'):\n            # if token complete yield it and reset token buffer\n            if token:\n                yield (token, token_line, False)\n                token = ''\n\n            # this character is a full token so yield it now\n            yield (char, line, False)\n            next_token_is_directive = True\n            continue\n\n        # append char to the token buffer\n        token += char"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _balance_braces(tokens, filename=None):\n    depth = 0\n\n    for token, line, quoted in tokens:\n        if token == '}' and not quoted:\n            depth -= 1\n        elif token == '{' and not quoted:\n            depth += 1\n\n        # raise error if we ever have more right braces than left\n        if depth < 0:\n            reason = 'unexpected \"}\"'\n            raise NgxParserSyntaxError(reason, filename, line)\n        else:\n            yield (token, line, quoted)\n\n    # raise error if we have less right braces than left at EOF\n    if depth > 0:\n        reason = 'unexpected end of file, expecting \"}\"'\n        raise NgxParserSyntaxError(reason, filename, line)", "response": "Yields tokens that are balanced with braces."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates tokens from an nginx config file", "response": "def lex(filename):\n    \"\"\"Generates tokens from an nginx config file\"\"\"\n    with io.open(filename, mode='r', encoding='utf-8') as f:\n        it = _lex_file_object(f)\n        it = _balance_braces(it, filename)\n        for token, line, quoted in it:\n            yield (token, line, quoted)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove parentheses from an if statement s arguments", "response": "def _prepare_if_args(stmt):\n    \"\"\"Removes parentheses from an \"if\" directive's arguments\"\"\"\n    args = stmt['args']\n    if args and args[0].startswith('(') and args[-1].endswith(')'):\n        args[0] = args[0][1:].lstrip()\n        args[-1] = args[-1][:-1].rstrip()\n        start = int(not args[0])\n        end = len(args) - int(not args[-1])\n        args[:] = args[start:end]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(filename, onerror=None, catch_errors=True, ignore=(), single=False,\n        comments=False, strict=False, combine=False, check_ctx=True,\n        check_args=True):\n    \"\"\"\n    Parses an nginx config file and returns a nested dict payload\n\n    :param filename: string contianing the name of the config file to parse\n    :param onerror: function that determines what's saved in \"callback\"\n    :param catch_errors: bool; if False, parse stops after first error\n    :param ignore: list or tuple of directives to exclude from the payload\n    :param combine: bool; if True, use includes to create a single config obj\n    :param single: bool; if True, including from other files doesn't happen\n    :param comments: bool; if True, including comments to json payload\n    :param strict: bool; if True, unrecognized directives raise errors\n    :param check_ctx: bool; if True, runs context analysis on directives\n    :param check_args: bool; if True, runs arg count analysis on directives\n    :returns: a payload that describes the parsed nginx config\n    \"\"\"\n    config_dir = os.path.dirname(filename)\n\n    payload = {\n        'status': 'ok',\n        'errors': [],\n        'config': [],\n    }\n\n    # start with the main nginx config file/context\n    includes = [(filename, ())]  # stores (filename, config context) tuples\n    included = {filename: 0} # stores {filename: array index} map\n\n    def _handle_error(parsing, e):\n        \"\"\"Adds representaions of an error to the payload\"\"\"\n        file = parsing['file']\n        error = str(e)\n        line = getattr(e, 'lineno', None)\n\n        parsing_error = {'error': error, 'line': line}\n        payload_error = {'file': file, 'error': error, 'line': line}\n        if onerror is not None:\n            payload_error['callback'] = onerror(e)\n\n        parsing['status'] = 'failed'\n        parsing['errors'].append(parsing_error)\n\n        payload['status'] = 'failed'\n        payload['errors'].append(payload_error)\n\n    def _parse(parsing, tokens, ctx=(), consume=False):\n        \"\"\"Recursively parses nginx config contexts\"\"\"\n        fname = parsing['file']\n        parsed = []\n\n        # parse recursively by pulling from a flat stream of tokens\n        for token, lineno, quoted in tokens:\n            comments_in_args = []\n\n            # we are parsing a block, so break if it's closing\n            if token == '}' and not quoted:\n                break\n\n            # if we are consuming, then just continue until end of context\n            if consume:\n                # if we find a block inside this context, consume it too\n                if token == '{' and not quoted:\n                    _parse(parsing, tokens, consume=True)\n                continue\n\n            # the first token should always(?) be an nginx directive\n            directive = token\n\n            if combine:\n                stmt = {\n                    'file': fname,\n                    'directive': directive,\n                    'line': lineno,\n                    'args': []\n                }\n            else:\n                stmt = {\n                    'directive': directive,\n                    'line': lineno,\n                    'args': []\n                }\n\n            # if token is comment\n            if directive.startswith('#') and not quoted:\n                if comments:\n                    stmt['directive'] = '#'\n                    stmt['comment'] = token[1:]\n                    parsed.append(stmt)\n                continue\n\n            # TODO: add external parser checking and handling\n\n            # parse arguments by reading tokens\n            args = stmt['args']\n            token, __, quoted = next(tokens)  # disregard line numbers of args\n            while token not in ('{', ';', '}') or quoted:\n                if token.startswith('#') and not quoted:\n                    comments_in_args.append(token[1:])\n                else:\n                    stmt['args'].append(token)\n\n                token, __, quoted = next(tokens)\n\n            # consume the directive if it is ignored and move on\n            if stmt['directive'] in ignore:\n                # if this directive was a block consume it too\n                if token == '{' and not quoted:\n                    _parse(parsing, tokens, consume=True)\n                continue\n\n            # prepare arguments\n            if stmt['directive'] == 'if':\n                _prepare_if_args(stmt)\n\n            try:\n                # raise errors if this statement is invalid\n                analyze(\n                    fname=fname, stmt=stmt, term=token, ctx=ctx, strict=strict,\n                    check_ctx=check_ctx, check_args=check_args\n                )\n            except NgxParserDirectiveError as e:\n                if catch_errors:\n                    _handle_error(parsing, e)\n\n                    # if it was a block but shouldn't have been then consume\n                    if e.strerror.endswith(' is not terminated by \";\"'):\n                        if token != '}' and not quoted:\n                            _parse(parsing, tokens, consume=True)\n                        else:\n                            break\n\n                    # keep on parsin'\n                    continue\n                else:\n                    raise e\n\n            # add \"includes\" to the payload if this is an include statement\n            if not single and stmt['directive'] == 'include':\n                pattern = args[0]\n                if not os.path.isabs(args[0]):\n                    pattern = os.path.join(config_dir, args[0])\n\n                stmt['includes'] = []\n\n                # get names of all included files\n                if glob.has_magic(pattern):\n                    fnames = glob.glob(pattern)\n                    fnames.sort()\n                else:\n                    try:\n                        # if the file pattern was explicit, nginx will check\n                        # that the included file can be opened and read\n                        open(str(pattern)).close()\n                        fnames = [pattern]\n                    except Exception as e:\n                        fnames = []\n                        e.lineno = stmt['line']\n                        if catch_errors:\n                            _handle_error(parsing, e)\n                        else:\n                            raise e\n\n                for fname in fnames:\n                    # the included set keeps files from being parsed twice\n                    # TODO: handle files included from multiple contexts\n                    if fname not in included:\n                        included[fname] = len(includes)\n                        includes.append((fname, ctx))\n                    index = included[fname]\n                    stmt['includes'].append(index)\n\n            # if this statement terminated with '{' then it is a block\n            if token == '{' and not quoted:\n                inner = enter_block_ctx(stmt, ctx)  # get context for block\n                stmt['block'] = _parse(parsing, tokens, ctx=inner)\n\n            parsed.append(stmt)\n\n            # add all comments found inside args after stmt is added\n            for comment in comments_in_args:\n                comment_stmt = {\n                    'directive': '#',\n                    'line': stmt['line'],\n                    'args': [],\n                    'comment': comment\n                }\n                parsed.append(comment_stmt)\n\n        return parsed\n\n    # the includes list grows as \"include\" directives are found in _parse\n    for fname, ctx in includes:\n        tokens = lex(fname)\n        parsing = {\n            'file': fname,\n            'status': 'ok',\n            'errors': [],\n            'parsed': []\n        }\n        try:\n            parsing['parsed'] = _parse(parsing, tokens, ctx=ctx)\n        except Exception as e:\n            _handle_error(parsing, e)\n\n        payload['config'].append(parsing)\n\n    if combine:\n        return _combine_parsed_configs(payload)\n    else:\n        return payload", "response": "Parses a nginx config file and returns a nested dict containing the parsed nginx config file and the nginx config object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining config files into one by using include directives.", "response": "def _combine_parsed_configs(old_payload):\n    \"\"\"\n    Combines config files into one by using include directives.\n\n    :param old_payload: payload that's normally returned by parse()\n    :return: the new combined payload\n    \"\"\"\n    old_configs = old_payload['config']\n\n    def _perform_includes(block):\n        for stmt in block:\n            if 'block' in stmt:\n                stmt['block'] = list(_perform_includes(stmt['block']))\n            if 'includes' in stmt:\n                for index in stmt['includes']:\n                    config = old_configs[index]['parsed']\n                    for stmt in _perform_includes(config):\n                        yield stmt\n            else:\n                yield stmt  # do not yield include stmt itself\n\n    combined_config = {\n        'file': old_configs[0]['file'],\n        'status': 'ok',\n        'errors': [],\n        'parsed': []\n    }\n\n    for config in old_configs:\n        combined_config['errors'] += config.get('errors', [])\n        if config.get('status', 'ok') == 'failed':\n            combined_config['status'] = 'failed'\n\n    first_config = old_configs[0]['parsed']\n    combined_config['parsed'] += _perform_includes(first_config)\n\n    combined_payload = {\n        'status': old_payload.get('status', 'ok'),\n        'errors': old_payload.get('errors', []),\n        'config': [combined_config]\n    }\n    return combined_payload"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_pep_479(generator):\n    @functools.wraps(generator)\n    def _wrapped_generator(*args, **kwargs):\n        try:\n            for x in generator(*args, **kwargs):\n                yield x\n        except RuntimeError:\n            return\n\n    return _wrapped_generator", "response": "A wrapper around a generator that fixes PEP 479."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_line(self, idx, line):\n        if '///' in line:  # \u6ce8\u91ca\n            py_line = '#' + line[3:]\n\n            # /// [T\u53bb\u6389ftdc\u524d\u7684\u5185\u5bb9\uff0c\u65b9\u4fbf\u540e\u9762\u6bd4\u5bf9]FtdcInvestorRangeType\u662f\u4e00\u4e2a\u6295\u8d44\u8005\u8303\u56f4\u7c7b\u578b ==>\n            # /// <summary>\n            # /// \u6295\u8d44\u8005\u8303\u56f4\u7c7b\u578b\n            # /// </summary>\"\"\"\n            if py_line.find('\u662f\u4e00\u4e2a') > 0:\n                self.enum_comment[py_line[py_line.find('Ftdc'):py_line.find('\u662f\u4e00\u4e2a')]] = '/// <summary>\\n/// %s\\n///</summary>' % py_line[py_line.find('\u662f\u4e00\u4e2a') + 3:-1]\n            else:\n                self.tmp_comment = '/// <summary>\\n\\t/// {0}\\n\\t///</summary>\\n\\t'.format(line[3:-1])  # -1\u53bb\u6389\u5c3e\u6362\u884c\n\n        elif '#define' in line:  # \u5b9a\u4e49\u5e38\u91cf\n            # define THOST_FTDC_IR_All '1' ==> defineDict[\"THOST_FTDC_IR_All\"] = '1'\n            content = line.split(' ')\n            constant = content[1]\n            if len(content) > 2:\n                value = content[-1][:-1]  # value\u5e26\u884c\u5c3e\u7684\\n\n                py_line = 'defineDict[\"%s\"] = %s\\n' % (constant, value)\n            else:\n                py_line = ''\n\n            # enum relate define\n            if py_line:  # \u547d\u540d\u4fdd\u6301\u4e00\u81f4\uff0c\u4e0d\u518d\u7cbe\u7b80\n                if len(value) > 3:  # \u5904\u7406\u7406'x' x\u957f\u5ea6>1\u7684\u60c5\u51b5\uff0c\u5982102001\n                    self.define.append(\"{2}{0} = {1},\".format(constant, value[1:-1], self.tmp_comment))\n                else:\n                    self.define.append(\"{2}{0} = (byte){1},\".format(constant, value, self.tmp_comment))\n\n        elif 'typedef' in line:  # \u7c7b\u578b\u7533\u660e\n            # typedef char TThostFtdcInvestorRangeType; ==> typedefDict[\"TThostFtdcInvestorRangeType\"] = \"c_char\"\n            py_line = self.process_typedef(line)\n            # public enum TThostFtdcInvestorRangeType : byte\n            # {\n            # \t/// <summary>\n            # \t///\u6240\u6709\n            # \t/// </summary>\n            # \tTHOST_FTDC_IR_All = (byte)'1',\n            # \t/// <summary>\n            # \t///\u6295\u8d44\u8005\u7ec4\n            # \t/// </summary>\n            # \tTHOST_FTDC_IR_Group = (byte)'2',\n            # \t/// <summary>\n            # \t///\u5355\u4e00\u6295\u8d44\u8005\n            # \t/// </summary>\n            # \tTHOST_FTDC_IR_Single = (byte)'3'\n            # }\n\n            if line.find(' char ') > 0 and line.find('[') < 0:\n                key = line.split(' ')[2][6:-2]  # TThostFtdcInvestorRangeType=>FtdcInvestorRangeType(\u53bb\u6389\u9996\u4e2aT)\n                enum_line = self.enum_comment[key]\n                enum_line += '\\npublic enum TThost%s : byte\\n{\\n' % key\n                for l in self.define:\n                    enum_line += '\\t%s\\n' % l\n                enum_line += '}\\n\\n'\n                # \u5904\u7406\u5f62\u5982 102001\u6b64\u7c7b\u503c\n                if enum_line.find(\"(byte)\") < 0:\n                    enum_line = enum_line.replace(': byte', ': int')\n\n                self.fenum.write(enum_line)\n            self.define.clear()\n\n        elif line == '\\n':  # \u7a7a\u884c\n            py_line = line\n        else:\n            py_line = ''\n\n        return py_line", "response": "Process a single line of FtdcInvestorRangeType."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _OnRspQryPositionDetail(self, pInvestorPositionDetail: CThostFtdcInvestorPositionDetailField, pRspInfo: CThostFtdcRspInfoField, nRequestID: int, bIsLast: bool):\n        if pInvestorPositionDetail.getInstrumentID() == '':\n            return\n        detail = PositionDetail()\n        detail.Instrument = pInvestorPositionDetail.getInstrumentID()\n        detail.CloseProfit = pInvestorPositionDetail.getCloseProfitByTrade()\n        detail.Direction = DirectType.Buy if pInvestorPositionDetail.getDirection() == TThostFtdcDirectionType.THOST_FTDC_D_Buy else DirectType.Sell\n        detail.HedgeFlag = HedgeType(list(TThostFtdcHedgeFlagType).index(pInvestorPositionDetail.getHedgeFlag()))\n        detail.OpenDate = pInvestorPositionDetail.getOpenDate()\n        detail.PositionProfit = pInvestorPositionDetail.getPositionProfitByTrade()\n        detail.OpenPrice = pInvestorPositionDetail.getOpenPrice()\n        detail.TradeType = TradeTypeType(list(TThostFtdcTradeTypeType).index(pInvestorPositionDetail.getTradeType()))\n        detail.Volume = pInvestorPositionDetail.getVolume()\n        self.position_details[pInvestorPositionDetail.getTradeID()] = detail", "response": "Update the internal state of the investor position detail."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when a message is received from the server.", "response": "def _OnRtnNotice(self, pTradingNoticeInfo: CThostFtdcTradingNoticeInfoField):\n        '''\u4ea4\u6613\u63d0\u9192'''\n        msg = pTradingNoticeInfo.getFieldContent()\n        if len(msg) > 0:\n            threading.Thread(target=self.OnRtnNotice, args=(self, pTradingNoticeInfo.getSendTime(), msg)).start()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ReqUserLogin(self, user: str, pwd: str, broker: str):\n        self.broker = broker\n        self.investor = user\n        self.password = pwd\n        self.t.ReqUserLogin(BrokerID=broker, UserID=user, Password=pwd)", "response": "This method is used to request a user login"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ReqOrderInsert(self, pInstrument: str, pDirection: DirectType, pOffset: OffsetType, pPrice: float = 0.0, pVolume: int = 1, pType: OrderType = OrderType.Limit, pCustom: int = 0):\n        OrderPriceType = TThostFtdcOrderPriceTypeType.THOST_FTDC_OPT_AnyPrice\n        TimeCondition = TThostFtdcTimeConditionType.THOST_FTDC_TC_IOC\n        LimitPrice = 0.0\n        VolumeCondition = TThostFtdcVolumeConditionType.THOST_FTDC_VC_AV\n\n        if pType == OrderType.Market:  # \u5e02\u4ef7\n            OrderPriceType = TThostFtdcOrderPriceTypeType.THOST_FTDC_OPT_AnyPrice\n            TimeCondition = TThostFtdcTimeConditionType.THOST_FTDC_TC_IOC\n            LimitPrice = 0.0\n            VolumeCondition = TThostFtdcVolumeConditionType.THOST_FTDC_VC_AV\n        elif pType == OrderType.Limit:  # \u9650\u4ef7\n            OrderPriceType = TThostFtdcOrderPriceTypeType.THOST_FTDC_OPT_LimitPrice\n            TimeCondition = TThostFtdcTimeConditionType.THOST_FTDC_TC_GFD\n            LimitPrice = pPrice\n            VolumeCondition = TThostFtdcVolumeConditionType.THOST_FTDC_VC_AV\n        elif pType == OrderType.FAK:  # FAK\n            OrderPriceType = TThostFtdcOrderPriceTypeType.THOST_FTDC_OPT_LimitPrice\n            TimeCondition = TThostFtdcTimeConditionType.THOST_FTDC_TC_IOC\n            LimitPrice = pPrice\n            VolumeCondition = TThostFtdcVolumeConditionType.THOST_FTDC_VC_AV\n        elif pType == OrderType.FOK:  # FOK\n            OrderPriceType = TThostFtdcOrderPriceTypeType.THOST_FTDC_OPT_LimitPrice\n            TimeCondition = TThostFtdcTimeConditionType.THOST_FTDC_TC_IOC\n            LimitPrice = pPrice\n            VolumeCondition = TThostFtdcVolumeConditionType.THOST_FTDC_VC_CV  # \u5168\u90e8\u6570\u91cf\n\n        self._req += 1\n        self.t.ReqOrderInsert(\n            BrokerID=self.broker,\n            InvestorID=self.investor,\n            InstrumentID=pInstrument,\n            OrderRef=\"%06d%06d\" % (self._req, pCustom % 1000000),\n            UserID=self.investor,\n            # \u6b64\u5904ctp_enum\u4e0eat_struct\u540d\u79f0\u51b2\u7a81\n            Direction=TThostFtdcDirectionType.THOST_FTDC_D_Buy if pDirection == DirectType.Buy else TThostFtdcDirectionType.THOST_FTDC_D_Sell,\n            CombOffsetFlag=chr(TThostFtdcOffsetFlagType.THOST_FTDC_OF_Open.value if pOffset == OffsetType.Open else TThostFtdcOffsetFlagType.THOST_FTDC_OF_CloseToday.value if pOffset == OffsetType.CloseToday else TThostFtdcOffsetFlagType.THOST_FTDC_OF_Close.value),\n            CombHedgeFlag=chr(TThostFtdcHedgeFlagType.THOST_FTDC_HF_Speculation.value),\n            IsAutoSuspend=0,\n            ForceCloseReason=TThostFtdcForceCloseReasonType.THOST_FTDC_FCC_NotForceClose,\n            IsSwapOrder=0,\n            ContingentCondition=TThostFtdcContingentConditionType.THOST_FTDC_CC_Immediately,\n            VolumeCondition=VolumeCondition,\n            MinVolume=1,\n            VolumeTotalOriginal=pVolume,\n            OrderPriceType=OrderPriceType,\n            TimeCondition=TimeCondition,\n            LimitPrice=LimitPrice,\n        )", "response": "This method is used to add an order to the set of available order resources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ReqOrderAction(self, OrderID: str):\n        of = self.orders[OrderID]\n\n        if not of:\n            return -1\n        else:\n            pOrderId = of.OrderID\n            return self.t.ReqOrderAction(\n                self.broker,\n                self.investor,\n                OrderRef=pOrderId.split('|')[2],\n                FrontID=int(pOrderId.split('|')[1]),\n                SessionID=int(pOrderId.split('|')[0]),\n                InstrumentID=of.InstrumentID,\n                ActionFlag=TThostFtdcActionFlagType.THOST_FTDC_AF_Delete)", "response": "This method is used to request an order action."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef OnInstrumentStatus(self, obj, inst: str, status: InstrumentStatus):\n        print('{}:{}'.format(inst, str(status).strip().split('.')[-1]))", "response": "Instrument status event handler"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ReqConnect(self, pAddress: str):\n        self.q.CreateApi()\n        spi = self.q.CreateSpi()\n        self.q.RegisterSpi(spi)\n\n        self.q.OnFrontConnected = self._OnFrontConnected\n        self.q.OnFrontDisconnected = self._OnFrontDisConnected\n        self.q.OnRspUserLogin = self._OnRspUserLogin\n        self.q.OnRtnDepthMarketData = self._OnRtnDepthMarketData\n        self.q.OnRspSubMarketData = self._OnRspSubMarketData\n\n        self.q.RegCB()\n\n        self.q.RegisterFront(pAddress)\n        self.q.Init()", "response": "Connect to a specific address"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ReqUserLogin(self, user: str, pwd: str, broker: str):\n        self.q.ReqUserLogin(BrokerID=broker, UserID=user, Password=pwd)", "response": "\u767b\u5f55\n\n        :param user:\n        :param pwd:\n        :param broker:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ReqUserLogout(self):\n\n        self.q.Release()\n        # \u786e\u4fdd\u9694\u591c\u6216\u91cd\u65b0\u767b\u5f55\u65f6\u7684\u7b2c1\u4e2atick\u4e0d\u88ab\u53d1\u9001\u5230\u5ba2\u6237\u7aef\n        self.inst_tick.clear()\n        self.logined = False\n        threading.Thread(target=self.OnDisConnected, args=(self, 0)).start()", "response": "Logout \u63a5\u53e3 ( \u6b63\u5e38\u9000\u51fa"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rmrf(items, verbose=True):\n    \"Silently remove a list of directories or files\"\n    if isinstance(items, str):\n        items = [items]\n\n    for item in items:\n        if verbose:\n            print(\"Removing {}\".format(item))\n        shutil.rmtree(item, ignore_errors=True)\n        # rmtree doesn't remove bare files\n        try:\n            os.remove(item)\n        except FileNotFoundError:\n            pass", "response": "Silently remove a list of directories or files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild documentation using sphinx", "response": "def docs(context, builder='html'):\n    \"Build documentation using sphinx\"\n    cmdline = 'python -msphinx -M {} {} {} {}'.format(builder, DOCS_SRCDIR, DOCS_BUILDDIR, SPHINX_OPTS)\n    context.run(cmdline)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlaunches webserver on http://localhost :8000 with rendered documentation", "response": "def livehtml(context):\n    \"Launch webserver on http://localhost:8000 with rendered documentation\"\n    builder = 'html'\n    outputdir = os.path.join(DOCS_BUILDDIR, builder)\n    cmdline = 'sphinx-autobuild -b {} {} {}'.format(builder, DOCS_SRCDIR, outputdir)\n    context.run(cmdline, pty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tag(context, name, message=''):\n    \"Add a Git tag and push it to origin\"\n    # If a tag was provided on the command-line, then add a Git tag and push it to origin\n    if name:\n        context.run('git tag -a {} -m {!r}'.format(name, message))\n        context.run('git push origin {}'.format(name))", "response": "Add a Git tag and push it to origin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validatetag(context):\n    \"Check to make sure that a tag exists for the current HEAD and it looks like a valid version number\"\n    # Validate that a Git tag exists for the current commit HEAD\n    result = context.run(\"git describe --exact-match --tags $(git log -n1 --pretty='%h')\")\n    tag = result.stdout.rstrip()\n\n    # Validate that the Git tag appears to be a valid version number\n    ver_regex = re.compile('(\\d+)\\.(\\d+)\\.(\\d+)')\n    match = ver_regex.fullmatch(tag)\n    if match is None:\n        print('Tag {!r} does not appear to be a valid version number'.format(tag))\n        sys.exit(-1)\n    else:\n        print('Tag {!r} appears to be a valid version number'.format(tag))", "response": "Check to make sure that a tag exists for the current HEAD and it looks like a valid version number"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _preloop_hook(self) -> None:\n        # This runs after cmdloop() acquires self.terminal_lock, which will be locked until the prompt appears.\n        # Therefore this is the best place to start the alerter thread since there is no risk of it alerting\n        # before the prompt is displayed. You can also start it via a command if its not something that should\n        # be running during the entire application. See do_start_alerts().\n        self._stop_thread = False\n\n        self._alerter_thread = threading.Thread(name='alerter', target=self._alerter_thread_func)\n        self._alerter_thread.start()", "response": "Start the alerter thread"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _postloop_hook(self) -> None:\n\n        # After this function returns, cmdloop() releases self.terminal_lock which could make the alerter\n        # thread think the prompt is on screen. Therefore this is the best place to stop the alerter thread.\n        # You can also stop it via a command. See do_stop_alerts().\n        self._stop_thread = True\n        if self._alerter_thread.is_alive():\n            self._alerter_thread.join()", "response": "Stop the alerter thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_start_alerts(self, _):\n        if self._alerter_thread.is_alive():\n            print(\"The alert thread is already started\")\n        else:\n            self._stop_thread = False\n            self._alerter_thread = threading.Thread(name='alerter', target=self._alerter_thread_func)\n            self._alerter_thread.start()", "response": "Starts the alerter thread"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping the alerter thread", "response": "def do_stop_alerts(self, _):\n        \"\"\" Stops the alerter thread \"\"\"\n        self._stop_thread = True\n        if self._alerter_thread.is_alive():\n            self._alerter_thread.join()\n        else:\n            print(\"The alert thread is already stopped\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the list of alerts in the system.", "response": "def _get_alerts(self) -> List[str]:\n        \"\"\"\n        Reports alerts\n        :return: the list of alerts\n        \"\"\"\n        global ALERTS\n\n        cur_time = time.monotonic()\n        if cur_time < self._next_alert_time:\n            return []\n\n        alerts = []\n\n        if self._alert_count < len(ALERTS):\n            alerts.append(ALERTS[self._alert_count])\n            self._alert_count += 1\n            self._next_alert_time = cur_time + 4\n\n        else:\n            rand_num = random.randint(1, 20)\n            if rand_num > 2:\n                return []\n\n            for i in range(0, rand_num):\n                self._alert_count += 1\n                alerts.append(\"Alert {}\".format(self._alert_count))\n\n            self._next_alert_time = 0\n\n        return alerts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_alert_str(self) -> str:\n        global ALERTS\n\n        alert_str = ''\n        alerts = self._get_alerts()\n\n        longest_alert = max(ALERTS, key=len)\n        num_asterisks = len(longest_alert) + 8\n\n        for i, cur_alert in enumerate(alerts):\n            # Use padding to center the alert\n            padding = ' ' * int((num_asterisks - len(cur_alert)) / 2)\n\n            if i > 0:\n                alert_str += '\\n'\n            alert_str += '*' * num_asterisks + '\\n'\n            alert_str += padding + cur_alert + padding + '\\n'\n            alert_str += '*' * num_asterisks + '\\n'\n\n        return alert_str", "response": "Generates the alert string that can be printed to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint alerts and updates the prompt any time the prompt is showing", "response": "def _alerter_thread_func(self) -> None:\n        \"\"\" Prints alerts and updates the prompt any time the prompt is showing \"\"\"\n\n        self._alert_count = 0\n        self._next_alert_time = 0\n\n        while not self._stop_thread:\n            # Always acquire terminal_lock before printing alerts or updating the prompt\n            # To keep the app responsive, do not block on this call\n            if self.terminal_lock.acquire(blocking=False):\n\n                # Get any alerts that need to be printed\n                alert_str = self._generate_alert_str()\n\n                # Generate a new prompt\n                new_prompt = self._generate_colored_prompt()\n\n                # Check if we have alerts to print\n                if alert_str:\n                    # new_prompt is an optional parameter to async_alert()\n                    self.async_alert(alert_str, new_prompt)\n                    new_title = \"Alerts Printed: {}\".format(self._alert_count)\n                    self.set_window_title(new_title)\n\n                # No alerts needed to be printed, check if the prompt changed\n                elif new_prompt != self.prompt:\n                    self.async_update_prompt(new_prompt)\n\n                # Don't forget to release the lock\n                self.terminal_lock.release()\n\n            time.sleep(0.5)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_quoted(arg: str) -> bool:\n    return len(arg) > 1 and arg[0] == arg[-1] and arg[0] in constants.QUOTES", "response": "Checks if a string is quoted"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quote_string_if_needed(arg: str) -> str:\n    if is_quoted(arg) or ' ' not in arg:\n        return arg\n\n    if '\"' in arg:\n        quote = \"'\"\n    else:\n        quote = '\"'\n\n    return quote + arg + quote", "response": "Quotes a string if it contains spaces and isn t already quoted"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cast(current: Any, new: str) -> Any:\n    typ = type(current)\n    orig_new = new\n\n    if typ == bool:\n        try:\n            return bool(int(new))\n        except (ValueError, TypeError):\n            pass\n        try:\n            new = new.lower()\n            if (new == 'on') or (new[0] in ('y', 't')):\n                return True\n            if (new == 'off') or (new[0] in ('n', 'f')):\n                return False\n        except AttributeError:\n            pass\n    else:\n        try:\n            return typ(new)\n        except (ValueError, TypeError):\n            pass\n    print(\"Problem setting parameter (now {}) to {}; incorrect type?\".format(current, orig_new))\n    return current", "response": "Tries to force a new value into the same type as the current value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef which(editor: str) -> Optional[str]:\n    try:\n        editor_path = subprocess.check_output(['which', editor], stderr=subprocess.STDOUT).strip()\n        editor_path = editor_path.decode()\n    except subprocess.CalledProcessError:\n        editor_path = None\n    return editor_path", "response": "Find the full path of a given editor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the file is a text file False if it is binary.", "response": "def is_text_file(file_path: str) -> bool:\n    \"\"\"Returns if a file contains only ASCII or UTF-8 encoded text.\n\n    :param file_path: path to the file being checked\n    :return: True if the file is a text file, False if it is binary.\n    \"\"\"\n    import codecs\n\n    expanded_path = os.path.abspath(os.path.expanduser(file_path.strip()))\n    valid_text_file = False\n\n    # Check if the file is ASCII\n    try:\n        with codecs.open(expanded_path, encoding='ascii', errors='strict') as f:\n            # Make sure the file has at least one line of text\n            # noinspection PyUnusedLocal\n            if sum(1 for line in f) > 0:\n                valid_text_file = True\n    except OSError:  # pragma: no cover\n        pass\n    except UnicodeDecodeError:\n        # The file is not ASCII. Check if it is UTF-8.\n        try:\n            with codecs.open(expanded_path, encoding='utf-8', errors='strict') as f:\n                # Make sure the file has at least one line of text\n                # noinspection PyUnusedLocal\n                if sum(1 for line in f) > 0:\n                    valid_text_file = True\n        except OSError:  # pragma: no cover\n            pass\n        except UnicodeDecodeError:\n            # Not UTF-8\n            pass\n\n    return valid_text_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove duplicates from a list while preserving order of the items.", "response": "def remove_duplicates(list_to_prune: List) -> List:\n    \"\"\"Removes duplicates from a list while preserving order of the items.\n\n    :param list_to_prune: the list being pruned of duplicates\n    :return: The pruned list\n    \"\"\"\n    temp_dict = collections.OrderedDict()\n    for item in list_to_prune:\n        temp_dict[item] = None\n\n    return list(temp_dict.keys())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort a list of strings alphabetically.", "response": "def alphabetical_sort(list_to_sort: Iterable[str]) -> List[str]:\n    \"\"\"Sorts a list of strings alphabetically.\n\n    For example: ['a1', 'A11', 'A2', 'a22', 'a3']\n\n    To sort a list in place, don't call this method, which makes a copy. Instead, do this:\n\n    my_list.sort(key=norm_fold)\n\n    :param list_to_sort: the list being sorted\n    :return: the sorted list\n    \"\"\"\n    return sorted(list_to_sort, key=norm_fold)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to convert the passed - in string to an integer or force to lower case.", "response": "def try_int_or_force_to_lower_case(input_str: str) -> Union[int, str]:\n    \"\"\"\n    Tries to convert the passed-in string to an integer. If that fails, it converts it to lower case using norm_fold.\n    :param input_str: string to convert\n    :return: the string as an integer or a lower case version of the string\n    \"\"\"\n    try:\n        return int(input_str)\n    except ValueError:\n        return norm_fold(input_str)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef natural_keys(input_str: str) -> List[Union[int, str]]:\n    return [try_int_or_force_to_lower_case(substr) for substr in re.split(r'(\\d+)', input_str)]", "response": "Converts a string into a list of integers and strings to support natural sorting."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef natural_sort(list_to_sort: Iterable[str]) -> List[str]:\n    return sorted(list_to_sort, key=natural_keys)", "response": "Sort a list of strings case insensitively as well as numerically."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind a reasonable editor to use by default for the system that the cmd2 application is running on.", "response": "def find_editor() -> str:\n    \"\"\"Find a reasonable editor to use by default for the system that the cmd2 application is running on.\"\"\"\n    editor = os.environ.get('EDITOR')\n    if not editor:\n        if sys.platform[:3] == 'win':\n            editor = 'notepad'\n        else:\n            # Favor command-line editors first so we don't leave the terminal to edit\n            for editor in ['vim', 'vi', 'emacs', 'nano', 'pico', 'gedit', 'kate', 'subl', 'geany', 'atom']:\n                if which(editor):\n                    break\n    return editor"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding str to internal bytes buffer and if echo is True echo contents to inner stream", "response": "def write(self, s: str) -> None:\n        \"\"\"Add str to internal bytes buffer and if echo is True, echo contents to inner stream\"\"\"\n        if not isinstance(s, str):\n            raise TypeError('write() argument must be str, not {}'.format(type(s)))\n\n        if not self.pause_storage:\n            self.buffer.byte_buf += s.encode(encoding=self.encoding, errors=self.errors)\n        if self.echo:\n            self.inner_stream.write(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the internal contents as a str", "response": "def getvalue(self) -> str:\n        \"\"\"Get the internal contents as a str\"\"\"\n        return self.buffer.byte_buf.decode(encoding=self.encoding, errors=self.errors)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite bytes to internal bytes buffer and if echo is True echo contents to inner stream.", "response": "def write(self, b: bytes) -> None:\n        \"\"\"Add bytes to internal bytes buffer and if echo is True, echo contents to inner stream.\"\"\"\n        if not isinstance(b, bytes):\n            raise TypeError('a bytes-like object is required, not {}'.format(type(b)))\n        if not self.std_sim_instance.pause_storage:\n            self.byte_buf += b\n        if self.std_sim_instance.echo:\n            self.std_sim_instance.inner_stream.buffer.write(b)\n\n            # Since StdSim wraps TextIO streams, we will flush the stream if line buffering is on\n            # and the bytes being written contain a new line character. This is helpful when StdSim\n            # is being used to capture output of a shell command because it causes the output to print\n            # to the screen more often than if we waited for the stream to flush its buffer.\n            if self.std_sim_instance.line_buffering:\n                if any(newline in b for newline in ByteBuf.NEWLINES):\n                    self.std_sim_instance.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a SIGINT to the process similar to if Ctrl + C was pressed.", "response": "def send_sigint(self) -> None:\n        \"\"\"Send a SIGINT to the process similar to if <Ctrl>+C were pressed.\"\"\"\n        import signal\n        if sys.platform.startswith('win'):\n            signal_to_send = signal.CTRL_C_EVENT\n        else:\n            signal_to_send = signal.SIGINT\n        self._proc.send_signal(signal_to_send)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for the process to finish.", "response": "def wait(self) -> None:\n        \"\"\"Wait for the process to finish\"\"\"\n        if self._out_thread.is_alive():\n            self._out_thread.join()\n        if self._err_thread.is_alive():\n            self._err_thread.join()\n\n        # Handle case where the process ended before the last read could be done.\n        # This will return None for the streams that weren't pipes.\n        out, err = self._proc.communicate()\n\n        if out:\n            self._write_bytes(self._stdout, out)\n        if err:\n            self._write_bytes(self._stderr, err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nthreads function that reads a stream from the process and writes it to the output stream.", "response": "def _reader_thread_func(self, read_stdout: bool) -> None:\n        \"\"\"\n        Thread function that reads a stream from the process\n        :param read_stdout: if True, then this thread deals with stdout. Otherwise it deals with stderr.\n        \"\"\"\n        if read_stdout:\n            read_stream = self._proc.stdout\n            write_stream = self._stdout\n        else:\n            read_stream = self._proc.stderr\n            write_stream = self._stderr\n\n        # The thread should have been started only if this stream was a pipe\n        assert read_stream is not None\n\n        # Run until process completes\n        while self._proc.poll() is None:\n            # noinspection PyUnresolvedReferences\n            available = read_stream.peek()\n            if available:\n                read_stream.read(len(available))\n                self._write_bytes(write_stream, available)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_bytes(stream: Union[StdSim, TextIO], to_write: bytes) -> None:\n        try:\n            stream.buffer.write(to_write)\n        except BrokenPipeError:\n            # This occurs if output is being piped to a process that closed\n            pass", "response": "Write bytes to a stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\naccepting unique abbreviated commands", "response": "def abbrev_hook(self, data: cmd2.plugin.PostparsingData) -> cmd2.plugin.PostparsingData:\n        \"\"\"Accept unique abbreviated commands\"\"\"\n        func = self.cmd_func(data.statement.command)\n        if func is None:\n            # check if the entered command might be an abbreviation\n            possible_cmds = [cmd for cmd in self.get_all_commands() if cmd.startswith(data.statement.command)]\n            if len(possible_cmds) == 1:\n                raw = data.statement.raw.replace(data.statement.command, possible_cmds[0], 1)\n                data.statement = self.statement_parser.parse(raw)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_list(self, arglist: List[str]) -> None:\n        if arglist:\n            first = arglist[0]\n            try:\n                first = int(first)\n            except ValueError:\n                first = 1\n        else:\n            first = 1\n        last = first + 10\n\n        for x in range(first, last):\n            self.poutput(str(x))", "response": "Generate a list of 10 numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmumbles what you tell me to.", "response": "def do_mumble(self, args):\n        \"\"\"Mumbles what you tell me to.\"\"\"\n        repetitions = args.repeat or 1\n        for i in range(min(repetitions, self.maxrepeats)):\n            output = []\n            if random.random() < .33:\n                output.append(random.choice(self.MUMBLE_FIRST))\n            for word in args.words:\n                if random.random() < .40:\n                    output.append(random.choice(self.MUMBLES))\n                output.append(word)\n            if random.random() < .25:\n                output.append(random.choice(self.MUMBLE_LAST))\n            self.poutput(' '.join(output))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(argv=None):\n\n    parser = argparse.ArgumentParser(\n        description='Commands as arguments'\n    )\n    command_help = 'optional command to run, if no command given, enter an interactive shell'\n    parser.add_argument('command', nargs='?',\n                        help=command_help)\n    arg_help = 'optional arguments for command'\n    parser.add_argument('command_args', nargs=argparse.REMAINDER,\n                        help=arg_help)\n\n    args = parser.parse_args(argv)\n\n    c = CmdLineApp()\n\n    if args.command:\n        # we have a command, run it and then exit\n        c.onecmd_plus_hooks('{} {}'.format(args.command, ' '.join(args.command_args)))\n    else:\n        # we have no command, drop into interactive mode\n        c.cmdloop()", "response": "Run when invoked from the operating system shell"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds item command help", "response": "def do_add_item(self, args):\n        \"\"\"Add item command help\"\"\"\n        if args.food:\n            add_item = args.food\n        elif args.sport:\n            add_item = args.sport\n        elif args.other:\n            add_item = args.other\n        else:\n            add_item = 'no items'\n\n        self.poutput(\"You added {}\".format(add_item))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nspeak what you tell me to.", "response": "def do_speak(self, args: argparse.Namespace):\n        \"\"\"Repeats what you tell me to.\"\"\"\n        words = []\n        for word in args.words:\n            if args.piglatin:\n                word = '%s%say' % (word[1:], word[0])\n            if args.shout:\n                word = word.upper()\n            words.append(word)\n        repetitions = args.repeat or 1\n        for i in range(min(repetitions, self.maxrepeats)):\n            self.poutput(' '.join(words))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an html tag", "response": "def do_tag(self, args: argparse.Namespace):\n        \"\"\"create an html tag\"\"\"\n        # The Namespace always includes the Statement object created when parsing the command line\n        statement = args.__statement__\n\n        self.poutput(\"The command line you ran was: {}\".format(statement.command_and_args))\n        self.poutput(\"It generated this tag:\")\n        self.poutput('<{0}>{1}</{0}>'.format(args.tag, ' '.join(args.content)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nversion of creating an html tag using arglist instead of argparser", "response": "def do_tagg(self, arglist: List[str]):\n        \"\"\"version of creating an html tag using arglist instead of argparser\"\"\"\n        if len(arglist) >= 2:\n            tag = arglist[0]\n            content = arglist[1:]\n            self.poutput('<{0}>{1}</{0}>'.format(tag, ' '.join(content)))\n        else:\n            self.perror(\"tagg requires at least 2 arguments\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_exit(self, arg_list: List[str]) -> bool:\n        # If an argument was provided\n        if arg_list:\n            try:\n                self.exit_code = int(arg_list[0])\n            except ValueError:\n                self.perror(\"{} isn't a valid integer exit code\".format(arg_list[0]))\n                self.exit_code = -1\n\n        self._should_quit = True\n        return self._STOP_AND_EXIT", "response": "Exit the application with an optional exit code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef postloop(self) -> None:\n        code = self.exit_code if self.exit_code is not None else 0\n        self.poutput('{!r} exiting with code: {}'.format(sys.argv[0], code))", "response": "Hook method executed once when the cmdloop method is about to return."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncategorize a function. The help command output will group this function under the specified category heading :param func: function to categorize :param category: category to put it in", "response": "def categorize(func: Union[Callable, Iterable], category: str) -> None:\n    \"\"\"Categorize a function.\n\n    The help command output will group this function under the specified category heading\n\n    :param func: function to categorize\n    :param category: category to put it in\n    \"\"\"\n    if isinstance(func, Iterable):\n        for item in func:\n            setattr(item, HELP_CATEGORY, category)\n    else:\n        setattr(func, HELP_CATEGORY, category)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef with_category(category: str) -> Callable:\n    def cat_decorator(func):\n        categorize(func, category)\n        return func\n    return cat_decorator", "response": "A decorator to apply a category to a command function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef with_argument_list(*args: List[Callable], preserve_quotes: bool = False) -> Callable[[List], Optional[bool]]:\n    import functools\n\n    def arg_decorator(func: Callable):\n        @functools.wraps(func)\n        def cmd_wrapper(cmd2_instance, statement: Union[Statement, str]):\n            _, parsed_arglist = cmd2_instance.statement_parser.get_command_arg_list(command_name,\n                                                                                    statement,\n                                                                                    preserve_quotes)\n\n            return func(cmd2_instance, parsed_arglist)\n\n        command_name = func.__name__[len(COMMAND_FUNC_PREFIX):]\n        cmd_wrapper.__doc__ = func.__doc__\n        return cmd_wrapper\n\n    if len(args) == 1 and callable(args[0]):\n        return arg_decorator(args[0])\n    else:\n        return arg_decorator", "response": "A decorator to alter the arguments passed to a do_* cmd2 method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a string to a fileobject stripping ANSI escape sequences if necessary Honor the current colors setting which requires us to check whether the fileobject is a tty.", "response": "def decolorized_write(self, fileobj: IO, msg: str) -> None:\n        \"\"\"Write a string to a fileobject, stripping ANSI escape sequences if necessary\n\n        Honor the current colors setting, which requires us to check whether the\n        fileobject is a tty.\n        \"\"\"\n        if self.colors.lower() == constants.COLORS_NEVER.lower() or \\\n                (self.colors.lower() == constants.COLORS_TERMINAL.lower() and not fileobj.isatty()):\n            msg = utils.strip_ansi(msg)\n        fileobj.write(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint a message to stdout.", "response": "def poutput(self, msg: Any, end: str = '\\n', color: str = '') -> None:\n        \"\"\"Smarter self.stdout.write(); color aware and adds newline of not present.\n\n        Also handles BrokenPipeError exceptions for when a commands's output has\n        been piped to another process and that process terminates before the\n        cmd2 command is finished executing.\n\n        :param msg: message to print to current stdout (anything convertible to a str with '{}'.format() is OK)\n        :param end: (optional) string appended after the end of the message if not already present, default a newline\n        :param color: (optional) color escape to output this message with\n        \"\"\"\n        if msg is not None and msg != '':\n            try:\n                msg_str = '{}'.format(msg)\n                if not msg_str.endswith(end):\n                    msg_str += end\n                if color:\n                    msg_str = color + msg_str + Fore.RESET\n                self.decolorized_write(self.stdout, msg_str)\n            except BrokenPipeError:\n                # This occurs if a command's output is being piped to another\n                # process and that process closes before the command is\n                # finished. If you would like your application to print a\n                # warning message, then set the broken_pipe_warning attribute\n                # to the message you want printed.\n                if self.broken_pipe_warning:\n                    sys.stderr.write(self.broken_pipe_warning)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints an error message to sys. stderr and if debug is True print an exception Traceback if one exists.", "response": "def perror(self, err: Union[str, Exception], traceback_war: bool = True, err_color: str = Fore.LIGHTRED_EX,\n               war_color: str = Fore.LIGHTYELLOW_EX) -> None:\n        \"\"\" Print error message to sys.stderr and if debug is true, print an exception Traceback if one exists.\n\n        :param err: an Exception or error message to print out\n        :param traceback_war: (optional) if True, print a message to let user know they can enable debug\n        :param err_color: (optional) color escape to output error with\n        :param war_color: (optional) color escape to output warning with\n        \"\"\"\n        if self.debug and sys.exc_info() != (None, None, None):\n            import traceback\n            traceback.print_exc()\n\n        if isinstance(err, Exception):\n            err_msg = \"EXCEPTION of type '{}' occurred with message: '{}'\\n\".format(type(err).__name__, err)\n        else:\n            err_msg = \"{}\\n\".format(err)\n        err_msg = err_color + err_msg + Fore.RESET\n        self.decolorized_write(sys.stderr, err_msg)\n\n        if traceback_war and not self.debug:\n            war = \"To enable full traceback, run the following command:  'set debug true'\\n\"\n            war = war_color + war + Fore.RESET\n            self.decolorized_write(sys.stderr, war)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ppaged(self, msg: str, end: str = '\\n', chop: bool = False) -> None:\n        import subprocess\n        if msg is not None and msg != '':\n            try:\n                msg_str = '{}'.format(msg)\n                if not msg_str.endswith(end):\n                    msg_str += end\n\n                # Attempt to detect if we are not running within a fully functional terminal.\n                # Don't try to use the pager when being run by a continuous integration system like Jenkins + pexpect.\n                functional_terminal = False\n\n                if self.stdin.isatty() and self.stdout.isatty():\n                    if sys.platform.startswith('win') or os.environ.get('TERM') is not None:\n                        functional_terminal = True\n\n                # Don't attempt to use a pager that can block if redirecting or running a script (either text or Python)\n                # Also only attempt to use a pager if actually running in a real fully functional terminal\n                if functional_terminal and not self.redirecting and not self._in_py and not self._script_dir:\n                    if self.colors.lower() == constants.COLORS_NEVER.lower():\n                        msg_str = utils.strip_ansi(msg_str)\n\n                    pager = self.pager\n                    if chop:\n                        pager = self.pager_chop\n\n                    # Prevent KeyboardInterrupts while in the pager. The pager application will\n                    # still receive the SIGINT since it is in the same process group as us.\n                    with self.sigint_protection:\n                        pipe_proc = subprocess.Popen(pager, shell=True, stdin=subprocess.PIPE)\n                        pipe_proc.communicate(msg_str.encode('utf-8', 'replace'))\n                else:\n                    self.decolorized_write(self.stdout, msg_str)\n            except BrokenPipeError:\n                # This occurs if a command's output is being piped to another process and that process closes before the\n                # command is finished. If you would like your application to print a warning message, then set the\n                # broken_pipe_warning attribute to the message you want printed.`\n                if self.broken_pipe_warning:\n                    sys.stderr.write(self.broken_pipe_warning)", "response": "Print a message to stdout and stdout if it would go off screen and stdout isn t currently being redirected."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets tab completion settings", "response": "def reset_completion_defaults(self) -> None:\n        \"\"\"\n        Resets tab completion settings\n        Needs to be called each time readline runs tab completion\n        \"\"\"\n        self.allow_appended_space = True\n        self.allow_closing_quote = True\n        self.completion_header = ''\n        self.display_matches = []\n        self.matches_delimited = False\n        self.matches_sorted = False\n\n        if rl_type == RlType.GNU:\n            readline.set_completion_display_matches_hook(self._display_matches_gnu_readline)\n        elif rl_type == RlType.PYREADLINE:\n            # noinspection PyUnresolvedReferences\n            readline.rl.mode._display_completions = self._display_matches_pyreadline"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of tokens that are needed for tab completion functions.", "response": "def tokens_for_completion(self, line: str, begidx: int, endidx: int) -> Tuple[List[str], List[str]]:\n        \"\"\"\n        Used by tab completion functions to get all tokens through the one being completed\n        :param line: the current input line with leading whitespace removed\n        :param begidx: the beginning index of the prefix text\n        :param endidx: the ending index of the prefix text\n        :return: A 2 item tuple where the items are\n                 On Success\n                     tokens: list of unquoted tokens\n                             this is generally the list needed for tab completion functions\n                     raw_tokens: list of tokens with any quotes preserved\n                                 this can be used to know if a token was quoted or is missing a closing quote\n\n                     Both lists are guaranteed to have at least 1 item\n                     The last item in both lists is the token being tab completed\n\n                 On Failure\n                    Two empty lists\n        \"\"\"\n        import copy\n        unclosed_quote = ''\n        quotes_to_try = copy.copy(constants.QUOTES)\n\n        tmp_line = line[:endidx]\n        tmp_endidx = endidx\n\n        # Parse the line into tokens\n        while True:\n            try:\n                initial_tokens = shlex_split(tmp_line[:tmp_endidx])\n\n                # If the cursor is at an empty token outside of a quoted string,\n                # then that is the token being completed. Add it to the list.\n                if not unclosed_quote and begidx == tmp_endidx:\n                    initial_tokens.append('')\n                break\n            except ValueError as ex:\n                # Make sure the exception was due to an unclosed quote and\n                # we haven't exhausted the closing quotes to try\n                if str(ex) == \"No closing quotation\" and quotes_to_try:\n                    # Add a closing quote and try to parse again\n                    unclosed_quote = quotes_to_try[0]\n                    quotes_to_try = quotes_to_try[1:]\n\n                    tmp_line = line[:endidx]\n                    tmp_line += unclosed_quote\n                    tmp_endidx = endidx + 1\n                else:\n                    # The parsing error is not caused by unclosed quotes.\n                    # Return empty lists since this means the line is malformed.\n                    return [], []\n\n        if self.allow_redirection:\n\n            # Since redirection is enabled, we need to treat redirection characters (|, <, >)\n            # as word breaks when they are in unquoted strings. Go through each token\n            # and further split them on these characters. Each run of redirect characters\n            # is treated as a single token.\n            raw_tokens = []\n\n            for cur_initial_token in initial_tokens:\n\n                # Save tokens up to 1 character in length or quoted tokens. No need to parse these.\n                if len(cur_initial_token) <= 1 or cur_initial_token[0] in constants.QUOTES:\n                    raw_tokens.append(cur_initial_token)\n                    continue\n\n                # Iterate over each character in this token\n                cur_index = 0\n                cur_char = cur_initial_token[cur_index]\n\n                # Keep track of the token we are building\n                cur_raw_token = ''\n\n                while True:\n                    if cur_char not in constants.REDIRECTION_CHARS:\n\n                        # Keep appending to cur_raw_token until we hit a redirect char\n                        while cur_char not in constants.REDIRECTION_CHARS:\n                            cur_raw_token += cur_char\n                            cur_index += 1\n                            if cur_index < len(cur_initial_token):\n                                cur_char = cur_initial_token[cur_index]\n                            else:\n                                break\n\n                    else:\n                        redirect_char = cur_char\n\n                        # Keep appending to cur_raw_token until we hit something other than redirect_char\n                        while cur_char == redirect_char:\n                            cur_raw_token += cur_char\n                            cur_index += 1\n                            if cur_index < len(cur_initial_token):\n                                cur_char = cur_initial_token[cur_index]\n                            else:\n                                break\n\n                    # Save the current token\n                    raw_tokens.append(cur_raw_token)\n                    cur_raw_token = ''\n\n                    # Check if we've viewed all characters\n                    if cur_index >= len(cur_initial_token):\n                        break\n        else:\n            raw_tokens = initial_tokens\n\n        # Save the unquoted tokens\n        tokens = [utils.strip_quotes(cur_token) for cur_token in raw_tokens]\n\n        # If the token being completed had an unclosed quote, we need\n        # to remove the closing quote that was added in order for it\n        # to match what was on the command line.\n        if unclosed_quote:\n            raw_tokens[-1] = raw_tokens[-1][:-1]\n\n        return tokens, raw_tokens"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basic_complete(text: str, line: str, begidx: int, endidx: int, match_against: Iterable) -> List[str]:\n        return [cur_match for cur_match in match_against if cur_match.startswith(text)]", "response": "Performs tab completion against a list of possible tab completions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delimiter_complete(self, text: str, line: str, begidx: int, endidx: int, match_against: Iterable,\n                           delimiter: str) -> List[str]:\n        \"\"\"\n        Performs tab completion against a list but each match is split on a delimiter and only\n        the portion of the match being tab completed is shown as the completion suggestions.\n        This is useful if you match against strings that are hierarchical in nature and have a\n        common delimiter.\n\n        An easy way to illustrate this concept is path completion since paths are just directories/files\n        delimited by a slash. If you are tab completing items in /home/user you don't get the following\n        as suggestions:\n\n        /home/user/file.txt     /home/user/program.c\n        /home/user/maps/        /home/user/cmd2.py\n\n        Instead you are shown:\n\n        file.txt                program.c\n        maps/                   cmd2.py\n\n        For a large set of data, this can be visually more pleasing and easier to search.\n\n        Another example would be strings formatted with the following syntax: company::department::name\n        In this case the delimiter would be :: and the user could easily narrow down what they are looking\n        for if they were only shown suggestions in the category they are at in the string.\n\n        :param text: the string prefix we are attempting to match (all returned matches must begin with it)\n        :param line: the current input line with leading whitespace removed\n        :param begidx: the beginning index of the prefix text\n        :param endidx: the ending index of the prefix text\n        :param match_against: the list being matched against\n        :param delimiter: what delimits each portion of the matches (ex: paths are delimited by a slash)\n        :return: a list of possible tab completions\n        \"\"\"\n        matches = self.basic_complete(text, line, begidx, endidx, match_against)\n\n        # Display only the portion of the match that's being completed based on delimiter\n        if matches:\n            # Set this to True for proper quoting of matches with spaces\n            self.matches_delimited = True\n\n            # Get the common beginning for the matches\n            common_prefix = os.path.commonprefix(matches)\n            prefix_tokens = common_prefix.split(delimiter)\n\n            # Calculate what portion of the match we are completing\n            display_token_index = 0\n            if prefix_tokens:\n                display_token_index = len(prefix_tokens) - 1\n\n            # Get this portion for each match and store them in self.display_matches\n            for cur_match in matches:\n                match_tokens = cur_match.split(delimiter)\n                display_token = match_tokens[display_token_index]\n\n                if not display_token:\n                    display_token = delimiter\n                self.display_matches.append(display_token)\n\n        return matches", "response": "Performs tab completion against a list of possible tab completions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of tab completion strings based on a particular flag preceding the token being completed .", "response": "def flag_based_complete(self, text: str, line: str, begidx: int, endidx: int,\n                            flag_dict: Dict[str, Union[Iterable, Callable]],\n                            all_else: Union[None, Iterable, Callable] = None) -> List[str]:\n        \"\"\"\n        Tab completes based on a particular flag preceding the token being completed\n        :param text: the string prefix we are attempting to match (all returned matches must begin with it)\n        :param line: the current input line with leading whitespace removed\n        :param begidx: the beginning index of the prefix text\n        :param endidx: the ending index of the prefix text\n        :param flag_dict: dictionary whose structure is the following:\n                          keys - flags (ex: -c, --create) that result in tab completion for the next\n                                 argument in the command line\n                          values - there are two types of values\n                             1. iterable list of strings to match against (dictionaries, lists, etc.)\n                             2. function that performs tab completion (ex: path_complete)\n        :param all_else: an optional parameter for tab completing any token that isn't preceded by a flag in flag_dict\n        :return: a list of possible tab completions\n        \"\"\"\n        # Get all tokens through the one being completed\n        tokens, _ = self.tokens_for_completion(line, begidx, endidx)\n        if not tokens:\n            return []\n\n        completions_matches = []\n        match_against = all_else\n\n        # Must have at least 2 args for a flag to precede the token being completed\n        if len(tokens) > 1:\n            flag = tokens[-2]\n            if flag in flag_dict:\n                match_against = flag_dict[flag]\n\n        # Perform tab completion using a Collection\n        if isinstance(match_against, Collection):\n            completions_matches = self.basic_complete(text, line, begidx, endidx, match_against)\n\n        # Perform tab completion using a function\n        elif callable(match_against):\n            completions_matches = match_against(text, line, begidx, endidx)\n\n        return completions_matches"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef path_complete(self, text: str, line: str, begidx: int, endidx: int,\n                      path_filter: Optional[Callable[[str], bool]] = None) -> List[str]:\n        \"\"\"Performs completion of local file system paths\n\n        :param text: the string prefix we are attempting to match (all returned matches must begin with it)\n        :param line: the current input line with leading whitespace removed\n        :param begidx: the beginning index of the prefix text\n        :param endidx: the ending index of the prefix text\n        :param path_filter: optional filter function that determines if a path belongs in the results\n                            this function takes a path as its argument and returns True if the path should\n                            be kept in the results\n        :return: a list of possible tab completions\n        \"\"\"\n\n        # Used to complete ~ and ~user strings\n        def complete_users() -> List[str]:\n\n            # We are returning ~user strings that resolve to directories,\n            # so don't append a space or quote in the case of a single result.\n            self.allow_appended_space = False\n            self.allow_closing_quote = False\n\n            users = []\n\n            # Windows lacks the pwd module so we can't get a list of users.\n            # Instead we will return a result once the user enters text that\n            # resolves to an existing home directory.\n            if sys.platform.startswith('win'):\n                expanded_path = os.path.expanduser(text)\n                if os.path.isdir(expanded_path):\n                    user = text\n                    if add_trailing_sep_if_dir:\n                        user += os.path.sep\n                    users.append(user)\n            else:\n                import pwd\n\n                # Iterate through a list of users from the password database\n                for cur_pw in pwd.getpwall():\n\n                    # Check if the user has an existing home dir\n                    if os.path.isdir(cur_pw.pw_dir):\n\n                        # Add a ~ to the user to match against text\n                        cur_user = '~' + cur_pw.pw_name\n                        if cur_user.startswith(text):\n                            if add_trailing_sep_if_dir:\n                                cur_user += os.path.sep\n                            users.append(cur_user)\n\n            return users\n\n        # Determine if a trailing separator should be appended to directory completions\n        add_trailing_sep_if_dir = False\n        if endidx == len(line) or (endidx < len(line) and line[endidx] != os.path.sep):\n            add_trailing_sep_if_dir = True\n\n        # Used to replace cwd in the final results\n        cwd = os.getcwd()\n        cwd_added = False\n\n        # Used to replace expanded user path in final result\n        orig_tilde_path = ''\n        expanded_tilde_path = ''\n\n        # If the search text is blank, then search in the CWD for *\n        if not text:\n            search_str = os.path.join(os.getcwd(), '*')\n            cwd_added = True\n        else:\n            # Purposely don't match any path containing wildcards\n            wildcards = ['*', '?']\n            for wildcard in wildcards:\n                if wildcard in text:\n                    return []\n\n            # Start the search string\n            search_str = text + '*'\n\n            # Handle tilde expansion and completion\n            if text.startswith('~'):\n                sep_index = text.find(os.path.sep, 1)\n\n                # If there is no slash, then the user is still completing the user after the tilde\n                if sep_index == -1:\n                    return complete_users()\n\n                # Otherwise expand the user dir\n                else:\n                    search_str = os.path.expanduser(search_str)\n\n                    # Get what we need to restore the original tilde path later\n                    orig_tilde_path = text[:sep_index]\n                    expanded_tilde_path = os.path.expanduser(orig_tilde_path)\n\n            # If the search text does not have a directory, then use the cwd\n            elif not os.path.dirname(text):\n                search_str = os.path.join(os.getcwd(), search_str)\n                cwd_added = True\n\n        # Set this to True for proper quoting of paths with spaces\n        self.matches_delimited = True\n\n        # Find all matching path completions\n        matches = glob.glob(search_str)\n\n        # Filter out results that don't belong\n        if path_filter is not None:\n            matches = [c for c in matches if path_filter(c)]\n\n        # Don't append a space or closing quote to directory\n        if len(matches) == 1 and os.path.isdir(matches[0]):\n            self.allow_appended_space = False\n            self.allow_closing_quote = False\n\n        # Sort the matches before any trailing slashes are added\n        matches.sort(key=self.matches_sort_key)\n        self.matches_sorted = True\n\n        # Build display_matches and add a slash to directories\n        for index, cur_match in enumerate(matches):\n\n            # Display only the basename of this path in the tab-completion suggestions\n            self.display_matches.append(os.path.basename(cur_match))\n\n            # Add a separator after directories if the next character isn't already a separator\n            if os.path.isdir(cur_match) and add_trailing_sep_if_dir:\n                matches[index] += os.path.sep\n                self.display_matches[index] += os.path.sep\n\n        # Remove cwd if it was added to match the text readline expects\n        if cwd_added:\n            if cwd == os.path.sep:\n                to_replace = cwd\n            else:\n                to_replace = cwd + os.path.sep\n            matches = [cur_path.replace(to_replace, '', 1) for cur_path in matches]\n\n        # Restore the tilde string if we expanded one to match the text readline expects\n        if expanded_tilde_path:\n            matches = [cur_path.replace(expanded_tilde_path, orig_tilde_path, 1) for cur_path in matches]\n\n        return matches", "response": "Performs completion of local file system paths."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all executables in a user s path that start with the given string.", "response": "def get_exes_in_path(starts_with: str) -> List[str]:\n        \"\"\"Returns names of executables in a user's path\n\n        :param starts_with: what the exes should start with. leave blank for all exes in path.\n        :return: a list of matching exe names\n        \"\"\"\n        # Purposely don't match any executable containing wildcards\n        wildcards = ['*', '?']\n        for wildcard in wildcards:\n            if wildcard in starts_with:\n                return []\n\n        # Get a list of every directory in the PATH environment variable and ignore symbolic links\n        paths = [p for p in os.getenv('PATH').split(os.path.pathsep) if not os.path.islink(p)]\n\n        # Use a set to store exe names since there can be duplicates\n        exes_set = set()\n\n        # Find every executable file in the user's path that matches the pattern\n        for path in paths:\n            full_path = os.path.join(path, starts_with)\n            matches = [f for f in glob.glob(full_path + '*') if os.path.isfile(f) and os.access(f, os.X_OK)]\n\n            for match in matches:\n                exes_set.add(os.path.basename(match))\n\n        return list(exes_set)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shell_cmd_complete(self, text: str, line: str, begidx: int, endidx: int,\n                           complete_blank: bool = False) -> List[str]:\n        \"\"\"Performs completion of executables either in a user's path or a given path\n        :param text: the string prefix we are attempting to match (all returned matches must begin with it)\n        :param line: the current input line with leading whitespace removed\n        :param begidx: the beginning index of the prefix text\n        :param endidx: the ending index of the prefix text\n        :param complete_blank: If True, then a blank will complete all shell commands in a user's path\n                               If False, then no completion is performed\n                               Defaults to False to match Bash shell behavior\n        :return: a list of possible tab completions\n        \"\"\"\n        # Don't tab complete anything if no shell command has been started\n        if not complete_blank and not text:\n            return []\n\n        # If there are no path characters in the search text, then do shell command completion in the user's path\n        if not text.startswith('~') and os.path.sep not in text:\n            return self.get_exes_in_path(text)\n\n        # Otherwise look for executables in the given path\n        else:\n            return self.path_complete(text, line, begidx, endidx,\n                                      lambda path: os.path.isdir(path) or os.access(path, os.X_OK))", "response": "Performs completion of executables in a user s path or in a given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _redirect_complete(self, text: str, line: str, begidx: int, endidx: int, compfunc: Callable) -> List[str]:\n        if self.allow_redirection:\n\n            # Get all tokens through the one being completed. We want the raw tokens\n            # so we can tell if redirection strings are quoted and ignore them.\n            _, raw_tokens = self.tokens_for_completion(line, begidx, endidx)\n            if not raw_tokens:\n                return []\n\n            if len(raw_tokens) > 1:\n\n                # Check if there are redirection strings prior to the token being completed\n                seen_pipe = False\n                has_redirection = False\n\n                for cur_token in raw_tokens[:-1]:\n                    if cur_token in constants.REDIRECTION_TOKENS:\n                        has_redirection = True\n\n                        if cur_token == constants.REDIRECTION_PIPE:\n                            seen_pipe = True\n\n                # Get token prior to the one being completed\n                prior_token = raw_tokens[-2]\n\n                # If a pipe is right before the token being completed, complete a shell command as the piped process\n                if prior_token == constants.REDIRECTION_PIPE:\n                    return self.shell_cmd_complete(text, line, begidx, endidx)\n\n                # Otherwise do path completion either as files to redirectors or arguments to the piped process\n                elif prior_token in constants.REDIRECTION_TOKENS or seen_pipe:\n                    return self.path_complete(text, line, begidx, endidx)\n\n                # If there were redirection strings anywhere on the command line, then we\n                # are no longer tab completing for the current command\n                elif has_redirection:\n                    return []\n\n        # Call the command's completer function\n        return compfunc(text, line, begidx, endidx)", "response": "This function is called by the completion function for all commands that are redirected to the current command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd padding to the matches being displayed as tab completion suggestions.", "response": "def _pad_matches_to_display(matches_to_display: List[str]) -> Tuple[List[str], int]:  # pragma: no cover\n        \"\"\"Adds padding to the matches being displayed as tab completion suggestions.\n        The default padding of readline/pyreadine is small and not visually appealing\n        especially if matches have spaces. It appears very squished together.\n\n        :param matches_to_display: the matches being padded\n        :return: the padded matches and length of padding that was added\n        \"\"\"\n        if rl_type == RlType.GNU:\n            # Add 2 to the padding of 2 that readline uses for a total of 4.\n            padding = 2 * ' '\n\n        elif rl_type == RlType.PYREADLINE:\n            # Add 3 to the padding of 1 that pyreadline uses for a total of 4.\n            padding = 3 * ' '\n\n        else:\n            return matches_to_display, 0\n\n        return [cur_match + padding for cur_match in matches_to_display], len(padding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints a list of matches using GNU readline s display_match_list method.", "response": "def _display_matches_gnu_readline(self, substitution: str, matches: List[str],\n                                      longest_match_length: int) -> None:  # pragma: no cover\n        \"\"\"Prints a match list using GNU readline's rl_display_match_list()\n        This exists to print self.display_matches if it has data. Otherwise matches prints.\n\n        :param substitution: the substitution written to the command line\n        :param matches: the tab completion matches to display\n        :param longest_match_length: longest printed length of the matches\n        \"\"\"\n        if rl_type == RlType.GNU:\n\n            # Check if we should show display_matches\n            if self.display_matches:\n                matches_to_display = self.display_matches\n\n                # Recalculate longest_match_length for display_matches\n                longest_match_length = 0\n\n                for cur_match in matches_to_display:\n                    cur_length = utils.ansi_safe_wcswidth(cur_match)\n                    if cur_length > longest_match_length:\n                        longest_match_length = cur_length\n            else:\n                matches_to_display = matches\n\n            # Add padding for visual appeal\n            matches_to_display, padding_length = self._pad_matches_to_display(matches_to_display)\n            longest_match_length += padding_length\n\n            # We will use readline's display function (rl_display_match_list()), so we\n            # need to encode our string as bytes to place in a C array.\n            encoded_substitution = bytes(substitution, encoding='utf-8')\n            encoded_matches = [bytes(cur_match, encoding='utf-8') for cur_match in matches_to_display]\n\n            # rl_display_match_list() expects matches to be in argv format where\n            # substitution is the first element, followed by the matches, and then a NULL.\n            # noinspection PyCallingNonCallable,PyTypeChecker\n            strings_array = (ctypes.c_char_p * (1 + len(encoded_matches) + 1))()\n\n            # Copy in the encoded strings and add a NULL to the end\n            strings_array[0] = encoded_substitution\n            strings_array[1:-1] = encoded_matches\n            strings_array[-1] = None\n\n            # Print the header if one exists\n            if self.completion_header:\n                sys.stdout.write('\\n' + self.completion_header)\n\n            # Call readline's display function\n            # rl_display_match_list(strings_array, number of completion matches, longest match length)\n            readline_lib.rl_display_match_list(strings_array, len(encoded_matches), longest_match_length)\n\n            # Redraw prompt and input line\n            rl_force_redisplay()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a match list using pyreadline s _display_completions method.", "response": "def _display_matches_pyreadline(self, matches: List[str]) -> None:  # pragma: no cover\n        \"\"\"Prints a match list using pyreadline's _display_completions()\n        This exists to print self.display_matches if it has data. Otherwise matches prints.\n\n        :param matches: the tab completion matches to display\n        \"\"\"\n        if rl_type == RlType.PYREADLINE:\n\n            # Check if we should show display_matches\n            if self.display_matches:\n                matches_to_display = self.display_matches\n            else:\n                matches_to_display = matches\n\n            # Add padding for visual appeal\n            matches_to_display, _ = self._pad_matches_to_display(matches_to_display)\n\n            # Print the header if one exists\n            if self.completion_header:\n                # noinspection PyUnresolvedReferences\n                readline.rl.mode.console.write('\\n' + self.completion_header)\n\n            # Display matches using actual display function. This also redraws the prompt and line.\n            orig_pyreadline_display(matches_to_display)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride of command method which returns the next possible completion for text.", "response": "def complete(self, text: str, state: int) -> Optional[str]:\n        \"\"\"Override of command method which returns the next possible completion for 'text'.\n\n        If a command has not been entered, then complete against command list.\n        Otherwise try to call complete_<command> to get list of completions.\n\n        This method gets called directly by readline because it is set as the tab-completion function.\n\n        This completer function is called as complete(text, state), for state in 0, 1, 2, \u2026, until it returns a\n        non-string value. It should return the next possible completion starting with text.\n\n        :param text: the current word that user is typing\n        :param state: non-negative integer\n        \"\"\"\n        import functools\n        if state == 0 and rl_type != RlType.NONE:\n            unclosed_quote = ''\n            self.reset_completion_defaults()\n\n            # lstrip the original line\n            orig_line = readline.get_line_buffer()\n            line = orig_line.lstrip()\n            stripped = len(orig_line) - len(line)\n\n            # Calculate new indexes for the stripped line. If the cursor is at a position before the end of a\n            # line of spaces, then the following math could result in negative indexes. Enforce a max of 0.\n            begidx = max(readline.get_begidx() - stripped, 0)\n            endidx = max(readline.get_endidx() - stripped, 0)\n\n            # Shortcuts are not word break characters when tab completing. Therefore shortcuts become part\n            # of the text variable if there isn't a word break, like a space, after it. We need to remove it\n            # from text and update the indexes. This only applies if we are at the the beginning of the line.\n            shortcut_to_restore = ''\n            if begidx == 0:\n                for (shortcut, _) in self.shortcuts:\n                    if text.startswith(shortcut):\n                        # Save the shortcut to restore later\n                        shortcut_to_restore = shortcut\n\n                        # Adjust text and where it begins\n                        text = text[len(shortcut_to_restore):]\n                        begidx += len(shortcut_to_restore)\n                        break\n\n            # If begidx is greater than 0, then we are no longer completing the command\n            if begidx > 0:\n\n                # Parse the command line\n                statement = self.statement_parser.parse_command_only(line)\n                command = statement.command\n                expanded_line = statement.command_and_args\n\n                # We overwrote line with a properly formatted but fully stripped version\n                # Restore the end spaces since line is only supposed to be lstripped when\n                # passed to completer functions according to Python docs\n                rstripped_len = len(line) - len(line.rstrip())\n                expanded_line += ' ' * rstripped_len\n\n                # Fix the index values if expanded_line has a different size than line\n                if len(expanded_line) != len(line):\n                    diff = len(expanded_line) - len(line)\n                    begidx += diff\n                    endidx += diff\n\n                # Overwrite line to pass into completers\n                line = expanded_line\n\n                # Get all tokens through the one being completed\n                tokens, raw_tokens = self.tokens_for_completion(line, begidx, endidx)\n\n                # Check if we either had a parsing error or are trying to complete the command token\n                # The latter can happen if \" or ' was entered as the command\n                if len(tokens) <= 1:\n                    self.completion_matches = []\n                    return None\n\n                # Text we need to remove from completions later\n                text_to_remove = ''\n\n                # Get the token being completed with any opening quote preserved\n                raw_completion_token = raw_tokens[-1]\n\n                # Check if the token being completed has an opening quote\n                if raw_completion_token and raw_completion_token[0] in constants.QUOTES:\n\n                    # Since the token is still being completed, we know the opening quote is unclosed\n                    unclosed_quote = raw_completion_token[0]\n\n                    # readline still performs word breaks after a quote. Therefore something like quoted search\n                    # text with a space would have resulted in begidx pointing to the middle of the token we\n                    # we want to complete. Figure out where that token actually begins and save the beginning\n                    # portion of it that was not part of the text readline gave us. We will remove it from the\n                    # completions later since readline expects them to start with the original text.\n                    actual_begidx = line[:endidx].rfind(tokens[-1])\n\n                    if actual_begidx != begidx:\n                        text_to_remove = line[actual_begidx:begidx]\n\n                        # Adjust text and where it begins so the completer routines\n                        # get unbroken search text to complete on.\n                        text = text_to_remove + text\n                        begidx = actual_begidx\n\n                # Check if a valid command was entered\n                if command in self.get_all_commands():\n                    # Get the completer function for this command\n                    compfunc = getattr(self, 'complete_' + command, None)\n\n                    if compfunc is None:\n                        # There's no completer function, next see if the command uses argparser\n                        func = self.cmd_func(command)\n                        if func and hasattr(func, 'argparser'):\n                            compfunc = functools.partial(self._autocomplete_default,\n                                                         argparser=getattr(func, 'argparser'))\n                        else:\n                            compfunc = self.completedefault\n\n                # Check if a macro was entered\n                elif command in self.macros:\n                    compfunc = self.path_complete\n\n                # A valid command was not entered\n                else:\n                    # Check if this command should be run as a shell command\n                    if self.default_to_shell and command in self.get_exes_in_path(command):\n                        compfunc = self.path_complete\n                    else:\n                        compfunc = self.completedefault\n\n                # Attempt tab completion for redirection first, and if that isn't occurring,\n                # call the completer function for the current command\n                self.completion_matches = self._redirect_complete(text, line, begidx, endidx, compfunc)\n\n                if self.completion_matches:\n\n                    # Eliminate duplicates\n                    self.completion_matches = utils.remove_duplicates(self.completion_matches)\n                    self.display_matches = utils.remove_duplicates(self.display_matches)\n\n                    if not self.display_matches:\n                        # Since self.display_matches is empty, set it to self.completion_matches\n                        # before we alter them. That way the suggestions will reflect how we parsed\n                        # the token being completed and not how readline did.\n                        import copy\n                        self.display_matches = copy.copy(self.completion_matches)\n\n                    # Check if we need to add an opening quote\n                    if not unclosed_quote:\n\n                        add_quote = False\n\n                        # This is the tab completion text that will appear on the command line.\n                        common_prefix = os.path.commonprefix(self.completion_matches)\n\n                        if self.matches_delimited:\n                            # Check if any portion of the display matches appears in the tab completion\n                            display_prefix = os.path.commonprefix(self.display_matches)\n\n                            # For delimited matches, we check what appears before the display\n                            # matches (common_prefix) as well as the display matches themselves.\n                            if (' ' in common_prefix) or (display_prefix and ' ' in ''.join(self.display_matches)):\n                                add_quote = True\n\n                        # If there is a tab completion and any match has a space, then add an opening quote\n                        elif common_prefix and ' ' in ''.join(self.completion_matches):\n                            add_quote = True\n\n                        if add_quote:\n                            # Figure out what kind of quote to add and save it as the unclosed_quote\n                            if '\"' in ''.join(self.completion_matches):\n                                unclosed_quote = \"'\"\n                            else:\n                                unclosed_quote = '\"'\n\n                            self.completion_matches = [unclosed_quote + match for match in self.completion_matches]\n\n                    # Check if we need to remove text from the beginning of tab completions\n                    elif text_to_remove:\n                        self.completion_matches = \\\n                            [m.replace(text_to_remove, '', 1) for m in self.completion_matches]\n\n                    # Check if we need to restore a shortcut in the tab completions\n                    # so it doesn't get erased from the command line\n                    if shortcut_to_restore:\n                        self.completion_matches = \\\n                            [shortcut_to_restore + match for match in self.completion_matches]\n\n            else:\n                # Complete token against anything a user can run\n                self.completion_matches = self.basic_complete(text, line, begidx, endidx,\n                                                              self.get_commands_aliases_and_macros_for_completion())\n\n            # Handle single result\n            if len(self.completion_matches) == 1:\n                str_to_append = ''\n\n                # Add a closing quote if needed and allowed\n                if self.allow_closing_quote and unclosed_quote:\n                    str_to_append += unclosed_quote\n\n                # If we are at the end of the line, then add a space if allowed\n                if self.allow_appended_space and endidx == len(line):\n                    str_to_append += ' '\n\n                self.completion_matches[0] += str_to_append\n\n            # Sort matches if they haven't already been sorted\n            if not self.matches_sorted:\n                self.completion_matches.sort(key=self.matches_sort_key)\n                self.display_matches.sort(key=self.matches_sort_key)\n                self.matches_sorted = True\n\n        try:\n            return self.completion_matches[state]\n        except IndexError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefaulting completion function for argparse commands.", "response": "def _autocomplete_default(self, text: str, line: str, begidx: int, endidx: int,\n                              argparser: argparse.ArgumentParser) -> List[str]:\n        \"\"\"Default completion function for argparse commands.\"\"\"\n        completer = AutoCompleter(argparser, self)\n\n        tokens, _ = self.tokens_for_completion(line, begidx, endidx)\n        if not tokens:\n            return []\n\n        return completer.complete_command(tokens, text, line, begidx, endidx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_all_commands(self) -> List[str]:\n        return [name[len(COMMAND_FUNC_PREFIX):] for name in self.get_names()\n                if name.startswith(COMMAND_FUNC_PREFIX) and callable(getattr(self, name))]", "response": "Returns a list of all commands."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_visible_commands(self) -> List[str]:\n        commands = self.get_all_commands()\n\n        # Remove the hidden commands\n        for name in self.hidden_commands:\n            if name in commands:\n                commands.remove(name)\n\n        # Remove the disabled commands\n        for name in self.disabled_commands:\n            if name in commands:\n                commands.remove(name)\n\n        return commands", "response": "Returns a list of commands that have not been hidden or disabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_commands_aliases_and_macros_for_completion(self) -> List[str]:\n        visible_commands = set(self.get_visible_commands())\n        alias_names = set(self.get_alias_names())\n        macro_names = set(self.get_macro_names())\n        return list(visible_commands | alias_names | macro_names)", "response": "Return a list of visible commands aliases and macros for tab completion"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of help topics", "response": "def get_help_topics(self) -> List[str]:\n        \"\"\" Returns a list of help topics \"\"\"\n        return [name[len(HELP_FUNC_PREFIX):] for name in self.get_names()\n                if name.startswith(HELP_FUNC_PREFIX) and callable(getattr(self, name))]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsignal handler for SIGINTs which typically come from Ctrl - C events.", "response": "def sigint_handler(self, signum: int, frame) -> None:\n        \"\"\"Signal handler for SIGINTs which typically come from Ctrl-C events.\n\n        If you need custom SIGINT behavior, then override this function.\n\n        :param signum: signal number\n        :param frame\n        \"\"\"\n        if self.cur_pipe_proc_reader is not None:\n            # Pass the SIGINT to the current pipe process\n            self.cur_pipe_proc_reader.send_sigint()\n\n        # Check if we are allowed to re-raise the KeyboardInterrupt\n        if not self.sigint_protection:\n            raise KeyboardInterrupt(\"Got a keyboard interrupt\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the line into a command name and a string containing the arguments.", "response": "def parseline(self, line: str) -> Tuple[str, str, str]:\n        \"\"\"Parse the line into a command name and a string containing the arguments.\n\n        NOTE: This is an override of a parent class method.  It is only used by other parent class methods.\n\n        Different from the parent class method, this ignores self.identchars.\n\n        :param line: line read by readline\n        :return: tuple containing (command, args, line)\n        \"\"\"\n        statement = self.statement_parser.parse_command_only(line)\n        return statement.command, statement.args, statement.command_and_args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef onecmd_plus_hooks(self, line: str, pyscript_bridge_call: bool = False) -> bool:\n        import datetime\n\n        stop = False\n        try:\n            statement = self._complete_statement(line)\n        except EmptyStatement:\n            return self._run_cmdfinalization_hooks(stop, None)\n        except ValueError as ex:\n            # If shlex.split failed on syntax, let user know whats going on\n            self.perror(\"Invalid syntax: {}\".format(ex), traceback_war=False)\n            return stop\n\n        # now that we have a statement, run it with all the hooks\n        try:\n            # call the postparsing hooks\n            data = plugin.PostparsingData(False, statement)\n            for func in self._postparsing_hooks:\n                data = func(data)\n                if data.stop:\n                    break\n            # unpack the data object\n            statement = data.statement\n            stop = data.stop\n            if stop:\n                # we should not run the command, but\n                # we need to run the finalization hooks\n                raise EmptyStatement\n\n            # Keep track of whether or not we were already redirecting before this command\n            already_redirecting = self.redirecting\n\n            # This will be a utils.RedirectionSavedState object for the command\n            saved_state = None\n\n            try:\n                # Get sigint protection while we set up redirection\n                with self.sigint_protection:\n                    if pyscript_bridge_call:\n                        # Start saving command's stdout at this point\n                        self.stdout.pause_storage = False\n\n                    redir_error, saved_state = self._redirect_output(statement)\n                    self.cur_pipe_proc_reader = saved_state.pipe_proc_reader\n\n                # Do not continue if an error occurred while trying to redirect\n                if not redir_error:\n                    # See if we need to update self.redirecting\n                    if not already_redirecting:\n                        self.redirecting = saved_state.redirecting\n\n                    timestart = datetime.datetime.now()\n\n                    # precommand hooks\n                    data = plugin.PrecommandData(statement)\n                    for func in self._precmd_hooks:\n                        data = func(data)\n                    statement = data.statement\n\n                    # call precmd() for compatibility with cmd.Cmd\n                    statement = self.precmd(statement)\n\n                    # go run the command function\n                    stop = self.onecmd(statement)\n\n                    # postcommand hooks\n                    data = plugin.PostcommandData(stop, statement)\n                    for func in self._postcmd_hooks:\n                        data = func(data)\n\n                    # retrieve the final value of stop, ignoring any statement modification from the hooks\n                    stop = data.stop\n\n                    # call postcmd() for compatibility with cmd.Cmd\n                    stop = self.postcmd(stop, statement)\n\n                    if self.timing:\n                        self.pfeedback('Elapsed: {}'.format(datetime.datetime.now() - timestart))\n            finally:\n                # Get sigint protection while we restore stuff\n                with self.sigint_protection:\n                    if saved_state is not None:\n                        self._restore_output(statement, saved_state)\n\n                    if not already_redirecting:\n                        self.redirecting = False\n\n                    if pyscript_bridge_call:\n                        # Stop saving command's stdout before command finalization hooks run\n                        self.stdout.pause_storage = True\n\n        except EmptyStatement:\n            # don't do anything, but do allow command finalization hooks to run\n            pass\n        except Exception as ex:\n            self.perror(ex)\n        finally:\n            return self._run_cmdfinalization_hooks(stop, statement)", "response": "This function is called by the command loop to parse a line of text and run all of the hooks that are needed to parse the command and return True if the command should exit False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the command finalization hooks.", "response": "def _run_cmdfinalization_hooks(self, stop: bool, statement: Optional[Statement]) -> bool:\n        \"\"\"Run the command finalization hooks\"\"\"\n\n        with self.sigint_protection:\n            if not sys.platform.startswith('win') and self.stdout.isatty():\n                # Before the next command runs, fix any terminal problems like those\n                # caused by certain binary characters having been printed to it.\n                import subprocess\n                proc = subprocess.Popen(['stty', 'sane'])\n                proc.communicate()\n\n        try:\n            data = plugin.CommandFinalizationData(stop, statement)\n            for func in self._cmdfinalization_hooks:\n                data = func(data)\n            # retrieve the final value of stop, ignoring any\n            # modifications to the statement\n            return data.stop\n        except Exception as ex:\n            self.perror(ex)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef runcmds_plus_hooks(self, cmds: List[str]) -> bool:\n        stop = False\n        self.cmdqueue = list(cmds) + self.cmdqueue\n        try:\n            while self.cmdqueue and not stop:\n                line = self.cmdqueue.pop(0)\n                if self.echo and line != 'eos':\n                    self.poutput('{}{}'.format(self.prompt, line))\n\n                stop = self.onecmd_plus_hooks(line)\n        finally:\n            # Clear out the command queue and script directory stack, just in\n            # case we hit an error and they were not completed.\n            self.cmdqueue = []\n            self._script_dir = []\n            # NOTE: placing this return here inside the finally block will\n            # swallow exceptions. This is consistent with what is done in\n            # onecmd_plus_hooks and _cmdloop, although it may not be\n            # necessary/desired here.\n            return stop", "response": "Convenience method to run multiple commands by onecmd_plus_hooks."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _complete_statement(self, line: str) -> Statement:\n        while True:\n            try:\n                statement = self.statement_parser.parse(line)\n                if statement.multiline_command and statement.terminator:\n                    # we have a completed multiline command, we are done\n                    break\n                if not statement.multiline_command:\n                    # it's not a multiline command, but we parsed it ok\n                    # so we are done\n                    break\n            except ValueError:\n                # we have unclosed quotation marks, lets parse only the command\n                # and see if it's a multiline\n                statement = self.statement_parser.parse_command_only(line)\n                if not statement.multiline_command:\n                    # not a multiline command, so raise the exception\n                    raise\n\n            # if we get here we must have:\n            #   - a multiline command with no terminator\n            #   - a multiline command with unclosed quotation marks\n            try:\n                self.at_continuation_prompt = True\n                newline = self.pseudo_raw_input(self.continuation_prompt)\n                if newline == 'eof':\n                    # they entered either a blank line, or we hit an EOF\n                    # for some other reason. Turn the literal 'eof'\n                    # into a blank line, which serves as a command\n                    # terminator\n                    newline = '\\n'\n                    self.poutput(newline)\n                line = '{}\\n{}'.format(statement.raw, newline)\n            except KeyboardInterrupt as ex:\n                if self.quit_on_sigint:\n                    raise ex\n                else:\n                    self.poutput('^C')\n                    statement = self.statement_parser.parse('')\n                    break\n            finally:\n                self.at_continuation_prompt = False\n\n        if not statement.command:\n            raise EmptyStatement()\n        return statement", "response": "This method is used to parse a line of input until the command is complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _redirect_output(self, statement: Statement) -> Tuple[bool, utils.RedirectionSavedState]:\n        import io\n        import subprocess\n\n        redir_error = False\n\n        # Initialize the saved state\n        saved_state = utils.RedirectionSavedState(self.stdout, sys.stdout, self.cur_pipe_proc_reader)\n\n        if not self.allow_redirection:\n            return redir_error, saved_state\n\n        if statement.pipe_to:\n            # Create a pipe with read and write sides\n            read_fd, write_fd = os.pipe()\n\n            # Open each side of the pipe\n            subproc_stdin = io.open(read_fd, 'r')\n            new_stdout = io.open(write_fd, 'w')\n\n            # We want Popen to raise an exception if it fails to open the process.  Thus we don't set shell to True.\n            try:\n                # Set options to not forward signals to the pipe process. If a Ctrl-C event occurs,\n                # our sigint handler will forward it only to the most recent pipe process. This makes\n                # sure pipe processes close in the right order (most recent first).\n                if sys.platform == 'win32':\n                    creationflags = subprocess.CREATE_NEW_PROCESS_GROUP\n                    start_new_session = False\n                else:\n                    creationflags = 0\n                    start_new_session = True\n\n                # For any stream that is a StdSim, we will use a pipe so we can capture its output\n                proc = \\\n                    subprocess.Popen(statement.pipe_to,\n                                     stdin=subproc_stdin,\n                                     stdout=subprocess.PIPE if isinstance(self.stdout, utils.StdSim) else self.stdout,\n                                     stderr=subprocess.PIPE if isinstance(sys.stderr, utils.StdSim) else sys.stderr,\n                                     creationflags=creationflags,\n                                     start_new_session=start_new_session)\n\n                saved_state.redirecting = True\n                saved_state.pipe_proc_reader = utils.ProcReader(proc, self.stdout, sys.stderr)\n                sys.stdout = self.stdout = new_stdout\n            except Exception as ex:\n                self.perror('Failed to open pipe because - {}'.format(ex), traceback_war=False)\n                subproc_stdin.close()\n                new_stdout.close()\n                redir_error = True\n\n        elif statement.output:\n            import tempfile\n            if (not statement.output_to) and (not self.can_clip):\n                self.perror(\"Cannot redirect to paste buffer; install 'pyperclip' and re-run to enable\",\n                            traceback_war=False)\n                redir_error = True\n\n            elif statement.output_to:\n                # going to a file\n                mode = 'w'\n                # statement.output can only contain\n                # REDIRECTION_APPEND or REDIRECTION_OUTPUT\n                if statement.output == constants.REDIRECTION_APPEND:\n                    mode = 'a'\n                try:\n                    new_stdout = open(statement.output_to, mode)\n                    saved_state.redirecting = True\n                    sys.stdout = self.stdout = new_stdout\n                except OSError as ex:\n                    self.perror('Failed to redirect because - {}'.format(ex), traceback_war=False)\n                    redir_error = True\n            else:\n                # going to a paste buffer\n                new_stdout = tempfile.TemporaryFile(mode=\"w+\")\n                saved_state.redirecting = True\n                sys.stdout = self.stdout = new_stdout\n\n                if statement.output == constants.REDIRECTION_APPEND:\n                    self.poutput(get_paste_buffer())\n\n        return redir_error, saved_state", "response": "Redirect the output of a user - specified statement to the most recent process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle restoring state after output redirection.", "response": "def _restore_output(self, statement: Statement, saved_state: utils.RedirectionSavedState) -> None:\n        \"\"\"Handles restoring state after output redirection as well as\n        the actual pipe operation if present.\n\n        :param statement: Statement object which contains the parsed input from the user\n        :param saved_state: contains information needed to restore state data\n        \"\"\"\n        if saved_state.redirecting:\n            # If we redirected output to the clipboard\n            if statement.output and not statement.output_to:\n                self.stdout.seek(0)\n                write_to_paste_buffer(self.stdout.read())\n\n            try:\n                # Close the file or pipe that stdout was redirected to\n                self.stdout.close()\n            except BrokenPipeError:\n                pass\n\n            # Restore the stdout values\n            self.stdout = saved_state.saved_self_stdout\n            sys.stdout = saved_state.saved_sys_stdout\n\n            # Check if we need to wait for the process being piped to\n            if self.cur_pipe_proc_reader is not None:\n                self.cur_pipe_proc_reader.wait()\n\n        # Restore cur_pipe_proc_reader. This always is done, regardless of whether this command redirected.\n        self.cur_pipe_proc_reader = saved_state.saved_pipe_proc_reader"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cmd_func(self, command: str) -> Optional[Callable]:\n        func_name = self.cmd_func_name(command)\n        if func_name:\n            return getattr(self, func_name)", "response": "Get the function for a command"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmd_func_name(self, command: str) -> str:\n        target = COMMAND_FUNC_PREFIX + command\n        return target if callable(getattr(self, target, None)) else ''", "response": "Get the method name associated with a given command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nresolve a macro and run the command line", "response": "def _run_macro(self, statement: Statement) -> bool:\n        \"\"\"\n        Resolve a macro and run the resulting string\n\n        :param statement: the parsed statement from the command line\n        :return: a flag indicating whether the interpretation of commands should stop\n        \"\"\"\n        from itertools import islice\n\n        if statement.command not in self.macros.keys():\n            raise KeyError('{} is not a macro'.format(statement.command))\n\n        macro = self.macros[statement.command]\n\n        # Make sure enough arguments were passed in\n        if len(statement.arg_list) < macro.minimum_arg_count:\n            self.perror(\"The macro '{}' expects at least {} argument(s)\".format(statement.command,\n                                                                                macro.minimum_arg_count),\n                        traceback_war=False)\n            return False\n\n        # Resolve the arguments in reverse and read their values from statement.argv since those\n        # are unquoted. Macro args should have been quoted when the macro was created.\n        resolved = macro.value\n        reverse_arg_list = sorted(macro.arg_list, key=lambda ma: ma.start_index, reverse=True)\n\n        for arg in reverse_arg_list:\n            if arg.is_escaped:\n                to_replace = '{{' + arg.number_str + '}}'\n                replacement = '{' + arg.number_str + '}'\n            else:\n                to_replace = '{' + arg.number_str + '}'\n                replacement = statement.argv[int(arg.number_str)]\n\n            parts = resolved.rsplit(to_replace, maxsplit=1)\n            resolved = parts[0] + replacement + parts[1]\n\n        # Append extra arguments and use statement.arg_list since these arguments need their quotes preserved\n        for arg in islice(statement.arg_list, macro.minimum_arg_count, None):\n            resolved += ' ' + arg\n\n        # Run the resolved command\n        return self.onecmd_plus_hooks(resolved)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef default(self, statement: Statement) -> Optional[bool]:\n        if self.default_to_shell:\n            if 'shell' not in self.exclude_from_history:\n                self.history.append(statement)\n\n            return self.do_shell(statement.command_and_args)\n        else:\n            err_msg = self.default_error.format(statement.command)\n            self.decolorized_write(sys.stderr, \"{}\\n\".format(err_msg))", "response": "Executed when the command given isn t a recognized command."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pseudo_raw_input(self, prompt: str) -> str:\n        if self.use_rawinput:\n            try:\n                if sys.stdin.isatty():\n                    # Wrap in try since terminal_lock may not be locked when this function is called from unit tests\n                    try:\n                        # A prompt is about to be drawn. Allow asynchronous changes to the terminal.\n                        self.terminal_lock.release()\n                    except RuntimeError:\n                        pass\n\n                    # Deal with the vagaries of readline and ANSI escape codes\n                    safe_prompt = rl_make_safe_prompt(prompt)\n                    line = input(safe_prompt)\n                else:\n                    line = input()\n                    if self.echo:\n                        sys.stdout.write('{}{}\\n'.format(prompt, line))\n            except EOFError:\n                line = 'eof'\n            finally:\n                if sys.stdin.isatty():\n                    # The prompt is gone. Do not allow asynchronous changes to the terminal.\n                    self.terminal_lock.acquire()\n        else:\n            if self.stdin.isatty():\n                # on a tty, print the prompt first, then read the line\n                self.poutput(prompt, end='')\n                self.stdout.flush()\n                line = self.stdin.readline()\n                if len(line) == 0:\n                    line = 'eof'\n            else:\n                # we are reading from a pipe, read the line to see if there is\n                # anything there, if so, then decide whether to print the\n                # prompt or not\n                line = self.stdin.readline()\n                if len(line):\n                    # we read something, output the prompt and the something\n                    if self.echo:\n                        self.poutput('{}{}'.format(prompt, line))\n                else:\n                    line = 'eof'\n\n        return line.strip()", "response": "This function is a copy of cmdloop s raw_input method that does not copy the cmdloop s raw_input method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alias_create(self, args: argparse.Namespace) -> None:\n\n        # Validate the alias name\n        valid, errmsg = self.statement_parser.is_valid_command(args.name)\n        if not valid:\n            self.perror(\"Invalid alias name: {}\".format(errmsg), traceback_war=False)\n            return\n\n        if args.name in self.macros:\n            self.perror(\"Alias cannot have the same name as a macro\", traceback_war=False)\n            return\n\n        utils.unquote_redirection_tokens(args.command_args)\n\n        # Build the alias value string\n        value = args.command\n        if args.command_args:\n            value += ' ' + ' '.join(args.command_args)\n\n        # Set the alias\n        result = \"overwritten\" if args.name in self.aliases else \"created\"\n        self.aliases[args.name] = value\n        self.poutput(\"Alias '{}' {}\".format(args.name, result))", "response": "Create or overwrite an alias"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists some or all aliases", "response": "def alias_list(self, args: argparse.Namespace) -> None:\n        \"\"\"List some or all aliases\"\"\"\n        if args.name:\n            for cur_name in utils.remove_duplicates(args.name):\n                if cur_name in self.aliases:\n                    self.poutput(\"alias create {} {}\".format(cur_name, self.aliases[cur_name]))\n                else:\n                    self.perror(\"Alias '{}' not found\".format(cur_name), traceback_war=False)\n        else:\n            sorted_aliases = utils.alphabetical_sort(self.aliases)\n            for cur_alias in sorted_aliases:\n                self.poutput(\"alias create {} {}\".format(cur_alias, self.aliases[cur_alias]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates or overwrite a macro.", "response": "def macro_create(self, args: argparse.Namespace) -> None:\n        \"\"\"Create or overwrite a macro\"\"\"\n\n        # Validate the macro name\n        valid, errmsg = self.statement_parser.is_valid_command(args.name)\n        if not valid:\n            self.perror(\"Invalid macro name: {}\".format(errmsg), traceback_war=False)\n            return\n\n        if args.name in self.get_all_commands():\n            self.perror(\"Macro cannot have the same name as a command\", traceback_war=False)\n            return\n\n        if args.name in self.aliases:\n            self.perror(\"Macro cannot have the same name as an alias\", traceback_war=False)\n            return\n\n        utils.unquote_redirection_tokens(args.command_args)\n\n        # Build the macro value string\n        value = args.command\n        if args.command_args:\n            value += ' ' + ' '.join(args.command_args)\n\n        # Find all normal arguments\n        arg_list = []\n        normal_matches = re.finditer(MacroArg.macro_normal_arg_pattern, value)\n        max_arg_num = 0\n        arg_nums = set()\n\n        while True:\n            try:\n                cur_match = normal_matches.__next__()\n\n                # Get the number string between the braces\n                cur_num_str = (re.findall(MacroArg.digit_pattern, cur_match.group())[0])\n                cur_num = int(cur_num_str)\n                if cur_num < 1:\n                    self.perror(\"Argument numbers must be greater than 0\", traceback_war=False)\n                    return\n\n                arg_nums.add(cur_num)\n                if cur_num > max_arg_num:\n                    max_arg_num = cur_num\n\n                arg_list.append(MacroArg(start_index=cur_match.start(), number_str=cur_num_str, is_escaped=False))\n\n            except StopIteration:\n                break\n\n        # Make sure the argument numbers are continuous\n        if len(arg_nums) != max_arg_num:\n            self.perror(\"Not all numbers between 1 and {} are present \"\n                        \"in the argument placeholders\".format(max_arg_num), traceback_war=False)\n            return\n\n        # Find all escaped arguments\n        escaped_matches = re.finditer(MacroArg.macro_escaped_arg_pattern, value)\n\n        while True:\n            try:\n                cur_match = escaped_matches.__next__()\n\n                # Get the number string between the braces\n                cur_num_str = re.findall(MacroArg.digit_pattern, cur_match.group())[0]\n\n                arg_list.append(MacroArg(start_index=cur_match.start(), number_str=cur_num_str, is_escaped=True))\n            except StopIteration:\n                break\n\n        # Set the macro\n        result = \"overwritten\" if args.name in self.macros else \"created\"\n        self.macros[args.name] = Macro(name=args.name, value=value, minimum_arg_count=max_arg_num, arg_list=arg_list)\n        self.poutput(\"Macro '{}' {}\".format(args.name, result))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef macro_list(self, args: argparse.Namespace) -> None:\n        if args.name:\n            for cur_name in utils.remove_duplicates(args.name):\n                if cur_name in self.macros:\n                    self.poutput(\"macro create {} {}\".format(cur_name, self.macros[cur_name].value))\n                else:\n                    self.perror(\"Macro '{}' not found\".format(cur_name), traceback_war=False)\n        else:\n            sorted_macros = utils.alphabetical_sort(self.macros)\n            for cur_macro in sorted_macros:\n                self.poutput(\"macro create {} {}\".format(cur_macro, self.macros[cur_macro].value))", "response": "List some or all macros"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompleting the command argument of help", "response": "def complete_help_command(self, text: str, line: str, begidx: int, endidx: int) -> List[str]:\n        \"\"\"Completes the command argument of help\"\"\"\n\n        # Complete token against topics and visible commands\n        topics = set(self.get_help_topics())\n        visible_commands = set(self.get_visible_commands())\n        strs_to_match = list(topics | visible_commands)\n        return self.basic_complete(text, line, begidx, endidx, strs_to_match)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncomplete the subcommand argument of help", "response": "def complete_help_subcommand(self, text: str, line: str, begidx: int, endidx: int) -> List[str]:\n        \"\"\"Completes the subcommand argument of help\"\"\"\n\n        # Get all tokens through the one being completed\n        tokens, _ = self.tokens_for_completion(line, begidx, endidx)\n\n        if not tokens:\n            return []\n\n        # Must have at least 3 args for 'help command sub-command'\n        if len(tokens) < 3:\n            return []\n\n        # Find where the command is by skipping past any flags\n        cmd_index = 1\n        for cur_token in tokens[cmd_index:]:\n            if not cur_token.startswith('-'):\n                break\n            cmd_index += 1\n\n        if cmd_index >= len(tokens):\n            return []\n\n        command = tokens[cmd_index]\n        matches = []\n\n        # Check if this is a command with an argparse function\n        func = self.cmd_func(command)\n        if func and hasattr(func, 'argparser'):\n            completer = AutoCompleter(getattr(func, 'argparser'), self)\n            matches = completer.complete_command_help(tokens[cmd_index:], text, line, begidx, endidx)\n\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists available commands or provide detailed help for a specific command", "response": "def do_help(self, args: argparse.Namespace) -> None:\n        \"\"\"List available commands or provide detailed help for a specific command\"\"\"\n        if not args.command or args.verbose:\n            self._help_menu(args.verbose)\n\n        else:\n            # Getting help for a specific command\n            func = self.cmd_func(args.command)\n            help_func = getattr(self, HELP_FUNC_PREFIX + args.command, None)\n\n            # If the command function uses argparse, then use argparse's help\n            if func and hasattr(func, 'argparser'):\n                completer = AutoCompleter(getattr(func, 'argparser'), self)\n                tokens = [args.command] + args.subcommand\n                self.poutput(completer.format_help(tokens))\n\n            # If there is no help information then print an error\n            elif help_func is None and (func is None or not func.__doc__):\n                err_msg = self.help_error.format(args.command)\n                self.decolorized_write(sys.stderr, \"{}\\n\".format(err_msg))\n\n            # Otherwise delegate to cmd base class do_help()\n            else:\n                super().do_help(args.command)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow a list of commands which help can be displayed for.", "response": "def _help_menu(self, verbose: bool = False) -> None:\n        \"\"\"Show a list of commands which help can be displayed for.\n        \"\"\"\n        # Get a sorted list of help topics\n        help_topics = utils.alphabetical_sort(self.get_help_topics())\n\n        # Get a sorted list of visible command names\n        visible_commands = utils.alphabetical_sort(self.get_visible_commands())\n\n        cmds_doc = []\n        cmds_undoc = []\n        cmds_cats = {}\n\n        for command in visible_commands:\n            func = self.cmd_func(command)\n            has_help_func = False\n\n            if command in help_topics:\n                # Prevent the command from showing as both a command and help topic in the output\n                help_topics.remove(command)\n\n                # Non-argparse commands can have help_functions for their documentation\n                if not hasattr(func, 'argparser'):\n                    has_help_func = True\n\n            if hasattr(func, HELP_CATEGORY):\n                category = getattr(func, HELP_CATEGORY)\n                cmds_cats.setdefault(category, [])\n                cmds_cats[category].append(command)\n            elif func.__doc__ or has_help_func:\n                cmds_doc.append(command)\n            else:\n                cmds_undoc.append(command)\n\n        if len(cmds_cats) == 0:\n            # No categories found, fall back to standard behavior\n            self.poutput(\"{}\\n\".format(str(self.doc_leader)))\n            self._print_topics(self.doc_header, cmds_doc, verbose)\n        else:\n            # Categories found, Organize all commands by category\n            self.poutput('{}\\n'.format(str(self.doc_leader)))\n            self.poutput('{}\\n\\n'.format(str(self.doc_header)))\n            for category in sorted(cmds_cats.keys()):\n                self._print_topics(category, cmds_cats[category], verbose)\n            self._print_topics('Other', cmds_doc, verbose)\n\n        self.print_topics(self.misc_header, help_topics, 15, 80)\n        self.print_topics(self.undoc_header, cmds_undoc, 15, 80)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncustomizing version of print_topics that can switch between traditional output", "response": "def _print_topics(self, header: str, cmds: List[str], verbose: bool) -> None:\n        \"\"\"Customized version of print_topics that can switch between verbose or traditional output\"\"\"\n        import io\n\n        if cmds:\n            if not verbose:\n                self.print_topics(header, cmds, 15, 80)\n            else:\n                self.stdout.write('{}\\n'.format(str(header)))\n                widest = 0\n                # measure the commands\n                for command in cmds:\n                    width = utils.ansi_safe_wcswidth(command)\n                    if width > widest:\n                        widest = width\n                # add a 4-space pad\n                widest += 4\n                if widest < 20:\n                    widest = 20\n\n                if self.ruler:\n                    self.stdout.write('{:{ruler}<{width}}\\n'.format('', ruler=self.ruler, width=80))\n\n                # Try to get the documentation string for each command\n                topics = self.get_help_topics()\n\n                for command in cmds:\n                    cmd_func = self.cmd_func(command)\n\n                    # Non-argparse commands can have help_functions for their documentation\n                    if not hasattr(cmd_func, 'argparser') and command in topics:\n                        help_func = getattr(self, HELP_FUNC_PREFIX + command)\n                        result = io.StringIO()\n\n                        # try to redirect system stdout\n                        with redirect_stdout(result):\n                            # save our internal stdout\n                            stdout_orig = self.stdout\n                            try:\n                                # redirect our internal stdout\n                                self.stdout = result\n                                help_func()\n                            finally:\n                                # restore internal stdout\n                                self.stdout = stdout_orig\n                        doc = result.getvalue()\n\n                    else:\n                        doc = cmd_func.__doc__\n\n                    # Attempt to locate the first documentation block\n                    if not doc:\n                        doc_block = ['']\n                    else:\n                        doc_block = []\n                        found_first = False\n                        for doc_line in doc.splitlines():\n                            stripped_line = doc_line.strip()\n\n                            # Don't include :param type lines\n                            if stripped_line.startswith(':'):\n                                if found_first:\n                                    break\n                            elif stripped_line:\n                                doc_block.append(stripped_line)\n                                found_first = True\n                            elif found_first:\n                                break\n\n                    for doc_line in doc_block:\n                        self.stdout.write('{: <{col_width}}{doc}\\n'.format(command,\n                                                                           col_width=widest,\n                                                                           doc=doc_line))\n                        command = ''\n                self.stdout.write(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select(self, opts: Union[str, List[str], List[Tuple[Any, Optional[str]]]],\n               prompt: str = 'Your choice? ') -> str:\n        \"\"\"Presents a numbered menu to the user.  Modeled after\n           the bash shell's SELECT.  Returns the item chosen.\n\n           Argument ``opts`` can be:\n\n             | a single string -> will be split into one-word options\n             | a list of strings -> will be offered as options\n             | a list of tuples -> interpreted as (value, text), so\n                                   that the return value can differ from\n                                   the text advertised to the user \"\"\"\n        local_opts = opts\n        if isinstance(opts, str):\n            local_opts = list(zip(opts.split(), opts.split()))\n        fulloptions = []\n        for opt in local_opts:\n            if isinstance(opt, str):\n                fulloptions.append((opt, opt))\n            else:\n                try:\n                    fulloptions.append((opt[0], opt[1]))\n                except IndexError:\n                    fulloptions.append((opt[0], opt[0]))\n        for (idx, (_, text)) in enumerate(fulloptions):\n            self.poutput('  %2d. %s\\n' % (idx + 1, text))\n        while True:\n            safe_prompt = rl_make_safe_prompt(prompt)\n            response = input(safe_prompt)\n\n            if rl_type != RlType.NONE:\n                hlen = readline.get_current_history_length()\n                if hlen >= 1 and response != '':\n                    readline.remove_history_item(hlen - 1)\n\n            try:\n                choice = int(response)\n                if choice < 1:\n                    raise IndexError\n                result = fulloptions[choice - 1][0]\n                break\n            except (ValueError, IndexError):\n                self.poutput(\"{!r} isn't a valid choice. Pick a number between 1 and {}:\\n\".format(response,\n                                                                                                   len(fulloptions)))\n        return result", "response": "Presents a numbered menu to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a summary report of read - only settings which the user cannot modify at runtime.", "response": "def cmdenvironment(self) -> str:\n        \"\"\"Get a summary report of read-only settings which the user cannot modify at runtime.\n\n        :return: summary report of read-only settings which the user cannot modify at runtime\n        \"\"\"\n        read_only_settings = \"\"\"\n        Commands may be terminated with: {}\n        Arguments at invocation allowed: {}\n        Output redirection and pipes allowed: {}\"\"\"\n        return read_only_settings.format(str(self.statement_parser.terminators), self.allow_cli_args,\n                                         self.allow_redirection)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show(self, args: argparse.Namespace, parameter: str = '') -> None:\n        param = utils.norm_fold(parameter.strip())\n        result = {}\n        maxlen = 0\n\n        for p in self.settable:\n            if (not param) or p.startswith(param):\n                result[p] = '{}: {}'.format(p, str(getattr(self, p)))\n                maxlen = max(maxlen, len(result[p]))\n\n        if result:\n            for p in sorted(result):\n                if args.long:\n                    self.poutput('{} # {}'.format(result[p].ljust(maxlen), self.settable[p]))\n                else:\n                    self.poutput(result[p])\n\n            # If user has requested to see all settings, also show read-only settings\n            if args.all:\n                self.poutput('\\nRead only settings:{}'.format(self.cmdenvironment()))\n        else:\n            self.perror(\"Parameter '{}' not supported (type 'set' for list of parameters).\".format(param),\n                        traceback_war=False)", "response": "Shows current settings of the current set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a settable parameter or show current settings of parameters", "response": "def do_set(self, args: argparse.Namespace) -> None:\n        \"\"\"Set a settable parameter or show current settings of parameters\"\"\"\n\n        # Check if param was passed in\n        if not args.param:\n            return self.show(args)\n        param = utils.norm_fold(args.param.strip())\n\n        # Check if value was passed in\n        if not args.value:\n            return self.show(args, param)\n        value = args.value\n\n        # Check if param points to just one settable\n        if param not in self.settable:\n            hits = [p for p in self.settable if p.startswith(param)]\n            if len(hits) == 1:\n                param = hits[0]\n            else:\n                return self.show(args, param)\n\n        # Update the settable's value\n        current_value = getattr(self, param)\n        value = utils.cast(current_value, value)\n        setattr(self, param, value)\n\n        self.poutput('{} - was: {}\\nnow: {}\\n'.format(param, current_value, value))\n\n        # See if we need to call a change hook for this settable\n        if current_value != value:\n            onchange_hook = getattr(self, '_onchange_{}'.format(param), None)\n            if onchange_hook is not None:\n                onchange_hook(old=current_value, new=value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a command as if at the OS prompt and return the result of the command.", "response": "def do_shell(self, args: argparse.Namespace) -> None:\n        \"\"\"Execute a command as if at the OS prompt\"\"\"\n        import subprocess\n\n        # Create a list of arguments to shell\n        tokens = [args.command] + args.command_args\n\n        # Support expanding ~ in quoted paths\n        for index, _ in enumerate(tokens):\n            if tokens[index]:\n                # Check if the token is quoted. Since parsing already passed, there isn't\n                # an unclosed quote. So we only need to check the first character.\n                first_char = tokens[index][0]\n                if first_char in constants.QUOTES:\n                    tokens[index] = utils.strip_quotes(tokens[index])\n\n                tokens[index] = os.path.expanduser(tokens[index])\n\n                # Restore the quotes\n                if first_char in constants.QUOTES:\n                    tokens[index] = first_char + tokens[index] + first_char\n\n        expanded_command = ' '.join(tokens)\n\n        # Prevent KeyboardInterrupts while in the shell process. The shell process will\n        # still receive the SIGINT since it is in the same process group as us.\n        with self.sigint_protection:\n            # For any stream that is a StdSim, we will use a pipe so we can capture its output\n            proc = subprocess.Popen(expanded_command,\n                                    stdout=subprocess.PIPE if isinstance(self.stdout, utils.StdSim) else self.stdout,\n                                    stderr=subprocess.PIPE if isinstance(sys.stderr, utils.StdSim) else sys.stderr,\n                                    shell=True)\n\n            proc_reader = utils.ProcReader(proc, self.stdout, sys.stderr)\n            proc_reader.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reset_py_display() -> None:\n        # Delete any prompts that have been set\n        attributes = ['ps1', 'ps2', 'ps3']\n        for cur_attr in attributes:\n            try:\n                del sys.__dict__[cur_attr]\n            except KeyError:\n                pass\n\n        # Reset functions\n        sys.displayhook = sys.__displayhook__\n        sys.excepthook = sys.__excepthook__", "response": "Resets the sys. displayhook and sys. excepthook to reset the sys. displayhook and sys. excepthook."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_py(self, args: argparse.Namespace) -> bool:\n        from .pyscript_bridge import PyscriptBridge\n        if self._in_py:\n            err = \"Recursively entering interactive Python consoles is not allowed.\"\n            self.perror(err, traceback_war=False)\n            return False\n\n        try:\n            self._in_py = True\n\n            # Support the run command even if called prior to invoking an interactive interpreter\n            def py_run(filename: str):\n                \"\"\"Run a Python script file in the interactive console.\n                :param filename: filename of *.py script file to run\n                \"\"\"\n                expanded_filename = os.path.expanduser(filename)\n\n                # cmd_echo defaults to False for scripts. The user can always toggle this value in their script.\n                bridge.cmd_echo = False\n\n                try:\n                    with open(expanded_filename) as f:\n                        interp.runcode(f.read())\n                except OSError as ex:\n                    error_msg = \"Error opening script file '{}': {}\".format(expanded_filename, ex)\n                    self.perror(error_msg, traceback_war=False)\n\n            def py_quit():\n                \"\"\"Function callable from the interactive Python console to exit that environment\"\"\"\n                raise EmbeddedConsoleExit\n\n            # Set up Python environment\n            bridge = PyscriptBridge(self)\n            self.pystate[self.pyscript_name] = bridge\n            self.pystate['run'] = py_run\n            self.pystate['quit'] = py_quit\n            self.pystate['exit'] = py_quit\n\n            if self.locals_in_py:\n                self.pystate['self'] = self\n            elif 'self' in self.pystate:\n                del self.pystate['self']\n\n            localvars = self.pystate\n            from code import InteractiveConsole\n            interp = InteractiveConsole(locals=localvars)\n            interp.runcode('import sys, os;sys.path.insert(0, os.getcwd())')\n\n            # Check if the user is running a Python statement on the command line\n            if args.command:\n                full_command = args.command\n                if args.remainder:\n                    full_command += ' ' + ' '.join(args.remainder)\n\n                # Set cmd_echo to True so PyscriptBridge statements like: py app('help')\n                # run at the command line will print their output.\n                bridge.cmd_echo = True\n\n                # noinspection PyBroadException\n                try:\n                    interp.runcode(full_command)\n                except BaseException:\n                    # We don't care about any exception that happened in the interactive console\n                    pass\n\n            # If there are no args, then we will open an interactive Python console\n            else:\n                # Set up readline for Python console\n                if rl_type != RlType.NONE:\n                    # Save cmd2 history\n                    saved_cmd2_history = []\n                    for i in range(1, readline.get_current_history_length() + 1):\n                        # noinspection PyArgumentList\n                        saved_cmd2_history.append(readline.get_history_item(i))\n\n                    readline.clear_history()\n\n                    # Restore py's history\n                    for item in self.py_history:\n                        readline.add_history(item)\n\n                    if self.use_rawinput and self.completekey:\n                        # Set up tab completion for the Python console\n                        # rlcompleter relies on the default settings of the Python readline module\n                        if rl_type == RlType.GNU:\n                            saved_basic_quotes = ctypes.cast(rl_basic_quote_characters, ctypes.c_void_p).value\n                            rl_basic_quote_characters.value = orig_rl_basic_quotes\n\n                            if 'gnureadline' in sys.modules:\n                                # rlcompleter imports readline by name, so it won't use gnureadline\n                                # Force rlcompleter to use gnureadline instead so it has our settings and history\n                                saved_readline = None\n                                if 'readline' in sys.modules:\n                                    saved_readline = sys.modules['readline']\n\n                                sys.modules['readline'] = sys.modules['gnureadline']\n\n                        saved_delims = readline.get_completer_delims()\n                        readline.set_completer_delims(orig_rl_delims)\n\n                        # rlcompleter will not need cmd2's custom display function\n                        # This will be restored by cmd2 the next time complete() is called\n                        if rl_type == RlType.GNU:\n                            readline.set_completion_display_matches_hook(None)\n                        elif rl_type == RlType.PYREADLINE:\n                            # noinspection PyUnresolvedReferences\n                            readline.rl.mode._display_completions = self._display_matches_pyreadline\n\n                        # Save off the current completer and set a new one in the Python console\n                        # Make sure it tab completes from its locals() dictionary\n                        saved_completer = readline.get_completer()\n                        interp.runcode(\"from rlcompleter import Completer\")\n                        interp.runcode(\"import readline\")\n                        interp.runcode(\"readline.set_completer(Completer(locals()).complete)\")\n\n                # Set up sys module for the Python console\n                self._reset_py_display()\n\n                saved_sys_stdout = sys.stdout\n                sys.stdout = self.stdout\n\n                saved_sys_stdin = sys.stdin\n                sys.stdin = self.stdin\n\n                cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n                instructions = ('End with `Ctrl-D` (Unix) / `Ctrl-Z` (Windows), `quit()`, `exit()`.\\n'\n                                'Non-Python commands can be issued with: {}(\"your command\")\\n'\n                                'Run Python code from external script files with: run(\"script.py\")'\n                                .format(self.pyscript_name))\n\n                # noinspection PyBroadException\n                try:\n                    interp.interact(banner=\"Python {} on {}\\n{}\\n\\n{}\\n\".\n                                    format(sys.version, sys.platform, cprt, instructions))\n                except BaseException:\n                    # We don't care about any exception that happened in the interactive console\n                    pass\n\n                finally:\n                    sys.stdout = saved_sys_stdout\n                    sys.stdin = saved_sys_stdin\n\n                    # Set up readline for cmd2\n                    if rl_type != RlType.NONE:\n                        # Save py's history\n                        self.py_history.clear()\n                        for i in range(1, readline.get_current_history_length() + 1):\n                            # noinspection PyArgumentList\n                            self.py_history.append(readline.get_history_item(i))\n\n                        readline.clear_history()\n\n                        # Restore cmd2's history\n                        for item in saved_cmd2_history:\n                            readline.add_history(item)\n\n                        if self.use_rawinput and self.completekey:\n                            # Restore cmd2's tab completion settings\n                            readline.set_completer(saved_completer)\n                            readline.set_completer_delims(saved_delims)\n\n                            if rl_type == RlType.GNU:\n                                rl_basic_quote_characters.value = saved_basic_quotes\n\n                                if 'gnureadline' in sys.modules:\n                                    # Restore what the readline module pointed to\n                                    if saved_readline is None:\n                                        del(sys.modules['readline'])\n                                    else:\n                                        sys.modules['readline'] = saved_readline\n\n        except KeyboardInterrupt:\n            pass\n\n        finally:\n            self._in_py = False\n\n        return self._should_quit", "response": "Invoke a Python command or shell."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a Python script file inside the console", "response": "def do_pyscript(self, args: argparse.Namespace) -> bool:\n        \"\"\"Run a Python script file inside the console\"\"\"\n        script_path = os.path.expanduser(args.script_path)\n        py_return = False\n\n        # Save current command line arguments\n        orig_args = sys.argv\n\n        try:\n            # Overwrite sys.argv to allow the script to take command line arguments\n            sys.argv = [script_path] + args.script_arguments\n\n            # Run the script - use repr formatting to escape things which\n            # need to be escaped to prevent issues on Windows\n            py_return = self.do_py(\"run({!r})\".format(script_path))\n\n        except KeyboardInterrupt:\n            pass\n\n        finally:\n            # Restore command line arguments to original state\n            sys.argv = orig_args\n\n        return py_return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_history(self, args: argparse.Namespace) -> None:\n\n        # -v must be used alone with no other options\n        if args.verbose:\n            if args.clear or args.edit or args.output_file or args.run or args.transcript \\\n                    or args.expanded or args.script:\n                self.poutput(\"-v can not be used with any other options\")\n                self.poutput(self.history_parser.format_usage())\n                return\n\n        # -s and -x can only be used if none of these options are present: [-c -r -e -o -t]\n        if (args.script or args.expanded) \\\n                and (args.clear or args.edit or args.output_file or args.run or args.transcript):\n            self.poutput(\"-s and -x can not be used with -c, -r, -e, -o, or -t\")\n            self.poutput(self.history_parser.format_usage())\n            return\n\n        if args.clear:\n            # Clear command and readline history\n            self.history.clear()\n\n            if rl_type != RlType.NONE:\n                readline.clear_history()\n                if self.persistent_history_file:\n                    os.remove(self.persistent_history_file)\n            return\n\n        # If an argument was supplied, then retrieve partial contents of the history\n        cowardly_refuse_to_run = False\n        if args.arg:\n            # If a character indicating a slice is present, retrieve\n            # a slice of the history\n            arg = args.arg\n            arg_is_int = False\n            try:\n                int(arg)\n                arg_is_int = True\n            except ValueError:\n                pass\n\n            if '..' in arg or ':' in arg:\n                # Get a slice of history\n                history = self.history.span(arg)\n            elif arg_is_int:\n                history = [self.history.get(arg)]\n            elif arg.startswith(r'/') and arg.endswith(r'/'):\n                history = self.history.regex_search(arg)\n            else:\n                history = self.history.str_search(arg)\n        else:\n            # If no arg given, then retrieve the entire history\n            cowardly_refuse_to_run = True\n            # Get a copy of the history so it doesn't get mutated while we are using it\n            history = self.history[:]\n\n        if args.run:\n            if cowardly_refuse_to_run:\n                self.perror(\"Cowardly refusing to run all previously entered commands.\", traceback_war=False)\n                self.perror(\"If this is what you want to do, specify '1:' as the range of history.\",\n                            traceback_war=False)\n            else:\n                for runme in history:\n                    self.pfeedback(runme)\n                    if runme:\n                        self.onecmd_plus_hooks(runme)\n        elif args.edit:\n            import tempfile\n            fd, fname = tempfile.mkstemp(suffix='.txt', text=True)\n            with os.fdopen(fd, 'w') as fobj:\n                for command in history:\n                    if command.statement.multiline_command:\n                        fobj.write('{}\\n'.format(command.expanded.rstrip()))\n                    else:\n                        fobj.write('{}\\n'.format(command))\n            try:\n                self.do_edit(fname)\n                self.do_load(fname)\n            except Exception:\n                raise\n            finally:\n                os.remove(fname)\n        elif args.output_file:\n            try:\n                with open(os.path.expanduser(args.output_file), 'w') as fobj:\n                    for command in history:\n                        if command.statement.multiline_command:\n                            fobj.write('{}\\n'.format(command.expanded.rstrip()))\n                        else:\n                            fobj.write('{}\\n'.format(command))\n                plural = 's' if len(history) > 1 else ''\n                self.pfeedback('{} command{} saved to {}'.format(len(history), plural, args.output_file))\n            except Exception as e:\n                self.perror('Saving {!r} - {}'.format(args.output_file, e), traceback_war=False)\n        elif args.transcript:\n            self._generate_transcript(history, args.transcript)\n        else:\n            # Display the history items retrieved\n            for hi in history:\n                self.poutput(hi.pr(script=args.script, expanded=args.expanded, verbose=args.verbose))", "response": "Retrieve history from history file and return the new history file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_transcript(self, history: List[Union[HistoryItem, str]], transcript_file: str) -> None:\n        import io\n        # Validate the transcript file path to make sure directory exists and write access is available\n        transcript_path = os.path.abspath(os.path.expanduser(transcript_file))\n        transcript_dir = os.path.dirname(transcript_path)\n        if not os.path.isdir(transcript_dir) or not os.access(transcript_dir, os.W_OK):\n            self.perror(\"{!r} is not a directory or you don't have write access\".format(transcript_dir),\n                        traceback_war=False)\n            return\n\n        try:\n            with self.sigint_protection:\n                # Disable echo while we manually redirect stdout to a StringIO buffer\n                saved_echo = self.echo\n                saved_stdout = self.stdout\n                self.echo = False\n\n            # The problem with supporting regular expressions in transcripts\n            # is that they shouldn't be processed in the command, just the output.\n            # In addition, when we generate a transcript, any slashes in the output\n            # are not really intended to indicate regular expressions, so they should\n            # be escaped.\n            #\n            # We have to jump through some hoops here in order to catch the commands\n            # separately from the output and escape the slashes in the output.\n            transcript = ''\n            for history_item in history:\n                # build the command, complete with prompts. When we replay\n                # the transcript, we look for the prompts to separate\n                # the command from the output\n                first = True\n                command = ''\n                for line in history_item.splitlines():\n                    if first:\n                        command += '{}{}\\n'.format(self.prompt, line)\n                        first = False\n                    else:\n                        command += '{}{}\\n'.format(self.continuation_prompt, line)\n                transcript += command\n                # create a new string buffer and set it to stdout to catch the output\n                # of the command\n                membuf = io.StringIO()\n                self.stdout = membuf\n                # then run the command and let the output go into our buffer\n                self.onecmd_plus_hooks(history_item)\n                # rewind the buffer to the beginning\n                membuf.seek(0)\n                # get the output out of the buffer\n                output = membuf.read()\n                # and add the regex-escaped output to the transcript\n                transcript += output.replace('/', r'\\/')\n        finally:\n            with self.sigint_protection:\n                # Restore altered attributes to their original state\n                self.echo = saved_echo\n                self.stdout = saved_stdout\n\n        # finally, we can write the transcript out to the file\n        try:\n            with open(transcript_file, 'w') as fout:\n                fout.write(transcript)\n        except OSError as ex:\n            self.perror('Failed to save transcript: {}'.format(ex), traceback_war=False)\n        else:\n            # and let the user know what we did\n            if len(history) > 1:\n                plural = 'commands and their outputs'\n            else:\n                plural = 'command and its output'\n            msg = '{} {} saved to transcript file {!r}'\n            self.pfeedback(msg.format(len(history), plural, transcript_file))", "response": "Generate a transcript file from a given history of commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_edit(self, args: argparse.Namespace) -> None:\n        if not self.editor:\n            raise EnvironmentError(\"Please use 'set editor' to specify your text editing program of choice.\")\n\n        command = utils.quote_string_if_needed(os.path.expanduser(self.editor))\n        if args.file_path:\n            command += \" \" + utils.quote_string_if_needed(os.path.expanduser(args.file_path))\n\n        self.do_shell(command)", "response": "Edit a file in a text editor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_eos(self, _: argparse.Namespace) -> None:\n        if self._script_dir:\n            self._script_dir.pop()", "response": "Handle cleanup when a script has finished executing"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the command queue from the specified script file.", "response": "def do_load(self, args: argparse.Namespace) -> None:\n        \"\"\"Run commands in script file that is encoded as either ASCII or UTF-8 text\"\"\"\n        expanded_path = os.path.abspath(os.path.expanduser(args.script_path))\n\n        # Make sure the path exists and we can access it\n        if not os.path.exists(expanded_path):\n            self.perror(\"'{}' does not exist or cannot be accessed\".format(expanded_path), traceback_war=False)\n            return\n\n        # Make sure expanded_path points to a file\n        if not os.path.isfile(expanded_path):\n            self.perror(\"'{}' is not a file\".format(expanded_path), traceback_war=False)\n            return\n\n        # Make sure the file is not empty\n        if os.path.getsize(expanded_path) == 0:\n            self.perror(\"'{}' is empty\".format(expanded_path), traceback_war=False)\n            return\n\n        # Make sure the file is ASCII or UTF-8 encoded text\n        if not utils.is_text_file(expanded_path):\n            self.perror(\"'{}' is not an ASCII or UTF-8 encoded text file\".format(expanded_path), traceback_war=False)\n            return\n\n        try:\n            # Read all lines of the script and insert into the head of the\n            # command queue. Add an \"end of script (eos)\" command to cleanup the\n            # self._script_dir list when done.\n            with open(expanded_path, encoding='utf-8') as target:\n                script_commands = target.read().splitlines()\n        except OSError as ex:  # pragma: no cover\n            self.perror(\"Problem accessing script from '{}': {}\".format(expanded_path, ex))\n            return\n\n        if args.transcript:\n            self._generate_transcript(script_commands, os.path.expanduser(args.transcript))\n            return\n\n        self.cmdqueue = script_commands + ['eos'] + self.cmdqueue\n        self._script_dir.append(os.path.dirname(expanded_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun commands in script file that is encoded as ASCII or UTF - 8 text", "response": "def do__relative_load(self, args: argparse.Namespace) -> None:\n        \"\"\"Run commands in script file that is encoded as either ASCII or UTF-8 text\"\"\"\n        file_path = args.file_path\n        # NOTE: Relative path is an absolute path, it is just relative to the current script directory\n        relative_path = os.path.join(self._current_script_dir or '', file_path)\n        self.do_load(relative_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef async_alert(self, alert_msg: str, new_prompt: Optional[str] = None) -> None:  # pragma: no cover\n        if not (vt100_support and self.use_rawinput):\n            return\n\n        import shutil\n        import colorama.ansi as ansi\n        from colorama import Cursor\n\n        # Sanity check that can't fail if self.terminal_lock was acquired before calling this function\n        if self.terminal_lock.acquire(blocking=False):\n\n            # Figure out what prompt is displaying\n            current_prompt = self.continuation_prompt if self.at_continuation_prompt else self.prompt\n\n            # Only update terminal if there are changes\n            update_terminal = False\n\n            if alert_msg:\n                alert_msg += '\\n'\n                update_terminal = True\n\n            # Set the prompt if its changed\n            if new_prompt is not None and new_prompt != self.prompt:\n                self.prompt = new_prompt\n\n                # If we aren't at a continuation prompt, then it's OK to update it\n                if not self.at_continuation_prompt:\n                    rl_set_prompt(self.prompt)\n                    update_terminal = True\n\n            if update_terminal:\n                # Get the size of the terminal\n                terminal_size = shutil.get_terminal_size()\n\n                # Split the prompt lines since it can contain newline characters.\n                prompt_lines = current_prompt.splitlines()\n\n                # Calculate how many terminal lines are taken up by all prompt lines except for the last one.\n                # That will be included in the input lines calculations since that is where the cursor is.\n                num_prompt_terminal_lines = 0\n                for line in prompt_lines[:-1]:\n                    line_width = utils.ansi_safe_wcswidth(line)\n                    num_prompt_terminal_lines += int(line_width / terminal_size.columns) + 1\n\n                # Now calculate how many terminal lines are take up by the input\n                last_prompt_line = prompt_lines[-1]\n                last_prompt_line_width = utils.ansi_safe_wcswidth(last_prompt_line)\n\n                input_width = last_prompt_line_width + utils.ansi_safe_wcswidth(readline.get_line_buffer())\n\n                num_input_terminal_lines = int(input_width / terminal_size.columns) + 1\n\n                # Get the cursor's offset from the beginning of the first input line\n                cursor_input_offset = last_prompt_line_width + rl_get_point()\n\n                # Calculate what input line the cursor is on\n                cursor_input_line = int(cursor_input_offset / terminal_size.columns) + 1\n\n                # Create a string that when printed will clear all input lines and display the alert\n                terminal_str = ''\n\n                # Move the cursor down to the last input line\n                if cursor_input_line != num_input_terminal_lines:\n                    terminal_str += Cursor.DOWN(num_input_terminal_lines - cursor_input_line)\n\n                # Clear each line from the bottom up so that the cursor ends up on the first prompt line\n                total_lines = num_prompt_terminal_lines + num_input_terminal_lines\n                terminal_str += (ansi.clear_line() + Cursor.UP(1)) * (total_lines - 1)\n\n                # Clear the first prompt line\n                terminal_str += ansi.clear_line()\n\n                # Move the cursor to the beginning of the first prompt line and print the alert\n                terminal_str += '\\r' + alert_msg\n\n                if rl_type == RlType.GNU:\n                    sys.stderr.write(terminal_str)\n                elif rl_type == RlType.PYREADLINE:\n                    # noinspection PyUnresolvedReferences\n                    readline.rl.mode.console.write(terminal_str)\n\n                # Redraw the prompt and input lines\n                rl_force_redisplay()\n\n            self.terminal_lock.release()\n\n        else:\n            raise RuntimeError(\"another thread holds terminal_lock\")", "response": "Displays an alert to the user while they are at the prompt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_window_title(self, title: str) -> None:  # pragma: no cover\n        if not vt100_support:\n            return\n\n        # Sanity check that can't fail if self.terminal_lock was acquired before calling this function\n        if self.terminal_lock.acquire(blocking=False):\n            try:\n                import colorama.ansi as ansi\n                sys.stderr.write(ansi.set_title(title))\n            except AttributeError:\n                # Debugging in Pycharm has issues with setting terminal title\n                pass\n            finally:\n                self.terminal_lock.release()\n\n        else:\n            raise RuntimeError(\"another thread holds terminal_lock\")", "response": "Set the terminal window title"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_command(self, command: str) -> None:\n        # If the commands is already enabled, then return\n        if command not in self.disabled_commands:\n            return\n\n        help_func_name = HELP_FUNC_PREFIX + command\n\n        # Restore the command and help functions to their original values\n        dc = self.disabled_commands[command]\n        setattr(self, self.cmd_func_name(command), dc.command_function)\n\n        if dc.help_function is None:\n            delattr(self, help_func_name)\n        else:\n            setattr(self, help_func_name, dc.help_function)\n\n        # Remove the disabled command entry\n        del self.disabled_commands[command]", "response": "Enable a command by restoring its functions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling all commands in a specific category.", "response": "def enable_category(self, category: str) -> None:\n        \"\"\"\n        Enable an entire category of commands\n        :param category: the category to enable\n        \"\"\"\n        for cmd_name in list(self.disabled_commands):\n            func = self.disabled_commands[cmd_name].command_function\n            if hasattr(func, HELP_CATEGORY) and getattr(func, HELP_CATEGORY) == category:\n                self.enable_command(cmd_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disable_command(self, command: str, message_to_print: str) -> None:\n        import functools\n\n        # If the commands is already disabled, then return\n        if command in self.disabled_commands:\n            return\n\n        # Make sure this is an actual command\n        command_function = self.cmd_func(command)\n        if command_function is None:\n            raise AttributeError(\"{} does not refer to a command\".format(command))\n\n        help_func_name = HELP_FUNC_PREFIX + command\n\n        # Add the disabled command record\n        self.disabled_commands[command] = DisabledCommand(command_function=command_function,\n                                                          help_function=getattr(self, help_func_name, None))\n\n        # Overwrite the command and help functions to print the message\n        new_func = functools.partial(self._report_disabled_command_usage,\n                                     message_to_print=message_to_print.replace(COMMAND_NAME, command))\n        setattr(self, self.cmd_func_name(command), new_func)\n        setattr(self, help_func_name, new_func)", "response": "Disable a command and overwrite its functions with the command being disabled and the help functions to print the message_to_print."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disable_category(self, category: str, message_to_print: str) -> None:\n        all_commands = self.get_all_commands()\n\n        for cmd_name in all_commands:\n            func = self.cmd_func(cmd_name)\n            if hasattr(func, HELP_CATEGORY) and getattr(func, HELP_CATEGORY) == category:\n                self.disable_command(cmd_name, message_to_print)", "response": "Disable all commands in a specific category."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreport when a disabled command has been run or had help called on it", "response": "def _report_disabled_command_usage(self, *args, message_to_print: str, **kwargs) -> None:\n        \"\"\"\n        Report when a disabled command has been run or had help called on it\n        :param args: not used\n        :param message_to_print: the message reporting that the command is disabled\n        :param kwargs: not used\n        \"\"\"\n        self.decolorized_write(sys.stderr, \"{}\\n\".format(message_to_print))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initialize_plugin_system(self) -> None:\n        self._preloop_hooks = []\n        self._postloop_hooks = []\n        self._postparsing_hooks = []\n        self._precmd_hooks = []\n        self._postcmd_hooks = []\n        self._cmdfinalization_hooks = []", "response": "Initialize the plugin system"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures a function has the given number of parameters.", "response": "def _validate_callable_param_count(cls, func: Callable, count: int) -> None:\n        \"\"\"Ensure a function has the given number of parameters.\"\"\"\n        signature = inspect.signature(func)\n        # validate that the callable has the right number of parameters\n        nparam = len(signature.parameters)\n        if nparam != count:\n            raise TypeError('{} has {} positional arguments, expected {}'.format(\n                func.__name__,\n                nparam,\n                count,\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking parameter and return types for preloop and postloop hooks.", "response": "def _validate_prepostloop_callable(cls, func: Callable[[None], None]) -> None:\n        \"\"\"Check parameter and return types for preloop and postloop hooks.\"\"\"\n        cls._validate_callable_param_count(func, 0)\n        # make sure there is no return notation\n        signature = inspect.signature(func)\n        if signature.return_annotation is not None:\n            raise TypeError(\"{} must declare return a return type of 'None'\".format(\n                func.__name__,\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_preloop_hook(self, func: Callable[[None], None]) -> None:\n        self._validate_prepostloop_callable(func)\n        self._preloop_hooks.append(func)", "response": "Register a function to be called at the beginning of the command loop."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_postloop_hook(self, func: Callable[[None], None]) -> None:\n        self._validate_prepostloop_callable(func)\n        self._postloop_hooks.append(func)", "response": "Register a function to be called at the end of the command loop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a function to be called after parsing user input but before running the command", "response": "def register_postparsing_hook(self, func: Callable[[plugin.PostparsingData], plugin.PostparsingData]) -> None:\n        \"\"\"Register a function to be called after parsing user input but before running the command\"\"\"\n        self._validate_postparsing_callable(func)\n        self._postparsing_hooks.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks parameter and return types for pre and post command hooks.", "response": "def _validate_prepostcmd_hook(cls, func: Callable, data_type: Type) -> None:\n        \"\"\"Check parameter and return types for pre and post command hooks.\"\"\"\n        signature = inspect.signature(func)\n        # validate that the callable has the right number of parameters\n        cls._validate_callable_param_count(func, 1)\n        # validate the parameter has the right annotation\n        paramname = list(signature.parameters.keys())[0]\n        param = signature.parameters[paramname]\n        if param.annotation != data_type:\n            raise TypeError('argument 1 of {} has incompatible type {}, expected {}'.format(\n                func.__name__,\n                param.annotation,\n                data_type,\n            ))\n        # validate the return value has the right annotation\n        if signature.return_annotation == signature.empty:\n            raise TypeError('{} does not have a declared return type, expected {}'.format(\n                func.__name__,\n                data_type,\n            ))\n        if signature.return_annotation != data_type:\n            raise TypeError('{} has incompatible return type {}, expected {}'.format(\n                func.__name__,\n                signature.return_annotation,\n                data_type,\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a hook to be called before the command function.", "response": "def register_precmd_hook(self, func: Callable[[plugin.PrecommandData], plugin.PrecommandData]) -> None:\n        \"\"\"Register a hook to be called before the command function.\"\"\"\n        self._validate_prepostcmd_hook(func, plugin.PrecommandData)\n        self._precmd_hooks.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a hook to be called after the command function.", "response": "def register_postcmd_hook(self, func: Callable[[plugin.PostcommandData], plugin.PostcommandData]) -> None:\n        \"\"\"Register a hook to be called after the command function.\"\"\"\n        self._validate_prepostcmd_hook(func, plugin.PostcommandData)\n        self._postcmd_hooks.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck parameter and return types for command finalization hooks.", "response": "def _validate_cmdfinalization_callable(cls, func: Callable[[plugin.CommandFinalizationData],\n                                                               plugin.CommandFinalizationData]) -> None:\n        \"\"\"Check parameter and return types for command finalization hooks.\"\"\"\n        cls._validate_callable_param_count(func, 1)\n        signature = inspect.signature(func)\n        _, param = list(signature.parameters.items())[0]\n        if param.annotation != plugin.CommandFinalizationData:\n            raise TypeError(\"{} must have one parameter declared with type \"\n                            \"'cmd2.plugin.CommandFinalizationData'\".format(func.__name__))\n        if signature.return_annotation != plugin.CommandFinalizationData:\n            raise TypeError(\"{} must declare return a return type of \"\n                            \"'cmd2.plugin.CommandFinalizationData'\".format(func.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a hook to be called after a command is completed.", "response": "def register_cmdfinalization_hook(self, func: Callable[[plugin.CommandFinalizationData],\n                                                           plugin.CommandFinalizationData]) -> None:\n        \"\"\"Register a hook to be called after a command is completed, whether it completes successfully or not.\"\"\"\n        self._validate_cmdfinalization_callable(func)\n        self._cmdfinalization_hooks.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid_command(self, word: str) -> Tuple[bool, str]:\n        valid = False\n\n        if not word:\n            return False, 'cannot be an empty string'\n\n        if word.startswith(constants.COMMENT_CHAR):\n            return False, 'cannot start with the comment character'\n\n        for (shortcut, _) in self.shortcuts:\n            if word.startswith(shortcut):\n                # Build an error string with all shortcuts listed\n                errmsg = 'cannot start with a shortcut: '\n                errmsg += ', '.join(shortcut for (shortcut, _) in self.shortcuts)\n                return False, errmsg\n\n        errmsg = 'cannot contain: whitespace, quotes, '\n        errchars = []\n        errchars.extend(constants.REDIRECTION_CHARS)\n        errchars.extend(self.terminators)\n        errmsg += ', '.join([shlex.quote(x) for x in errchars])\n\n        match = self._command_pattern.search(word)\n        if match:\n            if word == match.group(1):\n                valid = True\n                errmsg = ''\n        return valid, errmsg", "response": "Determine whether a word is a valid command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tokenize(self, line: str, expand: bool = True) -> List[str]:\n\n        # expand shortcuts and aliases\n        if expand:\n            line = self._expand(line)\n\n        # check if this line is a comment\n        if line.lstrip().startswith(constants.COMMENT_CHAR):\n            return []\n\n        # split on whitespace\n        tokens = shlex_split(line)\n\n        # custom lexing\n        tokens = self._split_on_punctuation(tokens)\n        return tokens", "response": "Lex a string into a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the argument list for the given command name and argument list.", "response": "def get_command_arg_list(self, command_name: str, to_parse: Union[Statement, str],\n                             preserve_quotes: bool) -> Tuple[Statement, List[str]]:\n        \"\"\"\n        Called by the argument_list and argparse wrappers to retrieve just the arguments being\n        passed to their do_* methods as a list.\n\n        :param command_name: name of the command being run\n        :param to_parse: what is being passed to the do_* method. It can be one of two types:\n                         1. An already parsed Statement\n                         2. An argument string in cases where a do_* method is explicitly called\n                            e.g.: Calling do_help('alias create') would cause to_parse to be 'alias create'\n\n                            In this case, the string will be converted to a Statement and returned along\n                            with the argument list.\n\n        :param preserve_quotes: if True, then quotes will not be stripped from the arguments\n        :return: A tuple containing:\n                    The Statement used to retrieve the arguments\n                    The argument list\n        \"\"\"\n        # Check if to_parse needs to be converted to a Statement\n        if not isinstance(to_parse, Statement):\n            to_parse = self.parse(command_name + ' ' + to_parse, expand=False)\n\n        if preserve_quotes:\n            return to_parse, to_parse.arg_list\n        else:\n            return to_parse, to_parse.argv[1:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands shortcuts and aliases and aliases and return the expanded line.", "response": "def _expand(self, line: str) -> str:\n        \"\"\"Expand shortcuts and aliases\"\"\"\n\n        # expand aliases\n        # make a copy of aliases so we can edit it\n        tmp_aliases = list(self.aliases.keys())\n        keep_expanding = bool(tmp_aliases)\n        while keep_expanding:\n            for cur_alias in tmp_aliases:\n                keep_expanding = False\n                # apply our regex to line\n                match = self._command_pattern.search(line)\n                if match:\n                    # we got a match, extract the command\n                    command = match.group(1)\n                    if command and command == cur_alias:\n                        # rebuild line with the expanded alias\n                        line = self.aliases[cur_alias] + match.group(2) + line[match.end(2):]\n                        tmp_aliases.remove(cur_alias)\n                        keep_expanding = bool(tmp_aliases)\n                        break\n\n        # expand shortcuts\n        for (shortcut, expansion) in self.shortcuts:\n            if line.startswith(shortcut):\n                # If the next character after the shortcut isn't a space, then insert one\n                shortcut_len = len(shortcut)\n                if len(line) == shortcut_len or line[shortcut_len] != ' ':\n                    expansion += ' '\n\n                # Expand the shortcut\n                line = line.replace(shortcut, expansion, 1)\n                break\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _command_and_args(tokens: List[str]) -> Tuple[str, str]:\n        command = ''\n        args = ''\n\n        if tokens:\n            command = tokens[0]\n\n        if len(tokens) > 1:\n            args = ' '.join(tokens[1:])\n\n        return command, args", "response": "Given a list of tokens return a tuple of the command and the args as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _split_on_punctuation(self, tokens: List[str]) -> List[str]:\n        punctuation = []\n        punctuation.extend(self.terminators)\n        if self.allow_redirection:\n            punctuation.extend(constants.REDIRECTION_CHARS)\n\n        punctuated_tokens = []\n\n        for cur_initial_token in tokens:\n\n            # Save tokens up to 1 character in length or quoted tokens. No need to parse these.\n            if len(cur_initial_token) <= 1 or cur_initial_token[0] in constants.QUOTES:\n                punctuated_tokens.append(cur_initial_token)\n                continue\n\n            # Iterate over each character in this token\n            cur_index = 0\n            cur_char = cur_initial_token[cur_index]\n\n            # Keep track of the token we are building\n            new_token = ''\n\n            while True:\n                if cur_char not in punctuation:\n\n                    # Keep appending to new_token until we hit a punctuation char\n                    while cur_char not in punctuation:\n                        new_token += cur_char\n                        cur_index += 1\n                        if cur_index < len(cur_initial_token):\n                            cur_char = cur_initial_token[cur_index]\n                        else:\n                            break\n\n                else:\n                    cur_punc = cur_char\n\n                    # Keep appending to new_token until we hit something other than cur_punc\n                    while cur_char == cur_punc:\n                        new_token += cur_char\n                        cur_index += 1\n                        if cur_index < len(cur_initial_token):\n                            cur_char = cur_initial_token[cur_index]\n                        else:\n                            break\n\n                # Save the new token\n                punctuated_tokens.append(new_token)\n                new_token = ''\n\n                # Check if we've viewed all characters\n                if cur_index >= len(cur_initial_token):\n                    break\n\n        return punctuated_tokens", "response": "Splits a list of tokens from a command line using punctuation characters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_aprint(self, statement):\n        self.poutput('aprint was called with argument: {!r}'.format(statement))\n        self.poutput('statement.raw = {!r}'.format(statement.raw))\n        self.poutput('statement.argv = {!r}'.format(statement.argv))\n        self.poutput('statement.command = {!r}'.format(statement.command))", "response": "Print the argument string this basic command is called with."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling the Application Management commands", "response": "def do_disable_commands(self, _):\n        \"\"\"Disable the Application Management commands\"\"\"\n        message_to_print = \"{} is not available while {} commands are disabled\".format(COMMAND_NAME,\n                                                                                       self.CMD_CAT_APP_MGMT)\n        self.disable_category(self.CMD_CAT_APP_MGMT, message_to_print)\n        self.poutput(\"The Application Management commands have been disabled\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister custom argument action types.", "response": "def register_custom_actions(parser: argparse.ArgumentParser) -> None:\n    \"\"\"Register custom argument action types\"\"\"\n    parser.register('action', None, _StoreRangeAction)\n    parser.register('action', 'store', _StoreRangeAction)\n    parser.register('action', 'append', _AppendRangeAction)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if a token looks like a potential flag. Based on argparse. _parse_optional.", "response": "def is_potential_flag(token: str, parser: argparse.ArgumentParser) -> bool:\n    \"\"\"Determine if a token looks like a potential flag. Based on argparse._parse_optional().\"\"\"\n    # if it's an empty string, it was meant to be a positional\n    if not token:\n        return False\n\n    # if it doesn't start with a prefix, it was meant to be positional\n    if not token[0] in parser.prefix_chars:\n        return False\n\n    # if it's just a single character, it was meant to be positional\n    if len(token) == 1:\n        return False\n\n    # if it looks like a negative number, it was meant to be positional\n    # unless there are negative-number-like options\n    if parser._negative_number_matcher.match(token):\n        if not parser._has_negative_number_optionals:\n            return False\n\n    # if it contains a space, it was meant to be a positional\n    if ' ' in token:\n        return False\n\n    # Looks like a flag\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompletes the command line.", "response": "def complete_command(self, tokens: List[str], text: str, line: str, begidx: int, endidx: int) -> List[str]:\n        \"\"\"Complete the command using the argparse metadata and provided argument dictionary\"\"\"\n        # Count which positional argument index we're at now. Loop through all tokens on the command line so far\n        # Skip any flags or flag parameter tokens\n        next_pos_arg_index = 0\n\n        # This gets set to True when flags will no longer be processed as argparse flags\n        # That can happen when -- is used or an argument with nargs=argparse.REMAINDER is used\n        skip_remaining_flags = False\n\n        pos_arg = AutoCompleter._ArgumentState()\n        pos_action = None\n\n        flag_arg = AutoCompleter._ArgumentState()\n        flag_action = None\n\n        # dict is used because object wrapper is necessary to allow inner functions to modify outer variables\n        remainder = {'arg': None, 'action': None}\n\n        matched_flags = []\n        current_is_positional = False\n        consumed_arg_values = {}  # dict(arg_name -> [values, ...])\n\n        # the following are nested functions that have full access to all variables in the parent\n        # function including variables declared and updated after this function.  Variable values\n        # are current at the point the nested functions are invoked (as in, they do not receive a\n        # snapshot of these values, they directly access the current state of variables in the\n        # parent function)\n\n        def consume_flag_argument() -> None:\n            \"\"\"Consuming token as a flag argument\"\"\"\n            # we're consuming flag arguments\n            # if the token does not look like a new flag, then count towards flag arguments\n            if not is_potential_flag(token, self._parser) and flag_action is not None:\n                flag_arg.count += 1\n\n                # does this complete a option item for the flag\n                arg_choices = self._resolve_choices_for_arg(flag_action)\n                # if the current token matches the current position's autocomplete argument list,\n                # track that we've used it already.  Unless this is the current token, then keep it.\n                if not is_last_token and token in arg_choices:\n                    consumed_arg_values.setdefault(flag_action.dest, [])\n                    consumed_arg_values[flag_action.dest].append(token)\n\n        def consume_positional_argument() -> None:\n            \"\"\"Consuming token as positional argument\"\"\"\n            pos_arg.count += 1\n\n            # does this complete a option item for the flag\n            arg_choices = self._resolve_choices_for_arg(pos_action)\n            # if the current token matches the current position's autocomplete argument list,\n            # track that we've used it already.  Unless this is the current token, then keep it.\n            if not is_last_token and token in arg_choices:\n                consumed_arg_values.setdefault(pos_action.dest, [])\n                consumed_arg_values[pos_action.dest].append(token)\n\n        def process_action_nargs(action: argparse.Action, arg_state: AutoCompleter._ArgumentState) -> None:\n            \"\"\"Process the current argparse Action and initialize the ArgumentState object used\n            to track what arguments we have processed for this action\"\"\"\n            if isinstance(action, _RangeAction):\n                arg_state.min = action.nargs_min\n                arg_state.max = action.nargs_max\n                arg_state.variable = True\n            if arg_state.min is None or arg_state.max is None:\n                if action.nargs is None:\n                    arg_state.min = 1\n                    arg_state.max = 1\n                elif action.nargs == '+':\n                    arg_state.min = 1\n                    arg_state.max = float('inf')\n                    arg_state.variable = True\n                elif action.nargs == '*' or action.nargs == argparse.REMAINDER:\n                    arg_state.min = 0\n                    arg_state.max = float('inf')\n                    arg_state.variable = True\n                    if action.nargs == argparse.REMAINDER:\n                        remainder['action'] = action\n                        remainder['arg'] = arg_state\n                elif action.nargs == '?':\n                    arg_state.min = 0\n                    arg_state.max = 1\n                    arg_state.variable = True\n                else:\n                    arg_state.min = action.nargs\n                    arg_state.max = action.nargs\n\n        # This next block of processing tries to parse all parameters before the last parameter.\n        # We're trying to determine what specific argument the current cursor positition should be\n        # matched with. When we finish parsing all of the arguments, we can determine whether the\n        # last token is a positional or flag argument and which specific argument it is.\n        #\n        # We're also trying to save every flag that has been used as well as every value that\n        # has been used for a positional or flag parameter.  By saving this information we can exclude\n        # it from the completion results we generate for the last token. For example, single-use flag\n        # arguments will be hidden from the list of available flags. Also, arguments with a\n        # defined list of possible values will exclude values that have already been used.\n\n        # notes when the last token has been reached\n        is_last_token = False\n\n        for idx, token in enumerate(tokens):\n            is_last_token = idx >= len(tokens) - 1\n\n            # Only start at the start token index\n            if idx >= self._token_start_index:\n\n                # If a remainder action is found, force all future tokens to go to that\n                if remainder['arg'] is not None:\n                    if remainder['action'] == pos_action:\n                        consume_positional_argument()\n                        continue\n                    elif remainder['action'] == flag_action:\n                        consume_flag_argument()\n                        continue\n\n                current_is_positional = False\n                # Are we consuming flag arguments?\n                if not flag_arg.needed:\n\n                    if not skip_remaining_flags:\n                        # Special case when each of the following is true:\n                        #   - We're not in the middle of consuming flag arguments\n                        #   - The current positional argument count has hit the max count\n                        #   - The next positional argument is a REMAINDER argument\n                        # Argparse will now treat all future tokens as arguments to the positional including tokens that\n                        # look like flags so the completer should skip any flag related processing once this happens\n                        if (pos_action is not None) and pos_arg.count >= pos_arg.max and \\\n                                next_pos_arg_index < len(self._positional_actions) and \\\n                                self._positional_actions[next_pos_arg_index].nargs == argparse.REMAINDER:\n                            skip_remaining_flags = True\n\n                    # At this point we're no longer consuming flag arguments. Is the current argument a potential flag?\n                    if is_potential_flag(token, self._parser) and not skip_remaining_flags:\n                        # reset some tracking values\n                        flag_arg.reset()\n                        # don't reset positional tracking because flags can be interspersed anywhere between positionals\n                        flag_action = None\n\n                        if token == '--':\n                            if is_last_token:\n                                # Exit loop and see if -- can be completed into a flag\n                                break\n                            else:\n                                # In argparse, all args after -- are non-flags\n                                skip_remaining_flags = True\n\n                        # does the token fully match a known flag?\n                        if token in self._flag_to_action:\n                            flag_action = self._flag_to_action[token]\n                        elif hasattr(self._parser, 'allow_abbrev') and self._parser.allow_abbrev:\n                            candidates_flags = [flag for flag in self._flag_to_action if flag.startswith(token)]\n                            if len(candidates_flags) == 1:\n                                flag_action = self._flag_to_action[candidates_flags[0]]\n\n                        if flag_action is not None:\n                            # resolve argument counts\n                            process_action_nargs(flag_action, flag_arg)\n                            if not is_last_token and not isinstance(flag_action, argparse._AppendAction):\n                                matched_flags.extend(flag_action.option_strings)\n\n                    # current token isn't a potential flag\n                    #   - does the last flag accept variable arguments?\n                    #   - have we reached the max arg count for the flag?\n                    elif not flag_arg.variable or flag_arg.count >= flag_arg.max:\n                        # previous flag doesn't accept variable arguments, count this as a positional argument\n\n                        # reset flag tracking variables\n                        flag_arg.reset()\n                        flag_action = None\n                        current_is_positional = True\n\n                        if len(token) > 0 and pos_action is not None and pos_arg.count < pos_arg.max:\n                            # we have positional action match and we haven't reached the max arg count, consume\n                            # the positional argument and move on.\n                            consume_positional_argument()\n                        elif pos_action is None or pos_arg.count >= pos_arg.max:\n                            # if we don't have a current positional action or we've reached the max count for the action\n                            # close out the current positional argument state and set up for the next one\n                            pos_index = next_pos_arg_index\n                            next_pos_arg_index += 1\n                            pos_arg.reset()\n                            pos_action = None\n\n                            # are we at a sub-command? If so, forward to the matching completer\n                            if pos_index < len(self._positional_actions):\n                                action = self._positional_actions[pos_index]\n                                pos_name = action.dest\n                                if pos_name in self._positional_completers:\n                                    sub_completers = self._positional_completers[pos_name]\n                                    if token in sub_completers:\n                                        return sub_completers[token].complete_command(tokens, text, line,\n                                                                                      begidx, endidx)\n                                pos_action = action\n                                process_action_nargs(pos_action, pos_arg)\n                                consume_positional_argument()\n\n                        elif not is_last_token and pos_arg.max is not None:\n                            pos_action = None\n                            pos_arg.reset()\n\n                    else:\n                        consume_flag_argument()\n\n                else:\n                    consume_flag_argument()\n\n                if remainder['arg'] is not None:\n                    skip_remaining_flags = True\n\n                # don't reset this if we're on the last token - this allows completion to occur on the current token\n                elif not is_last_token and flag_arg.min is not None:\n                    flag_arg.needed = flag_arg.count < flag_arg.min\n\n        # Here we're done parsing all of the prior arguments. We know what the next argument is.\n\n        completion_results = []\n\n        # if we don't have a flag to populate with arguments and the last token starts with\n        # a flag prefix then we'll complete the list of flag options\n        if not flag_arg.needed and len(tokens[-1]) > 0 and tokens[-1][0] in self._parser.prefix_chars and \\\n                not skip_remaining_flags:\n            return self._cmd2_app.basic_complete(text, line, begidx, endidx,\n                                                 [flag for flag in self._flags if flag not in matched_flags])\n        # we're not at a positional argument, see if we're in a flag argument\n        elif not current_is_positional:\n            if flag_action is not None:\n                consumed = consumed_arg_values[flag_action.dest]\\\n                    if flag_action.dest in consumed_arg_values else []\n                # current_items.extend(self._resolve_choices_for_arg(flag_action, consumed))\n                completion_results = self._complete_for_arg(flag_action, text, line, begidx, endidx, consumed)\n                if not completion_results:\n                    self._print_action_help(flag_action)\n                elif len(completion_results) > 1:\n                    completion_results = self._format_completions(flag_action, completion_results)\n\n        # ok, we're not a flag, see if there's a positional argument to complete\n        else:\n            if pos_action is not None:\n                pos_name = pos_action.dest\n                consumed = consumed_arg_values[pos_name] if pos_name in consumed_arg_values else []\n                completion_results = self._complete_for_arg(pos_action, text, line, begidx, endidx, consumed)\n                if not completion_results:\n                    self._print_action_help(pos_action)\n                elif len(completion_results) > 1:\n                    completion_results = self._format_completions(pos_action, completion_results)\n\n        return completion_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef complete_command_help(self, tokens: List[str], text: str, line: str, begidx: int, endidx: int) -> List[str]:\n        for idx, token in enumerate(tokens):\n            if idx >= self._token_start_index:\n                if self._positional_completers:\n                    # For now argparse only allows 1 sub-command group per level\n                    # so this will only loop once.\n                    for completers in self._positional_completers.values():\n                        if token in completers:\n                            return completers[token].complete_command_help(tokens, text, line, begidx, endidx)\n                        else:\n                            return self._cmd2_app.basic_complete(text, line, begidx, endidx, completers.keys())\n        return []", "response": "Supports the completion of sub - commands for commands through the cmd2 help command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_help(self, tokens: List[str]) -> str:\n        for idx, token in enumerate(tokens):\n            if idx >= self._token_start_index:\n                if self._positional_completers:\n                    # For now argparse only allows 1 sub-command group per level\n                    # so this will only loop once.\n                    for completers in self._positional_completers.values():\n                        if token in completers:\n                            return completers[token].format_help(tokens)\n        return self._parser.format_help()", "response": "Supports the completion of sub - commands for commands through the cmd2 help command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_help(self) -> str:\n        formatter = self._get_formatter()\n\n        # usage\n        formatter.add_usage(self.usage, self._actions,\n                            self._mutually_exclusive_groups)\n\n        # description\n        formatter.add_text(self.description)\n\n        # Begin cmd2 customization (separate required and optional arguments)\n\n        # positionals, optionals and user-defined groups\n        for action_group in self._action_groups:\n            if action_group.title == 'optional arguments':\n                # check if the arguments are required, group accordingly\n                req_args = []\n                opt_args = []\n                for action in action_group._group_actions:\n                    if action.required:\n                        req_args.append(action)\n                    else:\n                        opt_args.append(action)\n\n                # separately display required arguments\n                formatter.start_section('required arguments')\n                formatter.add_text(action_group.description)\n                formatter.add_arguments(req_args)\n                formatter.end_section()\n\n                # now display truly optional arguments\n                formatter.start_section(action_group.title)\n                formatter.add_text(action_group.description)\n                formatter.add_arguments(opt_args)\n                formatter.end_section()\n            else:\n                formatter.start_section(action_group.title)\n                formatter.add_text(action_group.description)\n                formatter.add_arguments(action_group._group_actions)\n                formatter.end_section()\n\n        # End cmd2 customization\n\n        # epilog\n        formatter.add_text(self.epilog)\n\n        # determine help from format above\n        return formatter.format_help()", "response": "This function returns the help text for the command line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef instance_query_movie_ids(self) -> List[str]:\n        completions_with_desc = []\n\n        # Sort the movie id strings with a natural sort since they contain numbers\n        for movie_id in utils.natural_sort(self.MOVIE_DATABASE_IDS):\n            if movie_id in self.MOVIE_DATABASE:\n                movie_entry = self.MOVIE_DATABASE[movie_id]\n                completions_with_desc.append(argparse_completer.CompletionItem(movie_id, movie_entry['title']))\n\n        # Mark that we already sorted the matches\n        self.matches_sorted = True\n        return completions_with_desc", "response": "Demonstrates showing tabular hinting of tab completion information for the movie entries in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_video(self, args):\n        func = getattr(args, 'func', None)\n        if func is not None:\n            # Call whatever subcommand function was selected\n            func(self, args)\n        else:\n            # No subcommand was provided, so call help\n            self.do_help('video')", "response": "Video management command demonstrates multiple layers of sub - commands being handled by AutoCompleter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding tab completion to media", "response": "def complete_media(self, text, line, begidx, endidx):\n        \"\"\" Adds tab completion to media\"\"\"\n        choices = {'actor': query_actors,  # function\n                   'director': TabCompleteExample.static_list_directors,  # static list\n                   'movie_file': (self.path_complete,)\n                   }\n        completer = argparse_completer.AutoCompleter(TabCompleteExample.media_parser,\n                                                     self,\n                                                     arg_choices=choices)\n\n        tokens, _ = self.tokens_for_completion(line, begidx, endidx)\n        results = completer.complete_command(tokens, text, line, begidx, endidx)\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_library(self, args):\n        func = getattr(args, 'func', None)\n        if func is not None:\n            # Call whatever subcommand function was selected\n            func(self, args)\n        else:\n            # No subcommand was provided, so call help\n            self.do_help('library')", "response": "Media management command demonstrates multiple layers of sub - commands being handled by AutoCompleter"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrepresents a HistoryItem in a pretty fashion suitable for printing.", "response": "def pr(self, script=False, expanded=False, verbose=False) -> str:\n        \"\"\"Represent a HistoryItem in a pretty fashion suitable for printing.\n\n        If you pass verbose=True, script and expanded will be ignored\n\n        :return: pretty print string version of a HistoryItem\n        \"\"\"\n        if verbose:\n            ret_str = self.listformat.format(self.idx, str(self).rstrip())\n            if self != self.expanded:\n                ret_str += self.ex_listformat.format(self.idx, self.expanded.rstrip())\n        else:\n            if script:\n                # display without entry numbers\n                if expanded or self.statement.multiline_command:\n                    ret_str = self.expanded.rstrip()\n                else:\n                    ret_str = str(self)\n            else:\n                # display a numbered list\n                if expanded or self.statement.multiline_command:\n                    ret_str = self.listformat.format(self.idx, self.expanded.rstrip())\n                else:\n                    ret_str = self.listformat.format(self.idx, str(self).rstrip())\n        return ret_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _zero_based_index(self, onebased: Union[int, str]) -> int:\n        result = int(onebased)\n        if result > 0:\n            result -= 1\n        return result", "response": "Convert a one - based index to a zero - based index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nappends a HistoryItem to the end of the History list", "response": "def append(self, new: Statement) -> None:\n        \"\"\"Append a HistoryItem to end of the History list\n\n        :param new: command line to convert to HistoryItem and add to the end of the History list\n        \"\"\"\n        new = HistoryItem(new)\n        list.append(self, new)\n        new.idx = len(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a single item from the History list using 1 - based indexing.", "response": "def get(self, index: Union[int, str]) -> HistoryItem:\n        \"\"\"Get item from the History list using 1-based indexing.\n\n        :param index: optional item to get (index as either integer or string)\n        :return: a single HistoryItem\n        \"\"\"\n        index = int(index)\n        if index == 0:\n            raise IndexError('The first command in history is command 1.')\n        elif index < 0:\n            return self[index]\n        else:\n            return self[index - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef span(self, span: str) -> List[HistoryItem]:\n        if span.lower() in ('*', '-', 'all'):\n            span = ':'\n        results = self.spanpattern.search(span)\n        if not results:\n            # our regex doesn't match the input, bail out\n            raise ValueError('History indices must be positive or negative integers, and may not be zero.')\n\n        sep = results.group('separator')\n        start = results.group('start')\n        if start:\n            start = self._zero_based_index(start)\n        end = results.group('end')\n        if end:\n            end = int(end)\n            # modify end so it's inclusive of the last element\n            if end == -1:\n                # -1 as the end means include the last command in the array, which in pythonic\n                # terms means to not provide an ending index. If you put -1 as the ending index\n                # python excludes the last item in the list.\n                end = None\n            elif end < -1:\n                # if the ending is smaller than -1, make it one larger so it includes\n                # the element (python native indices exclude the last referenced element)\n                end += 1\n\n        if start is not None and end is not None:\n            # we have both start and end, return a slice of history\n            result = self[start:end]\n        elif start is not None and sep is not None:\n            # take a slice of the array\n            result = self[start:]\n        elif end is not None and sep is not None:\n            result = self[:end]\n        elif start is not None:\n            # there was no separator so it's either a posative or negative integer\n            result = [self[start]]\n        else:\n            # we just have a separator, return the whole list\n            result = self[:]\n        return result", "response": "Return an index or slice of the History list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds history items which contain a given string", "response": "def str_search(self, search: str) -> List[HistoryItem]:\n        \"\"\"Find history items which contain a given string\n\n        :param search: the string to search for\n        :return: a list of history items, or an empty list if the string was not found\n        \"\"\"\n        def isin(history_item):\n            \"\"\"filter function for string search of history\"\"\"\n            sloppy = utils.norm_fold(search)\n            return sloppy in utils.norm_fold(history_item) or sloppy in utils.norm_fold(history_item.expanded)\n        return [item for item in self if isin(item)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching the history items which match a given regular expression.", "response": "def regex_search(self, regex: str) -> List[HistoryItem]:\n        \"\"\"Find history items which match a given regular expression\n\n        :param regex: the regular expression to search for.\n        :return: a list of history items, or an empty list if the string was not found\n        \"\"\"\n        regex = regex.strip()\n        if regex.startswith(r'/') and regex.endswith(r'/'):\n            regex = regex[1:-1]\n        finder = re.compile(regex, re.DOTALL | re.MULTILINE)\n\n        def isin(hi):\n            \"\"\"filter function for doing a regular expression search of history\"\"\"\n            return finder.search(hi) or finder.search(hi.expanded)\n        return [itm for itm in self if isin(itm)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_page_wrap(self, args: List[str]):\n        if not args:\n            self.perror('page_wrap requires a path to a file as an argument', traceback_war=False)\n            return\n        self.page_file(args[0], chop=False)", "response": "Read in a text file and display it in a pager if they don t fit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_page_truncate(self, args: List[str]):\n        if not args:\n            self.perror('page_truncate requires a path to a file as an argument', traceback_war=False)\n            return\n        self.page_file(args[0], chop=True)", "response": "Read in a text file and display it in a pager truncating long lines if they don t fit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rl_force_redisplay() -> None:  # pragma: no cover\n    if not sys.stdout.isatty():\n        return\n\n    if rl_type == RlType.GNU:\n        readline_lib.rl_forced_update_display()\n\n        # After manually updating the display, readline asks that rl_display_fixed be set to 1 for efficiency\n        display_fixed = ctypes.c_int.in_dll(readline_lib, \"rl_display_fixed\")\n        display_fixed.value = 1\n\n    elif rl_type == RlType.PYREADLINE:\n        # Call _print_prompt() first to set the new location of the prompt\n        readline.rl.mode._print_prompt()\n        readline.rl.mode._update_line()", "response": "Force the readline to display the prompt and input text wherever the cursor is and start\n    reading input from this location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rl_get_point() -> int:  # pragma: no cover\n    if rl_type == RlType.GNU:\n        return ctypes.c_int.in_dll(readline_lib, \"rl_point\").value\n\n    elif rl_type == RlType.PYREADLINE:\n        return readline.rl.mode.l_buffer.point\n\n    else:\n        return 0", "response": "Returns the current cursor position in rl_line_buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rl_set_prompt(prompt: str) -> None:  # pragma: no cover\n    safe_prompt = rl_make_safe_prompt(prompt)\n\n    if rl_type == RlType.GNU:\n        encoded_prompt = bytes(safe_prompt, encoding='utf-8')\n        readline_lib.rl_set_prompt(encoded_prompt)\n\n    elif rl_type == RlType.PYREADLINE:\n        readline.rl._set_prompt(safe_prompt)", "response": "Sets the prompt for the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rl_make_safe_prompt(prompt: str) -> str:  # pragma: no cover\n    if rl_type == RlType.GNU:\n        # start code to tell GNU Readline about beginning of invisible characters\n        start = \"\\x01\"\n\n        # end code to tell GNU Readline about end of invisible characters\n        end = \"\\x02\"\n\n        escaped = False\n        result = \"\"\n\n        for c in prompt:\n            if c == \"\\x1b\" and not escaped:\n                result += start + c\n                escaped = True\n            elif c.isalpha() and escaped:\n                result += c + end\n                escaped = False\n            else:\n                result += c\n\n        return result\n\n    else:\n        return prompt", "response": "Makes a prompt safe to pass to GNU Readline."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_prompt(self):\n        self.cwd = os.getcwd()\n        self.prompt = Fore.CYAN + '{!r} $ '.format(self.cwd) + Fore.RESET", "response": "Set prompt so it displays the current working directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef postcmd(self, stop: bool, line: str) -> bool:\n        \"\"\"Override this so prompt always displays cwd.\"\"\"\n        self._set_prompt()\n        return stop", "response": "Hook method executed just after a command dispatch is finished."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging directory. Usage: cd <new_dir>", "response": "def do_cd(self, arglist):\n        \"\"\"Change directory.\n    Usage:\n        cd <new_dir>\n        \"\"\"\n        # Expect 1 argument, the directory to change to\n        if not arglist or len(arglist) != 1:\n            self.perror(\"cd requires exactly 1 argument:\", traceback_war=False)\n            self.do_help('cd')\n            self._last_result = cmd2.CommandResult('', 'Bad arguments')\n            return\n\n        # Convert relative paths to absolute paths\n        path = os.path.abspath(os.path.expanduser(arglist[0]))\n\n        # Make sure the directory exists, is a directory, and we have read access\n        out = ''\n        err = None\n        data = None\n        if not os.path.isdir(path):\n            err = '{!r} is not a directory'.format(path)\n        elif not os.access(path, os.R_OK):\n            err = 'You do not have read access to {!r}'.format(path)\n        else:\n            try:\n                os.chdir(path)\n            except Exception as ex:\n                err = '{}'.format(ex)\n            else:\n                out = 'Successfully changed directory to {!r}\\n'.format(path)\n                self.stdout.write(out)\n                data = path\n\n        if err:\n            self.perror(err, traceback_war=False)\n        self._last_result = cmd2.CommandResult(out, err, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting contents of current directory.", "response": "def do_dir(self, args, unknown):\n        \"\"\"List contents of current directory.\"\"\"\n        # No arguments for this command\n        if unknown:\n            self.perror(\"dir does not take any positional arguments:\", traceback_war=False)\n            self.do_help('dir')\n            self._last_result = cmd2.CommandResult('', 'Bad arguments')\n            return\n\n        # Get the contents as a list\n        contents = os.listdir(self.cwd)\n\n        fmt = '{} '\n        if args.long:\n            fmt = '{}\\n'\n        for f in contents:\n            self.stdout.write(fmt.format(f))\n        self.stdout.write('\\n')\n\n        self._last_result = cmd2.CommandResult(data=contents)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_speak(self, args):\n        words = []\n        for word in args.words:\n            if args.piglatin:\n                word = '%s%say' % (word[1:], word[0])\n            if args.shout:\n                word = word.upper()\n            words.append(word)\n\n        repetitions = args.repeat or 1\n\n        color_on = ''\n        if args.fg:\n            color_on += FG_COLORS[args.fg]\n        if args.bg:\n            color_on += BG_COLORS[args.bg]\n        color_off = Fore.RESET + Back.RESET\n\n        for i in range(min(repetitions, self.maxrepeats)):\n            # .poutput handles newlines, and accommodates output redirection too\n            self.poutput(color_on + ' '.join(words) + color_off)", "response": "Repeats what you tell me to."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmumbles what you tell me to.", "response": "def do_mumble(self, args):\n        \"\"\"Mumbles what you tell me to.\"\"\"\n        color_on = ''\n        if args.fg and args.fg in FG_COLORS:\n            color_on += FG_COLORS[args.fg]\n        if args.bg and args.bg in BG_COLORS:\n            color_on += BG_COLORS[args.bg]\n        color_off = Fore.RESET + Back.RESET\n\n        repetitions = args.repeat or 1\n        for i in range(min(repetitions, self.maxrepeats)):\n            output = []\n            if random.random() < .33:\n                output.append(random.choice(self.MUMBLE_FIRST))\n            for word in args.words:\n                if random.random() < .40:\n                    output.append(random.choice(self.MUMBLES))\n                output.append(word)\n            if random.random() < .25:\n                output.append(random.choice(self.MUMBLE_LAST))\n            self.poutput(color_on + ' '.join(output) + color_off)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pop_density(data: CityInfo) -> str:\n    if not isinstance(data, CityInfo):\n        raise AttributeError(\"Argument to pop_density() must be an instance of CityInfo\")\n    return no_dec(data.get_population() / data.get_area())", "response": "Calculate the population density from the data entry"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef high_density_tuples(row_tuple: Tuple) -> dict:\n    opts = dict()\n    if len(row_tuple) >= 7 and row_tuple[6] > EXTREMELY_HIGH_POULATION_DENSITY:\n        opts[tf.TableFormatter.ROW_OPT_TEXT_COLOR] = tf.TableColors.TEXT_COLOR_RED\n    return opts", "response": "Color rows with extremely high population density red."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef high_density_objs(row_obj: CityInfo) -> dict:\n    opts = dict()\n    if float(pop_density(row_obj)) > EXTREMELY_HIGH_POULATION_DENSITY:\n        opts[tf.TableFormatter.ROW_OPT_TEXT_COLOR] = tf.TableColors.TEXT_COLOR_RED\n    return opts", "response": "Color rows with extremely high population density red."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a unique instance of an argparse Argument parser for processing table arguments.", "response": "def make_table_parser() -> cmd2.argparse_completer.ACArgumentParser:\n    \"\"\"Create a unique instance of an argparse Argument parser for processing table arguments.\n\n    NOTE: The two cmd2 argparse decorators require that each parser be unique, even if they are essentially a deep copy\n    of each other.  For cases like that, you can create a function to return a unique instance of a parser, which is\n    what is being done here.\n    \"\"\"\n    table_parser = cmd2.argparse_completer.ACArgumentParser()\n    table_item_group = table_parser.add_mutually_exclusive_group()\n    table_item_group.add_argument('-c', '--color', action='store_true', help='Enable color')\n    table_item_group.add_argument('-f', '--fancy', action='store_true', help='Fancy Grid')\n    table_item_group.add_argument('-s', '--sparse', action='store_true', help='Sparse Grid')\n    return table_parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ptable(self, rows, columns, grid_args, row_stylist):\n        if grid_args.color:\n            grid = tf.AlternatingRowGrid(BACK_PRI, BACK_ALT)\n        elif grid_args.fancy:\n            grid = tf.FancyGrid()\n        elif grid_args.sparse:\n            grid = tf.SparseGrid()\n        else:\n            grid = None\n\n        formatted_table = tf.generate_table(rows=rows, columns=columns, grid_style=grid, row_tagger=row_stylist)\n        self.ppaged(formatted_table, chop=True)", "response": "Format tabular data for pretty - printing using a pager."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning right before a command is about to return.", "response": "def postcmd(self, stop, line):\n        \"\"\"Runs right before a command is about to return.\"\"\"\n        if self.gold != self.initial_gold:\n            self.poutput('Now we gots {0} doubloons'.format(self.gold))\n        if self.gold < 0:\n            self.poutput(\"Off to debtorrr's prison.\")\n            stop = True\n        return stop"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndrowns your sorrrows in rrrum. drink [ n ] - drink [ n ] barrel [ s o rum.", "response": "def do_drink(self, arg):\n        \"\"\"Drown your sorrrows in rrrum.\n\n        drink [n] - drink [n] barrel[s] o' rum.\"\"\"\n        try:\n            self.gold -= int(arg)\n        except ValueError:\n            if arg:\n                self.poutput('''What's \"{0}\"?  I'll take rrrum.'''.format(arg))\n            self.gold -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsing a colorful song.", "response": "def do_sing(self, arg):\n        \"\"\"Sing a colorful song.\"\"\"\n        color_escape = COLORS.get(self.songcolor, Fore.RESET)\n        self.poutput(arg, color=color_escape)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_yo(self, args):\n        chant = ['yo'] + ['ho'] * args.ho\n        separator = ', ' if args.commas else ' '\n        chant = separator.join(chant)\n        self.poutput('{0} and a bottle of {1}'.format(chant, args.beverage))", "response": "Compose a yo - ho - ho type chant with flexible options."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of sub - commands for an ArgumentParser", "response": "def get_sub_commands(parser: argparse.ArgumentParser) -> List[str]:\n    \"\"\"Get a list of sub-commands for an ArgumentParser\"\"\"\n    sub_cmds = []\n\n    # Check if this is parser has sub-commands\n    if parser is not None and parser._subparsers is not None:\n\n        # Find the _SubParsersAction for the sub-commands of this parser\n        for action in parser._subparsers._actions:\n            if isinstance(action, argparse._SubParsersAction):\n                for sub_cmd, sub_cmd_parser in action.choices.items():\n                    sub_cmds.append(sub_cmd)\n\n                    # Look for nested sub-commands\n                    for nested_sub_cmd in get_sub_commands(sub_cmd_parser):\n                        sub_cmds.append('{} {}'.format(sub_cmd, nested_sub_cmd))\n\n                break\n\n    sub_cmds.sort()\n    return sub_cmds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_help_to_file(item: str, outfile: TextIO, is_command: bool) -> None:\n    if is_command:\n        label = \"COMMAND\"\n    else:\n        label = \"TOPIC\"\n\n    header = '{}\\n{}: {}\\n{}\\n'.format(ASTERISKS, label, item, ASTERISKS)\n    outfile.write(header)\n\n    result = app('help {}'.format(item))\n    outfile.write(result.stdout)", "response": "Write help text for commands and topics to the output file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main() -> None:\n\n    # Make sure we have access to self\n    if 'self' not in globals():\n        print(\"Run 'set locals_in_py true' and then rerun this script\")\n        return\n\n    # Make sure the user passed in an output file\n    if len(sys.argv) != 2:\n        print(\"Usage: {} <output_file>\".format(os.path.basename(sys.argv[0])))\n        return\n\n    # Open the output file\n    outfile_path = os.path.expanduser(sys.argv[1])\n    try:\n        outfile = open(outfile_path, 'w')\n    except OSError as e:\n        print(\"Error opening {} because: {}\".format(outfile_path, e))\n        return\n\n    # Write the help summary\n    header = '{0}\\nSUMMARY\\n{0}\\n'.format(ASTERISKS)\n    outfile.write(header)\n\n    result = app('help -v')\n    outfile.write(result.stdout)\n\n    # Get a list of all commands and help topics and then filter out duplicates\n    all_commands = set(self.get_all_commands())\n    all_topics = set(self.get_help_topics())\n    to_save = list(all_commands | all_topics)\n    to_save.sort()\n\n    for item in to_save:\n        is_command = item in all_commands\n        add_help_to_file(item, outfile, is_command)\n\n        if is_command:\n            # Add any sub-commands\n            for subcmd in get_sub_commands(getattr(self.cmd_func(item), 'argparser', None)):\n                full_cmd = '{} {}'.format(item, subcmd)\n                add_help_to_file(full_cmd, outfile, is_command)\n\n    outfile.close()\n    print(\"Output written to {}\".format(outfile_path))", "response": "Main function of the main function of the script"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef popitem(self):\n        try:\n            key = next(iter(self.__order))\n        except StopIteration:\n            raise KeyError('%s is empty' % self.__class__.__name__)\n        else:\n            return (key, self.pop(key))", "response": "Remove and return the lowest recently used key value pair."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lru_cache(maxsize=128, typed=False):\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    else:\n        return _cache(LRUCache(maxsize), typed)", "response": "Decorator to wrap a function with a memoizing callable that saves\n    up to maxsize results based on a Least Recently Used algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ttl_cache(maxsize=128, ttl=600, timer=default_timer, typed=False):\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), typed)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), typed)", "response": "Decorator to wrap a function with a memoizing callable that saves\n    up to maxsize results based on a Least Recently Used ( LRU ) algorithm with a per - item time - to - live ( TTL value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expire(self, time=None):\n        if time is None:\n            time = self.__timer()\n        root = self.__root\n        curr = root.next\n        links = self.__links\n        cache_delitem = Cache.__delitem__\n        while curr is not root and curr.expire < time:\n            cache_delitem(self, curr.key)\n            del links[curr.key]\n            next = curr.next\n            curr.unlink()\n            curr = next", "response": "Remove expired items from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving and return the key value pair least recently used that has not already expired.", "response": "def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least recently used that\n        has not already expired.\n\n        \"\"\"\n        with self.__timer as time:\n            self.expire(time)\n            try:\n                key = next(iter(self.__links))\n            except StopIteration:\n                raise KeyError('%s is empty' % self.__class__.__name__)\n            else:\n                return (key, self.pop(key))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cached(cache, key=keys.hashkey, lock=None):\n    def decorator(func):\n        if cache is None:\n            def wrapper(*args, **kwargs):\n                return func(*args, **kwargs)\n        elif lock is None:\n            def wrapper(*args, **kwargs):\n                k = key(*args, **kwargs)\n                try:\n                    return cache[k]\n                except KeyError:\n                    pass  # key not found\n                v = func(*args, **kwargs)\n                try:\n                    cache[k] = v\n                except ValueError:\n                    pass  # value too large\n                return v\n        else:\n            def wrapper(*args, **kwargs):\n                k = key(*args, **kwargs)\n                try:\n                    with lock:\n                        return cache[k]\n                except KeyError:\n                    pass  # key not found\n                v = func(*args, **kwargs)\n                try:\n                    with lock:\n                        cache[k] = v\n                except ValueError:\n                    pass  # value too large\n                return v\n        return _update_wrapper(wrapper, func)\n    return decorator", "response": "Decorator to wrap a function that saves\n    results in a cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a cache key for the specified hashable arguments.", "response": "def hashkey(*args, **kwargs):\n    \"\"\"Return a cache key for the specified hashable arguments.\"\"\"\n\n    if kwargs:\n        return _HashedTuple(args + sum(sorted(kwargs.items()), _kwmark))\n    else:\n        return _HashedTuple(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef typedkey(*args, **kwargs):\n\n    key = hashkey(*args, **kwargs)\n    key += tuple(type(v) for v in args)\n    key += tuple(type(v) for _, v in sorted(kwargs.items()))\n    return key", "response": "Return a typed cache key for the specified hashable arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove and return the key value pair least frequently used.", "response": "def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n        try:\n            (key, _), = self.__counter.most_common(1)\n        except ValueError:\n            raise KeyError('%s is empty' % self.__class__.__name__)\n        else:\n            return (key, self.pop(key))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef popitem(self):\n        try:\n            key = self.__choice(list(self))\n        except IndexError:\n            raise KeyError('%s is empty' % self.__class__.__name__)\n        else:\n            return (key, self.pop(key))", "response": "Remove and return a random key value pair."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over the ResultProxy.", "response": "def iter_result_proxy(rp, step=None):\n    \"\"\"Iterate over the ResultProxy.\"\"\"\n    while True:\n        if step is None:\n            chunk = rp.fetchall()\n        else:\n            chunk = rp.fetchmany(step)\n        if not chunk:\n            break\n        for row in chunk:\n            yield row"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_column_name(name):\n    if not isinstance(name, six.string_types):\n        raise ValueError('%r is not a valid column name.' % name)\n\n    # limit to 63 characters\n    name = name.strip()[:63]\n    # column names can be 63 *bytes* max in postgresql\n    if isinstance(name, six.text_type):\n        while len(name.encode('utf-8')) >= 64:\n            name = name[:len(name) - 1]\n\n    if not len(name) or '.' in name or '-' in name:\n        raise ValueError('%r is not a valid column name.' % name)\n    return name", "response": "Check if a string is a reasonable thing to use as a column name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_table_name(name):\n    if not isinstance(name, six.string_types):\n        raise ValueError(\"Invalid table name: %r\" % name)\n    name = name.strip()[:63]\n    if not len(name):\n        raise ValueError(\"Invalid table name: %r\" % name)\n    return name", "response": "Check if the table name is obviously invalid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef safe_url(url):\n    parsed = urlparse(url)\n    if parsed.password is not None:\n        pwd = ':%s@' % parsed.password\n        url = url.replace(pwd, ':*****@')\n    return url", "response": "Remove password from printed connection URLs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index_name(table, columns):\n    sig = '||'.join(columns)\n    key = sha1(sig.encode('utf-8')).hexdigest()[:16]\n    return 'ix_%s_%s' % (table, key)", "response": "Generate an artificial index name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrying and make the given argument into a tuple.", "response": "def ensure_tuple(obj):\n    \"\"\"Try and make the given argument into a tuple.\"\"\"\n    if obj is None:\n        return tuple()\n    if isinstance(obj, Iterable) and not isinstance(obj, six.string_types):\n        return tuple(obj)\n    return obj,"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pad_chunk_columns(chunk):\n    columns = set()\n    for record in chunk:\n        columns.update(record.keys())\n    for record in chunk:\n        for column in columns:\n            record.setdefault(column, None)\n    return chunk", "response": "Given a set of items to be inserted make sure they all have the same columns by padding with None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a single sample guess the column type for the field.", "response": "def guess(cls, sample):\n        \"\"\"Given a single sample, guess the column type for the field.\n\n        If the sample is an instance of an SQLAlchemy type, the type will be\n        used instead.\n        \"\"\"\n        if isinstance(sample, TypeEngine):\n            return sample\n        if isinstance(sample, bool):\n            return cls.boolean\n        elif isinstance(sample, int):\n            return cls.integer\n        elif isinstance(sample, float):\n            return cls.float\n        elif isinstance(sample, datetime):\n            return cls.datetime\n        elif isinstance(sample, date):\n            return cls.date\n        return cls.text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a row dict by inserting it into the table.", "response": "def insert(self, row, ensure=None, types=None):\n        \"\"\"Add a ``row`` dict by inserting it into the table.\n\n        If ``ensure`` is set, any of the keys of the row are not\n        table columns, they will be created automatically.\n\n        During column creation, ``types`` will be checked for a key\n        matching the name of a column to be created, and the given\n        SQLAlchemy column type will be used. Otherwise, the type is\n        guessed from the row value, defaulting to a simple unicode\n        field.\n        ::\n\n            data = dict(title='I am a banana!')\n            table.insert(data)\n\n        Returns the inserted row's primary key.\n        \"\"\"\n        row = self._sync_columns(row, ensure, types=types)\n        res = self.db.executable.execute(self.table.insert(row))\n        if len(res.inserted_primary_key) > 0:\n            return res.inserted_primary_key[0]\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_ignore(self, row, keys, ensure=None, types=None):\n        row = self._sync_columns(row, ensure, types=types)\n        if self._check_ensure(ensure):\n            self.create_index(keys)\n        args, _ = self._keys_to_args(row, keys)\n        if self.count(**args) == 0:\n            return self.insert(row, ensure=False)\n        return False", "response": "Add a row into the table if the row does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding many rows at a time.", "response": "def insert_many(self, rows, chunk_size=1000, ensure=None, types=None):\n        \"\"\"Add many rows at a time.\n\n        This is significantly faster than adding them one by one. Per default\n        the rows are processed in chunks of 1000 per commit, unless you specify\n        a different ``chunk_size``.\n\n        See :py:meth:`insert() <dataset.Table.insert>` for details on\n        the other parameters.\n        ::\n\n            rows = [dict(name='Dolly')] * 10000\n            table.insert_many(rows)\n        \"\"\"\n        chunk = []\n        for row in rows:\n            row = self._sync_columns(row, ensure, types=types)\n            chunk.append(row)\n            if len(chunk) == chunk_size:\n                chunk = pad_chunk_columns(chunk)\n                self.table.insert().execute(chunk)\n                chunk = []\n\n        if len(chunk):\n            chunk = pad_chunk_columns(chunk)\n            self.table.insert().execute(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, row, keys, ensure=None, types=None, return_count=False):\n        row = self._sync_columns(row, ensure, types=types)\n        args, row = self._keys_to_args(row, keys)\n        clause = self._args_to_clause(args)\n        if not len(row):\n            return self.count(clause)\n        stmt = self.table.update(whereclause=clause, values=row)\n        rp = self.db.executable.execute(stmt)\n        if rp.supports_sane_rowcount():\n            return rp.rowcount\n        if return_count:\n            return self.count(clause)", "response": "Update a row in the table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upsert(self, row, keys, ensure=None, types=None):\n        row = self._sync_columns(row, ensure, types=types)\n        if self._check_ensure(ensure):\n            self.create_index(keys)\n        row_count = self.update(row, keys, ensure=False, return_count=True)\n        if row_count == 0:\n            return self.insert(row, ensure=False)\n        return True", "response": "An UPSERT operation is a smart combination of insert and update."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting rows from the table.", "response": "def delete(self, *clauses, **filters):\n        \"\"\"Delete rows from the table.\n\n        Keyword arguments can be used to add column-based filters. The filter\n        criterion will always be equality:\n        ::\n\n            table.delete(place='Berlin')\n\n        If no arguments are given, all records are deleted.\n        \"\"\"\n        if not self.exists:\n            return False\n        clause = self._args_to_clause(filters, clauses=clauses)\n        stmt = self.table.delete(whereclause=clause)\n        rp = self.db.executable.execute(stmt)\n        return rp.rowcount > 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reflect_table(self):\n        with self.db.lock:\n            try:\n                self._table = SQLATable(self.name,\n                                        self.db.metadata,\n                                        schema=self.db.schema,\n                                        autoload=True)\n            except NoSuchTableError:\n                pass", "response": "Load the tables definition from the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sync_columns(self, row, ensure, types=None):\n        columns = self.columns\n        ensure = self._check_ensure(ensure)\n        types = types or {}\n        types = {normalize_column_name(k): v for (k, v) in types.items()}\n        out = {}\n        sync_columns = []\n        for name, value in row.items():\n            name = normalize_column_name(name)\n            if ensure and name not in columns:\n                _type = types.get(name)\n                if _type is None:\n                    _type = self.db.types.guess(value)\n                sync_columns.append(Column(name, _type))\n                columns.append(name)\n            if name in columns:\n                out[name] = value\n        self._sync_table(sync_columns)\n        return out", "response": "Create missing columns prior to writes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_column(self, name, type):\n        name = normalize_column_name(name)\n        if self.has_column(name):\n            log.debug(\"Column exists: %s\" % name)\n            return\n        self._sync_table((Column(name, type),))", "response": "Create a new column in the table with the specified name and type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_column_by_example(self, name, value):\n        type_ = self.db.types.guess(value)\n        self.create_column(name, type_)", "response": "Explicitly create a new column with a type that is appropriate\n            to store the given example value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_column(self, name):\n        if self.db.engine.dialect.name == 'sqlite':\n            raise RuntimeError(\"SQLite does not support dropping columns.\")\n        name = normalize_column_name(name)\n        with self.db.lock:\n            if not self.exists or not self.has_column(name):\n                log.debug(\"Column does not exist: %s\", name)\n                return\n\n            self._threading_warn()\n            self.db.op.drop_column(\n                self.table.name,\n                name,\n                self.table.schema\n            )\n            self._reflect_table()", "response": "Drop the column with the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef drop(self):\n        with self.db.lock:\n            if self.exists:\n                self._threading_warn()\n                self.table.drop(self.db.executable, checkfirst=True)\n                self._table = None", "response": "Drop the table from the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_index(self, columns):\n        if not self.exists:\n            return False\n        columns = set([normalize_column_name(c) for c in columns])\n        if columns in self._indexes:\n            return True\n        for column in columns:\n            if not self.has_column(column):\n                return False\n        indexes = self.db.inspect.get_indexes(self.name, schema=self.db.schema)\n        for index in indexes:\n            if columns == set(index.get('column_names', [])):\n                self._indexes.append(columns)\n                return True\n        return False", "response": "Check if an index exists to cover the given columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an index to speed up queries on a table.", "response": "def create_index(self, columns, name=None, **kw):\n        \"\"\"Create an index to speed up queries on a table.\n\n        If no ``name`` is given a random name is created.\n        ::\n\n            table.create_index(['name', 'country'])\n        \"\"\"\n        columns = [normalize_column_name(c) for c in ensure_tuple(columns)]\n        with self.db.lock:\n            if not self.exists:\n                raise DatasetException(\"Table has not been created yet.\")\n\n            for column in columns:\n                if not self.has_column(column):\n                    return\n\n            if not self.has_index(columns):\n                self._threading_warn()\n                name = name or index_name(self.name, columns)\n                columns = [self.table.c[c] for c in columns]\n                idx = Index(name, *columns, **kw)\n                idx.create(self.db.executable)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find(self, *_clauses, **kwargs):\n        if not self.exists:\n            return iter([])\n\n        _limit = kwargs.pop('_limit', None)\n        _offset = kwargs.pop('_offset', 0)\n        order_by = kwargs.pop('order_by', None)\n        _streamed = kwargs.pop('_streamed', False)\n        _step = kwargs.pop('_step', QUERY_STEP)\n        if _step is False or _step == 0:\n            _step = None\n\n        order_by = self._args_to_order_by(order_by)\n        args = self._args_to_clause(kwargs, clauses=_clauses)\n        query = self.table.select(whereclause=args,\n                                  limit=_limit,\n                                  offset=_offset)\n        if len(order_by):\n            query = query.order_by(*order_by)\n\n        conn = self.db.executable\n        if _streamed:\n            conn = self.db.engine.connect()\n            conn = conn.execution_options(stream_results=True)\n\n        return ResultIter(conn.execute(query),\n                          row_type=self.db.row_type,\n                          step=_step)", "response": "Perform a simple search on the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a single result from the table.", "response": "def find_one(self, *args, **kwargs):\n        \"\"\"Get a single result from the table.\n\n        Works just like :py:meth:`find() <dataset.Table.find>` but returns one\n        result, or ``None``.\n        ::\n\n            row = table.find_one(country='United States')\n        \"\"\"\n        if not self.exists:\n            return None\n\n        kwargs['_limit'] = 1\n        kwargs['_step'] = None\n        resiter = self.find(*args, **kwargs)\n        try:\n            for row in resiter:\n                return row\n        finally:\n            resiter.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the count of results for the given filter set.", "response": "def count(self, *_clauses, **kwargs):\n        \"\"\"Return the count of results for the given filter set.\"\"\"\n        # NOTE: this does not have support for limit and offset since I can't\n        # see how this is useful. Still, there might be compatibility issues\n        # with people using these flags. Let's see how it goes.\n        if not self.exists:\n            return 0\n\n        args = self._args_to_clause(kwargs, clauses=_clauses)\n        query = select([func.count()], whereclause=args)\n        query = query.select_from(self.table)\n        rp = self.db.executable.execute(query)\n        return rp.fetchone()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all the unique ( distinct ) values for the given columns.", "response": "def distinct(self, *args, **_filter):\n        \"\"\"Return all the unique (distinct) values for the given ``columns``.\n        ::\n\n            # returns only one row per year, ignoring the rest\n            table.distinct('year')\n            # works with multiple columns, too\n            table.distinct('year', 'country')\n            # you can also combine this with a filter\n            table.distinct('year', country='China')\n        \"\"\"\n        if not self.exists:\n            return iter([])\n\n        columns = []\n        clauses = []\n        for column in args:\n            if isinstance(column, ClauseElement):\n                clauses.append(column)\n            else:\n                if not self.has_column(column):\n                    raise DatasetException(\"No such column: %s\" % column)\n                columns.append(self.table.c[column])\n\n        clause = self._args_to_clause(_filter, clauses=clauses)\n        if not len(columns):\n            return iter([])\n\n        q = expression.select(columns,\n                              distinct=True,\n                              whereclause=clause,\n                              order_by=[c.asc() for c in columns])\n        return self.db.query(q)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nopens a new connection to a database.", "response": "def connect(url=None, schema=None, reflect_metadata=True, engine_kwargs=None,\n            reflect_views=True, ensure_schema=True, row_type=row_type):\n    \"\"\" Opens a new connection to a database.\n\n    *url* can be any valid `SQLAlchemy engine URL`_.  If *url* is not defined\n    it will try to use *DATABASE_URL* from environment variable.  Returns an\n    instance of :py:class:`Database <dataset.Database>`. Set *reflect_metadata*\n    to False if you don't want the entire database schema to be pre-loaded.\n    This significantly speeds up connecting to large databases with lots of\n    tables. *reflect_views* can be set to False if you don't want views to be\n    loaded.  Additionally, *engine_kwargs* will be directly passed to\n    SQLAlchemy, e.g.  set *engine_kwargs={'pool_recycle': 3600}* will avoid `DB\n    connection timeout`_. Set *row_type* to an alternate dict-like class to\n    change the type of container rows are stored in.::\n\n        db = dataset.connect('sqlite:///factbook.db')\n\n    .. _SQLAlchemy Engine URL: http://docs.sqlalchemy.org/en/latest/core/engines.html#sqlalchemy.create_engine\n    .. _DB connection timeout: http://docs.sqlalchemy.org/en/latest/core/pooling.html#setting-pool-recycle\n    \"\"\"\n    if url is None:\n        url = os.environ.get('DATABASE_URL', 'sqlite://')\n\n    return Database(url, schema=schema, reflect_metadata=reflect_metadata,\n                    engine_kwargs=engine_kwargs, reflect_views=reflect_views,\n                    ensure_schema=ensure_schema, row_type=row_type)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef executable(self):\n        if not hasattr(self.local, 'conn'):\n            self.local.conn = self.engine.connect()\n        return self.local.conn", "response": "Returns a connection against which statements will be executed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if this database is in a transactional context.", "response": "def in_transaction(self):\n        \"\"\"Check if this database is in a transactional context.\"\"\"\n        if not hasattr(self.local, 'tx'):\n            return False\n        return len(self.local.tx) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef begin(self):\n        if not hasattr(self.local, 'tx'):\n            self.local.tx = []\n        self.local.tx.append(self.executable.begin())", "response": "Enter a transaction explicitly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef commit(self):\n        if hasattr(self.local, 'tx') and self.local.tx:\n            tx = self.local.tx.pop()\n            tx.commit()\n            self._flush_tables()", "response": "Commit the current transaction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rollback(self):\n        if hasattr(self.local, 'tx') and self.local.tx:\n            tx = self.local.tx.pop()\n            tx.rollback()\n            self._flush_tables()", "response": "Roll back the current transaction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new table.", "response": "def create_table(self, table_name, primary_id=None, primary_type=None):\n        \"\"\"Create a new table.\n\n        Either loads a table or creates it if it doesn't exist yet. You can\n        define the name and type of the primary key field, if a new table is to\n        be created. The default is to create an auto-incrementing integer,\n        ``id``. You can also set the primary key to be a string or big integer.\n        The caller will be responsible for the uniqueness of ``primary_id`` if\n        it is defined as a text type.\n\n        Returns a :py:class:`Table <dataset.Table>` instance.\n        ::\n\n            table = db.create_table('population')\n\n            # custom id and type\n            table2 = db.create_table('population2', 'age')\n            table3 = db.create_table('population3',\n                                     primary_id='city',\n                                     primary_type=db.types.text)\n            # custom length of String\n            table4 = db.create_table('population4',\n                                     primary_id='city',\n                                     primary_type=db.types.string(25))\n            # no primary key\n            table5 = db.create_table('population5',\n                                     primary_id=False)\n        \"\"\"\n        assert not isinstance(primary_type, six.string_types), \\\n            'Text-based primary_type support is dropped, use db.types.'\n        table_name = normalize_table_name(table_name)\n        with self.lock:\n            if table_name not in self._tables:\n                self._tables[table_name] = Table(self, table_name,\n                                                 primary_id=primary_id,\n                                                 primary_type=primary_type,\n                                                 auto_create=True)\n            return self._tables.get(table_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_table(self, table_name):\n        table_name = normalize_table_name(table_name)\n        with self.lock:\n            if table_name not in self._tables:\n                self._tables[table_name] = Table(self, table_name)\n            return self._tables.get(table_name)", "response": "Load a table.\n\n        This will fail if the tables does not already exist in the database. If\n        the table exists, its columns will be reflected and are available on\n        the :py:class:`Table <dataset.Table>` object.\n\n        Returns a :py:class:`Table <dataset.Table>` instance.\n        ::\n\n            table = db.load_table('population')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_table(self, table_name, primary_id=None, primary_type=None):\n        return self.create_table(table_name, primary_id, primary_type)", "response": "Load or create a table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a query on the database directly.", "response": "def query(self, query, *args, **kwargs):\n        \"\"\"Run a statement on the database directly.\n\n        Allows for the execution of arbitrary read/write queries. A query can\n        either be a plain text string, or a `SQLAlchemy expression\n        <http://docs.sqlalchemy.org/en/latest/core/tutorial.html#selecting>`_.\n        If a plain string is passed in, it will be converted to an expression\n        automatically.\n\n        Further positional and keyword arguments will be used for parameter\n        binding. To include a positional argument in your query, use question\n        marks in the query (i.e. ``SELECT * FROM tbl WHERE a = ?```). For\n        keyword arguments, use a bind parameter (i.e. ``SELECT * FROM tbl\n        WHERE a = :foo``).\n        ::\n\n            statement = 'SELECT user, COUNT(*) c FROM photos GROUP BY user'\n            for row in db.query(statement):\n                print(row['user'], row['c'])\n\n        The returned iterator will yield each result sequentially.\n        \"\"\"\n        if isinstance(query, six.string_types):\n            query = text(query)\n        _step = kwargs.pop('_step', QUERY_STEP)\n        rp = self.executable.execute(query, *args, **kwargs)\n        return ResultIter(rp, row_type=self.row_type, step=_step)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef printcolour(text, sameline=False, colour=get_colour(\"ENDC\")):\n    if sameline:\n        sep = ''\n    else:\n        sep = '\\n'\n    sys.stdout.write(get_colour(colour) + text + bcolours[\"ENDC\"] + sep)", "response": "Print color text using escape codes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drange(start, stop, step=1.0, include_stop=False):\n    if step == 0:\n        step = 0.01\n    r = start\n\n    if include_stop:\n        while r <= stop:\n            yield r\n            r += step\n            r = round(r, 10)\n    else:\n        while r < stop:\n            yield r\n            r += step\n            r = round(r, 10)", "response": "Generate between start and stop numbers w / optional step optionally include upper bound"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nabbreviate labels without introducing ambiguities.", "response": "def abbreviate(labels, rfill=' '):\n    \"\"\"\n    Abbreviate labels without introducing ambiguities.\n    \"\"\"\n    max_len = max(len(l) for l in labels)\n    for i in range(1, max_len):\n        abbrev = [l[:i].ljust(i, rfill) for l in labels]\n        if len(abbrev) == len(set(abbrev)):\n            break\n    return abbrev"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef box_text(text, width, offset=0):\n    box = \" \" * offset + \"-\" * (width+2) + \"\\n\"\n    box += \" \" * offset + \"|\" + text.center(width) + \"|\" + \"\\n\"\n    box += \" \" * offset + \"-\" * (width+2)\n    return box", "response": "Return text inside an ascii textbox"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the number of bins for the histogram.", "response": "def calc_bins(n, min_val, max_val, h=None, binwidth=None):\n    \"\"\"\n    Calculate number of bins for the histogram\n    \"\"\"\n    if not h:\n        h = max(10, math.log(n + 1, 2))\n    if binwidth == 0:\n        binwidth = 0.1\n    if binwidth is None:\n        binwidth = (max_val - min_val) / h\n    for b in drange(min_val, max_val, step=binwidth, include_stop=True):\n        if b.is_integer():\n            yield int(b)\n        else:\n            yield b"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the input data in the most optimal way", "response": "def read_numbers(numbers):\n    \"\"\"\n    Read the input data in the most optimal way\n    \"\"\"\n    if isiterable(numbers):\n        for number in numbers:\n            yield float(str(number).strip())\n    else:\n        with open(numbers) as fh:\n            for number in fh:\n                yield float(number.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a demo file", "response": "def run_demo():\n    \"\"\"\n    Run a demonstration\n    \"\"\"\n    module_dir = dirname(dirname(os.path.realpath(__file__)))\n    demo_file = os.path.join(module_dir, 'examples/data/exp.txt')\n\n    if not os.path.isfile(demo_file):\n        sys.stderr.write(\"demo input file not found!\\n\")\n        sys.stderr.write(\"run the downloaddata.sh script in the example first\\n\")\n        sys.exit(1)\n\n    # plotting a histogram\n    print(\"plotting a basic histogram\")\n    print(\"plot_hist('%s')\" % demo_file)\n    print(\"hist -f %s\" % demo_file)\n    print(\"cat %s | hist\" % demo_file)\n    plot_hist(demo_file)\n    print(\"*\" * 80)\n\n    # with colours\n    print(\"histogram with colours\")\n    print(\"plot_hist('%s', colour='blue')\" % demo_file)\n    print(\"hist -f %s -c blue\" % demo_file)\n    plot_hist(demo_file, colour='blue')\n    print(\"*\" * 80)\n\n    # changing the shape of the point\n    print(\"changing the shape of the bars\")\n    print(\"plot_hist('%s', pch='.')\" % demo_file)\n    print(\"hist -f %s -p .\" % demo_file)\n    plot_hist(demo_file, pch='.')\n    print(\"*\" * 80)\n\n    # changing the size of the plot\n    print(\"changing the size of the plot\")\n    print(\"plot_hist('%s', height=35.0, bincount=40)\" % demo_file)\n    print(\"hist -f %s -s 35.0 -b 40\" % demo_file)\n    plot_hist(demo_file, height=35.0, bincount=40)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_hist(f, height=20.0, bincount=None, binwidth=None, pch=\"o\", colour=\"default\", title=\"\", xlab=None, showSummary=False, regular=False):\n    if pch is None:\n        pch = \"o\"\n\n    if isinstance(f, str):\n        with open(f) as fh:\n            f = fh.readlines()\n\n    min_val, max_val = None, None\n    n, mean, sd = 0.0, 0.0, 0.0\n\n    for number in read_numbers(f):\n        n += 1\n        if min_val is None or number < min_val:\n            min_val = number\n        if max_val is None or number > max_val:\n            max_val = number\n        mean += number\n\n    mean /= n\n\n    for number in read_numbers(f):\n        sd += (mean - number)**2\n\n    sd /= (n - 1)\n    sd **= 0.5\n\n    bins = list(calc_bins(n, min_val, max_val, bincount, binwidth))\n    hist = dict((i, 0) for i in range(len(bins)))\n\n    for number in read_numbers(f):\n        for i, b in enumerate(bins):\n            if number <= b:\n                hist[i] += 1\n                break\n        if number == max_val and max_val > bins[len(bins) - 1]:\n            hist[len(hist) - 1] += 1\n\n    min_y, max_y = min(hist.values()), max(hist.values())\n\n    start = max(min_y, 1)\n    stop = max_y + 1\n\n    if regular:\n        start = 1\n\n    if height is None:\n        height = stop - start\n        if height > 20:\n            height = 20\n\n    ys = list(drange(start, stop, float(stop - start) / height))\n    ys.reverse()\n\n    nlen = max(len(str(min_y)), len(str(max_y))) + 1\n\n    if title:\n        print(box_text(title, max(len(hist) * 2, len(title)), nlen))\n    print()\n\n    used_labs = set()\n    for y in ys:\n        ylab = str(int(y))\n        if ylab in used_labs:\n            ylab = \"\"\n        else:\n            used_labs.add(ylab)\n        ylab = \" \" * (nlen - len(ylab)) + ylab + \"|\"\n\n        print(ylab, end=' ')\n\n        for i in range(len(hist)):\n            if int(y) <= hist[i]:\n                printcolour(pch, True, colour)\n            else:\n                printcolour(\" \", True, colour)\n        print('')\n    xs = hist.keys()\n\n    print(\" \" * (nlen + 1) + \"-\" * len(xs))\n\n    if xlab:\n        labels = abbreviate([str(b) for b in bins])\n        xlen = len(labels[0])\n        for i in range(0, xlen):\n            printcolour(\" \" * (nlen + 1), True, colour)\n            for x in range(0, len(hist)):\n                num = labels[x]\n                if x % 2 != 0:\n                    pass\n                elif i < len(num):\n                    print(num[i], end=' ')\n                else:\n                    print(\" \", end=' ')\n            print('')\n\n    center = max(map(len, map(str, [n, min_val, mean, max_val])))\n    center += 15\n\n    if showSummary:\n        print()\n        print(\"-\" * (2 + center))\n        print(\"|\" + \"Summary\".center(center) + \"|\")\n        print(\"-\" * (2 + center))\n        summary = \"|\" + (\"observations: %d\" % n).center(center) + \"|\\n\"\n        summary += \"|\" + (\"min value: %f\" % min_val).center(center) + \"|\\n\"\n        summary += \"|\" + (\"mean : %f\" % mean).center(center) + \"|\\n\"\n        summary += \"|\" + (\"std dev : %f\" % sd).center(center) + \"|\\n\"\n        summary += \"|\" + (\"max value: %f\" % max_val).center(center) + \"|\\n\"\n        summary += \"-\" * (2 + center)\n        print(summary)", "response": "Make a histogram of the terminal entries in a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a scatter plot of the n - grams in a single file.", "response": "def plot_scatter(f, xs, ys, size, pch, colour, title):\n    \"\"\"\n    Form a complex number.\n\n    Arguments:\n        f -- comma delimited file w/ x,y coordinates\n        xs -- if f not specified this is a file w/ x coordinates\n        ys -- if f not specified this is a filew / y coordinates\n        size -- size of the plot\n        pch -- shape of the points (any character)\n        colour -- colour of the points\n        title -- title of the plot\n    \"\"\"\n    cs = None\n    if f:\n        if isinstance(f, str):\n            with open(f) as fh:\n                data = [tuple(line.strip().split(',')) for line in fh]\n        else:\n            data = [tuple(line.strip().split(',')) for line in f]\n        xs = [float(i[0]) for i in data]\n        ys = [float(i[1]) for i in data]\n        if len(data[0]) > 2:\n            cs = [i[2].strip() for i in data]\n    elif isinstance(xs, list) and isinstance(ys, list):\n        pass\n    else:\n        with open(xs) as fh:\n            xs = [float(str(row).strip()) for row in fh]\n        with open(ys) as fh:\n            ys = [float(str(row).strip()) for row in fh]\n\n    _plot_scatter(xs, ys, size, pch, colour, title, cs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naccepting a word and returns a string of an approximate pronounciation ( IPA", "response": "def transcribe(self, text, punctuation=True):\n        \"\"\"Accepts a word and returns a string of an approximate pronounciation (IPA)\"\"\"\n\n        if not punctuation:\n            text = re.sub(r\"[\\.\\\";\\,\\:\\[\\]\\(\\)!&?\u2018]\", \"\", text)\n\n        text = re.sub(r'sch', '\u0283', text)\n        text = re.sub(r'(?<=[aeiou\u00e4\u00eb\u00f6\u00fc\u00e2\u00e6\u0153\u00ea\u00ee\u00f4\u00fb])h', '\u03c7', text)\n        text = re.sub(r'h(?=[aeiou\u00e4\u00eb\u00f6\u00fc\u00e2\u00e6\u0153\u00ea\u00ee\u00f4\u00fb])', '\u03c7', text)\n        text = re.sub(r'(?<=[aeiou\u00e4\u00eb\u00f6\u00fc\u00e2\u00e6\u0153\u00ea\u00ee\u00f4\u00fb])s(?=[aeiou\u00e4\u00eb\u00f6\u00fc\u00e2\u00e6\u0153\u00ea\u00ee\u00f4\u00fb])', 'z\u0325', text)\n        text = re.sub(r'^s(?=[aeiou\u00e4\u00eb\u00f6\u00fc\u00e2\u00e6\u0153\u00ea\u00ee\u00f4\u00fb])', 'z\u0325', text)\n\n        for w, val in zip(Dipthongs_IPA.keys(), Dipthongs_IPA.values()):\n            text = text.replace(w, val)\n\n        for w, val in zip(IPA.keys(), IPA.values()):\n            text = text.replace(w, val)\n\n        return \"[\" + text + \"]\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsyllabifying module for Middle High German The algorithm works by applying the MOP(Maximal Onset Principle) on open syllables. For closed syllables, the legal partitions are checked and applied. The word is always returned in lowercase. Examples: >>> Word('entsl\u00e2fen').syllabify() ['ent', 'sl\u00e2', 'fen'] >>> Word('fr\u00f6ude').syllabify() ['fr\u00f6u', 'de'] >>> Word('f\u00fcerest').syllabify() ['f\u00fce', 'rest']", "response": "def syllabify(self):\n        \"\"\"\n        Syllabifier module for Middle High German\n\n        The algorithm works by applying the MOP(Maximal Onset Principle)\n        on open syllables. For closed syllables, the legal partitions\n        are checked and applied. The word is always returned in lowercase.\n\n        Examples:\n            >>> Word('entsl\u00e2fen').syllabify()\n            ['ent', 'sl\u00e2', 'fen']\n\n            >>> Word('fr\u00f6ude').syllabify()\n            ['fr\u00f6u', 'de']\n\n            >>> Word('f\u00fcerest').syllabify()\n\t    ['f\u00fce', 'rest']\n        \"\"\"\n\n        # Array holding the index of each given syllable\n        ind = []\n\n        i = 0\n        # Iterate through letters of word searching for the nuclei\n\n        while i < len(self.word) - 1:\n\n            if self.word[i] in SHORT_VOWELS + LONG_VOWELS:\n\n                nucleus = ''\n\n                # Find cluster of vowels\n                while self.word[i] in SHORT_VOWELS + LONG_VOWELS and i < len(self.word) - 1:\n                    nucleus += self.word[i]\n                    i += 1\n\n                try:\n                    # Check whether it is suceeded by a geminant\n\n                    if self.word[i] == self.word[i + 1]:\n                        ind.append(i)\n                        i += 2\n                        continue\n\n                except IndexError:\n                    pass\n\n                if nucleus in SHORT_VOWELS:\n                    ind.append(i + 2 if self.word[i:i+3] in TRIPHTHONGS else i + 1 if self.word[i:i + 2] in DIPHTHONGS else i)\n                    continue\n\n                else:\n                    ind.append(i - 1)\n                    continue\n\n            i += 1\n\n        self.syllabified = self.word\n\n        for n, k in enumerate(ind):\n            self.syllabified = self.syllabified[:k + n + 1] + \".\" + self.syllabified[k + n + 1:]\n\n        # Check whether the last syllable lacks a vowel nucleus\n\n        self.syllabified = self.syllabified.split(\".\")\n\n        if sum(map(lambda x: x in SHORT_VOWELS, self.syllabified[-1])) == 0:\n            self.syllabified[-2] += self.syllabified[-1]\n            self.syllabified = self.syllabified[:-1]\n\n        return self.syllabified"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the ASCII encoding of a string", "response": "def ASCII_encoding(self):\n        \"\"\"Returns the ASCII encoding of a string\"\"\"\n\n        w = unicodedata.normalize('NFKD', self.word).encode('ASCII',\n                                                            'ignore')  # Encode into ASCII, returns a bytestring\n        w = w.decode('utf-8')  # Convert back to string\n\n        return w"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints single text in file in markdown.", "response": "def markdown_single_text(self, catalog, cdli_number):\n        \"\"\"\n        Prints single text in file in markdown.\n       :param catalog: text ingested by cdli_corpus\n       :param cdli_number: text you wish to print\n       :return: output in filename.md\n       \"\"\"\n        if cdli_number in catalog:\n            pnum = catalog[cdli_number]['pnum']\n            edition = catalog[cdli_number]['edition']\n            metadata = '\\n\\t'.join(catalog[cdli_number]['metadata'])\n            transliteration = '\\n\\t'.join(catalog[cdli_number]['transliteration'])\n            normalization = '\\n\\t'.join(catalog[cdli_number]['normalization'])\n            translation = '\\n\\t'.join(catalog[cdli_number]['translation'])\n            m_d = \"\"\"{edition}\n{pnum}\n---\n### metadata\n    {metadata}\n### transliteration\n    {trans}\n### normalization\n    {norm}\n### translation\n    {translation}  \n\"\"\".format(pnum=pnum, edition=edition, metadata=metadata,\n           trans=transliteration, norm=normalization,\n           translation=translation)\n            self.markdown_text = m_d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef html_print_file(self, catalog, destination):\n        with open(destination, mode='r+', encoding='utf8') as t_f:\n            for text in catalog:\n                pnum = catalog[text]['pnum']\n                edition = catalog[text]['edition']\n                metadata = '<br>\\n'.join(catalog[text]['metadata'])\n                transliteration = '<br>\\n'.join(catalog[text]['transliteration'])\n                normalization = '<br>\\n'.join(catalog[text]['normalization'])\n                translation = '<br>\\n'.join(catalog[text]['translation'])\n                self.html_file = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{edition}</title>\n</head>\n<body><table cellpadding=\"10\"; border=\"1\">\n<tr><th>\n<h2>{edition}<br>{pnum}</h2>\n</th><th>\n<h3>transliteration</h3>\n</th><th>\n<h3>normalization</h3>\n</th><th>\n<h3>translation</h3>\n</tr><tr><td>\n{metadata}</td><td>\n<p>{trans}\n</td><td>\n<p>{norm}\n</td><td>\n<font size='2'>\n{translation}\n</font></td></tr>\n\n</table>\n<br>\n</body>\n</html>\"\"\".format(\n    pnum=pnum, edition=edition, metadata=metadata,\n    trans=transliteration, norm=normalization,\n    translation=translation)\n                t_f.write(self.html_file)", "response": "Print the text file in html."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints the single text in html.", "response": "def html_print_single_text(self, catalog, cdli_number, destination):\n        \"\"\"\n        Prints text_file in html.\n        :param catalog: CDLICorpus().catalog\n        :param cdli_number: which text you want printed\n        :param destination: where you wish to save the HTML data\n        :return: output in html_file.html.\n        \"\"\"\n        if cdli_number in catalog:\n            pnum = catalog[cdli_number]['pnum']\n            edition = catalog[cdli_number]['edition']\n            metadata = '<br>\\n'.join(catalog[cdli_number]['metadata'])\n            transliteration = '<br>\\n'.join(catalog[cdli_number]['transliteration'])\n            normalization = '<br>\\n'.join(catalog[cdli_number]['normalization'])\n            translation = '<br>\\n'.join(catalog[cdli_number]['translation'])\n            self.html_single = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{edition}</title>\n</head>\n<body><table cellpadding=\"10\"; border=\"1\">\n<tr><th>\n<h2>{edition}<br>{pnum}</h2>\n</th><th>\n<h3>transliteration</h3>\n</th><th>\n<h3>normalization</h3>\n</th><th>\n<h3>translation</h3>\n</tr><tr><td>\n{metadata}</td><td>\n<p>{trans}\n</td><td>\n<p>{norm}\n</td><td>\n<font size='2'>\n{translation}\n</font></td></tr>\n\n</table>\n<br>\n</body>\n</html>\"\"\".format(\n    pnum=pnum, edition=edition, metadata=metadata,\n    trans=transliteration, norm=normalization,\n    translation=translation)\n            with open(destination, mode='r+', encoding='utf8') as t_f:\n                t_f.write(self.html_single)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_consonant(sign):\n        for key in TITTLES:\n            sign = sign.replace(key, TITTLES[key])\n        return sign", "response": "Converts a consonant sign to a list of consonant names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a number into subscript.", "response": "def _convert_number_to_subscript(num):\n        \"\"\"\n        Converts number into subscript\n\n        input = [\"a\", \"a1\", \"a2\", \"a3\", \"be2\", \"be3\", \"bad2\", \"bad3\"]\n        output = [\"a\", \"a\u2081\", \"a\u2082\", \"a\u2083\", \"be\u2082\", \"be\u2083\", \"bad\u2082\", \"bad\u2083\"]\n\n        :param num: number called after sign\n        :return: number in subscript\n        \"\"\"\n        subscript = ''\n        for character in str(num):\n            subscript += chr(0x2080 + int(character))\n        return subscript"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_num(self, sign):\n        # Check if there's a number at the end\n        new_sign, num = self._get_number_from_sign(sign)\n        if num < 2:  # \"ab\" -> \"ab\"\n            return new_sign.replace(str(num),\n                                    self._convert_number_to_subscript(num))\n        if num > 3:  # \"buru14\" -> \"buru\u2081\u2084\"\n            return new_sign.replace(str(num),\n                                    self._convert_number_to_subscript(num))\n        if self.two_three:   # pylint: disable=no-else-return\n            return new_sign.replace(str(num),\n                                    self._convert_number_to_subscript(num))\n        else:\n            # \"bad3\" -> \"b\u00e0d\"\n            for i, character in enumerate(new_sign):\n                new_vowel = ''\n                if character in VOWELS:\n                    if num == 2:\n                        # noinspection PyUnusedLocal\n                        new_vowel = character + chr(0x0301)\n                    elif num == 3:\n                        new_vowel = character + chr(0x0300)\n                    break\n            return new_sign[:i] + normalize('NFC', new_vowel) + \\\n                   new_sign[i+1:].replace(str(num), '')", "response": "Converts a number registered in get_number_from_sign to a number in the format a2 a3 buru14 b\u00e0d."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self, text_string):\n        output = [self._convert_num(self._convert_consonant(token)) for\n                  token in text_string]\n        return output", "response": "Takes a list of tokens will return the list converted from ATF\n        format to print - format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute Levenshtein Distance between two words.", "response": "def Levenshtein_Distance(w1, w2):\n        \"\"\"\n        Computes Levenshtein Distance between two words\n\n        Args:\n            :param w1: str\n            :param w2: str\n            :return: int\n\n        Examples:\n\n            >>> Levenshtein.Levenshtein_Distance('noctis', 'noctem')\n            2\n\n            >>> Levenshtein.Levenshtein_Distance('nox', 'nochem')\n            4\n\n            >>> Levenshtein.Levenshtein_Distance('orbis', 'robis')\n            2\n        \"\"\"\n        m, n = len(w1), len(w2)\n        v1 = [i for i in range(n + 1)]\n        v2 = [0 for i in range(n + 1)]\n\n        for i in range(m):\n            v2[0] = i + 1\n\n            for j in range(n):\n                delCost = v1[j + 1] + 1\n                insCost = v2[j] + 1\n\n                subCost = v1[j]\n                if w1[i] != w2[j]: subCost += 1\n\n                v2[j + 1] = min(delCost, insCost, subCost)\n            v1, v2 = v2, v1\n\n        return v1[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the Damerau - Levenshtein Distance between two words and returns the distance in degrees between the two words.", "response": "def Damerau_Levenshtein_Distance(w1, w2):\n        \"\"\"\n        Computes Damerau-Levenshtein Distance between two words\n\n        Args:\n            :param w1: str\n            :param w2: str\n            :return int:\n\n        Examples:\n            For the most part, Damerau-Levenshtein behaves\n            identically to Levenshtein:\n\n            >>> Levenshtein.Damerau_Levenshtein_Distance('noctis', 'noctem')\n            2\n\n            >>> Levenshtein.Levenshtein_Distance('nox', 'nochem')\n            4\n\n            The strength of DL lies in detecting transposition of characters:\n\n            >>> Levenshtein.Damerau_Levenshtein_Distance('orbis', 'robis')\n            1\n\n        \"\"\"\n        # Define alphabet\n        alph = sorted(list(set(w1 + w2)))\n        # Calculate alphabet size\n        alph_s = len(alph)\n        dam_ar = [0 for _ in range(alph_s)]\n        mat = [[0 for _ in range(len(w2) + 2)] for _ in range(len(w1) + 2)]\n\n        max_dist = len(w1) + len(w2)\n        mat[0][0] = max_dist\n\n        # Initialize matrix margin to the maximum possible distance (essentially inf) for ease of calculations (avoiding try blocks)\n\n        for i in range(1, len(w1) + 2):\n            mat[i][0] = max_dist\n            mat[i][1] = i - 1\n\n        for i in range(1, len(w2) + 2):\n            mat[0][i] = max_dist\n            mat[1][i] = i - 1\n\n        for i in range(2, len(w1) + 2):\n            tem = 0\n\n            for j in range(2, len(w2) + 2):\n\n                k = dam_ar[alph.index(w2[j - 2])]\n                l = tem\n\n                if w1[i - 2] == w2[j - 2]:\n                    cost = 0\n                    tem = j\n                else:\n                    cost = 1\n\n                # The reccurence relation of DL is identical to that of Levenshtein with the addition of transposition\n                mat[i][j] = min(mat[i - 1][j - 1] + cost, mat[i][j - 1] + 1, mat[i - 1][j] + 1,\n                                mat[k - 1][l - 1] + i + j - k - l - 1)\n\n            dam_ar[alph.index(w1[i - 2])] = i\n\n        return mat[-1][-1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef counter_from_str(self, string):\n        string_list = [chars for chars in string if chars not in self.punctuation]\n        string_joined = ''.join(string_list)\n        tokens = self.punkt.word_tokenize(string_joined)\n        return Counter(tokens)", "response": "Build word frequency list from incoming string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds word frequency list from one of several available corpora.", "response": "def counter_from_corpus(self, corpus):\n        \"\"\"Build word frequency list from one of several available corpora.\n        TODO: Make this count iteratively, not all at once\n        \"\"\"\n        assert corpus in ['phi5', 'tlg'], \\\n            \"Corpus '{0}' not available. Choose from 'phi5' or 'tlg'.\".format(corpus)\n\n        all_strings = self._assemble_corpus_string(corpus=corpus)\n        return self.counter_from_str(all_strings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a list of filepaths returns a string containing contents of all files.", "response": "def _assemble_corpus_string(self, corpus):\n        \"\"\"Takes a list of filepaths, returns a string containing contents of\n        all files.\"\"\"\n\n        if corpus == 'phi5':\n            filepaths = assemble_phi5_author_filepaths()\n            file_cleaner = phi5_plaintext_cleanup\n        elif corpus == 'tlg':\n            filepaths = assemble_tlg_author_filepaths()\n            file_cleaner = tlg_plaintext_cleanup\n\n        for filepath in filepaths:\n            with open(filepath) as file_open:\n                file_read = file_open.read().lower()\n            file_clean = file_cleaner(file_read)\n            yield file_clean"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn bound form of nound given its gender.", "response": "def get_bound_form(self, noun, gender):\n        \"\"\"Return bound form of nound, given its gender.\"\"\"\n        syllables = self.syllabifier.syllabify(noun)\n        stem = self.stemmer.get_stem(noun, gender)\n        cv_pattern = self.cv_patterner.get_cv_pattern(stem)\n        # Based on Huehnergard Appendix 6.C.1: base in -VC\n        if [letter[0] for letter in cv_pattern[-2:]] == ['V', 'C'] or stem in ['nakr']:\n            # a. 2-syllable\n            if len(syllables) > 2:\n                # aw\u012blum > aw\u012bl, nakrum > naker\n                if stem in ['nakr']:\n                    return 'naker'\n                else:\n                    return stem\n            # b. 1-syllable\n            elif len(syllables) > 1:\n                # b\u0113lum > b\u0113l\n                return stem\n            # c. abum, a\u1e2bum\n            if stem in ['ab', 'a\u1e2b']:\n                return stem + 'i'\n        # Appendix 6.C.2: base in -C\u2081C\u2081\n        if cv_pattern[-1][:2] == cv_pattern[-2][:2]:\n            # a. 1-syllable\n            if 3 > len(syllables) > 1:\n                return stem + 'i'\n            # b. 2-syllable, -tt\n            if len(syllables) > 2 and cv_pattern[-1][2] + cv_pattern[-2][2] == 'tt':\n                return stem + 'i'\n            # c. 2-syllable, other\n            if len(syllables) > 2:\n                return stem[:-1]\n        # Appendix 6.C.3: base in -C\u2081C\u2082, C\u2082 \u2260 t, i.e. pVrs\n        if cv_pattern[-1][0] == cv_pattern[-2][0] and cv_pattern[-1][1] != cv_pattern[-2][1]:\n            return stem[:-1] + stem[1] + stem[-1]\n        # Appendix 6.C.4: base in -Ct (fem.)\n        if cv_pattern[-1][2] == 't' and cv_pattern[-2][0] == 'C':\n            if len(syllables) > 2:\n                return stem + 'i'\n            # Need to deal with fem. Ptcpl. m\u0101\u1e2birtum -> m\u0101\u1e2birat\n            if len(syllables) > 1:\n                # These are case by case\n                if stem in ['q\u012b\u0161t']:\n                    return stem + 'i'\n                if stem in ['m\u0101rt']:\n                    return stem[:-1] + stem[1] + stem[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_equal(self, other_consonnant):\n        return self.place == other_consonnant.place and self.manner == other_consonnant.manner and \\\n               self.voiced == other_consonnant.voiced and self.geminate == other_consonnant.geminate", "response": "Return True if two consonants are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef real_sound_match_abstract_sound(self, abstract_pos: AbstractPosition) -> bool:\n        assert isinstance(abstract_pos, AbstractPosition)\n        if self.before is not None and self.after is not None:\n            return self.position == abstract_pos.position and self.before.match_list(abstract_pos.before) and \\\n                   self.after.match_list(abstract_pos.after)\n        elif self.before is None and self.after is None:\n            return self.position == abstract_pos.position\n        elif self.before is None:\n            return self.position == abstract_pos.position and self.after.match_list(abstract_pos.after)\n        else:\n            return self.position == abstract_pos.position and self.before.match_list(abstract_pos.before)", "response": "Returns True if the real sound matches the abstract sound."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef text_to_phonemes(self, word: str) -> list:\n        phonemes = []\n        is_repeted = False\n        if len(word) >= 2:\n            for index in range(len(word) - 1):\n                if is_repeted:\n                    is_repeted = False\n                    continue\n                if word[index:index + 2] in self.diphthongs_ipa:  # diphthongs\n                    phonemes.append(self.diphthongs_ipa_class[word[index] + word[index + 1]])\n                    is_repeted = True\n                elif word[index] == word[index + 1]:\n                    phonemes.append(self.ipa_class[word[index]].lengthen())\n                    is_repeted = True\n                else:\n                    phonemes.append(self.ipa_class[word[index]])\n            if not is_repeted:\n                phonemes.append(self.ipa_class[word[len(word) - 1]])\n        else:\n            phonemes.append(self.ipa_class[word[0]])\n        return phonemes", "response": "Converts a text word into a list of phonemes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a list of phonemes to a list of phonetic representation.", "response": "def phonemes_to_phonetic_representation(self, phonemes: list) -> str:\n        \"\"\"\n        Use of rules to precise pronunciation of a preprocessed list of transcribed words\n        :param phonemes: list(Vowel or Consonant)\n        :return: str\n        \"\"\"\n        phonetic_representation = []\n        if len(phonemes) >= 2:\n            for i in range(len(phonemes)):\n                if i == 0:\n                    current_pos = Position(Rank.first, None, phonemes[i])\n                elif i < len(phonemes) - 1:\n                    current_pos = Position(Rank.inner, phonemes[i - 1], phonemes[i + 1])\n                else:\n                    current_pos = Position(Rank.last, phonemes[i - 1], None)\n                found = False\n                for rule in self.rules:\n                    if rule.temp_sound.ipar == phonemes[i].ipar:\n                        if rule.can_apply(current_pos):\n                            phonetic_representation.append(rule.estimated_sound.ipar)\n                            found = True\n                            break\n                if not found:\n                    phonetic_representation.append(phonemes[i].ipar)\n        else:\n            phonetic_representation.append(phonemes[0].ipar)\n        return \"\".join(phonetic_representation)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokenize(self: object, untokenized_string: str, include_blanks=False):\n        \n        # load tokenizer\n        assert isinstance(untokenized_string, str), 'Incoming argument must be a string.'\n\n        # make list of tokenized sentences\n        if include_blanks:\n            tokenized_lines = untokenized_string.splitlines()\n        else:\n            tokenized_lines = [line for line in untokenized_string.splitlines() if line != '']\n        return tokenized_lines", "response": "Tokenize lines by \\ n."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary with punctuation removed.", "response": "def remove_punctuation_dict() -> Dict[int, None]:\n    \"\"\"\n    Provide a dictionary for removing punctuation, swallowing spaces.\n\n    :return dict with punctuation from the unicode table\n\n    >>> print(\"I'm ok! Oh #%&*()[]{}!? Fine!\".translate(\n    ... remove_punctuation_dict()).lstrip())\n    Im ok Oh  Fine\n    \"\"\"\n    tmp = dict((i, None) for i in range(sys.maxunicode)\n               if unicodedata.category(chr(i)).startswith('P'))\n    return tmp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef punctuation_for_spaces_dict() -> Dict[int, str]:\n    return dict((i, \" \") for i in range(sys.maxunicode)\n                if unicodedata.category(chr(i)).startswith('P'))", "response": "Return a dictionary with punctuation removed and keeping spaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef differences(scansion: str, candidate: str) -> List[int]:\n    before = scansion.replace(\" \", \"\")\n    after = candidate.replace(\" \", \"\")\n    diffs = []\n    for idx, tmp in enumerate(before):\n        if before[idx] != after[idx]:\n            diffs.append(idx)\n    return diffs", "response": "Given two strings return a list of index positions where the contents differ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mark_list(line: str) -> List[int]:\n    marks = []\n    for idx, car in enumerate(list(line)):\n        if car != \" \":\n            marks.append(idx)\n    return marks", "response": "Given a string return a list of index positions where a character or non blank space exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef space_list(line: str) -> List[int]:\n    spaces = []\n    for idx, car in enumerate(list(line)):\n        if car == \" \":\n            spaces.append(idx)\n    return spaces", "response": "Given a string return a list of index positions where a blank space occurs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a line of syllables and spaces and a list of syllables produce a list of the syllables with trailing spaces attached as approriate.", "response": "def to_syllables_with_trailing_spaces(line: str, syllables: List[str]) -> List[str]:\n    \"\"\"\n    Given a line of syllables and spaces, and a list of syllables, produce a list of the\n    syllables with trailing spaces attached as approriate.\n\n    :param line:\n    :param syllables:\n    :return:\n\n    >>> to_syllables_with_trailing_spaces(' arma virumque cano ',\n    ... ['ar', 'ma', 'vi', 'rum', 'que', 'ca', 'no' ])\n    [' ar', 'ma ', 'vi', 'rum', 'que ', 'ca', 'no ']\n    \"\"\"\n    syllabs_spaces = []\n    idx = 0\n    linelen = len(line)\n    for position, syl in enumerate(syllables):\n        start = line.index(syl, idx)\n        idx = start + len(syl)\n        if position == 0 and start > 0:  # line starts with punctuation, substituted w/ spaces\n            syl = (start * \" \") + syl\n        if idx + 1 > len(line):\n            syllabs_spaces.append(syl)\n            return syllabs_spaces\n        nextchar = line[idx]\n        if nextchar != \" \":\n            syllabs_spaces.append(syl)\n            continue\n        else:\n            tmpidx = idx\n            while tmpidx < linelen and nextchar == \" \":\n                syl += \" \"\n                tmpidx += 1\n                if tmpidx == linelen:\n                    syllabs_spaces.append(syl)\n                    return syllabs_spaces\n                nextchar = line[tmpidx]\n            idx = tmpidx - 1\n            syllabs_spaces.append(syl)\n    return syllabs_spaces"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join_syllables_spaces(syllables: List[str], spaces: List[int]) -> str:\n    syllable_line = list(\"\".join(syllables))\n    for space in spaces:\n        syllable_line.insert(space, \" \")\n    return \"\".join(flatten(syllable_line))", "response": "Given a list of syllables and a list of integers indicating the position of spaces return a string that has a space inserted at the designated points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a stress value and a scansion line return the index positions of the stresses.", "response": "def stress_positions(stress: str, scansion: str) -> List[int]:\n    \"\"\"\n    Given a stress value and a scansion line, return the index positions of the stresses.\n\n    :param stress:\n    :param scansion:\n    :return:\n\n    >>> stress_positions(\"-\", \"    -  U   U - UU    - U U\")\n    [0, 3, 6]\n    \"\"\"\n    line = scansion.replace(\" \", \"\")\n    stresses = []\n    for idx, char in enumerate(line):\n        if char == stress:\n            stresses.append(idx)\n    return stresses"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a list of strings with different space swapping elisions applied merge the elisions and return the resulting list.", "response": "def merge_elisions(elided: List[str]) -> str:\n    \"\"\"\n    Given a list of strings with different space swapping elisions applied, merge the elisions,\n    taking the most without compounding the omissions.\n\n    :param elided:\n    :return:\n\n    >>> merge_elisions([\n    ... \"ignavae agua multum hiatus\", \"ignav   agua multum hiatus\" ,\"ignavae agua mult   hiatus\"])\n    'ignav   agua mult   hiatus'\n    \"\"\"\n    results = list(elided[0])\n    for line in elided:\n        for idx, car in enumerate(line):\n            if car == \" \":\n                results[idx] = \" \"\n    return \"\".join(results)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of letters and a list of consonant positions move the consonant positions to the right.", "response": "def move_consonant_right(letters: List[str], positions: List[int]) -> List[str]:\n    \"\"\"\n    Given a list of letters, and a list of consonant positions, move the consonant positions to\n    the right, merging strings as necessary.\n\n    :param letters:\n    :param positions:\n    :return:\n\n    >>> move_consonant_right(list(\"abbra\"), [ 2, 3])\n    ['a', 'b', '', '', 'bra']\n    \"\"\"\n    for pos in positions:\n        letters[pos + 1] = letters[pos] + letters[pos + 1]\n        letters[pos] = \"\"\n    return letters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of letters and a list of consonant positions move the consonant positions to the left.", "response": "def move_consonant_left(letters: List[str], positions: List[int]) -> List[str]:\n    \"\"\"\n    Given a list of letters, and a list of consonant positions, move the consonant positions to\n    the left, merging strings as necessary.\n\n    :param letters:\n    :param positions:\n    :return:\n\n    >>> move_consonant_left(['a', 'b', '', '', 'bra'], [1])\n    ['ab', '', '', '', 'bra']\n    \"\"\"\n    for pos in positions:\n        letters[pos - 1] = letters[pos - 1] + letters[pos]\n        letters[pos] = \"\"\n    return letters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_next(letters: List[str], positions: List[int]) -> List[str]:\n    for pos in positions:\n        letters[pos] = letters[pos] + letters[pos + 1]\n        letters[pos + 1] = \"\"\n    return letters", "response": "Given a list of letter positions merge each letter with its next neighbor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of letters remove any empty strings.", "response": "def remove_blanks(letters: List[str]):\n    \"\"\"\n    Given a list of letters, remove any empty strings.\n\n    :param letters:\n    :return:\n\n    >>> remove_blanks(['a', '', 'b', '', 'c'])\n    ['a', 'b', 'c']\n    \"\"\"\n    cleaned = []\n    for letter in letters:\n        if letter != \"\":\n            cleaned.append(letter)\n    return cleaned"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a string split on a section and return the two sections as a tuple.", "response": "def split_on(word: str, section: str) -> Tuple[str, str]:\n    \"\"\"\n    Given a string, split on a section, and return the two sections as a tuple.\n\n    :param word:\n    :param section:\n    :return:\n\n    >>> split_on('hamrye', 'ham')\n    ('ham', 'rye')\n    \"\"\"\n    return word[:word.index(section)] + section, word[word.index(section) + len(section):]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_blank_spaces(syllables: List[str]) -> List[str]:\n    cleaned = []\n    for syl in syllables:\n        if syl == \" \" or syl == '':\n            pass\n        else:\n            cleaned.append(syl)\n    return cleaned", "response": "Given a list of letters remove any blank spaces or empty strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef overwrite(char_list: List[str], regexp: str, quality: str, offset: int = 0) -> List[str]:\n    long_matcher = re.compile(regexp)\n    line = \"\".join(char_list)\n    long_positions = long_matcher.finditer(line)\n    for match in long_positions:\n        (start, end) = match.span()  # pylint: disable=unused-variable\n        char_list[start + offset] = quality\n    return char_list", "response": "Given a list of characters and spaces a matching regular expression and a quality or\n    character replace the matching character with a space overwriting with an offset and a multiplier."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a list of stressed positions and count of possible positions return a list of unstressed positions.", "response": "def get_unstresses(stresses: List[int], count: int) -> List[int]:\n    \"\"\"\n    Given a list of stressed positions, and count of possible positions, return a list of\n    the unstressed positions.\n\n    :param stresses: a list of stressed positions\n    :param count: the number of possible positions\n    :return: a list of unstressed positions\n\n    >>> get_unstresses([0, 3, 6, 9, 12, 15], 17)\n    [1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16]\n    \"\"\"\n    return list(set(range(count)) - set(stresses))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npresenting active subjunctive verbs and I Is and returns a new version of the active verbs.", "response": "def present_active_subjunctive(self):\n        \"\"\"\n        Strong verbs\n\n        I\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"l\u00edta\", \"l\u00edtr\", \"leit\", \"litu\", \"litinn\"])\n        >>> verb.present_active_subjunctive()\n        ['l\u00edta', 'l\u00edtir', 'l\u00edti', 'l\u00edtim', 'l\u00edti\u00f0', 'l\u00edti']\n\n        II\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"bj\u00f3\u00f0a\", \"b\u00fd\u00f0r\", \"bau\u00f0\", \"bu\u00f0u\", \"bo\u00f0inn\"])\n        >>> verb.present_active_subjunctive()\n        ['bj\u00f3\u00f0a', 'bj\u00f3\u00f0ir', 'bj\u00f3\u00f0i', 'bj\u00f3\u00f0im', 'bj\u00f3\u00f0i\u00f0', 'bj\u00f3\u00f0i']\n\n        III\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"ver\u00f0a\", \"ver\u00f0r\", \"var\u00f0\", \"ur\u00f0u\", \"or\u00f0inn\"])\n        >>> verb.present_active_subjunctive()\n        ['ver\u00f0a', 'ver\u00f0ir', 'ver\u00f0i', 'ver\u00f0im', 'ver\u00f0i\u00f0', 'ver\u00f0i']\n\n        IV\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"bera\", \"berr\", \"bar\", \"b\u00e1ru\", \"borinn\"])\n        >>> verb.present_active_subjunctive()\n        ['bera', 'berir', 'beri', 'berim', 'beri\u00f0', 'beri']\n\n        V\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"gefa\", \"gefr\", \"gaf\", \"g\u00e1fu\", \"gefinn\"])\n        >>> verb.present_active_subjunctive()\n        ['gefa', 'gefir', 'gefi', 'gefim', 'gefi\u00f0', 'gefi']\n\n        VI\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"fara\", \"ferr\", \"f\u00f3r\", \"f\u00f3ru\", \"farinn\"])\n        >>> verb.present_active_subjunctive()\n        ['fara', 'farir', 'fari', 'farim', 'fari\u00f0', 'fari']\n\n        VII\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"r\u00e1\u00f0a\", \"r\u00e6\u00f0r\", \"r\u00e9\u00f0\", \"r\u00e9\u00f0u\", \"r\u00e1\u00f0inn\"])\n        >>> verb.present_active_subjunctive()\n        ['r\u00e1\u00f0a', 'r\u00e1\u00f0ir', 'r\u00e1\u00f0i', 'r\u00e1\u00f0im', 'r\u00e1\u00f0i\u00f0', 'r\u00e1\u00f0i']\n\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"vera\", \"a\", \"a\", \"a\", \"a\"])\n        >>> verb.present_active_subjunctive()\n        ['s\u00e9', 's\u00e9r', 's\u00e9', 's\u00e9m', 's\u00e9\u00f0', 's\u00e9']\n\n        >>> verb = StrongOldNorseVerb()\n        >>> verb.set_canonic_forms([\"sj\u00e1\", \"a\", \"a\", \"a\", \"a\"])\n        >>> verb.present_active_subjunctive()\n        ['sj\u00e1', 's\u00e9r', 's\u00e9', 's\u00e9m', 's\u00e9\u00f0', 's\u00e9']\n\n        :return:\n        \"\"\"\n\n        if self.sng == \"vera\":\n            forms = [\"s\u00e9\", \"s\u00e9r\", \"s\u00e9\", \"s\u00e9m\", \"s\u00e9\u00f0\", \"s\u00e9\"]\n            return forms\n        elif self.sng == \"sj\u00e1\":\n            forms = [\"sj\u00e1\", \"s\u00e9r\", \"s\u00e9\", \"s\u00e9m\", \"s\u00e9\u00f0\", \"s\u00e9\"]\n            return forms\n        else:\n            subjunctive_root = self.sng[:-1] if self.sng[-1] == \"a\" else self.sng\n\n            forms = [subjunctive_root + \"a\"]\n            subjunctive_root = subjunctive_root[:-1] if subjunctive_root[-1] == \"j\" else subjunctive_root\n            forms.append(subjunctive_root + \"ir\")\n            forms.append(subjunctive_root + \"i\")\n            forms.append(subjunctive_root + \"im\")\n            forms.append(subjunctive_root + \"i\u00f0\")\n            forms.append(subjunctive_root + \"i\")\n            return forms"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new version of the current version of the current version that is present active.", "response": "def present_active(self):\n        \"\"\"\n        Weak verbs\n        I\n        >>> verb = WeakOldNorseVerb()\n        >>> verb.set_canonic_forms([\"kalla\", \"kalla\u00f0i\", \"kalla\u00f0inn\"])\n        >>> verb.present_active()\n        ['kalla', 'kallar', 'kallar', 'k\u00f6llum', 'kalli\u00f0', 'kalla']\n\n        II\n        >>> verb = WeakOldNorseVerb()\n        >>> verb.set_canonic_forms([\"m\u00e6la\", \"m\u00e6lti\", \"m\u00e6ltr\"])\n        >>> verb.present_active()\n        ['m\u00e6li', 'm\u00e6lir', 'm\u00e6lir', 'm\u00e6lum', 'm\u00e6li\u00f0', 'm\u00e6la']\n\n        III\n        >>> verb = WeakOldNorseVerb()\n        >>> verb.set_canonic_forms([\"telja\", \"taldi\", \"talinn\"])\n        >>> verb.present_active()\n        ['tel', 'telr', 'telr', 'teljum', 'teli\u00f0', 'telja']\n\n        IV\n        >>> verb = WeakOldNorseVerb()\n        >>> verb.set_canonic_forms([\"vaka\", \"vakti\", \"vakat\"])\n        >>> verb.present_active()\n        ['vaki', 'vakir', 'vakir', 'v\u00f6kum', 'vaki\u00f0', 'vaka']\n\n        :return:\n        \"\"\"\n        forms = []\n        stem_ending_by_j = self.sng[-1] == \"a\" and self.sng[-2] == \"j\"\n        stem_ending_by_v = self.sng[-1] == \"a\" and self.sng[-2] == \"v\"\n        stem = self.sng[:-1] if self.sng[-1] == \"a\" else self.sng\n        if stem_ending_by_j or stem_ending_by_v:\n            stem = stem[:-1]\n\n        if self.subclass == 1:\n            if stem_ending_by_v:\n                forms.append(stem+\"va\")\n                forms.append(stem + \"r\")\n                forms.append(stem + \"r\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"vi\u00f0\")\n                forms.append(stem+\"va\")\n            elif stem_ending_by_j:\n                forms.append(stem+\"ja\")\n                forms.append(stem + \"r\")\n                forms.append(stem + \"r\")\n                forms.append(apply_u_umlaut(stem) + \"jum\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(stem+\"ja\")\n            else:\n                forms.append(stem+\"a\")\n                forms.append(stem + \"ar\")\n                forms.append(stem + \"ar\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n\n        elif self.subclass == 2:\n            if stem_ending_by_v:\n                forms.append(stem + \"vi\")\n                forms.append(stem + \"vir\")\n                forms.append(stem + \"vir\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"vi\u00f0\")\n                forms.append(self.sng)\n\n            elif stem_ending_by_j:\n                forms.append(stem + \"i\")\n                forms.append(stem + \"ir\")\n                forms.append(stem + \"ir\")\n                forms.append(apply_u_umlaut(stem) + \"jum\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n\n            else:\n                forms.append(stem + \"i\")\n                forms.append(stem + \"ir\")\n                forms.append(stem + \"ir\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n\n        elif self.subclass == 3:\n            if stem_ending_by_v:\n                forms.append(stem)\n                forms.append(stem + \"r\")\n                forms.append(stem + \"r\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"vi\u00f0\")\n                forms.append(self.sng)\n\n            elif stem_ending_by_j:\n                forms.append(stem)\n                forms.append(stem + \"r\")\n                forms.append(stem + \"r\")\n                forms.append(apply_u_umlaut(stem) + \"jum\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n\n            else:\n                forms.append(stem)\n                forms.append(stem + \"r\")\n                forms.append(stem + \"r\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n\n        elif self.subclass == 4:\n\n            if stem_ending_by_v:\n                forms.append(stem + \"vi\")\n                forms.append(stem + \"vir\")\n                forms.append(stem + \"vir\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"vi\u00f0\")\n                forms.append(self.sng)\n\n            elif stem_ending_by_j:\n                forms.append(stem + \"i\")\n                forms.append(stem + \"ir\")\n                forms.append(stem + \"ir\")\n                forms.append(apply_u_umlaut(stem) + \"jum\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n\n            else:\n                forms.append(stem + \"i\")\n                forms.append(stem + \"ir\")\n                forms.append(stem + \"ir\")\n                forms.append(apply_u_umlaut(stem) + \"um\")  # apply u umlaut\n                forms.append(stem + \"i\u00f0\")\n                forms.append(self.sng)\n        return forms"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef past_active(self):\n        forms = []\n        stem = self.sfg3et[:-1]\n        forms.append(stem+\"a\")\n        forms.append(self.sfg3et+\"r\")\n        forms.append(self.sfg3et)\n        forms.append(apply_u_umlaut(stem)+\"um\")\n        forms.append(apply_u_umlaut(stem)+\"u\u00f0\")\n        forms.append(apply_u_umlaut(stem)+\"u\")\n        return forms", "response": "Return a new version of the current base set of canonic forms."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef present_active_subjunctive(self):\n        subjunctive_root = self.sng[:-1] if self.sng[-1] == \"a\" else self.sng\n        forms = [subjunctive_root + \"a\"]\n\n        subjunctive_root = subjunctive_root[:-1] if subjunctive_root[-1] == \"j\" else subjunctive_root\n        forms.append(subjunctive_root+\"ir\")\n        forms.append(subjunctive_root+\"i\")\n        forms.append(subjunctive_root+\"im\")\n        forms.append(subjunctive_root+\"i\u00f0\")\n        forms.append(subjunctive_root+\"i\")\n        return forms", "response": "Presents the active subjunctive entries in the current species."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decline_strong_masculine_noun(ns: str, gs: str, np: str):\n    np_syl = s.syllabify_ssp(np)\n\n    last_np_syl = np_syl[-1]\n\n    if last_np_syl.endswith(\"ar\"):\n        # a-stem\n        common_stem = extract_common_stem(ns, gs, np)\n\n        # nominative singular\n        print(ns)\n\n        # accusative singular\n        print(common_stem)\n\n        # dative singular\n        if np[len(common_stem):][0] == \"v\":\n            print(common_stem + \"vi\")\n        else:\n            print(common_stem + \"i\")\n\n        # genitive singular\n        print(gs)\n\n        # nominative plural\n        print(np)\n\n        # accusative plural\n        if last_np_syl.endswith(\"ar\"):\n            print(np[:-1])\n\n        elif last_np_syl.endswith(\"ir\"):\n            print(np[:-1])\n\n        # dative plural\n        if np[len(common_stem):][0] == \"v\":\n            print(apply_u_umlaut(common_stem) + \"vum\")\n\n        elif np[len(common_stem):][0] == \"j\":\n            print(apply_u_umlaut(common_stem) + \"jum\")\n        else:\n            print(apply_u_umlaut(common_stem) + \"um\")\n\n        # genitive plural\n        if np[len(common_stem):][0] == \"v\":\n            print(common_stem + \"va\")\n        elif np[len(common_stem):][0] == \"j\":\n            print(common_stem + \"ja\")\n        else:\n            print(common_stem + \"a\")\n\n    elif last_np_syl.endswith(\"ir\"):\n        # if has_u_umlaut(ns):\n        #     # u-stem\n        #     common_stem = ns[:-1]\n        #\n        #     # nominative singular\n        #     print(ns)\n        #\n        #     # accusative singular\n        #     print(common_stem)\n        #\n        #     # dative singular\n        #     if np[len(common_stem):][0] == \"v\":\n        #         print(common_stem + \"vi\")\n        #     else:\n        #         print(common_stem + \"i\")\n        #\n        #     # genitive singular\n        #     print(gs)\n        #\n        #     common_stem_p = np[:-2]\n        #     # nominative plural\n        #     print(np)\n        #\n        #     # accusative plural\n        #     print(apply_u_umlaut(common_stem_p)+\"u\")\n        #\n        #     # dative plural\n        #     if np[len(common_stem):][0] == \"v\":\n        #         print(apply_u_umlaut(common_stem_p) + \"vum\")\n        #\n        #     elif np[len(common_stem):][0] == \"j\":\n        #         print(apply_u_umlaut(common_stem_p) + \"jum\")\n        #     else:\n        #         print(apply_u_umlaut(common_stem_p) + \"um\")\n        #\n        #     # genitive plural\n        #     if np[len(common_stem):][0] == \"v\":\n        #         print(common_stem_p + \"va\")\n        #     elif np[len(common_stem):][0] == \"j\":\n        #         print(common_stem_p + \"ja\")\n        #     else:\n        #         print(common_stem_p + \"a\")\n        # else:\n\n        # i-stem\n        common_stem = extract_common_stem(ns, gs, np)\n\n        # nominative singular\n        print(ns)\n\n        # accusative singular\n        print(common_stem)\n\n        # dative singular\n        if np[len(common_stem):][0] == \"v\":\n            print(common_stem + \"vi\")\n        else:\n            print(common_stem + \"i\")\n\n        # genitive singular\n        print(gs)\n\n        # nominative plural\n        print(np)\n\n        # accusative plural\n        print(np[:-1])\n\n        # dative plural\n        if np[len(common_stem):][0] == \"v\":\n            print(apply_u_umlaut(common_stem) + \"vum\")\n\n        elif np[len(common_stem):][0] == \"j\":\n            print(apply_u_umlaut(common_stem) + \"jum\")\n        else:\n            print(apply_u_umlaut(common_stem) + \"um\")\n\n        # genitive plural\n        if np[len(common_stem):][0] == \"v\":\n            print(common_stem + \"va\")\n        elif np[len(common_stem):][0] == \"j\":\n            print(common_stem + \"ja\")\n        else:\n            print(common_stem + \"a\")", "response": "Returns the full declension of strong masculine nouns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the full declension of strong feminine nouns.", "response": "def decline_strong_feminine_noun(ns: str, gs: str, np: str):\n    \"\"\"\n    Gives the full declension of strong feminine nouns.\n\n    o macron-stem\n    Most of strong feminine nouns follows the declension of r\u00fan and f\u00f6r.\n    >>> decline_strong_feminine_noun(\"r\u00fan\", \"r\u00fanar\", \"r\u00fanar\")\n    r\u00fan\n    r\u00fan\n    r\u00fan\n    r\u00fanar\n    r\u00fanar\n    r\u00fanar\n    r\u00fanum\n    r\u00fana\n\n    >>> decline_strong_feminine_noun(\"f\u00f6r\", \"farar\", \"farar\")\n    f\u00f6r\n    f\u00f6r\n    f\u00f6r\n    farar\n    farar\n    farar\n    f\u00f6rum\n    fara\n\n    >>> decline_strong_feminine_noun(\"kerling\", \"kerlingar\", \"kerlingar\")\n    kerling\n    kerling\n    kerlingu\n    kerlingar\n    kerlingar\n    kerlingar\n    kerlingum\n    kerlinga\n\n    >>> decline_strong_feminine_noun(\"skel\", \"skeljar\", \"skeljar\")\n    skel\n    skel\n    skel\n    skeljar\n    skeljar\n    skeljar\n    skeljum\n    skelja\n\n    >>> decline_strong_feminine_noun(\"\u00f6r\", \"\u00f6rvar\", \"\u00f6rvar\")\n    \u00f6r\n    \u00f6r\n    \u00f6r\n    \u00f6rvar\n    \u00f6rvar\n    \u00f6rvar\n    \u00f6rum\n    \u00f6rva\n\n    >>> decline_strong_feminine_noun(\"hei\u00f0r\", \"hei\u00f0ar\", \"hei\u00f0ar\")\n    hei\u00f0r\n    hei\u00f0i\n    hei\u00f0i\n    hei\u00f0ar\n    hei\u00f0ar\n    hei\u00f0ar\n    hei\u00f0um\n    hei\u00f0a\n\n    i-stem\n\n    >>> decline_strong_feminine_noun(\"\u00f6xl\", \"axlar\", \"axlir\")\n    \u00f6xl\n    \u00f6xl\n    \u00f6xl\n    axlar\n    axlir\n    axlir\n    \u00f6xlum\n    axla\n\n    >>> decline_strong_feminine_noun(\"h\u00f6fn\", \"hafnar\", \"hafnir\")\n    h\u00f6fn\n    h\u00f6fn\n    h\u00f6fn\n    hafnar\n    hafnir\n    hafnir\n    h\u00f6fnum\n    hafna\n\n    >>> decline_strong_feminine_noun(\"norn\", \"nornar\", \"nornir\")\n    norn\n    norn\n    norn\n    nornar\n    nornir\n    nornir\n    nornum\n    norna\n\n    >>> decline_strong_feminine_noun(\"j\u00f6r\u00f0\", \"jar\u00f0ar\", \"jar\u00f0ir\")\n    j\u00f6r\u00f0\n    j\u00f6r\u00f0\n    j\u00f6r\u00f0\n    jar\u00f0ar\n    jar\u00f0ir\n    jar\u00f0ir\n    j\u00f6r\u00f0um\n    jar\u00f0a\n\n    >>> decline_strong_feminine_noun(\"borg\", \"borgar\", \"borgir\")\n    borg\n    borg\n    borgu\n    borgar\n    borgir\n    borgir\n    borgum\n    borga\n\n    :param ns: nominative singular\n    :param gs: genitive singular\n    :param np: nominative plural\n    :return:\n    \"\"\"\n\n    # nominative singular\n    print(ns)\n\n    # accusative singular\n    if len(ns) > 2 and ns[-1] == \"r\" and ns[-2] in CONSONANTS:\n        print(ns[:-1]+\"i\")\n    else:\n        print(ns)\n\n    # dative singular\n    if len(ns) > 2 and ns[-1] == \"r\" and ns[-2] in CONSONANTS:\n        print(ns[:-1]+\"i\")\n    elif ns.endswith(\"ing\") or ns.endswith(\"rg\"):\n        print(ns + \"u\")\n    else:\n        print(ns)\n\n    # genitive singular\n    print(gs)\n\n    # nominative plural\n    print(np)\n\n    # accusative plural\n    print(np)\n\n    # dative plural\n    # print(\"dative plural \"+np[len(np[:-3]):][0])\n    if np[len(np[:-3]):][0] == \"v\":\n        print(apply_u_umlaut(np[:-2])[:-1]+\"um\")\n    elif np[len(np[:-3]):][0] == \"j\":\n        print(apply_u_umlaut(np[:-2])+\"um\")\n    else:\n        print(apply_u_umlaut(np[:-2])+\"um\")\n\n    # genitive plural\n    print(np[:-2]+\"a\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decline_strong_neuter_noun(ns: str, gs: str, np: str):\n\n    # nominative singular\n    print(ns)\n\n    # accusative singular\n    print(ns)\n\n    # dative singular\n    if ns[-1] == \"i\":\n        print(ns)\n    # TODO  +\"vi\"\n    else:\n        print(ns+\"i\")\n\n    # genitive singular\n    print(gs)\n\n    # nominative plural\n    print(np)\n\n    # accusative plural\n    print(np)\n\n    # dative plural\n    if ns[-1] in CONSONANTS:\n        print(apply_u_umlaut(np)+\"um\")\n    else:\n        print(apply_u_umlaut(np[:-1]) + \"um\")\n    # TODO +\"vum\"\n\n    # genitive plural\n    if ns[-1] in CONSONANTS:\n        print(ns+\"a\")\n    # TODO + \"va\"\n    else:\n        print(ns[:-1]+\"a\")", "response": "This function declines the full declension of strong neuter nouns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decline_weak_masculine_noun(ns: str, gs: str, np: str):\n    # nominative singular\n    print(ns)\n\n    # accusative singular\n    print(gs)\n\n    # dative singular\n    print(gs)\n\n    # genitive singular\n    print(gs)\n\n    # nominative plural\n    print(np)\n\n    # accusative plural\n    print(np[:-1])\n\n    # dative plural\n    if len(np) > 3 and np[-3] == \"v\":\n        print(apply_u_umlaut(np[:-3]) + \"um\")\n    else:\n        print(apply_u_umlaut(np[:-2]) + \"um\")\n\n    # genitive plural\n    print(np[:-1])", "response": "Returns the full declension of weak masculine nouns."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decline_weak_feminine_noun(ns: str, gs: str, np: str):\n\n    if ns[-1] == \"i\" and gs[-1] == \"i\" and not np:\n        print(ns)\n        print(ns)\n        print(ns)\n        print(ns)\n    else:\n\n        # nominative singular\n        print(ns)\n\n        # accusative singular\n        print(gs)\n\n        # dative singular\n        print(gs)\n\n        # genitive singular\n        print(gs)\n\n        # nominative plural\n        print(np)\n\n        # accusative plural\n        print(np)\n\n        # dative plural\n        print(np[:-1]+\"m\")\n\n        # genitive plural\n        if ns == \"kona\":\n            print(\"kvenna\")\n        elif ns[-2] == \"v\" or ns[-2] == \"j\":\n            print(ns[:-2]+\"na\")\n        else:\n            print(ns[:-1]+\"na\")", "response": "Returns the full declension of weak feminine nouns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the full declension of weak neuter nouns.", "response": "def decline_weak_neuter_noun(ns: str, gs: str, np: str):\n    \"\"\"\n    Gives the full declension of weak neuter nouns.\n\n    >>> decline_weak_neuter_noun(\"auga\", \"auga\", \"augu\")\n    auga\n    auga\n    auga\n    auga\n    augu\n    augu\n    augum\n    augna\n\n    >>> decline_weak_neuter_noun(\"hjarta\", \"hjarta\", \"hj\u00f6rtu\")\n    hjarta\n    hjarta\n    hjarta\n    hjarta\n    hj\u00f6rtu\n    hj\u00f6rtu\n    hj\u00f6rtum\n    hjartna\n\n    >>> decline_weak_neuter_noun(\"lunga\", \"lunga\", \"lungu\")\n    lunga\n    lunga\n    lunga\n    lunga\n    lungu\n    lungu\n    lungum\n    lungna\n\n    >>> decline_weak_neuter_noun(\"eyra\", \"eyra\", \"eyru\")\n    eyra\n    eyra\n    eyra\n    eyra\n    eyru\n    eyru\n    eyrum\n    eyrna\n\n    The main pattern is:\n    -a\n    -a\n    -a\n    -a\n    -u\n    -u\n    -um\n    -na\n\n    :param ns: nominative singular\n    :param gs: genitive singular\n    :param np: nominative plural\n    :return:\n    \"\"\"\n    # nominative singular\n    print(ns)\n\n    # accusative singular\n    print(ns)\n\n    # dative singular\n    print(ns)\n\n    # genitive singular\n    print(gs)\n\n    # nominative plural\n    print(np)\n\n    # accusative plural\n    print(np)\n\n    # dative plural\n    print(np+\"m\")\n\n    # genitive plural\n    print(ns[:-1]+\"na\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_representative_cases(self, ns, gs, np):\n        self.set_void_declension(decl_utils.Number, decl_utils.Case)\n        self.declension[decl_utils.Number.singular.value-1][decl_utils.Case.nominative.value-1] = ns\n        self.declension[decl_utils.Number.singular.value-1][decl_utils.Case.genitive.value-1] = gs\n        self.declension[decl_utils.Number.plural.value-1][decl_utils.Case.nominative.value-1] = np", "response": "Set the declension of the current species to represent the given numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_representative_cases(self):\n        return (self.get_declined(decl_utils.Case.nominative, decl_utils.Number.singular),\n                self.get_declined(decl_utils.Case.genitive, decl_utils.Number.singular),\n                self.get_declined(decl_utils.Case.nominative, decl_utils.Number.plural))", "response": "Get the representative cases for this locale."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare_sentences(self, str_a, str_b, language):\n\n        sents_a = []\n        sents_b = []\n        ratios = []\n\n        # Make the latin tokenizer\n        if language == \"latin\":\n            sent_tokenizer = TokenizeSentence('latin')\n\n        # Make the greek tokenizer\n        elif language == \"greek\":\n            sent_tokenizer = TokenizeSentence('greek')\n\n        # Otherwise, if language, is unsupported, throw error stating accepted Language\n        # values that may be used to tokenize sentences\n        else:\n            print(\"Language for sentence tokenization not recognized. \"\n                  \"Accepted values are 'latin' and 'greek'.\")\n            return\n\n        # If class instance is set to stem words, do so\n        if self.stem_words:\n            stemmer = Stemmer()\n            str_a = stemmer.stem(str_a)\n            str_b = stemmer.stem(str_b)\n\n        # Tokenize input strings\n        sents_a = sent_tokenizer.tokenize_sentences(str_a)\n        sents_b = sent_tokenizer.tokenize_sentences(str_b)\n\n        # Process sentences for comparison (taking into account sanitization settings)\n        sents_a = self._process_sentences(sents_a)\n        sents_b = self._process_sentences(sents_b)\n\n        # Build matrix of edit distance ratios\n        comparisons = self._calculate_ratios(sents_a, sents_b)\n\n        return comparisons", "response": "Tokenize two input strings on sentence boundary and return a matrix of Levenshtein distance ratios."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare two strings with a sliding window method based on window_length and curse_forward values.", "response": "def compare_sliding_window(self, str_a, str_b, window_length=50, curse_forward=20):\n        \"\"\"\n        Compare two strings with a sliding window method based on window_length and curse_forward values\n        :param string_a: str\n        :param string_b: str\n        :param window_length: int\n        :param curse_forward: int\n        :return: list [[Comparison]]\n        \"\"\"\n\n        if self.stem_words:\n            stemmer = Stemmer()\n            str_a = stemmer.stem(str_a)\n            str_b = stemmer.stem(str_b)\n\n        substrs_a = self._str_to_windows(str_a, window_length, curse_forward)\n        substrs_b = self._str_to_windows(str_b, window_length, curse_forward)\n\n        # Build\n        comparisons = self._calculate_ratios(substrs_a, substrs_b)\n\n        return comparisons"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the ratio of the two lists of strings in a class and returns a list of list of comparison objects", "response": "def _calculate_ratios(self, list_a, list_b):\n        \"\"\"\n        Calulate a matrix of string comparisons given two input lists\n        :param list_a: list [object]\n        :param list_b: list [object]\n        :return: list [[Comparison]]\n        \"\"\"\n\n        comparisons = []\n        l = Levenshtein()\n\n        # For all strings in list a\n        for i, str_a in enumerate(list_a):\n\n            # Add a new list to our list of lists of comparisons\n            comparisons.append([])\n\n            # Compare str_a to every string in list_b\n            for str_b in list_b:\n\n                # If the sanitize, input flag is set, make the ratio with the sanitized values\n                if self.sanitize_input:\n                    new_comparison = Comparison(\n                                            str_a['text'],\n                                            str_b['text'],\n                                            l.ratio(str_a['sanitized'], str_b['sanitized'])\n                                        )\n\n                # Otherwise, make the ratio with the original, unsanitize text strings\n                else:\n                    new_comparison = Comparison(\n                                            str_a['text'],\n                                            str_b['text'],\n                                            l.ratio(str_a['text'], str_b['text'])\n                                        )\n\n                # If text metadata is set on this class for text a or b, save that data with the\n                # comparison\n                if self.text_ref_a:\n                    new_comparison.set_ref_a(self.text_ref_a)\n                if self.text_ref_b:\n                    new_comparison.set_ref_b(self.text_ref_b)\n\n                # Finally, append the new comparison to the list of comparisons\n                comparisons[i].append(new_comparison)\n\n        return comparisons"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndivides an input string to a list of substrings based on window_length and curse_forward values", "response": "def _process_sentences(self, sents_list):\n        \"\"\"\n        Divide an input string to a list of substrings based on window_length and curse_forward values\n        :param sents_list: list [str]\n        :return: list [object]\n        \"\"\"\n        processed_sents = []\n\n        for sent in sents_list:\n            processed_sent = {\n                                'text' : sent\n                            }\n            # If the class is set to santize input before comparison, do so\n            if self.sanitize_input:\n                processed_sent['sanitized'] = self._sanitize(sent),\n\n            processed_sents.append(processed_sent)\n\n        return processed_sents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _str_to_windows(self, input_str, window_length, curse_forward):\n        windows = []\n\n        i = 0\n        len_input = len(input_str)\n        while i < len_input:\n            window_text = input_str[i:i+window_length]\n\n            if self.sanitize_input:\n                windows.append({\n                                    'sanitized' : self._sanitize(window_text),\n                                    'text' : window_text\n                                })\n            else:\n                windows.append({\n                                    'text' : window_text\n                                })\n\n            i = i + curse_forward\n\n\n        return windows", "response": "Divide an input string to a list of substrings based on window_length and curse_forward values\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sanitize(self, unsanitized):\n\n        sanitized = \"\"\n\n        # strip punctuation and diacritics\n        replace_punct = str.maketrans( string.punctuation + \"\u1fbd\u1fbf\u1ffe\", ' '*(len(string.punctuation) + 3)  )\n        sanitized = unsanitized.translate( replace_punct )\n        sanitized = \"\".join(c for c in unicodedata.normalize(\"NFD\", sanitized) if unicodedata.category(c) != \"Mn\")\n\n        #finally, lose all whitespace\n        sanitized = re.sub(r'\\s+','', sanitized)\n\n        return sanitized", "response": "Sanitize input string to optimize string comparison"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns dict of epithets to a set of all author ids of that epithet.", "response": "def get_epithet_index():\n    \"\"\"Return dict of epithets (key) to a set of all author ids of that\n    epithet (value).\n    \"\"\"\n    _dict = {}\n    for k, v in AUTHOR_EPITHET.items():\n        _dict[k] = set(v)\n    return _dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npasses exact name of epithet name return ordered set of author ids.", "response": "def select_authors_by_epithet(query):\n    \"\"\"Pass exact name (case insensitive) of epithet name, return ordered set\n    of author ids.\n    \"\"\"\n\n    for epithet, ids in AUTHOR_EPITHET.items():\n        if epithet.casefold() == query.casefold():\n            return set(ids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_geo_index():\n    _dict = {}\n    for k, v in AUTHOR_EPITHET.items():\n        _dict[k] = set(v)\n\n    return _dict", "response": "Get entire index of geographic name and set of associated authors\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npass exact name of geography name return ordered set of author ids.", "response": "def select_authors_by_geo(query):\n    \"\"\"Pass exact name (case insensitive) of geography name, return ordered set\n    of author ids.\n    \"\"\"\n    for geo, ids in AUTHOR_GEO.items():\n        if geo.casefold() == query.casefold():\n            return set(ids)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndo a case - insensitive regex match on author name returns TLG id.", "response": "def select_id_by_name(query):\n    \"\"\"Do a case-insensitive regex match on author name, returns TLG id.\"\"\"\n    id_author = get_id_author()\n    comp = regex.compile(r'{}'.format(query.casefold()), flags=regex.VERSION1)\n    matches = []\n    for _id, author in id_author.items():\n        match = comp.findall(author.casefold())\n        if match:\n            matches.append((_id, author))\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npass author id and return the name of its associated date.", "response": "def get_date_of_author(_id):\n    \"\"\"Pass author id and return the name of its associated date.\"\"\"\n    _dict = get_date_author()\n    for date, ids in _dict.items():\n        if _id in ids:\n            return date\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_epoch(_str):\n    _return = None\n    if _str.startswith('A.D. '):\n        _return = 'ad'\n    elif _str.startswith('a. A.D. '):\n        _return = None #?\n    elif _str.startswith('p. A.D. '):\n        _return = 'ad'\n    elif regex.match(r'^[0-9]+ B\\.C\\. *', _str):\n        _return = 'bc'\n    elif regex.match(r'^a\\. *[0-9]+ B\\.C\\. *', _str):\n        _return = 'bc'\n    elif regex.match(r'^p\\. *[0-9]+ B\\.C\\. *', _str):\n        _return = None  #?\n    elif _str == 'Incertum' or _str == 'Varia':\n        _return = _str\n    return _return", "response": "Take incoming string return its epoch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if incoming date has a - or - or -", "response": "def _handle_splits(_str):\n    \"\"\"Check if incoming date has a '-\" or '/', if so do stuff.\"\"\"\n    _str = _str.replace('/', '-')\n    _tmp_dict = {}\n\n    if '-' in _str:\n        start, stop = _str.split('-')\n        if _check_number(start):\n            start = regex.sub(r'[0-9]+\\?*', start, stop)\n        elif _check_number(stop):\n            stop = regex.sub(r'[0-9]+\\?*', stop, start)\n    else:\n        start = _str\n        stop = _str\n    _tmp_dict['start_raw'] = start\n    _tmp_dict['stop_raw'] = stop\n\n    _tmp_dict['start_epoch'] = _get_epoch(start)\n    _tmp_dict['stop_epoch'] = _get_epoch(stop)\n\n    return _tmp_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_dates():\n    _dict = get_date_author()\n    for tlg_date in _dict:\n        date = {}\n        if tlg_date == 'Varia':\n            #give a homer-to-byz date for 'varia'\n            pass\n        elif tlg_date == 'Incertum':\n            #?\n            pass\n        else:\n            tmp_date = _handle_splits(tlg_date)\n            date.update(tmp_date)\n\n        print(date)", "response": "Experiment to make sense of TLG dates.\n    TODO: start here, parse everything with pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tag_ner(lang, input_text, output_type=list):\n\n    _check_latest_data(lang)\n\n    assert lang in NER_DICT.keys(), \\\n        'Invalid language. Choose from: {}'.format(', '.join(NER_DICT.keys()))\n    types = [str, list]\n    assert type(input_text) in types, 'Input must be: {}.'.format(', '.join(types))\n    assert output_type in types, 'Output must be a {}.'.format(', '.join(types))\n\n    if type(input_text) == str:\n        punkt = PunktLanguageVars()\n        tokens = punkt.word_tokenize(input_text)\n        new_tokens = []\n        for word in tokens:\n            if word.endswith('.'):\n                new_tokens.append(word[:-1])\n                new_tokens.append('.')\n            else:\n                new_tokens.append(word)\n        input_text = new_tokens\n\n    ner_file_path = os.path.expanduser(NER_DICT[lang])\n    with open(ner_file_path) as file_open:\n        ner_str = file_open.read()\n    ner_list = ner_str.split('\\n')\n\n    ner_tuple_list = []\n    for count, word_token in enumerate(input_text):\n        match = False\n        for ner_word in ner_list:\n            # the replacer slows things down, but is necessary\n            if word_token == ner_word:\n                ner_tuple = (word_token, 'Entity')\n                ner_tuple_list.append(ner_tuple)\n                match = True\n                break\n        if not match:\n            ner_tuple_list.append((word_token,))\n\n    if output_type is str:\n        string = ''\n        for tup in ner_tuple_list:\n            start_space = ' '\n            final_space = ''\n            # this is some mediocre string reconstitution\n            # maybe not worth the effort\n            if tup[0] in [',', '.', ';', ':', '?', '!']:\n                start_space = ''\n            if len(tup) == 2:\n                string += start_space + tup[0] + '/' + tup[1] + final_space\n            else:\n                string += start_space + tup[0] + final_space\n        return string\n\n    return ner_tuple_list", "response": "Tag NER for chosen language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decline_noun(self, noun, gender, mimation=True):\n        stem = self.stemmer.get_stem(noun, gender)\n        declension = []\n        for case in self.endings[gender]['singular']:\n            if gender == 'm':\n                form = stem + self.endings[gender]['singular'][case]\n            else:\n                form = stem + self.endings[gender]['singular'][case][1:]\n            declension.append((form, {'case': case, 'number': 'singular'}))\n        for case in self.endings[gender]['dual']:\n            if gender == 'm':\n                form = stem + self.endings[gender]['dual'][case]\n            else:\n                form = stem + self.endings[gender]['dual'][case][1:]\n            declension.append((form, {'case': case, 'number': 'dual'}))\n        for case in self.endings[gender]['plural']:\n            if gender == 'm':\n                form = stem + self.endings[gender]['plural'][case]\n            else:\n                if stem[-3] in self.akkadian['macron_vowels']:\n                    theme_vowel = stem[-3]\n                else:\n                    theme_vowel = '\u0101'\n                ending = [x for x in self.endings[gender]['plural'][case] if x[0] == theme_vowel]\n                if stem[-2] in self.akkadian['short_vowels']:\n                    form = stem[:-2] + ending[0]\n                elif stem[-1] in self.akkadian['consonants'] and stem[-2] in self.akkadian['macron_vowels']:\n                    form = stem + ending[0]\n                else:\n                    form = stem[:-1] + ending[0]\n            declension.append((form, {'case': case, 'number': 'plural'}))\n        return declension", "response": "Return a list of all possible declined forms given any form\n         of a noun and its gender."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstemming each word of the French text.", "response": "def stem(text):\n    \"\"\"make string lower-case\"\"\"\n    text = text.lower()\n    \"\"\"Stem each word of the French text.\"\"\"\n\n    stemmed_text = ''\n\n    word_tokenizer = WordTokenizer('french')\n    tokenized_text = word_tokenizer.tokenize(text)\n    for word in tokenized_text:\n        \"\"\"remove the simple endings from the target word\"\"\"\n        word, was_stemmed = matchremove_noun_endings(word)\n        \"\"\"if word didn't match the simple endings, try verb endings\"\"\"\n        if not was_stemmed:\n            word = matchremove_verb_endings(word)\n        \"\"\"add the stemmed word to the text\"\"\"\n        stemmed_text += word + ' '\n    return stemmed_text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef matchremove_noun_endings(word):\n\n    was_stemmed = False\n\n    \"\"\"common and proper noun and adjective word endings sorted by charlen, then alph\"\"\"\n    noun_endings = ['arons', 'ains', 'aron', 'ment', 'ain', 'age', 'on', 'es', '\u00e9e', 'ee', 'ie', 's']\n\n    for ending in noun_endings:\n        \"\"\"ignore exceptions\"\"\"\n        if word in exceptions:\n            word = word\n            was_stemmed = True\n            break\n        if word == ending:\n            word = word\n            was_stemmed = True\n            break\n        \"\"\"removes noun endings\"\"\"\n        if word.endswith(ending):\n            word = re.sub(r'{0}$'.format(ending), '', word)\n            was_stemmed = True\n            break\n\n    return word, was_stemmed", "response": "Remove the noun and adverb word endings from the given word."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matchremove_verb_endings(word):\n    \"\"\"verb endings sorted by charlen then alph\"\"\"\n    verb_endings =['issiiens', 'isseient', 'issiiez', 'issons', 'issent', 'issant', 'isseie', 'isseit', 'issons',\n                   'isseiz', 'assent', 'issons', 'isseiz', 'issent', 'iiens', 'eient', 'issez', 'oient', 'istes',\n                   '\u00efstes', 'istes', 'astes', 'erent', 'istes', 'irent', 'ustes', 'urent', 'a\u0302mes', 'a\u0302tes', '\u00e8rent',\n                   'asses', 'isses', 'issez', 'ssons', 'sseiz', 'ssent', 'erent', 'eies', 'iiez', 'oies', 'iens',\n                   'ions', 'oint', 'eret', 'imes', 'rent', '\u00fcmes', '\u00fctes', '\u00efmes', 'imes', 'asse', 'isse', 'usse',\n                   'ames', 'imes', 'umes', 'asse', 'isse', 'sses', 'ssez', 'ons', 'ent', 'ant', 'eie', 'eit', 'int',\n                   'ist', 'eiz', 'oie', 'oit', 'iez', 'ois', 'oit', 'iez', 'res', 'ert', 'ast', 'ist', 'sse', 'mes', 'er',\n                   'es', 'et', 'ez', 'is', 're', 'oi', '\u00efs', '\u00fcs', 'ai', 'as', 'at', 'is', 'it', 'ui',\n                   'us', 'ut', 'st', 's', 't', 'e', '\u00e9', 'z', 'u', 'a', 'i']\n\n    for ending in verb_endings:\n        if word == ending:\n            word = word\n            break\n        if word.endswith(ending):\n            word = re.sub(r'{0}$'.format(ending), '', word)\n            break\n\n    return word", "response": "Remove the verb endings from the given word"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstem a word into a list of words.", "response": "def stem_helper(word, rem_umlaut = True):\n\t\"\"\"rem_umlat: Remove umlaut from text\"\"\"\n\t\n\t#Define R1 and R2 regions\n\t\n\t#R1 is defined as the region after the first consonant followed by a vowel\n\t\n\ttry:\n\t\tR1 = list(re.finditer(r\"[a\u00ebeiou\u00e4\u00f6\u00fc\u00e2\u00ea\u00ee\u00f4\u00fb\u00e6\u0153][bdghfcjklmnspqrtvwz]\",word))[0].start() + 2\n\texcept:\n\t\tR1 = len(word)\n\t\t\n\t#R2 is defined as the region within R1 after the first consonant followed by a vowel\n\t\n\ttry:\n\t\tR2 = list(re.finditer(r\"[a\u00ebeiou\u00e4\u00f6\u00fc\u00e2\u00ea\u00ee\u00f4\u00fb\u00e6\u0153][bdghfcjklmnspqrtvwz]\",word[R1:]))[0].start() + 2 + R1\n\texcept:\n\t\tR2 = len(word)\n\t\t\n\t#Make sure the index of R1 is at least 3. \n\t\n\tif R1<3:\n\t\ttry:\n\t\t\tR1 = list(re.finditer(r\"[a\u00ebeiou\u00e4\u00f6\u00fc\u00e2\u00ea\u00ee\u00f4\u00fb\u00e6\u0153][bdghfcjklmnspqrtvwz]\",word[1:]))[0].start() + 2\n\t\texcept:\n\t\t\tR1 = len(word)\n\t\n\tif rem_umlaut:\n\t\tword = remove_umlaut(word)\n\t\n\tword = word[:R1] + re.sub(r'(wes|wen|est|ern|em|en|er|es|e\u0225(?=[klmrt])s|(?=[lr])n|e)$',\"\",word[R1:])\n\tword = word[:R1] + re.sub(r'(est|er|en|re|in|iu|(?=.{3})st,word[R1:])$',\"\",word[R1:])\n\tword = word[:R2] + re.sub(r'(lich?.?.|keit|inc|isch?.?.)$',\"\",word[R2:])\n\t\n\treturn word"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stemmer_middle_high_german(text_l, rem_umlauts = True, exceptions = exc_dict):\n\t\n\t#Normalize text\n\ttext_l = normalize_middle_high_german(text_l, to_lower_all = False, to_lower_beginning = True)\n\t\n\t#Tokenize text\n\tword_tokenizer = WordTokenizer(\"middle_high_german\")\n\ttext_l = word_tokenizer.tokenize(text_l)\n\ttext = []\n\n\t\n\tfor word in text_l:\n\t\ttry:\n\t\t\ttext.append(exceptions[word]) #test if word in exception dictionary\n\t\t\t\n\t\texcept:\n\t\t\tif word[0].isupper():\n\t\t\t\t#MHG only uses upper case for locations, people, etc. So any word that starts with a capital\n\t\t\t\t#letter while not being at the start of a sentence will automatically be excluded.\n\t\t\t\ttext.append(word)\n\t\t\t\t\n\t\t\telif word in MHG_STOPS: \n\t\t\t\ttext.append(word) #Filter stop words\n\t\t\t\t\n\t\t\telse:\n\t\t\t\ttext.append(stem_helper(word, rem_umlaut = rem_umlauts))\n\treturn text", "response": "This function is used to stemmer a middle - high - germann text."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform_i_to_j(self, line: str) -> str:\n\n        words = line.split(\" \")\n        space_list = string_utils.space_list(line)\n        corrected_words = []\n        for word in words:\n            found = False\n            for prefix in self.constants.PREFIXES:\n                if word.startswith(prefix) and word != prefix:\n                    corrected_words.append(self.syllabifier.convert_consonantal_i(prefix))\n                    corrected_words.append(\n                        self.syllabifier.convert_consonantal_i(word[len(prefix):]))\n                    found = True\n                    break\n            if not found:\n                corrected_words.append(self.syllabifier.convert_consonantal_i(word))\n        new_line = string_utils.join_syllables_spaces(corrected_words, space_list)\n        char_list = string_utils.overwrite(list(new_line),\n                                          r\"\\b[i\u012b][{}]\".format(\n                                              self.constants.VOWELS + self.constants.ACCENTED_VOWELS),\n                                          \"j\")\n        char_list = string_utils.overwrite(char_list,\n                                          r\"\\b[I][{}]\".format(self.constants.VOWELS_WO_I),\n                                          \"J\")\n        char_list = string_utils.overwrite(char_list, r\"[{}][i][{}]\".format(\n            self.constants.VOWELS_WO_I, self.constants.VOWELS),\n                                          \"j\", 1)\n        return \"\".join(char_list)", "response": "Transform instances of consonantal i to j"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform_i_to_j_optional(self, line: str) -> str:\n        words = line.split(\" \")\n        space_list = string_utils.space_list(line)\n        corrected_words = []\n        for word in words:\n            found = False\n            for prefix in self.constants.PREFIXES:\n                if word.startswith(prefix) and word != prefix:\n                    corrected_words.append(self.syllabifier.convert_consonantal_i(prefix))\n                    corrected_words.append(\n                        self.syllabifier.convert_consonantal_i(word[len(prefix):]))\n                    found = True\n                    break\n            if not found:\n                corrected_words.append(self.syllabifier.convert_consonantal_i(word))\n        new_line = string_utils.join_syllables_spaces(corrected_words, space_list)\n        #  the following two may be tunable and subject to improvement\n        char_list = string_utils.overwrite(list(new_line),\n                                          \"[bcdfgjkmpqrstvwxzBCDFGHJKMPQRSTVWXZ][i][{}]\".format(\n                                              self.constants.VOWELS_WO_I),\n                                          \"j\", 1)\n        char_list = string_utils.overwrite(char_list,\n                                          \"[{}][iI][{}]\".format(self.constants.LIQUIDS,\n                                                                self.constants.VOWELS_WO_I),\n                                          \"j\", 1)\n        return \"\".join(char_list)", "response": "This function transforms the i to j line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef accent_by_position(self, verse_line: str) -> str:\n        line = verse_line.translate(self.punctuation_substitutions)\n        line = self.transform_i_to_j(line)\n        marks = list(line)\n\n        # locate and save dipthong positions since we don't want them being accented\n        dipthong_positions = []\n        for dipth in self.constants.DIPTHONGS:\n            if dipth in line:\n                dipthong_positions.append(line.find(dipth))\n\n        # Vowels followed by 2 consonants\n        # The digraphs ch, ph, th, qu and sometimes gu and su count as single consonants.\n        # see http://people.virginia.edu/~jdk3t/epicintrog/scansion.htm\n        marks = string_utils.overwrite(marks, \"[{}][{}][{}]\".format(\n            self.constants.VOWELS,\n            self.constants.CONSONANTS,\n            self.constants.CONSONANTS_WO_H),\n                                      self.constants.STRESSED)\n        # one space (or more for 'dropped' punctuation may intervene)\n        marks = string_utils.overwrite(marks,\n                                      r\"[{}][{}]\\s*[{}]\".format(\n                                          self.constants.VOWELS,\n                                          self.constants.CONSONANTS,\n                                          self.constants.CONSONANTS_WO_H),\n                                      self.constants.STRESSED)\n        # ... if both consonants are in the next word, the vowel may be long\n        # .... but it could be short if the vowel is not on the thesis/emphatic part of the foot\n        # ... see Gildersleeve and Lodge p.446\n        marks = string_utils.overwrite(marks,\n                                      r\"[{}]\\s*[{}][{}]\".format(\n                                          self.constants.VOWELS,\n                                          self.constants.CONSONANTS,\n                                          self.constants.CONSONANTS_WO_H),\n                                      self.constants.STRESSED)\n        #  x is considered as two letters\n        marks = string_utils.overwrite(marks,\n                                      \"[{}][xX]\".format(self.constants.VOWELS),\n                                      self.constants.STRESSED)\n        #  z is considered as two letters\n        marks = string_utils.overwrite(marks,\n                                      r\"[{}][zZ]\".format(self.constants.VOWELS),\n                                      self.constants.STRESSED)\n        original_verse = list(line)\n        for idx, word in enumerate(original_verse):\n            if marks[idx] == self.constants.STRESSED:\n                original_verse[idx] = self.constants.VOWELS_TO_ACCENTS[original_verse[idx]]\n        # make sure dipthongs aren't accented\n        for idx in dipthong_positions:\n            if original_verse[idx + 1] in self.constants.ACCENTS_TO_VOWELS:\n                original_verse[idx + 1] = self.constants.ACCENTS_TO_VOWELS[original_verse[idx + 1]]\n\n        return \"\".join(original_verse)", "response": "Accent vowels according to the rules of scansion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nerasing all syllables in a given line and returns a list of syllables that would disappear according to the rules of elision.", "response": "def elide_all(self, line: str) -> str:\n        \"\"\"\n        Given a string of space separated syllables, erase with spaces the syllable portions\n        that would disappear according to the rules of elision.\n\n        :param line:\n        :return:\n        \"\"\"\n        marks = list(line.translate(self.remove_punct_map))\n        all_vowels = self.constants.VOWELS + self.constants.ACCENTED_VOWELS\n        tmp = \"\".join(marks)\n        # Elision rules are compound but not cummulative: we place all elision edits into a list\n        #  of candidates, and then merge, taking the least of each section of the line.\n        candidates = [tmp, self.elide(tmp, r\"[{}][{}]\\s+[{}]\".format(self.constants.CONSONANTS,\n                                                                     all_vowels, all_vowels), 1, 1),\n                      self.elide(tmp,\n                                 r\"[{}][{}]\\s+[hH]\".format(self.constants.CONSONANTS, all_vowels),\n                                 1, 1), self.elide(tmp, r\"[a\u0101u\u016b]m\\s+[{}]\".format(all_vowels), 2),\n                      self.elide(tmp, r\"ae\\s+[{}]\".format(all_vowels), 2),\n                      self.elide(tmp, r\"[{}]\\s+[{}]\".format(all_vowels, all_vowels), 1),\n                      self.elide(tmp, r\"[u\u016b]m\\s+h\", 2)]\n        results = string_utils.merge_elisions(candidates)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_offset(self, syllables_spaces: List[str]) -> Dict[int, int]:\n        line = string_utils.flatten(syllables_spaces)\n        mydict = {} # type: Dict[int, int]\n        # #defaultdict(int) #type: Dict[int, int]\n        for idx, syl in enumerate(syllables_spaces):\n            target_syllable = syllables_spaces[idx]\n            skip_qu = string_utils.starts_with_qu(target_syllable)\n            matches = list(self.syllable_matcher.finditer(target_syllable))\n            for position, possible in enumerate(matches):\n                if skip_qu:\n                    skip_qu = False\n                    continue\n                (start, end) = possible.span()\n                if target_syllable[start:end] in \\\n                        self.constants.VOWELS + self.constants.ACCENTED_VOWELS:\n                    part = line[:len(\"\".join(syllables_spaces[:idx]))]\n                    offset = len(part) + start\n                    if line[offset] not in self.constants.VOWELS + self.constants.ACCENTED_VOWELS:\n                        LOG.error(\"Problem at line {} offset {}\".format(line, offset))\n                    mydict[idx] = offset\n        return mydict", "response": "Calculates a dictionary of accent positions from a list of syllables with spaces."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef produce_scansion(self, stresses: list, syllables_wspaces: List[str],\n                         offset_map: Dict[int, int]) -> str:\n        \"\"\"\n        Create a scansion string that has stressed and unstressed syllable positions in locations\n        that correspond with the original texts syllable vowels.\n\n        :param stresses list of syllable positions\n        :param syllables_wspaces list of syllables with spaces escaped for punctuation or elision\n        :param offset_map dictionary of syllable positions, and an offset amount which is the\n        number of spaces to skip in the original line before inserting the accent.\n        \"\"\"\n        scansion = list(\" \" * len(string_utils.flatten(syllables_wspaces)))\n        unstresses = string_utils.get_unstresses(stresses, len(syllables_wspaces))\n        try:\n            for idx in unstresses:\n                location = offset_map.get(idx)\n                if location is not None:\n                    scansion[location] = self.constants.UNSTRESSED\n            for idx in stresses:\n                location = offset_map.get(idx)\n                if location is not None:\n                    scansion[location] = self.constants.STRESSED\n        except Exception as e:\n            LOG.error(\"problem with syllables; check syllabification {}, {}\".format(\n                syllables_wspaces, e))\n        return \"\".join(scansion)", "response": "Create a scansion string that has stressed and unstressed syllable positions in locations\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flag_dipthongs(self, syllables: List[str]) -> List[int]:\n        long_positions = []\n        for idx, syl in enumerate(syllables):\n            for dipthong in self.constants.DIPTHONGS:\n                if dipthong in syllables[idx]:\n                    if not string_utils.starts_with_qu(syllables[idx]):\n                        long_positions.append(idx)\n        return long_positions", "response": "Returns a list of syllables that contain a dipthong."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nerases a section of a line matching a regular expression and pushing in a quantity of blank spaces and jumping forward with an offset if necessary.", "response": "def elide(self, line: str, regexp: str, quantity: int = 1, offset: int = 0) -> str:\n        \"\"\"\n        Erase a section of a line, matching on a regex, pushing in a quantity of blank spaces,\n        and jumping forward with an offset if necessary.\n        If the elided vowel was strong, the vowel merged with takes on the stress.\n\n        :param line:\n        :param regexp:\n        :param quantity:\n        :param offset:\n        :return:\n\n        >>> print(VerseScanner().elide(\"uvae avaritia\", r\"[e]\\s*[a]\"))\n        uv   \u0101varitia\n        >>> print(VerseScanner().elide(\"mare avaritia\", r\"[e]\\s*[a]\"))\n        mar  avaritia\n        \"\"\"\n        matcher = re.compile(regexp)\n        positions = matcher.finditer(line)\n        new_line = line\n        for match in positions:\n            (start, end) = match.span()  # pylint: disable=unused-variable\n            if (start > 0) and new_line[start - 1: start + 1] in self.constants.DIPTHONGS:\n                vowel_to_coerce = new_line[end - 1]\n                new_line = new_line[:(start - 1) + offset] + (\" \" * (quantity + 2)) + \\\n                           self.constants.stress_accent_dict[vowel_to_coerce] + new_line[end:]\n            else:\n                new_line = new_line[:start + offset] + \\\n                           (\" \" * quantity) + new_line[start + quantity + offset:]\n        return new_line"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef correct_invalid_start(self, scansion: str) -> str:\n        mark_list = string_utils.mark_list(scansion)\n        raw_scansion = scansion.replace(\" \", \"\")\n        if raw_scansion.startswith(self.constants.SPONDEE + self.constants.UNSTRESSED):\n            new_scansion = list(self.constants.SPONDEE + self.constants.SPONDEE + raw_scansion[4:])\n            corrected = \"\".join(new_scansion)\n            new_sequence = list(\" \" * len(scansion))\n            for idx, car in enumerate(corrected):\n                new_sequence[mark_list[idx]] = car\n            return \"\".join(new_sequence)\n        return scansion", "response": "Corrects the start of a valid entry in a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __getRoots(self, lemma, model=None):\n\n        if lemma not in self.__lemmas__:\n            raise UnknownLemma(\"%s is unknown\" % lemma)\n\n        ROOT_IDS = {\n            \"K\": \"lemma\",\n            \"1\": \"geninf\",\n            \"2\": \"perf\"\n        }\n\n        lemma_entry = self.__lemmas__[lemma]\n        original_roots = {\n            root_id: lemma_entry[root_name].split(\",\")\n            for root_id, root_name in ROOT_IDS.items()\n            if root_id != \"K\" and lemma_entry[root_name]\n        }\n        returned_roots = {}\n\n        if not model:\n            model = self.__models__[lemma_entry[\"model\"]]\n\n        # For each registered root in the model,\n        for model_root_id, model_root_data in model[\"R\"].items():\n\n            # If we have K, it's equivalent to canonical form\n            if model_root_data[0] == \"K\":\n                returned_roots[model_root_id] = [lemma_entry[\"lemma\"]]\n            # Otherwise we have deletion number and addition char\n            else:\n                deletion, addition = int(model_root_data[0]), model_root_data[1] or \"\"\n\n                # If a the root is declared already,\n                # we retrieve the information\n                if model_root_id != \"1\" and model_root_id in returned_roots:\n                    lemma_roots = returned_roots[model_root_id]\n                else:\n                    lemma_roots = lemma_entry[\"lemma\"].split(\",\")\n                # We construct the roots\n                returned_roots[model_root_id] = [\n                    lemma_root[:-deletion] + addition\n                    for lemma_root in lemma_roots\n                ]\n\n        original_roots.update(returned_roots)\n        return original_roots", "response": "Retrieves the known roots of a lemma and returns a dictionary of the known roots with their root identifier as key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decline(self, lemma, flatten=False, collatinus_dict=False):\n\n        if lemma not in self.__lemmas__:\n            raise UnknownLemma(\"%s is unknown\" % lemma)\n\n        # Get data information\n        lemma_entry = self.__lemmas__[lemma]\n        model = self.__models__[lemma_entry[\"model\"]]\n\n        # Get the roots\n        roots = self.__getRoots(lemma, model=model)\n\n        # Get the known forms in order\n        keys = sorted([int(key) for key in model[\"des\"].keys()])\n        forms_data = [(key, model[\"des\"][str(key)]) for key in keys]\n\n        # Generate the return dict\n        forms = {key: [] for key in keys}\n        for key, form_list in forms_data:\n            for form in form_list:\n                root_id, endings = tuple(form)\n                for root in roots[root_id]:\n                    for ending in endings:\n                        forms[key].append(root + ending)\n\n        # sufd means we have the original forms of the parent but we add a suffix\n        if len(model[\"sufd\"]):\n            # For each constant form1\n            for key, iter_forms in forms.items():\n                new_forms = []\n                # We add the constant suffix\n                for sufd in model[\"sufd\"]:\n                    new_forms += [form+sufd for form in iter_forms]\n                forms[key] = new_forms\n\n        # If we need a secure version of the forms. For example, if we have variants\n        if len(model[\"suf\"]):\n            cached_forms = {k: v+[] for k, v in forms.items()}  # Making cache without using copy\n\n            # For each suffix\n            # The format is [suffix characters, [modified forms]]\n            for suffixes in model[\"suf\"]:\n                suffix, modified_forms = suffixes[0], suffixes[1]\n                for modified_form in modified_forms:\n                    forms[modified_form] += [f+suffix for f in cached_forms[modified_form]]\n            # We update with the new roots\n\n        # If some form do not exist, we delete them prehentively\n        if len(model[\"abs\"]):\n            for abs_form in model[\"abs\"]:\n                if abs_form in forms:\n                    del forms[abs_form]\n\n        if flatten:\n            return list([form for case_forms in forms.values() for form in case_forms])\n        elif collatinus_dict:\n            return forms\n        else:\n            return list(\n                [(form, self.__getPOS(key)) for key, case_forms in forms.items() for form in case_forms]\n            )", "response": "Decline a lemma into a list of forms and a dictionary of natural language information about them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _regex_span(_regex, _str, case_insensitive=True):\n    if case_insensitive:\n        flags = regex.IGNORECASE | regex.FULLCASE | regex.VERSION1\n    else:\n        flags = regex.VERSION1\n    comp = regex.compile(_regex, flags=flags)\n    matches = comp.finditer(_str)\n    for match in matches:\n        yield match", "response": "Yields all matches in an input string that match the given regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a regex match object and a language string return the sentence in which the match occurs.", "response": "def _sentence_context(match, language='latin', case_insensitive=True):\n    \"\"\"Take one incoming regex match object and return the sentence in which\n     the match occurs.\n\n    :rtype : str\n    :param match: regex.match\n    :param language: str\n    \"\"\"\n\n    language_punct = {'greek': r'\\.|;',\n                      'latin': r'\\.|\\?|!'}\n\n    assert language in language_punct.keys(), \\\n        'Available punctuation schemes: {}'.format(language_punct.keys())\n\n    start = match.start()\n    end = match.end()\n    window = 1000\n    snippet_left = match.string[start - window:start + 1]\n    snippet_right = match.string[end:end + window]\n    re_match = match.string[match.start():match.end()]\n\n    comp_sent_boundary = regex.compile(language_punct[language], flags=regex.VERSION1)\n    # Left\n    left_punct = []\n    for punct in comp_sent_boundary.finditer(snippet_left):\n        end = punct.end()\n        left_punct.append(end)\n    try:\n        last_period = left_punct.pop() + 1\n    except IndexError:\n        last_period = 0\n\n    # Right\n    right_punct = []\n    for punct in comp_sent_boundary.finditer(snippet_right):\n        end = punct.end()\n        right_punct.append(end)\n    try:\n        first_period = right_punct.pop(0)\n    except IndexError:\n        first_period = 0\n\n    sentence = snippet_left[last_period:-1] + '*' + re_match + '*' + snippet_right[0:first_period]\n\n    return sentence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _paragraph_context(match):\n    start = match.start()\n    end = match.end()\n    window = 100000\n    snippet_left = match.string[start - window:start + 1]\n    snippet_right = match.string[end:end + window]\n    re_match = match.string[match.start():match.end()]\n\n    # (1) Optional any whitespaces, (2) one newline, (3) optional any whitespaces.\n    para_break_pattern = r'\\s*?\\n\\s*?'\n    comp_sent_boundary = regex.compile(para_break_pattern, flags=regex.VERSION1)\n    # Left\n    left_punct = []\n    for punct in comp_sent_boundary.finditer(snippet_left):\n        end = punct.end()\n        left_punct.append(end)\n    try:\n        last_period = left_punct.pop()\n    except IndexError:\n        last_period = 0\n\n    # Right\n    right_punct = []\n    for punct in comp_sent_boundary.finditer(snippet_right):\n        end = punct.end()\n        right_punct.append(end)\n    try:\n        first_period = right_punct.pop(0)\n    except IndexError:\n        first_period = 0\n\n    sentence = snippet_left[last_period:-1] + '*' + re_match + '*' + snippet_right[0:first_period - 1]\n\n    # Remove any trailing whitespace. Necessary?\n    #comp_final_space = regex.compile(r'\\s*$')\n    #sentence = comp_final_space.sub('', sentence)\n\n    return sentence", "response": "Take one incoming regex match object and return the paragraph in which\n    the match occurs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _window_match(match, window=100):\n    window = int(window)\n    start = match.start()\n    end = match.end()\n    snippet_left = match.string[start - window:start]\n    snippet_match = match.string[match.start():match.end()]\n    snippet_right = match.string[end:end + window]\n\n    snippet = snippet_left + '*' + snippet_match + '*' + snippet_right\n\n    return snippet", "response": "Take incoming match and highlight in context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match_regex(input_str, pattern, language, context, case_insensitive=True):\n    if type(context) is str:\n        contexts = ['sentence', 'paragraph']\n        assert context in contexts or type(context) is int, 'Available contexts: {}'.format(contexts)\n    else:\n        context = int(context)\n    for match in _regex_span(pattern, input_str, case_insensitive=case_insensitive):\n        if context == 'sentence':\n            yield _sentence_context(match, language)\n        elif context == 'paragraph':\n            yield _paragraph_context(match)\n        else:\n            yield _window_match(match, context)", "response": "Yields a generator of matches\n     in desired format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for pattern in TLG or PHI5.", "response": "def search_corpus(pattern, corpus, context, case_insensitive=True, expand_keyword=False, lemmatized=False, threshold=0.70):\n    \"\"\"Search for pattern in TLG or PHI5.\n    TODO: Cleanup hyphenation.\n    \"\"\"\n\n    corpora = ['tlg', 'phi5']\n    assert corpus in corpora, \"Available corpora: '{}'.\".format(corpora)\n\n    if type(context) is str:\n        contexts = ['sentence', 'paragraph']\n        assert context in contexts or type(context) is int, 'Available contexts: {}'.format(contexts)\n    else:\n        context = int(context)\n\n    if corpus == 'phi5':\n        lang = 'latin'\n        index = PHI5_INDEX\n        paths = assemble_phi5_author_filepaths()\n    elif corpus == 'tlg':\n        index = TLG_INDEX\n        lang = 'greek'\n        paths = assemble_tlg_author_filepaths()\n\n    if expand_keyword:\n        # Strip off all regex characters from pattern for Word2Vec lookup\n        # First rm escaped chars\n        # TODO: Add '\\u', '\\U', '\\x' to this list\n        escapes_list = [r'\\a', r'\\b', r'\\f', r'\\n', r'\\r', r'\\t', r'\\v', r'\\\\']\n        escapes_str = '|'.join(escapes_list)\n        comp_escapes = regex.compile(escapes_str, flags=regex.VERSION1)\n        pattern = comp_escapes.sub('', pattern)\n        # Second rm remaining punctuation\n        punctuation = set(string.punctuation)\n        pattern = ''.join(ch for ch in pattern if ch not in punctuation)\n        similar_vectors = _keyword_expander(pattern, lang, lemmatized=lemmatized, threshold=threshold)\n        print(\"The following similar terms will be added to the '{0}' query: '{1}'.\".format(pattern, similar_vectors))\n        pattern = [pattern]\n        if similar_vectors:\n            pattern += similar_vectors\n        else:\n            pattern = pattern\n\n    for path in paths:\n        with open(path) as file_open:\n            text = file_open.read()\n        for one_pattern in pattern:\n            _matches = match_regex(text, one_pattern, language=lang, context=context, case_insensitive=case_insensitive)\n            for _match in _matches:\n                _id = os.path.split(path)[1][:-4]\n                author = index[_id]\n                yield (author, _match)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _keyword_expander(word, language, lemmatized=False, threshold=0.70):\n    try:\n        from cltk.vector.word2vec import get_sims\n    except ImportError as imp_err:\n        print(imp_err)\n        raise\n    similar_vectors = get_sims(word, language, lemmatized=lemmatized, threshold=threshold)\n\n    return similar_vectors", "response": "Find similar terms in Word2Vec models. Accepts string and returns a\n     list of terms of n similar similarity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_worlist_trie(wordlist):\n    dicts = dict()\n\n    for w in wordlist:\n        curr = dicts\n        for l in w:\n            curr = curr.setdefault(l, {})\n        curr['__end__'] = '__end__'\n\n    return dicts", "response": "Creates a nested dictionary representing the trie created\n    by the given word list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spellcheck(word, wordlist, depth = 2):\n    Aut = LevenshteinAutomaton(word, depth = depth).convert_to_deterministic()\n    W = make_worlist_trie(wordlist)\n\n    return sorted(list(walk_trie(W, '', Aut.s, Aut)))", "response": "Given a word list and a depth parameter return all words in the wordlist with LevenshteinDistance ( word w ) < = depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_final_state(self, f):\n\n        \"\"\"\n        :param f: int , the state qi to be added to F, epsilon is\n        conventionally defined as the last node (q_|S|)\n        \"\"\"\n        if f not in self.Q:\n            LOG.error(\"The specified value is invalid, f must be a member of Q\")\n            raise InputError(\"The specified value is invalid, f must be a member of Q\")\n\n        self.F.add(f)", "response": "Adds the final state to the set of state i. e. the state epsilon is\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding missing transition states such that \u03b4(q, u) is defined for every state q and any u \u2208 S", "response": "def complete_automaton(self):\n        \"\"\"\n        Adds missing transition states such that \u03b4(q, u) is defined\n        for every state q and any u \u2208 S\n        \"\"\"\n        self.term_state = object()\n\n        self.Q.add(self.term_state)\n\n        for tv in self.Q:\n            for u in self.S:\n                try:\n                    self.transition[tv][u]\n                except:\n                    self.add_transition(tv, u, self.term_state)\n\n        for u in self.S:\n            self.add_transition(self.term_state, u, self.term_state)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_transition(self, qi, u, qj):\n        if u not in self.S:\n            LOG.error(\"The specified value is invalid, f must be a member of S\")\n            raise InputError(\"The specified value is invalid, f must be a member of S\")\n\n        try:\n            self.transition[qi][u].add(qj)\n\n        except KeyError:\n            try:\n                self.transition[qi][u] = {qj}\n            except KeyError:\n                self.transition[qi] = dict()\n                self.transition[qi][u] = {qj}", "response": "Adds a transition to the specified state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Whoosh index of a pre - processed corpus.", "response": "def index_corpus(self):\n        \"\"\"Make a Whoosh index out of a pre-processed corpus, ie TLG, PHI5,\n        or PHI7.\n\n        TLG takes almost 13 min; PHI5 1.5 min.\n        To setup index parameters\n        >>> # cltk_index = CLTKIndex('latin', 'phi5')  # 1.5 min, 363 docs\n        >>> # cltk_index = CLTKIndex('latin', 'phi5', chunk='work')  # 2 min, 837 docs\n        >>> # cltk_index = CLTKIndex('greek', 'tlg')  # 13 min, 1823 docs\n        >>> # cltk_index = CLTKIndex('greek', 'tlg', chunk='work')  #15.5 min, 6625 docs\n\n        # And to start indexing:\n        >>> # cltk_index.index_corpus()\n\n        TODO: Prevent overwriting. Ask user to rm old dir before re-indexing.\n        TODO: Add option for lemmatizing.\n        TODO: Add for figure out lower() options.\n        TODO: Process TLG through forthcoming normalize().\n        TODO: Add name to each index.\n        TODO: Turn off any language-specific mods (eg, stemming, case) that\n        Whoosh might be doing by default.\n        \"\"\"\n\n        # Setup index dir\n        schema = Schema(path=ID(stored=True),\n                        author=TEXT(stored=True),\n                        content=TEXT)\n        try:\n            _index = create_in(self.index_path, schema)\n        except FileNotFoundError:\n            os.makedirs(self.index_path)\n            _index = create_in(self.index_path, schema)\n        writer = _index.writer()\n\n        # Setup corpus to be indexed\n        if self.lang == 'greek' and self.corpus == 'tlg':\n            corpus_path = os.path.expanduser('~/cltk_data/greek/text/tlg/plaintext/')\n            if self.chunk == 'work':\n                corpus_path = os.path.expanduser('~/cltk_data/greek/text/tlg/individual_works/')\n        elif self.lang == 'latin' and self.corpus == 'phi5':\n            corpus_path = os.path.expanduser('~/cltk_data/latin/text/phi5/plaintext/')\n            if self.chunk == 'work':\n                corpus_path = os.path.expanduser('~/cltk_data/latin/text/phi5/individual_works/')\n        assert os.path.isdir(corpus_path), 'Corpus does not exist in the following location: \"%s\". Use CLTK Corpus Importer and TLGU to create transformed corpus.' % corpus_path  # pylint: disable=line-too-long\n\n        files = os.listdir(corpus_path)\n        if self.lang == 'greek' and self.corpus == 'tlg':\n            files = [f[:-4] for f in files if f.startswith('TLG')]\n            corpus_index = TLG_AUTHOR_MAP\n        elif self.lang == 'latin' and self.corpus == 'phi5':\n            files = [f[:-4] for f in files if f.startswith('LAT')]\n            corpus_index = PHI5_AUTHOR_MAP\n\n        time_0 = time.time()\n        logger.info(\"Commencing indexing of %s documents of '%s' corpus.\" % (len(files), self.corpus))  # pylint: disable=line-too-long\n        logger.info('Index will be written to: \"%s\".' % self.index_path)\n        if self.chunk == 'author':\n            for count, file in enumerate(files, 1):\n\n                try:\n                    if self.lang == 'greek' and self.corpus == 'tlg':\n                        file = file[3:]\n                        author = corpus_index[file]\n                        path = os.path.join(corpus_path, 'TLG' + file + '.TXT')\n                    if self.lang == 'latin' and self.corpus == 'phi5':\n                        author = corpus_index[file]\n                        path = os.path.join(corpus_path, file + '.TXT')\n                except KeyError as key_error:\n                    if file.startswith('LAT9999'):\n                        continue\n                    logger.error(key_error)\n                    raise\n\n                with open(path) as file_open:\n                    content = file_open.read()\n                writer.add_document(path=path,\n                                    author=author,\n                                    content=content)\n\n                if count % 100 == 0:\n                    logger.info('Indexed doc %s.' % count)\n\n        if self.chunk == 'work':\n            for count, file in enumerate(files, 1):\n                try:\n                    if self.lang == 'greek' and self.corpus == 'tlg':\n                        path = os.path.join(corpus_path, file + '.TXT')\n                        author = corpus_index[file[3:-8]]\n                    if self.lang == 'latin' and self.corpus == 'phi5':\n                        path = os.path.join(corpus_path, file + '.TXT')\n                        author = corpus_index[file[:-8]]\n                except KeyError as key_error:\n                    if file.startswith('LAT9999'):\n                        continue\n                    logger.error(key_error)\n                    raise\n\n                with open(path) as file_open:\n                    content = file_open.read()\n\n                writer.add_document(path=path,\n                                    author=author,\n                                    content=content)\n                if count % 100 == 0:\n                    logger.info('Indexed doc %s.' % count)\n        logger.info('Commencing to commit changes.')\n        writer.commit()\n\n        time_1 = time.time()\n        elapsed = time_1 - time_0\n        logger.info('Finished indexing all documents in %s seconds (averaging %s docs per sec.)' % (elapsed, (len(files) / elapsed)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef corpus_query(self, query, save_file=None, window_size=300, surround_size=50):\n        _index = open_dir(self.index_path)\n\n        output_str = ''\n\n        with _index.searcher() as searcher:\n            _query = QueryParser(\"content\", _index.schema).parse(query)\n            results = searcher.search(_query, limit=None)\n            results.fragmenter.charlimit = None\n\n            # Allow larger fragments\n            results.fragmenter.maxchars = window_size\n            # Show more context before and after\n            results.fragmenter.surround = surround_size\n\n            docs_number = searcher.doc_count_all()\n\n            output_str += 'Docs containing hits: {}.'.format(docs_number) + '</br></br>'\n\n            for hit in results:\n                author = hit['author']\n                filepath = hit['path']\n                output_str += author + '</br>'\n                output_str += filepath + '</br>'\n\n                with open(filepath) as file_open:\n                    file_contents = file_open.read()\n\n                highlights = hit.highlights(\"content\", text=file_contents, top=10000000)\n                lines = highlights.split('\\n')\n                #lines_numbers = [l for l in lines]\n                lines_br = '</br>'.join(lines)\n                lines_number_approx = len(lines)\n                output_str += 'Approximate hits: {}.'.format(lines_number_approx) + '</br>'\n\n                output_str += lines_br + '</br></br>'\n\n        if save_file:\n            user_dir = os.path.expanduser('~/cltk_data/user_data/search')\n            output_path = os.path.join(user_dir, save_file + '.html')\n\n            try:\n                with open(output_path, 'w') as file_open:\n                    file_open.write(output_str)\n            except FileNotFoundError:\n                os.mkdir(user_dir)\n                with open(output_path, 'w') as file_open:\n                    file_open.write(output_str)\n        else:\n            return output_str", "response": "Send a query to a corpus s index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if a scansion pattern is one of the valid hexameter metrical patterns .", "response": "def is_valid_hexameter(self, scanned_line: str) -> bool:\n        \"\"\"Determine if a scansion pattern is one of the valid hexameter metrical patterns\n        :param scanned_line: a line containing a sequence of stressed and unstressed syllables\n        :return bool\n\n        >>> print(MetricalValidator().is_valid_hexameter(\"-UU---UU---UU-U\"))\n        True\n        \"\"\"\n        line = scanned_line.replace(self.constants.FOOT_SEPARATOR, \"\")\n        line = line.replace(\" \", \"\")\n        if len(line) < 12:\n            return False\n        line = line[:-1] + self.constants.OPTIONAL_ENDING\n        return self.VALID_HEXAMETERS.__contains__(line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines if a scansion pattern is one of the valid Hendecasyllables metrical patterns", "response": "def is_valid_hendecasyllables(self, scanned_line: str) -> bool:\n        \"\"\"Determine if a scansion pattern is one of the valid Hendecasyllables metrical patterns\n\n        :param scanned_line: a line containing a sequence of stressed and unstressed syllables\n        :return bool\n\n        >>> print(MetricalValidator().is_valid_hendecasyllables(\"-U-UU-U-U-U\"))\n        True\n        \"\"\"\n        line = scanned_line.replace(self.constants.FOOT_SEPARATOR, \"\")\n        line = line.replace(\" \", \"\")\n        if len(line) < 11:\n            return False\n        line = line[:-1] + self.constants.OPTIONAL_ENDING\n        return self.VALID_HENDECASYLLABLES.__contains__(line)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_valid_pentameter(self, scanned_line: str) -> bool:\n        line = scanned_line.replace(self.constants.FOOT_SEPARATOR, \"\")\n        line = line.replace(\" \", \"\")\n        if len(line) < 10:\n            return False\n        line = line[:-1] + self.constants.OPTIONAL_ENDING\n        return self.VALID_PENTAMETERS.__contains__(line)", "response": "Determine if a scansion pattern is one of the valid Pentameter metrical patterns\n            "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hexameter_feet(self, scansion: str) -> List[str]:\n        backwards_scan = list(scansion.rstrip())\n        feet = []\n        candidates = [self.constants.STRESSED + self.constants.OPTIONAL_ENDING,\n                      self.constants.STRESSED + self.constants.STRESSED,\n                      self.constants.STRESSED + self.constants.UNSTRESSED,\n                      self.constants.UNSTRESSED + self.constants.STRESSED]\n        incomplete_foot = self.constants.UNSTRESSED + self.constants.UNSTRESSED\n        try:\n            while len(backwards_scan) > 0:\n                spaces = []\n                chunk1 = backwards_scan.pop()\n                while len(\"\".join(chunk1).replace(\" \", \"\")) == 0:\n                    if len(backwards_scan) == 0:\n                        feet.append(chunk1)\n                        return feet[::-1]\n                    chunk1 = backwards_scan.pop() + \"\".join(chunk1)\n                chunk2 = backwards_scan.pop()\n                while chunk2 == \" \":\n                    spaces.append(chunk2)\n                    if len(backwards_scan) == 0:\n                        feet.append(chunk2)\n                        return feet[::-1]\n                    chunk2 = backwards_scan.pop()\n                new_candidate = \"\".join(chunk2) + \"\".join(spaces) + \"\".join(chunk1)\n                if new_candidate.replace(\" \", \"\") in candidates:\n                    feet.append(new_candidate)\n                else:\n                    if new_candidate.replace(\" \", \"\") == incomplete_foot:\n                        spaces2 = []\n                        previous_mark = backwards_scan.pop()\n                        while previous_mark == \" \":\n                            spaces2.append(previous_mark)\n                            previous_mark = backwards_scan.pop()\n                        if previous_mark == self.constants.STRESSED:\n                            new_candidate = \"\".join(previous_mark) + \"\".join(\n                                spaces2) + new_candidate\n                            feet.append(new_candidate)\n                        else:\n                            feet.append(new_candidate)  # invalid foot\n                            spaces3 = []\n                            next_mark = backwards_scan.pop()\n                            while next_mark == \" \":\n                                spaces3.append(previous_mark)\n                                next_mark = backwards_scan.pop()\n                            feet.append(\"\".join(next_mark) + \"\".join(spaces3) + previous_mark)\n        except Exception as ex:\n            LOG.error(\"err at: {}, {}\".format(scansion, ex))\n            return list()\n        return feet[::-1]", "response": "Takes a scansion line and returns a list of feet stressed and unstressed syllables with spaces intact."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closest_hexameter_patterns(self, scansion: str) -> List[str]:\n        return self._closest_patterns(self.VALID_HEXAMETERS, scansion)", "response": "Find the closest group of matching valid hexameter patterns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closest_pentameter_patterns(self, scansion: str) -> List[str]:\n        return self._closest_patterns(self.VALID_PENTAMETERS, scansion)", "response": "Find the closest group of matching valid pentameter patterns."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the closest group of matching valid hendecasyllable patterns.", "response": "def closest_hendecasyllable_patterns(self, scansion: str) -> List[str]:\n        \"\"\"\n        Find the closest group of matching valid hendecasyllable patterns.\n\n        :return: list of the closest valid hendecasyllable patterns; only candidates with a matching\n        length/number of syllables are considered.\n\n        >>> print(MetricalValidator().closest_hendecasyllable_patterns('UU-UU-U-U-X'))\n        ['-U-UU-U-U-X', 'U--UU-U-U-X']\n        \"\"\"\n        return self._closest_patterns(self.VALID_HENDECASYLLABLES, scansion)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the closest group of matching valid patterns.", "response": "def _closest_patterns(self, patterns: List[str], scansion: str) -> List[str]:\n        \"\"\"\n        Find the closest group of matching valid patterns.\n\n        :patterns: a list of patterns\n        :scansion: the scansion pattern thus far\n        :return: list of the closest valid patterns; only candidates with a matching\n        length/number of syllables are considered.\n        \"\"\"\n        pattern = scansion.replace(\" \", \"\")\n        pattern = pattern.replace(self.constants.FOOT_SEPARATOR, \"\")\n        ending = pattern[-1]\n        candidate = pattern[:len(pattern) - 1] + self.constants.OPTIONAL_ENDING\n        cans = [(distance(candidate, x), x) for x in patterns\n                if len(x) == len(candidate)]\n        if cans:\n            cans = sorted(cans, key=lambda tup: tup[0])\n            top = cans[0][0]\n            return [can[1][:-1] + ending for can in cans if can[0] == top]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_hexameter_template(self, stress_positions: str) -> str:\n        hexameter = []\n        for binary in stress_positions:\n            if binary == \"1\":\n                hexameter.append(self.constants.SPONDEE)\n            if binary == \"0\":\n                hexameter.append(self.constants.DACTYL)\n        hexameter.append(self.constants.HEXAMETER_ENDING)\n        return \"\".join(hexameter)", "response": "Build a hexameter scansion template from a string of 5 binary numbers representing stressed syllables and unstresssed syllables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload replacement patterns for a language.", "response": "def _load_replacement_patterns(self):\n        \"\"\"Check for availability of lemmatizer for a language.\"\"\"\n        if self.language == 'latin':\n            warnings.warn(\n                        \"LemmaReplacer is deprecated and will soon be removed from CLTK. Please use the BackoffLatinLemmatizer at cltk.lemmatize.latin.backoff.\",\n                        DeprecationWarning,\n                        stacklevel=2)\n            rel_path = os.path.join('~','cltk_data',\n                                    self.language,\n                                    'model','latin_models_cltk',\n                                    'lemmata','latin_lemmata_cltk.py')\n            path = os.path.expanduser(rel_path)\n            #logger.info('Loading lemmata. This may take a minute.')\n            loader = importlib.machinery.SourceFileLoader('latin_lemmata_cltk', path)\n\n        elif self.language == 'greek':\n            rel_path = os.path.join('~','cltk_data',\n                                    self.language,\n                                    'model','greek_models_cltk',\n                                    'lemmata','greek_lemmata_cltk.py')\n            path = os.path.expanduser(rel_path)\n            #logger.info('Loading lemmata. This may take a minute.')\n            loader = importlib.machinery.SourceFileLoader('greek_lemmata_cltk', path)\n        module = loader.load_module()\n        lemmata = module.LEMMATA\n        return lemmata"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes incoming string or list of tokens and returns a list of lemmatized words.", "response": "def lemmatize(self, input_text, return_raw=False, return_string=False):\n        \"\"\"Take incoming string or list of tokens. Lookup done against a\n        key-value list of lemmata-headword. If a string, tokenize with\n        ``PunktLanguageVars()``. If a final period appears on a token, remove\n        it, then re-add once replacement done.\n        TODO: rm check for final period, change PunktLanguageVars() to nltk_tokenize_words()\n        \"\"\"\n        assert type(input_text) in [list, str], \\\n            logger.error('Input must be a list or string.')\n        if type(input_text) is str:\n            punkt = PunktLanguageVars()\n            tokens = punkt.word_tokenize(input_text)\n        else:\n            tokens = input_text\n\n        lemmatized_tokens = []\n        for token in tokens:\n            # check for final period\n            final_period = False\n            if token[-1] == '.':\n                final_period = True\n                token = token[:-1]\n\n            # look for token in lemma dict keys\n            if token.lower() in self.lemmata.keys():\n                headword = self.lemmata[token.lower()]\n\n                # re-add final period if rm'd\n                if final_period:\n                    headword += '.'\n\n                # append to return list\n                if not return_raw:\n                    lemmatized_tokens.append(headword)\n                else:\n                    lemmatized_tokens.append(token + '/' + headword)\n            # if token not found in lemma-headword list\n            else:\n                # re-add final period if rm'd\n                if final_period:\n                    token += '.'\n\n                if not return_raw:\n                    lemmatized_tokens.append(token)\n                else:\n                    lemmatized_tokens.append(token + '/' + token)\n        if not return_string:\n            return lemmatized_tokens\n        elif return_string:\n            return ' '.join(lemmatized_tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking for a longest common string between any two given strings passed :param str_a: str :param str_b: str Big Thanks to Pulkit Kathuria(@kevincobain2000) for the function The function is derived from jProcessing toolkit suite", "response": "def long_substring(str_a, str_b):\n    \"\"\"\n    Looks for a longest common string between any two given strings passed\n    :param str_a: str\n    :param str_b: str\n\n    Big Thanks to Pulkit Kathuria(@kevincobain2000) for the function\n    The function is derived from jProcessing toolkit suite\n    \"\"\"\n    data = [str_a, str_b]\n    substr = ''\n    if len(data) > 1 and len(data[0]) > 0:\n        for i in range(len(data[0])):\n            for j in range(len(data[0])-i+1):\n                if j > len(substr) and all(data[0][i:i+j] in x for x in data):\n                    substr = data[0][i:i+j]\n    return substr.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minhash(str_a, str_b):\n        score = 0.0\n        tok_sent_1 = str_a\n        tok_sent_2 = str_b\n        shingles = lambda s: set(s[i:i+3] for i in range(len(s)-2))\n        try:\n            jaccard_distance = lambda seta, setb: len(seta & setb)/float(len(seta | setb))\n            score = jaccard_distance(shingles(tok_sent_1), shingles(tok_sent_2))\n            return score\n        except ZeroDivisionError: return score", "response": "This function calculates the similarity between two strings or texts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing allignment using Needleman-Wunsch algorithm. The alphabet parameter is used for specifying the alphabetical order of the similarity matrix. Similarity matrix is initialized to an unweighted matrix that returns 1 for match and -1 for substitution. Args: :param w1: str :param w2: str :param d: int/float :param alphabet: str :param S: list :return: str tuple Examples: NW calculates the optimal string alignment based on a weighted matrix M. By default, an unweighted similarity matrix is used to represent substitution cost (1 if match, -1 otherwise). >>> Needleman_Wunsch('piscis', 'pesce') ('piscis', 'pesc-e') You can also define your own alphabet and matrix >>> Needleman_Wunsch('pescare', 'piscia', alphabet = \"aceiprs\", S = Default_Matrix(7, 1, -1)) ('pesc-are', 'piscia--') Clearly, a weighted matrix should be used over the default one if linguistic accuracy is desired. The Matrix can be defined manually through matching of manners of articulation or stochastically by detecting the most common substitutions. A simple example follows: First define the similarity matrix >>> M = Default_Matrix(7, 1, -1) We know want to increase the score for matching a to i. >>> M[0][3] = 0.8 >>> M[3][0] = 0.8 >>> Needleman_Wunsch('pescare', 'piscia', alphabet = \"aceiprs\", S = M) ('pescare', 'pisci-a')", "response": "def Needleman_Wunsch(w1, w2, d=-1, alphabet = \"abcdefghijklmnopqrstuvwxyz\", S = Default_Matrix(26, 1, -1) ):\n    \"\"\"\n     Computes allignment using Needleman-Wunsch algorithm. The alphabet\n    parameter is used for specifying the alphabetical order of the similarity\n    matrix. Similarity matrix is initialized to an unweighted matrix that\n    returns 1 for match and -1 for substitution.\n\n    Args:\n        :param w1: str\n        :param w2: str\n        :param d: int/float\n        :param alphabet: str\n        :param S: list\n        :return: str tuple\n\n    Examples:\n        NW calculates the optimal string alignment based on a weighted matrix M.\n\n        By default, an unweighted similarity matrix is used to represent\n        substitution cost (1 if match, -1 otherwise).\n\n        >>> Needleman_Wunsch('piscis', 'pesce')\n        ('piscis', 'pesc-e')\n\n        You can also define your own alphabet and matrix\n\n        >>> Needleman_Wunsch('pescare', 'piscia', alphabet = \"aceiprs\", S = Default_Matrix(7, 1, -1))\n        ('pesc-are', 'piscia--')\n\n        Clearly, a weighted matrix should be used over the default one if\n        linguistic accuracy is desired. The Matrix can be defined manually\n        through matching of manners of articulation or stochastically by\n        detecting the most common substitutions. A simple example follows:\n\n        First define the similarity matrix\n        >>> M = Default_Matrix(7, 1, -1)\n\n        We know want to increase the score for matching a to i.\n\n        >>> M[0][3] = 0.8\n\n        >>> M[3][0] = 0.8\n\n        >>> Needleman_Wunsch('pescare', 'piscia', alphabet = \"aceiprs\", S = M)\n        ('pescare', 'pisci-a')\n\n    \"\"\"\n\n    #S must be a square matrix matching the length of your alphabet\n    if len(S) != len(alphabet) or len(S[0])!= len(alphabet):\n        raise AssertionError(\"Unexpected dimensions of Similarity matrix, S.\"\n                             \" S must be a n by n square matrix, where n is the\"\n                             \" length of your predefined alphabet\")\n\n    m,n = len(w1), len(w2)\n    F = [[0 for i in range(n+1)] for j in range(m+1)]\n\n    for i in range(m+1):\n        F[i][0] = d*i\n\n    for i in range(n+1):\n        F[0][i] = d*i\n\n    #F[i][j] is given by the reccurence relation F[i][j] = max(F[i-1][j-1] + S(A[i],B[i]), F[i][j-1] + d, F[i-1][j] + d)\n    #Where S the similarity matrix and d the gap penalty\n\n    for i in range(1,m+1):\n        for j in range(1,n+1):\n            F[i][j] = max(F[i-1][j-1] + S[alphabet.index(w1[i-1])][alphabet.index(w2[j-1])], F[i-1][j] + d,F[i][j-1] + d)\n\n    A1, A2 = \"\", \"\"\n    i, j = m, n\n\n    #Since F[n][m] gives the maximum score, we can now reconstruct the alignment by determining whether the optimal move\n    #is a match, insertion or deletion\n\n    while i>0 or j>0:\n\n        if i>0 and j>0 and F[i][j] == F[i-1][j-1] + S[alphabet.index(w1[i-1])][alphabet.index(w2[j-1])]:\n            A1 = w1[i-1] + A1\n            A2 = w2[j-1] + A2\n            i -= 1\n            j -= 1\n\n        elif i>0 and F[i][j] == F[i-1][j] + d:\n            A1 = w1[i-1] + A1\n            A2 = \"-\" + A2\n            i -= 1\n\n        else:\n            A1 = \"-\" + A1\n            A2 = w2[j-1] + A2\n            j -= 1\n\n    return (A1, A2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the reference values related to the str_a compared string", "response": "def set_ref_a(self, text_ref):\n        \"\"\"\n        Set the reference values related to the str_a compared string\n        :param text_info: dict\n                    -- author: str\n                    -- work: str\n                    -- subwork: str\n                    -- text_n: str (a string instead of integer for variations\n                     in numbering systems that may inlude integers and alpha\n                      characters (e.g. '101b'))\n        :return: void\n        \"\"\"\n\n        if 'author' in text_ref:\n            self.author_a = text_ref['author']\n        if 'work' in text_ref:\n            self.work_a = text_ref['work']\n        if 'subwork' in text_ref:\n            self.subwork_a = text_ref['subwork']\n        if 'text_n' in text_ref:\n            self.text_n_a = text_ref['text_n']\n        if 'language' in text_ref:\n            self.language_a = text_ref['language']\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_ref_b(self, text_ref):\n\n        if 'author' in text_ref:\n            self.author_b = text_ref['author']\n        if 'work' in text_ref:\n            self.work_b = text_ref['work']\n        if 'subwork' in text_ref:\n            self.subwork_b = text_ref['subwork']\n        if 'text_n' in text_ref:\n            self.text_n_b = text_ref['text_n']\n        if 'language' in text_ref:\n            self.language_b = text_ref['language']\n\n        return", "response": "Set the reference values related to the str_b compared string\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the file into a dictionary of texts.", "response": "def parse_file(self, file_lines):\n        \"\"\"\n        Parses lines of file into a dictionary of texts.\n        :param file_lines: file_importer.file_lines\n        :return: Each text as the form:\n            Pnum: {'metadata': List of lines of metadata,\n                   'pnum': P-number,\n                   'edition': Bibliographic edition,\n                   'raw_text': Raw lines of ATF text,\n                   'transliteration': lines of transliteration,\n                   'normalization': lines of normalization (if present),\n                   'translation': lines of translation (if present)}\n        \"\"\"\n        # separate the file into chunks of text\n        chunks, chunk = [], []\n        # check to see what format the corpus is in, we assume that the headers are the same for all\n        # texts in the file... (maybe not safe?)\n        if re.match('Primary publication:', file_lines[0]):\n            header = re.compile('Primary publication:')\n        else:\n            header = re.compile(r'&?P\\d{6}')\n        for line in file_lines:\n            if header.match(line):\n                if len(chunk) > 0:  # pylint: disable=len-as-condition\n                    chunks.append(chunk)\n                chunk = [line]\n            else:\n                if len(line) > 0:  # pylint: disable=len-as-condition\n                    chunk.append(line)\n        chunks.append(chunk)\n        self.chunks = chunks\n        # create a rich catalog from the chunks\n        re_translit = re.compile(r'(\\d+\\'?\\.) ?(.*)')\n        re_normaliz = re.compile(r'(#tr\\.ts:) ?(.*)')\n        re_translat = re.compile(r'(#tr\\.en:) ?(.*)')\n        for chunk in self.chunks:\n            text = chunk\n            if chunk[0].startswith('Primary publication:'):\n                # we've got full metadata, add additional parsing later\n                metadata = chunk[:25]\n                text = chunk[26:]\n            else:  # no metadata\n                metadata = []\n            pnum = ''.join([c for c in text[0].split('=')[0] if c != '&']).rstrip()\n            edition = text[0].split('=')[1].lstrip()\n            text = text[3:]\n            translit = []\n            normaliz = []\n            translat = []\n            for line in text:\n                if re.match(r'\\d+\\'?\\.', line):\n                    translit.append(re_translit.match(line).groups()[1])\n                if line.startswith('#tr.ts:'):\n                    normaliz.append(re_normaliz.match(line).groups()[1])\n                if line.startswith('#tr.en:'):\n                    translat.append(re_translat.match(line).groups()[1])\n            self.catalog[pnum] = {'metadata': metadata,\n                                  'pnum': pnum,\n                                  'edition': edition,\n                                  'raw_text': text,\n                                  'transliteration': translit,\n                                  'normalization': normaliz,\n                                  'translation': translat}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a rich list of texts in the catalog.", "response": "def toc(self):\n        \"\"\"\n        Returns a rich list of texts in the catalog.\n        \"\"\"\n        output = []\n        for key in sorted(self.catalog.keys()):\n            edition = self.catalog[key]['edition']\n            length = len(self.catalog[key]['transliteration'])\n            output.append(\n                \"Pnum: {key}, Edition: {edition}, length: {length} line(s)\".format(\n                    key=key, edition=edition, length=length))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints out a catalog of all the texts in the corpus.", "response": "def print_catalog(self, catalog_filter=[]):\n        \"\"\"\n        Prints out a catalog of all the texts in the corpus.  Can be filtered by passing\n        a list of keys you want present in the texts.\n        :param: catalog_filter = If you wish to sort the list, use the keys pnum,\n        edition, metadata, transliteration, normalization, or translation.\n        \"\"\"\n        keys = sorted(self.catalog.keys())\n        if len(catalog_filter) > 0:  # pylint: disable=len-as-condition\n            valid = []\n            for key in keys:\n                for f in catalog_filter:\n                    if len(self.catalog[key][f]) > 0:  # pylint: disable=len-as-condition\n                        valid.append(key)\n            keys = valid\n        for key in keys:\n            print(f\"Pnum: {self.catalog[key]['pnum']}\")\n            print(f\"Edition: {self.catalog[key]['edition']}\")\n            print(f\"Metadata: {len(self.catalog[key]['metadata']) > 0}\")\n            print(f\"Transliteration: {len(self.catalog[key]['transliteration']) > 0}\")\n            print(f\"Normalization: {len(self.catalog[key]['normalization']) > 0}\")\n            print(f\"Translation: {len(self.catalog[key]['translation']) > 0}\")\n            print()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef punToEnglish_number(number):\n    output = 0 #This is a simple logic, here we go to each digit and check the number and compare its index with DIGITS list in alphabet.py\n    for num in number:\n        output = 10 * output + DIGITS.index(num)\n    return output", "response": "This function converts a PUN number to English number"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_cltk_path(*fp_list):\n    \n    home = os.path.expanduser('~')\n    return os.path.join(home, 'cltk_data', *fp_list)", "response": "Take arbitrary number of str arguments and return expanded path to a user s cltk_data dir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a pickle file and return loaded pickle object.", "response": "def open_pickle(path: str):\n    \"\"\"Open a pickle and return loaded pickle object.\n    :type path: str\n    :param : path: File path to pickle file to be opened.\n    :rtype : object\n    \"\"\"\n    try:\n        with open(path, 'rb') as opened_pickle:\n            try:\n                return pickle.load(opened_pickle)\n            except Exception as pickle_error:\n                logger.error(pickle_error)\n                raise\n    except FileNotFoundError as fnf_error:\n        logger.error(fnf_error)\n        raise\n    except IOError as io_err:\n        logger.error(io_err)\n        raise\n    except EOFError as eof_error:\n        logger.error(eof_error)\n        raise\n    except pickle.UnpicklingError as unp_error:\n        logger.error(unp_error)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a filename produce an md5 hash of the contents of the file.", "response": "def md5(filename:str)->str:\n    \"\"\"\n    Given a filename produce an md5 hash of the contents.\n    >>> import tempfile, os\n    >>> f = tempfile.NamedTemporaryFile(delete=False)\n    >>> f.write(b'Hello Wirld!')\n    12\n    >>> f.close()\n    >>> md5(f.name)\n    '997c62b6afe9712cad3baffb49cb8c8a'\n    >>> os.unlink(f.name)\n    \"\"\"\n    hash_md5 = hashlib.md5()\n    with open(filename, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cv_pattern(self, word, pprint=False):\n        subscripts = {\n            1: '\u2081',\n            2: '\u2082',\n            3: '\u2083',\n            4: '\u2084',\n            5: '\u2085',\n            6: '\u2086',\n            7: '\u2087',\n            8: '\u2088',\n            9: '\u2089',\n            0: '\u2080'\n        }\n        pattern = []\n        c_count = 1\n        v_count = 1\n        count = 0\n        for char in word:\n            if char in self.akkadian['consonants']:\n                cv = 'C'\n            else:\n                cv = 'V'\n                # remove length:\n                if char in self.akkadian['macron_vowels']:\n                    char = self.akkadian['short_vowels'][self.akkadian['macron_vowels'].index(char)]\n                elif char in self.akkadian['circumflex_vowels']:\n                    char = self.akkadian['short_vowels'][self.akkadian['circumflex_vowels'].index(char)]\n            if char not in [x[2] for x in pattern]:\n                if cv == 'C':\n                    count = c_count\n                    c_count += 1\n                elif cv == 'V':\n                    count = v_count\n                    v_count += 1\n                pattern.append((cv, count, char))\n            elif char in [x[2] for x in pattern]:\n                pattern.append((cv, next(x[1] for x in pattern if x[2] == char), char))\n        if pprint:\n            output = ''\n            for item in pattern:\n                output += (item[0] + subscripts[item[1]])\n            return output\n        return pattern", "response": "Returns a pattern for the given word."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_indiclang_char(c,lang): \n    o=get_offset(c,lang)\n    return (o>=0 and o<=0x7f) or ord(c)==DANDA or ord(c)==DOUBLE_DANDA", "response": "Returns True if the character is in the specified language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nis the character a velar", "response": "def is_velar(c,lang): \n    \"\"\"\n    Is the character a velar\n    \"\"\"\n    o=get_offset(c,lang)\n    return (o>=VELAR_RANGE[0] and o<=VELAR_RANGE[1])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nam the character a palatal", "response": "def is_palatal(c,lang): \n    \"\"\"\n    Is the character a palatal\n    \"\"\"\n    o=get_offset(c,lang)\n    return (o>=PALATAL_RANGE[0] and o<=PALATAL_RANGE[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nis the character a retroflex?", "response": "def is_retroflex(c,lang): \n    \"\"\"\n    Is the character a retroflex\n    \"\"\"\n    o=get_offset(c,lang)\n    return (o>=RETROFLEX_RANGE[0] and o<=RETROFLEX_RANGE[1])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_dental(c,lang): \n    o=get_offset(c,lang)\n    return (o>=DENTAL_RANGE[0] and o<=DENTAL_RANGE[1])", "response": "Is the character a dental?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam the character a labial in a language?", "response": "def is_labial(c,lang): \n    \"\"\"\n    Is the character a labial\n    \"\"\"\n    o=get_offset(c,lang)\n    return (o>=LABIAL_RANGE[0] and o<=LABIAL_RANGE[1])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clausulae_analysis(prosody):\n\n        prosody = ''.join(prosody)\n\n        return {\n            'cretic + trochee': prosody.count('\u00af\u02d8\u00af\u00afx'),\n            '4th paeon + trochee': prosody.count('\u02d8\u02d8\u02d8\u00af\u00afx'),\n            '1st paeon + trochee': prosody.count('\u00af\u02d8\u02d8\u02d8\u00afx'),\n            'substituted cretic + trochee': prosody.count('\u02d8\u02d8\u02d8\u02d8\u02d8\u00afx'),\n            '1st paeon + anapest': prosody.count('\u00af\u02d8\u02d8\u02d8\u02d8\u02d8x'),\n            'double cretic': prosody.count('\u00af\u02d8\u00af\u00af\u02d8x'),\n            '4th paeon + cretic': prosody.count('\u02d8\u02d8\u02d8\u00af\u00af\u02d8x'),\n            'molossus + cretic': prosody.count('\u00af\u00af\u00af\u00af\u02d8x'),\n            'double trochee': prosody.count('\u00af\u02d8\u00afx'),\n            'molossus + double trochee': prosody.count('\u00af\u00af\u00af\u00af\u02d8\u00afx'),\n            'cretic + double trochee': prosody.count('\u00af\u02d8\u00af\u00af\u02d8\u00afx'),\n            'dactyl + double trochee': prosody.count('\u00af\u02d8\u02d8\u00af\u02d8\u00afx'),\n            'choriamb + double trochee': prosody.count('\u00af\u02d8\u02d8\u00af\u00af\u02d8\u00afx'),\n            'cretic + iamb': prosody.count('\u00af\u02d8\u00af\u02d8x'),\n            'molossus + iamb': prosody.count('\u00af\u00af\u00af\u02d8x'),\n            'double spondee': prosody.count('\u00af\u00af\u00afx'),\n            'cretic + double spondee': prosody.count('\u00af\u02d8\u00af\u00af\u00af\u00afx'),\n            'heroic': prosody.count('\u00af\u02d8\u02d8\u00afx')\n        }", "response": "An analysis of a list of prosody and returns a dictionary of the keys and values of the class of clausula."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rhyme_scheme(self):\n        rhymes = dict()\n        i = 64\n        strs = \"\"\n\n        for line in self.syllabified:\n\n            w = line[-1][-1][-3:]\n\n            for r in rhymes.keys():\n                if r.endswith(w) or w.endswith(r):\n                    rhymes[w] = rhymes[r]\n                    break\n\n            if w in rhymes:\n                strs += rhymes[w]\n            else:\n                i += 1\n                rhymes[w] = chr(i)\n                strs += chr(i)\n\n        return strs", "response": "Calculates the rhyme scheme of a given stanza."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef PositionedPhoneme(phoneme, \r\n\tword_initial = False, word_final = False, \r\n\tsyllable_initial = False, syllable_final = False,\r\n\tenv_start = False, env_end = False):\r\n\t'''\r\n\tA decorator for phonemes, used in applying rules over words.\r\n\tReturns a copy of the input phoneme, with additional attributes,\r\n\tspecifying whether the phoneme occurs at a word or syllable boundary,\r\n\tor its position in an environment.\r\n\t'''\r\n\t\r\n\tpos_phoneme = deepcopy(phoneme)\r\n\tpos_phoneme.word_initial = word_initial\r\n\tpos_phoneme.word_final = word_final\r\n\tpos_phoneme.syllable_initial = syllable_initial\r\n\tpos_phoneme.syllable_final = syllable_final\r\n\tpos_phoneme.env_start = env_start\r\n\tpos_phoneme.env_end = env_end\r\n\r\n\treturn pos_phoneme", "response": "Returns a new phoneme object that is positioned at the specified position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new Phoneme with the features of other merged into this feature bundle.", "response": "def merge(self, other):\r\n\t\t'''\r\n\t\tReturns a *copy* of this phoneme, with the features of other merged into this feature bundle.\r\n\t\tOther can be a list of phonemes, in which case the list is returned (for technical reasons).\r\n\t\tOther may also be a single feature value or a list of feature values.\r\n\t\t'''\r\n\t\tphoneme = deepcopy(self)\r\n\r\n\t\t# special case for list of phonemes\r\n\t\tif isinstance(other, list) and len(other) > 0 and isinstance(other[0], AbstractPhoneme):\r\n\t\t\treturn other\r\n\r\n\t\tif isinstance(other, AbstractPhoneme):\r\n\t\t\tfeature_values = other.features.values()\r\n\t\telif type(other) != list and type(other) != tuple:\r\n\t\t\tfeature_values = [other]\r\n\t\telse:\r\n\t\t\tfeature_values = other\r\n\r\n\t\tfor f in feature_values:\r\n\t\t\tif type(f) == list:\r\n\t\t\t\tfor inner_f in f:\r\n\t\t\t\t\tphoneme[type(inner_f)] = inner_f\r\n\t\t\telif isinstance(f, AbstractPhoneme):\r\n\t\t\t\tphoneme = phoneme << f\r\n\t\t\telse:\r\n\t\t\t\tphoneme[type(f)] = f\r\n\r\n\t\tif isinstance(other, AbstractPhoneme) and other.ipa is not None:\r\n\t\t\tphoneme.ipa = other.ipa\r\n\r\n\t\treturn phoneme"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if this disjunctive list matches the other.", "response": "def matches(self, other):\r\n\t\t'''\r\n\t\tA disjunctive list matches a phoneme if any of its members matches the phoneme.\r\n\t\tIf other is also a disjunctive list, any match between this list and the other returns true.\r\n\t\t'''\r\n\t\tif other is None:\r\n\t\t\treturn False\r\n\t\tif isinstance(other, PhonemeDisjunction):\r\n\t\t\treturn any([phoneme.matches(other) for phoneme in self])\r\n\t\tif isinstance(other, list) or isinstance(other, PhonologicalFeature):\r\n\t\t\tother = phoneme(other)\r\n\t\treturn any([phoneme <= other for phoneme in self])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if this phoneme is more sonorous than other.", "response": "def is_more_sonorous(self, other):\r\n\t\t'''\r\n\t\tcompare this phoneme to another for sonority.\r\n\t\tUsed for SSP considerations.\r\n\t\t'''\r\n\t\treturn True if isinstance(other, Consonant) and self[Manner] > other[Manner] else False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lengthen(self):\r\n\t\t'''\r\n\t\tReturns a new Vowel with its Length lengthened, \r\n\t\tand \":\" appended to its IPA symbol.\r\n\t\t'''\r\n\t\tvowel = deepcopy(self)\r\n\r\n\t\tif vowel[Length] == Length.short:\r\n\t\t\tvowel[Length] = Length.long\r\n\t\telif vowel[Length] == Length.long:\r\n\t\t\tvowel[Length] = Length.overlong\r\n\r\n\t\tvowel.ipa += ':'\r\n\t\treturn vowel", "response": "Returns a new Vowel with its Length lengthened and the IPA symbol appended to its IPA symbol."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if this phoneme is more sonorous than other.", "response": "def is_more_sonorous(self, other):\r\n\t\t'''\r\n\t\tcompare this phoneme to another for sonority.\r\n\t\tUsed for SSP considerations.\r\n\t\t'''\r\n\t\tif isinstance(other, Consonant):\r\n\t\t\treturn True\r\n\t\telif self[Height] > other[Height]:\r\n\t\t\treturn True\r\n\t\telif self[Height] == other[Height]:\r\n\t\t\treturn self[Backness] > other[Backness]\r\n\t\telse:\r\n\t\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _position_phonemes(self, phonemes):\r\n\t\t'''\r\n\t\tMark syllable boundaries, and, in future, other positional/suprasegmental features?\r\n\t\t'''\r\n\t\tfor i in range(len(phonemes)):\r\n\t\t\tphonemes[i] = PositionedPhoneme(phonemes[i])\r\n\t\t\tphonemes[i].syllable_initial = self.is_syllable_initial(phonemes, i)\r\n\t\t\tphonemes[i].syllable_final = self.is_syllable_final(phonemes, i)\r\n\r\n\t\treturn phonemes", "response": "Returns a list of phonemes that are positioned and are marked as suprasegmental."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transcribe_word(self, word):\r\n\t\t'''\r\n\t\tThe heart of the transcription process.\r\n\t\tSimilar to the system in in cltk.phonology.utils, the algorithm:\r\n\t\t1) Applies digraphs and diphthongs to the text of the word.\r\n\t\t2) Carries out a naive (\"greedy\", per @clemsciences) substitution of letters to phonemes,\r\n\t\taccording to the alphabet.\r\n\t\t3) Applies the conditions of the rules to the environment of each phoneme in turn.\r\n\t\tThe first rule matched fires.  There is no restart and later rules are not tested.\r\n\t\tAlso, if a rule returns multiple phonemes, these are never re-tested by the rule set.\r\n\t\t'''\r\n\t\tphonemes = []\r\n\r\n\t\ti = 0\r\n\t\twhile i < len(word):\r\n\t\t\t# check for digraphs and dipththongs\r\n\t\t\tif i < len(word) - 1 and word[i:i + 2] in self.di:\r\n\t\t\t\tletter_pair = word[i:i + 2]\r\n\t\t\t\treplacement = self.di[letter_pair]\r\n\t\t\t\treplacement = replacement if isinstance(replacement, list) else [replacement]\r\n\t\t\t\tphonemes.extend(replacement)\r\n\t\t\t\ti += 2\r\n\t\t\telse:\r\n\t\t\t\tphonemes.append(self[word[i]])\r\n\t\t\t\ti += 1\r\n\t\t\r\n\t\t# apply phonological rules.  Note: no restart!\r\n\t\ti = 0\r\n\t\twhile i < len(phonemes):\r\n\t\t    for rule in self.rules:\r\n\t\t    \tphonemes = self._position_phonemes(phonemes)\r\n\r\n\t\t    \tif rule.check_environment(phonemes, i):\r\n\t\t    \t\treplacement = rule(phonemes, i)\r\n\t\t    \t\treplacement = [replacement] if not isinstance(replacement, list) else replacement\r\n\t\t    \t\tnew_phonemes = [self._find_sound(p) for p in replacement]\r\n\t\t    \t\tphonemes[i:i + 1] = new_phonemes\r\n\t\t    \t\ti += len(replacement) - 1\r\n\t\t    \t\tbreak\r\n\t\t    i += 1\r\n\t\t    \t\t\r\n\t\treturn phonemes", "response": "The method that transcribe a word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of strings representing the transcription of the given text.", "response": "def transcribe(self, text, as_phonemes = False):\r\n\t\t'''\r\n\t\tTrascribes a text, which is first tokenized for words, then each word is transcribed.\r\n\t\tIf as_phonemes is true, returns a list of list of phoneme objects,\r\n\t\telse returns a string concatenation of the IPA symbols of the phonemes.\r\n\t\t'''\r\n\t\tphoneme_words = [self.transcribe_word(word) for word in self._tokenize(text)]\r\n\t\tif not as_phonemes:\r\n\t\t\twords = [''.join([phoneme.ipa for phoneme in word]) for word in phoneme_words]\r\n\t\t\treturn ' '.join(words)\r\n\t\telse:\r\n\t\t\treturn phoneme_words"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef voice(self, consonant) :\r\n\t\t'''\r\n\t\tVoices a consonant, by searching the sound inventory for a consonant having the same\r\n\t\tfeatures as the argument, but +voice.\r\n\t\t'''\r\n\t\tvoiced_consonant = deepcopy(consonant)\r\n\t\tvoiced_consonant[Voiced] = Voiced.pos\r\n\t\treturn self._find_sound(voiced_consonant)", "response": "Returns a list of all the voice features that are in the given consonant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aspirate(self, consonant) :\r\n\t\t'''\r\n\t\tAspirates a consonant, by searching the sound inventory for a consonant having the same\r\n\t\tfeatures as the argument, but +aspirated.\r\n\t\t'''\r\n\t\taspirated_consonant = deepcopy(consonant)\r\n\t\taspirated_consonant[Aspirated] = Aspirated.pos\r\n\t\treturn self._find_sound(aspirated_consonant)", "response": "Returns a list of sounds that are in the inventory for a given consonant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrain a sentence tokenizer.", "response": "def train_sentence_tokenizer(self: object, text: str):\n        \"\"\"\n        Train sentence tokenizer.\n        \"\"\"\n        language_punkt_vars = PunktLanguageVars\n\n        # Set punctuation\n        if self.punctuation:\n            if self.strict:\n                language_punkt_vars.sent_end_chars = self.punctuation + self.strict_punctuation\n            else:\n                language_punkt_vars.sent_end_chars = self.punctuation\n\n        # Set abbreviations\n        trainer = PunktTrainer(text, language_punkt_vars)\n        trainer.INCLUDE_ALL_COLLOCS = True\n        trainer.INCLUDE_ABBREV_COLLOCS = True\n\n        tokenizer = PunktSentenceTokenizer(trainer.get_params())\n\n        if self.abbreviations:\n            for abbreviation in self.abbreviations:\n                tokenizer._params.abbrev_types.add(abbreviation)\n\n        return tokenizer"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a list of stopwords from a list of strings.", "response": "def build_stoplist(self, texts, basis='zou', size=100, sort_words=True,\n                        inc_values=False, lower=True, remove_punctuation = True,\n                        remove_numbers=True, include =[], exclude=[]):\n        \"\"\"\n        :param texts: list of strings used as document collection for extracting stopwords\n        :param basis: Define the basis for extracting stopwords from the corpus. Available methods are:\n                      - 'frequency', word counts\n                      - 'mean', mean probabilities\n                      - 'variance', variance probabilities\n                      - 'entropy', entropy\n                      - 'zou', composite measure as defined in the following paper\n                        Zou, F., Wang, F.L., Deng, X., Han, S., and Wang, L.S. 2006. \u201cAutomatic Construction of Chinese Stop Word List.\u201d In Proceedings of the 5th WSEAS International Conference on Applied Computer Science, 1010\u20131015. https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf.\n        :param size: Set the size of the output list\n        :param sort_words: Sort output list alphabetically? (Otherwise return is descending by basis value)\n        :param inc_values: Include basis value; e.g. word counts for\n            'frequency', mean probabilities for 'mean'; for 'zou', the basis\n            value is the word's rank after the Borda sort\n        :param lower: Lowercase corpus or no?\n        :param remove_punctuation: Remove punctuation from corpus or no?\n        :param remove_numbers: Remove numbers from corpus or no?\n        :param include: List of words in addition to stopwords that are\n            extracted from the document collection to be added to the final\n            list; the 'value' in the returned tuple is set to None\n        :param exclude: List of words in addition to stopwords that are\n            extracted from the document collection to be removed from the final\n            list\n        :type texts: list\n        :type basis: str\n        :type size: int\n        :type sort_words: bool\n        :type inc_values: bool\n        :type lower: bool\n        :type remove_punctuation: bool\n        :type remove_numbers: bool\n        :type include: list\n        :type exclude: list\n        :return: a list of stopwords extracted from the corpus\n        :rtype: list\n        \"\"\"\n\n        # Check 'texts' type for string\n        if isinstance(texts, str):\n            texts = [texts]\n\n        # Move all of this preprocessing code outside 'build_stoplist'\n        if lower:\n            texts = [text.lower() for text in texts]\n\n        if remove_punctuation:\n            texts = self._remove_punctuation(texts, self.punctuation)\n\n        if remove_numbers:\n            translator = str.maketrans({key: \" \" for key in '0123456789'})\n            texts = [text.translate(translator) for text in texts]\n\n        # Get DTM and basic descriptive info\n        dtm, vocab = self._make_dtm_vocab(texts)\n        tfidf, _ = self._make_tfidf_vocab(texts)\n\n        M = len(vocab)\n        N = len(texts)\n\n        # Calculate probabilities\n        raw_lengths = self._get_raw_lengths(texts)\n        l = self._get_length_array(raw_lengths)\n        P = self._get_probabilities(dtm, l)\n\n        if basis == 'frequency':\n            # Calculate plain frequencies\n            freq = self.np.ravel(dtm.sum(axis=0))\n            freq_list = self._combine_vocabulary(vocab, freq)[:size]\n            stops = freq_list\n        elif basis == 'tfidf':\n            # Calculate tfidf\n            tfidf = self.np.ravel(tfidf.sum(axis=0))\n            tfidf_list = self._combine_vocabulary(vocab, tfidf)[:size]\n            stops = tfidf_list\n        elif basis == 'mean':\n            # Calculate mean probabilities\n            MP = self._get_mean_probabilities(P, N)\n            mp_list = self._combine_vocabulary(vocab, MP)[:size]\n            stops = mp_list\n        elif basis == 'variance':\n            bP = dtm / sum(raw_lengths)\n            VP = self._get_variance_probabilities(bP, P, N)\n            vp_list = self._combine_vocabulary(vocab, VP)[:size]\n            stops = vp_list\n        elif basis == 'entropy':\n            ent = self._get_entropies(P)\n            ent_list = self._combine_vocabulary(vocab, ent)[:size]\n            stops = set(ent_list)\n        elif basis == 'zou':\n            MP = self._get_mean_probabilities(P, N)\n            mp_list = self._combine_vocabulary(vocab, MP)\n            mp_list = [item[0] for item in mp_list]\n\n            bP = dtm / sum(raw_lengths)\n            VP = self._get_variance_probabilities(bP, P, N)\n            vp_list = self._combine_vocabulary(vocab, VP)\n            vp_list = [item[0] for item in vp_list]\n\n            ent = self._get_entropies(P)\n            ent_list = self._combine_vocabulary(vocab, ent)\n            ent_list = [item[0] for item in ent_list]\n\n            lists = [mp_list, vp_list, ent_list]\n            stops = self._borda_sort(lists)[:size]\n            stops = [(stop, rank) for rank, stop in enumerate(stops)]\n        else:\n            raise ValueError(\"Basis '{}' not supported.\".format(basis))\n\n        if exclude:\n            stops = [item for item in stops if item[0] not in exclude]\n\n        if include:\n            stops.extend((item, None) for item in include if item not in stops)\n\n        if sort_words:\n            stops = sorted(stops)\n\n        if inc_values:\n            return stops\n        else:\n            return [item[0] for item in stops]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a CorpusReader instance for the given corpus name and language.", "response": "def get_corpus_reader(corpus_name: str = None, language: str = None) -> CorpusReader:\n    \"\"\"\n    Corpus reader factory method\n    :param corpus_name: the name of the supported corpus, available as: [package].SUPPORTED_CORPORA\n    :param langugage: the language for search in\n    :return: NLTK compatible corpus reader\n    \"\"\"\n    BASE = '~/cltk_data/{}/text'.format(language)\n    root = os.path.join(os.path.expanduser(BASE), corpus_name)\n\n    if not os.path.exists(root) or corpus_name not in SUPPORTED_CORPORA.get(language):\n        raise ValueError(\n            'Specified corpus data not found, please install {} for language: {}'.format(\n                corpus_name, language))\n\n    sentence_tokenizer = TokenizeSentence(language)\n    the_word_tokenizer = WordTokenizer(language)\n    doc_pattern = r'.*\\.txt'  #: Generic file ending, override below in your own CorpusReader implementation\n\n    if language == 'latin':\n        if corpus_name == 'latin_text_latin_library':\n            skip_keywords = ['Latin', 'Library']\n            return FilteredPlaintextCorpusReader(root=root, fileids=doc_pattern,\n                                                 sent_tokenizer=sentence_tokenizer,\n                                                 word_tokenizer=the_word_tokenizer,\n                                                 skip_keywords=skip_keywords)\n        if corpus_name == 'latin_text_perseus':\n            valid_json_root = os.path.join(root, 'cltk_json')  #: we only support this subsection\n            return JsonfileCorpusReader(root=valid_json_root,\n                                        sent_tokenizer=sentence_tokenizer,\n                                        word_tokenizer=the_word_tokenizer,\n                                        target_language='latin')  # perseus also contains English\n\n    if language == 'greek':\n        if corpus_name == 'greek_text_perseus':\n            valid_json_root = os.path.join(root, 'cltk_json')  #: we only support this subsection\n            return JsonfileCorpusReader(root=valid_json_root,\n                                        sent_tokenizer=sentence_tokenizer,\n                                        word_tokenizer=the_word_tokenizer,\n                                        target_language='grc')  #: this abbreviation is required\n\n        if corpus_name == 'greek_text_tesserae':\n            # tokenizers/taggers need to be replaced with CLTK version\n            # most obv. for POS tagging!\n            return TesseraeCorpusReader(root=root, fileids=r'.*\\.tess',\n                                        sent_tokenizer=sent_tokenize,\n                                        word_tokenizer=word_tokenize,\n                                        pos_tagger=pos_tag,\n                                        target_language='grc')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassemble a new corpus.", "response": "def assemble_corpus(corpus_reader: CorpusReader,\n                    types_requested: List[str],\n                    type_dirs: Dict[str, List[str]] = None,\n                    type_files: Dict[str, List[str]] = None) -> CorpusReader:\n    \"\"\"\n    Create a filtered corpus.\n    :param corpus_reader: This get mutated\n    :param types_requested: a list of string types, which are to be found in the type_dirs and\n    type_files mappings\n    :param type_dirs: a dict of corpus types to directories\n    :param type_files: a dict of corpus types to files\n    :return: a CorpusReader object containing only the mappings desired\n    \"\"\"\n    fileid_names = []  # type: List[str]\n    try:\n        all_file_ids = list(corpus_reader.fileids())\n        clean_ids_types = []  # type: List[Tuple[str, str]]\n        if type_files:\n            for key, valuelist in type_files.items():\n                if key in types_requested:\n                    for value in valuelist:\n                        if value in all_file_ids:\n                            if key:\n                                clean_ids_types.append((value, key))\n        if type_dirs:\n            for key, valuelist in type_dirs.items():\n                if key in types_requested:\n                    for value in valuelist:\n                        corrected_dir = value.replace('./', '')\n                        corrected_dir = '{}/'.format(corrected_dir)\n                        for name in all_file_ids:\n                            if name and name.startswith(corrected_dir):\n                                clean_ids_types.append((name, key))\n        clean_ids_types.sort(key=lambda x: x[0])\n        fileid_names, categories = zip(*clean_ids_types)  # type: ignore\n        corpus_reader._fileids = fileid_names\n        return corpus_reader\n    except Exception:\n        LOG.exception('failure in corpus building')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the words of the corpus including punctuation one by one.", "response": "def words(self, fileids=None) -> Generator[str, str, None]:\n        \"\"\"\n        Provide the words of the corpus; skipping any paragraphs flagged by keywords to the main\n        class constructor\n        :param fileids:\n        :return: words, including punctuation, one by one\n        \"\"\"\n        if not fileids:\n            fileids = self.fileids()\n        for para in self.paras(fileids):\n            flat_para = flatten(para)\n            skip = False\n            if self.skip_keywords:\n                for keyword in self.skip_keywords:\n                    if keyword in flat_para:\n                        skip = True\n            if not skip:\n                for word in flat_para:\n                    yield word"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef docs(self, fileids=None) -> Generator[str, str, None]:\n        if not fileids:\n            fileids = self.fileids()\n        # Create a generator, loading one document into memory at a time.\n        for path, encoding in self.abspaths(fileids, include_encoding=True):\n            with codecs.open(path, 'r', encoding=encoding) as reader:\n                if self.skip_keywords:\n                    tmp_data = []\n                    for line in reader:\n                        skip = False\n                        for keyword in self.skip_keywords:\n                            if keyword in line:\n                                skip = True\n                        if not skip:\n                            tmp_data.append(line)\n                    yield ''.join(tmp_data)\n                else:\n                    yield reader.read()", "response": "Returns the complete text of an Text document."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sizes(self, fileids=None) -> Generator[int, int, None]:\n        if not fileids:\n            fileids = self.fileids()\n        # Create a generator, getting every path and computing filesize\n        for path in self.abspaths(fileids):\n            yield os.path.getsize(path)", "response": "Returns a list of tuples the fileid and size on disk of the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef words(self, fileids=None) -> Generator[str, str, None]:\n        for sentence in self.sents(fileids):\n            words = self._word_tokenizer.tokenize(sentence)\n            for word in words:\n                yield word", "response": "Returns the words of the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a generator of sentences.", "response": "def sents(self, fileids=None) -> Generator[str, str, None]:\n        \"\"\"\n        :param fileids:\n        :return: A generator of sentences\n        \"\"\"\n        for para in self.paras(fileids):\n            sentences = self._sent_tokenizer.tokenize(para)\n            for sentence in sentences:\n                yield sentence"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields paragraphs of the text as demarcated by double new lines.", "response": "def paras(self, fileids=None) -> Generator[str, str, None]:\n        \"\"\"\n        Yield paragraphs of the text, as demarcated by double new lines.\n        :param fileids: single document file or files of proper JSON objects with a text key,\n        and section subkey\n        :return: a generator of paragraphs\n        \"\"\"\n\n        def _recurse_to_strings(my_dict: Dict[str, Any]) -> List[str]:\n            \"\"\"Internal accumulator method.\"\"\"\n            vals = []  # type: List[str]\n            m_keys = sorted(list(my_dict.keys()))\n            for mkey in m_keys:\n                if isinstance(my_dict[mkey], dict):\n                    vals += _recurse_to_strings(my_dict[mkey])\n                else:\n                    vals += [my_dict[mkey]]\n            return vals\n\n        text_sections = []  # type: List[str]\n        for doc in self.docs(fileids):\n            text_data = _recurse_to_strings(doc['text'])\n            for text_part in text_data:\n                skip = False\n                if self.skip_keywords:\n                    for keyword in self.skip_keywords:\n                        if keyword in text_part:\n                            skip = True\n                if not skip:\n                    text_sections.append(text_part)\n            paras = (''.join(text_sections)).split(self.paragraph_separator)\n            for para in paras:\n                yield para"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef docs(self, fileids=None) -> Generator[Dict[str, Any], Dict[str, Any], None]:\n        # Create a generator, loading one document into memory at a time.\n        for path, encoding in self.abspaths(fileids, include_encoding=True):\n            with codecs.open(path, 'r', encoding=encoding) as reader:\n                the_doc = json.loads(reader.read())\n                if 'filename' not in the_doc:\n                    the_doc['filename'] = path\n                yield the_doc", "response": "Returns the complete text of an Text document containing the fileids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef docs(self: object, fileids:str):\n\n        for path, encoding in self.abspaths(fileids, include_encoding=True):\n            with codecs.open(path, 'r', encoding=encoding) as f:\n                yield f.read()", "response": "Yields the complete text of a. tess file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef texts(self: object, fileids: str, plaintext: bool = True):\n\n        for doc in self.docs(fileids):\n            if plaintext==True:\n                doc = re.sub(r'<.+?>\\s', '', doc) # Remove citation info\n            doc = doc.rstrip() # Clean up final line breaks\n            yield doc", "response": "Yields the text content of a. tess file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding the paragraphs in a. tess file.", "response": "def paras(self: object, fileids: str):\n        \"\"\"\n        Returns paragraphs in a .tess file, as defined by two \\n characters.\n        NB: Most .tess files do not have this feature; only the Homeric poems\n        from what I have noticed so far. Perhaps a feature worth looking into.\n        \"\"\"\n\n        for text in self.texts(fileids):\n            for para in text.split('\\n\\n'):\n                yield para"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding a list of lines from the corpus.", "response": "def lines(self: object, fileids: str, plaintext: bool = True):\n        \"\"\"\n        Tokenizes documents in the corpus by line\n        \"\"\"\n\n        for text in self.texts(fileids, plaintext):\n            text = re.sub(r'\\n\\s*\\n', '\\n', text, re.MULTILINE) # Remove blank lines\n            for line in text.split('\\n'):\n                yield line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sents(self: object, fileids: str):\n\n        for para in self.paras(fileids):\n            for sent in sent_tokenize(para):\n                yield sent", "response": "Tokenizes documents in the corpus by sentence\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef words(self: object, fileids: str):\n        for sent in self.sents(fileids):\n            for token in word_tokenize(sent):\n                yield token", "response": "Tokenizes documents in the corpus by word\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield a list of words that are POS tagged for each file in the corpus.", "response": "def pos_tokenize(self: object, fileids: str):\n        \"\"\"\n        Segments, tokenizes, and POS tag a document in the corpus.\n        \"\"\"\n        for para in self.paras(fileids):\n            yield [\n                self.pos_tagger(word_tokenize(sent))\n                for sent in sent_tokenize(para)\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef describe(self: object, fileids: str = None):\n        started = time.time()\n\n        # Structures to perform counting\n        counts = FreqDist()\n        tokens = FreqDist()\n\n        # Perform a single pass over paragraphs, tokenize, and counts\n        for para in self.paras(fileids):\n            counts['paras'] += 1\n\n            for sent in para:\n                counts['sents'] += 1\n\n                # Include POS at some point\n                for word in sent:\n                    counts['words'] += 1\n                    tokens[word] += 1\n\n        # Compute the number of files in the corpus\n        n_fileids = len(self.fileids())\n\n        # Return data structure with information\n        return {\n            'files': n_fileids,\n            'paras': counts['paras'],\n            'sents': counts['sents'],\n            'words': counts['words'],\n            'vocab': len(tokens),\n            'lexdiv': round((counts['words'] / len(tokens)), 3),\n            'ppdoc': round((counts['paras'] / n_fileids), 3),\n            'sppar':round((counts['sents'] / counts['paras']), 3),\n            'secs': round((time.time()-started), 3),\n        }", "response": "Performs a single pass of the corpus and returns a dictionary with the information that the corpus contains."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_entries(self):\n\n        rel_path = os.path.join('~','cltk_data',\n                                'french',\n                                'text','french_data_cltk'\n                                ,'entries.py')\n        path = os.path.expanduser(rel_path)\n        #logger.info('Loading entries. This may take a minute.')\n        loader = importlib.machinery.SourceFileLoader('entries', path)\n        module = loader.load_module()\n        entries = module.entries\n        return entries", "response": "Load the entries module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lemmatize(self, tokens):\n        entries = self.entries\n        forms_and_lemmas = self.forms_and_lemmas\n\n        lemma_list = [x[0] for x in entries]\n        \"\"\"Provide a lemma for each token\"\"\"\n        lemmatized = []\n        for token in tokens:\n            \"\"\"check for a match between token and list of lemmas\"\"\"\n            if token in lemma_list:\n                lemmed = (token, token)\n                lemmatized.append(lemmed)\n            else:\n                \"\"\"if no match check for a match between token and list of lemma forms\"\"\"\n                lemma = [k for k, v in forms_and_lemmas.items() if token in v]\n                if lemma != []:\n                    lemmed = (token, lemma)\n                    lemmatized.append(lemmed)\n                elif lemma == []:\n                    \"\"\"if no match apply regular expressions and check for a match against the list of lemmas again\"\"\"\n                    regexed = regex(token)\n                    if regexed in lemma_list:\n                        lemmed = (token, regexed)\n                        lemmatized.append(lemmed)\n                    else:\n                        lemmed = (token, \"None\")\n                        lemmatized.append(lemmed)\n        return lemmatized", "response": "Returns a list of lemmas for each token in the list of tokens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind characters with iota subscript and replace w / char + iota added.", "response": "def expand_iota_subscript(input_str, lowercase=False):\n    \"\"\"Find characters with iota subscript and replace w/ char + iota added.\"\"\"\n    new_list = []\n    for char in input_str:\n        new_char = MAP_SUBSCRIPT_NO_SUB.get(char)\n        if not new_char:\n            new_char = char\n        new_list.append(new_char)\n    new_str = ''.join(new_list)\n    if lowercase:\n        new_str = new_str.lower()\n    return new_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_non_greek(input_str):\n    greek_alphabet = LOWER + LOWER_ACUTE + LOWER_BREVE + LOWER_CIRCUMFLEX + LOWER_CONSONANTS + LOWER_DIAERESIS + LOWER_DIAERESIS_ACUTE + LOWER_DIAERESIS_CIRCUMFLEX + LOWER_DIAERESIS_GRAVE + LOWER_GRAVE + LOWER_MACRON + [LOWER_RHO] + LOWER_ROUGH + [LOWER_RHO_ROUGH] + [LOWER_RHO_SMOOTH] + LOWER_ROUGH_ACUTE + LOWER_ROUGH_CIRCUMFLEX + LOWER_ROUGH_GRAVE + LOWER_SMOOTH + LOWER_SMOOTH_ACUTE + LOWER_SMOOTH_CIRCUMFLEX + LOWER_SMOOTH_GRAVE + UPPER + UPPER_ACUTE + UPPER_BREVE + UPPER_CONSONANTS + UPPER_DIAERESIS + UPPER_GRAVE + UPPER_MACRON + [UPPER_RHO] + UPPER_ROUGH + [UPPER_RHO_ROUGH] + UPPER_ROUGH_ACUTE + UPPER_ROUGH_CIRCUMFLEX + UPPER_ROUGH_GRAVE + UPPER_SMOOTH + UPPER_SMOOTH_ACUTE + UPPER_SMOOTH_CIRCUMFLEX + UPPER_SMOOTH_GRAVE + NUMERAL_SIGNS + ACCENTS\n    greek_string = \"\".join([lem for lem in input_str if lem in greek_alphabet or lem == \" \"])\n    #\n    return greek_string.strip()", "response": "Filter out non - greek characters in a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tokenize(self, text: str, model: object = None):\n        if not self.model:\n            model = self.model\n\n        tokenizer = self.model\n        if self.lang_vars:\n            tokenizer._lang_vars = self.lang_vars\n        return tokenizer.tokenize(text)", "response": "Method for tokenizing sentences with pretrained punkt models ; can be overridden by language - specific tokenizers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokenize(self, text: str, model: object = None):\n        sentences = re.split(self.pattern, text)\n        return sentences", "response": "Method for tokenizing sentences with regular expressions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_language_variables(self, lang: str):  # pragma: no cover\n        assert lang in PUNCTUATION.keys(), \\\n            'Sentence tokenizer not available for {0} language.'.format(lang)\n        internal_punctuation = PUNCTUATION[lang]['internal']\n        external_punctuation = PUNCTUATION[lang]['external']\n        file = PUNCTUATION[lang]['file']\n        tokenizer_path = os.path.join(os.path.expanduser(self._get_models_path(language=lang)),\n                                      file)\n        assert os.path.isfile(tokenizer_path), \\\n            'CLTK linguistics data not found for language {0} {}'.format(lang)\n        return internal_punctuation, external_punctuation, tokenizer_path", "response": "Setup the internal and external punctuation characters for the given language and build tokenizer file and path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds tokenizer and punctuation variables. :type tokenizer: object :param tokenizer : Unpickled tokenizer object. :rtype : object", "response": "def _setup_tokenizer(self, tokenizer: object):  # pragma: no cover\n        \"\"\"Add tokenizer and punctuation variables.\n        :type tokenizer: object\n        :param tokenizer : Unpickled tokenizer object.\n        :rtype : object\n        \"\"\"\n        language_punkt_vars = PunktLanguageVars\n        language_punkt_vars.sent_end_chars = self.external_punctuation\n        language_punkt_vars.internal_punctuation = self.internal_punctuation\n        tokenizer.INCLUDE_ALL_COLLOCS = True\n        tokenizer.INCLUDE_ABBREV_COLLOCS = True\n        params = tokenizer.get_params()\n        return PunktSentenceTokenizer(params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tokenize(self, untokenized_string: str, model=None):\n        if self.language in INDIAN_LANGUAGES:\n            return self.indian_punctuation_tokenize_regex(untokenized_string)\n        else:\n            return self.tokenize_sentences(untokenized_string)", "response": "Alias for tokenize_sentences()\u2014NLTK s PlaintextCorpusReader needs a tokenize function called which uses the untokenized_string as a parameter for sentence\n        tokenization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the dictionary of lemmas and forms from the OE models repository.", "response": "def _load_forms_and_lemmas(self):\r\n\t\t\"\"\"Load the dictionary of lemmas and forms from the OE models repository.\"\"\"\r\n\r\n\t\trel_path = os.path.join(CLTK_DATA_DIR,\r\n                                'old_english',\r\n                                'model',\r\n                                'old_english_models_cltk',\r\n                                'data',\r\n                                'oe.lemmas')\r\n\t\tpath = os.path.expanduser(rel_path)\r\n\r\n\t\tself.lemma_dict = {}\r\n\r\n\t\twith open(path, 'r') as infile:\r\n\t\t\tlines = infile.read().splitlines()\r\n\t\t\tfor line in lines:\r\n\t\t\t\tforms = line.split('\\t')\r\n\t\t\t\t\r\n\t\t\t\tlemma = forms[0]\r\n\t\t\t\tfor form_seq in forms:\r\n\t\t\t\t\tindiv_forms = form_seq.split(',')\r\n\t\t\t\t\tfor form in indiv_forms:\r\n\t\t\t\t\t\tform = form.lower()\r\n\t\t\t\t\t\tlemma_list = self.lemma_dict.get(form, [])\r\n\t\t\t\t\t\tlemma_list.append(lemma)\r\n\t\t\t\t\t\tself.lemma_dict[form] = lemma_list\r\n\r\n\t\tfor form in self.lemma_dict.keys():\r\n\t\t\tself.lemma_dict[form] = list(set(self.lemma_dict[form]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_type_counts(self):\r\n\r\n\t\trel_path = os.path.join(CLTK_DATA_DIR,\r\n                                'old_english',\r\n                                'model',\r\n                                'old_english_models_cltk',\r\n                                'data',\r\n                                'oe.counts')\r\n\t\tpath = os.path.expanduser(rel_path)\r\n\r\n\t\tself.type_counts = {}\r\n\r\n\t\twith open(path, 'r') as infile:\r\n\t\t\tlines = infile.read().splitlines()\r\n\t\t\tfor line in lines:\r\n\t\t\t\tcount, word = line.split()\r\n\t\t\t\tself.type_counts[word] = int(count)", "response": "Load the table of frequency counts of word forms."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _relative_frequency(self, word):\r\n\r\n\t\tcount = self.type_counts.get(word, 0)\r\n\t\treturn math.log(count/len(self.type_counts)) if count > 0 else 0", "response": "Computes the log relative frequency for a word form"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lemmatize(self, text, best_guess=True, return_frequencies=False):\r\n\t\tif isinstance(text, str):\r\n\t\t\ttokens = wordpunct_tokenize(text)\r\n\t\telif isinstance(text, list):\r\n\t\t\ttokens= text\r\n\t\telse:\r\n\t\t\traise TypeError(\"lemmatize only works with strings or lists of string tokens.\")\r\n\r\n\t\treturn [self._lemmatize_token(token, best_guess, return_frequencies) for token in tokens]", "response": "Lemmatize all tokens in a string or a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluate(self, filename):\r\n\t\twith open(filename, 'r') as infile:\r\n\t\t\tlines = infile.read().splitlines()\r\n\r\n\t\t\tlemma_count = 0\r\n\t\t\ttoken_count = 0\r\n\r\n\t\t\tfor line in lines:\r\n\t\t\t\tline = re.sub(r'[.,!?:;0-9]', ' ', line)\r\n\t\t\t\tlemmas = [lemma for (_, lemma) in self.lemmatize(line, best_guess=False)]\r\n\r\n\t\t\t\ttoken_count += len(lemmas)\r\n\t\t\t\tlemma_count += len(lemmas) - lemmas.count([])\r\n\r\n\t\t\treturn lemma_count/token_count", "response": "Runs the lemmatize function over the contents of the file and returns the proportion of unfound lemmas."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_stem(self, noun, gender, mimation=True):\n        stem = ''\n        if mimation and noun[-1:] == 'm':\n            # noun = noun[:-1]\n            pass\n        # Take off ending\n        if gender == 'm':\n            if noun[-2:] in list(self.endings['m']['singular'].values()) + \\\n                    list(self.endings['m']['dual'].values()):\n                stem = noun[:-2]\n            elif noun[-1] in list(self.endings['m']['plural'].values()):\n                stem = noun[:-1]\n            else:\n                print(\"Unknown masculine noun: {}\".format(noun))\n        elif gender == 'f':\n            if noun[-4:] in self.endings['f']['plural']['nominative'] + \\\n                    self.endings['f']['plural']['oblique']:\n                stem = noun[:-4] + 't'\n            elif noun[-3:] in list(self.endings['f']['singular'].values()) + \\\n                    list(self.endings['f']['dual'].values()):\n                stem = noun[:-3] + 't'\n            elif noun[-2:] in list(self.endings['m']['singular'].values()) + \\\n                    list(self.endings['m']['dual'].values()):\n                stem = noun[:-2]\n            else:\n                print(\"Unknown feminine noun: {}\".format(noun))\n        else:\n            print(\"Unknown noun: {}\".format(noun))\n        return stem", "response": "Return the stem of a noun given its gender"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _retrieve_tag(self, text):\n        if self.tagger == 'tag_ngram_123_backoff':  # Data format: Perseus Style (see https://github.com/cltk/latin_treebank_perseus)\n            tags = POSTag('latin').tag_ngram_123_backoff(text.lower())\n            return [(tag[0], tag[1]) for tag in tags]\n        elif self.tagger == 'tag_tnt':\n            tags = POSTag('latin').tag_tnt(text.lower())\n            return [(tag[0], tag[1]) for tag in tags]\n        elif self.tagger == 'tag_crf':\n            tags = POSTag('latin').tag_crf(text.lower())\n            return [(tag[0], tag[1]) for tag in tags]", "response": "Tag text with chosen tagger and clean tags."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _retrieve_morpheus_entry(self, word):\n        entry = self.macron_data.get(word)\n        if entry is None:\n            logger.info('No Morpheus entry found for {}.'.format(word))\n            return None\n        elif len(entry) == 0:\n            logger.info('No Morpheus entry found for {}.'.format(word))\n        return entry", "response": "Retrieve the Morpheus entry for the given word."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning macronized word. :param word: (word, tag) :ptype word: tuple :return: (word, tag, macronized_form) :rtype : tuple", "response": "def _macronize_word(self, word):\n        \"\"\"Return macronized word.\n\n        :param word: (word, tag)\n        :ptype word: tuple\n        :return: (word, tag, macronized_form)\n        :rtype : tuple\n        \"\"\"\n        head_word = word[0]\n        tag = word[1]\n        if tag is None:\n            logger.info('Tagger {} could not tag {}.'.format(self.tagger, head_word))\n            return head_word, tag, head_word\n        elif tag == 'U--------':\n            return (head_word, tag.lower(), head_word)\n        else:\n            entries = self._retrieve_morpheus_entry(head_word)\n            if entries is None:\n                return head_word, tag.lower(), head_word\n            matched_entry = [entry for entry in entries if entry[0] == tag.lower()]\n            if len(matched_entry) == 0:\n                logger.info('No matching Morpheus entry found for {}.'.format(head_word))\n                return head_word, tag.lower(), entries[0][2]\n            elif len(matched_entry) == 1:\n                return head_word, tag.lower(), matched_entry[0][2].lower()\n            else:\n                logger.info('Multiple matching entries found for {}.'.format(head_word))\n                return head_word, tag.lower(), matched_entry[1][2].lower()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef macronize_tags(self, text):\n        return [self._macronize_word(word) for word in self._retrieve_tag(text)]", "response": "Return a list of word tag and macronized form along with POS tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string of text with the macronized version of the tag.", "response": "def macronize_text(self, text):\n        \"\"\"Return macronized form of text.\n\n        E.g. \"Gallia est omnis divisa in partes tres,\" ->\n        \"galli\u0101 est omnis d\u012bv\u012bsa in part\u0113s tr\u0113s ,\"\n\n        :param text: raw text\n        :return: macronized text\n        :rtype : str\n        \"\"\"\n        macronized_words = [entry[2] for entry in self.macronize_tags(text)]\n        return \" \".join(macronized_words)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef replace(self, text):\n        for (pattern, repl) in self.patterns:\n            text = re.subn(pattern, repl, text)[0]\n        return text", "response": "Do j / v replacement"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef string_tokenizer(self, untokenized_string: str, include_blanks=False):\n        line_output = []\n        assert isinstance(untokenized_string, str), \\\n            'Incoming argument must be a string.'\n        if include_blanks:\n            tokenized_lines = untokenized_string.splitlines()\n        else:\n            tokenized_lines = [line for line in untokenized_string.splitlines()\n                               if line != r'\\\\n']\n        for line in tokenized_lines:\n            # Strip out damage characters\n            if not self.damage:  # Add 'xn' -- missing sign or number?\n                line = ''.join(c for c in line if c not in \"#[]?!*\")\n                re.match(r'^\\d*\\.|\\d\\'\\.', line)\n                line_output.append(line.rstrip())\n        return line_output", "response": "This function is based off CLTK s line tokenizer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef affix_stemmer(words, exception_list=exceptions, strip_pref = True, strip_suf = True):\n\n    for i, w in enumerate(words):\n\n        try:\n            words[i] = exception_list[w]\n\n        except:\n            if len(w) <= 4:\n                continue\n\n            word = w\n\n            if strip_pref:\n\n                for prefix in PREFIXES:\n                    if word.startswith(prefix):\n                        word = word[len(prefix):]\n                        break\n\n            if strip_suf:\n\n                for en in ENDS:\n\n                    if len(word) <= 4:\n                        break\n\n                    # Strip suffixes\n                    for suffix in SUFFIXES:\n\n                        if len(suffix) <= len(en):\n                            break\n\n                        if (word + en).endswith(suffix):\n                            word = word[:-len(suffix) + len(en)]\n                            break\n\n                    if len(word) <= 4:\n                        break\n\n            words[i] = word\n\n    return \" \".join(words)", "response": "This function will take a list of words and return a list of words that are affix - based."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef measure_old_norse_syllable(syllable: list) -> Union[Length, None]:\n    index = 0\n    while index < len(syllable) and not isinstance(syllable[index], Vowel):\n        index += 1\n    if index == len(syllable):\n        return None\n    else:\n        long_vowel_number = 0\n        short_vowel_number = 0\n        geminated_consonant_number = 0\n        simple_consonant_number = 0\n        for c in syllable[index:]:\n            if isinstance(c, Vowel):\n                if c.length == Length.long:\n                    long_vowel_number += 1\n                elif c.length == Length.short:\n                    short_vowel_number += 1\n            elif isinstance(c, Consonant):\n                if c.geminate:\n                    geminated_consonant_number += 1\n                else:\n                    simple_consonant_number += 1\n        if long_vowel_number == 0 and short_vowel_number == 1 and simple_consonant_number <= 1 and\\\n                geminated_consonant_number == 0:\n            return Length.short\n        elif (short_vowel_number == 1 and (simple_consonant_number > 1 or geminated_consonant_number > 0)) or \\\n                long_vowel_number > 0 and simple_consonant_number <= 1 and geminated_consonant_number == 0:\n            return Length.long\n        elif long_vowel_number > 0 and (simple_consonant_number > 1 or geminated_consonant_number > 0):\n            return Length.overlong", "response": "This function measure the old syllables in a syllable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef phonetic_i_umlaut(sound: Vowel) -> Vowel:\n        if sound.is_equal(a):\n            return ee\n        elif sound.is_equal(a.lengthen()):\n            return ee.lengthen()\n        elif sound.is_equal(o):\n            return oee\n        elif sound.is_equal(o.lengthen()):\n            return oee.lengthen()\n        elif sound.is_equal(u):\n            return y\n        elif sound.is_equal(u.lengthen()):\n            return y.lengthen()\n        if sound.is_equal(DIPHTHONGS_IPA_class[\"au\"]):\n            return DIPHTHONGS_IPA_class[\"ey\"]", "response": "Return the phonetic i umlaut for the given sound."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the phonetic u laut of the given sound.", "response": "def phonetic_u_umlaut(sound: Vowel) -> Vowel:\n        \"\"\"\n        >>> umlaut_a = OldNorsePhonology.phonetic_u_umlaut(a)\n        >>> umlaut_a.ipar\n        '\u00f8'\n\n        >>> umlaut_o = OldNorsePhonology.phonetic_u_umlaut(o)\n        >>> umlaut_o.ipar\n        'u'\n\n        >>> umlaut_e = OldNorsePhonology.phonetic_u_umlaut(e)\n        >>> umlaut_e.ipar\n        'e'\n\n\n        :param sound: instance of Vowel\n        :return:\n        \"\"\"\n        if sound.is_equal(a):\n            return oee  # or oe\n        elif sound.is_equal(o):\n            return u\n        else:\n            return sound"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef orthographic_u_umlaut(sound: str) -> str:\n        if sound in OldNorsePhonology.U_UMLAUT:\n            return OldNorsePhonology.U_UMLAUT[sound]\n        else:\n            return sound", "response": "Return the orthographic UMLAUT code for the given sound."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the stressed syllable in a word.", "response": "def find_stress(self, word):\n        \"\"\"\n        Find the stressed syllable in a word.\n        The general logic follows Huehnergard 3rd edition (pgs. 3-4):\n        (a) Light: ending in a short vowel: e.g., -a, -ba\n        (b) Heavy: ending in a long vowel marked with a macron, or in a\n        short vowel plus a consonant: e.g., -\u0101, -b\u0101, -ak, -bak\n        (c) Ultraheavy: ending in a long vowel marked with a circumflex,\n        in any long vowel plus a consonant: e.g., -\u00e2, -b\u00e2, -\u0101k, -b\u0101k, -\u00e2k, -b\u00e2k.\n        (a) If the last syllable is ultraheavy, it bears the stress.\n        (b) Otherwise, stress falls on the last non-final heavy or ultraheavy syllable.\n        (c) Words that contain no non-final heavy or ultraheavy syllables have the\n        stress fall on the first syllable.\n        :param word: a string (or list) in Akkadian\n        :return: a list of syllables with stressed syllable surrounded by \"[]\"\n        \"\"\"\n        syllabifier = Syllabifier()\n\n        if type(word) is str:\n            word = syllabifier.syllabify(word)\n\n        syllables_stress = []\n\n        for i, syllable in enumerate(word):\n            # Enumerate over the syllables and mark them for length\n            # We check each type of length by looking at the length of the\n            # syllable and verifying rules based on character length.\n\n            # Ultraheavy:\n            # -\u00e2, -b\u00e2, -\u0101k, -b\u0101k, -\u00e2k, -b\u00e2k.\n            if len(syllable) == 1:\n                if self._is_circumflex_vowel(syllable):\n                    syllables_stress.append((syllable, \"Ultraheavy\"))\n                    continue\n            elif len(syllable) == 2:\n                if self._is_consonant(syllable[0]) and self._is_circumflex_vowel(syllable[1]):\n                    syllables_stress.append((syllable, \"Ultraheavy\"))\n                    continue\n                if (self._is_macron_vowel(syllable[0]) or self._is_circumflex_vowel(syllable[0])) \\\n                        and self._is_consonant(syllable[1]):\n                    syllables_stress.append((syllable, \"Ultraheavy\"))\n                    continue\n            elif len(syllable) == 3:\n                if self._is_macron_vowel(syllable[1]) or self._is_circumflex_vowel(syllable[1]):\n                    syllables_stress.append((syllable, \"Ultraheavy\"))\n                    continue\n\n            # Heavy:\n            # -\u0101, -b\u0101, -ak, -bak\n            if len(syllable) == 1:\n                if self._is_macron_vowel(syllable):\n                    syllables_stress.append((syllable, \"Heavy\"))\n                    continue\n            elif len(syllable) == 2:\n                if self._is_consonant(syllable[0]) and self._is_macron_vowel(syllable[1]):\n                    syllables_stress.append((syllable, \"Heavy\"))\n                    continue\n                if self._is_short_vowel(syllable[0]) and self._is_consonant(syllable[1]):\n                    syllables_stress.append((syllable, \"Heavy\"))\n                    continue\n            elif len(syllable) == 3:\n                if self._is_short_vowel(syllable[1]):\n                    syllables_stress.append((syllable, \"Heavy\"))\n                    continue\n\n            # Light:\n            # -a, -ba\n            if len(syllable) == 1:\n                if self._is_short_vowel(syllable):\n                    syllables_stress.append((syllable, \"Light\"))\n                    continue\n            elif len(syllable) == 2:\n                if self._is_consonant(syllable[0]) and self._is_short_vowel(syllable[1]):\n                    syllables_stress.append((syllable, \"Light\"))\n                    continue\n\n        # It's easier to find stress backwards\n        syllables_stress = syllables_stress[::-1]\n\n        syllables = []\n        found_stress = 0\n        for i, syllable in enumerate(syllables_stress):\n            # If we've found the stressed syllable just append the next syllable\n            if found_stress:\n                syllables.append(syllable[0])\n                continue\n\n            # Rule (a)\n            elif syllable[1] == \"Ultraheavy\" and i == 0:\n                syllables.append(\"[{}]\".format(syllable[0]))\n                found_stress = 1\n                continue\n\n            # Rule (b)\n            elif syllable[1] in ['Ultraheavy', 'Heavy'] and i > 0:\n                syllables.append(\"[{}]\".format(syllable[0]))\n                found_stress = 1\n                continue\n\n            # Final 'Heavy' syllable, gets no stress\n            elif syllable[1] == 'Heavy' and i == 0:\n                syllables.append(syllable[0])\n                continue\n\n            # Light syllable gets no stress\n            elif syllable[1] == \"Light\":\n                syllables.append(syllable[0])\n                continue\n\n        # Reverse the list again\n        syllables = syllables[::-1]\n\n        # If we still haven't found stress then rule (c) applies\n        # Rule (c)\n        if not found_stress:\n            syllables[0] = \"[{}]\".format(syllables[0])\n\n        return syllables"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine and call data for future use. Initializes and defines all variables which define the phonetic vectors.", "response": "def get_lang_data(self):\n        \"\"\"Define and call data for future use. Initializes and defines all\n        variables which define the phonetic vectors.\n        \"\"\"\n\n        root = os.path.expanduser('~')\n        csv_dir_path = os.path.join(root, 'cltk_data/sanskrit/model/sanskrit_models_cltk/phonetics')\n\n        all_phonetic_csv = os.path.join(csv_dir_path, 'all_script_phonetic_data.csv')\n        tamil_csv = os.path.join(csv_dir_path, 'tamil_script_phonetic_data.csv')\n\n        # Make helper function for this\n        with open(all_phonetic_csv,'r') as f:\n            reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n            next(reader, None) # Skip headers\n            all_phonetic_data = [row for row in reader]\n\n        with open(tamil_csv,'r') as f:\n            reader = csv.reader(f, delimiter = ',', quotechar = '\"')\n            next(reader, None) # Skip headers\n            # tamil_phonetic_data = [row[PHONETIC_VECTOR_START_OFFSET:] for row in reader]\n            tamil_phonetic_data = [row for row in reader]\n\n        # Handle better?\n        all_phonetic_data = [[int(cell) if cell=='0' or cell=='1' else cell for cell in row] for row in all_phonetic_data]\n        tamil_phonetic_data = [[int(cell) if cell=='0' or cell=='1' else cell for cell in row] for row in tamil_phonetic_data]\n\n        all_phonetic_vectors = np.array([row[PHONETIC_VECTOR_START_OFFSET:] for row in all_phonetic_data])\n        tamil_phonetic_vectors = np.array([row[PHONETIC_VECTOR_START_OFFSET:] for row in tamil_phonetic_data])\n\n        phonetic_vector_length = all_phonetic_vectors.shape[1]\n\n        return all_phonetic_data, tamil_phonetic_data, all_phonetic_vectors, tamil_phonetic_vectors, phonetic_vector_length"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_phonetic_info(self, lang):\n        phonetic_data = self.all_phonetic_data if lang != LC_TA else self.tamil_phonetic_data\n        phonetic_vectors = self.all_phonetic_vectors if lang != LC_TA else self.tamil_phonetic_vectors\n\n        return phonetic_data, phonetic_vectors", "response": "Returns the matrix and phonetic vectors for a specified language"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_phonetic_feature_vector(self, c, lang):\n\n        offset = self.get_offset(c, lang)\n        if not self.in_coordinated_range_offset(offset):\n            return self.invalid_vector()\n\n        phonetic_data, phonetic_vectors = self.get_phonetic_info(lang)\n\n        # 'Valid Vector Representation' is the [5] column\n        if phonetic_data[offset][5] == 0:\n            return self.invalid_vector()\n\n        return phonetic_vectors[offset]", "response": "Returns the phonetic feature vector for a given character in a language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_file(filepath: str) -> str:\n    # ? Check this is ok if absolute paths passed in\n    filepath = os.path.expanduser(filepath)\n    with open(filepath) as opened_file:  # type: IO\n        file_read = opened_file.read()  # type: str\n        return file_read", "response": "Read a file and return it as a string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_concordance(text_str: str) -> List[List[str]]:\n    punkt_vars = PunktLanguageVars()  # type: PunktLanguageVars\n    orig_tokens = punkt_vars.word_tokenize(text_str)  # type: List[str]\n    concordance_index = ConcordanceIndex(orig_tokens)  # type: Any\n    #! rm dupes after index, before loop\n    tokens_set = set(orig_tokens)  # type: Set[str]\n    punct_list = [',', '.', ';', ':', '\"', \"'\", '[', ']']  # type: List[str]\n    # this needs to be changed or rm'ed\n    tokens = [x for x in tokens_set if x not in punct_list]  # List[str]\n    index = concordance_index.return_concordance_all(tokens)  # List[List[str]]\n    return index", "response": "Build a list of concordance words from a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef return_concordance_word(self, word: str,\n                                width: int = 150,\n                                lines: int = 1000000) -> List[str]:\n        \"\"\"\n        Makes concordance for ``word`` with the specified context window.\n        Returns a list of concordance lines for the given input word.\n        :param word: The target word\n        :type word: str\n        :param width: The width of each line, in characters (default=80)\n        :type width: int\n        :param lines: The number of lines to display (default=25)\n        :type lines: int\n        \"\"\"\n\n        return_list = []  # type: List[str]\n\n        half_width = (width - len(word) - 2) // 2  # type: int\n        context = width // 4  # type: int  # approx number of words of context\n\n        offsets = self.offsets(word)  # type: List[int]\n        if offsets:\n            lines = min(lines, len(offsets))\n            while lines:\n                for i in offsets:\n                    left = (' ' * half_width +\n                            ' '.join(self._tokens[i-context:i]))  # type: str\n                    right = ' '.join(self._tokens[i+1:i+context])  # type: str\n                    left = left[-half_width:]\n                    right = right[:half_width]\n                    line_str = left + ' ' + self._tokens[i] + ' ' + right  # type: str\n                    return_list.append(line_str)  # type: List[str]\n                    lines -= 1\n            return return_list\n        return list()", "response": "Returns a list of concordance lines for the given word."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_concordance_all(self, tokens: List[str]) -> List[List[str]]:\n\n        coll = pyuca.Collator()  # type: pyuca.Collator\n        tokens = sorted(tokens, key=coll.sort_key)  #! is the list order preserved?\n\n        concordance_list = []  # type: List[List[str]]\n        for token in tokens:\n            concordance_list_for_word = self.return_concordance_word(token)  # List[str]\n            if concordance_list_for_word:\n                concordance_list.append(concordance_list_for_word)\n\n        return concordance_list", "response": "Take a list of tokens iteratively run each word through\n        return_concordance_word and build a list of all. This returns a list of all the words in the concordance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge a line of verse with its scansion string. Do not accent dipthongs.", "response": "def merge_line_scansion(self, line: str, scansion: str) -> str:\n        \"\"\"\n        Merge a line of verse with its scansion string. Do not accent dipthongs.\n\n        :param line: the original Latin verse line\n        :param scansion: the scansion pattern\n        :return: the original line with the scansion pattern applied via macrons\n\n        >>> print(ScansionFormatter().merge_line_scansion(\n        ... \"Arma virumque cano, Troiae qui pr\u012bmus ab \u014dr\u012bs\",\n        ... \"-  U  U -  U  U  -     UU-   -   - U  U  - -\"))\n        \u0100rma vir\u016bmque can\u014d, Troiae qu\u012b pr\u012bmus ab \u014dr\u012bs\n\n        >>> print(ScansionFormatter().merge_line_scansion(\n        ... \"l\u012btora, multum ille et terr\u012bs iact\u0101tus et alto\",\n        ... \" - U U   -     -    -   -  -   -  - U  U  -  U\"))\n        l\u012btora, m\u016bltum \u012blle \u0113t t\u0113rr\u012bs i\u0101ct\u0101tus et \u0101lto\n\n        >>> print(ScansionFormatter().merge_line_scansion(\n        ... 'aut facere, haec a te dictaque factaque sunt',\n        ... ' -   U U      -  -  -  -  U  U  -  U  U  -  '))\n        aut facere, haec \u0101 t\u0113 d\u012bctaque f\u0101ctaque s\u016bnt\n        \"\"\"\n        letters = list(line)\n        marks = list(scansion)\n        if len(scansion) < len(line):\n            marks += ((len(line) - len(scansion)) * \" \").split()\n        for idx in range(0, len(marks)):\n            if marks[idx] == self.constants.STRESSED:\n                vowel = letters[idx]\n                if vowel not in self.stress_accent_dict:\n                    LOG.error(\"problem! vowel: {} not in dict for line {}\".format(vowel, line))\n                    pass\n                else:\n                    if idx > 1:\n                        if (letters[idx -2] + letters[idx - 1]).lower() == \"qu\":\n                            new_vowel = self.stress_accent_dict[vowel]\n                            letters[idx] = new_vowel\n                            continue\n                    if idx > 0:\n                        if letters[idx - 1] + vowel in self.constants.DIPTHONGS:\n                            continue\n                        new_vowel = self.stress_accent_dict[vowel]\n                        letters[idx] = new_vowel\n                    else:\n                        new_vowel = self.stress_accent_dict[vowel]\n                        letters[idx] = new_vowel\n        return \"\".join(letters).rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransliterates the text to the target language script", "response": "def transliterate(text,lang1_code,lang2_code):\n        \"\"\"\n        convert the source language script (lang1) to target language script (lang2)\n\n        text: text to transliterate\n        lang1_code: language 1 code \n        lang1_code: language 2 code \n        \"\"\"\n        if (lang1_code in langinfo.SCRIPT_RANGES) and (lang2_code in langinfo.SCRIPT_RANGES):\n            \n            # if Sinhala is source, do a mapping to Devanagari first \n            if lang1_code=='si': \n                text=sdt.sinhala_to_devanagari(text)\n                lang1_code='hi'\n\n            # if Sinhala is target, make Devanagiri the intermediate target\n            org_lang2_code=''\n            if lang2_code=='si': \n                lang2_code='hi'\n                org_lang2_code='si'\n\n            trans_lit_text=[]\n            for c in text: \n                newc=c\n                offset=ord(c)-langinfo.SCRIPT_RANGES[lang1_code][0]\n                if offset >=langinfo.COORDINATED_RANGE_START_INCLUSIVE and offset <= langinfo.COORDINATED_RANGE_END_INCLUSIVE:\n                    if lang2_code=='ta': \n                        # tamil exceptions \n                        offset=UnicodeIndicTransliterator._correct_tamil_mapping(offset)\n                    newc=py23char(langinfo.SCRIPT_RANGES[lang2_code][0]+offset)\n\n                trans_lit_text.append(newc)        \n\n            # if Sinhala is source, do a mapping to Devanagari first \n            if org_lang2_code=='si': \n                return sdt.devanagari_to_sinhala(''.join(trans_lit_text))\n\n            return (''.join(trans_lit_text))\n        else:\n            return text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arabicrange():\n    mylist = []\n    for i in range(0x0600, 0x00653):\n        try:\n            mylist.append(unichr(i))\n        except NameError:\n            # python 3 compatible\n            mylist.append(chr(i))\n        except ValueError:\n            pass\n    return mylist", "response": "u Return a list of characteres between \\ u060c to \\ u0652\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_vocalized(word):\n    if word.isalpha():\n        return False\n    for char in word:\n        if is_tashkeel(char):\n            return True\n    else:\n        return False", "response": "Checks if the arabic word is vocalized."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the input text is an Arabic standard Unicode block characters", "response": "def is_arabicstring(text):\n    \"\"\" Checks for an  Arabic standard Unicode block characters\n    An arabic string can contain spaces,  digits and pounctuation.\n    but only arabic standard characters,  not extended arabic\n    @param text: input text\n    @type text: unicode\n    @return: True if all charaters are in Arabic block\n    @rtype: Boolean\n    \"\"\"\n    if re.search(u\"([^\\u0600-\\u0652%s%s%s\\s\\d])\" \\\n                         % (LAM_ALEF, LAM_ALEF_HAMZA_ABOVE, LAM_ALEF_MADDA_ABOVE), text):\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_arabicword(word):\n    if len(word) == 0:\n        return False\n    elif re.search(u\"([^\\u0600-\\u0652%s%s%s])\" \\\n                           % (LAM_ALEF, LAM_ALEF_HAMZA_ABOVE, LAM_ALEF_MADDA_ABOVE), word):\n        return False\n    elif is_haraka(word[0]) or word[0] in (WAW_HAMZA, YEH_HAMZA):\n        return False\n    # if Teh Marbuta or Alef_Maksura not in the end\n    elif re.match(u\"^(.)*[%s](.)+$\" % ALEF_MAKSURA, word):\n        return False\n    elif re.match(u\"^(.)*[%s]([^%s%s%s])(.)+$\" % \\\n                          (TEH_MARBUTA, DAMMA, KASRA, FATHA), word):\n        return False\n    else:\n        return True", "response": "Checks if a word is in Arabic word. Returns True if all charaters are in Arabic word False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strip_harakat(text):\n    # if text:\n    # return  re.sub(HARAKAT_PATTERN, u'', text)\n    # return text\n    if not text:\n        return text\n    elif is_vocalized(text):\n        for char in HARAKAT:\n            text = text.replace(char, '')\n    return text", "response": "Strip Harakat from arabic word except Shadda."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstrips the last Haraka from arabic word except Shadda.", "response": "def strip_lastharaka(text):\n    \"\"\"Strip the last Haraka from arabic word except Shadda.\n    The striped marks are :\n        - FATHA,  DAMMA,  KASRA\n        - SUKUN\n        - FATHATAN,  DAMMATAN,  KASRATAN\n\n    @param text: arabic text.\n    @type text: unicode.\n    @return: return a striped text.\n    @rtype: unicode.\n    \"\"\"\n    if text:\n        if is_vocalized(text):\n            return re.sub(LASTHARAKA_PATTERN, u'', text)\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strip_tashkeel(text):\n    if not text:\n        return text\n    elif is_vocalized(text):\n        for char in TASHKEEL:\n            text = text.replace(char, '')\n    return text", "response": "Strip vowels from a text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstandardize the Hamzat into one form of hamza and alef.", "response": "def normalize_hamza(word):\n    \"\"\"Standardize the Hamzat into one form of hamza,\n    replace Madda by hamza and alef.\n    Replace the LamAlefs by simplified letters.\n\n    @param word: arabic text.\n    @type word: unicode.\n    @return: return a converted text.\n    @rtype: unicode.\n    \"\"\"\n    if word.startswith(ALEF_MADDA):\n        if len(word) >= 3 and (word[1] not in HARAKAT) and \\\n                (word[2] == SHADDA or len(word) == 3):\n            word = HAMZA + ALEF + word[1:]\n        else:\n            word = HAMZA + HAMZA + word[1:]\n    # convert all Hamza from into one form\n    word = word.replace(ALEF_MADDA, HAMZA + HAMZA)\n    word = HAMZAT_PATTERN.sub(HAMZA, word)\n    return word"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nseparate the letters from the vowels in arabic word and return the letters and vowels in the arabic word.", "response": "def separate(word, extract_shadda=False):\n    \"\"\"\n    separate the letters from the vowels,  in arabic word,\n    if a letter hasn't a haraka,  the not definited haraka is attributed.\n    return ( letters, vowels)\n    @param word: the input word\n    @type word: unicode\n    @param extract_shadda: extract shadda as seperate text\n    @type extract_shadda: Boolean\n    @return: ( letters, vowels)\n    @rtype:couple of unicode\n    \"\"\"\n    stack1 = stack.Stack(word)\n    # the word is inversed in the stack\n    stack1.items.reverse()\n    letters = stack.Stack()\n    marks = stack.Stack()\n    vowels = HARAKAT\n    last1 = stack1.pop()\n    # if the last element must be a letter,\n    # the arabic word can't starts with a haraka\n    # in th stack the word is inversed\n    while last1 in vowels:\n        last1 = stack1.pop()\n    while last1 != None:\n        if last1 in vowels:\n            # we can't have two harakats beside.\n            # the shadda is considered as a letter\n            marks.pop()\n            marks.push(last1)\n        elif last1 == SHADDA:\n            # is the element is a Shadda,\n            # the previous letter must have a sukun as mark,\n            # and the shadda take the indefinate  mark\n            marks.pop()\n            marks.push(SUKUN)\n            marks.push(NOT_DEF_HARAKA)\n            letters.push(SHADDA)\n        else:\n            marks.push(NOT_DEF_HARAKA)\n            letters.push(last1)\n        last1 = stack1.pop()\n    if extract_shadda:\n        # the shadda is considered as letter\n        wordletters = u''.join(letters.items)\n        # print wordletters.encode('utf8')\n        shaddaplaces = re.sub(u'[^%s]' % SHADDA, TATWEEL, wordletters)\n        shaddaplaces = re.sub(u'%s%s' % (TATWEEL, SHADDA), SHADDA, shaddaplaces)\n        # print wordletters.encode('utf8')\n        wordletters = strip_shadda(wordletters)\n        # print wordletters.encode('utf8')\n        return (wordletters, u''.join(marks.items), shaddaplaces)\n    else:\n        return (u''.join(letters.items), u''.join(marks.items))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef joint(letters, marks):\n    # The length ot letters and marks must be equal\n    if len(letters) != len(marks):\n        return \"\"\n    stack_letter = stack.Stack(letters)\n    stack_letter.items.reverse()\n    stack_mark = stack.Stack(marks)\n    stack_mark.items.reverse()\n\n    word_stack = stack.Stack()\n    last_letter = stack_letter.pop()\n    last_mark = stack_mark.pop()\n    vowels = HARAKAT\n    while last_letter != None and last_mark != None:\n        if last_letter == SHADDA:\n            top = word_stack.pop()\n            if top not in vowels:\n                word_stack.push(top)\n            word_stack.push(last_letter)\n            if last_mark != NOT_DEF_HARAKA:\n                word_stack.push(last_mark)\n        else:\n            word_stack.push(last_letter)\n            if last_mark != NOT_DEF_HARAKA:\n                word_stack.push(last_mark)\n\n        last_letter = stack_letter.pop()\n        last_mark = stack_mark.pop()\n\n    if not (stack_letter.is_empty() and stack_mark.is_empty()):\n        return False\n    else:\n        return ''.join(word_stack.items)", "response": "joint the letters with the marks\n    return the word that is the same length as the letters and marks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if word1 is like a wazn False otherwise", "response": "def waznlike(word1, wazn):\n    \"\"\"If the  word1 is like a wazn (pattern),\n    the letters must be equal,\n    the wazn has FEH,  AIN,  LAM letters.\n    this are as generic letters.\n    The two words can be full vocalized,  or partial vocalized\n\n    @param word1: input word\n    @type word1: unicode\n    @param wazn: given word template  \u0648\u0632\u0646\n    @type wazn: unicode\n    @return: if two words have similar vocalization\n    @rtype: Boolean\n    \"\"\"\n    stack1 = stack.Stack(word1)\n    stack2 = stack.Stack(wazn)\n    root = stack.Stack()\n    last1 = stack1.pop()\n    last2 = stack2.pop()\n    vowels = HARAKAT\n    while last1 != None and last2 != None:\n        if last1 == last2 and last2 not in (FEH, AIN, LAM):\n            last1 = stack1.pop()\n            last2 = stack2.pop()\n        elif last1 not in vowels and last2 in (FEH, AIN, LAM):\n            root.push(last1)\n            # ~ print \"t\"\n            last1 = stack1.pop()\n            last2 = stack2.pop()\n        elif last1 in vowels and last2 not in vowels:\n            last1 = stack1.pop()\n        elif last1 not in vowels and last2 in vowels:\n            last2 = stack2.pop()\n        else:\n            break\n    # reverse the root letters\n    root.items.reverse()\n    # ~ print \" the root is \",  root.items#\"\".join(root.items)\n    if not (stack1.is_empty() and stack2.is_empty()):\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the two words have the same letters and the same harakats", "response": "def shaddalike(partial, fully):\n    \"\"\"\n    If the two words has the same letters and the same harakats,  this fuction return True.\n    The first word is partially vocalized, the second is fully\n    if the partially contians a shadda, it must be at the same place in the fully\n\n\n    @param partial: the partially vocalized word\n    @type partial: unicode\n    @param fully: the fully vocalized word\n    @type fully: unicode\n    @return: if contains shadda\n    @rtype: Boolean\n    \"\"\"\n    # \u0627\u0644\u0645\u062f\u062e\u0644 \u0644\u064a\u0633 \u0628\u0647 \u0634\u062f\u0629\u060c \u0644\u0627 \u062f\u0627\u0639\u064a \u0644\u0644\u0628\u062d\u062b\n    if not has_shadda(partial):\n        return True\n    # \u0627\u0644\u0645\u062f\u062e\u0644 \u0628\u0647 \u0634\u062f\u0629\u060c \u0648\u0627\u0644\u0646\u062a\u064a\u062c\u0629 \u0644\u064a\u0633 \u0628\u0647\u0627 \u0634\u062f\u0629\u060c \u062e\u0627\u0637\u0626\n    elif not has_shadda(fully) and has_shadda(partial):\n        return False\n        # \u0627\u0644\u0645\u062f\u062e\u0644 \u0648\u0627\u0644\u0645\u062e\u0631\u062c \u0628\u0647\u0645\u0627 \u0634\u062f\u0629\u060c \u0646\u062a\u0623\u0643\u062f \u0645\u0646 \u0645\u0648\u0642\u0639\u0647\u0645\u0627\n    partial = strip_harakat(partial)\n    fully = strip_harakat(fully)\n    pstack = stack.Stack(partial)\n    vstack = stack.Stack(fully)\n    plast = pstack.pop()\n    vlast = vstack.pop()\n    # if debug: print \"+0\",  Pstack,  Vstack\n    while plast != None and vlast != None:\n        if plast == vlast:\n            plast = pstack.pop()\n            vlast = vstack.pop()\n        elif plast == SHADDA and vlast != SHADDA:\n            # if debug: print \"+2\",  Pstack.items, Plast,  Vstack.items, Vlast\n            break\n        elif plast != SHADDA and vlast == SHADDA:\n            # if debug: print \"+2\",  Pstack.items, Plast,  Vstack.items, Vlast\n            vlast = vstack.pop()\n        else:\n            # if debug: print \"+2\",  Pstack.items, Plast,  Vstack.items, Vlast\n            break\n    if not (pstack.is_empty() and vstack.is_empty()):\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce_tashkeel(text):\n    patterns = [\n        # delete all fathat,   except on waw and yeh\n        u\"(?<!(%s|%s))(%s|%s)\" % (WAW, YEH, SUKUN, FATHA),\n        # delete damma if followed by waw.\n        u\"%s(?=%s)\" % (DAMMA, WAW),\n        # delete kasra if followed by yeh.\n        u\"%s(?=%s)\" % (KASRA, YEH),\n        # delete fatha if followed by alef to reduce yeh maftouha\n        #  and waw maftouha before alef.\n        u\"%s(?=%s)\" % (FATHA, ALEF),\n        # delete fatha from yeh and waw if they are in the word begining.\n        u\"(?<=\\s(%s|%s))%s\" % (WAW, YEH, FATHA),\n        # delete kasra if preceded by Hamza below alef.\n        u\"(?<=%s)%s\" % (ALEF_HAMZA_BELOW, KASRA),\n    ]\n    reduced = text\n    for pat in patterns:\n        reduced = re.sub(pat, '', reduced)\n    return reduced", "response": "Reduce the Tashkeel by deleting evident cases."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening and process files from a corpus and return a list of sentences for an author. Each sentence is itself a list of tokenized words.", "response": "def gen_docs(corpus, lemmatize, rm_stops):\n    \"\"\"Open and process files from a corpus. Return a list of sentences for an author. Each sentence\n    is itself a list of tokenized words.\n    \"\"\"\n\n    assert corpus in ['phi5', 'tlg']\n\n    if corpus == 'phi5':\n        language = 'latin'\n        filepaths = assemble_phi5_author_filepaths()\n        jv_replacer = JVReplacer()\n        text_cleaner = phi5_plaintext_cleanup\n        word_tokenizer = nltk_tokenize_words\n        if rm_stops:\n            stops = latin_stops\n        else:\n            stops = None\n    elif corpus == 'tlg':\n        language = 'greek'\n        filepaths = assemble_tlg_author_filepaths()\n        text_cleaner = tlg_plaintext_cleanup\n        word_tokenizer = nltk_tokenize_words\n\n        if rm_stops:\n            stops = latin_stops\n        else:\n            stops = None\n\n    if lemmatize:\n        lemmatizer = LemmaReplacer(language)\n\n    sent_tokenizer = TokenizeSentence(language)\n\n    for filepath in filepaths:\n        with open(filepath) as f:\n            text = f.read()\n        # light first-pass cleanup, before sentence tokenization (which relies on punctuation)\n        text = text_cleaner(text, rm_punctuation=False, rm_periods=False)\n        sent_tokens = sent_tokenizer.tokenize_sentences(text)\n        # doc_sentences = []\n        for sentence in sent_tokens:\n            # a second cleanup at sentence-level, to rm all punctuation\n            sentence = text_cleaner(sentence, rm_punctuation=True, rm_periods=True)\n            sentence = word_tokenizer(sentence)\n            sentence = [s.lower() for s in sentence]\n            sentence = [w for w in sentence if w]\n            if language == 'latin':\n                sentence = [w[1:] if w.startswith('-') else w for w in sentence]\n\n            if stops:\n                sentence = [w for w in sentence if w not in stops]\n\n            sentence = [w for w in sentence if len(w) > 1]  # rm short words\n\n            if sentence:\n                sentence = sentence\n\n            if lemmatize:\n                sentence = lemmatizer.lemmatize(sentence)\n            if sentence and language == 'latin':\n                sentence = [jv_replacer.replace(word) for word in sentence]\n            if sentence:\n                yield sentence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_model(corpus, lemmatize=False, rm_stops=False, size=100, window=10, min_count=5, workers=4, sg=1,\n               save_path=None):\n    \"\"\"Train W2V model.\"\"\"\n\n    # Simple training, with one large list\n    t0 = time.time()\n\n    sentences_stream = gen_docs(corpus, lemmatize=lemmatize, rm_stops=rm_stops)\n    # sentences_list = []\n    # for sent in sentences_stream:\n    #    sentences_list.append(sent)\n\n    model = Word2Vec(sentences=list(sentences_stream), size=size, window=window, min_count=min_count, workers=workers,\n                     sg=sg)\n\n    # \"Trim\" the model of unnecessary data. Model cannot be updated anymore.\n    model.init_sims(replace=True)\n\n    if save_path:\n        save_path = os.path.expanduser(save_path)\n        model.save(save_path)\n\n    print('Total training time for {0}: {1} minutes'.format(save_path, (time.time() - t0) / 60))", "response": "Train a W2V model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget similar Word2Vec terms from vocabulary or trained model.", "response": "def get_sims(word, language, lemmatized=False, threshold=0.70):\n    \"\"\"Get similar Word2Vec terms from vocabulary or trained model.\n\n    TODO: Add option to install corpus if not available.\n    \"\"\"\n    # Normalize incoming word string\n    jv_replacer = JVReplacer()\n    if language == 'latin':\n        # Note that casefold() seemingly does not work with diacritic\n        # Greek, likely because of it expects single code points, not\n        # diacritics. Look into global string normalization to code points\n        # for all languages, especially Greek.\n        word = jv_replacer.replace(word).casefold()\n\n    model_dirs = {'greek': '~/cltk_data/greek/model/greek_word2vec_cltk',\n                  'latin': '~/cltk_data/latin/model/latin_word2vec_cltk'}\n    assert language in model_dirs.keys(), 'Langauges available with Word2Vec model: {}'.format(model_dirs.keys())\n    if lemmatized:\n        lemma_str = '_lemmed'\n    else:\n        lemma_str = ''\n    model_name = '{0}_s100_w30_min5_sg{1}.model'.format(language, lemma_str)\n    model_dir_abs = os.path.expanduser(model_dirs[language])\n    model_path = os.path.join(model_dir_abs, model_name)\n    try:\n        model = Word2Vec.load(model_path)\n    except FileNotFoundError as fnf_error:\n        print(fnf_error)\n        print(\"CLTK's Word2Vec models cannot be found. Please import '{}_word2vec_cltk'.\".format(language))\n        raise\n    try:\n        similars = model.most_similar(word)\n    except KeyError as key_err:\n        print(key_err)\n        possible_matches = []\n        for term in model.vocab:\n            if term.startswith(word[:3]):\n                possible_matches.append(term)\n        print(\"The following terms in the Word2Vec model you may be looking for: '{}'.\".format(possible_matches))\n        return None\n    returned_sims = []\n    for similar in similars:\n        if similar[1] > threshold:\n            returned_sims.append(similar[0])\n    if not returned_sims:\n        print(\"Matches found, but below the threshold of 'threshold={}'. Lower it to see these results.\".format(threshold))\n    return returned_sims"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscans a line of Latin hexameter and produce a scansion pattern and other data.", "response": "def scan(self, original_line: str, optional_transform: bool = False,\n             dactyl_smoothing: bool = False) -> Verse:\n        \"\"\"\n        Scan a line of Latin hexameter and produce a scansion pattern, and other data.\n\n        :param original_line: the original line of Latin verse\n        :param optional_transform: whether or not to perform i to j transform for syllabification\n        :param dactyl_smoothing: whether or not to perform dactyl smoothing\n        :return: a Verse object\n\n        >>> scanner = HexameterScanner()\n        >>> print(scanner.scan(\"impulerit. Tantaene animis caelestibus irae?\"))\n        Verse(original='impulerit. Tantaene animis caelestibus irae?', scansion='-  U U -    -   -   U U -    - -  U U  -  - ', meter='hexameter', valid=True, syllable_count=15, accented='\u012bmpuler\u012bt. T\u0101ntaene anim\u012bs cael\u0113stibus \u012brae?', scansion_notes=['Valid by positional stresses.'], syllables = ['\u012bm', 'pu', 'le', 'r\u012bt', 'T\u0101n', 'taen', 'a', 'ni', 'm\u012bs', 'cae', 'l\u0113s', 'ti', 'bus', 'i', 'rae'])\n        >>> print(scanner.scan(\n        ... \"Arma virumque cano, Troiae qui pr\u012bmus ab \u014dr\u012bs\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        -  U  U -   U  U -    -  -   -   - U  U  - -\n        >>> # some hexameters need the optional transformations:\n        >>> optional_transform_scanner = HexameterScanner(optional_transform=True)\n        >>> print(optional_transform_scanner.scan(\n        ... \"\u012ataliam, f\u0101to profugus, L\u0101v\u012bniaque v\u0113nit\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        - -  -    - -   U U -    - -  U  U  - U\n        >>> print(HexameterScanner().scan(\n        ... \"l\u012btora, multum ille et terr\u012bs iact\u0101tus et alto\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        - U U   -     -    -   -  -   -  - U  U  -  U\n        >>> print(HexameterScanner().scan(\n        ... \"v\u012b superum saevae memorem I\u016bn\u014dnis ob \u012bram;\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        -  U U -    -  -  U U -   - - U  U  - U\n        >>> # handle multiple elisions\n        >>> print(scanner.scan(\"monstrum horrendum, informe, ingens, cui lumen ademptum\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        -        -  -      -  -     -  -      -  - U  U -   U\n        >>> # if we have 17 syllables, create a chain of all dactyls\n        >>> print(scanner.scan(\"quadrupedante putrem sonitu quatit ungula campum\"\n        ... ).scansion) # doctest: +NORMALIZE_WHITESPACE\n        -  U U -  U  U  -   U U -   U U  -  U U  -  U\n        >>> # if we have 13 syllables exactly, we'll create a spondaic hexameter\n        >>> print(HexameterScanner().scan(\n        ... \"illi inter sese multa vi bracchia tollunt\").scansion)  # doctest: +NORMALIZE_WHITESPACE\n        -    -  -   - -  -  -  -   -   UU  -  -\n        >>> print(HexameterScanner().scan(\n        ... \"dat latus; insequitur cumulo praeruptus aquae mons\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        -   U U   -  U  U -   U U -    - -  U  U   -  -\n        >>> print(optional_transform_scanner.scan(\n        ... \"Non quivis videt inmodulata po\u00ebmata iudex\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        -    - -   U U  -  U U - U  U- U U  - -\n        >>> print(HexameterScanner().scan(\n        ... \"certabant urbem Romam Remoramne vocarent\").scansion) # doctest: +NORMALIZE_WHITESPACE\n        -  - -   -  -   - -   U U -  U  U - -\n        >>> # advanced smoothing is available via keyword flags: dactyl_smoothing\n        >>> # print(HexameterScanner().scan(\n        #... \"his verbis: 'o gnata, tibi sunt ante ferendae\",\n        #... dactyl_smoothing=True).scansion) # doctest: +NORMALIZE_WHITESPACE\n        #-   -  -    -   - U   U -  -   -  U  U -   -\n        \"\"\"\n        verse = Verse(original_line, meter='hexameter')\n        # replace punctuation with spaces\n        line = original_line.translate(self.punctuation_substitutions)\n        # conservative i to j\n        line = self.transform_i_to_j(line)\n        working_line = self.elide_all(line)\n        working_line = self.accent_by_position(working_line)\n        syllables = self.syllabifier.syllabify(working_line)\n        if optional_transform:\n            working_line = self.transform_i_to_j_optional(line)\n            working_line = self.elide_all(working_line)\n            working_line = self.accent_by_position(working_line)\n            syllables = self.syllabifier.syllabify(working_line)\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"optional i to j\"]]\n        verse.working_line = working_line\n        verse.syllable_count = self.syllabifier.get_syllable_count(syllables)\n        verse.syllables = syllables\n        if verse.syllable_count < 12:\n            verse.valid = False\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"< 12\"]]\n            return verse\n        stresses = self.flag_dipthongs(syllables)\n        syllables_wspaces = string_utils.to_syllables_with_trailing_spaces(working_line, syllables)\n        offset_map = self.calc_offset(syllables_wspaces)\n        for idx, syl in enumerate(syllables):\n            for accented in self.constants.ACCENTED_VOWELS:\n                if accented in syl:\n                    stresses.append(idx)\n        # first syllable is always long in hexameter\n        stresses.append(0)\n        # second to last syllable is always long\n        stresses.append(verse.syllable_count - 2)\n\n        verse.scansion = self.produce_scansion(stresses,\n                                               syllables_wspaces, offset_map)\n        if len(string_utils.stress_positions(self.constants.STRESSED, verse.scansion)) != \\\n                len(set(stresses)):\n            verse.valid = False\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"invalid syllables\"]]\n            return verse\n\n        if self.metrical_validator.is_valid_hexameter(verse.scansion):\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"positionally\"]]\n            return self.assign_candidate(verse, verse.scansion)\n\n        # identify some obvious and probably choices based on number of syllables\n        if verse.syllable_count == 17:  # produce all dactyls\n            candidate = self.produce_scansion(\n                self.metrical_validator.hexameter_known_stresses(),\n                syllables_wspaces, offset_map)\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"17\"]]\n            if self.metrical_validator.is_valid_hexameter(candidate):\n                return self.assign_candidate(verse, candidate)\n        if verse.syllable_count == 12:  # create all spondee hexameter\n            candidate = self.produce_scansion(list(range(12)), syllables_wspaces, offset_map)\n            if self.metrical_validator.is_valid_hexameter(verse.scansion):\n                verse.scansion_notes += [self.constants.NOTE_MAP[\"12\"]]\n                return self.assign_candidate(verse, candidate)\n        if verse.syllable_count == 13:  # create spondee hexameter with a dactyl at 5th foot\n            known_unaccents = [9, 10]\n            last_syllable_accented = False\n            for vowel in self.constants.ACCENTED_VOWELS:\n                if vowel in verse.syllables[12]:\n                    last_syllable_accented = True\n            if not last_syllable_accented:\n                known_unaccents.append(12)\n            if set(known_unaccents) - set(stresses) != len(known_unaccents):\n                verse.scansion = self.produce_scansion([x for x in range(13)\n                                                        if x not in known_unaccents],\n                                                       syllables_wspaces, offset_map)\n                verse.scansion_notes += [self.constants.NOTE_MAP[\"5th dactyl\"]]\n                if self.metrical_validator.is_valid_hexameter(verse.scansion):\n                    return self.assign_candidate(verse, verse.scansion)\n        if verse.syllable_count > 17:\n            verse.valid = False\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"> 17\"]]\n            return verse\n\n        smoothed = self.correct_inverted_amphibrachs(verse.scansion)\n        if distance(verse.scansion, smoothed) > 0:\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"inverted\"]]\n            verse.scansion = smoothed\n            stresses += string_utils.differences(verse.scansion, smoothed)\n\n        if self.metrical_validator.is_valid_hexameter(verse.scansion):\n            return self.assign_candidate(verse, verse.scansion)\n\n        smoothed = self.correct_first_two_dactyls(verse.scansion)\n        if distance(verse.scansion, smoothed) > 0:\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"invalid start\"]]\n            verse.scansion = smoothed\n            stresses += string_utils.differences(verse.scansion, smoothed)\n\n        if self.metrical_validator.is_valid_hexameter(verse.scansion):\n            return self.assign_candidate(verse, verse.scansion)\n\n        smoothed = self.correct_invalid_fifth_foot(verse.scansion)\n        if distance(verse.scansion, smoothed) > 0:\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"invalid 5th\"]]\n            verse.scansion = smoothed\n            stresses += string_utils.differences(verse.scansion, smoothed)\n\n        if self.metrical_validator.is_valid_hexameter(verse.scansion):\n            return self.assign_candidate(verse, verse.scansion)\n\n        feet = self.metrical_validator.hexameter_feet(verse.scansion.replace(\" \", \"\"))\n        if feet:\n            #  Normal good citizens are unwelcome in the house of hexameter\n            invalid_feet_in_hexameter = [self.constants.IAMB, self.constants.TROCHEE]\n            current_foot = 0\n            ending = feet.pop()  # don't process the ending, a possible trochee, add it back after\n            scanned_line = \"\"\n            for foot in feet:\n                if foot.replace(\" \", \"\") in invalid_feet_in_hexameter:\n                    scanned_line = self.invalid_foot_to_spondee(feet, foot, current_foot)\n                    scanned_line = scanned_line + ending\n                current_foot += 1\n            smoothed = self.produce_scansion(stresses +\n                                             string_utils.stress_positions(\n                                                 self.constants.STRESSED, scanned_line),\n                                             syllables_wspaces, offset_map)\n\n            if self.metrical_validator.is_valid_hexameter(smoothed):\n                verse.scansion_notes += [self.constants.NOTE_MAP[\"invalid foot\"]]\n                return self.assign_candidate(verse, smoothed)\n\n        # need to do this again, since the scansion has changed\n        smoothed = self.correct_inverted_amphibrachs(verse.scansion)\n        if distance(verse.scansion, smoothed) > 0:\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"inverted\"]]\n            verse.scansion = smoothed\n            stresses += string_utils.differences(verse.scansion, smoothed)\n\n        if self.metrical_validator.is_valid_hexameter(verse.scansion):\n            return self.assign_candidate(verse, verse.scansion)\n\n        candidates = self.metrical_validator.closest_hexameter_patterns(verse.scansion)\n        if candidates is not None:\n            if len(candidates) == 1 \\\n                    and len(verse.scansion.replace(\" \", \"\")) == len(candidates[0]) \\\n                    and len(string_utils.differences(verse.scansion, candidates[0])) == 1:\n                tmp_scansion = self.produce_scansion(\n                    string_utils.differences(verse.scansion, candidates[0]),\n                    syllables_wspaces, offset_map)\n                if self.metrical_validator.is_valid_hexameter(tmp_scansion):\n                    verse.scansion_notes += [self.constants.NOTE_MAP[\"closest match\"]]\n                    return self.assign_candidate(verse, tmp_scansion)\n\n        # need to do this again, since the scansion has changed\n        smoothed = self.correct_inverted_amphibrachs(smoothed)\n        if self.metrical_validator.is_valid_hexameter(smoothed):\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"inverted\"]]\n            return self.assign_candidate(verse, smoothed)\n\n        if dactyl_smoothing:\n            smoothed = self.correct_dactyl_chain(smoothed)\n            if distance(verse.scansion, smoothed) > 0:\n                verse.scansion_notes += [self.constants.NOTE_MAP[\"dactyl smoothing\"]]\n                verse.scansion = smoothed\n            if self.metrical_validator.is_valid_hexameter(verse.scansion):\n                return self.assign_candidate(verse, verse.scansion)\n\n        # if the line doesn't scan \"as is\", if may scan if the optional i to j transformations\n        # are made, so here we set them and try again.\n        if self.optional_transform and not verse.valid:\n            return self.scan(original_line, optional_transform=True, dactyl_smoothing=True)\n        return verse"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_void_declension(self, number_type, case_type):\n        self.declension = []\n        for i, a_number in enumerate(number_type):\n            self.declension.append([])\n            for _ in case_type:\n                self.declension[i].append(\"\")", "response": "Set void declension for a set of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the attribute root of the word declined in same way as the attribute root of the DeclensionPattern instance.", "response": "def apply(self, root: str, gender: Union[Gender, None], number: Number, case: Case):\n        \"\"\"\n        >>> armr = DeclensionPattern(\"arm\")\n        >>> armr.set_declension([[\"armr\", \"arm\", \"armi\", \"arms\"], [\"armar\", \"arma\", \"\u00f6rmum\", \"arma\"]])\n        >>> armr.get_declined(Case.accusative, Number.singular, None)\n        'arm'\n\n        >>> armr.apply(\"hest\", None, Number.singular, Case.dative)\n        'hesti'\n\n        >>> armr.apply(\"hest\", None, Number.plural, Case.dative)\n        'hestum'\n\n        >>> armr.apply(\"hest\", None, Number.singular, Case.genitive)\n        'hests'\n\n        :param root: root of the word to decline\n        :param gender: instance of Gender of the word to decline (nouns have only one gender so the gender is not mentioned\n        in the declension: the value of gender must be None)\n        :param number: instance of Number\n        :param case: instance of Case\n        :return: word declined the same way as the the attribute root of the DeclensionPattern instance.\n        \"\"\"\n        if gender is None:\n            return root + self.declension[number.value - 1][case.value - 1][len(self.root):]\n        else:\n            return root + self.declension[gender.value - 1][number.value - 1][case.value - 1][len(self.root):]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _qu_fix(self, sents_syllables):\n\n        for sentence in sents_syllables:\n            for word in sentence:\n                for syllable in word:\n                    if 'qu' in syllable:\n                        qu_syll_index = word.index(syllable)\n                        next_syll = qu_syll_index + 1\n                        fixed_syllable = [''.join(word[qu_syll_index:\n                                                       (next_syll + 1)])]\n                        word[qu_syll_index:(next_syll + 1)] = fixed_syllable\n\n        return sents_syllables", "response": "Fix qu in syllables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _elidable_end(self, word):\n        if str(word[-1]).endswith('m'):\n            return True\n        elif str(word[-1][-1]) in self.long_vowels:\n            return True\n        elif str(word[-1][-2] + word[-1][-1]) in self.diphthongs:\n            return True\n        elif str(word[-1][-1]) in self.vowels:\n            return True\n        else:\n            return False", "response": "Check if the ending of a syllabified word is elidable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the beginning of a word is elidable.", "response": "def _elidable_begin(self, word):\n        \"\"\"Check word beginning to see if it is elidable. Elidable beginnings include:\n        1) A word begins with 'h'\n        2) A word begins with a vowel\n        3) A word begins with a diphthong\n\n        :param word: syllabified/'qu' fixed word\n        :return: True if the beginning of a word is elidable, otherwise False\n        :rtype : bool\n        \"\"\"\n        if str(word[0]).startswith('h'):\n            return True\n        elif str(word[0][0]) in self.long_vowels:\n            return True\n        elif str(word[0][0] + word[0][-1]) in self.diphthongs:\n            return True\n        elif str(word[0][0]) in self.vowels:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _elision_fixer(self, sent_syllables):\n        for sent in sent_syllables:\n            for word in sent:\n                try:\n                    next_word = sent[sent.index(word) + 1]\n                    if self._elidable_end(word) and \\\n                            self._elidable_begin(next_word):\n                        # Adds syllable to elided syllable\n                        next_word[0] = str(str(word[-1]) + str(next_word[0]))\n                        word.pop(-1)  # Removes redundant syllable\n                    else:\n                        pass\n                except IndexError:\n                    break\n        return sent_syllables", "response": "Fixes the syllables that are elided by combining the last syllable of a word with the first syllable of a word."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _syllable_condenser(self, words_syllables):\n\n        sentences_syllables = []\n        for sentence in words_syllables:\n            syllables_sentence = []\n            for word in sentence:\n                syllables_sentence += word\n            sentences_syllables.append(syllables_sentence)\n        return sentences_syllables", "response": "Reduce a list of sentences to a list of syllables"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a syllable is long by nature.", "response": "def _long_by_nature(self, syllable):\n        \"\"\"Check if syllable is long by nature. Long by nature includes:\n        1) Syllable contains a diphthong\n        2) Syllable contains a long vowel\n        :param syllable: current syllable\n        :return: True if long by nature\n        :rtype : bool\n        \"\"\"\n\n        # Find diphthongs\n        vowel_group = ''\n        for char in syllable:\n            if char in self.long_vowels:\n                return True\n            elif char not in self.sing_cons:\n                vowel_group += char\n\n        if vowel_group in self.diphthongs:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _long_by_position(self, syllable, sentence):\n\n        try:\n            next_syll = sentence[sentence.index(syllable) + 1]\n            # Long by postion by case 1\n            if (next_syll[0] in self.sing_cons and next_syll[1] in\n                self.sing_cons) and (next_syll[0] not in self.stops and next_syll[1] not in self.liquids):\n                return True\n            # Checks if next syllable begins with two liquids, if so, long by position by case 1\n            elif (next_syll[0] in self.sing_cons and next_syll[1] in\n                self.sing_cons) and (next_syll[0] in self.liquids and next_syll[1] in self.liquids):\n                return True\n            # Checks if next syllable begins with two stops, if so, long by position by case 1\n            elif (next_syll[0] in self.sing_cons and next_syll[1] in\n                self.sing_cons) and (next_syll[0] in self.stops and next_syll[1] in self.stops):\n                return True\n            # Long by position by case 2\n            elif syllable[-1] in self.vowels and next_syll[0] in \\\n                    self.doub_cons:\n                return True\n            # Long by position by case 3\n            elif syllable[-1] in self.sing_cons and next_syll[0] in \\\n                    self.sing_cons:\n                return True\n            else:\n                pass\n        except IndexError:\n            logger.info(\"IndexError while checking if syllable '%s' is long. Continuing.\", syllable)", "response": "Checks if a syllable is long by position."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces long and short values for each input syllable.", "response": "def _scansion(self, sentence_syllables):\n        \"\"\"Replace long and short values for each input syllable.\n        :param sentence_syllables: A list of strings\n        :return : '\u02d8' and '\u00af' to represent short and long syllables,\n        respectively\n        :rtype : list\n        \"\"\"\n\n        scanned_text = []\n        for sentence in sentence_syllables:\n            scanned_sent = []\n            for syllable in sentence:\n                if self._long_by_position(syllable, sentence) or \\\n                   self._long_by_nature(syllable):\n                    scanned_sent.append('\u00af')\n                else:\n                    scanned_sent.append('\u02d8')\n            if len(scanned_sent) > 1:\n                del scanned_sent[-1]\n                scanned_sent.append('x')\n            scanned_text.append(''.join(scanned_sent))\n        return scanned_text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndividing the word tokens into a list of syllables.", "response": "def make_syllables(self, sentences_words):\n        \"\"\"Divide the word tokens into a list of syllables. Note that a syllable\n        in this instance is defined as a vocalic group (i.e., a vowel or a\n        diphthong). This means that all syllables which are not the last\n        syllable in the word will end with a vowel or diphthong.\n\n        TODO: Determine whether Luke Hollis's module at\n        `cltk.stem.latin.syllabifier could replace this method.`\n\n        :param sentences_words: A list of sentences with tokenized words.\n        :return: Syllabified words\n        :rtype : list\n        \"\"\"\n        all_syllables = []\n        for sentence in sentences_words:\n            syll_per_sent = []\n            for word in sentence:\n                syll_start = 0  # Begins syllable iterator\n                syll_per_word = []\n                cur_letter_in = 0  # Begins general iterator\n                while cur_letter_in < len(word):\n                    letter = word[cur_letter_in]\n                    if not cur_letter_in == len(word) - 1:\n                        if word[cur_letter_in] + word[cur_letter_in + 1] in self.diphthongs:\n                            cur_letter_in += 1\n                            # Syllable ends with a diphthong\n                            syll_per_word.append(\n                                word[syll_start:cur_letter_in + 1])\n                            syll_start = cur_letter_in + 1\n                        elif (letter in self.vowels) or \\\n                             (letter in self.long_vowels):\n                            # Syllable ends with a vowel\n                            syll_per_word.append(\n                                word[syll_start:cur_letter_in + 1])\n                            syll_start = cur_letter_in + 1\n                    elif (letter in self.vowels) or \\\n                         (letter in self.long_vowels):\n                        # Syllable ends with a vowel\n                        syll_per_word.append(\n                            word[syll_start:cur_letter_in + 1])\n                        syll_start = cur_letter_in + 1\n                    cur_letter_in += 1\n                try:\n                    last_vowel = syll_per_word[-1][-1]  # Last vowel of a word\n                    # Modifies general iterator for consonants after the last\n                    # syllable in a word.\n                    cur_letter_in = len(\n                        word) - 1\n                    # Contains all of the consonants after the last vowel in a\n                    # word\n                    leftovers = ''\n                    while word[cur_letter_in] != last_vowel:\n                        if word[cur_letter_in] != '.':\n                            # Adds consonants to leftovers\n                            leftovers = word[cur_letter_in] + leftovers\n                        cur_letter_in -= 1\n                    # Adds leftovers to last syllable in a word\n                    syll_per_word[-1] += leftovers\n                    syll_per_sent.append(syll_per_word)\n                except IndexError:\n                    logger.info(\"IndexError while making syllables of '%s'. Continuing.\", word)\n            all_syllables.append(syll_per_sent)\n        return all_syllables"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing the middle german text.", "response": "def normalize_middle_high_german(text, to_lower_all = True, to_lower_beginning = False, alpha_conv = True, punct = True):\n    \"\"\"\n       to_lower_all: convert whole text to lowercase\n       alpha_conv: convert alphabet to canonical form\n       punct: remove punctuation\n    \"\"\"\n \n    if to_lower_all:\n        text = text.lower()\n   \n    if to_lower_beginning:\n        text = text[0].lower() + text[1:]\n        text = re.sub(r\"(?<=[\\.\\?\\!]\\s)(\\w)\",lambda x: x.group(1).lower(),text)\n\n    if alpha_conv:\n        text = text.replace(\"\u0113\",\"\u00ea\").replace(\"\u012b\",\"\u00ee\").replace(\"\u0101\",\"\u00e2\").replace(\"\u014d\",\"\u00f4\").replace(\"\u016b\",\"\u00fb\")\n        text = text.replace(\"ae\",\"\u00e6\").replace(\"oe\",\"\u0153\")\n\n    if punct:\n        text = re.sub(r\"[\\.\\\";\\,\\:\\[\\]\\(\\)!&?\u2018]\",\"\",text)\n    \n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_common_stem(*args):\n    l_s = [[OldNorseSyllable(syllable, VOWELS, CONSONANTS) for syllable in s.syllabify_ssp(word)] for word in args]\n\n    nuclei = [\"\".join(syllables[0].nucleus) for syllables in l_s]\n\n    # all_equal = True\n    # if len(nuclei) > 1:\n    #     for nucleus in nuclei:\n    #         all_equal = nuclei[0] == nucleus and all_equal\n    #         if not all_equal:\n    #             break\n    # if all_equal:\n    #      return os.path.commonprefix(args)\n    smallest = numpy.argmin([len(s) for s in args])\n    for i, c in enumerate(args[smallest]):\n        for other_word in args:\n            if c != other_word[i]:\n                return args[smallest][:i]\n    return args[smallest]", "response": "Function which extract the longest common substring in a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an the -r ending to the last syllable of an Old Norse word. In some cases, it really adds an -r. In other cases, it on doubles the last character or left the syllable unchanged. >>> add_r_ending_to_syllable(\"arm\", True) 'armr' >>> add_r_ending_to_syllable(\"\u00e1s\", True) '\u00e1ss' >>> add_r_ending_to_syllable(\"st\u00f3l\", True) 'st\u00f3ll' >>> \"j\u00f6\"+add_r_ending_to_syllable(\"kul\", False) 'j\u00f6kull' >>> add_r_ending_to_syllable(\"stein\", True) 'steinn' >>> 'mi'+add_r_ending_to_syllable('kil', False) 'mikill' >>> add_r_ending_to_syllable('s\u00e6l', True) 's\u00e6ll' >>> 'li'+add_r_ending_to_syllable('til', False) 'litill' >>> add_r_ending_to_syllable('v\u00e6nn', True) 'v\u00e6nn' >>> add_r_ending_to_syllable('lauss', True) 'lauss' >>> add_r_ending_to_syllable(\"vin\", True) 'vinr' >>> add_r_ending_to_syllable(\"sel\", True) 'selr' >>> add_r_ending_to_syllable('fagr', True) 'fagr' >>> add_r_ending_to_syllable('vitr', True) 'vitr' >>> add_r_ending_to_syllable('vetr', True) 'vetr' >>> add_r_ending_to_syllable('akr', True) 'akr' >>> add_r_ending_to_syllable('Bj\u00f6rn', True) 'Bj\u00f6rn' >>> add_r_ending_to_syllable('\u00feurs', True) '\u00feurs' >>> add_r_ending_to_syllable('karl', True) 'karl' >>> add_r_ending_to_syllable('hrafn', True) 'hrafn' :param last_syllable: last syllable of the word :param is_first: is it the first syllable of the word? :return: inflected syllable", "response": "def add_r_ending_to_syllable(last_syllable: str, is_first=True) -> str:\n    \"\"\"\n    Adds an the -r ending to the last syllable of an Old Norse word.\n    In some cases, it really adds an -r. In other cases, it on doubles the last character or left the syllable\n    unchanged.\n\n    >>> add_r_ending_to_syllable(\"arm\", True)\n    'armr'\n\n    >>> add_r_ending_to_syllable(\"\u00e1s\", True)\n    '\u00e1ss'\n\n    >>> add_r_ending_to_syllable(\"st\u00f3l\", True)\n    'st\u00f3ll'\n\n    >>> \"j\u00f6\"+add_r_ending_to_syllable(\"kul\", False)\n    'j\u00f6kull'\n\n    >>> add_r_ending_to_syllable(\"stein\", True)\n    'steinn'\n\n    >>> 'mi'+add_r_ending_to_syllable('kil', False)\n    'mikill'\n\n    >>> add_r_ending_to_syllable('s\u00e6l', True)\n    's\u00e6ll'\n\n    >>> 'li'+add_r_ending_to_syllable('til', False)\n    'litill'\n\n    >>> add_r_ending_to_syllable('v\u00e6nn', True)\n    'v\u00e6nn'\n\n    >>> add_r_ending_to_syllable('lauss', True)\n    'lauss'\n\n    >>> add_r_ending_to_syllable(\"vin\", True)\n    'vinr'\n\n    >>> add_r_ending_to_syllable(\"sel\", True)\n    'selr'\n\n    >>> add_r_ending_to_syllable('fagr', True)\n    'fagr'\n\n    >>> add_r_ending_to_syllable('vitr', True)\n    'vitr'\n\n    >>> add_r_ending_to_syllable('vetr', True)\n    'vetr'\n\n    >>> add_r_ending_to_syllable('akr', True)\n    'akr'\n\n    >>> add_r_ending_to_syllable('Bj\u00f6rn', True)\n    'Bj\u00f6rn'\n\n    >>> add_r_ending_to_syllable('\u00feurs', True)\n    '\u00feurs'\n\n    >>> add_r_ending_to_syllable('karl', True)\n    'karl'\n\n    >>> add_r_ending_to_syllable('hrafn', True)\n    'hrafn'\n\n    :param last_syllable: last syllable of the word\n    :param is_first: is it the first syllable of the word?\n    :return: inflected syllable\n    \"\"\"\n    if len(last_syllable) >= 2:\n        if last_syllable[-1] in ['l', 'n', 's', 'r']:\n            if last_syllable[-2] in CONSONANTS:\n                # Apocope of r\n                return last_syllable\n            else:\n                # Assimilation of r\n                if len(last_syllable) >= 3 and last_syllable[-3:-1] in DIPHTHONGS:\n                    return apply_raw_r_assimilation(last_syllable)\n                elif last_syllable[-2] in SHORT_VOWELS and is_first:\n                    # No assimilation when r is supposed to be added to a stressed syllable\n                    # whose last letter is l, n or s and the penultimate letter is a short vowel\n                    return last_syllable + \"r\"\n                elif last_syllable[-2] in SHORT_VOWELS:\n                    return apply_raw_r_assimilation(last_syllable)\n                elif last_syllable[-2] in LONG_VOWELS:\n                    return apply_raw_r_assimilation(last_syllable)\n                return apply_raw_r_assimilation(last_syllable)\n        else:\n            return last_syllable + \"r\"\n    else:\n        return last_syllable + \"r\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an - r ending to an old norse noun.", "response": "def add_r_ending(stem: str) -> str:\n    \"\"\"\n    Adds an -r ending to an Old Norse noun.\n\n    >>> add_r_ending(\"arm\")\n    'armr'\n\n    >>> add_r_ending(\"\u00e1s\")\n    '\u00e1ss'\n\n    >>> add_r_ending(\"st\u00f3l\")\n    'st\u00f3ll'\n\n    >>> add_r_ending(\"j\u00f6kul\")\n    'j\u00f6kull'\n\n    >>> add_r_ending(\"stein\")\n    'steinn'\n\n    >>> add_r_ending('mikil')\n    'mikill'\n\n    >>> add_r_ending('s\u00e6l')\n    's\u00e6ll'\n\n    >>> add_r_ending('litil')\n    'litill'\n\n    >>> add_r_ending('v\u00e6nn')\n    'v\u00e6nn'\n\n    >>> add_r_ending('lauss')\n    'lauss'\n\n    >>> add_r_ending(\"vin\")\n    'vinr'\n\n    >>> add_r_ending(\"sel\")\n    'selr'\n\n    >>> add_r_ending('fagr')\n    'fagr'\n\n    >>> add_r_ending('vitr')\n    'vitr'\n\n    >>> add_r_ending('vetr')\n    'vetr'\n\n    >>> add_r_ending('akr')\n    'akr'\n\n    >>> add_r_ending('Bj\u00f6rn')\n    'bj\u00f6rn'\n\n    >>> add_r_ending('\u00feurs')\n    '\u00feurs'\n\n    >>> add_r_ending('karl')\n    'karl'\n\n    >>> add_r_ending('hrafn')\n    'hrafn'\n\n    :param stem:\n    :return:\n    \"\"\"\n    s_stem = s.syllabify_ssp(stem.lower())\n    n_stem = len(s_stem)\n    last_syllable = Syllable(s_stem[-1], VOWELS, CONSONANTS)\n    return \"\".join(s_stem[:-1]) + add_r_ending_to_syllable(last_syllable.text, n_stem == 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_u_umlaut(word: str) -> bool:\n    word_syl = s.syllabify_ssp(word)\n    s_word_syl = [Syllable(syl, VOWELS, CONSONANTS) for syl in word_syl]\n\n    if len(s_word_syl) == 1 and s_word_syl[-1].nucleus[0] in [\"\u00f6\", \"\u01eb\"]:\n        return True\n    elif len(s_word_syl) >= 2 and s_word_syl[-1].nucleus[0] == \"u\":\n        return s_word_syl[-2].nucleus[0] in [\"\u00f6\", \"\u01eb\"]\n    return False", "response": "Returns True if the word has an u - umlaut."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new object with the vowel of the last syllable of the given stem according to an i - umlaut.", "response": "def apply_i_umlaut(stem: str):\n    \"\"\"\n    Changes the vowel of the last syllable of the given stem according to an i-umlaut.\n\n    >>> apply_i_umlaut(\"m\u00e6l\")\n    'm\u00e6l'\n    >>> apply_i_umlaut(\"lag\u00f0\")\n    'leg\u00f0'\n    >>> apply_i_umlaut(\"vak\")\n    'vek'\n    >>> apply_i_umlaut(\"haf\")\n    'hef'\n    >>> apply_i_umlaut(\"bu\u00f0\")\n    'by\u00f0'\n    >>> apply_i_umlaut(\"b\u00e1r\")\n    'b\u00e6r'\n    >>> apply_i_umlaut(\"r\u00e9\u00f0\")\n    'r\u00e9\u00f0'\n    >>> apply_i_umlaut(\"f\u00f3r\")\n    'f\u0153r'\n\n    :param stem:\n    :return:\n    \"\"\"\n    assert len(stem) > 0\n    s_stem = s.syllabify_ssp(stem.lower())\n    last_syllable = OldNorseSyllable(s_stem[-1], VOWELS, CONSONANTS)\n    last_syllable.apply_i_umlaut()\n    return \"\".join(s_stem[:-1]) + str(last_syllable)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the vowel of the last syllable of the given stem if the vowel is affected by an u-umlaut. >>> apply_u_umlaut(\"far\") 'f\u00f6r' >>> apply_u_umlaut(\"\u00f6r\") '\u00f6r' >>> apply_u_umlaut(\"axl\") '\u00f6xl' >>> apply_u_umlaut(\"hafn\") 'h\u00f6fn' :param stem: :return:", "response": "def apply_u_umlaut(stem: str):\n    \"\"\"\n    Changes the vowel of the last syllable of the given stem if the vowel is affected by an u-umlaut.\n    >>> apply_u_umlaut(\"far\")\n    'f\u00f6r'\n    >>> apply_u_umlaut(\"\u00f6r\")\n    '\u00f6r'\n    >>> apply_u_umlaut(\"axl\")\n    '\u00f6xl'\n    >>> apply_u_umlaut(\"hafn\")\n    'h\u00f6fn'\n\n    :param stem:\n    :return:\n    \"\"\"\n    assert len(stem) > 0\n    s_stem = s.syllabify_ssp(stem.lower())\n    if len(s_stem) == 1:\n        last_syllable = OldNorseSyllable(s_stem[-1], VOWELS, CONSONANTS)\n        last_syllable.apply_u_umlaut()\n        return \"\".join(s_stem[:-1]) + str(last_syllable)\n\n    else:\n        penultimate_syllable = OldNorseSyllable(s_stem[-2], VOWELS, CONSONANTS)\n        last_syllable = OldNorseSyllable(s_stem[-1], VOWELS, CONSONANTS)\n        penultimate_syllable.apply_u_umlaut()\n        last_syllable.apply_u_umlaut(True)\n        last_syllable.apply_u_umlaut(True)\n        return \"\".join(s_stem[:-2]) + str(penultimate_syllable) + str(last_syllable)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ns_has_i_umlaut(ns: str, gs: str, np: str):\n\n    ns_syl = s.syllabify_ssp(ns)\n    gs_syl = s.syllabify_ssp(gs)\n    np_syl = s.syllabify_ssp(np)\n    s_ns_syl = [Syllable(syl, VOWELS, CONSONANTS) for syl in ns_syl]\n    s_gs_syl = [Syllable(syl, VOWELS, CONSONANTS) for syl in gs_syl]\n    s_np_syl = [Syllable(syl, VOWELS, CONSONANTS) for syl in np_syl]\n    if len(gs_syl) >= 2 and s_gs_syl[-1].nucleus[0] == \"i\":\n        if len(ns_syl) >= 2:\n            vowel = s_ns_syl[-2].nucleus[0]\n        else:\n            vowel = s_ns_syl[-1].nucleus[0]\n        return vowel in BACK_TO_FRONT_VOWELS and s_gs_syl[-2].nucleus[0] == BACK_TO_FRONT_VOWELS[vowel]\n\n    if len(np_syl) >= 2 and s_np_syl[-1].nucleus[0] == \"i\":\n        if len(ns_syl) >= 2:\n            vowel = s_ns_syl[-2].nucleus[0]\n        else:\n            vowel = s_ns_syl[-1].nucleus[0]\n        return vowel in BACK_TO_FRONT_VOWELS and s_np_syl[-2].nucleus[0] in BACK_TO_FRONT_VOWELS[vowel]\n\n    return False", "response": "Checks if the nominative singular has an i - umlaut."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscan a line of Latin hendecasyllables and produce a scansion pattern and other data.", "response": "def scan(self, original_line: str, optional_transform: bool = False) -> Verse:\n        \"\"\"\n        Scan a line of Latin hendecasyllables and produce a scansion pattern, and other data.\n\n        :param original_line: the original line of Latin verse\n        :param optional_transform: whether or not to perform i to j transform for syllabification\n        :return: a Verse object\n\n        >>> scanner = HendecasyllableScanner()\n        >>> print(scanner.scan(\"Cui dono lepidum novum libellum\"))\n        Verse(original='Cui dono lepidum novum libellum', scansion='  -  U -  U U -   U -   U -  U ', meter='hendecasyllable', valid=True, syllable_count=11, accented='Cui don\u014d lepid\u016bm nov\u016bm lib\u0113llum', scansion_notes=['Corrected invalid start.'], syllables = ['Cui', 'do', 'no', 'le', 'pi', 'd\u016bm', 'no', 'v\u016bm', 'li', 'b\u0113l', 'lum'])\n        >>> print(scanner.scan(\n        ... \"\u0101rida modo pumice expolitum?\").scansion)  # doctest: +NORMALIZE_WHITESPACE\n        - U -  U U  - U   -  U - U\n        \"\"\"\n        verse = Verse(original_line, meter='hendecasyllable')\n        # replace punctuation with spaces\n        line = original_line.translate(self.punctuation_substitutions)\n        # conservative i to j\n        line = self.transform_i_to_j(line)\n        working_line = self.elide_all(line)\n        working_line = self.accent_by_position(working_line)\n        syllables = self.syllabifier.syllabify(working_line)\n        if optional_transform:\n            working_line = self.transform_i_to_j_optional(line)\n            working_line = self.elide_all(working_line)\n            working_line = self.accent_by_position(working_line)\n            syllables = self.syllabifier.syllabify(working_line)\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"optional i to j\"]]\n        verse.working_line = working_line\n        verse.syllable_count = self.syllabifier.get_syllable_count(syllables)\n        verse.syllables = syllables\n        # identify some obvious and probably choices based on number of syllables\n        if verse.syllable_count > 11:\n            verse.valid = False\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"> 11\"]]\n            return verse\n        if verse.syllable_count < 11:\n            verse.valid = False\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"< 11\"]]\n            return verse\n\n        stresses = self.flag_dipthongs(syllables)\n        syllables_wspaces = string_utils.to_syllables_with_trailing_spaces(working_line, syllables)\n        offset_map = self.calc_offset(syllables_wspaces)\n        for idx, syl in enumerate(syllables):\n            for accented in self.constants.ACCENTED_VOWELS:\n                if accented in syl:\n                    stresses.append(idx)\n        # second to last syllable is always long\n        stresses.append(verse.syllable_count - 2)\n\n        verse.scansion = self.produce_scansion(stresses,\n                                               syllables_wspaces, offset_map)\n        if len(string_utils.stress_positions(self.constants.STRESSED, verse.scansion)) != \\\n                len(set(stresses)):\n            verse.valid = False\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"invalid syllables\"]]\n            return verse\n\n        if self.metrical_validator.is_valid_hendecasyllables(verse.scansion):\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"positionally\"]]\n            return self.assign_candidate(verse, verse.scansion)\n\n        smoothed = self.correct_invalid_start(verse.scansion)\n\n        if distance(verse.scansion, smoothed) > 0:\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"invalid start\"]]\n            verse.scansion = smoothed\n            stresses += string_utils.differences(verse.scansion, smoothed)\n\n        if self.metrical_validator.is_valid_hendecasyllables(verse.scansion):\n            return self.assign_candidate(verse, verse.scansion)\n\n        smoothed = self.correct_antepenult_chain(verse.scansion)\n\n        if distance(verse.scansion, smoothed) > 0:\n            verse.scansion_notes += [self.constants.NOTE_MAP[\"antepenult chain\"]]\n            verse.scansion = smoothed\n            stresses += string_utils.differences(verse.scansion, smoothed)\n\n        if self.metrical_validator.is_valid_hendecasyllables(verse.scansion):\n            return self.assign_candidate(verse, verse.scansion)\n\n        candidates = self.metrical_validator.closest_hendecasyllable_patterns(verse.scansion)\n        if candidates is not None:\n            if len(candidates) == 1 \\\n                    and len(verse.scansion.replace(\" \", \"\")) == len(candidates[0]) \\\n                    and len(string_utils.differences(verse.scansion, candidates[0])) == 1:\n                tmp_scansion = self.produce_scansion(\n                    string_utils.differences(verse.scansion, candidates[0]),\n                    syllables_wspaces, offset_map)\n                if self.metrical_validator.is_valid_hendecasyllables(tmp_scansion):\n                    verse.scansion_notes += [self.constants.NOTE_MAP[\"closest match\"]]\n                    return self.assign_candidate(verse, tmp_scansion)\n\n        # if the line doesn't scan \"as is\", if may scan if the optional i to j transformations\n        # are made, so here we set them and try again.\n        if self.optional_transform and not verse.valid:\n            return self.scan(original_line, optional_transform=True)\n\n        verse.accented = self.formatter.merge_line_scansion(\n            verse.original, verse.scansion)\n        return verse"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tag(self: object, tokens: List[str]):\n        tags = []\n        taggers = []\n        for i in range(len(tokens)):\n            tag, tagger = self.tag_one(tokens, i, tags)\n            tags.append(tag)\n            taggers.append(str(tagger)) if tag else taggers.append(None)\n\n        if self.VERBOSE:\n            return list(zip(tokens, tags, taggers))\n        else:\n            return list(zip(tokens, tags))", "response": "Tag a list of tokens with the current tagger."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlooking up token in ``lemmas`` dict and returns the corresponding value as lemma. :rtype: str :type tokens: list :param tokens: List of tokens to be lemmatized :type index: int :param index: Int with current token :type history: list :param history: List with tokens that have already been lemmatized; NOT USED", "response": "def choose_tag(self: object, tokens: List[str], index: int, history: List[str]):\n        \"\"\"\n        Looks up token in ``lemmas`` dict and returns the corresponding\n        value as lemma.\n        :rtype: str\n        :type tokens: list\n        :param tokens: List of tokens to be lemmatized\n        :type index: int\n        :param index: Int with current token\n        :type history: list\n        :param history: List with tokens that have already been lemmatized; NOT USED\n        \"\"\"\n        keys = self.lemmas.keys()\n        if tokens[index] in keys:\n            return self.lemmas[tokens[index]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nltk_tokenize_words(string, attached_period=False, language=None):\n    assert isinstance(string, str), \"Incoming string must be type str.\"\n    if language == 'sanskrit':\n        periods = ['.', '\u0964','\u0965']\n    else:\n        periods = ['.']\n    punkt = PunktLanguageVars()\n    tokens = punkt.word_tokenize(string)\n    if attached_period:\n        return tokens\n    new_tokens = []\n    for word in tokens:\n        for char in periods:\n            if word.endswith(char):\n                new_tokens.append(word[:-1])\n                new_tokens.append(char)\n                break\n        else:\n            new_tokens.append(word)\n    return new_tokens", "response": "Wrap NLTK s tokenizer PunktLanguageVars and make final period\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking a single line of text and returns all words in the line as a list of tuples.", "response": "def tokenize_akkadian_words(line):\n    \"\"\"\n    Operates on a single line of text, returns all words in the line as a\n    tuple in a list.\n\n    input: \"1. isz-pur-ram a-na\"\n    output: [(\"isz-pur-ram\", \"akkadian\"), (\"a-na\", \"akkadian\")]\n\n    :param: line: text string\n    :return: list of tuples: (word, language)\n    \"\"\"\n    beginning_underscore = \"_[^_]+(?!_)$\"\n    # only match a string if it has a beginning underscore anywhere\n    ending_underscore = \"^(?<!_)[^_]+_\"\n    # only match a string if it has an ending underscore anywhere\n    two_underscores = \"_[^_]+_\"\n    # only match a string if it has two underscores\n\n    words = line.split()\n    # split the line on spaces ignoring the first split (which is the\n    # line number)\n    language = \"akkadian\"\n    output_words = []\n    for word in words:\n        if re.search(two_underscores, word):\n            # If the string has two underscores in it then the word is\n            # in Sumerian while the neighboring words are in Akkadian.\n            output_words.append((word, \"sumerian\"))\n        elif re.search(beginning_underscore, word):\n            # If the word has an initial underscore somewhere\n            # but no other underscores than we're starting a block\n            # of Sumerian.\n            language = \"sumerian\"\n            output_words.append((word, language))\n        elif re.search(ending_underscore, word):\n            # If the word has an ending underscore somewhere\n            # but not other underscores than we're ending a block\n            # of Sumerian.\n            output_words.append((word, language))\n            language = \"akkadian\"\n        else:\n            # If there are no underscore than we are continuing\n            # whatever language we're currently in.\n            output_words.append((word, language))\n    return output_words"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tokenize_akkadian_signs(word):\n    word_signs = []\n    sign = ''\n    language = word[1]\n    determinative = False\n    for char in word[0]:\n        if determinative is True:\n            if char == '}':\n                determinative = False\n                if len(sign) > 0:  # pylint: disable=len-as-condition\n                    word_signs.append((sign, 'determinative'))\n                sign = ''\n                language = word[1]\n                continue\n            else:\n                sign += char\n                continue\n        else:\n            if language == 'akkadian':\n                if char == '{':\n                    if len(sign) > 0:  # pylint: disable=len-as-condition\n                        word_signs.append((sign, language))\n                    sign = ''\n                    determinative = True\n                    continue\n                elif char == '_':\n                    if len(sign) > 0:  # pylint: disable=len-as-condition\n                        word_signs.append((sign, language))\n                    sign = ''\n                    language = 'sumerian'\n                    continue\n                elif char == '-':\n                    if len(sign) > 0:  # pylint: disable=len-as-condition\n                        word_signs.append((sign, language))\n                    sign = ''\n                    language = word[1] # or default word[1]?\n                    continue\n                else:\n                    sign += char\n            elif language == 'sumerian':\n                if char == '{':\n                    if len(sign) > 0:  # pylint: disable=len-as-condition\n                        word_signs.append((sign, language))\n                    sign = ''\n                    determinative = True\n                    continue\n                elif char == '_':\n                    if len(sign) > 0:  # pylint: disable=len-as-condition\n                        word_signs.append((sign, language))\n                    sign = ''\n                    language = word[1]\n                    continue\n                elif char == '-':\n                    if len(sign) > 0:  # pylint: disable=len-as-condition\n                        word_signs.append((sign, language))\n                    sign = ''\n                    language = word[1]\n                    continue\n                else:\n                    sign += char\n    if len(sign) > 0:\n        word_signs.append((sign, language))\n\n    return word_signs", "response": "Takes a word and language and splits it up into individual words into individual words and returns a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tokenize_arabic_words(text):\n\n    \"\"\"\n        Tokenize text into words\n        @param text: the input text.\n        @type text: unicode.\n        @return: list of words.\n        @rtype: list.\n    \"\"\"\n    specific_tokens = []\n    if not text:\n        return specific_tokens\n    else:\n        specific_tokens = araby.tokenize(text)\n        return specific_tokens", "response": "Tokenize text into words\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tokenize_latin_words(string):\n    from cltk.tokenize.latin_exceptions import latin_exceptions\n\n    assert isinstance(string, str), \"Incoming string must be type str.\"\n\n    def matchcase(word):\n        # From Python Cookbook\n        def replace(m):\n            text = m.group()\n            if text.isupper():\n                return word.upper()\n            elif text.islower():\n                return word.lower()\n            elif text[0].isupper():\n                return word.capitalize()\n            else:\n                return word\n\n        return replace\n\n    replacements = [(r'\\bmecum\\b', 'cum me'),\n                    (r'\\btecum\\b', 'cum te'),\n                    (r'\\bsecum\\b', 'cum se'),\n                    (r'\\bnobiscum\\b', 'cum nobis'),\n                    (r'\\bvobiscum\\b', 'cum vobis'),\n                    (r'\\bquocum\\b', 'cum quo'),\n                    (r'\\bquacum\\b', 'cum qua'),\n                    (r'\\bquicum\\b', 'cum qui'),\n                    (r'\\bquibuscum\\b', 'cum quibus'),\n                    (r'\\bsodes\\b', 'si audes'),\n                    (r'\\bsatin\\b', 'satis ne'),\n                    (r'\\bscin\\b', 'scis ne'),\n                    (r'\\bsultis\\b', 'si vultis'),\n                    (r'\\bsimilist\\b', 'similis est'),\n                    (r'\\bqualist\\b', 'qualis est')\n                    ]\n\n    for replacement in replacements:\n        string = re.sub(replacement[0], matchcase(replacement[1]), string, flags=re.IGNORECASE)\n\n    punkt_param = PunktParameters()\n    abbreviations = ['c', 'l', 'm', 'p', 'q', 't', 'ti', 'sex', 'a', 'd', 'cn', 'sp', \"m'\", 'ser', 'ap', 'n',\n                     'v', 'k', 'mam', 'post', 'f', 'oct', 'opet', 'paul', 'pro', 'sert', 'st', 'sta', 'v', 'vol', 'vop']\n    punkt_param.abbrev_types = set(abbreviations)\n    sent_tokenizer = PunktSentenceTokenizer(punkt_param)\n\n    word_tokenizer = PunktLanguageVars()\n    sents = sent_tokenizer.tokenize(string)\n\n    enclitics = ['que', 'n', 'ue', 've', 'st']\n    exceptions = enclitics\n    exceptions = list(set(exceptions + latin_exceptions))\n\n    tokens = []\n\n    for sent in sents:\n        temp_tokens = word_tokenizer.word_tokenize(sent)\n        # Need to check that tokens exist before handling them;\n        # needed to make stream.readlines work in PlaintextCorpusReader\n\n        if temp_tokens:\n            if temp_tokens[0].endswith('ne') and len(temp_tokens[0]) > 2:\n                if temp_tokens[0].lower() not in exceptions:\n                    temp = [temp_tokens[0][:-2], '-ne']\n                    temp_tokens = temp + temp_tokens[1:]\n\n            if temp_tokens[-1].endswith('.'):\n                final_word = temp_tokens[-1][:-1]\n                del temp_tokens[-1]\n                temp_tokens += [final_word, '.']\n\n            for token in temp_tokens:\n                tokens.append(token)\n\n    # Break enclitic handling into own function?\n    specific_tokens = []\n\n    for token in tokens:\n        is_enclitic = False\n        if token.lower() not in exceptions:\n            for enclitic in enclitics:\n                if token.endswith(enclitic):\n                    if enclitic == 'n':\n                        specific_tokens += [token[:-len(enclitic)]] + ['-ne']\n                    elif enclitic == 'st':\n                        if token.endswith('ust'):\n                            specific_tokens += [token[:-len(enclitic) + 1]] + ['est']\n                        else:\n                            specific_tokens += [token[:-len(enclitic)]] + ['est']\n                    else:\n                        specific_tokens += [token[:-len(enclitic)]] + ['-' + enclitic]\n                    is_enclitic = True\n                    break\n        if not is_enclitic:\n            specific_tokens.append(token)\n\n    return specific_tokens", "response": "Tokenizes a string into a list of substrings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_import_source():\n        path_rel = '~/cltk_data/greek/software/greek_software_tlgu/tlgu.h'\n        path = os.path.expanduser(path_rel)\n        if not os.path.isfile(path):\n            try:\n                corpus_importer = CorpusImporter('greek')\n                corpus_importer.import_corpus('greek_software_tlgu')\n            except Exception as exc:\n                logger.error('Failed to import TLGU: %s', exc)\n                raise", "response": "Check if tlgu imported if not import it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if tlgu installed if not install it.", "response": "def _check_install(self):\n        \"\"\"Check if tlgu installed, if not install it.\"\"\"\n        try:\n            subprocess.check_output(['which', 'tlgu'])\n        except Exception as exc:\n            logger.info('TLGU not installed: %s', exc)\n            logger.info('Installing TLGU.')\n            if not subprocess.check_output(['which', 'gcc']):\n                logger.error('GCC seems not to be installed.')\n            else:\n                tlgu_path_rel = '~/cltk_data/greek/software/greek_software_tlgu'\n                tlgu_path = os.path.expanduser(tlgu_path_rel)\n                if not self.testing:\n                    print('Do you want to install TLGU?')\n                    print('To continue, press Return. To exit, Control-C.')\n                    input()\n                else:\n                    print('Automated or test build, skipping keyboard input confirmation for installation of TLGU.')\n                try:\n                    command = 'cd {0} && make install'.format(tlgu_path)\n                    print('Going to run command:', command)\n                    p_out = subprocess.call(command, shell=True)\n                    if p_out == 0:\n                        logger.info('TLGU installed.')\n                    else:\n                        logger.error('TLGU install without sudo failed.')\n                except Exception as exc:\n                    logger.error('TLGU install failed: %s', exc)\n                else:  # for Linux needing root access to '/usr/local/bin'\n                    if not self.testing:\n                        print('Could not install without root access. Do you want to install TLGU with sudo?')\n                        command = 'cd {0} && sudo make install'.format(tlgu_path)\n                        print('Going to run command:', command)\n                        print('To continue, press Return. To exit, Control-C.')\n                        input()\n                        p_out = subprocess.call(command, shell=True)\n                    else:\n                        command = 'cd {0} && sudo make install'.format(tlgu_path)\n                        p_out = subprocess.call(command, shell=True)\n                    if p_out == 0:\n                        logger.info('TLGU installed.')\n                    else:\n                        logger.error('TLGU install with sudo failed.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a TLG file to a new file.", "response": "def convert(self, input_path=None, output_path=None, markup=None,\n                break_lines=False, divide_works=False, latin=False,\n                extra_args=None):\n        \"\"\"\n        :param input_path: TLG filepath to convert.\n        :param output_path: filepath of new converted text.\n        :param markup: Specificity of inline markup. Default None removes all\n        numerical markup; 'full' gives most detailed, with reference numbers\n        included before each text line.\n        :param break_lines: No spaces; removes line ends and hyphens before an\n         ID code; hyphens and spaces before page and column ends are retained.\n        :param divide_works: Each work (book) is output as a separate file in\n        the form output_file-xxx.txt; if an output file is not specified, this\n         option has no effect.\n        :param latin: Primarily Latin text (PHI). Some TLG texts, notably\n        doccan1.txt and doccan2.txt are mostly roman texts lacking explicit\n        language change codes. Setting this option will force a change to\n        Latin text after each citation block is encountered.\n        :param extra_args: Any other tlgu args to be passed, in list form and\n        without dashes, e.g.: ['p', 'b', 'B'].\n        \"\"\"\n        # setup file paths\n        input_path = os.path.expanduser(input_path)\n        output_path = os.path.expanduser(output_path)\n\n        # check input path exists\n        assert os.path.isfile(input_path), 'File {0} does not exist.'.format(input_path)\n\n        # setup tlgu flags\n        tlgu_options = []\n        if markup == 'full':\n            full_args = ['v', 'w', 'x', 'y', 'z']\n            [tlgu_options.append(x) for x in full_args]  # pylint: disable=W0106\n        if break_lines:\n            tlgu_options.append('N')\n        if divide_works:\n            tlgu_options.append('W')\n        if latin:\n            tlgu_options.append('r')\n        # setup extra args\n        if extra_args is None:\n            extra_args = []\n        else:\n            try:\n                extra_args = list(extra_args)\n            except Exception as exc:\n                logger.error(\"Argument 'extra_args' must be a list: %s.\", exc)\n                raise\n        tlgu_options = tlgu_options + extra_args\n        # assemble all tlgu flags\n        tlgu_options = list(set(tlgu_options))\n        if tlgu_options:\n            tlgu_flags = '-' + ' -'.join(tlgu_options)\n        else:\n            tlgu_flags = ''\n        # make tlgu call\n        tlgu_call = 'tlgu {0} {1} {2}'.format(tlgu_flags,\n                                              input_path,\n                                              output_path)\n        logger.info(tlgu_call)\n        try:\n            p_out = subprocess.call(tlgu_call, shell=True)\n            if p_out == 1:\n                logger.error('Failed to convert %s to %s.',\n                             input_path,\n                             output_path)\n        except Exception as exc:\n            logger.error('Failed to convert %s to %s: %s',\n                         input_path,\n                         output_path,\n                         exc)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_corpus(self, corpus, markup=None, break_lines=False, divide_works=False, latin=None, extra_args=None):  # pylint: disable=W0613\n        orig_path_rel = '~/cltk_data/originals'\n        orig_path = os.path.expanduser(orig_path_rel)\n        target_path_rel = '~/cltk_data'\n        target_path = os.path.expanduser(target_path_rel)\n        assert corpus in ['tlg', 'phi5', 'phi7'], \"Corpus must be 'tlg', 'phi5', or 'phi7'\"\n        if corpus in ['tlg', 'phi5', 'phi7']:\n            orig_path = os.path.join(orig_path, corpus)\n            if corpus in ['tlg', 'phi7']:\n                if 'phi7' and latin is True:\n                    latin = True\n                    target_path = os.path.join(target_path, 'latin', 'text', corpus)\n                else:\n                    latin = None\n                    target_path = os.path.join(target_path, 'greek', 'text', corpus)\n            else:\n                target_path = os.path.join(target_path, 'latin', 'text', corpus)\n                latin = True\n        try:\n            corpus_files = os.listdir(orig_path)\n        except Exception as exception:\n            logger.error(\"Failed to find TLG files: %s\", exception)\n            raise\n        # make a list of files to be converted\n        txts = []\n        [txts.append(x) for x in corpus_files if x.endswith('TXT')]  # pylint: disable=W0106\n        # loop through list and convert one at a time\n        for txt in txts:\n            orig_txt_path = os.path.join(orig_path, txt)\n            if markup is None:\n                target_txt_dir = os.path.join(target_path, 'plaintext')\n            else:\n                target_txt_dir = os.path.join(target_path, str(markup))\n            if not os.path.isdir(target_txt_dir):\n                os.makedirs(target_txt_dir)\n            target_txt_path = os.path.join(target_txt_dir, txt)\n            try:\n                self.convert(orig_txt_path, target_txt_path, markup=None,\n                             break_lines=False, divide_works=False, latin=latin,\n                             extra_args=None)\n            except Exception as exception:\n                logger.error(\"Failed to convert file '%s' to '%s': %s\", orig_txt_path, target_txt_path, exception)", "response": "Convert a corpus to a TLG or PHI file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndivide the work - breaks into a set of files.", "response": "def divide_works(self, corpus):\n        \"\"\"Use the work-breaking option.\n        TODO: Maybe incorporate this into ``convert_corpus()``\n        TODO: Write test for this\n        \"\"\"\n        if corpus == 'tlg':\n            orig_dir_rel = '~/cltk_data/originals/tlg'\n            works_dir_rel = '~/cltk_data/greek/text/tlg/individual_works'\n            file_prefix = 'TLG'\n            latin = False\n        elif corpus == 'phi5':\n            orig_dir_rel = '~/cltk_data/originals/phi5'\n            works_dir_rel = '~/cltk_data/latin/text/phi5/individual_works'\n            file_prefix = 'LAT'\n            latin = True  # this is for the optional TLGU argument to convert()\n\n        orig_dir = os.path.expanduser(orig_dir_rel)\n        works_dir = os.path.expanduser(works_dir_rel)\n        if not os.path.exists(works_dir):\n            os.makedirs(works_dir)\n\n        files = os.listdir(orig_dir)\n        texts = [x for x in files if x.endswith('.TXT') and x.startswith(file_prefix)]\n\n        for file in texts:\n            orig_file_path = os.path.join(orig_dir, file)\n            new_file_path = os.path.join(works_dir, file)\n\n            try:\n                self.convert(orig_file_path, new_file_path, divide_works=True, latin=latin)\n                logger.info('Writing files at %s to %s.', orig_file_path, works_dir)\n            except Exception as err:\n                logger.error('Failed to convert files: %s.', err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves edge punctuation from a single string.", "response": "def drop_edge_punct(word: str) -> str:\n    \"\"\"\n    Remove edge punctuation.\n    :param word: a single string\n    >>> drop_edge_punct(\"'fieri\")\n    'fieri'\n    >>> drop_edge_punct('sedes.')\n    'sedes'\n    \"\"\"\n    if not word:\n        return word\n    try:\n        if not word[0].isalpha():\n            word = word[1:]\n        if not word[-1].isalpha():\n            word = word[:-1]\n    except:\n        pass\n    return word"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_camel(word: str) -> str:\n    m = re.match('[a-z]+[A-Z][a-z]', word)\n    if m:\n        _, end = m.span()\n        return word[:end - 2] + ' ' + word[end - 2:]\n    return word", "response": "Separate any words joined in Camel case fashion using a single space."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform macronized vowels into normal vowels.", "response": "def demacronize(string_matrix: List[List[str]]) -> List[List[str]]:\n    \"\"\"\n    Transform macronized vowels into normal vowels\n    :param string_matrix: a data matrix: a list wrapping a list of strings, with each sublist being a sentence.\n    :return: string_matrix\n    >>> demacronize([['\u014dd\u012b', 'et', 'am\u014d',]])\n    [['odi', 'et', 'amo']]\n    \"\"\"\n    scansion = ScansionConstants()\n    accent_dropper = str.maketrans(scansion.ACCENTED_VOWELS, scansion.VOWELS)\n    return [[word.translate(accent_dropper)\n             for word in sentence]\n            for sentence in string_matrix]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef divide_separate_words(string_matrix: List[List[str]]) -> List[List[str]]:\n    new_X = []\n    for sentence in string_matrix:\n        data_row = []  # type: List[str]\n        for word in sentence:\n            if ' ' in word:\n                data_row += word.split()\n            else:\n                data_row.append(word)\n        new_X.append(data_row)\n    return new_X", "response": "As part of processing some words obviously need to be separated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndrop punctuation that occurs on the edges of words.", "response": "def drop_fringe_punctuation(string_matrix: List[List[str]]) -> List[List[str]]:\n    \"\"\"\n    Drop punctuation that occurs on the edges of words.\n    :param string_matrix: a data matrix: a list wrapping a list of strings, with each sublist being a sentence.\n    >>> drop_fringe_punctuation([['this' , 'is', 'cool!'], [\"'But\", 'so', 'is', 'this'] ])\n    [['this', 'is', 'cool'], ['But', 'so', 'is', 'this']]\n    \"\"\"\n    return [[drop_edge_punct(word)\n             for word in sentence]\n            for sentence in string_matrix]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_middle_english(text, to_lower=True, alpha_conv=True, punct=True):\n\n    if to_lower:\n        text = text.lower()\n\n    if alpha_conv:\n        text = text.replace(\"\u00e6\", \"ae\").replace(\"\u00fe\", \"th\").replace(\"\u00f0\", \"th\")\n        text = re.sub(r'(?<!\\w)(?=\\w)\u021d', 'y', text)\n        text = text.replace(\"\u021d\", \"gh\")\n\n    if punct:\n        text = re.sub(r\"[\\.\\\";\\,\\:\\[\\]\\(\\)!&?\u2018]\", \"\", text)\n\n    return text", "response": "normalize the middle of a string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit input Latin word into a list of syllables based on the language dictionary and returns a list of syllables.", "response": "def syllabify(self, word):\n        \"\"\"Splits input Latin word into a list of syllables, based on \n        the language syllables loaded for the Syllabifier instance\"\"\"\n\n        prefixes = self.language['single_syllable_prefixes']\n        prefixes.sort(key=len, reverse=True)\n\n        # Check if word is in exception dictionary\n        if word in self.language['exceptions']:\n            syllables = self.language['exceptions'][word]\n\n        # Else, breakdown syllables for word\n        else:\n            syllables = []\n\n            # Remove prefixes\n            for prefix in prefixes:\n                if word.startswith(prefix):\n                    syllables.append(prefix)\n                    word = re.sub('^%s' % prefix, '', word)\n                    break\n\n            # Initialize syllable to build by iterating through over characters\n            syllable = ''\n\n            # Get word length for determining character position in word\n            word_len = len(word)\n\n            # Iterate over characters to build syllables\n            for i, char in enumerate(word):\n\n                # Build syllable\n                syllable = syllable + char\n                syllable_complete = False\n\n                # Checks to process syllable logic\n                char_is_vowel = self._is_vowel(char)\n                has_next_char = i < word_len - 1\n                has_prev_char = i > 0\n\n                # If it's the end of the word, the syllable is complete\n                if not has_next_char:\n                    syllable_complete = True\n\n                else:\n                    next_char = word[i + 1]\n                    if has_prev_char:\n                        prev_char = word[i - 1]\n\n                    # 'i' is a special case for a vowel. when i is at the \n                    # beginning of the word (Iesu) or i is between \n                    # vowels (alleluia), then the i is treated as a \n                    # consonant (y) Note: what about compounds like 'adiungere'\n                    if char == 'i' and has_next_char and self._is_vowel(next_char): \n                        if i == 0:\n                            char_is_vowel = False\n                        elif self._is_vowel(prev_char):\n                            char_is_vowel = False\n\n                    # Determine if the syllable is complete\n                    if char_is_vowel:\n\n                        if (\n                                    (  # If the next character's a vowel\n                                       self._is_vowel(\n                                               next_char)  # And it doesn't compose a dipthong with the current character\n                                       and not self._is_diphthong(char,\n                                                                  next_char)  # And the current character isn't preceded by a q, unless followed by a u\n                                       and not (\n                                                           has_prev_char\n                                                       and prev_char == \"q\"\n                                                   and char == \"u\"\n                                               and next_char != \"u\"\n                                       )\n\n                                       )\n                                or (\n                                        # If the next character's a consonant but not a double consonant, unless it's a mute consonant followed by a liquid consonant\n                                        i < word_len - 2\n                                        and (\n                                                        (\n                                                                    (\n                                                                                        has_prev_char\n                                                                                    and prev_char != \"q\"\n                                                                                and char == \"u\"\n                                                                            and self._is_vowel(word[i + 2])\n                                                                    )\n                                                                or (\n                                                                                not has_prev_char\n                                                                            and char == \"u\"\n                                                                        and self._is_vowel(word[i + 2])\n                                                                )\n                                                        )\n                                                    or (\n                                                                        char != \"u\"\n                                                                and self._is_vowel(word[i + 2])\n                                                            and not self._is_diphthong(char, next_char)\n                                                    )\n                                                or (\n                                                            self._is_mute_consonant_or_f(next_char)\n                                                        and self._is_liquid_consonant(word[i + 2])\n                                                )\n                                        )\n\n                                )\n                        ):\n                            syllable_complete = True\n\n                    # Otherwise, it's a consonant\n                    else:\n\n                        if (  # If the next character's also a consonant (but it's not the last in the word)\n                              (\n\n                                          not self._is_vowel(next_char)\n                                      and i < word_len - 2\n                              )  # If the char's not a mute consonant followed by a liquid consonant\n                              and not (\n                                          self._is_mute_consonant_or_f(char)\n                                      and self._is_liquid_consonant(next_char)\n                              )  # If the char's not a c, p, or t followed by an h\n                              and not (\n                                          (\n                                                              has_prev_char\n                                                          and not self._is_vowel(prev_char)\n                                                      and char in ['c', 'p', 't'] and next_char == 'h'\n                                          )\n                                      or (\n                                                      not has_prev_char\n                                                  and char in ['c', 'p', 't'] and next_char == 'h'\n                                      )\n                              )  # And it's not the only letter in the syllable\n                              and not len(syllable) == 1\n\n                              ):\n                            syllable_complete = True\n\n                # If it's a complete syllable, append it to syllables list and reset syllable\n                if syllable_complete:\n                    syllables.append(syllable)\n                    syllable = ''\n\n        return syllables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clean_text(self, text):\n        clean = []\n        for char in text:\n            if char in self.punc_stops:\n                clean += '.'\n            elif char not in self.punc:\n                clean += char\n            else:\n                pass\n        return (''.join(clean)).lower()", "response": "Clean the text of extraneous punction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clean_accents(self, text):\n        accents = {\n            '\u1f72\u03ad\u1f10\u1f11\u1f12\u1f13\u1f15\u1f14': '\u03b5',\n            '\u1f7a\u03cd\u1f51\u1f50\u1f52\u1f53\u1f54\u1f55': '\u03c5',\n            '\u1f78\u03cc\u1f40\u1f41\u1f42\u1f43\u1f44\u1f45': '\u03bf',\n            '\u1f76\u03af\u1f30\u1f31\u1f32\u1f33\u1f35\u1f34': '\u03b9',\n            '\u1f70\u03ac\u1f01\u1f00\u1f02\u1f03\u1f05\u1f04\u1fb3\u1f82\u1f83': '\u03b1',\n            '\u1f74\u03ae\u1f20\u1f21\u1f22\u1f23\u1f25\u1f24\u1f27\u1f26\u1fc6\u1fc4\u1fc2\u1fc7\u1fc3\u1f93\u1f92\u1f97\u1f96\u1f91\u1f90': '\u03b7',\n            '\u1f7c\u03ce\u1f60\u1f61\u1f62\u1f63\u1f64\u1f65\u1f66\u1f67\u1ff6\u1ff2\u1ff4\u1ff7\u1ff3\u1fa7\u1fa6\u1fa2\u1fa3\u1fa1\u1fa0': '\u03c9',\n            '\u1f36\u1f37': '\u1fd6',\n            '\u1f06\u1f07\u1fb7\u1f86\u1f87': '\u1fb6',\n            '\u1f56\u1f57': '\u1fe6',\n            }\n        text = self._clean_text(text)\n        for char in text:\n            for key in accents.keys():\n                if char in key:\n                    text = text.replace(char, accents.get(key))\n                else:\n                    pass\n        return text", "response": "Remove most accent marks from the given text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _tokenize(self, text):\n        sentences = []\n        tokens = []\n        for word in self._clean_accents(text).split(' '):\n            tokens.append(word)\n            if '.' in word:\n                sentences.append(tokens)\n                tokens = []\n        return sentences", "response": "Tokenize the text into a list of sentences with a list of words."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _long_by_nature(self, syllable):\n        # Find diphthongs\n        vowel_group = []\n        for char in syllable:\n            print\n            if char in self.long_vowels:\n                return True\n            elif char not in self.sing_cons and char not in self.doub_cons:\n                vowel_group += char\n\n        if ''.join(vowel_group) in self.diphthongs:\n            return True", "response": "Check if a syllable is long by nature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a syllable is long by position.", "response": "def _long_by_position(self, syllable, sentence):\n        \"\"\"Check if syllable is long by position.\n\n        Long by position includes:\n        1) Next syllable begins with two consonants, unless those consonants\n        are a stop + liquid combination\n        2) Next syllable begins with a double consonant\n        3) Syllable ends with a consonant and the next syllable begins with a\n        consonant\n        :param syllable: Current syllable\n        :param sentence: Current sentence\n        :return: True if syllable is long by position\n        :rtype : bool\n        \"\"\"\n        try:\n            next_syll = sentence[sentence.index(syllable) + 1]\n            # Long by position by case 1\n            if (next_syll[0] in self.sing_cons and next_syll[1] in\n                    self.sing_cons) and (next_syll[0] not in self.stops and\n                                         next_syll[1] not in self.liquids):\n                return True\n            # Long by position by case 2\n            elif syllable[-1] in self.vowels and next_syll[0] in self.doub_cons:\n                return True\n            # Long by position by case 3\n            elif syllable[-1] in self.sing_cons and (next_syll[0] in self.sing_cons):\n                return True\n            else:\n                pass\n        except IndexError:\n            logger.info(\"IndexError while checking if syllable '%s' is long. Continuing.\", syllable)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scan_text(self, input_string):\n        syllables = self._make_syllables(input_string)\n        sentence_syllables = self._syllable_condenser(syllables)\n        meter = self._scansion(sentence_syllables)\n        return meter", "response": "The primary method for the class. This method returns a list of the meter of the macronized text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stem(self, text):\n\n        stemmed_text = ''\n\n        for word in text.split(' '):\n            if word not in self.stops:\n\n                # remove '-que' suffix\n                word, in_que_pass_list = self._checkremove_que(word)\n                if not in_que_pass_list:\n\n                    # remove the simple endings from the target word\n                    word, was_stemmed = self._matchremove_simple_endings(word)\n\n                    # if word didn't match the simple endings, try verb endings\n                    if not was_stemmed:\n                        word = self._matchremove_verb_endings(word)\n\n            # add the stemmed word to the text\n            stemmed_text += word + ' '\n\n\n        return stemmed_text", "response": "Stem each word of the Latin text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the given word ends in - que and strips - que.", "response": "def _checkremove_que(self, word):\n        \"\"\"If word ends in -que and if word is not in pass list, strip -que\"\"\"\n\n        in_que_pass_list = False\n\n        que_pass_list = ['atque',\n                        'quoque',\n                        'neque',\n                        'itaque',\n                        'absque',\n                        'apsque',\n                        'abusque',\n                        'adaeque',\n                        'adusque',\n                        'denique',\n                        'deque',\n                        'susque',\n                        'oblique',\n                        'peraeque',\n                        'plenisque',\n                        'quandoque',\n                        'quisque',\n                        'quaeque',\n                        'cuiusque',\n                        'cuique',\n                        'quemque',\n                        'quamque',\n                        'quaque',\n                        'quique',\n                        'quorumque',\n                        'quarumque',\n                        'quibusque',\n                        'quosque',\n                        'quasque',\n                        'quotusquisque',\n                        'quousque',\n                        'ubique',\n                        'undique',\n                        'usque',\n                        'uterque',\n                        'utique',\n                        'utroque',\n                        'utribique',\n                        'torque',\n                        'coque',\n                        'concoque',\n                        'contorque',\n                        'detorque',\n                        'decoque',\n                        'excoque',\n                        'extorque',\n                        'obtorque',\n                        'optorque',\n                        'retorque',\n                        'recoque',\n                        'attorque',\n                        'incoque',\n                        'intorque',\n                        'praetorque']\n\n        if word not in que_pass_list:\n            word = re.sub(r'que$', '', word)\n        else:\n            in_que_pass_list = True\n\n        return word, in_que_pass_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _matchremove_simple_endings(self, word):\n\n        was_stemmed = False\n\n        # noun, adjective, and adverb word endings sorted by charlen, then alph\n        simple_endings = ['ibus',\n                            'ius',\n                            'ae',\n                            'am',\n                            'as',\n                            'em',\n                            'es',\n                            'ia',\n                            'is',\n                            'nt',\n                            'os',\n                            'ud',\n                            'um',\n                            'us',\n                            'a',\n                            'e',\n                            'i',\n                            'o',\n                            'u']\n\n        for ending in simple_endings:\n            if word.endswith(ending):\n                word = re.sub(r'{0}$'.format(ending), '', word)\n                was_stemmed = True\n                break\n\n        return word, was_stemmed", "response": "Remove the noun adjective and adverb word endings from the word."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _matchremove_verb_endings(self, word):\n\n        i_verb_endings = ['iuntur',\n                            'erunt',\n                            'untur',\n                            'iunt',\n                            'unt']\n\n        bi_verb_endings = ['beris',\n                            'bor',\n                            'bo']\n\n        eri_verb_endings = ['ero']\n\n        verb_endings = ['mini',\n                            'ntur',\n                            'stis',\n                            'mur',\n                            'mus',\n                            'ris',\n                            'sti',\n                            'tis',\n                            'tur',\n                            'ns',\n                            'nt',\n                            'ri',\n                            'm',\n                            'r',\n                            's',\n                            't']\n\n        # replace i verb endings with i\n        for ending in i_verb_endings:\n            if word.endswith(ending):\n                word = re.sub(r'{0}$'.format(ending), 'i', word)\n                return word\n\n        # replace bi verb endings with bi\n        for ending in bi_verb_endings:\n            if word.endswith(ending):\n                word = re.sub(r'{0}$'.format(ending), 'bi', word)\n                return word\n\n        # replace eri verb endings with eri\n        for ending in eri_verb_endings:\n            if word.endswith(ending):\n                word = re.sub(r'{0}$'.format(ending), 'eri', word)\n                return word\n\n        # otherwise, remove general verb endings\n        for ending in verb_endings:\n            if word.endswith(ending):\n                word = re.sub(r'{0}$'.format(ending), '', word)\n                break\n\n        return word", "response": "Remove the verb endings from the word if they are present in the word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace method. Note that regex. subn returns a tuple. Note that regex. subn returns a tuple.", "response": "def beta_code(self, text):\n        \"\"\"Replace method. Note: regex.subn() returns a tuple (new_string,\n        number_of_subs_made).\n        \"\"\"\n        text = text.upper().replace('-', '')\n        for (pattern, repl) in self.pattern1:\n            text = pattern.subn(repl, text)[0]\n        for (pattern, repl) in self.pattern2:\n            text = pattern.subn(repl, text)[0]\n        # remove third run, if punct list not used\n        for (pattern, repl) in self.pattern3:\n            text = pattern.subn(repl, text)[0]\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef syllabify(self, words: str) -> List[str]:\n        cleaned = words.translate(self.remove_punct_map)\n        cleaned = cleaned.replace(\"qu\", \"kw\")\n        cleaned = cleaned.replace(\"Qu\", \"Kw\")\n        cleaned = cleaned.replace(\"gua\", \"gwa\")\n        cleaned = cleaned.replace(\"Gua\", \"Gwa\")\n        cleaned = cleaned.replace(\"gue\", \"gwe\")\n        cleaned = cleaned.replace(\"Gue\", \"Gwe\")\n        cleaned = cleaned.replace(\"gui\", \"gwi\")\n        cleaned = cleaned.replace(\"Gui\", \"Gwi\")\n        cleaned = cleaned.replace(\"guo\", \"gwo\")\n        cleaned = cleaned.replace(\"Guo\", \"Gwo\")\n        cleaned = cleaned.replace(\"guu\", \"gwu\")\n        cleaned = cleaned.replace(\"Guu\", \"Gwu\")\n        cleaned = cleaned.replace(\"gu\u0101\", \"gw\u0101\")\n        cleaned = cleaned.replace(\"Gu\u0101\", \"Gw\u0101\")\n        cleaned = cleaned.replace(\"gu\u0113\", \"gw\u0113\")\n        cleaned = cleaned.replace(\"Gu\u0113\", \"Gw\u0113\")\n        cleaned = cleaned.replace(\"gu\u012b\", \"gw\u012b\")\n        cleaned = cleaned.replace(\"Gu\u012b\", \"Gw\u012b\")\n        cleaned = cleaned.replace(\"gu\u014d\", \"gw\u014d\")\n        cleaned = cleaned.replace(\"Gu\u014d\", \"Gw\u014d\")\n        cleaned = cleaned.replace(\"gu\u016b\", \"gw\u016b\")\n        cleaned = cleaned.replace(\"Gu\u016b\", \"Gw\u016b\")\n        items = cleaned.strip().split(\" \")\n\n        for char in cleaned:\n            if not char in self.ACCEPTABLE_CHARS:\n                LOG.error(\"Unsupported character found in %s \" % cleaned)\n                return items\n        syllables: list = []\n        for item in items:\n            syllables += self._setup(item)\n        for idx, syl in enumerate(syllables):\n            if \"kw\" in syl:\n                syl = syl.replace(\"kw\", \"qu\")\n                syllables[idx] = syl\n            if \"Kw\" in syl:\n                syl = syl.replace(\"Kw\", \"Qu\")\n                syllables[idx] = syl\n            if \"gw\" in syl:\n                syl = syl.replace(\"gw\", \"gu\")\n                syllables[idx] = syl\n            if \"Gw\" in syl:\n                syl = syl.replace(\"Gw\", \"Gu\")\n                syllables[idx] = syl\n\n        return string_utils.remove_blank_spaces(syllables)", "response": "Parse a list of words into a list of syllable strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares a word for syllable processing.", "response": "def _setup(self, word) -> List[str]:\n        \"\"\"\n        Prepares a word for syllable processing.\n\n        If the word starts with a prefix, process it separately.\n        :param word:\n        :return:\n        \"\"\"\n        if len(word) == 1:\n            return [word]\n        for prefix in self.constants.PREFIXES:\n            if word.startswith(prefix):\n                (first, rest) = string_utils.split_on(word, prefix)\n                if self._contains_vowels(rest):\n                    return string_utils.remove_blank_spaces(\n                        self._process(first) + self._process(rest))\n                # a word like pror can happen from ellision\n                return string_utils.remove_blank_spaces(self._process(word))\n        if word in self.constants.UI_EXCEPTIONS.keys():\n            return self.constants.UI_EXCEPTIONS[word]\n        return string_utils.remove_blank_spaces(self._process(word))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_consonantal_i(self, word) -> str:\n        match = list(self.consonantal_i_matcher.finditer(word))\n        if match:\n            if word[0].isupper():\n                return \"J\" + word[1:]\n            return \"j\" + word[1:]\n        return word", "response": "Convert i to j when at the start of a word."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process(self, word: str) -> List[str]:\n        #   if a blank arrives from splitting, just return an empty list\n        if len(word.strip()) == 0:\n            return []\n        word = self.convert_consonantal_i(word)\n        my_word = \" \" + word + \" \"\n        letters = list(my_word)\n        positions = []\n        for dipth in self.diphthongs:\n            if dipth in my_word:\n                dipth_matcher = re.compile(\"{}\".format(dipth))\n                matches = dipth_matcher.finditer(my_word)\n                for match in matches:\n                    (start, end) = match.span()\n                    positions.append(start)\n        matches = self.kw_matcher.finditer(my_word)\n        for match in matches:\n            (start, end) = match.span()\n            positions.append(start)\n        letters = string_utils.merge_next(letters, positions)\n        letters = string_utils.remove_blanks(letters)\n        positions.clear()\n        if not self._contains_vowels(\"\".join(letters)):\n            return [\"\".join(letters).strip()]  # occurs when only 'qu' appears by ellision\n        positions = self._starting_consonants_only(letters)\n        while len(positions) > 0:\n            letters = string_utils.move_consonant_right(letters, positions)\n            letters = string_utils.remove_blanks(letters)\n            positions = self._starting_consonants_only(letters)\n        positions = self._ending_consonants_only(letters)\n        while len(positions) > 0:\n            letters = string_utils.move_consonant_left(letters, positions)\n            letters = string_utils.remove_blanks(letters)\n            positions = self._ending_consonants_only(letters)\n        positions = self._find_solo_consonant(letters)\n        while len(positions) > 0:\n            letters = self._move_consonant(letters, positions)\n            letters = string_utils.remove_blanks(letters)\n            positions = self._find_solo_consonant(letters)\n        positions = self._find_consonant_cluster(letters)\n        while len(positions) > 0:\n            letters = self._move_consonant(letters, positions)\n            letters = string_utils.remove_blanks(letters)\n            positions = self._find_consonant_cluster(letters)\n        return letters", "response": "This method processes a word into a list of strings representing the syllables of the word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a string ends with a vowel.", "response": "def _ends_with_vowel(self, letter_group: str) -> bool:\n        \"\"\"Check if a string ends with a vowel.\"\"\"\n        if len(letter_group) == 0:\n            return False\n        return self._contains_vowels(letter_group[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _starts_with_vowel(self, letter_group: str) -> bool:\n        if len(letter_group) == 0:\n            return False\n        return self._contains_vowels(letter_group[0])", "response": "Check if a string starts with a vowel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _starting_consonants_only(self, letters: list) -> list:\n        for idx, letter in enumerate(letters):\n            if not self._contains_vowels(letter) and self._contains_consonants(letter):\n                return [idx]\n            if self._contains_vowels(letter):\n                return []\n            if self._contains_vowels(letter) and self._contains_consonants(letter):\n                return []\n        return []", "response": "Return a list of starting consonant positions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of positions for ending consonants.", "response": "def _ending_consonants_only(self, letters: List[str]) -> List[int]:\n        \"\"\"Return a list of positions for ending consonants.\"\"\"\n        reversed_letters = list(reversed(letters))\n        length = len(letters)\n        for idx, letter in enumerate(reversed_letters):\n            if not self._contains_vowels(letter) and self._contains_consonants(letter):\n                return [(length - idx) - 1]\n            if self._contains_vowels(letter):\n                return []\n            if self._contains_vowels(letter) and self._contains_consonants(letter):\n                return []\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_solo_consonant(self, letters: List[str]) -> List[int]:\n        solos = []\n        for idx, letter in enumerate(letters):\n            if len(letter) == 1 and self._contains_consonants(letter):\n                solos.append(idx)\n        return solos", "response": "Find the positions of any solo consonants that are not yet paired with a vowel."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds clusters of consonants that do not contain a vowel.", "response": "def _find_consonant_cluster(self, letters: List[str]) -> List[int]:\n        \"\"\"\n        Find clusters of consonants that do not contain a vowel.\n        :param letters:\n        :return:\n        \"\"\"\n        for idx, letter_group in enumerate(letters):\n            if self._contains_consonants(letter_group) and not self._contains_vowels(letter_group):\n                return [idx]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of consonant positions move the consonants according to certain consonant syllable behavioral rules for gathering and grouping.", "response": "def _move_consonant(self, letters: list, positions: List[int]) -> List[str]:\n        \"\"\"\n        Given a list of consonant positions, move the consonants according to certain\n        consonant syllable behavioral rules for gathering and grouping.\n\n        :param letters:\n        :param positions:\n        :return:\n        \"\"\"\n        for pos in positions:\n            previous_letter = letters[pos - 1]\n            consonant = letters[pos]\n            next_letter = letters[pos + 1]\n            if self._contains_vowels(next_letter) and self._starts_with_vowel(next_letter):\n                return string_utils.move_consonant_right(letters, [pos])\n            if self._contains_vowels(previous_letter) and self._ends_with_vowel(\n                    previous_letter) and len(previous_letter) == 1:\n                return string_utils.move_consonant_left(letters, [pos])\n            if previous_letter + consonant in self.constants.ASPIRATES:\n                return string_utils.move_consonant_left(letters, [pos])\n            if consonant + next_letter in self.constants.ASPIRATES:\n                return string_utils.move_consonant_right(letters, [pos])\n            if next_letter[0] == consonant:\n                return string_utils.move_consonant_left(letters, [pos])\n            if consonant in self.constants.MUTES and next_letter[0] in self.constants.LIQUIDS:\n                return string_utils.move_consonant_right(letters, [pos])\n            if consonant in ['k', 'K'] and next_letter[0] in ['w', 'W']:\n                return string_utils.move_consonant_right(letters, [pos])\n            if self._contains_consonants(next_letter[0]) and self._starts_with_vowel(\n                    previous_letter[-1]):\n                return string_utils.move_consonant_left(letters, [pos])\n            # fall through case\n            if self._contains_consonants(next_letter[0]):\n                return string_utils.move_consonant_right(letters, [pos])\n        return letters"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the number of syllables that would occur after the ellision.", "response": "def get_syllable_count(self, syllables: List[str]) -> int:\n        \"\"\"\n        Counts the number of syllable groups that would occur after ellision.\n\n        Often we will want preserve the position and separation of syllables so that they\n        can be used to reconstitute a line, and apply stresses to the original word positions.\n        However, we also want to be able to count the number of syllables accurately.\n\n        :param syllables:\n        :return:\n\n        >>> syllabifier = Syllabifier()\n        >>> print(syllabifier.get_syllable_count([\n        ... 'J\u0101m', 't\u016bm', 'c', 'au', 'sus', 'es', 'u', 'nus', 'I', 'ta', 'lo', 'rum']))\n        11\n        \"\"\"\n        tmp_syllables = copy.deepcopy(syllables)\n        return len(string_utils.remove_blank_spaces(\n            string_utils.move_consonant_right(tmp_syllables,\n                                             self._find_solo_consonant(tmp_syllables))))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transliterate(text, inFormat, outFormat, requestOptions={}):\n    def asciiEncode(chr):\n        value = ord(chr)\n        if value > 255:\n            return '&#x%x;' % (value)\n        return chr\n      \n    try:\n        options.update(requestOptions)\n    \n        \"\"\" Ensure we have the correct encoding for input text. \"\"\"\n        if isinstance(text, str):\n            try:\n                text = text.decode(options['inputEncoding'])\n            except:\n            \tpass\n\n        \"\"\" Look up input & output format names. \"\"\"\n        def findFormat(fmt):\n            if isinstance(fmt, basestring):\n                try:\n                    fmt = _names[fmt.upper()]\n                except KeyError:\n                    raise ValueError('unrecognised format ' + fmt)\n            return fmt\n        inFormat = findFormat(inFormat)\n        outFormat = findFormat(outFormat)\n        \n        \"\"\" Perform sanity checks. \"\"\"\n    \n        if not isinstance(text, basestring): \n                raise TypeError(\"The text must be a string or a unicode object\")\n        \n        def getBlock(format):\n            if isinstance(format, CharacterBlock):\n                return format\n            else:\n                return format.block\n        inBlock = getBlock(inFormat)\n        outBlock = getBlock(outFormat)\n        if not inBlock is outBlock:\n            raise ValueError(\"incompatible input and output formats\")\n            \n        if inFormat is outFormat:\n            # They're trying to trick us. Just do a quick sanity check & bounce it back.\n            if inFormat._longestEntry == 1:\n                [inFormat[c] for c in set(text) if not c.isspace()] \n                # -> KeyError for extraneous chars.\n                return text\n            \n        \"\"\" At last we're happy. Do it. \"\"\"\n            \n        result = inFormat._transliterate(text, outFormat)\n        if options['outputASCIIEncoded']:\n            result = [asciiEncode(c) for c in result]\n        return u''.join(result).encode(options['outputEncoding'])\n    finally:\n        resetOptions()", "response": "Transliterate a text into a target character set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _transliterate (self, text, outFormat):\n        result = []\n        for c in text:\n            if c.isspace(): result.append(c)\n            try: \n                result.append(self[c].equivalents[outFormat.name])\n            except KeyError:\n                result.append(_unrecognised(c))\n        return result", "response": "Transliterate the text to the target transliteration scheme."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setupParseTree(self, rowFrom, rowTo, colIndex, tree):\n        if colIndex == self._longestEntry:\n            return\n        prevchar = None\n        rowIndex = rowFrom\n        while rowIndex <= rowTo:\n            if colIndex < len(self._parsedata[rowIndex]):\n                c = self._parsedata[rowIndex][colIndex]\n                if c != prevchar:\n                    tree[c] = {}\n                    if  prevchar is not None:\n                        self._setupParseTree(rowFrom, rowIndex - 1, colIndex + 1, tree[prevchar])\n                    rowFrom = rowIndex\n                    prevchar = c\n                if rowIndex == rowTo:\n                    self._setupParseTree(rowFrom, rowIndex, colIndex + 1, tree[prevchar])\n            rowIndex = rowIndex + 1", "response": "Build the search tree for multi - character encodings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _transliterate (self, text, outFormat):\n        result = []\n        text = self._preprocess(text)\n        i = 0\n        while i < len(text):\n            if text[i].isspace(): \n                result.append(text[i])\n                i = i+1\n            else: \n                chr = self._getNextChar(text, i)\n                try:\n                    result.append(self[chr].unichr)\n                except KeyError:\n                    result.append(_unrecognised(chr))\n                i = i + len(chr)\n        return result", "response": "Transliterate the text to Unicode."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of equivalent characters.", "response": "def _equivalent(self, char, prev, next, implicitA):\n        \"\"\" Transliterate a Devanagari character to Latin.\n        \n        Add implicit As unless overridden by VIRAMA.\n        \n        \"\"\"\n        result = []\n        if char.unichr != DevanagariCharacter._VIRAMA:\n            result.append(char.equivalents[self.name])\n        \"\"\" Append implicit A to consonants if the next character isn't a vowel. \"\"\"\n        if implicitA and char.isConsonant \\\n        and ((next is not None \\\n        and next.unichr != DevanagariCharacter._VIRAMA \\\n        and not next.isVowel) \\\n        or next is None):\n            result.append(characterBlocks['DEVANAGARI']\\\n                   [DevanagariCharacter._LETTER_A].equivalents[self.name])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef choose_tag(self: object, tokens: List[str], index: int, history: List[str]):\n        for pattern, replace in self._regexs:\n            if re.search(pattern, tokens[index]):\n                if self.default:\n                    return self.default\n                else:\n                    return replace", "response": "Use regular expressions to choose a tag based on word endings ; use regular expressions for rules - based lemmatizing based on word endings ; use default if set to True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransliterating a string into a mdc - style unicode tree", "response": "def mdc_unicode(string, q_kopf=True):\n    \"\"\"\n    parameters:\n    string: str\n    q_kopf: boolean\n    return:\n    unicode_text: str\n    The translitterated text passes to the\n    function under the variable 'string'.\n    The search and replace operation\n    is done for the related caracters. If\n    the q_kopf parameter is False, we replace\n    'q' with '\u1e33'\n    \"\"\"\n    #\n    # lettres miniscules/lower case letters/k\u00fc\u00e7\u00fck harfler\n    #\n    alef =  string.replace(\"\\u0041\", \"\\ua723\") # A -> \ua723\n    ayin = alef.replace(\"\\u0061\", \"\\ua725\") # a -> \ua725\n    h2 = ayin.replace(\"\\u0048\", \"\\u1e25\") # H -> \u1e25\n    h3 = h2.replace(\"\\u0078\", \"\\u1e2b\") # x -> \u1e2b\n    h4 = h3.replace(\"\\u0058\", \"\\u1e96\") # X -> \u1e96\n    h5 = h4.replace(\"\\u0056\", \"\\u0068\"+\"\\u032d\") # V -> \uec50\n    shin = h5.replace(\"\\u0053\", \"\\u0161\") # S -> \u0161\n    s_acute = shin.replace(\"\\u0063\", \"\\u015b\") # c -> \u015b\n    tche = s_acute.replace(\"\\u0054\", \"\\u1e6f\") # T -> \u1e6f\n    tj = tche.replace(\"\\u0076\", \"\\u1e71\") # v -> \u1e71\n    djed = tj.replace(\"\\u0044\", \"\\u1e0f\") # D -> \u1e0f\n    egy_yod = djed.replace(\"\\u0069\", \"\\u0069\"+\"\\u0486\") # i -> i\u0486\n    equal = egy_yod.replace(\"\\u003d\", \"\\u2e17\") # = -> \u2e17\n    left_brackets = equal.replace(\"\\u003c\", \"\\u2329\") # < -> \u2329\n    right_brackets = left_brackets.replace(\"\\u003e\", \"\\u232a\") # > -> \u232a\n    #\n    if q_kopf is False:\n        kopf = right_brackets.replace(\"\\u0071\", \"\\u1e33\") # q -> \u1e33\n        kopf_capital = kopf.replace(\"\\u0051\", \"\\u1e32\") # Q -> \u1e32\n    else:\n        kopf_capital = right_brackets\n    #\n    # LETTRES MAJUSCULES/ UPPER CASE LETTERS/ B\u00dcY\u00dcK HARFLER\n    #\n    h2_capital = re.sub(\"[\\u00a1\\u0040]\", \"\\u1e24\", kopf_capital) # \u00a1|@ -> \u1e24\n    h3_capital = re.sub(\"[\\u0023\\u00a2]\", \"\\u1e2a\", h2_capital) # #|\u00a2 -> \u1e2a\n    h4_capital = re.sub(\"[\\u0024\\u00a3]\", \"\\u0048\"+\"\\u0331\", h3_capital) # $|\u00a3 -> H\u0331\n    shin_capital = re.sub(\"[\\u00a5\\u005e]\", \"\\u0160\", h4_capital) # \u00a5|^ -> \u0160\n    tche_capital = re.sub(\"[\\u002a\\u00a7]\", \"\\u1e6e\", shin_capital) # *|\u00a7 -> \u1e6e\n    djed_capital = re.sub(\"[\\u00a9\\u002b]\", \"\\u1e0e\", tche_capital) # \u00a9|+ -> \u1e0e\n    unicode_text = djed_capital.replace(\"\\u0043\", \"\\u015a\") # C -> \u015a\n    #\n    return unicode_text"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tonos_oxia_converter(text, reverse=False):\n    for char_tonos, char_oxia in TONOS_OXIA.items():\n        if not reverse:        \n            text = text.replace(char_tonos, char_oxia)\n        else:\n            text = text.replace(char_oxia, char_tonos)\n    return text", "response": "Converts the tonos characters in the input text into the oxia equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_non_ascii(input_string):\n    no_ascii = \"\".join(i for i in input_string if ord(i) < 128)\n    return no_ascii", "response": "Remove non - ASCII characters from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves non - Latin characters from a string.", "response": "def remove_non_latin(input_string, also_keep=None):\n    \"\"\"Remove non-Latin characters.\n    `also_keep` should be a list which will add chars (e.g. punctuation)\n    that will not be filtered.\n    \"\"\"\n    if also_keep:\n        also_keep += [' ']\n    else:\n        also_keep = [' ']\n    latin_chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    latin_chars += latin_chars.lower()\n    latin_chars += ''.join(also_keep)\n    no_latin = \"\".join([char for char in input_string if char in latin_chars])\n    return no_latin"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tlg_plaintext_cleanup(text, rm_punctuation=False, rm_periods=False):\n    remove_comp = regex.compile(r'-\\n|\u00ab|\u00bb|<|>|\\.\\.\\.|\u2018|\u2019|_|{.+?}|\\(.+?\\)|[a-zA-Z0-9]', flags=regex.VERSION1)\n    text = remove_comp.sub('', text)\n\n    new_text = None\n    if rm_punctuation:\n        new_text = ''\n        punctuation = [',', '\u00b7', ':', '\"', \"'\", '?', '-', '!', '*', '[', ']', '{', '}']\n        if rm_periods:\n            punctuation += ['.', ';']\n        for char in text:\n            # second try at rming some punctuation; merge with above regex\n            if char in punctuation:\n                pass\n            else:\n                new_text += char\n    if new_text:\n        text = new_text\n\n    # replace line breaks w/ space\n    replace_comp = regex.compile(r'\\n')\n    text = replace_comp.sub(' ', text)\n\n    comp_space = regex.compile(r'\\s+')\n    text = comp_space.sub(' ', text)\n\n    return text", "response": "Remove and substitute post - processing for Greek TLG text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving and substitute post - processing for Greek PHI5 text.", "response": "def phi5_plaintext_cleanup(text, rm_punctuation=False, rm_periods=False):\n    \"\"\"Remove and substitute post-processing for Greek PHI5 text.\n    TODO: Surely more junk to pull out. Please submit bugs!\n    TODO: This is a rather slow now, help in speeding up welcome.\n    \"\"\"\n    # This works OK, doesn't get some\n    # Note: rming all characters between {} and ()\n    remove_comp = regex.compile(r'-\\n|\u00ab|\u00bb|\\<|\\>|\\.\\.\\.|\u2018|\u2019|_|{.+?}|\\(.+?\\)|\\(|\\)|\u201c|#|%|\u2694|&|=|/|\\\\|\u301a|\u2020|\u300e|\u2696|\u2013|\u02d8|\u2695|\u263e|\u25cc|\u25c4|\u25ba|\u2310|\u230a|\u230b|\u2248|\u2237|\u2248|\u221e|\u201d|[0-9]')\n    text = remove_comp.sub('', text)\n\n    new_text = None\n    if rm_punctuation:\n        new_text = ''\n        punctuation = [',', ';', ':', '\"', \"'\", '?', '-', '!', '*', '[', ']', '{', '}']\n        if rm_periods:\n            punctuation += ['.']\n        for char in text:\n            # rm acute combining acute accents made by TLGU\n            # Could be caught by regex, tried and failed, not sure why\n            if bytes(char, 'utf-8') == b'\\xcc\\x81':\n                pass\n            # second try at rming some punctuation; merge with above regex\n            elif char in punctuation:\n                pass\n            else:\n                new_text += char\n    if new_text:\n        text = new_text\n\n    # replace line breaks w/ space\n    replace_comp = regex.compile(r'\\n')\n    text = replace_comp.sub(' ', text)\n\n    comp_space = regex.compile(r'\\s+')\n    text = comp_space.sub(' ', text)\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assemble_tlg_author_filepaths():\n    plaintext_dir_rel = '~/cltk_data/greek/text/tlg/plaintext/'\n    plaintext_dir = os.path.expanduser(plaintext_dir_rel)\n    filepaths = [os.path.join(plaintext_dir, x + '.TXT') for x in TLG_INDEX]\n    return filepaths", "response": "Reads TLG index and builds a list of absolute filepaths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread PHI5 index and builds a list of absolute filepaths.", "response": "def assemble_phi5_author_filepaths():\n    \"\"\"Reads PHI5 index and builds a list of absolute filepaths.\n    \"\"\"\n    plaintext_dir_rel = '~/cltk_data/latin/text/phi5/plaintext/'\n    plaintext_dir = os.path.expanduser(plaintext_dir_rel)\n    filepaths = [os.path.join(plaintext_dir, x + '.TXT') for x in PHI5_INDEX]\n    return filepaths"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the TLG index and builds a list of absolute filepaths.", "response": "def assemble_tlg_works_filepaths():\n    \"\"\"Reads TLG index and builds a list of absolute filepaths.\"\"\"\n    plaintext_dir_rel = '~/cltk_data/greek/text/tlg/individual_works/'\n    plaintext_dir = os.path.expanduser(plaintext_dir_rel)\n    all_filepaths = []\n    for author_code in TLG_WORKS_INDEX:\n        author_data = TLG_WORKS_INDEX[author_code]\n        works = author_data['works']\n        for work in works:\n            f = os.path.join(plaintext_dir, author_code + '.TXT' + '-' + work + '.txt')\n            all_filepaths.append(f)\n    return all_filepaths"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads PHI5 index and builds a list of absolute filepaths.", "response": "def assemble_phi5_works_filepaths():\n    \"\"\"Reads PHI5 index and builds a list of absolute filepaths.\"\"\"\n    plaintext_dir_rel = '~/cltk_data/latin/text/phi5/individual_works/'\n    plaintext_dir = os.path.expanduser(plaintext_dir_rel)\n    all_filepaths = []\n    for author_code in PHI5_WORKS_INDEX:\n        author_data = PHI5_WORKS_INDEX[author_code]\n        works = author_data['works']\n        for work in works:\n            f = os.path.join(plaintext_dir, author_code + '.TXT' + '-' + work + '.txt')\n            all_filepaths.append(f)\n    return all_filepaths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngrabbing filename and enables it to be read.", "response": "def read_file(self):\n        \"\"\"\n        Grabs filename and enables it to be read.\n        :return: raw_file = unaltered text; file_lines = text split by lines.\n        \"\"\"\n        with open(self.filename, mode='r+', encoding='utf8') as text_file:\n            self.raw_file = text_file.read()  # pylint: disable= attribute-defined-outside-init\n        self.file_lines = [x.rstrip() for x in self.raw_file.splitlines()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlooks at the folder filename is in and lists other files in the folder.", "response": "def file_catalog(self):\n        \"\"\"\n        Looks at the folder filename is in and lists other files in the folder.\n        :return: list of files.\n        \"\"\"\n        pathway = os.path.split(self.filename)\n        self.catalog = sorted(os.listdir(pathway[0]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representation of the current object in syllabified form.", "response": "def syllabified_str(self, separator=\".\"):\n        \"\"\"\n        Returns:\n             str: Syllabified word in string format\n\n        Examples:\n            >>> Word('conseil').syllabified_str()\n            'con.seil'\n\n            You can also specify the separator('.' by default)\n\n            >>> Word('sikerly').syllabified_str(separator = '-')\n            'sik-er-ly'\n        \"\"\"\n        return separator.join(self.syllabified if self.syllabified else self.syllabify())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stresser(self, stress_rule='FSR'):\n\n        # Syllabify word\n        if not self.syllabified:\n            self.syllabify()\n\n        # Check whether word is monosyllabic\n        if len(self.syllabified) == 1:\n            return self.syllabified\n\n        if stress_rule == 'FSR':\n            # Check whether ultima ends in e\n            if self.syllabified[-1][-1] == 'e':\n                return self.syllabified[:-2] + ['\\'{0}'.format(self.syllabified[-2])] + self.syllabified[-1:]\n\n            else:\n                return self.syllabified[:-1] + ['\\'{0}'.format(self.syllabified[-1])]\n\n        elif stress_rule == 'GSR':\n            # The word striped of suffixes\n            st_word = affix_stemmer([self.word], strip_suf=False)\n            affix = self.word[:len(self.word) - len(st_word)]\n\n            # Syllabify stripped word and affix\n\n            syl_word = Word(st_word).syllabify()\n\n            # Add stress\n            syl_word = ['\\'{0}'.format(syl_word[0])] + syl_word[1:]\n\n            if affix:\n                affix = Word(affix).syllabify()\n                syl_word = affix + syl_word\n\n            return syl_word\n\n        elif stress_rule == 'LSR':\n            # Check whether penult is heavy (contains more than one mora)\n            if sum(map(lambda x: x in SHORT_VOWELS, self.syllabified[-1])) > 1:\n                return self.syllabified[:-2] + ['\\'{0}'.format(self.syllabified[-2])] + self.syllabified[-1:]\n\n            elif len(self.syllabified) > 2:\n                return self.syllabified[:-3] + ['\\'{0}'.format(self.syllabified[-3])] + self.syllabified[-2:]\n\n            else:\n                return self.syllabified[:-1] + ['\\'{0}'.format(self.syllabified[-1])]", "response": "Returns a list of the syllables in the current set that are stressed on the ultima."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking ~ / cltk_data / distributed_corpora. yaml for any custom distributed corpora that the user wants to load locally.", "response": "def _check_distributed_corpora_file(self):\n        \"\"\"Check '~/cltk_data/distributed_corpora.yaml' for any custom,\n        distributed corpora that the user wants to load locally.\n\n        TODO: write check or try if `cltk_data` dir is not present\n        \"\"\"\n        if self.testing:\n            distributed_corpora_fp = os.path.expanduser('~/cltk_data/test_distributed_corpora.yaml')\n        else:\n            distributed_corpora_fp = os.path.expanduser('~/cltk_data/distributed_corpora.yaml')\n\n        try:\n            with open(distributed_corpora_fp) as file_open:\n                corpora_dict = yaml.safe_load(file_open)\n        except FileNotFoundError:\n            logger.info('`~/cltk_data/distributed_corpora.yaml` file not found.')\n            return []\n        except yaml.parser.ParserError as parse_err:\n            logger.debug('Yaml parsing error: %s' % parse_err)\n            return []\n\n        user_defined_corpora = []\n        for corpus_name in corpora_dict:\n            about = corpora_dict[corpus_name]\n\n            if about['language'].lower() == self.language:\n                user_defined_corpus = dict()\n                # user_defined_corpus['git_remote'] = about['git_remote']\n                user_defined_corpus['origin'] = about['origin']\n                user_defined_corpus['type'] = about['type']\n                user_defined_corpus['name'] = corpus_name\n                user_defined_corpora.append(user_defined_corpus)\n\n        return user_defined_corpora"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_language_variables(self):\n        if self.language not in AVAILABLE_LANGUAGES:\n            # If no official repos, check if user has custom\n            user_defined_corpora = self._check_distributed_corpora_file()\n            if user_defined_corpora:\n                return user_defined_corpora\n            else:\n                msg = 'Corpora not available (either core or user-defined) for the \"{}\" language.'.format(self.language)\n                logger.info(msg)\n                raise CorpusImportError(msg)\n        else:\n            user_defined_corpora = self._check_distributed_corpora_file()\n            return user_defined_corpora", "response": "Check for availability of corpora for a language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_corpora(self):\n        try:\n            # corpora = LANGUAGE_CORPORA[self.language]\n            corpora = self.all_corpora\n            corpus_names = [corpus['name'] for corpus in corpora]\n            return corpus_names\n        except (NameError, KeyError) as error:\n            msg = 'Corpus not available for language \"{}\": {}'.format(self.language, error)\n            logger.error(msg)\n            raise CorpusImportError(msg)", "response": "Show corpora available for the CLTK to download."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies contents of one directory to another.", "response": "def _copy_dir_recursive(src_rel, dst_rel):\n        \"\"\"Copy contents of one directory to another. `dst_rel` dir cannot\n        exist. Source: http://stackoverflow.com/a/1994840\n        TODO: Move this to file_operations.py module.\n        :type src_rel: str\n        :param src_rel: Directory to be copied.\n        :type dst_rel: str\n        :param dst_rel: Directory to be created with contents of ``src_rel``.\n        \"\"\"\n        src = os.path.expanduser(src_rel)\n        dst = os.path.expanduser(dst_rel)\n        try:\n            shutil.copytree(src, dst)\n            logger.info('Files copied from %s to %s', src, dst)\n        except OSError as exc:\n            if exc.errno == errno.ENOTDIR:\n                shutil.copy(src, dst)\n                logger.info('Files copied from %s to %s', src, dst)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_corpus_properties(self, corpus_name):\n        try:\n            # corpora = LANGUAGE_CORPORA[self.language]\n            corpora = self.all_corpora\n        except NameError as name_error:\n            msg = 'Corpus not available for language ' \\\n                  '\"%s\": %s' % (self.language, name_error)\n            logger.error(msg)\n            raise CorpusImportError(msg)\n        for corpus_properties in corpora:\n            if corpus_properties['name'] == corpus_name:\n                return corpus_properties\n        msg = 'Corpus \"%s\" not available for the ' \\\n              '\"%s\" language.' % (corpus_name, self.language)\n        logger.error(msg)\n        raise CorpusImportError(msg)", "response": "Check whether a corpus is available for import."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _git_user_defined_corpus(self, corpus_name, corpus_type, uri: str, branch='master'):\n        # git_uri = urljoin('https://github.com/cltk/', corpus_name + '.git')\n        # self._download_corpus(corpus_type, corpus_name, path)\n        type_dir_rel = os.path.join(CLTK_DATA_DIR, self.language, corpus_type)\n        type_dir = os.path.expanduser(type_dir_rel)\n        repo_name = uri.split('/')[-1]  # eg, 'latin_corpus_newton_example.git'\n        repo_name = repo_name.rstrip('.git')\n        target_dir = os.path.join(type_dir, repo_name)\n        target_file = os.path.join(type_dir, repo_name, 'README.md')\n        # check if corpus already present\n        # if not, clone\n        if not os.path.isfile(target_file):\n            if not os.path.isdir(type_dir):\n                os.makedirs(type_dir)\n            try:\n                msg = \"Cloning '{}' from '{}'\".format(corpus_name, uri)\n                logger.info(msg)\n                Repo.clone_from(uri, target_dir, branch=branch, depth=1,\n                                progress=ProgressPrinter())\n            except CorpusImportError as corpus_imp_err:\n                msg = \"Git clone of '{}' failed: '{}'\".format(uri, corpus_imp_err)\n                logger.error(msg)\n        # if corpus is present, pull latest\n        else:\n            try:\n                repo = Repo(target_dir)\n                assert not repo.bare  # or: assert repo.exists()\n                git_origin = repo.remotes.origin\n                msg = \"Pulling latest '{}' from '{}'.\".format(corpus_name, uri)\n                logger.info(msg)\n                git_origin.pull()\n            except CorpusImportError as corpus_imp_err:\n                msg = \"Git pull of '{}' failed: '{}'\".format(uri, corpus_imp_err)\n                logger.error(msg)", "response": "Clone or update a git repo defined by user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload a remote or load a local corpus into a local file.", "response": "def import_corpus(self, corpus_name, local_path=None, branch='master'):  # pylint: disable=R0912\n        \"\"\"Download a remote or load local corpus into dir ``~/cltk_data``.\n        TODO: maybe add ``from git import RemoteProgress``\n        TODO: refactor this, it's getting kinda long\n        :type corpus_name: str\n        :param corpus_name: The name of an available corpus.\n        :param local_path: str\n        :param local_path: A filepath, required when importing local corpora.\n        :param branch: What Git branch to clone.\n        \"\"\"\n        corpus_properties = self._get_corpus_properties(corpus_name)\n        try:\n            location = corpus_properties['location']\n        except KeyError:\n            # git_uri = corpus_properties['git_remote']\n            git_name = corpus_properties['']\n            git_uri = corpus_properties['origin']\n            git_type = corpus_properties['type']\n            # pass this off to a special downloader just for custom urls\n            self._git_user_defined_corpus(git_name, git_type, git_uri)\n            return\n        corpus_type = corpus_properties['type']\n        if location == 'remote':\n            # git_uri = urljoin('https://github.com/cltk/', corpus_name + '.git')\n            git_uri = corpus_properties['origin']\n            type_dir_rel = os.path.join(CLTK_DATA_DIR, self.language, corpus_type)\n            type_dir = os.path.expanduser(type_dir_rel)\n            target_dir = os.path.join(type_dir, corpus_name)\n            target_file = os.path.join(type_dir, corpus_name, 'README.md')\n            # check if corpus already present\n            # if not, clone\n            if not os.path.isfile(target_file):\n                if not os.path.isdir(type_dir):\n                    os.makedirs(type_dir)\n                try:\n                    msg = \"Cloning '{}' from '{}'\".format(corpus_name, git_uri)\n                    logger.info(msg)\n                    Repo.clone_from(git_uri, target_dir, branch=branch, depth=1,\n                                    progress=ProgressPrinter())\n                except CorpusImportError as corpus_imp_err:\n                    msg = \"Git clone of '{}' failed: '{}'\".format(git_uri, corpus_imp_err)\n                    logger.error(msg)\n            # if corpus is present, pull latest\n            else:\n                try:\n                    repo = Repo(target_dir)\n                    assert not repo.bare  # or: assert repo.exists()\n                    git_origin = repo.remotes.origin\n                    msg = \"Pulling latest '{}' from '{}'.\".format(corpus_name, git_uri)\n                    logger.info(msg)\n                    git_origin.pull()\n                except CorpusImportError as corpus_imp_err:\n                    msg = \"Git pull of '{}' failed: '{}'\".format(git_uri, corpus_imp_err)\n                    logger.error(msg)\n        elif location == 'local':\n            msg = \"Importing from local path: '{}'\".format(local_path)\n            logger.info(msg)\n            if corpus_name in ('phi5', 'phi7', 'tlg'):\n                if corpus_name == 'phi5':\n                    # normalize path for checking dir\n                    if local_path.endswith('/'):\n                        local_path = local_path[:-1]\n                    # check for right corpus dir\n                    if os.path.split(local_path)[1] != 'PHI5':\n                        logger.info(\"Directory must be named 'PHI5'.\")\n                if corpus_name == 'phi7':\n                    # normalize local_path for checking dir\n                    if local_path.endswith('/'):\n                        local_path = local_path[:-1]\n                    # check for right corpus dir\n                    if os.path.split(local_path)[1] != 'PHI7':\n                        logger.info(\"Directory must be named 'PHI7'.\")\n                if corpus_name == 'tlg':\n                    # normalize path for checking dir\n                    if local_path.endswith('/'):\n                        local_path = local_path[:-1]\n                    # check for right corpus dir\n                    if os.path.split(local_path)[1] != 'TLG_E':\n                        logger.info(\"Directory must be named 'TLG_E'.\")\n                # move the dir-checking commands into a function\n                data_dir = os.path.expanduser(CLTK_DATA_DIR)\n                originals_dir = os.path.join(data_dir, 'originals')\n                # check for `originals` dir; if not present mkdir\n                if not os.path.isdir(originals_dir):\n                    os.makedirs(originals_dir)\n                    msg = \"Wrote directory at '{}'.\".format(originals_dir)\n                    logger.info(msg)\n                tlg_originals_dir = os.path.join(data_dir,\n                                                 'originals',\n                                                 corpus_name)\n                # check for `originals/<corpus_name>`; if pres, delete\n                if os.path.isdir(tlg_originals_dir):\n                    shutil.rmtree(tlg_originals_dir)\n                    msg = \"Removed directory at '{}'.\".format(tlg_originals_dir)\n                    logger.info(msg)\n                # copy_dir requires that target\n                if not os.path.isdir(tlg_originals_dir):\n                    self._copy_dir_recursive(local_path, tlg_originals_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transliterate(mode, string, ignore = '', reverse = False ):\n    # @todo: arabtex and iso8859-6 need individual handling because in some cases using one-two mapping\n    \"\"\"\n    encode & decode different  romanization systems\n    :param mode:\n    :param string:\n    :param ignore:\n    :param reverse:\n    :return:\n    \"\"\"\n\n    if mode in available_transliterate_systems():\n        MAPPING = ROMANIZATION_SYSTEMS_MAPPINGS[mode]\n    else:\n        print(mode+\"  not supported! \\n\")\n        MAPPING = {}\n\n    if reverse:\n        mapping = {}\n        for k, v in MAPPING.items():\n            # reverse the mapping buckwalter <-> unicode\n            mapping[v] = k\n    else:\n        mapping = MAPPING\n\n    result = \"\"\n    for char in string:\n        if char in mapping.keys() and char not in ignore:\n            result += mapping[char]\n        else:\n            result += char\n    return result", "response": "transliterate a string into a random base64 - encoded version of the current language"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onekgreek_tei_xml_to_text():\n    if not bs4_installed:\n        logger.error('Install `bs4` and `lxml` to parse these TEI files.')\n        raise ImportError\n    xml_dir = os.path.expanduser('~/cltk_data/greek/text/greek_text_first1kgreek/data/*/*/*.xml')\n    xml_paths = glob.glob(xml_dir)\n    if not len(xml_paths):\n        logger.error('1K Greek corpus not installed. Use CorpusInstaller to get `First1KGreek`.')\n        raise FileNotFoundError\n    xml_paths = [path for path in xml_paths if '__cts__' not in path]\n\n    # new dir\n    new_dir = os.path.expanduser('~/cltk_data/greek/text/greek_text_first1kgreek_plaintext/')\n    if not os.path.isdir(new_dir):\n        os.makedirs(new_dir)\n\n    for xml_path in xml_paths:\n        _, xml_name = os.path.split(xml_path)\n        xml_name = xml_name.rstrip('.xml')\n        xml_name += '.txt'\n        with open(xml_path) as file_open:\n            soup = BeautifulSoup(file_open, 'lxml')\n        body = soup.body\n        text = body.get_text()\n        new_plaintext_path = os.path.join(new_dir, xml_name)\n        with open(new_plaintext_path, 'w') as file_open:\n            file_open.write(text)", "response": "Find TEI XML dir of TEI text for the First 1k Years of Greek corpus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing MyCapitains program to convert TEI to plaintext.", "response": "def onekgreek_tei_xml_to_text_capitains():\n    \"\"\"Use MyCapitains program to convert TEI to plaintext.\"\"\"\n    file = os.path.expanduser(\n        '~/cltk_data/greek/text/greek_text_first1kgreek/data/tlg0627/tlg021/tlg0627.tlg021.1st1K-grc1.xml')\n    xml_dir = os.path.expanduser('~/cltk_data/greek/text/greek_text_first1kgreek/data/*/*/*.xml')\n    xml_paths = glob.glob(xml_dir)\n    if not len(xml_paths):\n        logger.error('1K Greek corpus not installed. Use CorpusInstaller to get `First1KGreek`.')\n        raise FileNotFoundError\n    xml_paths = [path for path in xml_paths if '__cts__' not in path]\n\n    # new dir\n    new_dir = os.path.expanduser('~/cltk_data/greek/text/greek_text_first1kgreek_plaintext/')\n    if not os.path.isdir(new_dir):\n        os.makedirs(new_dir)\n\n    for xml_path in xml_paths:\n        _, xml_name = os.path.split(xml_path)\n        xml_name = xml_name.rstrip('.xml')\n        xml_name += '.txt'\n\n        plain_text = ''\n        with open(xml_path) as file_open:\n            text = CapitainsCtsText(resource=file_open)\n            for ref in text.getReffs(level=len(text.citation)):\n                psg = text.getTextualNode(subreference=ref, simple=True)\n                text_line = psg.export(Mimetypes.PLAINTEXT, exclude=[\"tei:note\"])\n                plain_text += text_line\n\n        new_plaintext_path = os.path.join(new_dir, xml_name)\n        with open(new_plaintext_path, 'w') as file_open:\n            file_open.write(plain_text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_replacement_patterns(self):\n        filename = self.dictionary + '.py'\n        models = self.language + '_models_cltk'\n        rel_path = os.path.join('~/cltk_data',\n                                self.language,\n                                'model',\n                                models,\n                                'semantics',\n                                filename)\n        path = os.path.expanduser(rel_path)\n        logger.info('Loading lemmata or synonyms. This may take a minute.')\n        loader = importlib.machinery.SourceFileLoader(filename, path)\n        module = types.ModuleType(loader.name)\n        loader.exec_module(module)\n        return module.DICTIONARY", "response": "Load the replacement patterns for the specified dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of possible lemmata and their probabilities for each token", "response": "def lookup(self, tokens):\n        \"\"\"Return a list of possible lemmata and their probabilities for each token\"\"\"\n        lemmatized_tokens = []\n        if type(tokens) == list:\n            for token in tokens:\n                # look for token in lemma dict keys\n                if token.lower() in self.lemmata.keys():\n                    # `lemmas` is a list of possible lemmata. Probability values must be assigned.\n                    # `lemmalist` is a list of the form [(LEMMA, PROBABILITY), (LEMMA, PROBABILITY)]\n                    # `lemmaobj` is a tuple with the form (LEMMA, LIST)\n                    lemmas = self.lemmata[token.lower()]\n                    lemmalist = []\n                    for lemma in lemmas:\n                        lemmalist.append((lemma, 1/len(lemmas)))\n                    lemmaobj = (token, lemmalist)\n                else:\n                # if token not found in lemma-headword list, return the token itself\n                    lemmalist = []\n                    lemmalist.append((token, 1))\n                    lemmaobj = (token, lemmalist)\n                lemmatized_tokens.append(lemmaobj)\n        if type(tokens) == str:\n            if tokens.lower() in self.lemmata.keys():\n                # `lemmas` is a list of possible lemmata. Probability values must be assigned.\n                # `lemmalist` is a list of the form [(LEMMA, PROBABILITY), (LEMMA, PROBABILITY)]\n                # `lemmaobj` is a tuple with the form (LEMMA, LIST)\n                lemmas = self.lemmata[tokens.lower()]\n                lemmalist = []\n                for lemma in lemmas:\n                    lemmalist.append((lemma, 1/len(lemmas)))\n                lemmaobj = (tokens, lemmalist)\n            else:\n            # if token not found in lemma-headword list, return the token itself\n                lemmalist = []\n                lemmalist.append((tokens, 1))\n                lemmaobj = (tokens, lemmalist)\n            lemmatized_tokens.append(lemmaobj)\n        return lemmatized_tokens"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isolate(obj):\n        answers = []\n        for token in obj:\n            lemmata = token[1]\n            for pair in lemmata:\n                answers.append(pair[0])\n        return answers", "response": "Feed a standard semantic object in and receive a simple list of all lemmata\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of synapses for a list of lems.", "response": "def lookup_synonyms(self, lems):\n        \"\"\"Requires a list of lemmata, not tokens. Feed tokens through a\n        Lemmata object's lookup() and isolate() methods first.\n        \"\"\"\n        final_synonyms = []\n        if type(lems) == list:\n            for lemma in lems:\n                if lemma.lower() in self.synonyms.keys():\n                    syns = self.synonyms[lemma.lower()]\n                    synlist = []\n                    for syn in syns:\n                        synlist.append((syn, 1/len(syns)))\n                    synobj = (lemma, synlist)\n                    final_synonyms.append(synobj)\n                else:\n                    pass\n        if type(lems) == str:\n            if lems.lower() in self.synonyms.keys():\n                syns = self.synonyms[lems.lower()]\n                synlist = []\n                for syn in syns:\n                    synlist.append((syn, 1/len(syns)))\n                synobj = (lems, synlist)\n                final_synonyms.append(synobj)\n            else:\n                pass\n        return final_synonyms"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsource Resonances in Middle High German: New Methodologies in Prosody, 2017, C. L. Hench :param text: str list: text to be analysed :param vowels: str: valid vowels constituting the syllable :param threshold: minimum frequency count for valid onset, C. Hench noted that the algorithm produces the best result for an untagged wordset of MHG, when retaining onsets which appear in at least 0.02% of the words Example: Let's test it on the opening lines of Nibelungenlied >>> text = ['uns', 'ist', 'in', 'alten', 'm\u00e6ren', 'wunders', 'vil', 'geseit', 'von', 'helden', 'lobeb\u00e6ren', 'von', 'gr\u00f4zer', 'arebeit', 'von', 'fr\u00f6uden', 'h\u00f4chgez\u00eeten', 'von', 'weinen', 'und', 'von', 'klagen', 'von', 'k\u00fcener', 'recken', 'str\u00eeten', 'muget', 'ir', 'nu', 'wunder', 'h\u0153ren', 'sagen'] >>> vowels = \"aeiou\u00e6\u0153\u00f4\u00ee\u00f6\u00fc\" >>> get_onsets(text, vowels=vowels) ['lt', 'm', 'r', 'w', 'nd', 'v', 'g', 's', 'h', 'ld', 'l', 'b', 'gr', 'z', 'fr', 'd', 'chg', 't', 'n', 'kl', 'k', 'ck', 'str'] Of course, this is an insignificant sample, but we could try and see how modifying the threshold affects the returned onset: >>> get_onsets(text, threshold = 0.05, vowels=vowels) ['m', 'r', 'w', 'nd', 'v', 'g', 's', 'h', 'b', 'z', 't', 'n']", "response": "def get_onsets(text, vowels=\"aeiou\", threshold=0.0002):\n    \"\"\"\n    Source: Resonances in Middle High German: New Methodologies in Prosody,\n    2017, C. L. Hench\n\n    :param text: str list: text to be analysed\n\n    :param vowels: str: valid vowels constituting the syllable\n\n    :param threshold: minimum frequency count for valid onset, C. Hench noted\n    that the algorithm produces the best result for an untagged wordset of MHG,\n    when retaining onsets which appear in at least 0.02% of the words\n\n    Example:\n        Let's test it on the opening lines of Nibelungenlied\n\n        >>> text = ['uns', 'ist', 'in', 'alten', 'm\u00e6ren', 'wunders', 'vil', 'geseit', 'von', 'helden', 'lobeb\u00e6ren', 'von', 'gr\u00f4zer', 'arebeit', 'von', 'fr\u00f6uden', 'h\u00f4chgez\u00eeten', 'von', 'weinen', 'und', 'von', 'klagen', 'von', 'k\u00fcener', 'recken', 'str\u00eeten', 'muget', 'ir', 'nu', 'wunder', 'h\u0153ren', 'sagen']\n\n        >>> vowels = \"aeiou\u00e6\u0153\u00f4\u00ee\u00f6\u00fc\"\n\n        >>> get_onsets(text, vowels=vowels)\n        ['lt', 'm', 'r', 'w', 'nd', 'v', 'g', 's', 'h', 'ld', 'l', 'b', 'gr', 'z', 'fr', 'd', 'chg', 't', 'n', 'kl', 'k', 'ck', 'str']\n\n         Of course, this is an insignificant sample, but we could try and see\n         how modifying the threshold affects the returned onset:\n\n        >>> get_onsets(text, threshold = 0.05, vowels=vowels)\n        ['m', 'r', 'w', 'nd', 'v', 'g', 's', 'h', 'b', 'z', 't', 'n']\n    \"\"\"\n    onset_dict = defaultdict(lambda: 0)\n    n = len(text)\n\n    for word in text:\n        onset = ''\n        candidates = []\n\n        for l in word:\n\n            if l not in vowels:\n                onset += l\n\n            else:\n                if onset != '':\n                    candidates.append(onset)\n                    onset = ''\n\n        for c in candidates:\n            onset_dict[c] += 1\n\n    return [onset for onset, i in onset_dict.items() if i/n > threshold]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_hierarchy(self, hierarchy):\n        self.hierarchy = dict([(k, i) for i, j in enumerate(hierarchy) for k in j])", "response": "Sets the alternative sonority hierarchy for the module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsyllabify a word according to the Sonority Sequencing Principle and returns a list of syllables.", "response": "def syllabify_ssp(self, word):\n        \"\"\"\n        Syllabifies a word according to the Sonority Sequencing Principle\n\n        :param word: Word to be syllabified\n        :return: List consisting of syllables\n\n        Example:\n            First you need to define the matters of articulation\n            >>> high_vowels = ['a']\n\n            >>> mid_vowels = ['e']\n\n            >>> low_vowels = ['i', 'u']\n\n            >>> flaps = ['r']\n\n            >>> nasals = ['m', 'n']\n\n            >>> fricatives = ['f']\n\n            >>> s = Syllabifier(high_vowels=high_vowels, mid_vowels=mid_vowels, low_vowels=low_vowels, flaps=flaps, nasals=nasals, fricatives=fricatives)\n\n            >>> s.syllabify(\"feminarum\")\n            ['fe', 'mi', 'na', 'rum']\n\n            Not specifying your alphabet results in an error:\n\n            >>> s.syllabify(\"foemina\")\n            Traceback (most recent call last):\n                ...\n            cltk.exceptions.InputError\n            \n            Additionally, you can utilize the language parameter:\n            \n            >>> s = Syllabifier(language='middle_high_german')\n            \n            >>> s.syllabify('lobeb\u00e6ren')\n            ['lo', 'be', 'b\u00e6', 'ren']\n            \n            >>> s = Syllabifier(language='middle_english')\n            \n            >>> s.syllabify(\"huntyng\")\n            ['hun', 'tyng']\n            \n            >>> s = Syllabifier(language='old_english')\n            \n            >>> s.syllabify(\"arcebiscop\")\n            ['ar', 'ce', 'bis', 'cop']\n            \n            The break_geminants parameter ensures a breakpoint is placed between geminants:\n            \n            >>> geminant_s = Syllabifier(break_geminants=True)\n            \n            >>> hierarchy = [[\"a\", \"\u00e1\", \"\u00e6\", \"e\", \"\u00e9\", \"i\", \"\u00ed\", \"o\", \"\u01eb\", \"\u00f8\", \"\u00f6\", \"\u0153\", \"\u00f3\", \"u\", \"\u00fa\", \"y\", \"\u00fd\"], [\"j\"], [\"m\"], [\"n\"], [\"p\", \"b\", \"d\", \"g\", \"t\", \"k\"], [\"c\", \"f\", \"s\", \"h\", \"v\", \"x\", \"\u00fe\", \"\u00f0\"], [\"r\"], [\"l\"]]\n            \n            >>> geminant_s.set_hierarchy(hierarchy)\n            \n            >>> geminant_s.set_vowels(hierarchy[0])\n            \n            >>> geminant_s.syllabify(\"ennitungl\")\n            ['en', 'ni', 'tungl']\n\n            \n        \"\"\"\n\n        # List indicating the syllable indices\n        syllables = []\n\n        find_nucleus = True\n\n        i = 0\n\n        try:\n            # Replace each letter occurence with its corresponding number\n            # indicating its position in the sonority hierarchy\n            encoded = list(map(lambda x: self.hierarchy[x], word))\n\n        except KeyError:\n            LOG.error(\n                \"The given string contains invalid characters. \"\n                \"Make sure to define the mater of articulation for each phoneme.\")\n            raise InputError\n\n        while i < len(word) - 1:\n            # Search for nucleus\n            while word[i] not in self.vowels and i < len(word) - 1 and find_nucleus:\n                i += 1\n            \n            if find_nucleus is True:\n                i += 1\n            \n            if i >= len(word) - 1:\n                break\n\n            else:\n                # If the break_geminants parameter is set to True, prioritize geminants\n                if self.break_geminants and word[i-1] == word[i]:\n                    syllables.append(i-1)\n                    find_nucleus = True \n                    \n                # If a cluster of three phonemes with the same values exist, break syllable\n                elif encoded[i - 1] == encoded[i] == encoded[i + 1]:\n                    syllables.append(i)\n                    find_nucleus = True\n\n                elif encoded[i] > encoded[i - 1] and encoded[i] > encoded[i + 1]:\n                    syllables.append(i)\n                    find_nucleus = True\n\n                elif encoded[i] < encoded[i - 1] and encoded[i] < encoded[i + 1]:\n                    syllables.append(i)\n                    find_nucleus = True\n\n                else:\n                    find_nucleus = False\n\n                i += 1\n\n        for n, k in enumerate(syllables):\n            word = word[:k + n + 1] + \".\" + word[k + n + 1:]\n\n        word = word.split('.')\n\n        # Check if last syllable has a nucleus\n\n        if sum([x in self.vowels for x in word[-1]]) == 0:\n            word[-2] += word[-1]\n            word = word[:-1]\n\n        return self.onset_maximization(word)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef legal_onsets(self, syllables):\n\n        vowels = self.vowels\n\n        for i in range(1, len(syllables)):\n            onset = \"\"\n            \n            for letter in syllables[i]:\n\n                if letter in vowels:\n                    break\n\n                onset += letter\n\n            for j in range(len(onset)):\n                # Check whether the given onset is valid\n                if onset[j:] not in self.invalid_onsets:\n                    syllables[i - 1] += onset[:j]\n                    syllables[i] = syllables[i][j:]\n                    break\n\n        # Check whether ultima is invalid\n\n        if syllables[-1] in self.invalid_ultima:\n            syllables[-2] += syllables[-1]\n            syllables = syllables[:-1]\n\n        return syllables", "response": "Filters a list of syllables for invalid onsets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing IPA string and syllabifies it", "response": "def syllabify_ipa(self, word):\n        \"\"\"\n        Parses IPA string\n        :param word: word to be syllabified\n        \"\"\"\n        word = word[1:-1]\n        word = ''.join(l for l in unicodedata.normalize('NFD', word) if unicodedata.category(l) != 'Mn')\n\n        return self.syllabify_ssp(word)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_syllable(self, text):\n        is_in_onset = True\n        is_in_nucleus = False\n        is_in_coda = False\n        if len(text) > 0:\n            for c in text:\n                if is_in_onset and c in self.consonants:\n                    self.onset.append(c)\n\n                elif is_in_onset and c in self.vowels:\n                    is_in_onset = False\n                    is_in_nucleus = True\n                    self.nucleus.append(c)\n\n                elif is_in_nucleus and c in self.vowels:\n                    self.nucleus.append(c)\n\n                elif is_in_nucleus and c in self.consonants:\n                    is_in_nucleus = False\n                    is_in_coda = True\n                    self.coda.append(c)\n\n                elif is_in_coda and c in self.consonants:\n                    self.coda.append(c)\n\n                elif is_in_coda and c in self.vowels:\n                    raise ValueError(\"This is not a correct syllable \"\n                                     \"(a vowel '{}' cannot be inserted in coda)\".format(c))\n\n                else:\n                    raise ValueError(\"{} is an unknown character\".format(c))\n\n            if len(self.nucleus) == 0:\n                raise ValueError(\"This is not a correct syllable\")\n        else:\n            raise ValueError(\"A syllable can't be void\")", "response": "Compute the syllabified entry point for a given text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_spondaic(self, scansion: str) -> str:\n        mark_list = string_utils.mark_list(scansion)\n        vals = list(scansion.replace(\" \", \"\"))\n        new_vals = self.SPONDAIC_PENTAMETER[:-1] + vals[-1]\n        corrected = \"\".join(new_vals)\n        new_line = list(\" \" * len(scansion))\n        for idx, car in enumerate(corrected):\n            new_line[mark_list[idx]] = car\n        return \"\".join(new_line)", "response": "This function makes a scansion pattern string starting with two spondees."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncorrect the penultimate dactyl chain.", "response": "def correct_penultimate_dactyl_chain(self, scansion: str) -> str:\n        \"\"\"\n        For pentameter the last two feet of the verse are predictable dactyls,\n        and do not regularly allow substitutions.\n\n        :param scansion: scansion line thus far\n        :return: corrected line of scansion\n\n        >>> print(PentameterScanner().correct_penultimate_dactyl_chain(\n        ... \"U  U  U  U  U  U  U  U  U  U  U  U  U  U\"))\n        U  U  U  U  U  U  U  -  U  U  -  U  U  U\n        \"\"\"\n        mark_list = string_utils.mark_list(scansion)\n        vals = list(scansion.replace(\" \", \"\"))\n        n_vals = vals[:-7] + [self.constants.DACTYL + self.constants.DACTYL] + [vals[-1]]\n        corrected = \"\".join(n_vals)\n        new_line = list(\" \" * len(scansion))\n        for idx, car in enumerate(corrected):\n            new_line[mark_list[idx]] = car\n        return \"\".join(new_line)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setup_language_variables(self, lang: str):  # pylint: disable=no-self-use\n        assert lang in TAGGERS.keys(), \\\n            'POS tagger not available for {0} language.'.format(lang)\n        rel_path = os.path.join('~/cltk_data',\n                                lang,\n                                'model/' + lang + '_models_cltk/taggers/pos')  # pylint: disable=C0301\n        path = os.path.expanduser(rel_path)\n        tagger_paths = {}\n        for tagger_key, tagger_val in TAGGERS[lang].items():\n            tagger_path = os.path.join(path, tagger_val)\n            assert os.path.isfile(tagger_path), \\\n                'CLTK linguistics models not available for {0}, looking for .'.format([tagger_val, tagger_path])\n            tagger_paths[tagger_key] = tagger_path\n        return tagger_paths", "response": "Setup the dictionary of tagger paths and their values for the given language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntag POS with unigram tagger.", "response": "def tag_unigram(self, untagged_string: str):\n        \"\"\"Tag POS with unigram tagger.\n        :type untagged_string: str\n        :param : An untagged, untokenized string of text.\n        :rtype tagged_text: str\n        \"\"\"\n        untagged_tokens = wordpunct_tokenize(untagged_string)\n        pickle_path = self.available_taggers['unigram']\n        tagger = open_pickle(pickle_path)\n        tagged_text = tagger.tag(untagged_tokens)\n        return tagged_text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntagging a string with CRF tagger.", "response": "def tag_crf(self, untagged_string: str):\n        \"\"\"Tag POS with CRF tagger.\n        :type untagged_string: str\n        :param : An untagged, untokenized string of text.\n        :rtype tagged_text: str\n        \"\"\"\n        untagged_tokens = wordpunct_tokenize(untagged_string)\n        pickle_path = self.available_taggers['crf']\n        tagger = CRFTagger()\n        tagger.set_model_file(pickle_path)\n        tagged_text = tagger.tag(untagged_tokens)\n        return tagged_text"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nturning str into str or tuple.", "response": "def eval_str_to_list(input_str: str) -> List[str]:\n    \"\"\"Turn str into str or tuple.\"\"\"\n    inner_cast = ast.literal_eval(input_str)  # type: List[str]\n    if isinstance(inner_cast, list):\n        return inner_cast\n    else:\n        raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen file and check for author info.", "response": "def get_authors(filepath: str) -> List[str]:\n    \"\"\"Open file and check for author info.\"\"\"\n    str_oneline = r'(^__author__ = )(\\[.*?\\])'  # type\" str\n    comp_oneline = re.compile(str_oneline, re.MULTILINE)  # type: Pattern[str]\n    with open(filepath) as file_open:\n        file_read = file_open.read()  # type: str\n    match = comp_oneline.findall(file_read)\n    if match:\n        inner_list_as_str = match[0][1]  # type: str\n        inner_list = eval_str_to_list(inner_list_as_str)  # type: List[str]\n        return inner_list\n    return list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scantree(path: str) -> Generator:\n    for entry in os.scandir(path):\n        if entry.is_dir(follow_symlinks=False):\n            yield from scantree(entry.path)\n        else:\n            if entry.name.endswith('.py'):\n                yield entry", "response": "Recursively yield DirEntry objects for given directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_contribs(def_dict_list: Dict[str, List[str]]) -> None:\n    file_str = ''  # type: str\n    note = '# Contributors\\nCLTK Core authors, ordered alphabetically by first name\\n\\n'  # type: str  # pylint: disable=line-too-long\n    file_str += note\n    for contrib in def_dict_list:\n        file_str += '## ' + contrib + '\\n'\n        for module in def_dict_list[contrib]:\n            file_str += '* ' + module + '\\n'\n        file_str += '\\n'\n    file_name = 'contributors.md'  # type: str\n    with open(file_name, 'w') as file_open:  # type: IO\n        file_open.write(file_str)\n    logger.info('Wrote contribs file at \"%s\".', file_name)", "response": "Write to file in current dir contributors. md."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsorts values of the lists of a defaultdict.", "response": "def sort_def_dict(def_dict: Dict[str, List[str]]) -> Dict[str, List[str]]:\n    \"\"\"Sort values of the lists of a defaultdict(list).\"\"\"\n    for _, dd_list in def_dict.items():\n        dd_list.sort()\n    return def_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlook for files find authors sort write file.", "response": "def find_write_contribs() -> None:\n    \"\"\"Look for files, find authors, sort, write file.\"\"\"\n    map_file_auth = {}  # type: Dict[str, List[str]]\n    for filename in scantree('cltk'):\n        filepath = filename.path  # type: str\n        authors_list = get_authors(filepath)  # type: List[str]\n        if authors_list:\n            map_file_auth[filepath] = authors_list\n\n    map_auth_file = defaultdict(list)  # type: Dict[str, List[str]]\n    for file, authors_file in map_file_auth.items():\n        for author in authors_file:\n            map_auth_file[author].append(file)\n    # now sort the str contents of the list value\n    map_auth_file = sort_def_dict(map_auth_file)\n    map_auth_file_sorted = sorted(map_auth_file.items())  # type: List[Tuple[str, List[str]]]\n    map_auth_file = OrderedDict(map_auth_file_sorted)\n\n    write_contribs(map_auth_file)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_first_sounds(self):\n        self.first_sounds = []\n        for viisuord in self.phonological_features_text:\n            self.first_sounds.append(viisuord[0])", "response": "Gets the first sound of each word of the ShortLine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_phonetics(self, transcriber):\n        for viisuordh in tokenize_old_norse_words(self.text):\n            word = normalize(viisuordh)\n            if word:\n                transcribed_word = transcriber.text_to_phonetic_representation(word)\n                pfl = transcriber.text_to_phonemes(word)\n\n                self.transcribed.append(transcribed_word)\n                self.phonological_features_text.append(pfl)\n        self.get_first_sounds()", "response": "This method converts the text to Phonetics."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind alliterations and return them as a list of tuples.", "response": "def find_alliterations(self):\n        \"\"\"\n        Alliterations is the repetition of a same sound pattern (usually the first sound) of important words.\n        This usually excludes stop words.\n        :return:\n        \"\"\"\n        self.n_alliterations = 0\n        self.alliterations = []\n        for j, sound1 in enumerate(self.first_sounds):\n            word1 = normalize(self.tokenized_text[j])\n            if j < len(self.first_sounds)-1:\n                for k, sound2 in enumerate(self.first_sounds[j+1:]):\n                    word2 = normalize(self.tokenized_text[k])\n                    if word1 not in STOPS_LIST and sound2 not in STOPS_LIST:\n                        if isinstance(sound1, Consonant) and sound1.ipar == sound2.ipar:\n                            self.alliterations.append((word1, word2))\n                            self.n_alliterations += 1\n                        elif isinstance(sound1, Vowel) and isinstance(sound2, Vowel):\n                            self.alliterations.append((word1, word2))\n                            self.n_alliterations += 1\n        return self.alliterations, self.n_alliterations"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the transcribed words in verse to phonetics.", "response": "def to_phonetics(self):\n        \"\"\"\n        Transcribing words in verse helps find alliteration.\n        \"\"\"\n        if len(self.long_lines) == 0:\n            logger.error(\"No text was imported\")\n            self.syllabified_text = []\n        else:\n            transcriber = Transcriber(DIPHTHONGS_IPA, DIPHTHONGS_IPA_class, IPA_class, old_norse_rules)\n            transcribed_text = []\n            phonological_features_text = []\n            for i, long_line in enumerate(self.long_lines):\n                transcribed_text.append([])\n                phonological_features_text.append([])\n                for short_line in long_line:\n                    assert isinstance(short_line, ShortLine) or isinstance(short_line, LongLine)\n                    short_line.to_phonetics(transcriber)\n                    transcribed_text[i].append(short_line.transcribed)\n                    phonological_features_text[i].append(short_line.phonological_features_text)\n\n            self.transcribed_text = transcribed_text\n            self.phonological_features_text = phonological_features_text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_alliteration(self):\n        if len(self.phonological_features_text) == 0:\n            logger.error(\"No phonological transcription found\")\n            raise ValueError\n        else:\n            first_sounds = []\n            for i, line in enumerate(self.phonological_features_text):\n                first_sounds.append([])\n                for j, short_line in enumerate(line):\n                    first_sounds[i].append([])\n                    for viisuord in short_line:\n                        first_sounds[i][j].append(viisuord[0])\n\n            verse_alliterations = []\n            n_alliterations_lines = []\n            for i, first_sound_line in enumerate(first_sounds):\n                if isinstance(self.long_lines[i][0], ShortLine) and isinstance(self.long_lines[i][1], ShortLine):\n                    self.long_lines[i][0].get_first_sounds()\n                    self.long_lines[i][1].get_first_sounds()\n                    alli, counter = self.long_lines[i][0].find_alliterations(self.long_lines[i][1])\n                    verse_alliterations.append(alli)\n                    n_alliterations_lines.append(counter)\n                elif isinstance(self.long_lines[i][0], LongLine):\n                    self.long_lines[i][0].get_first_sounds()\n                    alli, counter = self.long_lines[i][0].find_alliterations()\n                    verse_alliterations.append(alli)\n                    n_alliterations_lines.append(counter)\n            return verse_alliterations, n_alliterations_lines", "response": "Find alliterations in the complete verse."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_short_lines_text(self, text: str):\n        Metre.from_short_lines_text(self, text)\n        self.short_lines = [ShortLine(line) for line in text.split(\"\\n\") if line]\n        self.long_lines = None", "response": "Example from V\u00f6lsup\u00e1 28 - 11 - 12 - 13"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsyllabify the given hierarchy.", "response": "def syllabify(self, hierarchy):\n        \"\"\"\n        >>> stanza = \"Ein sat hon \u00fati,\\\\n\u00fe\u00e1 er inn aldni kom\\\\nyggjungr \u00e1sa\\\\nok \u00ed augu leit.\\\\nHvers fregni\u00f0 mik?\\\\nHv\u00ed freisti\u00f0 m\u00edn?\\\\nAllt veit ek, \u00d3\u00f0inn,\\\\nhvar \u00fe\u00fa auga falt,\\\\n\u00ed inum m\u00e6ra\\\\nM\u00edmisbrunni.\\\\nDrekkr mj\u00f6\u00f0 M\u00edmir\\\\nmorgun hverjan\\\\naf ve\u00f0i Valf\u00f6\u00f0rs.\\\\nVitu\u00f0 \u00e9r enn - e\u00f0a hvat?\"\n        >>> us = UnspecifiedStanza()\n        >>> us.from_short_lines_text(stanza)\n        >>> us.syllabify(old_norse_syllabifier.hierarchy)\n        >>> us.syllabified_text\n        [[['ein'], ['sat'], ['hon'], ['\u00fat', 'i']], [['\u00fe\u00e1'], ['er'], ['inn'], ['al', 'dni'], ['kom']], [['yg', 'gjungr'], ['\u00e1s', 'a']], [['ok'], ['\u00ed'], ['aug', 'u'], ['leit']], [['hvers'], ['freg', 'ni\u00f0'], ['mik']], [['hv\u00ed'], ['freis', 'ti\u00f0'], ['m\u00edn']], [['allt'], ['veit'], ['ek'], ['\u00f3', '\u00f0inn']], [['hvar'], ['\u00fe\u00fa'], ['aug', 'a'], ['falt']], [['\u00ed'], ['i', 'num'], ['m\u00e6r', 'a']], [['m\u00ed', 'mis', 'brun', 'ni']], [['drekkr'], ['mj\u00f6\u00f0'], ['m\u00ed', 'mir']], [['mor', 'gun'], ['hver', 'jan']], [['af'], ['ve\u00f0', 'i'], ['val', 'f\u00f6\u00f0rs']], [['vi', 'tu\u00f0'], ['\u00e9r'], ['enn'], ['e\u00f0', 'a'], ['hvat']]]\n\n        :param hierarchy:\n        :return:\n        \"\"\"\n        syllabifier = Syllabifier(language=\"old_norse\", break_geminants=True)\n        syllabifier.set_hierarchy(hierarchy)\n        syllabified_text = []\n        for short_line in self.short_lines:\n            assert isinstance(short_line, ShortLine)\n            short_line.syllabify(syllabifier)\n            syllabified_text.append(short_line.syllabified)\n        self.syllabified_text = syllabified_text"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the current object to a list of phonetics.", "response": "def to_phonetics(self):\n        \"\"\"\n        >>> stanza = \"Ein sat hon \u00fati,\\\\n\u00fe\u00e1 er inn aldni kom\\\\nyggjungr \u00e1sa\\\\nok \u00ed augu leit.\\\\nHvers fregni\u00f0 mik?\\\\nHv\u00ed freisti\u00f0 m\u00edn?\\\\nAllt veit ek, \u00d3\u00f0inn,\\\\nhvar \u00fe\u00fa auga falt,\\\\n\u00ed inum m\u00e6ra\\\\nM\u00edmisbrunni.\\\\nDrekkr mj\u00f6\u00f0 M\u00edmir\\\\nmorgun hverjan\\\\naf ve\u00f0i Valf\u00f6\u00f0rs.\\\\nVitu\u00f0 \u00e9r enn - e\u00f0a hvat?\"\n        >>> us = UnspecifiedStanza()\n        >>> us.from_short_lines_text(stanza)\n        >>> us.to_phonetics()\n        >>> us.transcribed_text\n        [['[\u025bin]', '[sat]', '[h\u0254n]', '[u\u02d0ti]'], ['[\u03b8a\u02d0]', '[\u025br]', '[in\u02d0]', '[aldni]', '[k\u0254m]'], ['[yg\u02d0jun\u0263r]', '[a\u02d0sa]'], ['[\u0254k]', '[i\u02d0]', '[\u0252u\u0263u]', '[l\u025bit]'], ['[hv\u025brs]', '[fr\u025b\u0263ni\u00f0]', '[mik]'], ['[hvi\u02d0]', '[fr\u025bisti\u00f0]', '[mi\u02d0n]'], ['[al\u02d0t]', '[v\u025bit]', '[\u025bk]', '[o\u02d0\u00f0in\u02d0]'], ['[hvar]', '[\u03b8u\u02d0]', '[\u0252u\u0263a]', '[falt]'], ['[i\u02d0]', '[inum]', '[m\u025b\u02d0ra]'], ['[mi\u02d0misbrun\u02d0i]'], ['[dr\u025bk\u02d0r]', '[mj\u0153\u00f0]', '[mi\u02d0mir]'], ['[m\u0254r\u0263un]', '[hv\u025brjan]'], ['[av]', '[v\u025b\u00f0i]', '[valv\u0153\u00f0rs]'], ['[vitu\u00f0]', '[e\u02d0r]', '[\u025bn\u02d0]', '[\u025b\u00f0a]', '[hvat]']]\n\n        :return:\n        \"\"\"\n        transcriber = Transcriber(DIPHTHONGS_IPA, DIPHTHONGS_IPA_class, IPA_class, old_norse_rules)\n        transcribed_text = []\n        phonological_features_text = []\n        for short_line in self.short_lines:\n            assert isinstance(short_line, ShortLine) or isinstance(short_line, LongLine)\n            short_line.to_phonetics(transcriber)\n            transcribed_text.append(short_line.transcribed)\n            phonological_features_text.append(short_line.phonological_features_text)\n        self.transcribed_text = transcribed_text\n        self.phonological_features_text = phonological_features_text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_short_lines_text(self, text: str):\n        self.text = text\n        self.short_lines = [ShortLine(line) for line in text.split(\"\\n\") if line]\n        self.long_lines = [self.short_lines[2*i:2*i+2] for i in range(int(floor(len(self.short_lines)/2)))]", "response": "Famous example from V\u00f6lsup\u00e1 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_short_lines_text(self, text: str):\n        Metre.from_short_lines_text(self, text)\n        lines = [line for line in text.split(\"\\n\") if line]\n        self.short_lines = [ShortLine(lines[0]), ShortLine(lines[1]), LongLine(lines[2]), ShortLine(lines[3]),\n                            ShortLine(lines[4]), LongLine(lines[5])]\n        self.long_lines = [self.short_lines[0:2], [self.short_lines[2]], self.short_lines[3:5], [self.short_lines[5]]]", "response": "Famous example from H\u00e1vam\u00e1l 77\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the word with IPA representation and stores the transcription of each syllable in the internal list.", "response": "def parse_word_with(self, poetry_tools: PoetryTools):\n        \"\"\"\n        Compute the phonetic transcription of the word with IPA representation\n        Compute the syllables of the word\n        Compute the length of each syllable\n        Compute if a syllable is stress of noe\n        Compute the POS category the word is in\n\n        :param poetry_tools: instance of PoetryTools\n        :return:\n        \"\"\"\n        phonemes = poetry_tools.tr.text_to_phonemes(self.text)\n        self.syl = poetry_tools.syllabifier.syllabify_phonemes(phonemes)\n        for i, syllable in enumerate(self.syl):\n            self.ipa_transcription.append([])\n            syl_len = measure_old_norse_syllable(syllable).value\n            syl_stress = 1 if i == 0 else 0\n\n            self.length.append(syl_len)\n            self.stress.append(syl_stress)\n            for c in syllable:\n                self.ipa_transcription[i].append(c.ipar)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a root - to - leaf path from a treebank.", "response": "def get_paths(src):\n    \"\"\" Generates root-to-leaf paths, given a treebank in string format. Note that\n    get_path is an iterator and does not return all the paths simultaneously.\n\n    :param src: str: treebank\n\n    Examples:\n        >>> st = \"((IP-MAT-SPE (' ') (INTJ Yes) (, ,) (' ') (IP-MAT-PRN (NP-SBJ (PRO he)) (VBD seyde)) (, ,) (' ') (NP-SBJ (PRO I)) (MD shall)\t(VB promyse) (NP-OB2 (PRO you)) (IP-INF (TO to)\t(VB fullfylle) (NP-OB1 (PRO$ youre) (N desyre))) (. .) (' '))\"\n\n        Get the sixth generated path:\n\n        >>> list(get_paths(st))[5]\n        ['IP-MAT-SPE', 'IP-MAT-PRN', 'VBD', 'seyde']\n    \"\"\"\n    st = list()\n    tmp = ''\n    for let in src:\n        if let == '(':\n            if tmp != '':\n                st.append(tmp)\n                tmp = ''\n        elif let == ')':\n            if tmp != '':\n                st.append(tmp)\n                yield st\n            st = st[:-1 - (tmp != '')]\n            tmp = ''\n        elif let == ' ':\n            if tmp != '':\n                st.append(tmp)\n                tmp = ''\n        else:\n            tmp += let"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_treebanks(st):\n    d = dict()\n    for path in get_paths(st):\n        set_path(d, path[:-1], path[-1])\n    return d", "response": "Parses a treebank string into a nested dictionary of the treebanks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransliterates the given text into the specified language.", "response": "def transliterate(self, text, mode='Latin'):\n        \"\"\"\n        Transliterates Anglo-Saxon runes into latin and vice versa.\n\n        Sources:\n            http://www.arild-hauge.com/eanglor.htm\n            https://en.wikipedia.org/wiki/Anglo-Saxon_runes\n\n        :param text: str: The text to be transcribed\n        :param mode: Specifies transliteration mode, options:\n\n            Latin (default): Transliterates Anglo-Saxon runes into the latin\n            alphabet, using the Dickins system\n\n            Anglo-Saxon/Anglo-Frisian : Transliterates Latin text into Anglo-Saxon runes\n\n        Examples:\n            >>> Transliterate().transliterate(\"H\u01bf\u00e6t \u01f7e Gardena in geardagum\", \"Anglo-Saxon\")\n            '\u16bb\u16b9\u16ab\u16cf \u16b9\u16d6 \u16b7\u16aa\u16b1\u16de\u16d6\u16be\u16aa \u16c1\u16be \u16b7\u16e0\u16b1\u16de\u16aa\u16b7\u16a2\u16d7'\n\n            >>> Transliterate().transliterate(\"\u16a9\u16a0\u16cf \u16cb\u16b3\u16a3\u16da\u16de \u16cb\u16b3\u16d6\u16a0\u16c1\u16dd \u16cb\u16b3\u16e0\u16a6\u16d6\u16be\u16aa \u16a6\u16b1\u16e0\u16cf\u16a2\u16d7\", \"Latin\")\n            'oft scyld scefin sceathena threatum'\n        \"\"\"\n        if mode == 'Latin':\n            return Transliterate.__transliterate_helper(text, L_Transliteration)\n\n        elif mode in ['Anglo-Saxon', 'Anglo-Frisian']:\n            return Transliterate.__transliterate_helper(text, R_Transliteration)\n\n        else:\n            LOG.error(\"The specified mode is currently not supported\")\n            raise InputError(\"The specified mode is currently not supported\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_diacritics(self):\n\n        w = ''\n        for c in unicodedata.normalize('NFKD', self.word):\n            if 'LATIN' == unicodedata.name(c)[:5]:\n                w += c\n\n        return w", "response": "Returns the input string stripped of its diacritics."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the ASCII - encoded version of the locale.", "response": "def ascii_encoding(self):\n        \"\"\"\n        :return: str: Returns the ASCII-encoded string\n\n        Thorn (\u00de, \u00fe) and Ash(\u00c6, \u00e6) are substituted by the digraphs\n        'th' and 'ae' respectively. Wynn(\u01f7, \u01bf) and Eth(\u00d0, \u00f0) are replaced\n        by 'w' and 'd'.\n\n        Examples:\n\n            >>> Word('\u0121el\u01e3d').ascii_encoding()\n            'gelaed'\n\n            >>> Word('\u01bfeor\u00f0unga').ascii_encoding()\n            'weordunga'\n\n        \"\"\"\n\n        w = self.remove_diacritics()\n\n        for k, val in zip(Normalize.keys(), Normalize.values()):\n            w = w.replace(k, val)\n\n        return w"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear_objects(self, obj):\n        for obj_name, obj_mjcf in self.mujoco_objects.items():\n            if obj_name == obj:\n                continue\n            else:\n                sim_state = self.sim.get_state()\n                # print(self.sim.model.get_joint_qpos_addr(obj_name))\n                sim_state.qpos[self.sim.model.get_joint_qpos_addr(obj_name)[0]] = 10\n                self.sim.set_state(sim_state)\n                self.sim.forward()", "response": "Clears all objects with name obj out of the task space."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of staged rewards based on current physical states.", "response": "def staged_rewards(self):\n        \"\"\"\n        Returns staged rewards based on current physical states.\n        Stages consist of reaching, grasping, lifting, and hovering.\n        \"\"\"\n\n        reach_mult = 0.1\n        grasp_mult = 0.35\n        lift_mult = 0.5\n        hover_mult = 0.7\n\n        # filter out objects that are already on the correct pegs\n        names_to_reach = []\n        objs_to_reach = []\n        geoms_to_grasp = []\n        geoms_by_array = []\n\n        for i in range(len(self.ob_inits)):\n            if self.objects_on_pegs[i]:\n                continue\n            obj_str = str(self.item_names[i]) + \"0\"\n            names_to_reach.append(obj_str)\n            objs_to_reach.append(self.obj_body_id[obj_str])\n            geoms_to_grasp.extend(self.obj_geom_id[obj_str])\n            geoms_by_array.append(self.obj_geom_id[obj_str])\n\n        ### reaching reward governed by distance to closest object ###\n        r_reach = 0.\n        if len(objs_to_reach):\n            # reaching reward via minimum distance to the handles of the objects (the last geom of each nut)\n            geom_ids = [elem[-1] for elem in geoms_by_array]\n            target_geom_pos = self.sim.data.geom_xpos[geom_ids]\n            gripper_site_pos = self.sim.data.site_xpos[self.eef_site_id]\n            dists = np.linalg.norm(\n                target_geom_pos - gripper_site_pos.reshape(1, -1), axis=1\n            )\n            r_reach = (1 - np.tanh(10.0 * min(dists))) * reach_mult\n\n        ### grasping reward for touching any objects of interest ###\n        touch_left_finger = False\n        touch_right_finger = False\n        for i in range(self.sim.data.ncon):\n            c = self.sim.data.contact[i]\n            if c.geom1 in geoms_to_grasp:\n                if c.geom2 in self.l_finger_geom_ids:\n                    touch_left_finger = True\n                if c.geom2 in self.r_finger_geom_ids:\n                    touch_right_finger = True\n            elif c.geom2 in geoms_to_grasp:\n                if c.geom1 in self.l_finger_geom_ids:\n                    touch_left_finger = True\n                if c.geom1 in self.r_finger_geom_ids:\n                    touch_right_finger = True\n        has_grasp = touch_left_finger and touch_right_finger\n        r_grasp = int(has_grasp) * grasp_mult\n\n        ### lifting reward for picking up an object ###\n        r_lift = 0.\n        if len(objs_to_reach) and r_grasp > 0.:\n            z_target = self.table_pos[2] + 0.2\n            object_z_locs = self.sim.data.body_xpos[objs_to_reach][:, 2]\n            z_dists = np.maximum(z_target - object_z_locs, 0.)\n            r_lift = grasp_mult + (1 - np.tanh(15.0 * min(z_dists))) * (\n                lift_mult - grasp_mult\n            )\n\n        ### hover reward for getting object above peg ###\n        r_hover = 0.\n        if len(objs_to_reach):\n            r_hovers = np.zeros(len(objs_to_reach))\n            for i in range(len(objs_to_reach)):\n                if names_to_reach[i].startswith(self.item_names[0]):\n                    peg_pos = self.peg1_pos[:2]\n                elif names_to_reach[i].startswith(self.item_names[1]):\n                    peg_pos = self.peg2_pos[:2]\n                else:\n                    raise Exception(\n                        \"Got invalid object to reach: {}\".format(names_to_reach[i])\n                    )\n                ob_xy = self.sim.data.body_xpos[objs_to_reach[i]][:2]\n                dist = np.linalg.norm(peg_pos - ob_xy)\n                r_hovers[i] = r_lift + (1 - np.tanh(10.0 * dist)) * (\n                    hover_mult - lift_mult\n                )\n            r_hover = np.max(r_hovers)\n\n        return r_reach, r_grasp, r_lift, r_hover"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_contact(self):\n        collision = False\n        for contact in self.sim.data.contact[: self.sim.data.ncon]:\n            if (\n                self.sim.model.geom_id2name(contact.geom1) in self.finger_names\n                or self.sim.model.geom_id2name(contact.geom2) in self.finger_names\n            ):\n                collision = True\n                break\n        return collision", "response": "Checks if gripper is in contact with an object. Returns True if gripper is in contact with an object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_success(self):\n\n        # remember objects that are on the correct pegs\n        gripper_site_pos = self.sim.data.site_xpos[self.eef_site_id]\n        for i in range(len(self.ob_inits)):\n            obj_str = str(self.item_names[i]) + \"0\"\n            obj_pos = self.sim.data.body_xpos[self.obj_body_id[obj_str]]\n            dist = np.linalg.norm(gripper_site_pos - obj_pos)\n            r_reach = 1 - np.tanh(10.0 * dist)\n            self.objects_on_pegs[i] = int(self.on_peg(obj_pos, i) and r_reach < 0.6)\n\n        if self.single_object_mode > 0:\n            return np.sum(self.objects_on_pegs) > 0  # need one object on peg\n\n        # returns True if all objects are on correct pegs\n        return np.sum(self.objects_on_pegs) == len(self.ob_inits)", "response": "Returns True if the task has been completed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo any needed visualization here. Overrides superclass implementation.", "response": "def _gripper_visualization(self):\n        \"\"\"\n        Do any needed visualization here. Overrides superclass implementations.\n        \"\"\"\n        # color the gripper site appropriately based on distance to nearest object\n        if self.gripper_visualization:\n            # find closest object\n            square_dist = lambda x: np.sum(\n                np.square(x - self.sim.data.get_site_xpos(\"grip_site\"))\n            )\n            dists = np.array(list(map(square_dist, self.sim.data.site_xpos)))\n            dists[self.eef_site_id] = np.inf  # make sure we don't pick the same site\n            dists[self.eef_cylinder_id] = np.inf\n            ob_dists = dists[\n                self.object_site_ids\n            ]  # filter out object sites we care about\n            min_dist = np.min(ob_dists)\n            ob_id = np.argmin(ob_dists)\n            ob_name = self.object_names[ob_id]\n\n            # set RGBA for the EEF site here\n            max_dist = 0.1\n            scaled = (1.0 - min(min_dist / max_dist, 1.)) ** 15\n            rgba = np.zeros(4)\n            rgba[0] = 1 - scaled\n            rgba[1] = scaled\n            rgba[3] = 0.5\n\n            self.sim.model.site_rgba[self.eef_site_id] = rgba"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_reference(self):\n        super()._get_reference()\n        self.cubeA_body_id = self.sim.model.body_name2id(\"cubeA\")\n        self.cubeB_body_id = self.sim.model.body_name2id(\"cubeB\")\n        self.l_finger_geom_ids = [\n            self.sim.model.geom_name2id(x) for x in self.gripper.left_finger_geoms\n        ]\n        self.r_finger_geom_ids = [\n            self.sim.model.geom_name2id(x) for x in self.gripper.right_finger_geoms\n        ]\n        self.cubeA_geom_id = self.sim.model.geom_name2id(\"cubeA\")\n        self.cubeB_geom_id = self.sim.model.geom_name2id(\"cubeB\")", "response": "Sets up references to important components."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset_internal(self):\n        super()._reset_internal()\n\n        # reset positions of objects\n        self.model.place_objects()\n\n        # reset joint positions\n        init_pos = np.array([-0.5538, -0.8208, 0.4155, 1.8409, -0.4955, 0.6482, 1.9628])\n        init_pos += np.random.randn(init_pos.shape[0]) * 0.02\n        self.sim.data.qpos[self._ref_joint_pos_indexes] = np.array(init_pos)", "response": "Resets the internal configuration of the object store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrewarding function for the task. The dense reward has five components. Reaching: in [0, 1], to encourage the arm to reach the cube Grasping: in {0, 0.25}, non-zero if arm is grasping the cube Lifting: in {0, 1}, non-zero if arm has lifted the cube Aligning: in [0, 0.5], encourages aligning one cube over the other Stacking: in {0, 2}, non-zero if cube is stacked on other cube The sparse reward only consists of the stacking component. However, the sparse reward is either 0 or 1. Args: action (np array): unused for this task Returns: reward (float): the reward", "response": "def reward(self, action):\n        \"\"\"\n        Reward function for the task.\n\n        The dense reward has five components.\n\n            Reaching: in [0, 1], to encourage the arm to reach the cube\n            Grasping: in {0, 0.25}, non-zero if arm is grasping the cube\n            Lifting: in {0, 1}, non-zero if arm has lifted the cube\n            Aligning: in [0, 0.5], encourages aligning one cube over the other\n            Stacking: in {0, 2}, non-zero if cube is stacked on other cube\n\n        The sparse reward only consists of the stacking component.\n        However, the sparse reward is either 0 or 1.\n\n        Args:\n            action (np array): unused for this task\n\n        Returns:\n            reward (float): the reward\n        \"\"\"\n        r_reach, r_lift, r_stack = self.staged_rewards()\n        if self.reward_shaping:\n            reward = max(r_reach, r_lift, r_stack)\n        else:\n            reward = 1.0 if r_stack > 0 else 0.0\n\n        return reward"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an OrderedDict containing observations for the current object.", "response": "def _get_observation(self):\n        \"\"\"\n        Returns an OrderedDict containing observations [(name_string, np.array), ...].\n\n        Important keys:\n            robot-state: contains robot-centric information.\n            object-state: requires @self.use_object_obs to be True.\n                contains object-centric information.\n            image: requires @self.use_camera_obs to be True.\n                contains a rendered frame from the simulation.\n            depth: requires @self.use_camera_obs and @self.camera_depth to be True.\n                contains a rendered depth map from the simulation\n        \"\"\"\n        di = super()._get_observation()\n        if self.use_camera_obs:\n            camera_obs = self.sim.render(\n                camera_name=self.camera_name,\n                width=self.camera_width,\n                height=self.camera_height,\n                depth=self.camera_depth,\n            )\n            if self.camera_depth:\n                di[\"image\"], di[\"depth\"] = camera_obs\n            else:\n                di[\"image\"] = camera_obs\n\n        # low-level object information\n        if self.use_object_obs:\n            # position and rotation of the first cube\n            cubeA_pos = np.array(self.sim.data.body_xpos[self.cubeA_body_id])\n            cubeA_quat = convert_quat(\n                np.array(self.sim.data.body_xquat[self.cubeA_body_id]), to=\"xyzw\"\n            )\n            di[\"cubeA_pos\"] = cubeA_pos\n            di[\"cubeA_quat\"] = cubeA_quat\n\n            # position and rotation of the second cube\n            cubeB_pos = np.array(self.sim.data.body_xpos[self.cubeB_body_id])\n            cubeB_quat = convert_quat(\n                np.array(self.sim.data.body_xquat[self.cubeB_body_id]), to=\"xyzw\"\n            )\n            di[\"cubeB_pos\"] = cubeB_pos\n            di[\"cubeB_quat\"] = cubeB_quat\n\n            # relative positions between gripper and cubes\n            gripper_site_pos = np.array(self.sim.data.site_xpos[self.eef_site_id])\n            di[\"gripper_to_cubeA\"] = gripper_site_pos - cubeA_pos\n            di[\"gripper_to_cubeB\"] = gripper_site_pos - cubeB_pos\n            di[\"cubeA_to_cubeB\"] = cubeA_pos - cubeB_pos\n\n            di[\"object-state\"] = np.concatenate(\n                [\n                    cubeA_pos,\n                    cubeA_quat,\n                    cubeB_pos,\n                    cubeB_quat,\n                    di[\"gripper_to_cubeA\"],\n                    di[\"gripper_to_cubeB\"],\n                    di[\"cubeA_to_cubeB\"],\n                ]\n            )\n\n        return di"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_arena(self, mujoco_arena):\n        self.arena = mujoco_arena\n        self.table_offset = mujoco_arena.table_top_abs\n        self.table_size = mujoco_arena.table_full_size\n        self.table_body = mujoco_arena.table_body\n        self.peg1_body = mujoco_arena.peg1_body\n        self.peg2_body = mujoco_arena.peg2_body\n        self.merge(mujoco_arena)", "response": "Adds the arena model to the MJCF model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding physical objects to the MJCF model.", "response": "def merge_objects(self, mujoco_objects):\n        \"\"\"Adds physical objects to the MJCF model.\"\"\"\n        self.mujoco_objects = mujoco_objects\n        self.objects = {}  # xml manifestation\n        self.max_horizontal_radius = 0\n        for obj_name, obj_mjcf in mujoco_objects.items():\n            self.merge_asset(obj_mjcf)\n            # Load object\n            obj = obj_mjcf.get_collision(name=obj_name, site=True)\n            obj.append(new_joint(name=obj_name, type=\"free\", damping=\"0.0005\"))\n            self.objects[obj_name] = obj\n            self.worldbody.append(obj)\n\n            self.max_horizontal_radius = max(\n                self.max_horizontal_radius, obj_mjcf.get_horizontal_radius()\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplacing objects randomly until no collisions or max iterations hit.", "response": "def place_objects(self):\n        \"\"\"Places objects randomly until no collisions or max iterations hit.\"\"\"\n        pos_arr, quat_arr = self.initializer.sample()\n        for k, obj_name in enumerate(self.objects):\n            self.objects[obj_name].set(\"pos\", array_to_string(pos_arr[k]))\n            self.objects[obj_name].set(\"quat\", array_to_string(quat_arr[k]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hide_visualization(self):\n        for site_name in self.visualization_sites:\n            site = self.worldbody.find(\".//site[@name='{}']\".format(site_name))\n            site.set(\"rgba\", \"0 0 0 0\")\n        for geom_name in self.visualization_geoms:\n            geom = self.worldbody.find(\".//geom[@name='{}']\".format(geom_name))\n            geom.set(\"rgba\", \"0 0 0 0\")", "response": "Hides all visualization sites and geometries and agents."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_model(self):\n        super()._load_model()\n        self.mujoco_robot.set_base_xpos([0, 0, 0])\n\n        # load model for table top workspace\n        self.mujoco_arena = TableArena(\n            table_full_size=self.table_full_size, table_friction=self.table_friction\n        )\n        if self.use_indicator_object:\n            self.mujoco_arena.add_pos_indicator()\n\n        # The sawyer robot has a pedestal, we want to align it with the table\n        self.mujoco_arena.set_origin([0.45 + self.table_full_size[0] / 2, 0, 0])\n\n        # task includes arena, robot, and objects of interest\n        self.model = TableTopTask(\n            self.mujoco_arena,\n            self.mujoco_robot,\n            self.mujoco_objects,\n            self.object_initializer,\n        )\n        self.model.place_objects()", "response": "Loads the arena and pot object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_reference(self):\n        super()._get_reference()\n        self.cube_body_id = self.sim.model.body_name2id(\"pot\")\n        self.handle_1_site_id = self.sim.model.site_name2id(\"pot_handle_1\")\n        self.handle_2_site_id = self.sim.model.site_name2id(\"pot_handle_2\")\n        self.table_top_id = self.sim.model.site_name2id(\"table_top\")\n        self.pot_center_id = self.sim.model.site_name2id(\"pot_center\")", "response": "Sets up references to important components."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reward(self, action):\n        reward = 0\n\n        cube_height = self.sim.data.site_xpos[self.pot_center_id][2] - self.pot.get_top_offset()[2]\n        table_height = self.sim.data.site_xpos[self.table_top_id][2]\n\n        # check if the pot is tilted more than 30 degrees\n        mat = T.quat2mat(self._pot_quat)\n        z_unit = [0, 0, 1]\n        z_rotated = np.matmul(mat, z_unit)\n        cos_z = np.dot(z_unit, z_rotated)\n        cos_30 = np.cos(np.pi / 6)\n        direction_coef = 1 if cos_z >= cos_30 else 0\n\n        # cube is higher than the table top above a margin\n        if cube_height > table_height + 0.15:\n            reward = 1.0 * direction_coef\n\n        # use a shaping reward\n        if self.reward_shaping:\n            reward = 0\n\n            # lifting reward\n            elevation = cube_height - table_height\n            r_lift = min(max(elevation - 0.05, 0), 0.2)\n            reward += 10. * direction_coef * r_lift\n\n            l_gripper_to_handle = self._l_gripper_to_handle\n            r_gripper_to_handle = self._r_gripper_to_handle\n\n            # gh stands for gripper-handle\n            # When grippers are far away, tell them to be closer\n            l_contacts = list(\n                self.find_contacts(\n                    self.gripper_left.contact_geoms(), self.pot.handle_1_geoms()\n                )\n            )\n            r_contacts = list(\n                self.find_contacts(\n                    self.gripper_right.contact_geoms(), self.pot.handle_2_geoms()\n                )\n            )\n            l_gh_dist = np.linalg.norm(l_gripper_to_handle)\n            r_gh_dist = np.linalg.norm(r_gripper_to_handle)\n\n            if len(l_contacts) > 0:\n                reward += 0.5\n            else:\n                reward += 0.5 * (1 - np.tanh(l_gh_dist))\n\n            if len(r_contacts) > 0:\n                reward += 0.5\n            else:\n                reward += 0.5 * (1 - np.tanh(r_gh_dist))\n\n        return reward", "response": "Reward function for the task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pot_quat(self):\n        return T.convert_quat(self.sim.data.body_xquat[self.cube_body_id], to=\"xyzw\")", "response": "Returns the orientation of the potential."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an OrderedDict containing observations for the current object.", "response": "def _get_observation(self):\n        \"\"\"\n        Returns an OrderedDict containing observations [(name_string, np.array), ...].\n\n        Important keys:\n            robot-state: contains robot-centric information.\n            object-state: requires @self.use_object_obs to be True.\n                contains object-centric information.\n            image: requires @self.use_camera_obs to be True.\n                contains a rendered frame from the simulation.\n            depth: requires @self.use_camera_obs and @self.camera_depth to be True.\n                contains a rendered depth map from the simulation\n        \"\"\"\n        di = super()._get_observation()\n        # camera observations\n        if self.use_camera_obs:\n            camera_obs = self.sim.render(\n                camera_name=self.camera_name,\n                width=self.camera_width,\n                height=self.camera_height,\n                depth=self.camera_depth,\n            )\n            if self.camera_depth:\n                di[\"image\"], di[\"depth\"] = camera_obs\n            else:\n                di[\"image\"] = camera_obs\n\n        # low-level object information\n        if self.use_object_obs:\n            # position and rotation of object\n            cube_pos = self.sim.data.body_xpos[self.cube_body_id]\n            cube_quat = T.convert_quat(\n                self.sim.data.body_xquat[self.cube_body_id], to=\"xyzw\"\n            )\n            di[\"cube_pos\"] = cube_pos\n            di[\"cube_quat\"] = cube_quat\n\n            di[\"l_eef_xpos\"] = self._l_eef_xpos\n            di[\"r_eef_xpos\"] = self._r_eef_xpos\n            di[\"handle_1_xpos\"] = self._handle_1_xpos\n            di[\"handle_2_xpos\"] = self._handle_2_xpos\n            di[\"l_gripper_to_handle\"] = self._l_gripper_to_handle\n            di[\"r_gripper_to_handle\"] = self._r_gripper_to_handle\n\n            di[\"object-state\"] = np.concatenate(\n                [\n                    di[\"cube_pos\"],\n                    di[\"cube_quat\"],\n                    di[\"l_eef_xpos\"],\n                    di[\"r_eef_xpos\"],\n                    di[\"handle_1_xpos\"],\n                    di[\"handle_2_xpos\"],\n                    di[\"l_gripper_to_handle\"],\n                    di[\"r_gripper_to_handle\"],\n                ]\n            )\n\n        return di"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_new_episode(self):\n\n        # flush any data left over from the previous episode if any interactions have happened\n        if self.has_interaction:\n            self._flush()\n\n        # timesteps in current episode\n        self.t = 0\n        self.has_interaction = False", "response": "Start a new episode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _flush(self):\n        t1, t2 = str(time.time()).split(\".\")\n        state_path = os.path.join(self.ep_directory, \"state_{}_{}.npz\".format(t1, t2))\n        if hasattr(self.env, \"unwrapped\"):\n            env_name = self.env.unwrapped.__class__.__name__\n        else:\n            env_name = self.env.__class__.__name__\n        np.savez(\n            state_path,\n            states=np.array(self.states),\n            action_infos=self.action_infos,\n            env=env_name,\n        )\n        self.states = []\n        self.action_infos = []", "response": "Method to flush internal state to disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts every file dependency into absolute path so when we merge we don t break things.", "response": "def resolve_asset_dependency(self):\n        \"\"\"\n        Converts every file dependency into absolute path so when we merge we don't break things.\n        \"\"\"\n\n        for node in self.asset.findall(\"./*[@file]\"):\n            file = node.get(\"file\")\n            abs_path = os.path.abspath(self.folder)\n            abs_path = os.path.join(abs_path, file)\n            node.set(\"file\", abs_path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a default element under root if there is none.", "response": "def create_default_element(self, name):\n        \"\"\"\n        Creates a <@name/> tag under root if there is none.\n        \"\"\"\n\n        found = self.root.find(name)\n        if found is not None:\n            return found\n        ele = ET.Element(name)\n        self.root.append(ele)\n        return ele"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge the contents of other into self.", "response": "def merge(self, other, merge_body=True):\n        \"\"\"\n        Default merge method.\n\n        Args:\n            other: another MujocoXML instance\n                raises XML error if @other is not a MujocoXML instance.\n                merges <worldbody/>, <actuator/> and <asset/> of @other into @self\n            merge_body: True if merging child bodies of @other. Defaults to True.\n        \"\"\"\n        if not isinstance(other, MujocoXML):\n            raise XMLError(\"{} is not a MujocoXML instance.\".format(type(other)))\n        if merge_body:\n            for body in other.worldbody:\n                self.worldbody.append(body)\n        self.merge_asset(other)\n        for one_actuator in other.actuator:\n            self.actuator.append(one_actuator)\n        for one_equality in other.equality:\n            self.equality.append(one_equality)\n        for one_contact in other.contact:\n            self.contact.append(one_contact)\n        for one_default in other.default:\n            self.default.append(one_default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a MjModel instance from the current xml tree.", "response": "def get_model(self, mode=\"mujoco_py\"):\n        \"\"\"\n        Returns a MjModel instance from the current xml tree.\n        \"\"\"\n\n        available_modes = [\"mujoco_py\"]\n        with io.StringIO() as string:\n            string.write(ET.tostring(self.root, encoding=\"unicode\"))\n            if mode == \"mujoco_py\":\n                from mujoco_py import load_model_from_xml\n\n                model = load_model_from_xml(string.getvalue())\n                return model\n            raise ValueError(\n                \"Unkown model mode: {}. Available options are: {}\".format(\n                    mode, \",\".join(available_modes)\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string of the MJCF XML file.", "response": "def get_xml(self):\n        \"\"\"\n        Returns a string of the MJCF XML file.\n        \"\"\"\n        with io.StringIO() as string:\n            string.write(ET.tostring(self.root, encoding=\"unicode\"))\n            return string.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the xml to file.", "response": "def save_model(self, fname, pretty=False):\n        \"\"\"\n        Saves the xml to file.\n\n        Args:\n            fname: output file location\n            pretty: attempts!! to pretty print the output\n        \"\"\"\n        with open(fname, \"w\") as f:\n            xml_str = ET.tostring(self.root, encoding=\"unicode\")\n            if pretty:\n                # TODO: get a better pretty print library\n                parsed_xml = xml.dom.minidom.parseString(xml_str)\n                xml_str = parsed_xml.toprettyxml(newl=\"\")\n            f.write(xml_str)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd asset to the end of the asset list.", "response": "def merge_asset(self, other):\n        \"\"\"\n        Useful for merging other files in a custom logic.\n        \"\"\"\n        for asset in other.asset:\n            asset_name = asset.get(\"name\")\n            asset_type = asset.tag\n            # Avoids duplication\n            pattern = \"./{}[@name='{}']\".format(asset_type, asset_name)\n            if self.asset.find(pattern) is None:\n                self.asset.append(asset)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflatten the observations in a dictionary into a single array.", "response": "def _flatten_obs(self, obs_dict, verbose=False):\n        \"\"\"\n        Filters keys of interest out and concatenate the information.\n\n        Args:\n            obs_dict: ordered dictionary of observations\n        \"\"\"\n        ob_lst = []\n        for key in obs_dict:\n            if key in self.keys:\n                if verbose:\n                    print(\"adding key: {}\".format(key))\n                ob_lst.append(obs_dict[key])\n        return np.concatenate(ob_lst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(self, mujoco_objects, table_top_offset, table_size):\n        self.mujoco_objects = mujoco_objects\n        self.n_obj = len(self.mujoco_objects)\n        self.table_top_offset = table_top_offset\n        self.table_size = table_size", "response": "Sets up the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the arena model to the MJCF model.", "response": "def merge_arena(self, mujoco_arena):\n        \"\"\"Adds arena model to the MJCF model.\"\"\"\n        self.arena = mujoco_arena\n        self.bin_offset = mujoco_arena.bin_abs\n        self.bin_size = mujoco_arena.table_full_size\n        self.bin2_body = mujoco_arena.bin2_body\n        self.merge(mujoco_arena)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_visual(self, mujoco_objects):\n        self.visual_obj_mjcf = []\n        for obj_name, obj_mjcf in mujoco_objects.items():\n            self.merge_asset(obj_mjcf)\n            # Load object\n            obj = obj_mjcf.get_visual(name=obj_name, site=False)\n            self.visual_obj_mjcf.append(obj)\n            self.worldbody.append(obj)", "response": "Adds visual objects to the MJCF model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef place_objects(self):\n        placed_objects = []\n        index = 0\n\n        # place objects by rejection sampling\n        for _, obj_mjcf in self.mujoco_objects.items():\n            horizontal_radius = obj_mjcf.get_horizontal_radius()\n            bottom_offset = obj_mjcf.get_bottom_offset()\n            success = False\n            for _ in range(5000):  # 5000 retries\n                bin_x_half = self.bin_size[0] / 2 - horizontal_radius - 0.05\n                bin_y_half = self.bin_size[1] / 2 - horizontal_radius - 0.05\n                object_x = np.random.uniform(high=bin_x_half, low=-bin_x_half)\n                object_y = np.random.uniform(high=bin_y_half, low=-bin_y_half)\n\n                # make sure objects do not overlap\n                object_xy = np.array([object_x, object_y, 0])\n                pos = self.bin_offset - bottom_offset + object_xy\n                location_valid = True\n                for pos2, r in placed_objects:\n                    dist = np.linalg.norm(pos[:2] - pos2[:2], np.inf)\n                    if dist <= r + horizontal_radius:\n                        location_valid = False\n                        break\n\n                # place the object\n                if location_valid:\n                    # add object to the position\n                    placed_objects.append((pos, horizontal_radius))\n                    self.objects[index].set(\"pos\", array_to_string(pos))\n                    # random z-rotation\n                    quat = self.sample_quat()\n                    self.objects[index].set(\"quat\", array_to_string(quat))\n                    success = True\n                    break\n\n            # raise error if all objects cannot be placed after maximum retries\n            if not success:\n                raise RandomizationError(\"Cannot place all objects in the bins\")\n            index += 1", "response": "Places objects randomly until no collisions or max iterations hit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef place_visual(self):\n        index = 0\n        bin_pos = string_to_array(self.bin2_body.get(\"pos\"))\n        bin_size = self.bin_size\n\n        for _, obj_mjcf in self.visual_objects:\n\n            bin_x_low = bin_pos[0]\n            bin_y_low = bin_pos[1]\n            if index == 0 or index == 2:\n                bin_x_low -= bin_size[0] / 2\n            if index < 2:\n                bin_y_low -= bin_size[1] / 2\n\n            bin_x_high = bin_x_low + bin_size[0] / 2\n            bin_y_high = bin_y_low + bin_size[1] / 2\n            bottom_offset = obj_mjcf.get_bottom_offset()\n\n            bin_range = [bin_x_low + bin_x_high, bin_y_low + bin_y_high, 2 * bin_pos[2]]\n            bin_center = np.array(bin_range) / 2.0\n\n            pos = bin_center - bottom_offset\n            self.visual_obj_mjcf[index].set(\"pos\", array_to_string(pos))\n            index += 1", "response": "Places visual objects randomly until no collisions or max iterations hit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_model(self):\n        super()._load_model()\n        self.mujoco_robot.set_base_xpos([0, 0, 0])\n\n        # load model for table top workspace\n        self.mujoco_arena = TableArena(\n            table_full_size=self.table_full_size, table_friction=self.table_friction\n        )\n        if self.use_indicator_object:\n            self.mujoco_arena.add_pos_indicator()\n\n        # The sawyer robot has a pedestal, we want to align it with the table\n        self.mujoco_arena.set_origin([0.16 + self.table_full_size[0] / 2, 0, 0])\n\n        # initialize objects of interest\n        cube = BoxObject(\n            size_min=[0.020, 0.020, 0.020],  # [0.015, 0.015, 0.015],\n            size_max=[0.022, 0.022, 0.022],  # [0.018, 0.018, 0.018])\n            rgba=[1, 0, 0, 1],\n        )\n        self.mujoco_objects = OrderedDict([(\"cube\", cube)])\n\n        # task includes arena, robot, and objects of interest\n        self.model = TableTopTask(\n            self.mujoco_arena,\n            self.mujoco_robot,\n            self.mujoco_objects,\n            initializer=self.placement_initializer,\n        )\n        self.model.place_objects()", "response": "Loads an xml model for the current master and puts it in self. model"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_reference(self):\n        super()._get_reference()\n        self.cube_body_id = self.sim.model.body_name2id(\"cube\")\n        self.l_finger_geom_ids = [\n            self.sim.model.geom_name2id(x) for x in self.gripper.left_finger_geoms\n        ]\n        self.r_finger_geom_ids = [\n            self.sim.model.geom_name2id(x) for x in self.gripper.right_finger_geoms\n        ]\n        self.cube_geom_id = self.sim.model.geom_name2id(\"cube\")", "response": "Sets up references to important components."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reward(self, action=None):\n        reward = 0.\n\n        # sparse completion reward\n        if self._check_success():\n            reward = 1.0\n\n        # use a shaping reward\n        if self.reward_shaping:\n\n            # reaching reward\n            cube_pos = self.sim.data.body_xpos[self.cube_body_id]\n            gripper_site_pos = self.sim.data.site_xpos[self.eef_site_id]\n            dist = np.linalg.norm(gripper_site_pos - cube_pos)\n            reaching_reward = 1 - np.tanh(10.0 * dist)\n            reward += reaching_reward\n\n            # grasping reward\n            touch_left_finger = False\n            touch_right_finger = False\n            for i in range(self.sim.data.ncon):\n                c = self.sim.data.contact[i]\n                if c.geom1 in self.l_finger_geom_ids and c.geom2 == self.cube_geom_id:\n                    touch_left_finger = True\n                if c.geom1 == self.cube_geom_id and c.geom2 in self.l_finger_geom_ids:\n                    touch_left_finger = True\n                if c.geom1 in self.r_finger_geom_ids and c.geom2 == self.cube_geom_id:\n                    touch_right_finger = True\n                if c.geom1 == self.cube_geom_id and c.geom2 in self.r_finger_geom_ids:\n                    touch_right_finger = True\n            if touch_left_finger and touch_right_finger:\n                reward += 0.25\n\n        return reward", "response": "This function calculates the reward of the task based on the current task s state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_observation(self):\n        di = super()._get_observation()\n        # camera observations\n        if self.use_camera_obs:\n            camera_obs = self.sim.render(\n                camera_name=self.camera_name,\n                width=self.camera_width,\n                height=self.camera_height,\n                depth=self.camera_depth,\n            )\n            if self.camera_depth:\n                di[\"image\"], di[\"depth\"] = camera_obs\n            else:\n                di[\"image\"] = camera_obs\n\n        # low-level object information\n        if self.use_object_obs:\n            # position and rotation of object\n            cube_pos = np.array(self.sim.data.body_xpos[self.cube_body_id])\n            cube_quat = convert_quat(\n                np.array(self.sim.data.body_xquat[self.cube_body_id]), to=\"xyzw\"\n            )\n            di[\"cube_pos\"] = cube_pos\n            di[\"cube_quat\"] = cube_quat\n\n            gripper_site_pos = np.array(self.sim.data.site_xpos[self.eef_site_id])\n            di[\"gripper_to_cube\"] = gripper_site_pos - cube_pos\n\n            di[\"object-state\"] = np.concatenate(\n                [cube_pos, cube_quat, di[\"gripper_to_cube\"]]\n            )\n\n        return di", "response": "Returns an OrderedDict containing observations [ ( name_string np. array )... ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _gripper_visualization(self):\n\n        # color the gripper site appropriately based on distance to cube\n        if self.gripper_visualization:\n            # get distance to cube\n            cube_site_id = self.sim.model.site_name2id(\"cube\")\n            dist = np.sum(\n                np.square(\n                    self.sim.data.site_xpos[cube_site_id]\n                    - self.sim.data.get_site_xpos(\"grip_site\")\n                )\n            )\n\n            # set RGBA for the EEF site here\n            max_dist = 0.1\n            scaled = (1.0 - min(dist / max_dist, 1.)) ** 15\n            rgba = np.zeros(4)\n            rgba[0] = 1 - scaled\n            rgba[1] = scaled\n            rgba[3] = 0.5\n\n            self.sim.model.site_rgba[self.eef_site_id] = rgba", "response": "Do any needed visualization here. Overrides superclass implementation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncollect a human - readable trajectory of the environment.", "response": "def collect_human_trajectory(env, device):\n    \"\"\"\n    Use the device (keyboard or SpaceNav 3D mouse) to collect a demonstration.\n    The rollout trajectory is saved to files in npz format.\n    Modify the DataCollectionWrapper wrapper to add new fields or change data formats.\n\n    Args:\n        env: environment to control\n        device (instance of Device class): to receive controls from the device\n    \"\"\"\n\n    obs = env.reset()\n\n    # rotate the gripper so we can see it easily\n    env.set_robot_joint_positions([0, -1.18, 0.00, 2.18, 0.00, 0.57, 1.5708])\n\n    env.viewer.set_camera(camera_id=2)\n    env.render()\n\n    is_first = True\n\n    # episode terminates on a spacenav reset input or if task is completed\n    reset = False\n    task_completion_hold_count = -1 # counter to collect 10 timesteps after reaching goal\n    device.start_control()\n    while not reset:\n        state = device.get_controller_state()\n        dpos, rotation, grasp, reset = (\n            state[\"dpos\"],\n            state[\"rotation\"],\n            state[\"grasp\"],\n            state[\"reset\"],\n        )\n\n        # convert into a suitable end effector action for the environment\n        current = env._right_hand_orn\n        drotation = current.T.dot(rotation)  # relative rotation of desired from current\n        dquat = T.mat2quat(drotation)\n        grasp = grasp - 1.  # map 0 to -1 (open) and 1 to 0 (closed halfway)\n        action = np.concatenate([dpos, dquat, [grasp]])\n\n        obs, reward, done, info = env.step(action)\n\n        if is_first:\n            is_first = False\n\n            # We grab the initial model xml and state and reload from those so that\n            # we can support deterministic playback of actions from our demonstrations.\n            # This is necessary due to rounding issues with the model xml and with\n            # env.sim.forward(). We also have to do this after the first action is \n            # applied because the data collector wrapper only starts recording\n            # after the first action has been played.\n            initial_mjstate = env.sim.get_state().flatten()\n            xml_str = env.model.get_xml()\n            env.reset_from_xml_string(xml_str)\n            env.sim.reset()\n            env.sim.set_state_from_flattened(initial_mjstate)\n            env.sim.forward()\n            env.viewer.set_camera(camera_id=2)\n\n        env.render()\n\n        if task_completion_hold_count == 0:\n            break\n\n        # state machine to check for having a success for 10 consecutive timesteps\n        if env._check_success():\n            if task_completion_hold_count > 0:\n                task_completion_hold_count -= 1 # latched state, decrement count\n            else:\n                task_completion_hold_count = 10 # reset count on first success timestep\n        else:\n            task_completion_hold_count = -1 # null the counter if there's no success\n\n    # cleanup for end of data collection episodes\n    env.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gather_demonstrations_as_hdf5(directory, out_dir):\n\n    # store model xmls in this directory\n    model_dir = os.path.join(out_dir, \"models\")\n    if os.path.isdir(model_dir):\n        shutil.rmtree(model_dir)\n    os.makedirs(model_dir)\n\n    hdf5_path = os.path.join(out_dir, \"demo.hdf5\")\n    f = h5py.File(hdf5_path, \"w\")\n\n    # store some metadata in the attributes of one group\n    grp = f.create_group(\"data\")\n\n    num_eps = 0\n    env_name = None  # will get populated at some point\n\n    for ep_directory in os.listdir(directory):\n\n        state_paths = os.path.join(directory, ep_directory, \"state_*.npz\")\n        states = []\n        joint_velocities = []\n        gripper_actuations = []\n        right_dpos = []\n        right_dquat = []\n        left_dpos = []\n        left_dquat = []\n\n        for state_file in sorted(glob(state_paths)):\n            dic = np.load(state_file)\n            env_name = str(dic[\"env\"])\n\n            states.extend(dic[\"states\"])\n            for ai in dic[\"action_infos\"]:\n                joint_velocities.append(ai[\"joint_velocities\"])\n                gripper_actuations.append(ai[\"gripper_actuation\"])\n                right_dpos.append(ai.get(\"right_dpos\", []))\n                right_dquat.append(ai.get(\"right_dquat\", []))\n                left_dpos.append(ai.get(\"left_dpos\", []))\n                left_dquat.append(ai.get(\"left_dquat\", []))\n                \n        if len(states) == 0:\n            continue\n\n        # Delete the first actions and the last state. This is because when the DataCollector wrapper\n        # recorded the states and actions, the states were recorded AFTER playing that action.\n        del states[-1]\n        del joint_velocities[0]\n        del gripper_actuations[0]\n        del right_dpos[0]\n        del right_dquat[0]\n        del left_dpos[0]\n        del left_dquat[0]\n\n        num_eps += 1\n        ep_data_grp = grp.create_group(\"demo_{}\".format(num_eps))\n\n        # store model file name as an attribute\n        ep_data_grp.attrs[\"model_file\"] = \"model_{}.xml\".format(num_eps)\n\n        # write datasets for states and actions\n        ep_data_grp.create_dataset(\"states\", data=np.array(states))\n        ep_data_grp.create_dataset(\"joint_velocities\", data=np.array(joint_velocities))\n        ep_data_grp.create_dataset(\n            \"gripper_actuations\", data=np.array(gripper_actuations)\n        )\n        ep_data_grp.create_dataset(\"right_dpos\", data=np.array(right_dpos))\n        ep_data_grp.create_dataset(\"right_dquat\", data=np.array(right_dquat))\n        ep_data_grp.create_dataset(\"left_dpos\", data=np.array(left_dpos))\n        ep_data_grp.create_dataset(\"left_dquat\", data=np.array(left_dquat))\n\n        # copy over and rename model xml\n        xml_path = os.path.join(directory, ep_directory, \"model.xml\")\n        shutil.copy(xml_path, model_dir)\n        os.rename(\n            os.path.join(model_dir, \"model.xml\"),\n            os.path.join(model_dir, \"model_{}.xml\".format(num_eps)),\n        )\n\n    # write dataset attributes (metadata)\n    now = datetime.datetime.now()\n    grp.attrs[\"date\"] = \"{}-{}-{}\".format(now.month, now.day, now.year)\n    grp.attrs[\"time\"] = \"{}:{}:{}\".format(now.hour, now.minute, now.second)\n    grp.attrs[\"repository_version\"] = robosuite.__version__\n    grp.attrs[\"env\"] = env_name\n\n    f.close()", "response": "Gather the demstrations saved in the directory into a single hdf5 file and another directory that contains the raw model. xml files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of staged rewards based on current physical states.", "response": "def staged_rewards(self):\n        \"\"\"\n        Returns staged rewards based on current physical states.\n        Stages consist of reaching, grasping, lifting, and hovering.\n        \"\"\"\n\n        reach_mult = 0.1\n        grasp_mult = 0.35\n        lift_mult = 0.5\n        hover_mult = 0.7\n\n        # filter out objects that are already in the correct bins\n        objs_to_reach = []\n        geoms_to_grasp = []\n        target_bin_placements = []\n        for i in range(len(self.ob_inits)):\n            if self.objects_in_bins[i]:\n                continue\n            obj_str = str(self.item_names[i]) + \"0\"\n            objs_to_reach.append(self.obj_body_id[obj_str])\n            geoms_to_grasp.append(self.obj_geom_id[obj_str])\n            target_bin_placements.append(self.target_bin_placements[i])\n        target_bin_placements = np.array(target_bin_placements)\n\n        ### reaching reward governed by distance to closest object ###\n        r_reach = 0.\n        if len(objs_to_reach):\n            # get reaching reward via minimum distance to a target object\n            target_object_pos = self.sim.data.body_xpos[objs_to_reach]\n            gripper_site_pos = self.sim.data.site_xpos[self.eef_site_id]\n            dists = np.linalg.norm(\n                target_object_pos - gripper_site_pos.reshape(1, -1), axis=1\n            )\n            r_reach = (1 - np.tanh(10.0 * min(dists))) * reach_mult\n\n        ### grasping reward for touching any objects of interest ###\n        touch_left_finger = False\n        touch_right_finger = False\n        for i in range(self.sim.data.ncon):\n            c = self.sim.data.contact[i]\n            if c.geom1 in geoms_to_grasp:\n                bin_id = geoms_to_grasp.index(c.geom1)\n                if c.geom2 in self.l_finger_geom_ids:\n                    touch_left_finger = True\n                if c.geom2 in self.r_finger_geom_ids:\n                    touch_right_finger = True\n            elif c.geom2 in geoms_to_grasp:\n                bin_id = geoms_to_grasp.index(c.geom2)\n                if c.geom1 in self.l_finger_geom_ids:\n                    touch_left_finger = True\n                if c.geom1 in self.r_finger_geom_ids:\n                    touch_right_finger = True\n        has_grasp = touch_left_finger and touch_right_finger\n        r_grasp = int(has_grasp) * grasp_mult\n\n        ### lifting reward for picking up an object ###\n        r_lift = 0.\n        if len(objs_to_reach) and r_grasp > 0.:\n            z_target = self.bin_pos[2] + 0.25\n            object_z_locs = self.sim.data.body_xpos[objs_to_reach][:, 2]\n            z_dists = np.maximum(z_target - object_z_locs, 0.)\n            r_lift = grasp_mult + (1 - np.tanh(15.0 * min(z_dists))) * (\n                lift_mult - grasp_mult\n            )\n\n        ### hover reward for getting object above bin ###\n        r_hover = 0.\n        if len(objs_to_reach):\n            # segment objects into left of the bins and above the bins\n            object_xy_locs = self.sim.data.body_xpos[objs_to_reach][:, :2]\n            y_check = (\n                np.abs(object_xy_locs[:, 1] - target_bin_placements[:, 1])\n                < self.bin_size[1] / 4.\n            )\n            x_check = (\n                np.abs(object_xy_locs[:, 0] - target_bin_placements[:, 0])\n                < self.bin_size[0] / 4.\n            )\n            objects_above_bins = np.logical_and(x_check, y_check)\n            objects_not_above_bins = np.logical_not(objects_above_bins)\n            dists = np.linalg.norm(\n                target_bin_placements[:, :2] - object_xy_locs, axis=1\n            )\n            # objects to the left get r_lift added to hover reward, those on the right get max(r_lift) added (to encourage dropping)\n            r_hover_all = np.zeros(len(objs_to_reach))\n            r_hover_all[objects_above_bins] = lift_mult + (\n                1 - np.tanh(10.0 * dists[objects_above_bins])\n            ) * (hover_mult - lift_mult)\n            r_hover_all[objects_not_above_bins] = r_lift + (\n                1 - np.tanh(10.0 * dists[objects_not_above_bins])\n            ) * (hover_mult - lift_mult)\n            r_hover = np.max(r_hover_all)\n\n        return r_reach, r_grasp, r_lift, r_hover"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an OrderedDict containing observations for the current object.", "response": "def _get_observation(self):\n        \"\"\"\n        Returns an OrderedDict containing observations [(name_string, np.array), ...].\n        \n        Important keys:\n            robot-state: contains robot-centric information.\n            object-state: requires @self.use_object_obs to be True.\n                contains object-centric information.\n            image: requires @self.use_camera_obs to be True.\n                contains a rendered frame from the simulation.\n            depth: requires @self.use_camera_obs and @self.camera_depth to be True.\n                contains a rendered depth map from the simulation\n        \"\"\"\n        di = super()._get_observation()\n        if self.use_camera_obs:\n            camera_obs = self.sim.render(\n                camera_name=self.camera_name,\n                width=self.camera_width,\n                height=self.camera_height,\n                depth=self.camera_depth,\n            )\n            if self.camera_depth:\n                di[\"image\"], di[\"depth\"] = camera_obs\n            else:\n                di[\"image\"] = camera_obs\n\n        # low-level object information\n        if self.use_object_obs:\n\n            # remember the keys to collect into object info\n            object_state_keys = []\n\n            # for conversion to relative gripper frame\n            gripper_pose = T.pose2mat((di[\"eef_pos\"], di[\"eef_quat\"]))\n            world_pose_in_gripper = T.pose_inv(gripper_pose)\n\n            for i in range(len(self.item_names_org)):\n\n                if self.single_object_mode == 2 and self.object_id != i:\n                    # Skip adding to observations\n                    continue\n\n                obj_str = str(self.item_names_org[i]) + \"0\"\n                obj_pos = self.sim.data.body_xpos[self.obj_body_id[obj_str]]\n                obj_quat = T.convert_quat(\n                    self.sim.data.body_xquat[self.obj_body_id[obj_str]], to=\"xyzw\"\n                )\n                di[\"{}_pos\".format(obj_str)] = obj_pos\n                di[\"{}_quat\".format(obj_str)] = obj_quat\n\n                # get relative pose of object in gripper frame\n                object_pose = T.pose2mat((obj_pos, obj_quat))\n                rel_pose = T.pose_in_A_to_pose_in_B(object_pose, world_pose_in_gripper)\n                rel_pos, rel_quat = T.mat2pose(rel_pose)\n                di[\"{}_to_eef_pos\".format(obj_str)] = rel_pos\n                di[\"{}_to_eef_quat\".format(obj_str)] = rel_quat\n\n                object_state_keys.append(\"{}_pos\".format(obj_str))\n                object_state_keys.append(\"{}_quat\".format(obj_str))\n                object_state_keys.append(\"{}_to_eef_pos\".format(obj_str))\n                object_state_keys.append(\"{}_to_eef_quat\".format(obj_str))\n\n            if self.single_object_mode == 1:\n                # Zero out other objects observations\n                for obj_str, obj_mjcf in self.mujoco_objects.items():\n                    if obj_str == self.obj_to_use:\n                        continue\n                    else:\n                        di[\"{}_pos\".format(obj_str)] *= 0.0\n                        di[\"{}_quat\".format(obj_str)] *= 0.0\n                        di[\"{}_to_eef_pos\".format(obj_str)] *= 0.0\n                        di[\"{}_to_eef_quat\".format(obj_str)] *= 0.0\n\n            di[\"object-state\"] = np.concatenate([di[k] for k in object_state_keys])\n\n        return di"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the camera view to the specified camera ID.", "response": "def set_camera(self, camera_id):\n        \"\"\"\n        Set the camera view to the specified camera ID.\n        \"\"\"\n        self.viewer.cam.fixedcamid = camera_id\n        self.viewer.cam.type = const.CAMERA_FIXED"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a callback function to be called on a key down.", "response": "def add_keypress_callback(self, key, fn):\n        \"\"\"\n        Allows for custom callback functions for the viewer. Called on key down.\n        Parameter 'any' will ensure that the callback is called on any key down,\n        and block default mujoco viewer callbacks from executing, except for\n        the ESC callback to close the viewer.\n        \"\"\"\n        self.viewer.keypress[key].append(fn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a callback function to be called on a viewer key up.", "response": "def add_keyup_callback(self, key, fn):\n        \"\"\"\n        Allows for custom callback functions for the viewer. Called on key up.\n        Parameter 'any' will ensure that the callback is called on any key up,\n        and block default mujoco viewer callbacks from executing, except for \n        the ESC callback to close the viewer.\n        \"\"\"\n        self.viewer.keyup[key].append(fn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_keyrepeat_callback(self, key, fn):\n        self.viewer.keyrepeat[key].append(fn)", "response": "Adds a callback function to be called on a viewer key repeat."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        state = self.sample()\n        if state is None:\n            # None indicates that a normal env reset should occur\n            return self.env.reset()\n        else:\n            if self.need_xml:\n                # reset the simulation from the model if necessary\n                state, xml = state\n                self.env.reset_from_xml_string(xml)\n\n            if isinstance(state, tuple):\n                state = state[0]\n\n            # force simulator state to one from the demo\n            self.sim.set_state_from_flattened(state)\n            self.sim.forward()\n\n            return self.env._get_observation()", "response": "reset the environment to the latest state"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample(self):\n\n        # chooses a sampling scheme randomly based on the mixing ratios\n        seed = random.uniform(0, 1)\n        ratio = np.cumsum(self.scheme_ratios)\n        ratio = ratio > seed\n        for i, v in enumerate(ratio):\n            if v:\n                break\n\n        sample_method = getattr(self, self.sample_method_dict[self.sampling_schemes[i]])\n        return sample_method()", "response": "This method is the core sampling method. Samples a state from a\n        demonstration in accordance with the configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _uniform_sample(self):\n\n        # get a random episode index\n        ep_ind = random.choice(self.demo_list)\n\n        # select a flattened mujoco state uniformly from this episode\n        states = self.demo_file[\"data/{}/states\".format(ep_ind)].value\n        state = random.choice(states)\n\n        if self.need_xml:\n            model_xml = self._xml_for_episode_index(ep_ind)\n            xml = postprocess_model_xml(model_xml)\n            return state, xml\n        return state", "response": "Uniformly sample a mujoco state from the set of demonstrations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reverse_sample_open_loop(self):\n\n        # get a random episode index\n        ep_ind = random.choice(self.demo_list)\n\n        # sample uniformly in a window that grows backwards from the end of the demos\n        states = self.demo_file[\"data/{}/states\".format(ep_ind)].value\n        eps_len = states.shape[0]\n        index = np.random.randint(max(eps_len - self.open_loop_window_size, 0), eps_len)\n        state = states[index]\n\n        # increase window size at a fixed frequency (open loop)\n        self.demo_sampled += 1\n        if self.demo_sampled >= self.open_loop_increment_freq:\n            if self.open_loop_window_size < eps_len:\n                self.open_loop_window_size += self.open_loop_window_increment\n            self.demo_sampled = 0\n\n        if self.need_xml:\n            model_xml = self._xml_for_episode_index(ep_ind)\n            xml = postprocess_model_xml(model_xml)\n            return state, xml\n\n        return state", "response": "Reverse sampling from states near the end of the demos."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _xml_for_episode_index(self, ep_ind):\n\n        # read the model xml, using the metadata stored in the attribute for this episode\n        model_file = self.demo_file[\"data/{}\".format(ep_ind)].attrs[\"model_file\"]\n        model_path = os.path.join(self.demo_path, \"models\", model_file)\n        with open(model_path, \"r\") as model_f:\n            model_xml = model_f.read()\n        return model_xml", "response": "Helper method to retrieve the corresponding model xml string for the passed episode index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_int16(y1, y2):\n    x = (y1) | (y2 << 8)\n    if x >= 32768:\n        x = -(65536 - x)\n    return x", "response": "Convert two 8 bit bytes to a signed 16 bit integer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scale_to_control(x, axis_scale=350., min_v=-1.0, max_v=1.0):\n    x = x / axis_scale\n    x = min(max(x, min_v), max_v)\n    return x", "response": "Normalize raw HID readings to target range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_controller_state(self):\n        dpos = self.control[:3] * 0.005\n        roll, pitch, yaw = self.control[3:] * 0.005\n        self.grasp = self.control_gripper\n\n        # convert RPY to an absolute orientation\n        drot1 = rotation_matrix(angle=-pitch, direction=[1., 0, 0], point=None)[:3, :3]\n        drot2 = rotation_matrix(angle=roll, direction=[0, 1., 0], point=None)[:3, :3]\n        drot3 = rotation_matrix(angle=yaw, direction=[0, 0, 1.], point=None)[:3, :3]\n\n        self.rotation = self.rotation.dot(drot1.dot(drot2.dot(drot3)))\n\n        return dict(\n            dpos=dpos, rotation=self.rotation, grasp=self.grasp, reset=self._reset_state\n        )", "response": "Returns the current state of the 3d mouse a dictionary of pos orn grasp and reset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_gripper(self, arm_name, gripper):\n        if arm_name in self.grippers:\n            raise ValueError(\"Attempts to add multiple grippers to one body\")\n\n        arm_subtree = self.worldbody.find(\".//body[@name='{}']\".format(arm_name))\n\n        for actuator in gripper.actuator:\n\n            if actuator.get(\"name\") is None:\n                raise XMLError(\"Actuator has no name\")\n\n            if not actuator.get(\"name\").startswith(\"gripper\"):\n                raise XMLError(\n                    \"Actuator name {} does not have prefix 'gripper'\".format(\n                        actuator.get(\"name\")\n                    )\n                )\n\n        for body in gripper.worldbody:\n            arm_subtree.append(body)\n\n        self.merge(gripper, merge_body=False)\n        self.grippers[arm_name] = gripper", "response": "Adds a gripper to an existing arm."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying a constant offset to all objects.", "response": "def set_origin(self, offset):\n        \"\"\"Applies a constant offset to all objects.\"\"\"\n        offset = np.array(offset)\n        for node in self.worldbody.findall(\"./*[@pos]\"):\n            cur_pos = string_to_array(node.get(\"pos\"))\n            new_pos = cur_pos + offset\n            node.set(\"pos\", array_to_string(new_pos))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_pos_indicator(self):\n        body = new_body(name=\"pos_indicator\")\n        body.append(\n            new_geom(\n                \"sphere\",\n                [0.03],\n                rgba=[1, 0, 0, 0.5],\n                group=1,\n                contype=\"0\",\n                conaffinity=\"0\",\n            )\n        )\n        body.append(new_joint(type=\"free\", name=\"pos_indicator\"))\n        self.worldbody.append(body)", "response": "Adds a new position indicator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef table_top_abs(self):\n        table_height = np.array([0, 0, self.table_full_size[2]])\n        return string_to_array(self.floor.get(\"pos\")) + table_height", "response": "Returns the absolute position of the table top"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload robot and optionally add grippers.", "response": "def _load_model(self):\n        \"\"\"Loads robot and optionally add grippers.\"\"\"\n        super()._load_model()\n        self.mujoco_robot = Baxter()\n        if self.has_gripper_right:\n            self.gripper_right = gripper_factory(self.gripper_right_name)\n            if not self.gripper_visualization:\n                self.gripper_right.hide_visualization()\n            self.mujoco_robot.add_gripper(\"right_hand\", self.gripper_right)\n\n        if self.has_gripper_left:\n            self.gripper_left = gripper_factory(self.gripper_left_name)\n            if not self.gripper_visualization:\n                self.gripper_left.hide_visualization()\n            self.mujoco_robot.add_gripper(\"left_hand\", self.gripper_left)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _reset_internal(self):\n        super()._reset_internal()\n        self.sim.data.qpos[self._ref_joint_pos_indexes] = self.mujoco_robot.init_qpos\n\n        if self.has_gripper_right:\n            self.sim.data.qpos[\n                self._ref_joint_gripper_right_actuator_indexes\n            ] = self.gripper_right.init_qpos\n\n        if self.has_gripper_left:\n            self.sim.data.qpos[\n                self._ref_joint_gripper_left_actuator_indexes\n            ] = self.gripper_left.init_qpos", "response": "Resets the pose of the arm and grippers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_reference(self):\n        super()._get_reference()\n\n        # indices for joints in qpos, qvel\n        self.robot_joints = list(self.mujoco_robot.joints)\n        self._ref_joint_pos_indexes = [\n            self.sim.model.get_joint_qpos_addr(x) for x in self.robot_joints\n        ]\n        self._ref_joint_vel_indexes = [\n            self.sim.model.get_joint_qvel_addr(x) for x in self.robot_joints\n        ]\n        if self.use_indicator_object:\n            ind_qpos = self.sim.model.get_joint_qpos_addr(\"pos_indicator\")\n            self._ref_indicator_pos_low, self._ref_indicator_pos_high = ind_qpos\n\n            ind_qvel = self.sim.model.get_joint_qvel_addr(\"pos_indicator\")\n            self._ref_indicator_vel_low, self._ref_indicator_vel_high = ind_qvel\n\n            self.indicator_id = self.sim.model.body_name2id(\"pos_indicator\")\n\n        # indices for grippers in qpos, qvel\n        if self.has_gripper_left:\n            self.gripper_left_joints = list(self.gripper_left.joints)\n            self._ref_gripper_left_joint_pos_indexes = [\n                self.sim.model.get_joint_qpos_addr(x) for x in self.gripper_left_joints\n            ]\n            self._ref_gripper_left_joint_vel_indexes = [\n                self.sim.model.get_joint_qvel_addr(x) for x in self.gripper_left_joints\n            ]\n            self.left_eef_site_id = self.sim.model.site_name2id(\"l_g_grip_site\")\n\n        if self.has_gripper_right:\n            self.gripper_right_joints = list(self.gripper_right.joints)\n            self._ref_gripper_right_joint_pos_indexes = [\n                self.sim.model.get_joint_qpos_addr(x) for x in self.gripper_right_joints\n            ]\n            self._ref_gripper_right_joint_vel_indexes = [\n                self.sim.model.get_joint_qvel_addr(x) for x in self.gripper_right_joints\n            ]\n            self.right_eef_site_id = self.sim.model.site_name2id(\"grip_site\")\n\n        # indices for joint pos actuation, joint vel actuation, gripper actuation\n        self._ref_joint_pos_actuator_indexes = [\n            self.sim.model.actuator_name2id(actuator)\n            for actuator in self.sim.model.actuator_names\n            if actuator.startswith(\"pos\")\n        ]\n\n        self._ref_joint_vel_actuator_indexes = [\n            self.sim.model.actuator_name2id(actuator)\n            for actuator in self.sim.model.actuator_names\n            if actuator.startswith(\"vel\")\n        ]\n\n        if self.has_gripper_left:\n            self._ref_joint_gripper_left_actuator_indexes = [\n                self.sim.model.actuator_name2id(actuator)\n                for actuator in self.sim.model.actuator_names\n                if actuator.startswith(\"gripper_l\")\n            ]\n\n        if self.has_gripper_right:\n            self._ref_joint_gripper_right_actuator_indexes = [\n                self.sim.model.actuator_name2id(actuator)\n                for actuator in self.sim.model.actuator_names\n                if actuator.startswith(\"gripper_r\")\n            ]\n\n        if self.has_gripper_right:\n            # IDs of sites for gripper visualization\n            self.eef_site_id = self.sim.model.site_name2id(\"grip_site\")\n            self.eef_cylinder_id = self.sim.model.site_name2id(\"grip_site_cylinder\")", "response": "Sets up references for robots grippers and objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move_indicator(self, pos):\n        if self.use_indicator_object:\n            self.sim.data.qpos[\n                self._ref_indicator_pos_low : self._ref_indicator_pos_low + 3\n            ] = pos", "response": "Moves the position of the indicator object to pos."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _post_action(self, action):\n        ret = super()._post_action(action)\n        self._gripper_visualization()\n        return ret", "response": "Optionally performs gripper visualization after the actions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_observation(self):\n        di = super()._get_observation()\n        # proprioceptive features\n        di[\"joint_pos\"] = np.array(\n            [self.sim.data.qpos[x] for x in self._ref_joint_pos_indexes]\n        )\n        di[\"joint_vel\"] = np.array(\n            [self.sim.data.qvel[x] for x in self._ref_joint_vel_indexes]\n        )\n        robot_states = [\n            np.sin(di[\"joint_pos\"]),\n            np.cos(di[\"joint_pos\"]),\n            di[\"joint_vel\"],\n        ]\n\n        if self.has_gripper_right:\n            di[\"right_gripper_qpos\"] = np.array(\n                [\n                    self.sim.data.qpos[x]\n                    for x in self._ref_gripper_right_joint_pos_indexes\n                ]\n            )\n            di[\"right_gripper_qvel\"] = np.array(\n                [\n                    self.sim.data.qvel[x]\n                    for x in self._ref_gripper_right_joint_vel_indexes\n                ]\n            )\n            di[\"right_eef_pos\"] = self.sim.data.site_xpos[self.right_eef_site_id]\n            di[\"right_eef_quat\"] = T.convert_quat(\n                self.sim.data.get_body_xquat(\"right_hand\"), to=\"xyzw\"\n            )\n            robot_states.extend(\n                [di[\"right_gripper_qpos\"], di[\"right_eef_pos\"], di[\"right_eef_quat\"]]\n            )\n\n        if self.has_gripper_left:\n            di[\"left_gripper_qpos\"] = np.array(\n                [\n                    self.sim.data.qpos[x]\n                    for x in self._ref_gripper_left_joint_pos_indexes\n                ]\n            )\n            di[\"left_gripper_qvel\"] = np.array(\n                [\n                    self.sim.data.qvel[x]\n                    for x in self._ref_gripper_left_joint_vel_indexes\n                ]\n            )\n            di[\"left_eef_pos\"] = self.sim.data.site_xpos[self.left_eef_site_id]\n            di[\"left_eef_quat\"] = T.convert_quat(\n                self.sim.data.get_body_xquat(\"left_hand\"), to=\"xyzw\"\n            )\n            robot_states.extend(\n                [di[\"left_gripper_qpos\"], di[\"left_eef_pos\"], di[\"left_eef_quat\"]]\n            )\n\n        di[\"robot-state\"] = np.concatenate(robot_states)\n        return di", "response": "Returns an OrderedDict containing observations for the current state of the current state of the current state of the current state of the current state of the current state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dof(self):\n        dof = self.mujoco_robot.dof\n        if self.has_gripper_left:\n            dof += self.gripper_left.dof\n        if self.has_gripper_right:\n            dof += self.gripper_right.dof\n        return dof", "response": "Returns the DoF of the robot ( with grippers )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pose_in_base_from_name(self, name):\n\n        pos_in_world = self.sim.data.get_body_xpos(name)\n        rot_in_world = self.sim.data.get_body_xmat(name).reshape((3, 3))\n        pose_in_world = T.make_pose(pos_in_world, rot_in_world)\n\n        base_pos_in_world = self.sim.data.get_body_xpos(\"base\")\n        base_rot_in_world = self.sim.data.get_body_xmat(\"base\").reshape((3, 3))\n        base_pose_in_world = T.make_pose(base_pos_in_world, base_rot_in_world)\n        world_pose_in_base = T.pose_inv(base_pose_in_world)\n\n        pose_in_base = T.pose_in_A_to_pose_in_B(pose_in_world, world_pose_in_base)\n        return pose_in_base", "response": "A helper function that takes in a named data field and returns the pose of that data field in the base frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the robot joint positions to the passed values.", "response": "def set_robot_joint_positions(self, jpos):\n        \"\"\"\n        Helper method to force robot joint positions to the passed values.\n        \"\"\"\n        self.sim.data.qpos[self._ref_joint_pos_indexes] = jpos\n        self.sim.forward()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the total eef velocity in the base frame as a numpy array of shape ( 6", "response": "def _left_hand_total_velocity(self):\n        \"\"\"\n        Returns the total eef velocity (linear + angular) in the base frame as a numpy\n        array of shape (6,)\n        \"\"\"\n\n        # Use jacobian to translate joint velocities to end effector velocities.\n        Jp = self.sim.data.get_body_jacp(\"left_hand\").reshape((3, -1))\n        Jp_joint = Jp[:, self._ref_joint_vel_indexes[7:]]\n\n        Jr = self.sim.data.get_body_jacr(\"left_hand\").reshape((3, -1))\n        Jr_joint = Jr[:, self._ref_joint_vel_indexes[7:]]\n\n        eef_lin_vel = Jp_joint.dot(self._joint_velocities)\n        eef_rot_vel = Jr_joint.dot(self._joint_velocities)\n        return np.concatenate([eef_lin_vel, eef_rot_vel])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_base_xpos(self, pos):\n        node = self.worldbody.find(\"./body[@name='base']\")\n        node.set(\"pos\", array_to_string(pos - self.bottom_offset))", "response": "Places the robot on position pos."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of geoms corresponding to the 5 sides of the pot used in BaxterLift", "response": "def five_sided_box(size, rgba, group, thickness):\n    \"\"\"\n    Args:\n        size ([float,flat,float]):\n        rgba ([float,float,float,float]): color\n        group (int): Mujoco group\n        thickness (float): wall thickness\n\n    Returns:\n        []: array of geoms corresponding to the\n            5 sides of the pot used in BaxterLift\n    \"\"\"\n    geoms = []\n    x, y, z = size\n    r = thickness / 2\n    geoms.append(\n        new_geom(\n            geom_type=\"box\", size=[x, y, r], pos=[0, 0, -z + r], rgba=rgba, group=group\n        )\n    )\n    geoms.append(\n        new_geom(\n            geom_type=\"box\", size=[x, r, z], pos=[0, -y + r, 0], rgba=rgba, group=group\n        )\n    )\n    geoms.append(\n        new_geom(\n            geom_type=\"box\", size=[x, r, z], pos=[0, y - r, 0], rgba=rgba, group=group\n        )\n    )\n    geoms.append(\n        new_geom(\n            geom_type=\"box\", size=[r, y, z], pos=[x - r, 0, 0], rgba=rgba, group=group\n        )\n    )\n    geoms.append(\n        new_geom(\n            geom_type=\"box\", size=[r, y, z], pos=[-x + r, 0, 0], rgba=rgba, group=group\n        )\n    )\n    return geoms"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_size(size,\n              size_max,\n              size_min,\n              default_max,\n              default_min):\n    \"\"\"\n        Helper method for providing a size,\n        or a range to randomize from\n    \"\"\"\n    if len(default_max) != len(default_min):\n        raise ValueError('default_max = {} and default_min = {}'\n                         .format(str(default_max), str(default_min)) +\n                         ' have different lengths')\n    if size is not None:\n        if (size_max is not None) or (size_min is not None):\n            raise ValueError('size = {} overrides size_max = {}, size_min = {}'\n                             .format(size, size_max, size_min))\n    else:\n        if size_max is None:\n            size_max = default_max\n        if size_min is None:\n            size_min = default_min\n        size = np.array([np.random.uniform(size_min[i], size_max[i])\n                         for i in range(len(default_max))])\n    return size", "response": "Helper method for providing a size or a range to randomize from a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_randomized_range(val,\n                          provided_range,\n                          default_range):\n    \"\"\"\n        Helper to initialize by either value or a range\n        Returns a range to randomize from\n    \"\"\"\n    if val is None:\n        if provided_range is None:\n            return default_range\n        else:\n            return provided_range\n    else:\n        if provided_range is not None:\n            raise ValueError('Value {} overrides range {}'\n                             .format(str(val), str(provided_range)))\n        return [val]", "response": "Helper to initialize by either value or a range\nAttributeNames Returns a list of randomized values from the specified range"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gripper_factory(name):\n    if name == \"TwoFingerGripper\":\n        return TwoFingerGripper()\n    if name == \"LeftTwoFingerGripper\":\n        return LeftTwoFingerGripper()\n    if name == \"PR2Gripper\":\n        return PR2Gripper()\n    if name == \"RobotiqGripper\":\n        return RobotiqGripper()\n    if name == \"PushingGripper\":\n        return PushingGripper()\n    if name == \"RobotiqThreeFingerGripper\":\n        return RobotiqThreeFingerGripper()\n    raise ValueError(\"Unkown gripper name {}\".format(name))", "response": "Returns a Gripper instance with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _display_controls(self):\n\n        def print_command(char, info):\n            char += \" \" * (10 - len(char))\n            print(\"{}\\t{}\".format(char, info))\n\n        print(\"\")\n        print_command(\"Keys\", \"Command\")\n        print_command(\"q\", \"reset simulation\")\n        print_command(\"spacebar\", \"toggle gripper (open/close)\")\n        print_command(\"w-a-s-d\", \"move arm horizontally in x-y plane\")\n        print_command(\"r-f\", \"move arm vertically\")\n        print_command(\"z-x\", \"rotate arm about x-axis\")\n        print_command(\"t-g\", \"rotate arm about y-axis\")\n        print_command(\"c-v\", \"rotate arm about z-axis\")\n        print_command(\"ESC\", \"quit\")\n        print(\"\")", "response": "Method to pretty print the control screen."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset_internal_state(self):\n        self.rotation = np.array([[-1., 0., 0.], [0., 1., 0.], [0., 0., -1.]])\n        self.pos = np.zeros(3)  # (x, y, z)\n        self.last_pos = np.zeros(3)\n        self.grasp = False", "response": "Resets internal state of the controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_controller_state(self):\n        dpos = self.pos - self.last_pos\n        self.last_pos = np.array(self.pos)\n        return dict(\n            dpos=dpos,\n            rotation=self.rotation,\n            grasp=int(self.grasp),\n            reset=self._reset_state,\n        )", "response": "Returns the current state of the keyboard a dictionary of pos orn grasp and reset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle key presses on the current keyboard.", "response": "def on_press(self, window, key, scancode, action, mods):\n        \"\"\"\n        Key handler for key presses.\n        \"\"\"\n\n        # controls for moving position\n        if key == glfw.KEY_W:\n            self.pos[0] -= self._pos_step  # dec x\n        elif key == glfw.KEY_S:\n            self.pos[0] += self._pos_step  # inc x\n        elif key == glfw.KEY_A:\n            self.pos[1] -= self._pos_step  # dec y\n        elif key == glfw.KEY_D:\n            self.pos[1] += self._pos_step  # inc y\n        elif key == glfw.KEY_F:\n            self.pos[2] -= self._pos_step  # dec z\n        elif key == glfw.KEY_R:\n            self.pos[2] += self._pos_step  # inc z\n\n        # controls for moving orientation\n        elif key == glfw.KEY_Z:\n            drot = rotation_matrix(angle=0.1, direction=[1., 0., 0.])[:3, :3]\n            self.rotation = self.rotation.dot(drot)  # rotates x\n        elif key == glfw.KEY_X:\n            drot = rotation_matrix(angle=-0.1, direction=[1., 0., 0.])[:3, :3]\n            self.rotation = self.rotation.dot(drot)  # rotates x\n        elif key == glfw.KEY_T:\n            drot = rotation_matrix(angle=0.1, direction=[0., 1., 0.])[:3, :3]\n            self.rotation = self.rotation.dot(drot)  # rotates y\n        elif key == glfw.KEY_G:\n            drot = rotation_matrix(angle=-0.1, direction=[0., 1., 0.])[:3, :3]\n            self.rotation = self.rotation.dot(drot)  # rotates y\n        elif key == glfw.KEY_C:\n            drot = rotation_matrix(angle=0.1, direction=[0., 0., 1.])[:3, :3]\n            self.rotation = self.rotation.dot(drot)  # rotates z\n        elif key == glfw.KEY_V:\n            drot = rotation_matrix(angle=-0.1, direction=[0., 0., 1.])[:3, :3]\n            self.rotation = self.rotation.dot(drot)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nkey handler for key releases.", "response": "def on_release(self, window, key, scancode, action, mods):\n        \"\"\"\n        Key handler for key releases.\n        \"\"\"\n\n        # controls for grasping\n        if key == glfw.KEY_SPACE:\n            self.grasp = not self.grasp  # toggle gripper\n\n        # user-commanded reset\n        elif key == glfw.KEY_Q:\n            self._reset_state = 1\n            self._enabled = False\n            self._reset_internal_state()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the arena model to the MJCF model.", "response": "def merge_arena(self, mujoco_arena):\n        \"\"\"Adds arena model to the MJCF model.\"\"\"\n        self.arena = mujoco_arena\n        self.table_top_offset = mujoco_arena.table_top_abs\n        self.table_size = mujoco_arena.table_full_size\n        self.merge(mujoco_arena)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef place_objects(self):\n        pos_arr, quat_arr = self.initializer.sample()\n        for i in range(len(self.objects)):\n            self.objects[i].set(\"pos\", array_to_string(pos_arr[i]))\n            self.objects[i].set(\"quat\", array_to_string(quat_arr[i]))", "response": "Places objects randomly until no collisions or max iterations hit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef xml_path_completion(xml_path):\n    if xml_path.startswith(\"/\"):\n        full_path = xml_path\n    else:\n        full_path = os.path.join(robosuite.models.assets_root, xml_path)\n    return full_path", "response": "Takes in a local xml path and returns a full path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_alpha(node, alpha=0.1):\n    for child_node in node.findall(\".//*[@rgba]\"):\n        rgba_orig = string_to_array(child_node.get(\"rgba\"))\n        child_node.set(\"rgba\", array_to_string(list(rgba_orig[0:3]) + [alpha]))", "response": "Sets the alpha attribute of the node to be alpha."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an actuator tag with attributes specified by kwargs.", "response": "def new_actuator(joint, act_type=\"actuator\", **kwargs):\n    \"\"\"\n    Creates an actuator tag with attributes specified by @**kwargs.\n\n    Args:\n        joint: type of actuator transmission.\n            see all types here: http://mujoco.org/book/modeling.html#actuator\n        act_type (str): actuator type. Defaults to \"actuator\"\n\n    \"\"\"\n    element = ET.Element(act_type, attrib=kwargs)\n    element.set(\"joint\", joint)\n    return element"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_site(name, rgba=RED, pos=(0, 0, 0), size=(0.005,), **kwargs):\n    kwargs[\"rgba\"] = array_to_string(rgba)\n    kwargs[\"pos\"] = array_to_string(pos)\n    kwargs[\"size\"] = array_to_string(size)\n    kwargs[\"name\"] = name\n    element = ET.Element(\"site\", attrib=kwargs)\n    return element", "response": "Create a new site element with attributes specified by kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_geom(geom_type, size, pos=(0, 0, 0), rgba=RED, group=0, **kwargs):\n    kwargs[\"type\"] = str(geom_type)\n    kwargs[\"size\"] = array_to_string(size)\n    kwargs[\"rgba\"] = array_to_string(rgba)\n    kwargs[\"group\"] = str(group)\n    kwargs[\"pos\"] = array_to_string(pos)\n    element = ET.Element(\"geom\", attrib=kwargs)\n    return element", "response": "Create a new geom element."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_body(name=None, pos=None, **kwargs):\n    if name is not None:\n        kwargs[\"name\"] = name\n    if pos is not None:\n        kwargs[\"pos\"] = array_to_string(pos)\n    element = ET.Element(\"body\", attrib=kwargs)\n    return element", "response": "Create a new body element with attributes specified by kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new inertial element with attributes specified by kwargs.", "response": "def new_inertial(name=None, pos=(0, 0, 0), mass=None, **kwargs):\n    \"\"\"\n    Creates a inertial element with attributes specified by @**kwargs.\n\n    Args:\n        mass: The mass of inertial\n    \"\"\"\n    if mass is not None:\n        kwargs[\"mass\"] = str(mass)\n    kwargs[\"pos\"] = array_to_string(pos)\n    element = ET.Element(\"inertial\", attrib=kwargs)\n    return element"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef postprocess_model_xml(xml_str):\n\n    path = os.path.split(robosuite.__file__)[0]\n    path_split = path.split(\"/\")\n\n    # replace mesh and texture file paths\n    tree = ET.fromstring(xml_str)\n    root = tree\n    asset = root.find(\"asset\")\n    meshes = asset.findall(\"mesh\")\n    textures = asset.findall(\"texture\")\n    all_elements = meshes + textures\n\n    for elem in all_elements:\n        old_path = elem.get(\"file\")\n        if old_path is None:\n            continue\n        old_path_split = old_path.split(\"/\")\n        ind = max(\n            loc for loc, val in enumerate(old_path_split) if val == \"robosuite\"\n        )  # last occurrence index\n        new_path_split = path_split + old_path_split[ind + 1 :]\n        new_path = \"/\".join(new_path_split)\n        elem.set(\"file\", new_path)\n\n    return ET.tostring(root, encoding=\"utf8\").decode(\"utf8\")", "response": "This function postprocesses the model. xml collected from a MuJoCo demonstration\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_control(self, right=None, left=None):\n        # Sync joint positions for IK.\n        self.sync_ik_robot(self.robot_jpos_getter())\n\n        # Compute new target joint positions if arguments are provided\n        if (right is not None) and (left is not None):\n            self.commanded_joint_positions = self.joint_positions_for_eef_command(\n                right, left\n            )\n\n        # P controller from joint positions (from IK) to velocities\n        velocities = np.zeros(14)\n        deltas = self._get_current_error(\n            self.robot_jpos_getter(), self.commanded_joint_positions\n        )\n\n        for i, delta in enumerate(deltas):\n            velocities[i] = -2 * delta\n        velocities = self.clip_joint_velocities(velocities)\n\n        self.commanded_joint_velocities = velocities\n        return velocities", "response": "Returns the joint velocities to control the robot after the target end effector is recorded."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsynchronizing the internal state of the IK robot to the joint positions of the Pybullet robot being controlled.", "response": "def sync_state(self):\n        \"\"\"\n        Syncs the internal Pybullet robot state to the joint positions of the\n        robot being controlled.\n        \"\"\"\n\n        # sync IK robot state to the current robot joint positions\n        self.sync_ik_robot(self.robot_jpos_getter())\n\n        # make sure target pose is up to date\n        pos_r, orn_r, pos_l, orn_l = self.ik_robot_eef_joint_cartesian_pose()\n\n        self.ik_robot_target_pos_right = pos_r\n        self.ik_robot_target_orn_right = orn_r\n        self.ik_robot_target_pos_left = pos_l\n        self.ik_robot_target_orn_left = orn_l"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsynchronize the internal robot state with the provided joint angles.", "response": "def sync_ik_robot(self, joint_positions, simulate=False, sync_last=True):\n        \"\"\"\n        Force the internal robot model to match the provided joint angles.\n\n        Args:\n            joint_positions (list): a list or flat numpy array of joint positions.\n            simulate (bool): If True, actually use physics simulation, else \n                write to physics state directly.\n            sync_last (bool): If False, don't sync the last joint angle. This\n                is useful for directly controlling the roll at the end effector.\n        \"\"\"\n        num_joints = len(joint_positions)\n        if not sync_last:\n            num_joints -= 1\n        for i in range(num_joints):\n            if simulate:\n                p.setJointMotorControl2(\n                    self.ik_robot,\n                    self.actual[i],\n                    p.POSITION_CONTROL,\n                    targetVelocity=0,\n                    targetPosition=joint_positions[i],\n                    force=500,\n                    positionGain=0.5,\n                    velocityGain=1.,\n                )\n            else:\n                # Note that we use self.actual[i], and not i\n                p.resetJointState(self.ik_robot, self.actual[i], joint_positions[i])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bullet_base_pose_to_world_pose(self, pose_in_base):\n        pose_in_base = T.pose2mat(pose_in_base)\n\n        base_pos_in_world = np.array(p.getBasePositionAndOrientation(self.ik_robot)[0])\n        base_orn_in_world = np.array(p.getBasePositionAndOrientation(self.ik_robot)[1])\n        base_pose_in_world = T.pose2mat((base_pos_in_world, base_orn_in_world))\n\n        pose_in_world = T.pose_in_A_to_pose_in_B(\n            pose_A=pose_in_base, pose_A_in_B=base_pose_in_world\n        )\n        return T.mat2pose(pose_in_world)", "response": "Convert a pose in the base frame to a pose in the world frame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clip_joint_velocities(self, velocities):\n        for i in range(len(velocities)):\n            if velocities[i] >= 1.0:\n                velocities[i] = 1.0\n            elif velocities[i] <= -1.0:\n                velocities[i] = -1.0\n        return velocities", "response": "Clips joint velocities into a valid range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the peg and the hole models.", "response": "def _load_model(self):\n        \"\"\"\n        Loads the peg and the hole models.\n        \"\"\"\n        super()._load_model()\n        self.mujoco_robot.set_base_xpos([0, 0, 0])\n\n        # Add arena and robot\n        self.model = MujocoWorldBase()\n        self.arena = EmptyArena()\n        if self.use_indicator_object:\n            self.arena.add_pos_indicator()\n        self.model.merge(self.arena)\n        self.model.merge(self.mujoco_robot)\n\n        # Load hole object\n        self.hole_obj = self.hole.get_collision(name=\"hole\", site=True)\n        self.hole_obj.set(\"quat\", \"0 0 0.707 0.707\")\n        self.hole_obj.set(\"pos\", \"0.11 0 0.18\")\n        self.model.merge_asset(self.hole)\n        self.model.worldbody.find(\".//body[@name='left_hand']\").append(self.hole_obj)\n\n        # Load cylinder object\n        self.cyl_obj = self.cylinder.get_collision(name=\"cylinder\", site=True)\n        self.cyl_obj.set(\"pos\", \"0 0 0.15\")\n        self.model.merge_asset(self.cylinder)\n        self.model.worldbody.find(\".//body[@name='right_hand']\").append(self.cyl_obj)\n        self.model.worldbody.find(\".//geom[@name='cylinder']\").set(\"rgba\", \"0 1 0 1\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset up references to important components.", "response": "def _get_reference(self):\n        \"\"\"\n        Sets up references to important components. A reference is typically an\n        index or a list of indices that point to the corresponding elements\n        in a flattened array, which is how MuJoCo stores physical simulation data.\n        \"\"\"\n        super()._get_reference()\n        self.hole_body_id = self.sim.model.body_name2id(\"hole\")\n        self.cyl_body_id = self.sim.model.body_name2id(\"cylinder\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_orientation(self):\n        cyl_mat = self.sim.data.body_xmat[self.cyl_body_id]\n        cyl_mat.shape = (3, 3)\n        cyl_pos = self.sim.data.body_xpos[self.cyl_body_id]\n\n        hole_pos = self.sim.data.body_xpos[self.hole_body_id]\n        hole_mat = self.sim.data.body_xmat[self.hole_body_id]\n        hole_mat.shape = (3, 3)\n\n        v = cyl_mat @ np.array([0, 0, 1])\n        v = v / np.linalg.norm(v)\n        center = hole_pos + hole_mat @ np.array([0.1, 0, 0])\n\n        t = (center - cyl_pos) @ v / (np.linalg.norm(v) ** 2)\n        d = np.linalg.norm(np.cross(v, cyl_pos - center)) / np.linalg.norm(v)\n\n        hole_normal = hole_mat @ np.array([0, 0, 1])\n        return (\n            t,\n            d,\n            abs(\n                np.dot(hole_normal, v) / np.linalg.norm(hole_normal) / np.linalg.norm(v)\n            ),\n        )", "response": "Compute the relative positions between the cylinder and the plane."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreward function for the task. The sparse reward is 0 if the peg is outside the hole, and 1 if it's inside. We enforce that it's inside at an appropriate angle (cos(theta) > 0.95). The dense reward has four components. Reaching: in [0, 1], to encourage the arms to get together. Perpendicular and parallel distance: in [0,1], for the same purpose. Cosine of the angle: in [0, 1], to encourage having the right orientation.", "response": "def reward(self, action):\n        \"\"\"\n        Reward function for the task.\n\n        The sparse reward is 0 if the peg is outside the hole, and 1 if it's inside.\n        We enforce that it's inside at an appropriate angle (cos(theta) > 0.95).\n\n        The dense reward has four components.\n\n            Reaching: in [0, 1], to encourage the arms to get together.\n            Perpendicular and parallel distance: in [0,1], for the same purpose.\n            Cosine of the angle: in [0, 1], to encourage having the right orientation.\n        \"\"\"\n        reward = 0\n\n        t, d, cos = self._compute_orientation()\n\n        # Right location and angle\n        if d < 0.06 and t >= -0.12 and t <= 0.14 and cos > 0.95:\n            reward = 1\n\n        # use a shaping reward\n        if self.reward_shaping:\n            # reaching reward\n            hole_pos = self.sim.data.body_xpos[self.hole_body_id]\n            gripper_site_pos = self.sim.data.body_xpos[self.cyl_body_id]\n            dist = np.linalg.norm(gripper_site_pos - hole_pos)\n            reaching_reward = 1 - np.tanh(1.0 * dist)\n            reward += reaching_reward\n\n            # Orientation reward\n            reward += 1 - np.tanh(d)\n            reward += 1 - np.tanh(np.abs(t))\n            reward += cos\n\n        return reward"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _peg_pose_in_hole_frame(self):\n        # World frame\n        peg_pos_in_world = self.sim.data.get_body_xpos(\"cylinder\")\n        peg_rot_in_world = self.sim.data.get_body_xmat(\"cylinder\").reshape((3, 3))\n        peg_pose_in_world = T.make_pose(peg_pos_in_world, peg_rot_in_world)\n\n        # World frame\n        hole_pos_in_world = self.sim.data.get_body_xpos(\"hole\")\n        hole_rot_in_world = self.sim.data.get_body_xmat(\"hole\").reshape((3, 3))\n        hole_pose_in_world = T.make_pose(hole_pos_in_world, hole_rot_in_world)\n\n        world_pose_in_hole = T.pose_inv(hole_pose_in_world)\n\n        peg_pose_in_hole = T.pose_in_A_to_pose_in_B(\n            peg_pose_in_world, world_pose_in_hole\n        )\n        return peg_pose_in_hole", "response": "A helper function that takes in a named data field and returns the pose of that data field in the hole frame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_observation(self):\n        di = super()._get_observation()\n\n        # camera observations\n        if self.use_camera_obs:\n            camera_obs = self.sim.render(\n                camera_name=self.camera_name,\n                width=self.camera_width,\n                height=self.camera_height,\n                depth=self.camera_depth,\n            )\n            if self.camera_depth:\n                di[\"image\"], di[\"depth\"] = camera_obs\n            else:\n                di[\"image\"] = camera_obs\n\n        # low-level object information\n        if self.use_object_obs:\n            # position and rotation of cylinder and hole\n            hole_pos = self.sim.data.body_xpos[self.hole_body_id]\n            hole_quat = T.convert_quat(\n                self.sim.data.body_xquat[self.hole_body_id], to=\"xyzw\"\n            )\n            di[\"hole_pos\"] = hole_pos\n            di[\"hole_quat\"] = hole_quat\n\n            cyl_pos = self.sim.data.body_xpos[self.cyl_body_id]\n            cyl_quat = T.convert_quat(\n                self.sim.data.body_xquat[self.cyl_body_id], to=\"xyzw\"\n            )\n            di[\"cyl_to_hole\"] = cyl_pos - hole_pos\n            di[\"cyl_quat\"] = cyl_quat\n\n            # Relative orientation parameters\n            t, d, cos = self._compute_orientation()\n            di[\"angle\"] = cos\n            di[\"t\"] = t\n            di[\"d\"] = d\n\n            di[\"object-state\"] = np.concatenate(\n                [\n                    di[\"hole_pos\"],\n                    di[\"hole_quat\"],\n                    di[\"cyl_to_hole\"],\n                    di[\"cyl_quat\"],\n                    [di[\"angle\"]],\n                    [di[\"t\"]],\n                    [di[\"d\"]],\n                ]\n            )\n\n        return di", "response": "Returns an OrderedDict containing observations for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the task is successfully completed.", "response": "def _check_success(self):\n        \"\"\"\n        Returns True if task is successfully completed.\n        \"\"\"\n        t, d, cos = self._compute_orientation()\n\n        return d < 0.06 and t >= -0.12 and t <= 0.14 and cos > 0.95"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn multiplication of two quaternions.", "response": "def quat_multiply(quaternion1, quaternion0):\n    \"\"\"Return multiplication of two quaternions.\n    >>> q = quat_multiply([1, -2, 3, 4], [-5, 6, 7, 8])\n    >>> np.allclose(q, [-44, -14, 48, 28])\n    True\n    \"\"\"\n    x0, y0, z0, w0 = quaternion0\n    x1, y1, z1, w1 = quaternion1\n    return np.array(\n        (\n            x1 * w0 + y1 * z0 - z1 * y0 + w1 * x0,\n            -x1 * z0 + y1 * w0 + z1 * x0 + w1 * y0,\n            x1 * y0 - y1 * x0 + z1 * w0 + w1 * z0,\n            -x1 * x0 - y1 * y0 - z1 * z0 + w1 * w0,\n        ),\n        dtype=np.float32,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning conjugate of quaternion.", "response": "def quat_conjugate(quaternion):\n    \"\"\"Return conjugate of quaternion.\n    >>> q0 = random_quaternion()\n    >>> q1 = quat_conjugate(q0)\n    >>> q1[3] == q0[3] and all(q1[:3] == -q0[:3])\n    True\n    \"\"\"\n    return np.array(\n        (-quaternion[0], -quaternion[1], -quaternion[2], quaternion[3]),\n        dtype=np.float32,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn spherical linear interpolation between two quaternions.", "response": "def quat_slerp(quat0, quat1, fraction, spin=0, shortestpath=True):\n    \"\"\"Return spherical linear interpolation between two quaternions.\n    >>> q0 = random_quat()\n    >>> q1 = random_quat()\n    >>> q = quat_slerp(q0, q1, 0.0)\n    >>> np.allclose(q, q0)\n    True\n    >>> q = quat_slerp(q0, q1, 1.0, 1)\n    >>> np.allclose(q, q1)\n    True\n    >>> q = quat_slerp(q0, q1, 0.5)\n    >>> angle = math.acos(np.dot(q0, q))\n    >>> np.allclose(2.0, math.acos(np.dot(q0, q1)) / angle) or \\\n        np.allclose(2.0, math.acos(-np.dot(q0, q1)) / angle)\n    True\n    \"\"\"\n    q0 = unit_vector(quat0[:4])\n    q1 = unit_vector(quat1[:4])\n    if fraction == 0.0:\n        return q0\n    elif fraction == 1.0:\n        return q1\n    d = np.dot(q0, q1)\n    if abs(abs(d) - 1.0) < _EPS:\n        return q0\n    if shortestpath and d < 0.0:\n        # invert rotation\n        d = -d\n        q1 *= -1.0\n    angle = math.acos(d) + spin * math.pi\n    if abs(angle) < _EPS:\n        return q0\n    isin = 1.0 / math.sin(angle)\n    q0 *= math.sin((1.0 - fraction) * angle) * isin\n    q1 *= math.sin(fraction * angle) * isin\n    q0 += q1\n    return q0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns uniform random unit quaternion.", "response": "def random_quat(rand=None):\n    \"\"\"Return uniform random unit quaternion.\n    rand: array like or None\n        Three independent random variables that are uniformly distributed\n        between 0 and 1.\n    >>> q = random_quat()\n    >>> np.allclose(1.0, vector_norm(q))\n    True\n    >>> q = random_quat(np.random.random(3))\n    >>> q.shape\n    (4,)\n    \"\"\"\n    if rand is None:\n        rand = np.random.rand(3)\n    else:\n        assert len(rand) == 3\n    r1 = np.sqrt(1.0 - rand[0])\n    r2 = np.sqrt(rand[0])\n    pi2 = math.pi * 2.0\n    t1 = pi2 * rand[1]\n    t2 = pi2 * rand[2]\n    return np.array(\n        (np.sin(t1) * r1, np.cos(t1) * r1, np.sin(t2) * r2, np.cos(t2) * r2),\n        dtype=np.float32,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mat2quat(rmat, precise=False):\n    M = np.array(rmat, dtype=np.float32, copy=False)[:3, :3]\n    if precise:\n        q = np.empty((4,))\n        t = np.trace(M)\n        if t > M[3, 3]:\n            q[0] = t\n            q[3] = M[1, 0] - M[0, 1]\n            q[2] = M[0, 2] - M[2, 0]\n            q[1] = M[2, 1] - M[1, 2]\n        else:\n            i, j, k = 0, 1, 2\n            if M[1, 1] > M[0, 0]:\n                i, j, k = 1, 2, 0\n            if M[2, 2] > M[i, i]:\n                i, j, k = 2, 0, 1\n            t = M[i, i] - (M[j, j] + M[k, k]) + M[3, 3]\n            q[i] = t\n            q[j] = M[i, j] + M[j, i]\n            q[k] = M[k, i] + M[i, k]\n            q[3] = M[k, j] - M[j, k]\n            q = q[[3, 0, 1, 2]]\n        q *= 0.5 / math.sqrt(t * M[3, 3])\n    else:\n        m00 = M[0, 0]\n        m01 = M[0, 1]\n        m02 = M[0, 2]\n        m10 = M[1, 0]\n        m11 = M[1, 1]\n        m12 = M[1, 2]\n        m20 = M[2, 0]\n        m21 = M[2, 1]\n        m22 = M[2, 2]\n        # symmetric matrix K\n        K = np.array(\n            [\n                [m00 - m11 - m22, 0.0, 0.0, 0.0],\n                [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n                [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n                [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22],\n            ]\n        )\n        K /= 3.0\n        # quaternion is Eigen vector of K that corresponds to largest eigenvalue\n        w, V = np.linalg.eigh(K)\n        q = V[[3, 0, 1, 2], np.argmax(w)]\n    if q[0] < 0.0:\n        np.negative(q, q)\n    return q[[1, 2, 3, 0]]", "response": "Converts given rotation matrix to quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mat2euler(rmat, axes=\"sxyz\"):\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes.lower()]\n    except (AttributeError, KeyError):\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis\n    j = _NEXT_AXIS[i + parity]\n    k = _NEXT_AXIS[i - parity + 1]\n\n    M = np.array(rmat, dtype=np.float32, copy=False)[:3, :3]\n    if repetition:\n        sy = math.sqrt(M[i, j] * M[i, j] + M[i, k] * M[i, k])\n        if sy > EPS:\n            ax = math.atan2(M[i, j], M[i, k])\n            ay = math.atan2(sy, M[i, i])\n            az = math.atan2(M[j, i], -M[k, i])\n        else:\n            ax = math.atan2(-M[j, k], M[j, j])\n            ay = math.atan2(sy, M[i, i])\n            az = 0.0\n    else:\n        cy = math.sqrt(M[i, i] * M[i, i] + M[j, i] * M[j, i])\n        if cy > EPS:\n            ax = math.atan2(M[k, j], M[k, k])\n            ay = math.atan2(-M[k, i], cy)\n            az = math.atan2(M[j, i], M[i, i])\n        else:\n            ax = math.atan2(-M[j, k], M[j, j])\n            ay = math.atan2(-M[k, i], cy)\n            az = 0.0\n\n    if parity:\n        ax, ay, az = -ax, -ay, -az\n    if frame:\n        ax, az = az, ax\n    return vec((ax, ay, az))", "response": "Convert a 3x3 rotation matrix to euler angles in radian."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pose2mat(pose):\n    homo_pose_mat = np.zeros((4, 4), dtype=np.float32)\n    homo_pose_mat[:3, :3] = quat2mat(pose[1])\n    homo_pose_mat[:3, 3] = np.array(pose[0], dtype=np.float32)\n    homo_pose_mat[3, 3] = 1.\n    return homo_pose_mat", "response": "Converts a pose to homogeneous matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts given quaternion x y z w to matrix.", "response": "def quat2mat(quaternion):\n    \"\"\"\n    Converts given quaternion (x, y, z, w) to matrix.\n\n    Args:\n        quaternion: vec4 float angles\n\n    Returns:\n        3x3 rotation matrix\n    \"\"\"\n    q = np.array(quaternion, dtype=np.float32, copy=True)[[3, 0, 1, 2]]\n    n = np.dot(q, q)\n    if n < EPS:\n        return np.identity(3)\n    q *= math.sqrt(2.0 / n)\n    q = np.outer(q, q)\n    return np.array(\n        [\n            [1.0 - q[2, 2] - q[3, 3], q[1, 2] - q[3, 0], q[1, 3] + q[2, 0]],\n            [q[1, 2] + q[3, 0], 1.0 - q[1, 1] - q[3, 3], q[2, 3] - q[1, 0]],\n            [q[1, 3] - q[2, 0], q[2, 3] + q[1, 0], 1.0 - q[1, 1] - q[2, 2]],\n        ]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pose_inv(pose):\n\n    # Note, the inverse of a pose matrix is the following\n    # [R t; 0 1]^-1 = [R.T -R.T*t; 0 1]\n\n    # Intuitively, this makes sense.\n    # The original pose matrix translates by t, then rotates by R.\n    # We just invert the rotation by applying R-1 = R.T, and also translate back.\n    # Since we apply translation first before rotation, we need to translate by\n    # -t in the original frame, which is -R-1*t in the new frame, and then rotate back by\n    # R-1 to align the axis again.\n\n    pose_inv = np.zeros((4, 4))\n    pose_inv[:3, :3] = pose[:3, :3].T\n    pose_inv[:3, 3] = -pose_inv[:3, :3].dot(pose[:3, 3])\n    pose_inv[3, 3] = 1.0\n    return pose_inv", "response": "Computes the inverse of a homogenous matrix corresponding to the pose of some\n    frame A in frame B."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _skew_symmetric_translation(pos_A_in_B):\n    return np.array(\n        [\n            0.,\n            -pos_A_in_B[2],\n            pos_A_in_B[1],\n            pos_A_in_B[2],\n            0.,\n            -pos_A_in_B[0],\n            -pos_A_in_B[1],\n            pos_A_in_B[0],\n            0.,\n        ]\n    ).reshape((3, 3))", "response": "Helper function to get a skew symmetric translation matrix for converting quantities\n            between frames."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting linear and angular velocity of a point in frame A to equivalent in frame B.", "response": "def vel_in_A_to_vel_in_B(vel_A, ang_vel_A, pose_A_in_B):\n    \"\"\"\n    Converts linear and angular velocity of a point in frame A to the equivalent in frame B.\n\n    Args:\n        vel_A: 3-dim iterable for linear velocity in A\n        ang_vel_A: 3-dim iterable for angular velocity in A\n        pose_A_in_B: numpy array of shape (4,4) corresponding to the pose of A in frame B\n\n    Returns:\n        vel_B, ang_vel_B: two numpy arrays of shape (3,) for the velocities in B\n    \"\"\"\n    pos_A_in_B = pose_A_in_B[:3, 3]\n    rot_A_in_B = pose_A_in_B[:3, :3]\n    skew_symm = _skew_symmetric_translation(pos_A_in_B)\n    vel_B = rot_A_in_B.dot(vel_A) + skew_symm.dot(rot_A_in_B.dot(ang_vel_A))\n    ang_vel_B = rot_A_in_B.dot(ang_vel_A)\n    return vel_B, ang_vel_B"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef force_in_A_to_force_in_B(force_A, torque_A, pose_A_in_B):\n    pos_A_in_B = pose_A_in_B[:3, 3]\n    rot_A_in_B = pose_A_in_B[:3, :3]\n    skew_symm = _skew_symmetric_translation(pos_A_in_B)\n    force_B = rot_A_in_B.T.dot(force_A)\n    torque_B = -rot_A_in_B.T.dot(skew_symm.dot(force_A)) + rot_A_in_B.T.dot(torque_A)\n    return force_B, torque_B", "response": "Converts linear and rotational force at a point in frame A to equivalent in frame B."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning matrix to rotate about axis defined by point and direction.", "response": "def rotation_matrix(angle, direction, point=None):\n    \"\"\"\n    Returns matrix to rotate about axis defined by point and direction.\n\n    Examples:\n\n        >>> angle = (random.random() - 0.5) * (2*math.pi)\n        >>> direc = numpy.random.random(3) - 0.5\n        >>> point = numpy.random.random(3) - 0.5\n        >>> R0 = rotation_matrix(angle, direc, point)\n        >>> R1 = rotation_matrix(angle-2*math.pi, direc, point)\n        >>> is_same_transform(R0, R1)\n        True\n        >>> R0 = rotation_matrix(angle, direc, point)\n        >>> R1 = rotation_matrix(-angle, -direc, point)\n        >>> is_same_transform(R0, R1)\n        True\n        >>> I = numpy.identity(4, numpy.float32)\n        >>> numpy.allclose(I, rotation_matrix(math.pi*2, direc))\n        True\n        >>> numpy.allclose(2., numpy.trace(rotation_matrix(math.pi/2,\n        ...                                                direc, point)))\n        True\n\n    \"\"\"\n    sina = math.sin(angle)\n    cosa = math.cos(angle)\n    direction = unit_vector(direction[:3])\n    # rotation matrix around unit vector\n    R = np.array(\n        ((cosa, 0.0, 0.0), (0.0, cosa, 0.0), (0.0, 0.0, cosa)), dtype=np.float32\n    )\n    R += np.outer(direction, direction) * (1.0 - cosa)\n    direction *= sina\n    R += np.array(\n        (\n            (0.0, -direction[2], direction[1]),\n            (direction[2], 0.0, -direction[0]),\n            (-direction[1], direction[0], 0.0),\n        ),\n        dtype=np.float32,\n    )\n    M = np.identity(4)\n    M[:3, :3] = R\n    if point is not None:\n        # rotation not around origin\n        point = np.array(point[:3], dtype=np.float32, copy=False)\n        M[:3, 3] = point - np.dot(R, point)\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_pose(translation, rotation):\n    pose = np.zeros((4, 4))\n    pose[:3, :3] = rotation\n    pose[:3, 3] = translation\n    pose[3, 3] = 1.0\n    return pose", "response": "Makes a homogenous pose matrix from a translation vector and a rotation matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_orientation_error(target_orn, current_orn):\n    current_orn = np.array(\n        [current_orn[3], current_orn[0], current_orn[1], current_orn[2]]\n    )\n    target_orn = np.array([target_orn[3], target_orn[0], target_orn[1], target_orn[2]])\n\n    pinv = np.zeros((3, 4))\n    pinv[0, :] = [-current_orn[1], current_orn[0], -current_orn[3], current_orn[2]]\n    pinv[1, :] = [-current_orn[2], current_orn[3], current_orn[0], -current_orn[1]]\n    pinv[2, :] = [-current_orn[3], -current_orn[2], current_orn[1], current_orn[0]]\n    orn_error = 2.0 * pinv.dot(np.array(target_orn))\n    return orn_error", "response": "Returns the difference between two quaternion orientations as a 3 DOF numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the error corresponding to the target pose - current pose as a 6 - dim vector.", "response": "def get_pose_error(target_pose, current_pose):\n    \"\"\"\n    Computes the error corresponding to target pose - current pose as a 6-dim vector.\n    The first 3 components correspond to translational error while the last 3 components\n    correspond to the rotational error.\n\n    Args:\n        target_pose: a 4x4 homogenous matrix for the target pose\n        current_pose: a 4x4 homogenous matrix for the current pose\n\n    Returns:\n        A 6-dim numpy array for the pose error.\n    \"\"\"\n    error = np.zeros(6)\n\n    # compute translational error\n    target_pos = target_pose[:3, 3]\n    current_pos = current_pose[:3, 3]\n    pos_err = target_pos - current_pos\n\n    # compute rotational error\n    r1 = current_pose[:3, 0]\n    r2 = current_pose[:3, 1]\n    r3 = current_pose[:3, 2]\n    r1d = target_pose[:3, 0]\n    r2d = target_pose[:3, 1]\n    r3d = target_pose[:3, 2]\n    rot_err = 0.5 * (np.cross(r1, r1d) + np.cross(r2, r2d) + np.cross(r3, r3d))\n\n    error[:3] = pos_err\n    error[3:] = rot_err\n    return error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make(env_name, *args, **kwargs):\n    if env_name not in REGISTERED_ENVS:\n        raise Exception(\n            \"Environment {} not found. Make sure it is a registered environment among: {}\".format(\n                env_name, \", \".join(REGISTERED_ENVS)\n            )\n        )\n    return REGISTERED_ENVS[env_name](*args, **kwargs)", "response": "Try to get the equivalent functionality of gym. make in a sloppy way."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the time constants used for simulation.", "response": "def initialize_time(self, control_freq):\n        \"\"\"\n        Initializes the time constants used for simulation.\n        \"\"\"\n        self.cur_time = 0\n        self.model_timestep = self.sim.model.opt.timestep\n        if self.model_timestep <= 0:\n            raise XMLError(\"xml model defined non-positive time step\")\n        self.control_freq = control_freq\n        if control_freq <= 0:\n            raise SimulationError(\n                \"control frequency {} is invalid\".format(control_freq)\n            )\n        self.control_timestep = 1. / control_freq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting internal configuration of the internal simulation.", "response": "def _reset_internal(self):\n        \"\"\"Resets simulation internal configurations.\"\"\"\n        # instantiate simulation from MJCF model\n        self._load_model()\n        self.mjpy_model = self.model.get_model(mode=\"mujoco_py\")\n        self.sim = MjSim(self.mjpy_model)\n        self.initialize_time(self.control_freq)\n\n        # create visualization screen or renderer\n        if self.has_renderer and self.viewer is None:\n            self.viewer = MujocoPyRenderer(self.sim)\n            self.viewer.viewer.vopt.geomgroup[0] = (\n                1 if self.render_collision_mesh else 0\n            )\n            self.viewer.viewer.vopt.geomgroup[1] = 1 if self.render_visual_mesh else 0\n\n            # hiding the overlay speeds up rendering significantly\n            self.viewer.viewer._hide_overlay = True\n\n        elif self.has_offscreen_renderer:\n            if self.sim._render_context_offscreen is None:\n                render_context = MjRenderContextOffscreen(self.sim)\n                self.sim.add_render_context(render_context)\n            self.sim._render_context_offscreen.vopt.geomgroup[0] = (\n                1 if self.render_collision_mesh else 0\n            )\n            self.sim._render_context_offscreen.vopt.geomgroup[1] = (\n                1 if self.render_visual_mesh else 0\n            )\n\n        # additional housekeeping\n        self.sim_state_initial = self.sim.get_state()\n        self._get_reference()\n        self.cur_time = 0\n        self.timestep = 0\n        self.done = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef step(self, action):\n        if self.done:\n            raise ValueError(\"executing action in terminated episode\")\n\n        self.timestep += 1\n        self._pre_action(action)\n        end_time = self.cur_time + self.control_timestep\n        while self.cur_time < end_time:\n            self.sim.step()\n            self.cur_time += self.model_timestep\n        reward, done, info = self._post_action(action)\n        return self._get_observation(), reward, done, info", "response": "Takes a step in simulation with control command @action."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndoing any housekeeping after taking an action.", "response": "def _post_action(self, action):\n        \"\"\"Do any housekeeping after taking an action.\"\"\"\n        reward = self.reward(action)\n\n        # done if number of elapsed timesteps is greater than horizon\n        self.done = (self.timestep >= self.horizon) and not self.ignore_done\n        return reward, self.done, {}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreload the environment from an XML description of the environment.", "response": "def reset_from_xml_string(self, xml_string):\n        \"\"\"Reloads the environment from an XML description of the environment.\"\"\"\n\n        # if there is an active viewer window, destroy it\n        self.close()\n\n        # load model from xml\n        self.mjpy_model = load_model_from_xml(xml_string)\n\n        self.sim = MjSim(self.mjpy_model)\n        self.initialize_time(self.control_freq)\n        if self.has_renderer and self.viewer is None:\n            self.viewer = MujocoPyRenderer(self.sim)\n            self.viewer.viewer.vopt.geomgroup[0] = (\n                1 if self.render_collision_mesh else 0\n            )\n            self.viewer.viewer.vopt.geomgroup[1] = 1 if self.render_visual_mesh else 0\n\n            # hiding the overlay speeds up rendering significantly\n            self.viewer.viewer._hide_overlay = True\n\n        elif self.has_offscreen_renderer:\n            render_context = MjRenderContextOffscreen(self.sim)\n            render_context.vopt.geomgroup[0] = 1 if self.render_collision_mesh else 0\n            render_context.vopt.geomgroup[1] = 1 if self.render_visual_mesh else 0\n            self.sim.add_render_context(render_context)\n\n        self.sim_state_initial = self.sim.get_state()\n        self._get_reference()\n        self.cur_time = 0\n        self.timestep = 0\n        self.done = False\n\n        # necessary to refresh MjData\n        self.sim.forward()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind contacts between two geom groups.", "response": "def find_contacts(self, geoms_1, geoms_2):\n        \"\"\"\n        Finds contact between two geom groups.\n\n        Args:\n            geoms_1: a list of geom names (string)\n            geoms_2: another list of geom names (string)\n\n        Returns:\n            iterator of all contacts between @geoms_1 and @geoms_2\n        \"\"\"\n        for contact in self.sim.data.contact[0 : self.sim.data.ncon]:\n            # check contact geom in geoms\n            c1_in_g1 = self.sim.model.geom_id2name(contact.geom1) in geoms_1\n            c2_in_g2 = self.sim.model.geom_id2name(contact.geom2) in geoms_2\n            # check contact geom in geoms (flipped)\n            c2_in_g1 = self.sim.model.geom_id2name(contact.geom2) in geoms_1\n            c1_in_g2 = self.sim.model.geom_id2name(contact.geom1) in geoms_2\n            if (c1_in_g1 and c2_in_g2) or (c1_in_g2 and c2_in_g1):\n                yield contact"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sync_state(self):\n\n        # sync IK robot state to the current robot joint positions\n        self.sync_ik_robot(self.robot_jpos_getter())\n\n        # make sure target pose is up to date\n        self.ik_robot_target_pos, self.ik_robot_target_orn = (\n            self.ik_robot_eef_joint_cartesian_pose()\n        )", "response": "Synchronizes the internal state of the IK robot to the joint positions of the Pybullet robot being controlled."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_inverse_kinematics(self):\n\n        # Set up a connection to the PyBullet simulator.\n        p.connect(p.DIRECT)\n        p.resetSimulation()\n\n        # get paths to urdfs\n        self.robot_urdf = pjoin(\n            self.bullet_data_path, \"sawyer_description/urdf/sawyer_arm.urdf\"\n        )\n\n        # load the urdfs\n        self.ik_robot = p.loadURDF(self.robot_urdf, (0, 0, 0.9), useFixedBase=1)\n\n        # Simulation will update as fast as it can in real time, instead of waiting for\n        # step commands like in the non-realtime case.\n        p.setRealTimeSimulation(1)", "response": "This function is responsible for setting up the inverse kinematics."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ik_robot_eef_joint_cartesian_pose(self):\n        eef_pos_in_world = np.array(p.getLinkState(self.ik_robot, 6)[0])\n        eef_orn_in_world = np.array(p.getLinkState(self.ik_robot, 6)[1])\n        eef_pose_in_world = T.pose2mat((eef_pos_in_world, eef_orn_in_world))\n\n        base_pos_in_world = np.array(p.getBasePositionAndOrientation(self.ik_robot)[0])\n        base_orn_in_world = np.array(p.getBasePositionAndOrientation(self.ik_robot)[1])\n        base_pose_in_world = T.pose2mat((base_pos_in_world, base_orn_in_world))\n        world_pose_in_base = T.pose_inv(base_pose_in_world)\n\n        eef_pose_in_base = T.pose_in_A_to_pose_in_B(\n            pose_A=eef_pose_in_world, pose_A_in_B=world_pose_in_base\n        )\n\n        return T.mat2pose(eef_pose_in_base)", "response": "Returns the current cartesian pose of the last joint of the ik robot with respect to the base frame as a tuple where pos orn is a x - y - z - w tuple where orn is a x - y - z - w tuple where orn is a x - y - z - w tuple where pos orn is a x - y - z - w tuple where orn is a x - y - z - w tuple where orn is a x - y - z - w tuple where pos or n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef joint_positions_for_eef_command(self, dpos, rotation):\n\n        self.ik_robot_target_pos += dpos * self.user_sensitivity\n\n        # this rotation accounts for rotating the end effector by 90 degrees\n        # from its rest configuration. The corresponding line in most demo\n        # scripts is:\n        #   `env.set_robot_joint_positions([0, -1.18, 0.00, 2.18, 0.00, 0.57, 1.5708])`\n        rotation = rotation.dot(\n            T.rotation_matrix(angle=-np.pi / 2, direction=[0., 0., 1.], point=None)[\n                :3, :3\n            ]\n        )\n\n        self.ik_robot_target_orn = T.mat2quat(rotation)\n\n        # convert from target pose in base frame to target pose in bullet world frame\n        world_targets = self.bullet_base_pose_to_world_pose(\n            (self.ik_robot_target_pos, self.ik_robot_target_orn)\n        )\n\n        rest_poses = [0, -1.18, 0.00, 2.18, 0.00, 0.57, 3.3161]\n\n        for bullet_i in range(100):\n            arm_joint_pos = self.inverse_kinematics(\n                world_targets[0], world_targets[1], rest_poses=rest_poses\n            )\n            self.sync_ik_robot(arm_joint_pos, sync_last=True)\n\n        return arm_joint_pos", "response": "This function runs inverse kinematics to back out target joint positions for the specified end effector command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads the robot and optionally adds grippers.", "response": "def _load_model(self):\n        \"\"\"\n        Loads robot and optionally add grippers.\n        \"\"\"\n        super()._load_model()\n        self.mujoco_robot = Sawyer()\n        if self.has_gripper:\n            self.gripper = gripper_factory(self.gripper_type)\n            if not self.gripper_visualization:\n                self.gripper.hide_visualization()\n            self.mujoco_robot.add_gripper(\"right_hand\", self.gripper)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the initial pose of arm and grippers.", "response": "def _reset_internal(self):\n        \"\"\"\n        Sets initial pose of arm and grippers.\n        \"\"\"\n        super()._reset_internal()\n        self.sim.data.qpos[self._ref_joint_pos_indexes] = self.mujoco_robot.init_qpos\n\n        if self.has_gripper:\n            self.sim.data.qpos[\n                self._ref_joint_gripper_actuator_indexes\n            ] = self.gripper.init_qpos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_reference(self):\n        super()._get_reference()\n\n        # indices for joints in qpos, qvel\n        self.robot_joints = list(self.mujoco_robot.joints)\n        self._ref_joint_pos_indexes = [\n            self.sim.model.get_joint_qpos_addr(x) for x in self.robot_joints\n        ]\n        self._ref_joint_vel_indexes = [\n            self.sim.model.get_joint_qvel_addr(x) for x in self.robot_joints\n        ]\n\n        if self.use_indicator_object:\n            ind_qpos = self.sim.model.get_joint_qpos_addr(\"pos_indicator\")\n            self._ref_indicator_pos_low, self._ref_indicator_pos_high = ind_qpos\n\n            ind_qvel = self.sim.model.get_joint_qvel_addr(\"pos_indicator\")\n            self._ref_indicator_vel_low, self._ref_indicator_vel_high = ind_qvel\n\n            self.indicator_id = self.sim.model.body_name2id(\"pos_indicator\")\n\n        # indices for grippers in qpos, qvel\n        if self.has_gripper:\n            self.gripper_joints = list(self.gripper.joints)\n            self._ref_gripper_joint_pos_indexes = [\n                self.sim.model.get_joint_qpos_addr(x) for x in self.gripper_joints\n            ]\n            self._ref_gripper_joint_vel_indexes = [\n                self.sim.model.get_joint_qvel_addr(x) for x in self.gripper_joints\n            ]\n\n        # indices for joint pos actuation, joint vel actuation, gripper actuation\n        self._ref_joint_pos_actuator_indexes = [\n            self.sim.model.actuator_name2id(actuator)\n            for actuator in self.sim.model.actuator_names\n            if actuator.startswith(\"pos\")\n        ]\n\n        self._ref_joint_vel_actuator_indexes = [\n            self.sim.model.actuator_name2id(actuator)\n            for actuator in self.sim.model.actuator_names\n            if actuator.startswith(\"vel\")\n        ]\n\n        if self.has_gripper:\n            self._ref_joint_gripper_actuator_indexes = [\n                self.sim.model.actuator_name2id(actuator)\n                for actuator in self.sim.model.actuator_names\n                if actuator.startswith(\"gripper\")\n            ]\n\n        # IDs of sites for gripper visualization\n        self.eef_site_id = self.sim.model.site_name2id(\"grip_site\")\n        self.eef_cylinder_id = self.sim.model.site_name2id(\"grip_site_cylinder\")", "response": "Sets up necessary reference for robots grippers and objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides the superclass method to actuate the robot with the passed joint velocities and gripper control. Args: action (numpy array): The control to apply to the robot. The first @self.mujoco_robot.dof dimensions should be the desired normalized joint velocities and if the robot has a gripper, the next @self.gripper.dof dimensions should be actuation controls for the gripper.", "response": "def _pre_action(self, action):\n        \"\"\"\n        Overrides the superclass method to actuate the robot with the \n        passed joint velocities and gripper control.\n\n        Args:\n            action (numpy array): The control to apply to the robot. The first\n                @self.mujoco_robot.dof dimensions should be the desired \n                normalized joint velocities and if the robot has \n                a gripper, the next @self.gripper.dof dimensions should be\n                actuation controls for the gripper.\n        \"\"\"\n\n        # clip actions into valid range\n        assert len(action) == self.dof, \"environment got invalid action dimension\"\n        low, high = self.action_spec\n        action = np.clip(action, low, high)\n\n        if self.has_gripper:\n            arm_action = action[: self.mujoco_robot.dof]\n            gripper_action_in = action[\n                self.mujoco_robot.dof : self.mujoco_robot.dof + self.gripper.dof\n            ]\n            gripper_action_actual = self.gripper.format_action(gripper_action_in)\n            action = np.concatenate([arm_action, gripper_action_actual])\n\n        # rescale normalized action to control ranges\n        ctrl_range = self.sim.model.actuator_ctrlrange\n        bias = 0.5 * (ctrl_range[:, 1] + ctrl_range[:, 0])\n        weight = 0.5 * (ctrl_range[:, 1] - ctrl_range[:, 0])\n        applied_action = bias + weight * action\n        self.sim.data.ctrl[:] = applied_action\n\n        # gravity compensation\n        self.sim.data.qfrc_applied[\n            self._ref_joint_vel_indexes\n        ] = self.sim.data.qfrc_bias[self._ref_joint_vel_indexes]\n\n        if self.use_indicator_object:\n            self.sim.data.qfrc_applied[\n                self._ref_indicator_vel_low : self._ref_indicator_vel_high\n            ] = self.sim.data.qfrc_bias[\n                self._ref_indicator_vel_low : self._ref_indicator_vel_high\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an OrderedDict containing observations [ name_string np. array )...", "response": "def _get_observation(self):\n        \"\"\"\n        Returns an OrderedDict containing observations [(name_string, np.array), ...].\n\n        Important keys:\n            robot-state: contains robot-centric information.\n        \"\"\"\n\n        di = super()._get_observation()\n        # proprioceptive features\n        di[\"joint_pos\"] = np.array(\n            [self.sim.data.qpos[x] for x in self._ref_joint_pos_indexes]\n        )\n        di[\"joint_vel\"] = np.array(\n            [self.sim.data.qvel[x] for x in self._ref_joint_vel_indexes]\n        )\n\n        robot_states = [\n            np.sin(di[\"joint_pos\"]),\n            np.cos(di[\"joint_pos\"]),\n            di[\"joint_vel\"],\n        ]\n\n        if self.has_gripper:\n            di[\"gripper_qpos\"] = np.array(\n                [self.sim.data.qpos[x] for x in self._ref_gripper_joint_pos_indexes]\n            )\n            di[\"gripper_qvel\"] = np.array(\n                [self.sim.data.qvel[x] for x in self._ref_gripper_joint_vel_indexes]\n            )\n\n            di[\"eef_pos\"] = self.sim.data.site_xpos[self.eef_site_id]\n            di[\"eef_quat\"] = T.convert_quat(\n                self.sim.data.get_body_xquat(\"right_hand\"), to=\"xyzw\"\n            )\n\n            # add in gripper information\n            robot_states.extend([di[\"gripper_qpos\"], di[\"eef_pos\"], di[\"eef_quat\"]])\n\n        di[\"robot-state\"] = np.concatenate(robot_states)\n        return di"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dof(self):\n        dof = self.mujoco_robot.dof\n        if self.has_gripper:\n            dof += self.gripper.dof\n        return dof", "response": "Returns the DoF of the robot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect_random_trajectory(env, timesteps=1000):\n\n    obs = env.reset()\n    dof = env.dof\n\n    for t in range(timesteps):\n        action = 0.5 * np.random.randn(dof)\n        obs, reward, done, info = env.step(action)\n        env.render()\n        if t % 100 == 0:\n            print(t)", "response": "Run a random policy to collect trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef playback_trajectory(env, ep_dir):\n\n    # first reload the model from the xml\n    xml_path = os.path.join(ep_dir, \"model.xml\")\n    with open(xml_path, \"r\") as f:\n        env.reset_from_xml_string(f.read())\n\n    state_paths = os.path.join(ep_dir, \"state_*.npz\")\n\n    # read states back, load them one by one, and render\n    t = 0\n    for state_file in sorted(glob(state_paths)):\n        print(state_file)\n        dic = np.load(state_file)\n        states = dic[\"states\"]\n        for state in states:\n            env.sim.set_state_from_flattened(state)\n            env.sim.forward()\n            env.render()\n            t += 1\n            if t % 100 == 0:\n                print(t)", "response": "Play data from an episode."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the joint positions of the IK controller.", "response": "def set_robot_joint_positions(self, positions):\n        \"\"\"\n        Overrides the function to set the joint positions directly, since we need to notify\n        the IK controller of the change.\n        \"\"\"\n        self.env.set_robot_joint_positions(positions)\n        self.controller.sync_state()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef step(self, action):\n\n        input_1 = self._make_input(action[:7], self.env._right_hand_quat)\n        if self.env.mujoco_robot.name == \"sawyer\":\n            velocities = self.controller.get_control(**input_1)\n            low_action = np.concatenate([velocities, action[7:]])\n        elif self.env.mujoco_robot.name == \"baxter\":\n            input_2 = self._make_input(action[7:14], self.env._left_hand_quat)\n            velocities = self.controller.get_control(input_1, input_2)\n            low_action = np.concatenate([velocities, action[14:]])\n        else:\n            raise Exception(\n                \"Only Sawyer and Baxter robot environments are supported for IK \"\n                \"control currently.\"\n            )\n\n        # keep trying to reach the target in a closed-loop\n        for i in range(self.action_repeat):\n            ret = self.env.step(low_action)\n            velocities = self.controller.get_control()\n            if self.env.mujoco_robot.name == \"sawyer\":\n                low_action = np.concatenate([velocities, action[7:]])\n            else:\n                low_action = np.concatenate([velocities, action[14:]])\n\n        return ret", "response": "Move the end effector according to the input control."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_message(self, request, facility, audience='any'):\n        prefix = self.get_prefix()\n        channels = []\n        if audience in ('session', 'any',):\n            if request and request.session:\n                channels.append('{prefix}session:{0}:{facility}'.format(request.session.session_key, prefix=prefix, facility=facility))\n        if audience in ('user', 'any',):\n            if is_authenticated(request):\n                channels.append('{prefix}user:{0}:{facility}'.format(request.user.get_username(), prefix=prefix, facility=facility))\n        if audience in ('group', 'any',):\n            try:\n                if is_authenticated(request):\n                    groups = request.session['ws4redis:memberof']\n                    channels.extend('{prefix}group:{0}:{facility}'.format(g, prefix=prefix, facility=facility)\n                                for g in groups)\n            except (KeyError, AttributeError):\n                pass\n        if audience in ('broadcast', 'any',):\n            channels.append('{prefix}broadcast:{facility}'.format(prefix=prefix, facility=facility))\n        for channel in channels:\n            message = self._connection.get(channel)\n            if message:\n                return message", "response": "Fetch the first available message for the given facility and audience."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the publishers and subscribers for the messages in the request.", "response": "def set_pubsub_channels(self, request, channels):\n        \"\"\"\n        Initialize the channels used for publishing and subscribing messages through the message queue.\n        \"\"\"\n        facility = request.path_info.replace(settings.WEBSOCKET_URL, '', 1)\n\n        # initialize publishers\n        audience = {\n            'users': 'publish-user' in channels and [SELF] or [],\n            'groups': 'publish-group' in channels and [SELF] or [],\n            'sessions': 'publish-session' in channels and [SELF] or [],\n            'broadcast': 'publish-broadcast' in channels,\n        }\n        self._publishers = set()\n        for key in self._get_message_channels(request=request, facility=facility, **audience):\n            self._publishers.add(key)\n\n        # initialize subscribers\n        audience = {\n            'users': 'subscribe-user' in channels and [SELF] or [],\n            'groups': 'subscribe-group' in channels and [SELF] or [],\n            'sessions': 'subscribe-session' in channels and [SELF] or [],\n            'broadcast': 'subscribe-broadcast' in channels,\n        }\n        self._subscription = self._connection.pubsub()\n        for key in self._get_message_channels(request=request, facility=facility, **audience):\n            self._subscription.subscribe(key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_persisted_messages(self, websocket):\n        for channel in self._subscription.channels:\n            message = self._connection.get(channel)\n            if message:\n                websocket.send(message)", "response": "Send persisted messages to the client."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_file_descriptor(self):\n        return self._subscription.connection and self._subscription.connection._sock.fileno()", "response": "Returns the file descriptor used for passing to the select call when listening\n        on the message queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef release(self):\n        if self._subscription and self._subscription.subscribed:\n            self._subscription.unsubscribe()\n            self._subscription.reset()", "response": "Release the current session."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of context variables that can be used to create a new context.", "response": "def default(request):\n    \"\"\"\n    Adds additional context variables to the default context.\n    \"\"\"\n    protocol = request.is_secure() and 'wss://' or 'ws://'\n    heartbeat_msg = settings.WS4REDIS_HEARTBEAT and '\"{0}\"'.format(settings.WS4REDIS_HEARTBEAT) or 'null'\n    context = {\n        'WEBSOCKET_URI': protocol + request.get_host() + settings.WEBSOCKET_URL,\n        'WS4REDIS_HEARTBEAT': mark_safe(heartbeat_msg),\n    }\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _decode_bytes(self, bytestring):\n        if not bytestring:\n            return u''\n        try:\n            return bytestring.decode('utf-8')\n        except UnicodeDecodeError:\n            self.close(1007)\n            raise", "response": "Internal method used to decode the utf - 8 encoded bytestring into unicode."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes the given text into a utf - 8 byte string equivalent of text.", "response": "def _encode_bytes(self, text):\n        \"\"\"\n        :returns: The utf-8 byte string equivalent of `text`.\n        \"\"\"\n        if isinstance(text, six.binary_type):\n            return text\n        if not isinstance(text, six.text_type):\n            text = six.text_type(text or '')\n        return text.encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the code is a valid hybi return code.", "response": "def _is_valid_close_code(self, code):\n        \"\"\"\n        :returns: Whether the returned close code is a valid hybi return code.\n        \"\"\"\n        if code < 1000:\n            return False\n        if 1004 <= code <= 1006:\n            return False\n        if 1012 <= code <= 1016:\n            return False\n        if code == 1100:\n            # not sure about this one but the autobahn fuzzer requires it.\n            return False\n        if 2000 <= code <= 2999:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_close(self, header, payload):\n        if not payload:\n            self.close(1000, None)\n            return\n        if len(payload) < 2:\n            raise WebSocketError('Invalid close frame: {0} {1}'.format(header, payload))\n        rv = payload[:2]\n        if six.PY2:\n            code = struct.unpack('!H', str(rv))[0]\n        else:\n            code = struct.unpack('!H', bytes(rv))[0]\n        payload = payload[2:]\n        if payload:\n            validator = Utf8Validator()\n            val = validator.validate(payload)\n            if not val[0]:\n                raise UnicodeError\n        if not self._is_valid_close_code(code):\n            raise WebSocketError('Invalid close code {0}'.format(code))\n        self.close(code, payload)", "response": "Called when a close frame has been decoded from the stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a full frame from the socket.", "response": "def read_frame(self):\n        \"\"\"\n        Block until a full frame has been read from the socket.\n\n        This is an internal method as calling this will not cleanup correctly\n        if an exception is called. Use `receive` instead.\n\n        :return: The header and payload as a tuple.\n        \"\"\"\n        header = Header.decode_header(self.stream)\n        if header.flags:\n            raise WebSocketError\n        if not header.length:\n            return header, ''\n        try:\n            payload = self.stream.read(header.length)\n        except socket_error:\n            payload = ''\n        except Exception:\n            logger.debug(\"{}: {}\".format(type(e), six.text_type(e)))\n            payload = ''\n        if len(payload) != header.length:\n            raise WebSocketError('Unexpected EOF reading frame payload')\n        if header.mask:\n            payload = header.unmask_payload(payload)\n        return header, payload"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_message(self):\n        opcode = None\n        message = None\n        while True:\n            header, payload = self.read_frame()\n            f_opcode = header.opcode\n            if f_opcode in (self.OPCODE_TEXT, self.OPCODE_BINARY):\n                # a new frame\n                if opcode:\n                    raise WebSocketError(\"The opcode in non-fin frame is expected to be zero, got {0!r}\".format(f_opcode))\n                # Start reading a new message, reset the validator\n                self.utf8validator.reset()\n                self.utf8validate_last = (True, True, 0, 0)\n                opcode = f_opcode\n            elif f_opcode == self.OPCODE_CONTINUATION:\n                if not opcode:\n                    raise WebSocketError(\"Unexpected frame with opcode=0\")\n            elif f_opcode == self.OPCODE_PING:\n                self.handle_ping(header, payload)\n                continue\n            elif f_opcode == self.OPCODE_PONG:\n                self.handle_pong(header, payload)\n                continue\n            elif f_opcode == self.OPCODE_CLOSE:\n                self.handle_close(header, payload)\n                return\n            else:\n                raise WebSocketError(\"Unexpected opcode={0!r}\".format(f_opcode))\n            if opcode == self.OPCODE_TEXT:\n                self.validate_utf8(payload)\n                if six.PY3:\n                    payload = payload.decode()\n            if message is None:\n                message = six.text_type() if opcode == self.OPCODE_TEXT else six.binary_type()\n            message += payload\n            if header.fin:\n                break\n        if opcode == self.OPCODE_TEXT:\n            if six.PY2:\n                self.validate_utf8(message)\n            else:\n                self.validate_utf8(message.encode())\n            return message\n        else:\n            return bytearray(message)", "response": "Reads a message from the socket and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef receive(self):\n        if self._closed:\n            raise WebSocketError(\"Connection is already closed\")\n        try:\n            return self.read_message()\n        except UnicodeError as e:\n            logger.info('websocket.receive: UnicodeError {}'.format(e))\n            self.close(1007)\n        except WebSocketError as e:\n            logger.info('websocket.receive: WebSocketError {}'.format(e))\n            self.close(1002)\n        except Exception as e:\n            logger.info('websocket.receive: Unknown error {}'.format(e))\n            raise e", "response": "Read and return a message from the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a frame over the websocket with the given message as its payload.", "response": "def send_frame(self, message, opcode):\n        \"\"\"\n        Send a frame over the websocket with message as its payload\n        \"\"\"\n        if self._closed:\n            raise WebSocketError(\"Connection is already closed\")\n        if opcode == self.OPCODE_TEXT:\n            message = self._encode_bytes(message)\n        elif opcode == self.OPCODE_BINARY:\n            message = six.binary_type(message)\n        header = Header.encode_header(True, opcode, '', len(message), 0)\n        try:\n            self.stream.write(header + message)\n        except socket_error:\n            raise WebSocketError(\"Socket is dead\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message over the websocket.", "response": "def send(self, message, binary=False):\n        \"\"\"\n        Send a frame over the websocket with message as its payload\n        \"\"\"\n        if binary is None:\n            binary = not isinstance(message, six.string_types)\n        opcode = self.OPCODE_BINARY if binary else self.OPCODE_TEXT\n        try:\n            self.send_frame(message, opcode)\n        except WebSocketError:\n            raise WebSocketError(\"Socket is dead\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the underlying socket and connection.", "response": "def close(self, code=1000, message=''):\n        \"\"\"\n        Close the websocket and connection, sending the specified code and\n        message.  The underlying socket object is _not_ closed, that is the\n        responsibility of the initiator.\n        \"\"\"\n        try:\n            message = self._encode_bytes(message)\n            self.send_frame(\n                struct.pack('!H%ds' % len(message), code, message),\n                opcode=self.OPCODE_CLOSE)\n        except WebSocketError:\n            # Failed to write the closing frame but it's ok because we're\n            # closing the socket anyway.\n            logger.debug(\"Failed to write closing frame -> closing socket\")\n        finally:\n            logger.debug(\"Closed WebSocket\")\n            self._closed = True\n            self.stream = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes a WebSocket header.", "response": "def decode_header(cls, stream):\n        \"\"\"\n        Decode a WebSocket header.\n\n        :param stream: A file like object that can be 'read' from.\n        :returns: A `Header` instance.\n        \"\"\"\n        read = stream.read\n        data = read(2)\n        if len(data) != 2:\n            raise WebSocketError(\"Unexpected EOF while decoding header\")\n        first_byte, second_byte = struct.unpack('!BB', data)\n        header = cls(\n            fin=first_byte & cls.FIN_MASK == cls.FIN_MASK,\n            opcode=first_byte & cls.OPCODE_MASK,\n            flags=first_byte & cls.HEADER_FLAG_MASK,\n            length=second_byte & cls.LENGTH_MASK)\n        has_mask = second_byte & cls.MASK_MASK == cls.MASK_MASK\n        if header.opcode > 0x07:\n            if not header.fin:\n                raise WebSocketError('Received fragmented control frame: {0!r}'.format(data))\n            # Control frames MUST have a payload length of 125 bytes or less\n            if header.length > 125:\n                raise FrameTooLargeException('Control frame cannot be larger than 125 bytes: {0!r}'.format(data))\n        if header.length == 126:\n            # 16 bit length\n            data = read(2)\n            if len(data) != 2:\n                raise WebSocketError('Unexpected EOF while decoding header')\n            header.length = struct.unpack('!H', data)[0]\n        elif header.length == 127:\n            # 64 bit length\n            data = read(8)\n            if len(data) != 8:\n                raise WebSocketError('Unexpected EOF while decoding header')\n            header.length = struct.unpack('!Q', data)[0]\n        if has_mask:\n            mask = read(4)\n            if len(mask) != 4:\n                raise WebSocketError('Unexpected EOF while decoding header')\n            header.mask = mask\n        return header"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_header(cls, fin, opcode, mask, length, flags):\n        first_byte = opcode\n        second_byte = 0\n        if six.PY2:\n            extra = ''\n        else:\n            extra = b''\n        if fin:\n            first_byte |= cls.FIN_MASK\n        if flags & cls.RSV0_MASK:\n            first_byte |= cls.RSV0_MASK\n        if flags & cls.RSV1_MASK:\n            first_byte |= cls.RSV1_MASK\n        if flags & cls.RSV2_MASK:\n            first_byte |= cls.RSV2_MASK\n        # now deal with length complexities\n        if length < 126:\n            second_byte += length\n        elif length <= 0xffff:\n            second_byte += 126\n            extra = struct.pack('!H', length)\n        elif length <= 0xffffffffffffffff:\n            second_byte += 127\n            extra = struct.pack('!Q', length)\n        else:\n            raise FrameTooLargeException\n        if mask:\n            second_byte |= cls.MASK_MASK\n            extra += mask\n        if six.PY3:\n            return bytes([first_byte, second_byte]) + extra\n        return chr(first_byte) + chr(second_byte) + extra", "response": "Encodes a WebSocket header."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmonkeys patch the manage. py runserver function to monkey patch the internal Django command manage. py runserver", "response": "def run(addr, port, wsgi_handler, ipv6=False, threading=False, **kwargs):\n    \"\"\"\n    Function to monkey patch the internal Django command: manage.py runserver\n    \"\"\"\n    logger.info('Websocket support is enabled')\n    server_address = (addr, port)\n    if not threading:\n        raise Exception(\"Django's Websocket server must run with threading enabled\")\n    httpd_cls = type('WSGIServer', (socketserver.ThreadingMixIn, WSGIServer), {'daemon_threads': True})\n    httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6)\n    httpd.set_app(wsgi_handler)\n    httpd.serve_forever()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_one_request(self):\n        \"\"\"Copy of WSGIRequestHandler, but with different ServerHandler\"\"\"\n\n        self.raw_requestline = self.rfile.readline(65537)\n        if len(self.raw_requestline) > 65536:\n            self.requestline = ''\n            self.request_version = ''\n            self.command = ''\n            self.send_error(414)\n            return\n\n        if not self.parse_request():  # An error code has been sent, just exit\n            return\n\n        handler = ServerHandler(\n            self.rfile, self.wfile, self.get_stderr(), self.get_environ()\n        )\n        handler.request_handler = self      # backpointer for logging\n        handler.run(self.server.get_app())", "response": "Copy of WSGIRequestHandler. handle but with different ServerHandler"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upgrade_websocket(self, environ, start_response):\n        websocket_version = environ.get('HTTP_SEC_WEBSOCKET_VERSION', '')\n        if not websocket_version:\n            raise UpgradeRequiredError\n        elif websocket_version not in self.WS_VERSIONS:\n            raise HandshakeError('Unsupported WebSocket Version: {0}'.format(websocket_version))\n\n        key = environ.get('HTTP_SEC_WEBSOCKET_KEY', '').strip()\n        if not key:\n            raise HandshakeError('Sec-WebSocket-Key header is missing/empty')\n        try:\n            key_len = len(base64.b64decode(key))\n        except TypeError:\n            raise HandshakeError('Invalid key: {0}'.format(key))\n        if key_len != 16:\n            # 5.2.1 (3)\n            raise HandshakeError('Invalid key: {0}'.format(key))\n\n        sec_ws_accept = base64.b64encode(sha1(six.b(key) + self.WS_GUID).digest())\n        if six.PY3:\n            sec_ws_accept = sec_ws_accept.decode('ascii')\n        headers = [\n            ('Upgrade', 'websocket'),\n            ('Connection', 'Upgrade'),\n            ('Sec-WebSocket-Accept', sec_ws_accept),\n            ('Sec-WebSocket-Version', str(websocket_version))\n        ]\n        if environ.get('HTTP_SEC_WEBSOCKET_PROTOCOL') is not None:\n            headers.append(('Sec-WebSocket-Protocol', environ.get('HTTP_SEC_WEBSOCKET_PROTOCOL')))\n\n        logger.debug('WebSocket request accepted, switching protocols')\n        start_response(force_str('101 Switching Protocols'), headers)\n        six.get_method_self(start_response).finish_content()\n        return WebSocket(environ['wsgi.input'])", "response": "Upgrade the socket into a websocket enabled connection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a list of users into a list of users and the currently logged in user.", "response": "def _wrap_users(users, request):\n    \"\"\"\n    Returns a list with the given list of users and/or the currently logged in user, if the list\n    contains the magic item SELF.\n    \"\"\"\n    result = set()\n    for u in users:\n        if u is SELF and is_authenticated(request):\n            result.add(request.user.get_username())\n        else:\n            result.add(u)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrap_groups(groups, request):\n    result = set()\n    for g in groups:\n        if g is SELF and is_authenticated(request):\n            result.update(request.session.get('ws4redis:memberof', []))\n        else:\n            result.add(g)\n    return result", "response": "Wrap a list of groups into a set of groups."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _wrap_sessions(sessions, request):\n    result = set()\n    for s in sessions:\n        if s is SELF and request:\n            result.add(request.session.session_key)\n        else:\n            result.add(s)\n    return result", "response": "Wrap a list of sessions and the session key of the current logged in user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef publish_message(self, message, expire=None):\n        if expire is None:\n            expire = self._expire\n        if not isinstance(message, RedisMessage):\n            raise ValueError('message object is not of type RedisMessage')\n        for channel in self._publishers:\n            self._connection.publish(channel, message)\n            if expire > 0:\n                self._connection.setex(channel, expire, message)", "response": "Publish a message on all the subscribed channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file_descriptor(self):\n        try:\n            return uwsgi.connection_fd()\n        except IOError as e:\n            self.close()\n            raise WebSocketError(e)", "response": "Return the file descriptor for the given websocket"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets left and right frames from a single image.", "response": "def get_frames_singleimage(self):\n        \"\"\"\n        Get current left and right frames from a single image,\n        by splitting the image in half.\n        \"\"\"\n        frame = self.captures[0].read()[1]\n        height, width, colors = frame.shape\n        left_frame = frame[:, :width/2, :]\n        right_frame = frame[:, width/2:, :]\n        return [left_frame, right_frame]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_frames(self, wait=0):\n        for window, frame in zip(self.windows, self.get_frames()):\n            cv2.imshow(window, frame)\n        cv2.waitKey(wait)", "response": "Show current frames from cameras."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a picture with a chessboard visible in both captures.", "response": "def get_chessboard(self, columns, rows, show=False):\n        \"\"\"\n        Take a picture with a chessboard visible in both captures.\n\n        ``columns`` and ``rows`` should be the number of inside corners in the\n        chessboard's columns and rows. ``show`` determines whether the frames\n        are shown while the cameras search for a chessboard.\n        \"\"\"\n        found_chessboard = [False, False]\n        while not all(found_chessboard):\n            frames = self.get_frames()\n            if show:\n                self.show_frames(1)\n            for i, frame in enumerate(frames):\n                (found_chessboard[i],\n                 corners) = cv2.findChessboardCorners(frame, (columns, rows),\n                                                  flags=cv2.CALIB_CB_FAST_CHECK)\n        return frames"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrectifying and return current frames from cameras.", "response": "def get_frames(self):\n        \"\"\"Rectify and return current frames from cameras.\"\"\"\n        frames = super(CalibratedPair, self).get_frames()\n        return self.calibration.rectify(frames)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget 3D point cloud from image pair.", "response": "def get_point_cloud(self, pair):\n        \"\"\"Get 3D point cloud from image pair.\"\"\"\n        disparity = self.block_matcher.get_disparity(pair)\n        points = self.block_matcher.get_3d(disparity,\n                                           self.calibration.disp_to_depth_mat)\n        colors = cv2.cvtColor(pair[0], cv2.COLOR_BGR2RGB)\n        return PointCloud(points, colors)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_settings(self, settings):\n        with open(settings) as settings_file:\n            settings_dict = simplejson.load(settings_file)\n        for key, value in settings_dict.items():\n            self.__setattr__(key, value)", "response": "Load settings from file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_settings(self, settings_file):\n        settings = {}\n        for parameter in self.parameter_maxima:\n            settings[parameter] = self.__getattribute__(parameter)\n        with open(settings_file, \"w\") as settings_file:\n            simplejson.dump(settings, settings_file)", "response": "Save block matcher settings to a file object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset private _search_range and reset _block_matcher.", "response": "def search_range(self, value):\n        \"\"\"Set private ``_search_range`` and reset ``_block_matcher``.\"\"\"\n        if value == 0 or not value % 16:\n            self._search_range = value\n        else:\n            raise InvalidSearchRangeError(\"Search range must be a multiple of \"\n                                          \"16.\")\n        self._replace_bm()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef window_size(self, value):\n        if (value > 4 and\n            value < self.parameter_maxima[\"window_size\"] and\n            value % 2):\n            self._window_size = value\n        else:\n            raise InvalidWindowSizeError(\"Window size must be an odd number \"\n                                      \"between 0 and {}.\".format(\n                                      self.parameter_maxima[\"window_size\"] + 1))\n        self._replace_bm()", "response": "Sets the internal _window_size and reset _block_matcher."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the private _stereo_bm_preset and reset _block_matcher.", "response": "def stereo_bm_preset(self, value):\n        \"\"\"Set private ``_stereo_bm_preset`` and reset ``_block_matcher``.\"\"\"\n        if value in (cv2.STEREO_BM_BASIC_PRESET,\n                     cv2.STEREO_BM_FISH_EYE_PRESET,\n                     cv2.STEREO_BM_NARROW_PRESET):\n            self._bm_preset = value\n        else:\n            raise InvalidBMPresetError(\"Stereo BM preset must be defined as \"\n                                       \"cv2.STEREO_BM_*_PRESET.\")\n        self._replace_bm()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace _block_matcher with current values.", "response": "def _replace_bm(self):\n        \"\"\"Replace ``_block_matcher`` with current values.\"\"\"\n        self._block_matcher = cv2.StereoBM(preset=self._bm_preset,\n                                          ndisparities=self._search_range,\n                                          SADWindowSize=self._window_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing disparity from image pair.", "response": "def get_disparity(self, pair):\n        \"\"\"\n        Compute disparity from image pair (left, right).\n\n        First, convert images to grayscale if needed. Then pass to the\n        ``_block_matcher`` for stereo matching.\n        \"\"\"\n        gray = []\n        if pair[0].ndim == 3:\n            for side in pair:\n                gray.append(cv2.cvtColor(side, cv2.COLOR_BGR2GRAY))\n        else:\n            gray = pair\n        return self._block_matcher.compute(gray[0], gray[1],\n                                          disptype=cv2.CV_32F)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef numDisparities(self, value):\n        if value > 0 and value % 16 == 0:\n            self._num_disp = value\n        else:\n            raise InvalidNumDisparitiesError(\"numDisparities must be a \"\n                                             \"positive integer evenly \"\n                                             \"divisible by 16.\")\n        self._replace_bm()", "response": "Set internal _num_disp and reset _block_matcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the SAD window size.", "response": "def SADWindowSize(self, value):\n        \"\"\"Set private ``_sad_window_size`` and reset ``_block_matcher``.\"\"\"\n        if value >= 1 and value <= 11 and value % 2:\n            self._sad_window_size = value\n        else:\n            raise InvalidSADWindowSizeError(\"SADWindowSize must be odd and \"\n                                            \"between 1 and 11.\")\n        self._replace_bm()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the uniqueness ratio of the resource.", "response": "def uniquenessRatio(self, value):\n        \"\"\"Set private ``_uniqueness`` and reset ``_block_matcher``.\"\"\"\n        if value >= 5 and value <= 15:\n            self._uniqueness = value\n        else:\n            raise InvalidUniquenessRatioError(\"Uniqueness ratio must be \"\n                                              \"between 5 and 15.\")\n        self._replace_bm()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef speckleWindowSize(self, value):\n        if value >= 0 and value <= 200:\n            self._speckle_window_size = value\n        else:\n            raise InvalidSpeckleWindowSizeError(\"Speckle window size must be 0 \"\n                                                \"for disabled checks or \"\n                                                \"between 50 and 200.\")\n        self._replace_bm()", "response": "Sets the speckle window size."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef speckleRange(self, value):\n        if value >= 0:\n            self._speckle_range = value\n        else:\n            raise InvalidSpeckleRangeError(\"Speckle range cannot be negative.\")\n        self._replace_bm()", "response": "Set internal _speckle_range and reset _block_matcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset private _P1 and reset _block_matcher.", "response": "def P1(self, value):\n        \"\"\"Set private ``_P1`` and reset ``_block_matcher``.\"\"\"\n        if value < self.P2:\n            self._P1 = value\n        else:\n            raise InvalidFirstDisparityChangePenaltyError(\"P1 must be less \"\n                                                          \"than P2.\")\n        self._replace_bm()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef P2(self, value):\n        if value > self.P1:\n            self._P2 = value\n        else:\n            raise InvalidSecondDisparityChangePenaltyError(\"P2 must be greater \"\n                                                          \"than P1.\")\n        self._replace_bm()", "response": "Sets private _P2 and reset _block_matcher."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _replace_bm(self):\n        self._block_matcher = cv2.StereoSGBM(minDisparity=self._min_disparity,\n                        numDisparities=self._num_disp,\n                        SADWindowSize=self._sad_window_size,\n                        uniquenessRatio=self._uniqueness,\n                        speckleWindowSize=self._speckle_window_size,\n                        speckleRange=self._speckle_range,\n                        disp12MaxDiff=self._max_disparity,\n                        P1=self._P1,\n                        P2=self._P2,\n                        fullDP=self._full_dp)", "response": "Replace _block_matcher with current values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_disparity(self, pair):\n        return self._block_matcher.compute(pair[0],\n                                          pair[1]).astype(np.float32) / 16.0", "response": "Compute disparity from image pair ( left right )."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _copy_calibration(self, calibration):\n        for key, item in calibration.__dict__.items():\n            self.__dict__[key] = item", "response": "Copy another StereoCalibration object s values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninteract with the current object and save the result to a folder.", "response": "def _interact_with_folder(self, output_folder, action):\n        \"\"\"\n        Export/import matrices as *.npy files to/from an output folder.\n\n        ``action`` is a string. It determines whether the method reads or writes\n        to disk. It must have one of the following values: ('r', 'w').\n        \"\"\"\n        if not action in ('r', 'w'):\n            raise ValueError(\"action must be either 'r' or 'w'.\")\n        for key, item in self.__dict__.items():\n            if isinstance(item, dict):\n                for side in (\"left\", \"right\"):\n                    filename = os.path.join(output_folder,\n                                            \"{}_{}.npy\".format(key, side))\n                    if action == 'w':\n                        np.save(filename, self.__dict__[key][side])\n                    else:\n                        self.__dict__[key][side] = np.load(filename)\n            else:\n                filename = os.path.join(output_folder, \"{}.npy\".format(key))\n                if action == 'w':\n                    np.save(filename, self.__dict__[key])\n                else:\n                    self.__dict__[key] = np.load(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting matrices as. npy files to an output folder.", "response": "def export(self, output_folder):\n        \"\"\"Export matrices as ``*.npy`` files to an output folder.\"\"\"\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n        self._interact_with_folder(output_folder, 'w')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rectify(self, frames):\n        new_frames = []\n        for i, side in enumerate((\"left\", \"right\")):\n            new_frames.append(cv2.remap(frames[i],\n                                        self.undistortion_map[side],\n                                        self.rectification_map[side],\n                                        cv2.INTER_NEAREST))\n        return new_frames", "response": "Rectify the given frames according to the undistortion and rectification map."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind subpixel chessboard corners in image.", "response": "def _get_corners(self, image):\n        \"\"\"Find subpixel chessboard corners in image.\"\"\"\n        temp = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        ret, corners = cv2.findChessboardCorners(temp,\n                                                 (self.rows, self.columns))\n        if not ret:\n            raise ChessboardNotFoundError(\"No chessboard could be found.\")\n        cv2.cornerSubPix(temp, corners, (11, 11), (-1, -1),\n                         (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,\n                          30, 0.01))\n        return corners"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow chessboard corners found in image.", "response": "def _show_corners(self, image, corners):\n        \"\"\"Show chessboard corners found in image.\"\"\"\n        temp = image\n        cv2.drawChessboardCorners(temp, (self.rows, self.columns), corners,\n                                  True)\n        window_name = \"Chessboard\"\n        cv2.imshow(window_name, temp)\n        if cv2.waitKey(0):\n            cv2.destroyWindow(window_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds chessboard corners found in an image pair.", "response": "def add_corners(self, image_pair, show_results=False):\n        \"\"\"\n        Record chessboard corners found in an image pair.\n\n        The image pair should be an iterable composed of two CvMats ordered\n        (left, right).\n        \"\"\"\n        side = \"left\"\n        self.object_points.append(self.corner_coordinates)\n        for image in image_pair:\n            corners = self._get_corners(image)\n            if show_results:\n                self._show_corners(image, corners)\n            self.image_points[side].append(corners.reshape(-1, 2))\n            side = \"right\"\n            self.image_count += 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calibrate_cameras(self):\n        criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,\n                    100, 1e-5)\n        flags = (cv2.CALIB_FIX_ASPECT_RATIO + cv2.CALIB_ZERO_TANGENT_DIST +\n                 cv2.CALIB_SAME_FOCAL_LENGTH)\n        calib = StereoCalibration()\n        (calib.cam_mats[\"left\"], calib.dist_coefs[\"left\"],\n         calib.cam_mats[\"right\"], calib.dist_coefs[\"right\"],\n         calib.rot_mat, calib.trans_vec, calib.e_mat,\n         calib.f_mat) = cv2.stereoCalibrate(self.object_points,\n                                            self.image_points[\"left\"],\n                                            self.image_points[\"right\"],\n                                            self.image_size,\n                                            calib.cam_mats[\"left\"],\n                                            calib.dist_coefs[\"left\"],\n                                            calib.cam_mats[\"right\"],\n                                            calib.dist_coefs[\"right\"],\n                                            calib.rot_mat,\n                                            calib.trans_vec,\n                                            calib.e_mat,\n                                            calib.f_mat,\n                                            criteria=criteria,\n                                            flags=flags)[1:]\n        (calib.rect_trans[\"left\"], calib.rect_trans[\"right\"],\n         calib.proj_mats[\"left\"], calib.proj_mats[\"right\"],\n         calib.disp_to_depth_mat, calib.valid_boxes[\"left\"],\n         calib.valid_boxes[\"right\"]) = cv2.stereoRectify(calib.cam_mats[\"left\"],\n                                                      calib.dist_coefs[\"left\"],\n                                                      calib.cam_mats[\"right\"],\n                                                      calib.dist_coefs[\"right\"],\n                                                      self.image_size,\n                                                      calib.rot_mat,\n                                                      calib.trans_vec,\n                                                      flags=0)\n        for side in (\"left\", \"right\"):\n            (calib.undistortion_map[side],\n             calib.rectification_map[side]) = cv2.initUndistortRectifyMap(\n                                                        calib.cam_mats[side],\n                                                        calib.dist_coefs[side],\n                                                        calib.rect_trans[side],\n                                                        calib.proj_mats[side],\n                                                        self.image_size,\n                                                        cv2.CV_32FC1)\n        # This is replaced because my results were always bad. Estimates are\n        # taken from the OpenCV samples.\n        width, height = self.image_size\n        focal_length = 0.8 * width\n        calib.disp_to_depth_mat = np.float32([[1, 0, 0, -0.5 * width],\n                                              [0, -1, 0, 0.5 * height],\n                                              [0, 0, 0, -focal_length],\n                                              [0, 0, 1, 0]])\n        return calib", "response": "Calibrates cameras based on found chessboard corners."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck the quality of the image and the calibration.", "response": "def check_calibration(self, calibration):\n        \"\"\"\n        Check calibration quality by computing average reprojection error.\n\n        First, undistort detected points and compute epilines for each side.\n        Then compute the error between the computed epipolar lines and the\n        position of the points detected on the other side for each point and\n        return the average error.\n        \"\"\"\n        sides = \"left\", \"right\"\n        which_image = {sides[0]: 1, sides[1]: 2}\n        undistorted, lines = {}, {}\n        for side in sides:\n            undistorted[side] = cv2.undistortPoints(\n                         np.concatenate(self.image_points[side]).reshape(-1,\n                                                                         1, 2),\n                         calibration.cam_mats[side],\n                         calibration.dist_coefs[side],\n                         P=calibration.cam_mats[side])\n            lines[side] = cv2.computeCorrespondEpilines(undistorted[side],\n                                              which_image[side],\n                                              calibration.f_mat)\n        total_error = 0\n        this_side, other_side = sides\n        for side in sides:\n            for i in range(len(undistorted[side])):\n                total_error += abs(undistorted[this_side][i][0][0] *\n                                   lines[other_side][i][0][0] +\n                                   undistorted[this_side][i][0][1] *\n                                   lines[other_side][i][0][1] +\n                                   lines[other_side][i][0][2])\n            other_side, this_side = sides\n        total_points = self.image_count * len(self.object_points)\n        return total_error / total_points"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndiscovering stereo photos and return them as a pairwise sorted list.", "response": "def find_files(folder):\n    \"\"\"Discover stereo photos and return them as a pairwise sorted list.\"\"\"\n    files = [i for i in os.listdir(folder) if i.startswith(\"left\")]\n    files.sort()\n    for i in range(len(files)):\n        insert_string = \"right{}\".format(files[i * 2][4:])\n        files.insert(i * 2 + 1, insert_string)\n    files = [os.path.join(folder, filename) for filename in files]\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calibrate_folder(args):\n    height, width = cv2.imread(args.input_files[0]).shape[:2]\n    calibrator = StereoCalibrator(args.rows, args.columns, args.square_size,\n                                  (width, height))\n    progress = ProgressBar(maxval=len(args.input_files),\n                          widgets=[Bar(\"=\", \"[\", \"]\"),\n                          \" \", Percentage()])\n    print(\"Reading input files...\")\n    progress.start()\n    while args.input_files:\n        left, right = args.input_files[:2]\n        img_left, im_right = cv2.imread(left), cv2.imread(right)\n        calibrator.add_corners((img_left, im_right),\n                               show_results=args.show_chessboards)\n        args.input_files = args.input_files[2:]\n        progress.update(progress.maxval - len(args.input_files))\n\n    progress.finish()\n    print(\"Calibrating cameras. This can take a while.\")\n    calibration = calibrator.calibrate_cameras()\n    avg_error = calibrator.check_calibration(calibration)\n    print(\"The average error between chessboard points and their epipolar \"\n          \"lines is \\n\"\n          \"{} pixels. This should be as small as possible.\".format(avg_error))\n    calibration.export(args.output_folder)", "response": "Calibrates camera based on chessboard images and writes results to output folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry setting new parameter on block_matcher and update disparity map.", "response": "def _set_value(self, parameter, new_value):\n        \"\"\"Try setting new parameter on ``block_matcher`` and update map.\"\"\"\n        try:\n            self.block_matcher.__setattr__(parameter, new_value)\n        except BadBlockMatcherArgumentError:\n            return\n        self.update_disparity_map()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing trackbars by discovering the maximum value of each parameter.", "response": "def _initialize_trackbars(self):\n        \"\"\"\n        Initialize trackbars by discovering ``block_matcher``'s parameters.\n        \"\"\"\n        for parameter in self.block_matcher.parameter_maxima.keys():\n            maximum = self.block_matcher.parameter_maxima[parameter]\n            if not maximum:\n                maximum = self.shortest_dimension\n            cv2.createTrackbar(parameter, self.window_name,\n                               self.block_matcher.__getattribute__(parameter),\n                               maximum,\n                               partial(self._set_value, parameter))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _save_bm_state(self):\n        for parameter in self.block_matcher.parameter_maxima.keys():\n            self.bm_settings[parameter].append(\n                               self.block_matcher.__getattribute__(parameter))", "response": "Save current state of block matcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the disparity map in GUI.", "response": "def update_disparity_map(self):\n        \"\"\"\n        Update disparity map in GUI.\n\n        The disparity image is normalized to the range 0-255 and then divided by\n        255, because OpenCV multiplies it by 255 when displaying. This is\n        because the pixels are stored as floating points.\n        \"\"\"\n        disparity = self.block_matcher.get_disparity(self.pair)\n        norm_coeff = 255 / disparity.max()\n        cv2.imshow(self.window_name, disparity * norm_coeff / 255)\n        cv2.waitKey()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tune_pair(self, pair):\n        self._save_bm_state()\n        self.pair = pair\n        self.update_disparity_map()", "response": "Tune a pair of images."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreport chosen settings for a particular parameter in the block matcher.", "response": "def report_settings(self, parameter):\n        \"\"\"\n        Report chosen settings for ``parameter`` in ``block_matcher``.\n\n        ``bm_settings`` is updated to include the latest state before work is\n        begun. This state is removed at the end so that the method has no side\n        effects. All settings are reported except for the first one on record,\n        which is ``block_matcher``'s default setting.\n        \"\"\"\n        self._save_bm_state()\n        report = []\n        settings_list = self.bm_settings[parameter][1:]\n        unique_values = list(set(settings_list))\n        value_frequency = {}\n        for value in unique_values:\n            value_frequency[settings_list.count(value)] = value\n        frequencies = value_frequency.keys()\n        frequencies.sort(reverse=True)\n        header = \"{} value | Selection frequency\".format(parameter)\n        left_column_width = len(header[:-21])\n        right_column_width = 21\n        report.append(header)\n        report.append(\"{}|{}\".format(\"-\" * left_column_width,\n                                    \"-\" * right_column_width))\n        for frequency in frequencies:\n            left_column = str(value_frequency[frequency]).center(\n                                                             left_column_width)\n            right_column = str(frequency).center(right_column_width)\n            report.append(\"{}|{}\".format(left_column, right_column))\n        # Remove newest settings\n        for param in self.block_matcher.parameter_maxima.keys():\n            self.bm_settings[param].pop(-1)\n        return \"\\n\".join(report)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports PointCloud to PLY file for viewing in MeshLab.", "response": "def write_ply(self, output_file):\n        \"\"\"Export ``PointCloud`` to PLY file for viewing in MeshLab.\"\"\"\n        points = np.hstack([self.coordinates, self.colors])\n        with open(output_file, 'w') as outfile:\n            outfile.write(self.ply_header.format(\n                                            vertex_count=len(self.coordinates)))\n            np.savetxt(outfile, points, '%f %f %f %d %d %d')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter infinite distances from PointCloud.", "response": "def filter_infinity(self):\n        \"\"\"Filter infinite distances from ``PointCloud.``\"\"\"\n        mask = self.coordinates[:, 2] > self.coordinates[:, 2].min()\n        coords = self.coordinates[mask]\n        colors = self.colors[mask]\n        return PointCloud(coords, colors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a filename to a file:// url", "response": "def to_file_url(filename):\n    \"\"\" Convert a filename to a file:// url\n    \"\"\"\n    url = 'file://' + os.path.abspath(filename).replace(os.path.sep, '/')\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_file_url(url):\n    if url.startswith('file://'):\n        url = url[len('file://'):].replace('/', os.path.sep)\n\n    return url", "response": "Convert from file:// url to file path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the last line of a file in a seekable file.", "response": "def read_last_line(fh, offset=256):\n    \"\"\" Read last line from a seekable file. Start reading\n    from buff before end of file, and double backwards seek\n    until line break is found. If reached beginning of file\n    (no lines), just return whole file\n    \"\"\"\n    fh.seek(0, 2)\n    size = fh.tell()\n\n    while offset < size:\n        fh.seek(-offset, 2)\n        lines = fh.readlines()\n        if len(lines) > 1:\n            return lines[-1]\n        offset *= 2\n\n    fh.seek(0, 0)\n    return fh.readlines()[-1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_loader_for_url(self, url):\n        parts = url.split('://', 1)\n        if len(parts) < 2:\n            type_ = 'file'\n        else:\n            type_ = parts[0]\n\n        if '+' in type_:\n            profile_name, scheme = type_.split('+', 1)\n            if len(parts) == 2:\n                url = scheme + '://' + parts[1]\n        else:\n            profile_name = ''\n            scheme = type_\n\n        loader = self.cached.get(type_)\n        if loader:\n            return loader, url\n\n        loader_cls = self._get_loader_class_for_type(scheme)\n\n        if not loader_cls:\n            raise IOError('No Loader for type: ' + scheme)\n\n        profile = self.kwargs\n\n        if self.profile_loader:\n            profile = self.profile_loader(profile_name, scheme)\n\n        loader = loader_cls(**profile)\n\n        self.cached[type_] = loader\n        return loader, url", "response": "Determine the loader for a given url."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a file - like reader from the local file system", "response": "def load(self, url, offset=0, length=-1):\n        \"\"\"\n        Load a file-like reader from the local file system\n        \"\"\"\n\n        # if starting with . or /, can only be a file path..\n        file_only = url.startswith(('/', '.'))\n\n        # convert to filename\n        filename = from_file_url(url)\n        if filename != url:\n            file_only = True\n            url = filename\n\n        try:\n            # first, try as file\n            afile = open(url, 'rb')\n\n        except IOError:\n            if file_only:\n                raise\n\n            return super(LocalFileLoader, self).load(url, offset, length)\n\n        if offset > 0:\n            afile.seek(offset)\n\n        if length >= 0:\n            return LimitReader(afile, length)\n        else:\n            return afile"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a file - like reader over http using range requests and an optional cookie created via a cookie_maker", "response": "def load(self, url, offset, length):\n        \"\"\"\n        Load a file-like reader over http using range requests\n        and an optional cookie created via a cookie_maker\n        \"\"\"\n        headers = {}\n        if offset != 0 or length != -1:\n            headers['Range'] = BlockLoader._make_range_header(offset, length)\n\n        if self.cookie_maker:\n            if isinstance(self.cookie_maker, six.string_types):\n                headers['Cookie'] = self.cookie_maker\n            else:\n                headers['Cookie'] = self.cookie_maker.make()\n\n        if not self.session:\n            self.session = requests.Session()\n\n        r = self.session.get(url, headers=headers, stream=True)\n        r.raise_for_status()\n        return r.raw"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raise_on_self_redirect(self, params, cdx, status_code, location_url):\n        if cdx.get('is_live'):\n            return\n\n        if not status_code.startswith('3') or status_code == '304':\n            return\n\n        request_url = params['url'].lower()\n        if not location_url:\n            return\n\n\n        location_url = location_url.lower()\n        if location_url.startswith('/'):\n            host = urlsplit(cdx['url']).netloc\n            location_url = host + location_url\n\n        location_url = location_url.split('://', 1)[-1].rstrip('/')\n        request_url = request_url.split('://', 1)[-1].rstrip('/')\n\n        self_redir = False\n\n        if request_url == location_url:\n            self_redir = True\n        elif params.get('sr-urlkey'):\n            # if new location canonicalized matches old key, also self-redirect\n            if canonicalize(location_url) == params.get('sr-urlkey'):\n                self_redir = True\n\n        if self_redir:\n            msg = 'Self Redirect {0} -> {1}'\n            msg = msg.format(request_url, location_url)\n            params['sr-urlkey'] = cdx['urlkey']\n            raise LiveResourceException(msg)", "response": "Check if response is a 3xx redirect to the same url"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a single record from given url at offset with lengthLength and parse as either warc or arc record", "response": "def load(self, url, offset, length, no_record_parse=False):\n        \"\"\" Load a single record from given url at offset with length\n        and parse as either warc or arc record\n        \"\"\"\n        try:\n            length = int(length)\n        except:\n            length = -1\n\n        stream = self.loader.load(url, int(offset), length)\n        decomp_type = 'gzip'\n\n        # Create decompressing stream\n        stream = DecompressingBufferedReader(stream=stream,\n                                             decomp_type=decomp_type,\n                                             block_size=self.block_size)\n\n        return self.parse_record_stream(stream, no_record_parse=no_record_parse)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef percent_encode_host(url):\n\n        # only continue if punycode encoded\n        if 'xn--' not in url:\n            return url\n\n        parts = urlsplit(url)\n        domain = parts.netloc.encode('utf-8')\n        try:\n            domain = domain.decode('idna')\n            if six.PY2:\n                domain = domain.encode('utf-8', 'ignore')\n        except:\n            # likely already encoded, so use as is\n            pass\n\n        domain = quote(domain)#, safe=r':\\/')\n\n        return urlunsplit((parts[0], domain, parts[2], parts[3], parts[4]))", "response": "Convert the host of uri formatted with to_uri()\n        to have a % - encoded host instead of punycode host\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_uri(url):\n        parts = WbUrl.FIRST_PATH.split(url, 1)\n\n        sep = url[len(parts[0])] if len(parts) > 1 else None\n\n        scheme_dom = unquote_plus(parts[0])\n\n        if six.PY2 and isinstance(scheme_dom, six.binary_type):\n            if scheme_dom == parts[0]:\n                return url\n\n            scheme_dom = scheme_dom.decode('utf-8', 'ignore')\n\n        scheme_dom = scheme_dom.rsplit('/', 1)\n        domain = scheme_dom[-1]\n\n        try:\n            domain = to_native_str(domain.encode('idna'), 'utf-8')\n        except UnicodeError:\n            # the url is invalid and this is probably not a domain\n            pass\n\n        if len(scheme_dom) > 1:\n            url = to_native_str(scheme_dom[0], 'utf-8') + '/' + domain\n        else:\n            url = domain\n\n        if len(parts) > 1:\n            url += sep\n\n            rest = parts[1]\n            try:\n                rest.encode('ascii')\n            except UnicodeEncodeError:\n                rest = quote(to_native_str(rest, 'utf-8'))\n\n            url += rest\n\n        return url", "response": "Converts a url to an ascii % - encoded form where the first part is a scheme + domain + the second part is a remainder of the url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the CDX record to text.", "response": "def to_text(self, fields=None):\n        \"\"\"\n        return plaintext CDX record (includes newline).\n        if ``fields`` is ``None``, output will have all fields\n        in the order they are stored.\n\n        :param fields: list of field names to output.\n        \"\"\"\n        if fields is None:\n            return str(self) + '\\n'\n\n        try:\n            result = ' '.join(str(self[x]) for x in fields) + '\\n'\n        except KeyError as ke:\n            msg = 'Invalid field \"{0}\" found in fields= argument'\n            msg = msg.format(str(ke))\n            raise CDXException(msg)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef conv_to_json(obj, fields=None):\n        if fields is None:\n            return json_encode(OrderedDict(((x, obj[x]) for x in obj if not x.startswith('_')))) + '\\n'\n\n        result = json_encode(OrderedDict([(x, obj[x]) for x in fields if x in obj])) + '\\n'\n\n        return result", "response": "converts a cdx dict to json dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rewrite_text_stream_to_gen(self, stream, rwinfo):\n        try:\n            buff = self.first_buff\n\n            # for html rewriting:\n            # if charset is utf-8, use that, otherwise default to encode to ascii-compatible encoding\n            # encoding only used for url rewriting, encoding back to bytes after rewriting\n            if rwinfo.charset == 'utf-8' and rwinfo.text_type == 'html':\n                charset = 'utf-8'\n            else:\n                charset = 'iso-8859-1'\n\n            if buff:\n                yield buff.encode(charset)\n\n            decoder = codecs.getincrementaldecoder(charset)()\n\n            while True:\n                buff = stream.read(BUFF_SIZE)\n                if not buff:\n                    break\n\n                if self.align_to_line:\n                    buff += stream.readline()\n\n                try:\n                    buff = decoder.decode(buff)\n                except UnicodeDecodeError:\n                    if charset == 'utf-8':\n                        rwinfo.charset = 'iso-8859-1'\n                        charset = rwinfo.charset\n                        decoder = codecs.getincrementaldecoder(charset)()\n                        buff = decoder.decode(buff)\n\n                buff = self.rewrite(buff)\n\n                yield buff.encode(charset)\n\n            # For adding a tail/handling final buffer\n            buff = self.final_read()\n\n            # ensure decoder is marked as finished (final buffer already decoded)\n            decoder.decode(b'', final=True)\n\n            if buff:\n                yield buff.encode(charset)\n\n        finally:\n            stream.close()", "response": "Convert stream to generator using applying rewriting func\n            to each portion of the stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload text CDX lines from sources return an iterator for merged text CDX lines filtered and access - checked sequence of CDX objects.", "response": "def cdx_load(sources, query, process=True):\n    \"\"\"\n    merge text CDX lines from sources, return an iterator for\n    filtered and access-checked sequence of CDX objects.\n\n    :param sources: iterable for text CDX sources.\n    :param process: bool, perform processing sorting/filtering/grouping ops\n    \"\"\"\n    cdx_iter = create_merged_cdx_gen(sources, query)\n\n    # page count is a special case, no further processing\n    if query.page_count:\n        return cdx_iter\n\n    cdx_iter = make_obj_iter(cdx_iter, query)\n\n    if process and not query.secondary_index_only:\n        cdx_iter = process_cdx(cdx_iter, query)\n\n    custom_ops = query.custom_ops\n    for op in custom_ops:\n        cdx_iter = op(cdx_iter, query)\n\n    if query.output == 'text':\n        cdx_iter = cdx_to_text(cdx_iter, query.fields)\n    elif query.output == 'json':\n        cdx_iter = cdx_to_json(cdx_iter, query.fields)\n\n    return cdx_iter"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_merged_cdx_gen(sources, query):\n    # Optimize: no need to merge if just one input\n    if len(sources) == 1:\n        cdx_iter = sources[0].load_cdx(query)\n    else:\n        source_iters = map(lambda src: src.load_cdx(query), sources)\n        cdx_iter = merge(*(source_iters))\n\n    for cdx in cdx_iter:\n        yield cdx", "response": "create a generator which loads and merges cdxs from sources"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_obj_iter(text_iter, query):\n    if query.secondary_index_only:\n        cls = IDXObject\n    else:\n        cls = CDXObject\n\n    return (cls(line) for line in text_iter)", "response": "convert text cdx stream to CDXObject or IDXObject."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cdx_limit(cdx_iter, limit):\n#    for cdx, _ in itertools.izip(cdx_iter, xrange(limit)):\n#        yield cdx\n    return (cdx for cdx, _ in zip(cdx_iter, range(limit)))", "response": "limit cdx to at most limit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning cdx records in reverse order. cdx_iter is iterable of cdx objects. limit is the number of records to return.", "response": "def cdx_reverse(cdx_iter, limit):\n    \"\"\"\n    return cdx records in reverse order.\n    \"\"\"\n    # optimize for single last\n    if limit == 1:\n        last = None\n\n        for cdx in cdx_iter:\n            last = cdx\n\n        if not last:\n            return\n        yield last\n\n    reverse_cdxs = deque(maxlen=limit)\n\n    for cdx in cdx_iter:\n        reverse_cdxs.appendleft(cdx)\n\n    for cdx in reverse_cdxs:\n        yield cdx"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters CDX by regex.", "response": "def cdx_filter(cdx_iter, filter_strings):\n    \"\"\"\n    filter CDX by regex if each filter is :samp:`{field}:{regex}` form,\n    apply filter to :samp:`cdx[{field}]`.\n    \"\"\"\n    # Support single strings as well\n    if isinstance(filter_strings, str):\n        filter_strings = [filter_strings]\n\n    filters = [CDXFilter(filter_str) for filter_str in filter_strings]\n\n    for cdx in cdx_iter:\n        if all(x(cdx) for x in filters):\n            yield cdx"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cdx_clamp(cdx_iter, from_ts, to_ts):\n    if from_ts and len(from_ts) < 14:\n        from_ts = pad_timestamp(from_ts, PAD_14_DOWN)\n\n    if to_ts and len(to_ts) < 14:\n        to_ts = pad_timestamp(to_ts, PAD_14_UP)\n\n    for cdx in cdx_iter:\n        if from_ts and cdx[TIMESTAMP] < from_ts:\n            continue\n\n        if to_ts and cdx[TIMESTAMP] > to_ts:\n            continue\n\n        yield cdx", "response": "Clamp the given CDX list by start and end ts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cdx_collapse_time_status(cdx_iter, timelen=10):\n    timelen = int(timelen)\n\n    last_token = None\n\n    for cdx in cdx_iter:\n        curr_token = (cdx[TIMESTAMP][:timelen], cdx.get(STATUSCODE, ''))\n\n        # yield if last_dedup_time is diff, otherwise skip\n        if curr_token != last_token:\n            last_token = curr_token\n            yield cdx", "response": "collapse time status of a single CDX into a single CDX."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsorts CDXCaptureResult by closest to timestamp.", "response": "def cdx_sort_closest(closest, cdx_iter, limit=10):\n    \"\"\"\n    sort CDXCaptureResult by closest to timestamp.\n    \"\"\"\n    closest_cdx = []\n    closest_keys = []\n    closest_sec = timestamp_to_sec(closest)\n\n    for cdx in cdx_iter:\n        sec = timestamp_to_sec(cdx[TIMESTAMP])\n        key = abs(closest_sec - sec)\n\n        # create tuple to sort by key\n        #bisect.insort(closest_cdx, (key, cdx))\n\n        i = bisect.bisect_right(closest_keys, key)\n        closest_keys.insert(i, key)\n        closest_cdx.insert(i, cdx)\n\n        if len(closest_cdx) == limit:\n            # assuming cdx in ascending order and keys have started increasing\n            if key > closest_keys[-1]:\n                break\n\n        if len(closest_cdx) > limit:\n            closest_cdx.pop()\n\n    for cdx in closest_cdx:\n        yield cdx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding CDX records that are revisits.", "response": "def cdx_resolve_revisits(cdx_iter):\n    \"\"\"\n    resolve revisits.\n\n    this filter adds three fields to CDX: ``orig.length``, ``orig.offset``,\n    and ``orig.filename``. for revisit records, these fields have corresponding\n    field values in previous non-revisit (original) CDX record.\n    They are all ``\"-\"`` for non-revisit records.\n    \"\"\"\n    originals = {}\n\n    for cdx in cdx_iter:\n        is_revisit = cdx.is_revisit()\n\n        digest = cdx.get(DIGEST)\n\n        original_cdx = None\n\n        # only set if digest is valid, otherwise no way to resolve\n        if digest:\n            original_cdx = originals.get(digest)\n\n            if not original_cdx and not is_revisit:\n                originals[digest] = cdx\n\n        if original_cdx and is_revisit:\n            fill_orig = lambda field: original_cdx.get(field, '-')\n            # Transfer mimetype and statuscode\n            if MIMETYPE in cdx:\n                cdx[MIMETYPE] = original_cdx.get(MIMETYPE, '')\n            if STATUSCODE in cdx:\n                cdx[STATUSCODE] = original_cdx.get(STATUSCODE, '')\n        else:\n            fill_orig = lambda field: '-'\n\n        # Always add either the original or empty '- - -'\n        for field in ORIG_TUPLE:\n            cdx['orig.' + field] = fill_orig(field)\n\n        yield cdx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self):\n        if self.r.live:\n            self.extra_config['collections'] = {'live':\n                    {'index': '$live'}}\n\n        if self.r.debug:\n            self.extra_config['debug'] = True\n\n        if self.r.record:\n            self.extra_config['recorder'] = 'live'", "response": "This method loads the application. Subclasses must return a application\n        that can be used by used by pywb. utils. GeventServer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_gevent(self):\n        from pywb.utils.geventserver import GeventServer, RequestURIWSGIHandler\n        logging.info('Starting Gevent Server on ' + str(self.r.port))\n        ge = GeventServer(self.application,\n                          port=self.r.port,\n                          hostname=self.r.bind,\n                          handler_class=RequestURIWSGIHandler,\n                          direct=True)", "response": "Creates the server that runs the application supplied a subclass of GeventServer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ensure_url_has_path(self, url):\n        inx = url.find('://')\n        if inx > 0:\n            rest = url[inx + 3:]\n        elif url.startswith('//'):\n            rest = url[2:]\n        else:\n            rest = url\n\n        if '/' in rest:\n            return url\n\n        scheme, netloc, path, query, frag = urlsplit(url)\n        if not path:\n            path = '/'\n\n        url = urlunsplit((scheme, netloc, path, query, frag))\n        return url", "response": "ensure the url has a path component"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rewrite_tag_attrs(self, tag, tag_attrs, set_parsing_context=True):\n        # special case: head insertion, before-head tags\n        if (self.head_insert and\n              not self._wb_parse_context\n              and (tag not in self.BEFORE_HEAD_TAGS)):\n            self.out.write(self.head_insert)\n            self.head_insert = None\n\n        if set_parsing_context:\n            self._set_parse_context(tag, tag_attrs)\n\n        # attr rewriting\n        handler = self.rewrite_tags.get(tag)\n        if not handler:\n            handler = {}\n\n        self.out.write('<' + tag)\n\n        for attr_name, attr_value in tag_attrs:\n            empty_attr = False\n            if attr_value is None:\n                attr_value = ''\n                empty_attr = True\n\n            # special case: inline JS/event handler\n            if ((attr_value and attr_value.startswith('javascript:'))\n                 or attr_name.startswith('on') and attr_name[2:3] != '-'):\n                attr_value = self._rewrite_script(attr_value, True)\n\n            # special case: inline CSS/style attribute\n            elif attr_name == 'style':\n                attr_value = self._rewrite_css(attr_value)\n\n            # special case: deprecated background attribute\n            elif attr_name == 'background':\n                rw_mod = 'im_'\n                attr_value = self._rewrite_url(attr_value, rw_mod)\n\n            # special case: srcset list\n            elif attr_name == 'srcset':\n                rw_mod = handler.get(attr_name, '')\n                attr_value = self._rewrite_srcset(attr_value, rw_mod)\n\n            # special case: disable crossorigin and integrity attr\n            # as they may interfere with rewriting semantics\n            elif attr_name in ('crossorigin', 'integrity'):\n                attr_name = '_' + attr_name\n\n            # special case: if rewrite_canon not set,\n            # don't rewrite rel=canonical\n            elif tag == 'link' and attr_name == 'href':\n                rw_mod = handler.get(attr_name)\n                attr_value = self._rewrite_link_href(attr_value, tag_attrs, rw_mod)\n\n            # special case: meta tag\n            elif (tag == 'meta') and (attr_name == 'content'):\n                if self.has_attr(tag_attrs, ('http-equiv', 'refresh')):\n                    attr_value = self._rewrite_meta_refresh(attr_value)\n                elif self.has_attr(tag_attrs, ('http-equiv', 'content-security-policy')):\n                    attr_name = '_' + attr_name\n                elif self.has_attr(tag_attrs, ('name', 'referrer')):\n                    attr_value = 'no-referrer-when-downgrade'\n                elif attr_value.startswith(self.DATA_RW_PROTOCOLS):\n                    rw_mod = handler.get(attr_name)\n                    attr_value = self._rewrite_url(attr_value, rw_mod)\n\n            # special case: param value, conditional rewrite\n            elif (tag == 'param'):\n                if attr_value.startswith(self.DATA_RW_PROTOCOLS):\n                    rw_mod = handler.get(attr_name)\n                    attr_value = self._rewrite_url(attr_value, rw_mod)\n\n            # special case: data- attrs, conditional rewrite\n            elif attr_name and attr_value and attr_name.startswith('data-'):\n                if attr_value.startswith(self.DATA_RW_PROTOCOLS):\n                    rw_mod = 'oe_'\n                    attr_value = self._rewrite_url(attr_value, rw_mod)\n\n            # special case: base tag\n            elif (tag == 'base') and (attr_name == 'href') and attr_value:\n                rw_mod = handler.get(attr_name)\n                attr_value = self._rewrite_base(attr_value, rw_mod)\n\n            elif attr_name == 'href':\n                rw_mod = self.defmod\n                attr_value = self._rewrite_url(attr_value, rw_mod)\n\n            elif tag == 'script' and attr_name == 'src':\n                rw_mod = handler.get(attr_name)\n                ov = attr_value\n                attr_value = self._rewrite_url(attr_value, rw_mod)\n                if attr_value == ov and not ov.startswith(self.url_rewriter.NO_REWRITE_URI_PREFIX):\n                    # URL not skipped, likely src='js/....', forcing abs to make sure, cause PHP MIME(JS) === HTML\n                    attr_value = self._rewrite_url(attr_value, rw_mod, True)\n                    self._write_attr('__wb_orig_src', ov, empty_attr=None)\n            else:\n                # rewrite url using tag handler\n                rw_mod = handler.get(attr_name)\n                if rw_mod is not None:\n                    attr_value = self._rewrite_url(attr_value, rw_mod)\n\n            # write the attr!\n            self._write_attr(attr_name, attr_value, empty_attr)\n\n        return True", "response": "Rewrite a tag s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the template loaders based on the supplied paths and packages.", "response": "def _make_loaders(self, paths, packages):\n        \"\"\"Initialize the template loaders based on the supplied paths and packages.\n\n        :param list[str] paths: List of paths to search for templates\n        :param list[str] packages: List of assets package names\n        :return: A list of loaders to be used for loading the template assets\n        :rtype: list[FileSystemLoader|PackageLoader]\n        \"\"\"\n        loaders = []\n        # add loaders for paths\n        for path in paths:\n            loaders.append(FileSystemLoader(path))\n\n        # add loaders for all specified packages\n        for package in packages:\n            loaders.append(PackageLoader(package))\n\n        return loaders"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a decorator that adds the wrapped function to dictionary of template filters.", "response": "def template_filter(self, param=None):\n        \"\"\"Returns a decorator that adds the wrapped function to dictionary of template filters.\n\n        The wrapped function is keyed by either the supplied param (if supplied)\n        or by the wrapped functions name.\n\n        :param param: Optional name to use instead of the name of the function to be wrapped\n        :return: A decorator to wrap a template filter function\n        :rtype: callable\n        \"\"\"\n        def deco(func):\n            name = param or func.__name__\n            self.filters[name] = func\n            return func\n\n        return deco"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _init_filters(self):\n        self.filters = {}\n\n        @self.template_filter()\n        def format_ts(value, format_='%a, %b %d %Y %H:%M:%S'):\n            \"\"\"Formats the supplied timestamp using format_\n\n            :param str value: The timestamp to be formatted\n            :param str format_:  The format string\n            :return: The correctly formatted timestamp as determined by format_\n            :rtype: str\n            \"\"\"\n            if format_ == '%s':\n                return timestamp_to_sec(value)\n            else:\n                value = timestamp_to_datetime(value)\n                return value.strftime(format_)\n\n        @self.template_filter('urlsplit')\n        def get_urlsplit(url):\n            \"\"\"Splits the supplied URL\n\n            :param str url: The url to be split\n            :return: The split url\n            :rtype: urllib.parse.SplitResult\n            \"\"\"\n            split = urlsplit(url)\n            return split\n\n        @self.template_filter()\n        def tojson(obj):\n            \"\"\"Converts the supplied object/array/any to a JSON string if it can be JSONified\n\n            :param any obj: The value to be converted to a JSON string\n            :return: The JSON string representation of the supplied value\n            :rtype: str\n            \"\"\"\n            return json.dumps(obj)\n\n        @self.template_filter()\n        def tobool(bool_val):\n            \"\"\"Converts a python boolean to a JS \"true\" or \"false\" string\n            :param any obj: A value to be evaluated as a boolean\n            :return: The string \"true\" or \"false\" to be inserted into JS\n            \"\"\"\n\n            return 'true' if bool_val else 'false'", "response": "Initialize the default Jninja filters for the current page"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_to_string(self, env, **kwargs):\n        template = None\n        template_path = env.get(self.jenv.env_template_dir_key)\n\n        if template_path:\n            # jinja paths are not os paths, always use '/' as separator\n            # https://github.com/pallets/jinja/issues/411\n            template_path = template_path + '/' + self.insert_file\n\n            try:\n                template = self.jenv.jinja_env.get_template(template_path)\n            except TemplateNotFound as te:\n                pass\n\n        if not template:\n            template = self.jenv.jinja_env.get_template(self.insert_file)\n\n        params = env.get(self.jenv.env_template_params_key)\n        if params:\n            kwargs.update(params)\n\n        kwargs['env'] = env\n        kwargs['static_prefix'] = env.get('pywb.host_prefix', '') + env.get('pywb.app_prefix', '') + '/static'\n\n\n        return template.render(**kwargs)", "response": "Render this template.\n\n        :param dict env: The WSGI environment associated with the request causing this template to be rendered\n        :param any kwargs: The keyword arguments to be supplied to the Jninja template render method\n        :return: The rendered template\n        :rtype: str"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_insert_func(self, wb_url,\n                           wb_prefix,\n                           host_prefix,\n                           top_url,\n                           env,\n                           is_framed,\n                           coll='',\n                           include_ts=True,\n                           **kwargs):\n        \"\"\"Create the function used to render the header insert template for the current request.\n\n        :param rewrite.wburl.WbUrl wb_url: The WbUrl for the request this template is being rendered for\n        :param str wb_prefix: The URL prefix pywb is serving the content using (e.g. http://localhost:8080/live/)\n        :param str host_prefix: The host URL prefix pywb is running on (e.g. http://localhost:8080)\n        :param str top_url: The full URL for this request (e.g. http://localhost:8080/live/http://example.com)\n        :param dict env: The WSGI environment dictionary for this request\n        :param bool is_framed: Is pywb or a specific collection running in framed mode\n        :param str coll: The name of the collection this request is associated with\n        :param bool include_ts: Should a timestamp be included in the rendered template\n        :param kwargs: Additional keyword arguments to be supplied to the Jninja template render method\n        :return: A function to be used to render the header insert for the request this template is being rendered for\n        :rtype: callable\n        \"\"\"\n        params = kwargs\n        params['host_prefix'] = host_prefix\n        params['wb_prefix'] = wb_prefix\n        params['wb_url'] = wb_url\n        params['top_url'] = top_url\n        params['coll'] = coll\n        params['is_framed'] = is_framed\n\n        def make_head_insert(rule, cdx):\n            params['wombat_ts'] = cdx['timestamp'] if include_ts else ''\n            params['wombat_sec'] = timestamp_to_sec(cdx['timestamp'])\n            params['is_live'] = cdx.get('is_live')\n\n            if self.banner_view:\n                banner_html = self.banner_view.render_to_string(env, cdx=cdx, **params)\n                params['banner_html'] = banner_html\n\n            return self.render_to_string(env, cdx=cdx, **params)\n\n        return make_head_insert", "response": "Create a function to render the header insert template for the current request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_top_frame(self, wb_url,\n                      wb_prefix,\n                      host_prefix,\n                      env,\n                      frame_mod,\n                      replay_mod,\n                      coll='',\n                      extra_params=None):\n        \"\"\"\n        :param rewrite.wburl.WbUrl wb_url: The WbUrl for the request this template is being rendered for\n        :param str wb_prefix: The URL prefix pywb is serving the content using (e.g. http://localhost:8080/live/)\n        :param str host_prefix: The host URL prefix pywb is running on (e.g. http://localhost:8080)\n        :param dict env: The WSGI environment dictionary for the request this template is being rendered for\n        :param str frame_mod:  The modifier to be used for framing (e.g. if_)\n        :param str replay_mod: The modifier to be used in the URL of the page being replayed (e.g. mp_)\n        :param str coll: The name of the collection this template is being rendered for\n        :param dict extra_params: Additional parameters to be supplied to the Jninja template render method\n        :return: The frame insert string\n        :rtype: str\n        \"\"\"\n\n        embed_url = wb_url.to_str(mod=replay_mod)\n\n        if wb_url.timestamp:\n            timestamp = wb_url.timestamp\n        else:\n            timestamp = timestamp_now()\n\n        is_proxy = 'wsgiprox.proxy_host' in env\n\n        params = {'host_prefix': host_prefix,\n                  'wb_prefix': wb_prefix,\n                  'wb_url': wb_url,\n                  'coll': coll,\n\n                  'options': {'frame_mod': frame_mod,\n                              'replay_mod': replay_mod},\n\n                  'embed_url': embed_url,\n                  'is_proxy': is_proxy,\n                  'timestamp': timestamp,\n                  'url': wb_url.get_url()\n                 }\n\n        if extra_params:\n            params.update(extra_params)\n\n        if self.banner_view:\n            banner_html = self.banner_view.render_to_string(env, **params)\n            params['banner_html'] = banner_html\n\n        return self.render_to_string(env, **params)", "response": "Returns the top frame of the current page."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the netloc and path from the items package path", "response": "def get_pkg_path(self, item):\n        \"\"\"Get the package path for the\n\n        :param str item: A resources full package path\n        :return: The netloc and path from the items package path\n        :rtype: tuple[str, str]\n        \"\"\"\n        if not isinstance(item, str):\n            return None\n\n        parts = urlsplit(item)\n        if parts.scheme == 'pkg' and parts.netloc:\n            return (parts.netloc, parts.path)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef text_response(text, status='200 OK', content_type='text/plain; charset=utf-8'):\n        encoded_text = text.encode('utf-8')\n        status_headers = StatusAndHeaders(status,\n                                          [('Content-Type', content_type),\n                                           ('Content-Length', str(len(encoded_text)))])\n\n        return WbResponse(status_headers, value=[encoded_text])", "response": "Utility method for constructing a text response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json_response(obj, status='200 OK', content_type='application/json; charset=utf-8'):\n        return WbResponse.text_response(json.dumps(obj), status, content_type)", "response": "Utility method for constructing a JSON response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef redir_response(location, status='302 Redirect', headers=None):\n        redir_headers = [('Location', location), ('Content-Length', '0')]\n        if headers:\n            redir_headers += headers\n\n        return WbResponse(StatusAndHeaders(status, redir_headers))", "response": "Utility method for constructing a redirection response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a WbResponse for OPTIONS based on the WSGI environment dictionary", "response": "def options_response(env):\n        \"\"\"Construct WbResponse for OPTIONS based on the WSGI env dictionary\n\n        :param dict env: The WSGI environment dictionary\n        :return: The WBResponse for the options request\n        :rtype: WbResponse\n        \"\"\"\n        status_headers = StatusAndHeaders('200 Ok', [\n            ('Content-Type', 'text/plain'),\n            ('Content-Length', '0'),\n        ])\n        response = WbResponse(status_headers)\n        response.add_access_control_headers(env=env)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_access_control_headers(self, env=None):\n        allowed_methods = 'GET, POST, PUT, OPTIONS, DELETE, PATCH, HEAD, TRACE, CONNECT'\n        allowed_origin = None\n        if env is not None:\n            acr_method = env.get('HTTP_ACCESS_CONTROL_REQUEST_METHOD')\n            if acr_method is not None and acr_method not in allowed_methods:\n                allowed_methods = allowed_methods + ', ' + acr_method\n            r_method = env.get('REQUEST_METHOD')\n            if r_method is not None and r_method not in allowed_methods:\n                allowed_methods = allowed_methods + ', ' + r_method\n            acr_headers = env.get('HTTP_ACCESS_CONTROL_REQUEST_HEADERS')\n            if acr_headers is not None:\n                self.status_headers.add_header('Access-Control-Allow-Headers', acr_headers)\n            allowed_origin = env.get('HTTP_ORIGIN', env.get('HTTP_REFERER', allowed_origin))\n        if allowed_origin is None:\n            allowed_origin = '*'\n        self.status_headers.replace_header('Access-Control-Allow-Origin',  allowed_origin)\n        self.status_headers.add_header('Access-Control-Allow-Methods', allowed_methods)\n        self.status_headers.add_header('Access-Control-Allow-Credentials', 'true')\n        self.status_headers.add_header('Access-Control-Max-Age', '1800')\n        return self", "response": "Adds the values for the Access - Control HTTP headers to the current response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unsurt(surt):\n\n    try:\n        index = surt.index(')/')\n        parts = surt[0:index].split(',')\n        parts.reverse()\n        host = '.'.join(parts)\n        host += surt[index + 1:]\n        return host\n\n    except ValueError:\n        # May not be a valid surt\n        return surt", "response": "Unsurt a string into a base64 encoded unicode object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_search_range(url, match_type, surt_ordered=True, url_canon=None):\n    def inc_last_char(x):\n        return x[0:-1] + chr(ord(x[-1]) + 1)\n\n    if not url_canon:\n        # make new canon\n        url_canon = UrlCanonicalizer(surt_ordered)\n    else:\n        # ensure surt order matches url_canon\n        surt_ordered = url_canon.surt_ordered\n\n    start_key = url_canon(url)\n\n    if match_type == 'exact':\n        end_key = start_key + '!'\n\n    elif match_type == 'prefix':\n        # add trailing slash if url has it\n        if url.endswith('/') and not start_key.endswith('/'):\n            start_key += '/'\n\n        if url.endswith('?') and not start_key.endswith('?'):\n            start_key += '?'\n\n        end_key = inc_last_char(start_key)\n\n    elif match_type == 'host':\n        if surt_ordered:\n            host = start_key.split(')/')[0]\n\n            start_key = host + ')/'\n            end_key = host + '*'\n        else:\n            host = urlparse.urlsplit(url).netloc\n\n            start_key = host + '/'\n            end_key = host + '0'\n\n    elif match_type == 'domain':\n        if not surt_ordered:\n            msg = 'matchType=domain unsupported for non-surt'\n            raise UrlCanonicalizeException(msg)\n\n        host = start_key.split(')/')[0]\n\n        # if tld, use com, as start_key\n        # otherwise, stick with com,example)/\n        if ',' not in host:\n            start_key = host + ','\n        else:\n            start_key = host + ')/'\n\n        end_key = host + '-'\n    else:\n        raise UrlCanonicalizeException('Invalid match_type: ' + match_type)\n\n    return (start_key, end_key)", "response": "Compute a start and end search url search range for a given match type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a prefix cookie for all common modifiers.", "response": "def add_prefix_cookie_for_all_mods(self, morsel, results, header):\n        \"\"\"  If HttpOnly cookie that is set to a path ending in /,\n             and current mod is mp_ or if_,\n             then assume its meant to be a prefix, and likely needed for\n             other content.\n             Set cookie with same prefix but for all common modifiers:\n             (mp_, js_, cs_, oe_, if_)\n        \"\"\"\n        curr_mod = self.url_rewriter.wburl.mod\n        if curr_mod not in ('mp_', 'if_'):\n            return False\n\n        if not morsel.get('httponly'):\n            return False\n\n        path = morsel.get('path')\n        if not path or not path.endswith('/'):\n            return False\n\n        for mod in ('mp_', 'cs_', 'js_', 'im_', 'oe_', 'if_'):\n            new_path = path.replace(curr_mod + '/', mod + '/')\n            morsel['path'] = new_path\n            results.append((header, morsel.OutputString()))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_fuzzy_rule(self, rule):\n        url_prefix = rule.get('url_prefix')\n        config = rule.get('fuzzy_lookup')\n        if not config:\n            return\n\n        if not isinstance(url_prefix, list):\n            url_prefix = [url_prefix]\n\n        if not isinstance(config, dict):\n            regex = self.make_regex(config)\n            replace_after = self.DEFAULT_REPLACE_AFTER\n            filter_str = self.DEFAULT_FILTER\n            match_type = self.DEFAULT_MATCH_TYPE\n            find_all = False\n\n        else:\n            regex = self.make_regex(config.get('match'))\n            replace_after = config.get('replace', self.DEFAULT_REPLACE_AFTER)\n            filter_str = config.get('filter', self.DEFAULT_FILTER)\n            match_type = config.get('type', self.DEFAULT_MATCH_TYPE)\n            find_all = config.get('find_all', False)\n\n        return FuzzyRule(url_prefix, regex, replace_after, filter_str, match_type, find_all)", "response": "Parse a fuzzy rule."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving headers and payload for a given capture.", "response": "def load_headers_and_payload(self, cdx, failed_files, cdx_loader):\n        \"\"\"\n        Resolve headers and payload for a given capture\n        In the simple case, headers and payload are in the same record.\n        In the case of revisit records, the payload and headers may be in\n        different records.\n\n        If the original has already been found, lookup original using\n        orig. fields in cdx dict.\n        Otherwise, call _load_different_url_payload() to get cdx index\n        from a different url to find the original record.\n        \"\"\"\n        has_curr = (cdx['filename'] != '-')\n        #has_orig = (cdx.get('orig.filename', '-') != '-')\n        orig_f = cdx.get('orig.filename')\n        has_orig = orig_f and orig_f != '-'\n\n        # load headers record from cdx['filename'] unless it is '-' (rare)\n        headers_record = None\n        if has_curr:\n            headers_record = self._resolve_path_load(cdx, False, failed_files)\n\n        # two index lookups\n        # Case 1: if mimetype is still warc/revisit\n        if cdx.get('mime') == 'warc/revisit' and headers_record:\n            payload_record = self._load_different_url_payload(cdx,\n                                                              headers_record,\n                                                              failed_files,\n                                                              cdx_loader)\n\n        # single lookup cases\n        # case 2: non-revisit\n        elif (has_curr and not has_orig):\n            payload_record = headers_record\n\n        # case 3: identical url revisit, load payload from orig.filename\n        elif (has_orig):\n            payload_record = self._resolve_path_load(cdx, True, failed_files)\n\n        return headers_record, payload_record"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving a filename to full path using a list of path resolvers.", "response": "def _resolve_path_load(self, cdx, is_original, failed_files):\n        \"\"\"\n        Load specific record based on filename, offset and length\n        fields in the cdx.\n        If original=True, use the orig.* fields for the cdx\n\n        Resolve the filename to full path using specified path resolvers\n\n        If failed_files list provided, keep track of failed resolve attempts\n        \"\"\"\n\n        if is_original:\n            (filename, offset, length) = (cdx['orig.filename'],\n                                          cdx['orig.offset'],\n                                          cdx['orig.length'])\n        else:\n            (filename, offset, length) = (cdx['filename'],\n                                          cdx['offset'],\n                                          cdx.get('length', '-'))\n\n        # optimization: if same file already failed this request,\n        # don't try again\n        if failed_files is not None and filename in failed_files:\n            raise ArchiveLoadFailed('Skipping Already Failed: ' + filename)\n\n        any_found = False\n        last_exc = None\n        last_traceback = None\n        for resolver in self.path_resolvers:\n            possible_paths = resolver(filename, cdx)\n\n            if not possible_paths:\n                continue\n\n            if isinstance(possible_paths, six.string_types):\n                possible_paths = [possible_paths]\n\n            for path in possible_paths:\n                any_found = True\n                try:\n                    return (self.record_loader.\n                             load(path, offset, length,\n                               no_record_parse=self.no_record_parse))\n\n                except Exception as ue:\n                    last_exc = ue\n                    import sys\n                    last_traceback = sys.exc_info()[2]\n\n        # Unsuccessful if reached here\n        if failed_files is not None:\n            failed_files.append(filename)\n\n        if last_exc:\n            #msg = str(last_exc.__class__.__name__)\n            msg = str(last_exc)\n        else:\n            msg = 'Archive File Not Found'\n\n        #raise ArchiveLoadFailed(msg, filename), None, last_traceback\n        six.reraise(ArchiveLoadFailed, ArchiveLoadFailed(filename + ': ' + msg), last_traceback)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_different_url_payload(self, cdx, headers_record,\n                                    failed_files, cdx_loader):\n        \"\"\"\n        Handle the case where a duplicate of a capture with same digest\n        exists at a different url.\n\n        If a cdx_server is provided, a query is made for matching\n        url, timestamp and digest.\n\n        Raise exception if no matches found.\n        \"\"\"\n\n        ref_target_uri = (headers_record.rec_headers.\n                          get_header('WARC-Refers-To-Target-URI'))\n\n        target_uri = headers_record.rec_headers.get_header('WARC-Target-URI')\n\n        # if no target uri, no way to find the original\n        if not ref_target_uri:\n            raise ArchiveLoadFailed(self.MISSING_REVISIT_MSG)\n\n        ref_target_date = (headers_record.rec_headers.\n                           get_header('WARC-Refers-To-Date'))\n\n        if not ref_target_date:\n            ref_target_date = cdx['timestamp']\n        else:\n            ref_target_date = iso_date_to_timestamp(ref_target_date)\n\n        digest = cdx.get('digest', '-')\n\n        try:\n            orig_cdx_lines = self.load_cdx_for_dupe(ref_target_uri,\n                                                    ref_target_date,\n                                                    digest,\n                                                    cdx_loader)\n        except NotFoundException:\n            raise ArchiveLoadFailed(self.MISSING_REVISIT_MSG)\n\n        for orig_cdx in orig_cdx_lines:\n            try:\n                payload_record = self._resolve_path_load(orig_cdx, False,\n                                                         failed_files)\n                return payload_record\n\n            except ArchiveLoadFailed as e:\n                pass\n\n        raise ArchiveLoadFailed(self.MISSING_REVISIT_MSG)", "response": "Handle the case where a duplicate of a capture with same url exists at a different url."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_cdx_for_dupe(self, url, timestamp, digest, cdx_loader):\n        if not cdx_loader:\n            return iter([])\n\n        filters = []\n\n        filters.append('!mime:warc/revisit')\n\n        if digest and digest != '-':\n            filters.append('digest:' + digest)\n\n        params = dict(url=url,\n                      closest=timestamp,\n                      filter=filters)\n\n        return cdx_loader(params)", "response": "Load a cdx from a server if it is available and return it otherwise empty list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef binsearch_offset(reader, key, compare_func=cmp, block_size=8192):\n    min_ = 0\n\n    reader.seek(0, 2)\n    max_ = int(reader.tell() / block_size)\n\n    while max_ - min_ > 1:\n        mid = int(min_ + ((max_ - min_) / 2))\n        reader.seek(mid * block_size)\n\n        if mid > 0:\n            reader.readline()  # skip partial line\n\n        line = reader.readline()\n\n        if compare_func(key, line) > 0:\n            min_ = mid\n        else:\n            max_ = mid\n\n    return min_ * block_size", "response": "Find the offset of the first line which matches a given key using binary search."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef binsearch(reader, key, compare_func=cmp, block_size=8192):\n\n    min_ = binsearch_offset(reader, key, compare_func, block_size)\n\n    reader.seek(min_)\n\n    if min_ > 0:\n        reader.readline()  # skip partial line\n\n    def gen_iter(line):\n        while line:\n            yield line.rstrip()\n            line = reader.readline()\n\n    return gen_iter(reader.readline())", "response": "Perform a binary search for a specified key to within a specified block_size granularity and return the first full line found."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a linear search over an iterator until the key is less than or equal to the given key.", "response": "def linearsearch(iter_, key, prev_size=0, compare_func=cmp):\n    \"\"\"\n    Perform a linear search over iterator until\n    current_line >= key\n\n    optionally also tracking upto N previous lines, which are\n    returned before the first matched line.\n\n    if end of stream is reached before a match is found,\n    nothing is returned (prev lines discarded also)\n    \"\"\"\n\n    prev_deque = deque(maxlen=prev_size + 1)\n\n    matched = False\n\n    for line in iter_:\n        prev_deque.append(line)\n        if compare_func(line, key) >= 0:\n            matched = True\n            break\n\n    # no matches, so return empty iterator\n    if not matched:\n        return iter([])\n\n    return itertools.chain(prev_deque, iter_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a binary search for a specified key to within a block of size block_size.", "response": "def search(reader, key, prev_size=0, compare_func=cmp, block_size=8192):\n    \"\"\"\n    Perform a binary search for a specified key to within a 'block_size'\n    (default 8192) sized block followed by linear search\n    within the block to find first matching line.\n\n    When performin_g linear search, keep track of up to N previous lines before\n    first matching line.\n    \"\"\"\n    iter_ = binsearch(reader, key, compare_func, block_size)\n    iter_ = linearsearch(iter_,\n                         key, prev_size=prev_size,\n                         compare_func=compare_func)\n    return iter_"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_range(reader, start, end, prev_size=0):\n\n    iter_ = search(reader, start, prev_size=prev_size)\n\n    end_iter = itertools.takewhile(\n        lambda line: line < end,\n        iter_)\n\n    return end_iter", "response": "Creates an iterator which iterates over lines where start < line < end"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an iterator over lines that start with prefix key in a sorted text file.", "response": "def iter_prefix(reader, key):\n    \"\"\"\n    Creates an iterator which iterates over lines that start with prefix\n    'key' in a sorted text file.\n    \"\"\"\n\n    return itertools.takewhile(\n        lambda line: line.startswith(key),\n        search(reader, key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_routes(self):\n        self.url_map = Map()\n        self.url_map.add(Rule('/static/_/<coll>/<path:filepath>', endpoint=self.serve_static))\n        self.url_map.add(Rule('/static/<path:filepath>', endpoint=self.serve_static))\n        self.url_map.add(Rule('/collinfo.json', endpoint=self.serve_listing))\n\n        if self.is_valid_coll('$root'):\n            coll_prefix = ''\n        else:\n            coll_prefix = '/<coll>'\n            self.url_map.add(Rule('/', endpoint=self.serve_home))\n\n        self.url_map.add(Rule(coll_prefix + self.cdx_api_endpoint, endpoint=self.serve_cdx))\n        self.url_map.add(Rule(coll_prefix + '/', endpoint=self.serve_coll_page))\n        self.url_map.add(Rule(coll_prefix + '/timemap/<timemap_output>/<path:url>', endpoint=self.serve_content))\n\n        if self.recorder_path:\n            self.url_map.add(Rule(coll_prefix + self.RECORD_ROUTE + '/<path:url>', endpoint=self.serve_record))\n\n        if self.proxy_prefix is not None:\n            # Add the proxy-fetch endpoint to enable PreservationWorker to make CORS fetches worry free in proxy mode\n            self.url_map.add(Rule('/proxy-fetch/<path:url>', endpoint=self.proxy_fetch,\n                                  methods=['GET', 'HEAD', 'OPTIONS']))\n        self.url_map.add(Rule(coll_prefix + '/<path:url>', endpoint=self.serve_content))", "response": "Initialize the routes based on the configuration file makes available\n            specific routes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a dictionary containing the full URLs of the upstream apps", "response": "def get_upstream_paths(self, port):\n        \"\"\"Retrieve a dictionary containing the full URLs of the upstream apps\n\n        :param int port: The port used by the replay and cdx servers\n        :return: A dictionary containing the upstream paths (replay, cdx-server, record [if enabled])\n        :rtype: dict[str, str]\n        \"\"\"\n        base_paths = {\n                'replay': self.REPLAY_API % port,\n                'cdx-server': self.CDX_API % port,\n               }\n\n        if self.recorder_path:\n            base_paths['record'] = self.recorder_path\n\n        return base_paths"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_recorder(self, recorder_config):\n        if not recorder_config:\n            self.recorder = None\n            self.recorder_path = None\n            return\n\n        if isinstance(recorder_config, str):\n            recorder_coll = recorder_config\n            recorder_config = {}\n        else:\n            recorder_coll = recorder_config['source_coll']\n\n        # TODO: support dedup\n        dedup_index = None\n        warc_writer = MultiFileWARCWriter(self.warcserver.archive_paths,\n                                          max_size=int(recorder_config.get('rollover_size', 1000000000)),\n                                          max_idle_secs=int(recorder_config.get('rollover_idle_secs', 600)),\n                                          filename_template=recorder_config.get('filename_template'),\n                                          dedup_index=dedup_index)\n\n        self.recorder = RecorderApp(self.RECORD_SERVER % str(self.warcserver_server.port), warc_writer,\n                                    accept_colls=recorder_config.get('source_filter'))\n\n\n        recorder_server = GeventServer(self.recorder, port=0)\n\n        self.recorder_path = self.RECORD_API % (recorder_server.port, recorder_coll)", "response": "Initialize the recording functionality of pywb."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes and start the auto - indexing of the collections.", "response": "def init_autoindex(self, auto_interval):\n        \"\"\"Initialize and start the auto-indexing of the collections. If auto_interval is None this is a no op.\n\n        :param str|int auto_interval: The auto-indexing interval from the configuration file or CLI argument\n        \"\"\"\n        if not auto_interval:\n            return\n\n        from pywb.manager.autoindex import AutoIndexer\n\n        colls_dir = self.warcserver.root_dir if self.warcserver.root_dir else None\n\n        indexer = AutoIndexer(colls_dir=colls_dir, interval=int(auto_interval))\n\n        if not os.path.isdir(indexer.root_path):\n            msg = 'No managed directory \"{0}\" for auto-indexing'\n            logging.error(msg.format(indexer.root_path))\n            import sys\n            sys.exit(2)\n\n        msg = 'Auto-Indexing Enabled on \"{0}\", checking every {1} secs'\n        logging.info(msg.format(indexer.root_path, auto_interval))\n        indexer.start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserve the home view of pywb", "response": "def serve_home(self, environ):\n        \"\"\"Serves the home (/) view of pywb (not a collections)\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :return: The WbResponse for serving the home (/) path\n        :rtype: WbResponse\n        \"\"\"\n        home_view = BaseInsertView(self.rewriterapp.jinja_env, 'index.html')\n        fixed_routes = self.warcserver.list_fixed_routes()\n        dynamic_routes = self.warcserver.list_dynamic_routes()\n\n        routes = fixed_routes + dynamic_routes\n\n        all_metadata = self.metadata_cache.get_all(dynamic_routes)\n\n        content = home_view.render_to_string(environ,\n                                             routes=routes,\n                                             all_metadata=all_metadata)\n\n        return WbResponse.text_response(content, content_type='text/html; charset=\"utf-8\"')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserves a static file associated with a specific collection or one of pywb s own static assets.", "response": "def serve_static(self, environ, coll='', filepath=''):\n        \"\"\"Serve a static file associated with a specific collection or one of pywb's own static assets\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :param str coll: The collection the static file is associated with\n        :param str filepath: The file path (relative to the collection) for the static assest\n        :return: The WbResponse for the static asset\n        :rtype: WbResponse\n        \"\"\"\n        proxy_enabled = self.is_proxy_enabled(environ)\n        if proxy_enabled and environ.get('REQUEST_METHOD') == 'OPTIONS':\n            return WbResponse.options_response(environ)\n        if coll:\n            path = os.path.join(self.warcserver.root_dir, coll, self.static_dir)\n        else:\n            path = self.static_dir\n\n        environ['pywb.static_dir'] = path\n        try:\n            response = self.static_handler(environ, filepath)\n            if proxy_enabled:\n                response.add_access_control_headers(env=environ)\n            return response\n        except:\n            self.raise_not_found(environ, 'Static File Not Found: {0}'.format(filepath))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the metadata associated with a collection", "response": "def get_metadata(self, coll):\n        \"\"\"Retrieve the metadata associated with a collection\n\n        :param str coll: The name of the collection to receive metadata for\n        :return: The collections metadata if it exists\n        :rtype: dict\n        \"\"\"\n        #if coll == self.all_coll:\n        #    coll = '*'\n\n        metadata = {'coll': coll,\n                    'type': 'replay'}\n\n        if coll in self.warcserver.list_fixed_routes():\n            metadata.update(self.warcserver.get_coll_config(coll))\n        else:\n            metadata.update(self.metadata_cache.load(coll))\n\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender and serve a collections search page.", "response": "def serve_coll_page(self, environ, coll='$root'):\n        \"\"\"Render and serve a collections search page (search.html).\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :param str coll: The name of the collection to serve the collections search page for\n        :return: The WbResponse containing the collections search page\n        :rtype: WbResponse\n        \"\"\"\n        if not self.is_valid_coll(coll):\n            self.raise_not_found(environ, 'No handler for \"/{0}\"'.format(coll))\n\n        self.setup_paths(environ, coll)\n\n        metadata = self.get_metadata(coll)\n\n        view = BaseInsertView(self.rewriterapp.jinja_env, 'search.html')\n\n        wb_prefix = environ.get('SCRIPT_NAME')\n        if wb_prefix:\n            wb_prefix += '/'\n\n        content = view.render_to_string(environ,\n                                        wb_prefix=wb_prefix,\n                                        metadata=metadata,\n                                        coll=coll)\n\n        return WbResponse.text_response(content, content_type='text/html; charset=\"utf-8\"')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking the upstream CDX query for a collection and return the results of the query", "response": "def serve_cdx(self, environ, coll='$root'):\n        \"\"\"Make the upstream CDX query for a collection and response with the results of the query\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :param str coll: The name of the collection this CDX query is for\n        :return: The WbResponse containing the results of the CDX query\n        :rtype: WbResponse\n        \"\"\"\n        base_url = self.rewriterapp.paths['cdx-server']\n\n        #if coll == self.all_coll:\n        #    coll = '*'\n\n        cdx_url = base_url.format(coll=coll)\n\n        if environ.get('QUERY_STRING'):\n            cdx_url += '&' if '?' in cdx_url else '?'\n            cdx_url += environ.get('QUERY_STRING')\n\n        try:\n            res = requests.get(cdx_url, stream=True)\n\n            content_type = res.headers.get('Content-Type')\n\n            return WbResponse.bin_stream(StreamIter(res.raw),\n                                         content_type=content_type)\n\n        except Exception as e:\n            return WbResponse.text_response('Error: ' + str(e), status='400 Bad Request')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef serve_record(self, environ, coll='$root', url=''):\n        if coll in self.warcserver.list_fixed_routes():\n            return WbResponse.text_response('Error: Can Not Record Into Custom Collection \"{0}\"'.format(coll))\n\n        return self.serve_content(environ, coll, url, record=True)", "response": "Serve a URL s content from a WARC record in replay mode or from the live web infrastructure and record mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserves the contents of a URL or Record.", "response": "def serve_content(self, environ, coll='$root', url='', timemap_output='', record=False):\n        \"\"\"Serve the contents of a URL/Record rewriting the contents of the response when applicable.\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :param str coll: The name of the collection the record is to be served from\n        :param str url: The URL for the corresponding record to be served if it exists\n        :param str timemap_output: The contents of the timemap included in the link header of the response\n        :param bool record: Should the content being served by recorded (save to a warc). Only valid in record mode\n        :return: WbResponse containing the contents of the record/URL\n        :rtype: WbResponse\n        \"\"\"\n        if not self.is_valid_coll(coll):\n            self.raise_not_found(environ, 'No handler for \"/{0}\"'.format(coll))\n\n        self.setup_paths(environ, coll, record)\n\n        request_uri = environ.get('REQUEST_URI')\n        script_name = environ.get('SCRIPT_NAME', '') + '/'\n        if request_uri and request_uri.startswith(script_name):\n            wb_url_str = request_uri[len(script_name):]\n\n        else:\n            wb_url_str = to_native_str(url)\n\n            if environ.get('QUERY_STRING'):\n                wb_url_str += '?' + environ.get('QUERY_STRING')\n\n        metadata = self.get_metadata(coll)\n        if record:\n            metadata['type'] = 'record'\n\n        if timemap_output:\n            metadata['output'] = timemap_output\n            # ensure that the timemap path information is not included\n            wb_url_str = wb_url_str.replace('timemap/{0}/'.format(timemap_output), '')\n        try:\n            response = self.rewriterapp.render_content(wb_url_str, metadata, environ)\n        except UpstreamException as ue:\n            response = self.rewriterapp.handle_error(environ, ue)\n            raise HTTPException(response=response)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npopulates the WSGI environment dictionary with the path information necessary to perform a response for the content or record.", "response": "def setup_paths(self, environ, coll, record=False):\n        \"\"\"Populates the WSGI environment dictionary with the path information necessary to perform a response for\n        content or record.\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :param str coll: The name of the collection the record is to be served from\n        :param bool record: Should the content being served by recorded (save to a warc). Only valid in record mode\n        \"\"\"\n        if not coll or not self.warcserver.root_dir:\n            return\n\n        if coll != '$root':\n            pop_path_info(environ)\n            if record:\n                pop_path_info(environ)\n\n        paths = [self.warcserver.root_dir]\n\n        if coll != '$root':\n            paths.append(coll)\n\n        paths.append(self.templates_dir)\n\n        # jinja2 template paths always use '/' as separator\n        environ['pywb.templates_dir'] = '/'.join(paths)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserving the response for WARCServer fixed and dynamic listing", "response": "def serve_listing(self, environ):\n        \"\"\"Serves the response for WARCServer fixed and dynamic listing (paths)\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :return: WbResponse containing the frontend apps WARCServer URL paths\n        :rtype: WbResponse\n        \"\"\"\n        result = {'fixed': self.warcserver.list_fixed_routes(),\n                  'dynamic': self.warcserver.list_dynamic_routes()\n                 }\n\n        return WbResponse.json_response(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid_coll(self, coll):\n        #if coll == self.all_coll:\n        #    return True\n\n        return (coll in self.warcserver.list_fixed_routes() or\n                coll in self.warcserver.list_dynamic_routes())", "response": "Determines if the collection name for a request is valid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raise_not_found(self, environ, msg):\n        raise NotFound(response=self.rewriterapp._error_response(environ, msg))", "response": "Utility function for raising a werkzeug. exceptions. NotFound execption with the supplied WSGI environment dictionary and message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a WbResponse for a HTTP 307 redirection if the HTTP referer header is the same as the HTTP host header.", "response": "def _check_refer_redirect(self, environ):\n        \"\"\"Returns a WbResponse for a HTTP 307 redirection if the HTTP referer header is the same as the HTTP host header\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :return: WbResponse HTTP 307 redirection\n        :rtype: WbResponse\n        \"\"\"\n        referer = environ.get('HTTP_REFERER')\n        if not referer:\n            return\n\n        host = environ.get('HTTP_HOST')\n        if host not in referer:\n            return\n\n        inx = referer[1:].find('http')\n        if not inx:\n            inx = referer[1:].find('///')\n            if inx > 0:\n                inx + 1\n\n        if inx < 0:\n            return\n\n        url = referer[inx + 1:]\n        host = referer[:inx + 1]\n\n        orig_url = environ['PATH_INFO']\n        if environ.get('QUERY_STRING'):\n            orig_url += '?' + environ['QUERY_STRING']\n\n        full_url = host + urljoin(url, orig_url)\n        return WbResponse.redir_response(full_url, '307 Redirect')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the route handler and calls it returning its the response", "response": "def handle_request(self, environ, start_response):\n        \"\"\"Retrieves the route handler and calls the handler returning its the response\n\n        :param dict environ: The WSGI environment dictionary for the request\n        :param start_response:\n        :return: The WbResponse for the request\n        :rtype: WbResponse\n        \"\"\"\n        urls = self.url_map.bind_to_environ(environ)\n        try:\n            endpoint, args = urls.match()\n            # store original script_name (original prefix) before modifications are made\n            environ['pywb.app_prefix'] = environ.get('SCRIPT_NAME')\n\n            response = endpoint(environ, **args)\n            return response(environ, start_response)\n\n        except HTTPException as e:\n            redir = self._check_refer_redirect(environ)\n            if redir:\n                return redir(environ, start_response)\n\n            return e(environ, start_response)\n\n        except Exception as e:\n            if self.debug:\n                traceback.print_exc()\n\n            response = self.rewriterapp._error_response(environ, 'Internal Error: ' + str(e), '500 Server Error')\n            return response(environ, start_response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new instance of FrontEndApp that listens on port with a hostname of 0. 0", "response": "def create_app(cls, port):\n        \"\"\"Create a new instance of FrontEndApp that listens on port with a hostname of 0.0.0.0\n\n        :param int port: The port FrontEndApp is to listen on\n        :return: A new instance of FrontEndApp wrapped in GeventServer\n        :rtype: GeventServer\n        \"\"\"\n        app = FrontEndApp()\n        app_server = GeventServer(app, port=port, hostname='0.0.0.0')\n        return app_server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_proxy(self, config):\n        proxy_config = config.get('proxy')\n        if not proxy_config:\n            return\n\n        if isinstance(proxy_config, str):\n            proxy_coll = proxy_config\n            proxy_config = {}\n        else:\n            proxy_coll = proxy_config['coll']\n\n        if '/' in proxy_coll:\n            raise Exception('Proxy collection can not contain \"/\"')\n\n        proxy_config['ca_name'] = proxy_config.get('ca_name', self.PROXY_CA_NAME)\n        proxy_config['ca_file_cache'] = proxy_config.get('ca_file_cache', self.PROXY_CA_PATH)\n\n        if proxy_config.get('recording'):\n            logging.info('Proxy recording into collection \"{0}\"'.format(proxy_coll))\n            if proxy_coll in self.warcserver.list_fixed_routes():\n                raise Exception('Can not record into fixed collection')\n\n            proxy_coll += self.RECORD_ROUTE\n            if not config.get('recorder'):\n                config['recorder'] = 'live'\n\n        else:\n            logging.info('Proxy enabled for collection \"{0}\"'.format(proxy_coll))\n\n        if proxy_config.get('enable_content_rewrite', True):\n            self.proxy_prefix = '/{0}/bn_/'.format(proxy_coll)\n        else:\n            self.proxy_prefix = '/{0}/id_/'.format(proxy_coll)\n\n        self.proxy_default_timestamp = proxy_config.get('default_timestamp')\n        if self.proxy_default_timestamp:\n            if not self.ALL_DIGITS.match(self.proxy_default_timestamp):\n                try:\n                    self.proxy_default_timestamp = iso_date_to_timestamp(self.proxy_default_timestamp)\n                except:\n                    raise Exception('Invalid Proxy Timestamp: Must Be All-Digit Timestamp or ISO Date Format')\n\n        self.proxy_coll = proxy_coll\n\n        self.handler = WSGIProxMiddleware(self.handle_request,\n                                          self.proxy_route_request,\n                                          proxy_host=proxy_config.get('host', 'pywb.proxy'),\n                                          proxy_options=proxy_config)", "response": "Initialize and start proxy mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the full url that this proxy request will be routed to", "response": "def proxy_route_request(self, url, environ):\n        \"\"\" Return the full url that this proxy request will be routed to\n        The 'environ' PATH_INFO and REQUEST_URI will be modified based on the returned url\n\n        Default is to use the 'proxy_prefix' to point to the proxy collection\n        \"\"\"\n        if self.proxy_default_timestamp:\n            environ['pywb_proxy_default_timestamp'] = self.proxy_default_timestamp\n\n        return self.proxy_prefix + url"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, coll):\n        path = self.template_str.format(coll=coll)\n        try:\n            mtime = os.path.getmtime(path)\n            obj = self.cache.get(path)\n        except:\n            return {}\n\n        if not obj:\n            return self.store_new(coll, path, mtime)\n\n        cached_mtime, data = obj\n        if mtime == cached_mtime == mtime:\n            return obj\n\n        return self.store_new(coll, path, mtime)", "response": "Load and receive the metadata associated with a collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store_new(self, coll, path, mtime):\n        obj = load_yaml_config(path)\n        self.cache[coll] = (mtime, obj)\n        return obj", "response": "Load a collections metadata file and store it in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the metadata for all routes and populate the cache", "response": "def get_all(self, routes):\n        \"\"\"Load the metadata for all routes (collections) and populate the cache\n\n        :param list[str] routes: List of collection names\n        :return: A dictionary containing each collections metadata\n        :rtype: dict\n        \"\"\"\n        for route in routes:\n            self.load(route)\n\n        return {name: value[1] for name, value in iteritems(self.cache)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads one or more blocks of compressed cdx lines and return one line at a time", "response": "def load_blocks(self, location, blocks, ranges, query):\n        \"\"\" Load one or more blocks of compressed cdx lines, return\n        a line iterator which decompresses and returns one line at a time,\n        bounded by query.key and query.end_key\n        \"\"\"\n        if (logging.getLogger().getEffectiveLevel() <= logging.DEBUG):\n            msg = 'Loading {b.count} blocks from {loc}:{b.offset}+{b.length}'\n            logging.debug(msg.format(b=blocks, loc=location))\n\n        reader = self.blk_loader.load(location, blocks.offset, blocks.length)\n\n        def decompress_block(range_):\n            decomp = gzip_decompressor()\n            buff = decomp.decompress(reader.read(range_))\n            for line in BytesIO(buff):\n                yield line\n\n        def iter_blocks(reader):\n            try:\n                for r in ranges:\n                    yield decompress_block(r)\n            finally:\n                reader.close()\n\n        # iterate over all blocks\n        iter_ = itertools.chain.from_iterable(iter_blocks(reader))\n\n        # start bound\n        iter_ = linearsearch(iter_, query.key)\n\n        # end bound\n        iter_ = itertools.takewhile(lambda line: line < query.end_key, iter_)\n        return iter_"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting status code only from status line", "response": "def extract_status(self, status_headers):\n        \"\"\" Extract status code only from status line\n        \"\"\"\n        self['status'] = status_headers.get_statuscode()\n        if not self['status']:\n            self['status'] = '-'\n        elif self['status'] == '204' and 'Error' in status_headers.statusline:\n            self['status'] = '-'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_warc_record(self, record):\n\n        entry = self._create_index_entry(record.rec_type)\n\n        if record.rec_type == 'warcinfo':\n            entry['url'] = record.rec_headers.get_header('WARC-Filename')\n            entry['urlkey'] = entry['url']\n            entry['_warcinfo'] = record.raw_stream.read(record.length)\n            return entry\n\n        entry['url'] = record.rec_headers.get_header('WARC-Target-Uri')\n\n        # timestamp\n        entry['timestamp'] = iso_date_to_timestamp(record.rec_headers.\n                                                   get_header('WARC-Date'))\n\n        # mime\n        if record.rec_type == 'revisit':\n            entry['mime'] = 'warc/revisit'\n        elif self.options.get('minimal'):\n            entry['mime'] = '-'\n        else:\n            def_mime = '-' if record.rec_type == 'request' else 'unk'\n            entry.extract_mime(record.http_headers.\n                               get_header('Content-Type'),\n                               def_mime)\n            # detected mime from WARC-Identified-Payload-Type\n            entry['mime-detected'] = record.rec_headers.get_header(\n                                        'WARC-Identified-Payload-Type')\n\n        # status -- only for response records (by convention):\n        if record.rec_type == 'response' and not self.options.get('minimal'):\n            entry.extract_status(record.http_headers)\n        else:\n            entry['status'] = '-'\n\n        # digest\n        digest = record.rec_headers.get_header('WARC-Payload-Digest')\n        entry['digest'] = digest\n        if digest and digest.startswith('sha1:'):\n            entry['digest'] = digest[len('sha1:'):]\n\n        elif not entry.get('digest'):\n            entry['digest'] = '-'\n\n        # optional json metadata, if present\n        metadata = record.rec_headers.get_header('WARC-Json-Metadata')\n        if metadata:\n            entry['metadata'] = metadata\n\n        return entry", "response": "Parse a WARC record into a dictionary of keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an arc record and return a dictionary of the index entries.", "response": "def parse_arc_record(self, record):\n        \"\"\" Parse arc record\n        \"\"\"\n        url = record.rec_headers.get_header('uri')\n        url = url.replace('\\r', '%0D')\n        url = url.replace('\\n', '%0A')\n        # replace formfeed\n        url = url.replace('\\x0c', '%0C')\n        # replace nulls\n        url = url.replace('\\x00', '%00')\n\n        entry = self._create_index_entry(record.rec_type)\n        entry['url'] = url\n\n        # timestamp\n        entry['timestamp'] = record.rec_headers.get_header('archive-date')\n        if len(entry['timestamp']) > 14:\n            entry['timestamp'] = entry['timestamp'][:14]\n\n        if not self.options.get('minimal'):\n            # mime\n            entry.extract_mime(record.rec_headers.get_header('content-type'))\n\n            # status\n            entry.extract_status(record.http_headers)\n\n        # digest\n        entry['digest'] = '-'\n\n        return entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef field_type(field):\n    if hasattr(field, 'field') and field.field:\n        return field.field.__class__.__name__.lower()\n    return ''", "response": "Returns the type of the field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef widget_type(field):\n    if hasattr(field, 'field') and hasattr(field.field, 'widget') and field.field.widget:\n        return field.field.widget.__class__.__name__.lower()\n    return ''", "response": "Template filter that returns field widget class name in lower case."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render_field(parser, token):\n    error_msg = '%r tag requires a form field followed by a list of attributes and values in the form attr=\"value\"' % token.split_contents()[0]\n    try:\n        bits = token.split_contents()\n        tag_name = bits[0]\n        form_field = bits[1]\n        attr_list = bits[2:]\n    except ValueError:\n        raise TemplateSyntaxError(error_msg)\n\n    form_field = parser.compile_filter(form_field)\n\n    set_attrs = []\n    append_attrs = []\n    for pair in attr_list:\n        match = ATTRIBUTE_RE.match(pair)\n        if not match:\n            raise TemplateSyntaxError(error_msg + \": %s\" % pair)\n        dct = match.groupdict()\n        attr, sign, value = \\\n            dct['attr'], dct['sign'], parser.compile_filter(dct['value'])\n        if sign == \"=\":\n            set_attrs.append((attr, value))\n        else:\n            append_attrs.append((attr, value))\n\n    return FieldAttributeNode(form_field, set_attrs, append_attrs)", "response": "Render a form field using given attribute - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef response(cls, dic, status=200):\n        response = JsonResponse(dic, status=status)\n        response['Cache-Control'] = 'no-store'\n        response['Pragma'] = 'no-cache'\n\n        return response", "response": "Create and return a response object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary with all the requested claims about the End - User.", "response": "def userinfo(request, *args, **kwargs):\n    \"\"\"\n    Create a dictionary with all the requested claims about the End-User.\n    See: http://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse\n\n    Return a dictionary.\n    \"\"\"\n\n    def set_headers(response):\n        response['Cache-Control'] = 'no-store'\n        response['Pragma'] = 'no-cache'\n        cors_allow_any(request, response)\n        return response\n\n    if request.method == 'OPTIONS':\n        return set_headers(HttpResponse())\n\n    token = kwargs['token']\n\n    dic = {\n        'sub': token.id_token.get('sub'),\n    }\n\n    standard_claims = StandardScopeClaims(token)\n    dic.update(standard_claims.create_response_dic())\n\n    if settings.get('OIDC_EXTRA_SCOPE_CLAIMS'):\n        extra_claims = settings.get('OIDC_EXTRA_SCOPE_CLAIMS', import_str=True)(token)\n        dic.update(extra_claims.create_response_dic())\n\n    success_response = JsonResponse(dic, status=200)\n    set_headers(success_response)\n\n    return success_response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_response_dic(self):\n        dic = {}\n\n        for scope in self.scopes:\n            if scope in self._scopes_registered():\n                dic.update(getattr(self, 'scope_' + scope)())\n\n        dic = self._clean_dic(dic)\n\n        return dic", "response": "Generate the dic that will be jsonify. Checking scopes given vs\n        registered."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _scopes_registered(self):\n        scopes = []\n\n        for name in dir(self.__class__):\n            if name.startswith('scope_'):\n                scope = name.split('scope_')[1]\n                scopes.append(scope)\n\n        return scopes", "response": "Return a list of all the scopes registered in the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclean all empty or None values inside a dict.", "response": "def _clean_dic(self, dic):\n        \"\"\"\n        Clean recursively all empty or None values inside a dict.\n        \"\"\"\n        aux_dic = dic.copy()\n        for key, value in iter(dic.items()):\n\n            if value is None or value == '':\n                del aux_dic[key]\n            elif type(value) is dict:\n                cleaned_dict = self._clean_dic(value)\n                if not cleaned_dict:\n                    del aux_dic[key]\n                    continue\n                aux_dic[key] = cleaned_dict\n        return aux_dic"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract all the params used by the Authorization Code Flow.", "response": "def _extract_params(self):\n        \"\"\"\n        Get all the params used by the Authorization Code Flow\n        (and also for the Implicit and Hybrid).\n\n        See: http://openid.net/specs/openid-connect-core-1_0.html#AuthRequest\n        \"\"\"\n        # Because in this endpoint we handle both GET\n        # and POST request.\n        query_dict = (self.request.POST if self.request.method == 'POST'\n                      else self.request.GET)\n\n        self.params['client_id'] = query_dict.get('client_id', '')\n        self.params['redirect_uri'] = query_dict.get('redirect_uri', '')\n        self.params['response_type'] = query_dict.get('response_type', '')\n        self.params['scope'] = query_dict.get('scope', '').split()\n        self.params['state'] = query_dict.get('state', '')\n        self.params['nonce'] = query_dict.get('nonce', '')\n\n        self.params['prompt'] = self._allowed_prompt_params.intersection(\n            set(query_dict.get('prompt', '').split()))\n\n        self.params['code_challenge'] = query_dict.get('code_challenge', '')\n        self.params['code_challenge_method'] = query_dict.get('code_challenge_method', '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the user consent given to a specific client. Return None.", "response": "def set_client_user_consent(self):\n        \"\"\"\n        Save the user consent given to a specific client.\n\n        Return None.\n        \"\"\"\n        date_given = timezone.now()\n        expires_at = date_given + timedelta(\n            days=settings.get('OIDC_SKIP_CONSENT_EXPIRE'))\n\n        uc, created = UserConsent.objects.get_or_create(\n            user=self.request.user,\n            client=self.client,\n            defaults={\n                'expires_at': expires_at,\n                'date_given': date_given,\n            }\n        )\n        uc.scope = self.params['scope']\n\n        # Rewrite expires_at and date_given if object already exists.\n        if not created:\n            uc.expires_at = expires_at\n            uc.date_given = date_given\n\n        uc.save()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if user has user consent for some client.", "response": "def client_has_user_consent(self):\n        \"\"\"\n        Check if already exists user consent for some client.\n\n        Return bool.\n        \"\"\"\n        value = False\n        try:\n            uc = UserConsent.objects.get(user=self.request.user, client=self.client)\n            if (set(self.params['scope']).issubset(uc.scope)) and not (uc.has_expired()):\n                value = True\n        except UserConsent.DoesNotExist:\n            pass\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list with the description of all the scopes requested.", "response": "def get_scopes_information(self):\n        \"\"\"\n        Return a list with the description of all the scopes requested.\n        \"\"\"\n        scopes = StandardScopeClaims.get_scopes_info(self.params['scope'])\n        if settings.get('OIDC_EXTRA_SCOPE_CLAIMS'):\n            scopes_extra = settings.get(\n                'OIDC_EXTRA_SCOPE_CLAIMS', import_str=True).get_scopes_info(self.params['scope'])\n            for index_extra, scope_extra in enumerate(scopes_extra):\n                for index, scope in enumerate(scopes[:]):\n                    if scope_extra['scope'] == scope['scope']:\n                        del scopes[index]\n        else:\n            scopes_extra = []\n\n        return scopes + scopes_extra"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nattempt to import a class from a string representation.", "response": "def import_from_str(value):\n    \"\"\"\n    Attempt to import a class from a string representation.\n    \"\"\"\n    try:\n        parts = value.split('.')\n        module_path, class_name = '.'.join(parts[:-1]), parts[-1]\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except ImportError as e:\n        msg = 'Could not import %s for settings. %s: %s.' % (value, e.__class__.__name__, e)\n        raise ImportError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(name, import_str=False):\n    value = None\n    default_value = getattr(default_settings, name)\n\n    try:\n        value = getattr(settings, name)\n    except AttributeError:\n        if name in default_settings.required_attrs:\n            raise Exception('You must set ' + name + ' in your settings.')\n\n    if isinstance(default_value, dict) and value:\n        default_value.update(value)\n        value = default_value\n    else:\n        if value is None:\n            value = default_value\n        value = import_from_str(value) if import_str else value\n\n    return value", "response": "Helper function to use inside the package."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef OIDC_UNAUTHENTICATED_SESSION_MANAGEMENT_KEY(self):\n\n        # Memoize generated value\n        if not self._unauthenticated_session_management_key:\n            self._unauthenticated_session_management_key = ''.join(\n                random.choice(string.ascii_uppercase + string.digits) for _ in range(100))\n        return self._unauthenticated_session_management_key", "response": "OPTIONAL. Supply a fixed string to use as browser - state key for unauthenticated clients."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strip_prompt_login(path):\n    uri = urlsplit(path)\n    query_params = parse_qs(uri.query)\n    prompt_list = query_params.get('prompt', '')[0].split()\n    if 'login' in prompt_list:\n        prompt_list.remove('login')\n        query_params['prompt'] = ' '.join(prompt_list)\n    if not query_params['prompt']:\n        del query_params['prompt']\n    uri = uri._replace(query=urlencode(query_params, doseq=True))\n    return urlunsplit(uri)", "response": "Strips login from the prompt query parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_access_token(request):\n    auth_header = request.META.get('HTTP_AUTHORIZATION', '')\n\n    if re.compile('^[Bb]earer\\s{1}.+$').match(auth_header):\n        access_token = auth_header.split()[1]\n    else:\n        access_token = request.GET.get('access_token', '')\n\n    return access_token", "response": "Extract the access token from the request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_client_auth(request):\n    auth_header = request.META.get('HTTP_AUTHORIZATION', '')\n\n    if re.compile('^Basic\\s{1}.+$').match(auth_header):\n        b64_user_pass = auth_header.split()[1]\n        try:\n            user_pass = b64decode(b64_user_pass).decode('utf-8').split(':')\n            client_id, client_secret = tuple(user_pass)\n        except Exception:\n            client_id = client_secret = ''\n    else:\n        client_id = request.POST.get('client_id', '')\n        client_secret = request.POST.get('client_secret', '')\n\n    return (client_id, client_secret)", "response": "Extract client credentials from the request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nview decorator. The client accesses protected resources by presenting the access token to the resource server. https://tools.ietf.org/html/rfc6749#section-7", "response": "def protected_resource_view(scopes=None):\n    \"\"\"\n    View decorator. The client accesses protected resources by presenting the\n    access token to the resource server.\n    https://tools.ietf.org/html/rfc6749#section-7\n    \"\"\"\n    if scopes is None:\n        scopes = []\n\n    def wrapper(view):\n        def view_wrapper(request,  *args, **kwargs):\n            access_token = extract_access_token(request)\n\n            try:\n                try:\n                    kwargs['token'] = Token.objects.get(access_token=access_token)\n                except Token.DoesNotExist:\n                    logger.debug('[UserInfo] Token does not exist: %s', access_token)\n                    raise BearerTokenError('invalid_token')\n\n                if kwargs['token'].has_expired():\n                    logger.debug('[UserInfo] Token has expired: %s', access_token)\n                    raise BearerTokenError('invalid_token')\n\n                if not set(scopes).issubset(set(kwargs['token'].scope)):\n                    logger.debug('[UserInfo] Missing openid scope.')\n                    raise BearerTokenError('insufficient_scope')\n            except BearerTokenError as error:\n                response = HttpResponse(status=error.status)\n                response['WWW-Authenticate'] = 'error=\"{0}\", error_description=\"{1}\"'.format(\n                    error.code, error.description)\n                return response\n\n            return view(request,  *args, **kwargs)\n\n        return view_wrapper\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_site_url(site_url=None, request=None):\n    site_url = site_url or settings.get('SITE_URL')\n    if site_url:\n        return site_url\n    elif request:\n        return '{}://{}'.format(request.scheme, request.get_host())\n    else:\n        raise Exception('Either pass `site_url`, '\n                        'or set `SITE_URL` in settings, '\n                        'or pass `request` object.')", "response": "Return the site url."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct the issuer full url. Basically is the site url with some path appended.", "response": "def get_issuer(site_url=None, request=None):\n    \"\"\"\n    Construct the issuer full url. Basically is the site url with some path\n    appended.\n    \"\"\"\n    site_url = get_site_url(site_url=site_url, request=request)\n    path = reverse('oidc_provider:provider-info') \\\n        .split('/.well-known/openid-configuration')[0]\n    issuer = site_url + path\n\n    return str(issuer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine value to use as session state.", "response": "def get_browser_state_or_default(request):\n    \"\"\"\n    Determine value to use as session state.\n    \"\"\"\n    key = (request.session.session_key or\n           settings.get('OIDC_UNAUTHENTICATED_SESSION_MANAGEMENT_KEY'))\n    return sha224(key.encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cors_allow_any(request, response):\n    origin = request.META.get('HTTP_ORIGIN')\n    if not origin:\n        return response\n\n    # From the CORS spec: The string \"*\" cannot be used for a resource that supports credentials.\n    response['Access-Control-Allow-Origin'] = origin\n    patch_vary_headers(response, ['Origin'])\n    response['Access-Control-Allow-Credentials'] = 'true'\n\n    if request.method == 'OPTIONS':\n        if 'HTTP_ACCESS_CONTROL_REQUEST_HEADERS' in request.META:\n            response['Access-Control-Allow-Headers'] \\\n                = request.META['HTTP_ACCESS_CONTROL_REQUEST_HEADERS']\n        response['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'\n\n    return response", "response": "Add headers to permit CORS requests from any origin with or without credentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the id_token dictionary.", "response": "def create_id_token(token, user, aud, nonce='', at_hash='', request=None, scope=None):\n    \"\"\"\n    Creates the id_token dictionary.\n    See: http://openid.net/specs/openid-connect-core-1_0.html#IDToken\n    Return a dic.\n    \"\"\"\n    if scope is None:\n        scope = []\n    sub = settings.get('OIDC_IDTOKEN_SUB_GENERATOR', import_str=True)(user=user)\n\n    expires_in = settings.get('OIDC_IDTOKEN_EXPIRE')\n\n    # Convert datetimes into timestamps.\n    now = int(time.time())\n    iat_time = now\n    exp_time = int(now + expires_in)\n    user_auth_time = user.last_login or user.date_joined\n    auth_time = int(dateformat.format(user_auth_time, 'U'))\n\n    dic = {\n        'iss': get_issuer(request=request),\n        'sub': sub,\n        'aud': str(aud),\n        'exp': exp_time,\n        'iat': iat_time,\n        'auth_time': auth_time,\n    }\n\n    if nonce:\n        dic['nonce'] = str(nonce)\n\n    if at_hash:\n        dic['at_hash'] = at_hash\n\n    # Inlude (or not) user standard claims in the id_token.\n    if settings.get('OIDC_IDTOKEN_INCLUDE_CLAIMS'):\n        if settings.get('OIDC_EXTRA_SCOPE_CLAIMS'):\n            custom_claims = settings.get('OIDC_EXTRA_SCOPE_CLAIMS', import_str=True)(token)\n            claims = custom_claims.create_response_dic()\n        else:\n            claims = StandardScopeClaims(token).create_response_dic()\n        dic.update(claims)\n\n    dic = run_processing_hook(\n        dic, 'OIDC_IDTOKEN_PROCESSING_HOOK',\n        user=user, token=token, request=request)\n\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_id_token(payload, client):\n    keys = get_client_alg_keys(client)\n    _jws = JWS(payload, alg=client.jwt_alg)\n    return _jws.sign_compact(keys)", "response": "Encode the ID Token as a JSON Web Token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_id_token(token, client):\n    keys = get_client_alg_keys(client)\n    return JWS().verify_compact(token, keys=keys)", "response": "Decodes an ID Token as a JSON Web Token."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef client_id_from_id_token(id_token):\n    payload = JWT().unpack(id_token).payload()\n    aud = payload.get('aud', None)\n    if aud is None:\n        return None\n    if isinstance(aud, list):\n        return aud[0]\n    return aud", "response": "Extracts the client id from a JSON Web Token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_token(user, client, scope, id_token_dic=None):\n    token = Token()\n    token.user = user\n    token.client = client\n    token.access_token = uuid.uuid4().hex\n\n    if id_token_dic is not None:\n        token.id_token = id_token_dic\n\n    token.refresh_token = uuid.uuid4().hex\n    token.expires_at = timezone.now() + timedelta(\n        seconds=settings.get('OIDC_TOKEN_EXPIRE'))\n    token.scope = scope\n\n    return token", "response": "Create and populate a Token object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and populate a Code object.", "response": "def create_code(user, client, scope, nonce, is_authentication,\n                code_challenge=None, code_challenge_method=None):\n    \"\"\"\n    Create and populate a Code object.\n    Return a Code object.\n    \"\"\"\n    code = Code()\n    code.user = user\n    code.client = client\n\n    code.code = uuid.uuid4().hex\n\n    if code_challenge and code_challenge_method:\n        code.code_challenge = code_challenge\n        code.code_challenge_method = code_challenge_method\n\n    code.expires_at = timezone.now() + timedelta(\n        seconds=settings.get('OIDC_CODE_EXPIRE'))\n    code.scope = scope\n    code.nonce = nonce\n    code.is_authentication = is_authentication\n\n    return code"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake a client and returns the set of keys associated with it.", "response": "def get_client_alg_keys(client):\n    \"\"\"\n    Takes a client and returns the set of keys associated with it.\n    Returns a list of keys.\n    \"\"\"\n    if client.jwt_alg == 'RS256':\n        keys = []\n        for rsakey in RSAKey.objects.all():\n            keys.append(jwk_RSAKey(key=importKey(rsakey.key), kid=rsakey.kid))\n        if not keys:\n            raise Exception('You must add at least one RSA Key.')\n    elif client.jwt_alg == 'HS256':\n        keys = [SYMKey(key=client.client_secret, alg=client.jwt_alg)]\n    else:\n        raise Exception('Unsupported key algorithm.')\n\n    return keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self, key, fops, force=None):\n        ops = ';'.join(fops)\n        data = {'bucket': self.bucket, 'key': key, 'fops': ops}\n        if self.pipeline:\n            data['pipeline'] = self.pipeline\n        if self.notify_url:\n            data['notifyURL'] = self.notify_url\n        if force == 1:\n            data['force'] = 1\n\n        url = '{0}/pfop'.format(config.get_default('default_api_host'))\n        return http._post_with_auth(url, data, self.auth)", "response": "Execute the PFOP command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting - List the items in a bucket", "response": "def list(self, bucket, prefix=None, marker=None, limit=None, delimiter=None):\n        \"\"\"\u524d\u7f00\u67e5\u8be2:\n\n        1. \u9996\u6b21\u8bf7\u6c42 marker = None\n        2. \u65e0\u8bba err \u503c\u5982\u4f55\uff0c\u5747\u5e94\u8be5\u5148\u770b ret.get('items') \u662f\u5426\u6709\u5185\u5bb9\n        3. \u5982\u679c\u540e\u7eed\u6ca1\u6709\u66f4\u591a\u6570\u636e\uff0cerr \u8fd4\u56de EOF\uff0cmarker \u8fd4\u56de None\uff08\u4f46\u4e0d\u901a\u8fc7\u8be5\u7279\u5f81\u6765\u5224\u65ad\u662f\u5426\u7ed3\u675f\uff09\n        \u5177\u4f53\u89c4\u683c\u53c2\u8003:\n        http://developer.qiniu.com/docs/v6/api/reference/rs/list.html\n\n        Args:\n            bucket:     \u7a7a\u95f4\u540d\n            prefix:     \u5217\u4e3e\u524d\u7f00\n            marker:     \u5217\u4e3e\u6807\u8bc6\u7b26\n            limit:      \u5355\u6b21\u5217\u4e3e\u4e2a\u6570\u9650\u5236\n            delimiter:  \u6307\u5b9a\u76ee\u5f55\u5206\u9694\u7b26\n\n        Returns:\n            \u4e00\u4e2adict\u53d8\u91cf\uff0c\u7c7b\u4f3c {\"hash\": \"<Hash string>\", \"key\": \"<Key string>\"}\n            \u4e00\u4e2aResponseInfo\u5bf9\u8c61\n            \u4e00\u4e2aEOF\u4fe1\u606f\u3002\n        \"\"\"\n        options = {\n            'bucket': bucket,\n        }\n        if marker is not None:\n            options['marker'] = marker\n        if limit is not None:\n            options['limit'] = limit\n        if prefix is not None:\n            options['prefix'] = prefix\n        if delimiter is not None:\n            options['delimiter'] = delimiter\n\n        url = '{0}/list'.format(config.get_default('default_rsf_host'))\n        ret, info = self.__get(url, options)\n\n        eof = False\n        if ret and not ret.get('marker'):\n            eof = True\n\n        return ret, eof, info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, bucket, key):\n        resource = entry(bucket, key)\n        return self.__rs_do('delete', resource)", "response": "Delete a single object from a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename(self, bucket, key, key_to, force='false'):\n        return self.move(bucket, key, bucket, key_to, force)", "response": "Rename a key in a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving an object from one bucket to another.", "response": "def move(self, bucket, key, bucket_to, key_to, force='false'):\n        \"\"\"\u79fb\u52a8\u6587\u4ef6:\n\n        \u5c06\u8d44\u6e90\u4ece\u4e00\u4e2a\u7a7a\u95f4\u5230\u53e6\u4e00\u4e2a\u7a7a\u95f4\uff0c\u5177\u4f53\u89c4\u683c\u53c2\u8003\uff1a\n        http://developer.qiniu.com/docs/v6/api/reference/rs/move.html\n\n        Args:\n            bucket:     \u5f85\u64cd\u4f5c\u8d44\u6e90\u6240\u5728\u7a7a\u95f4\n            bucket_to:  \u76ee\u6807\u8d44\u6e90\u7a7a\u95f4\u540d\n            key:        \u5f85\u64cd\u4f5c\u8d44\u6e90\u6587\u4ef6\u540d\n            key_to:     \u76ee\u6807\u8d44\u6e90\u6587\u4ef6\u540d\n\n        Returns:\n            \u4e00\u4e2adict\u53d8\u91cf\uff0c\u6210\u529f\u8fd4\u56deNULL\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            \u4e00\u4e2aResponseInfo\u5bf9\u8c61\n        \"\"\"\n        resource = entry(bucket, key)\n        to = entry(bucket_to, key_to)\n        return self.__rs_do('move', resource, to, 'force/{0}'.format(force))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch a single object from a URL.", "response": "def fetch(self, url, bucket, key=None):\n        \"\"\"\u6293\u53d6\u6587\u4ef6:\n        \u4ece\u6307\u5b9aURL\u6293\u53d6\u8d44\u6e90\uff0c\u5e76\u5c06\u8be5\u8d44\u6e90\u5b58\u50a8\u5230\u6307\u5b9a\u7a7a\u95f4\u4e2d\uff0c\u5177\u4f53\u89c4\u683c\u53c2\u8003\uff1a\n        http://developer.qiniu.com/docs/v6/api/reference/rs/fetch.html\n\n        Args:\n            url:    \u6307\u5b9a\u7684URL\n            bucket: \u76ee\u6807\u8d44\u6e90\u7a7a\u95f4\n            key:    \u76ee\u6807\u8d44\u6e90\u6587\u4ef6\u540d\n\n        Returns:\n            \u4e00\u4e2adict\u53d8\u91cf\uff0c\u6210\u529f\u8fd4\u56deNULL\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            \u4e00\u4e2aResponseInfo\u5bf9\u8c61\n        \"\"\"\n        resource = urlsafe_base64_encode(url)\n        to = entry(bucket, key)\n        return self.__io_do(bucket, 'fetch', resource, 'to/{0}'.format(to))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_mime(self, bucket, key, mime):\n        resource = entry(bucket, key)\n        encode_mime = urlsafe_base64_encode(mime)\n        return self.__rs_do('chgm', resource, 'mime/{0}'.format(encode_mime))", "response": "Change mime type of a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_type(self, bucket, key, storage_type):\n        resource = entry(bucket, key)\n        return self.__rs_do('chtype', resource, 'type/{0}'.format(storage_type))", "response": "Change the type of a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the status of a key in a bucket.", "response": "def change_status(self, bucket, key, status, cond):\n        \"\"\"\u4fee\u6539\u6587\u4ef6\u7684\u72b6\u6001\n\n        \u4fee\u6539\u6587\u4ef6\u7684\u5b58\u50a8\u7c7b\u578b\u4e3a\u53ef\u7528\u6216\u7981\u7528\uff1a\n\n        Args:\n            bucket:         \u5f85\u64cd\u4f5c\u8d44\u6e90\u6240\u5728\u7a7a\u95f4\n            key:            \u5f85\u64cd\u4f5c\u8d44\u6e90\u6587\u4ef6\u540d\n            storage_type:   \u5f85\u64cd\u4f5c\u8d44\u6e90\u5b58\u50a8\u7c7b\u578b\uff0c0\u4e3a\u542f\u7528\uff0c1\u4e3a\u7981\u7528\n        \"\"\"\n        resource = entry(bucket, key)\n        if cond and isinstance(cond, dict):\n            condstr = \"\"\n            for k, v in cond.items():\n                condstr += \"{0}={1}&\".format(k, v)\n            condstr = urlsafe_base64_encode(condstr[:-1])\n            return self.__rs_do('chstatus', resource, 'status/{0}'.format(status), 'cond', condstr)\n        return self.__rs_do('chstatus', resource, 'status/{0}'.format(status))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_after_days(self, bucket, key, days):\n        resource = entry(bucket, key)\n        return self.__rs_do('deleteAfterDays', resource, days)", "response": "Delete a resource after a number of days."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new object in the specified region.", "response": "def mkbucketv2(self, bucket_name, region):\n        \"\"\"\n        \u521b\u5efa\u5b58\u50a8\u7a7a\u95f4\n        https://developer.qiniu.com/kodo/api/1382/mkbucketv2\n\n        Args:\n            bucket_name: \u5b58\u50a8\u7a7a\u95f4\u540d\n            region: \u5b58\u50a8\u533a\u57df\n        \"\"\"\n        bucket_name = urlsafe_base64_encode(bucket_name)\n        return self.__rs_do('mkbucketv2', bucket_name, 'region', region)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all keys for an app.", "response": "def get_app_keys(self, app_uri):\n        \"\"\"\u83b7\u5f97\u8d26\u53f7\u4e0b\u5e94\u7528\u7684\u5bc6\u94a5\n\n        \u5217\u51fa\u6307\u5b9a\u5e94\u7528\u7684\u5bc6\u94a5\uff0c\u4ec5\u5f53\u8bbf\u95ee\u8005\u5bf9\u6307\u5b9a\u5e94\u7528\u6709\u7ba1\u7406\u6743\u9650\u65f6\u6709\u6548\uff1a\n            \u7528\u6237\u5bf9\u521b\u5efa\u7684\u5e94\u7528\u6709\u7ba1\u7406\u6743\u9650\u3002\n            \u7528\u6237\u5bf9\u4f7f\u7528\u7684\u7b2c\u4e09\u65b9\u5e94\u7528\u6ca1\u6709\u7ba1\u7406\u6743\u9650\uff0c\u7b2c\u4e09\u65b9\u5e94\u7528\u7684\u8fd0\u7ef4\u65b9\u6709\u7ba1\u7406\u6743\u9650\u3002\n\n        Args:\n            - app_uri: \u5e94\u7528\u7684\u5b8c\u6574\u6807\u8bc6\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u79d8\u94a5\u5217\u8868\uff0c\u5931\u8d25\u8fd4\u56deNone\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n\n        url = '{0}/v3/apps/{1}/keys'.format(self.host, app_uri)\n        return http._get_with_qiniu_mac(url, None, self.auth)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a tuple of QiniuMacAuth and ResponseInfo if the app_uri is valid.", "response": "def get_valid_app_auth(self, app_uri):\n        \"\"\"\u83b7\u5f97\u8d26\u53f7\u4e0b\u53ef\u7528\u7684\u5e94\u7528\u7684\u5bc6\u94a5\n\n        \u5217\u51fa\u6307\u5b9a\u5e94\u7528\u7684\u53ef\u7528\u5bc6\u94a5\n\n        Args:\n            - app_uri: \u5e94\u7528\u7684\u5b8c\u6574\u6807\u8bc6\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u53ef\u7528\u79d8\u94a5\u5217\u8868\uff0c\u5931\u8d25\u8fd4\u56deNone\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n\n        ret, retInfo = self.get_app_keys(app_uri)\n\n        if ret is None:\n            return None\n\n        for k in ret:\n            if (k.get('state') == 'enabled'):\n                return QiniuMacAuth(k.get('ak'), k.get('sk'))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget information about the current user.", "response": "def get_account_info(self):\n        \"\"\"\u83b7\u5f97\u5f53\u524d\u8d26\u53f7\u7684\u4fe1\u606f\n\n        \u67e5\u770b\u5f53\u524d\u8bf7\u6c42\u65b9\uff08\u8bf7\u6c42\u9274\u6743\u4f7f\u7528\u7684 AccessKey \u7684\u5c5e\u4e3b\uff09\u7684\u8d26\u53f7\u4fe1\u606f\u3002\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7528\u6237\u4fe1\u606f\uff0c\u5931\u8d25\u8fd4\u56deNone\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n\n        url = '{0}/v3/info'.format(self.host)\n        return http._get_with_qiniu_mac(url, None, self.auth)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_app_region_products(self, app_uri):\n        apps, retInfo = self.list_apps()\n        if apps is None:\n            return None\n\n        for app in apps:\n            if (app.get('uri') == app_uri):\n                return self.get_region_products(app.get('region'))\n\n        return", "response": "Get all the app product"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of products in a region", "response": "def get_region_products(self, region):\n        \"\"\"\u83b7\u5f97\u6307\u5b9a\u533a\u57df\u7684\u4ea7\u54c1\u4fe1\u606f\n\n        Args:\n            - region: \u533a\u57df\uff0c\u5982\uff1a\"nq\"\n\n        Returns:\n            \u8fd4\u56de\u8be5\u533a\u57df\u7684\u4ea7\u54c1\u4fe1\u606f\uff0c\u82e5\u5931\u8d25\u5219\u8fd4\u56deNone\n        \"\"\"\n\n        regions, retInfo = self.list_regions()\n        if regions is None:\n            return None\n\n        for r in regions:\n            if r.get('name') == region:\n                return r.get('products')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_regions(self):\n\n        url = '{0}/v3/regions'.format(self.host)\n        return http._get_with_qiniu_mac(url, None, self.auth)", "response": "\u83b7\u5f97\u8d26\u53f7\u53ef\u89c1\u7684\u533a\u57df\u7684\u4fe1\u606f\n\n        \u5217\u51fa\u5f53\u524d\u7528\u6237\u6240\u6709\u53ef\u4f7f\u7528\u7684\u533a\u57df\u3002\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u533a\u57df\u5217\u8868\uff0c\u5931\u8d25\u8fd4\u56deNone\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_app(self, args):\n\n        url = '{0}/v3/apps'.format(self.host)\n        return http._post_with_qiniu_mac(url, args, self.auth)", "response": "\u521b\u5efa\u5e94\u7528\n\n        \u5728\u6307\u5b9a\u533a\u57df\u521b\u5efa\u4e00\u4e2a\u65b0\u5e94\u7528\uff0c\u6240\u5c5e\u5e94\u7528\u4e3a\u5f53\u524d\u8bf7\u6c42\u65b9\u3002\n\n        Args:\n            - args: \u8bf7\u6c42\u53c2\u6570(json)\uff0c\u53c2\u8003 http://kirk-docs.qiniu.com/apidocs/\n\n        Returns:\n            - result        \u6210\u529f\u8fd4\u56de\u6240\u521b\u5efa\u7684\u5e94\u7528\u4fe1\u606f\uff0c\u82e5\u5931\u8d25\u5219\u8fd4\u56deNone\n            - ResponseInfo  \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef token_of_request(self, url, body=None, content_type=None):\n        parsed_url = urlparse(url)\n        query = parsed_url.query\n        path = parsed_url.path\n        data = path\n        if query != '':\n            data = ''.join([data, '?', query])\n        data = ''.join([data, \"\\n\"])\n\n        if body:\n            mimes = [\n                'application/x-www-form-urlencoded'\n            ]\n            if content_type in mimes:\n                data += body\n\n        return '{0}:{1}'.format(self.__access_key, self.__token(data))", "response": "Returns the token of the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_callback(\n            self,\n            origin_authorization,\n            url,\n            body,\n            content_type='application/x-www-form-urlencoded'):\n        \"\"\"\u56de\u8c03\u9a8c\u8bc1\n\n        Args:\n            origin_authorization: \u56de\u8c03\u65f6\u8bf7\u6c42Header\u4e2d\u7684Authorization\u5b57\u6bb5\n            url:                  \u56de\u8c03\u8bf7\u6c42\u7684url\n            body:                 \u56de\u8c03\u8bf7\u6c42\u7684body\n            content_type:         \u56de\u8c03\u8bf7\u6c42body\u7684Content-Type\n\n        Returns:\n            \u8fd4\u56detrue\u8868\u793a\u9a8c\u8bc1\u6210\u529f\uff0c\u8fd4\u56defalse\u8868\u793a\u9a8c\u8bc1\u5931\u8d25\n        \"\"\"\n        token = self.token_of_request(url, body, content_type)\n        authorization = 'QBox {0}'.format(token)\n        return origin_authorization == authorization", "response": "Verify that a callback is valid for a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a token that can be used to access a specific object.", "response": "def token_of_request(\n            self,\n            method,\n            host,\n            url,\n            qheaders,\n            content_type=None,\n            body=None):\n        \"\"\"\n        <Method> <PathWithRawQuery>\n        Host: <Host>\n        Content-Type: <ContentType>\n        [<X-Qiniu-*> Headers]\n\n        [<Body>] #\u8fd9\u91cc\u7684 <Body> \u53ea\u6709\u5728 <ContentType> \u5b58\u5728\u4e14\u4e0d\u4e3a application/octet-stream \u65f6\u624d\u7b7e\u8fdb\u53bb\u3002\n\n        \"\"\"\n        parsed_url = urlparse(url)\n        netloc = parsed_url.netloc\n        path = parsed_url.path\n        query = parsed_url.query\n\n        if not host:\n            host = netloc\n\n        path_with_query = path\n        if query != '':\n            path_with_query = ''.join([path_with_query, '?', query])\n        data = ''.join([\"%s %s\" %\n                        (method, path_with_query), \"\\n\", \"Host: %s\" %\n                        host, \"\\n\"])\n\n        if content_type:\n            data += \"Content-Type: %s\" % (content_type) + \"\\n\"\n\n        data += qheaders\n        data += \"\\n\"\n\n        if content_type and content_type != \"application/octet-stream\" and body:\n            data += body.decode(encoding='UTF-8')\n\n        return '{0}:{1}'.format(self.__access_key, self.__token(data))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_stack(self, args):\n        url = '{0}/v3/stacks'.format(self.host)\n        return self.__post(url, args)", "response": "Create a new stack"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stack(self, stack):\n        url = '{0}/v3/stacks/{1}'.format(self.host, stack)\n        return self.__get(url)", "response": "Get a single stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_services(self, stack):\n        url = '{0}/v3/stacks/{1}/services'.format(self.host, stack)\n        return self.__get(url)", "response": "List all services in a stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_service(self, stack, args):\n        url = '{0}/v3/stacks/{1}/services'.format(self.host, stack)\n        return self.__post(url, args)", "response": "Create a new service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a service from a stack.", "response": "def delete_service(self, stack, service):\n        \"\"\"\u5220\u9664\u670d\u52a1\n\n        \u5220\u9664\u6307\u5b9a\u540d\u79f0\u670d\u52a1\uff0c\u5e76\u81ea\u52a8\u9500\u6bc1\u670d\u52a1\u5df2\u90e8\u7f72\u7684\u6240\u6709\u5bb9\u5668\u548c\u5b58\u50a8\u5377\u3002\n\n        Args:\n            - stack:    \u670d\u52a1\u6240\u5c5e\u7684\u670d\u52a1\u7ec4\u540d\u79f0\n            - service:  \u670d\u52a1\u540d\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/v3/stacks/{1}/services/{2}'.format(self.host, stack, service)\n        return self.__delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_service_inspect(self, stack, service):\n        url = '{0}/v3/stacks/{1}/services/{2}/inspect'.format(self.host, stack, service)\n        return self.__get(url)", "response": "Get the details of a service in a stack."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_service(self, stack, service):\n        url = '{0}/v3/stacks/{1}/services/{2}/start'.format(self.host, stack, service)\n        return self.__post(url)", "response": "Start a service in a stack."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop a service in a stack.", "response": "def stop_service(self, stack, service):\n        \"\"\"\u505c\u6b62\u670d\u52a1\n\n        \u505c\u6b62\u6307\u5b9a\u540d\u79f0\u670d\u52a1\u7684\u6240\u6709\u5bb9\u5668\u3002\n\n        Args:\n            - stack:    \u670d\u52a1\u6240\u5c5e\u7684\u670d\u52a1\u7ec4\u540d\u79f0\n            - service:  \u670d\u52a1\u540d\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/v3/stacks/{1}/services/{2}/stop'.format(self.host, stack, service)\n        return self.__post(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_service(self, stack, service, args):\n        url = '{0}/v3/stacks/{1}/services/{2}'.format(self.host, stack, service)\n        return self.__post(url, args)", "response": "Update a service in a stack."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextend a service volume", "response": "def extend_service_volume(self, stack, service, volume, args):\n        \"\"\"\u6269\u5bb9\u5b58\u50a8\u5377\n\n        \u4e3a\u6307\u5b9a\u540d\u79f0\u7684\u670d\u52a1\u589e\u52a0\u5b58\u50a8\u5377\u8d44\u6e90\uff0c\u5e76\u6302\u8f7d\u5230\u90e8\u7f72\u7684\u5bb9\u5668\u4e2d\u3002\n\n        Args:\n            - stack:    \u670d\u52a1\u6240\u5c5e\u7684\u670d\u52a1\u7ec4\u540d\u79f0\n            - service:  \u670d\u52a1\u540d\n            - volume:   \u5b58\u50a8\u5377\u540d\n            - args:     \u8bf7\u6c42\u53c2\u6570(json)\uff0c\u53c2\u8003 http://kirk-docs.qiniu.com/apidocs/\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/v3/stacks/{1}/services/{2}/volumes/{3}/extend'.format(self.host, stack, service, volume)\n        return self.__post(url, args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a service volume", "response": "def delete_service_volume(self, stack, service, volume):\n        \"\"\"\u5220\u9664\u5b58\u50a8\u5377\n\n        \u4ece\u90e8\u7f72\u7684\u5bb9\u5668\u4e2d\u79fb\u9664\u6302\u8f7d\uff0c\u5e76\u9500\u6bc1\u6307\u5b9a\u670d\u52a1\u4e0b\u6307\u5b9a\u540d\u79f0\u7684\u5b58\u50a8\u5377, \u5e76\u91cd\u65b0\u542f\u52a8\u8be5\u5bb9\u5668\u3002\n\n        Args:\n            - stack:    \u670d\u52a1\u6240\u5c5e\u7684\u670d\u52a1\u7ec4\u540d\u79f0\n            - service:  \u670d\u52a1\u540d\n            - volume:   \u5b58\u50a8\u5377\u540d\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/v3/stacks/{1}/services/{2}/volumes/{3}'.format(self.host, stack, service, volume)\n        return self.__delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_containers(self, stack=None, service=None):\n        url = '{0}/v3/containers'.format(self.host)\n        params = {}\n        if stack is not None:\n            params['stack'] = stack\n        if service is not None:\n            params['service'] = service\n        return self.__get(url, params or None)", "response": "\u5217\u51fa\u5bb9\u5668\u5217\u8868\n\n        \u5217\u51fa\u5e94\u7528\u5185\u6240\u6709\u90e8\u7f72\u7684\u5bb9\u5668, \u8fd4\u56de\u4e00\u7ec4\u5bb9\u5668IP\u3002\n\n        Args:\n            - stack:    \u8981\u5217\u51fa\u5bb9\u5668\u7684\u670d\u52a1\u7ec4\u540d(\u53ef\u4e0d\u586b\uff0c\u8868\u793a\u9ed8\u8ba4\u5217\u51fa\u6240\u6709)\n            - service:  \u8981\u5217\u51fa\u5bb9\u5668\u670d\u52a1\u7684\u670d\u52a1\u540d(\u53ef\u4e0d\u586b\uff0c\u8868\u793a\u9ed8\u8ba4\u5217\u51fa\u6240\u6709)\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u5bb9\u5668\u7684ip\u6570\u7ec4\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a new container.", "response": "def start_container(self, ip):\n        \"\"\"\u542f\u52a8\u5bb9\u5668\n\n        \u542f\u52a8\u6307\u5b9aIP\u7684\u5bb9\u5668\u3002\n\n        Args:\n            - ip:   \u5bb9\u5668ip\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/v3/containers/{1}/start'.format(self.host, ip)\n        return self.__post(url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_ap(self, args):\n        url = '{0}/v3/aps'.format(self.host)\n        return self.__post(url, args)", "response": "\u7533\u8bf7\u63a5\u5165\u70b9\n\n        \u7533\u8bf7\u6307\u5b9a\u914d\u7f6e\u7684\u63a5\u5165\u70b9\u8d44\u6e90\u3002\n\n        Args:\n            - args:   \u8bf7\u6c42\u53c2\u6570(json)\uff0c\u53c2\u8003 http://kirk-docs.qiniu.com/apidocs/\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7533\u8bf7\u5230\u7684\u63a5\u5165\u70b9\u4fe1\u606f\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search_ap(self, mode, query):\n        url = '{0}/v3/aps/search?{1}={2}'.format(self.host, mode, query)\n        return self.__get(url)", "response": "\u641c\u7d22\u63a5\u5165\u70b9\n\n        \u67e5\u770b\u6307\u5b9a\u63a5\u5165\u70b9\u7684\u6240\u6709\u914d\u7f6e\u4fe1\u606f\uff0c\u5305\u62ec\u6240\u6709\u76d1\u542c\u7aef\u53e3\u7684\u914d\u7f6e\u3002\n\n        Args:\n            - mode:     \u641c\u7d22\u6a21\u5f0f\uff0c\u53ef\u4ee5\u662fdomain\u3001ip\u3001host\n            - query:    \u641c\u7d22\u6587\u672c\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u641c\u7d22\u7ed3\u679c\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a single APT.", "response": "def get_ap(self, apid):\n        \"\"\"\u67e5\u770b\u63a5\u5165\u70b9\n\n        \u7ed9\u51fa\u63a5\u5165\u70b9\u7684\u57df\u540d\u6216IP\uff0c\u67e5\u770b\u914d\u7f6e\u4fe1\u606f\uff0c\u5305\u62ec\u6240\u6709\u76d1\u542c\u7aef\u53e3\u7684\u914d\u7f6e\u3002\n\n        Args:\n            - apid:   \u63a5\u5165\u70b9ID\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u63a5\u5165\u70b9\u4fe1\u606f\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/v3/aps/{1}'.format(self.host, apid)\n        return self.__get(url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_ap_port(self, apid, port, args):\n        url = '{0}/v3/aps/{1}/{2}'.format(self.host, apid, port)\n        return self.__post(url, args)", "response": "Set the port of an app."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_ap(self, apid):\n        url = '{0}/v3/aps/{1}'.format(self.host, apid)\n        return self.__delete(url)", "response": "\u91ca\u653e\u63a5\u5165\u70b9\n\n        \u9500\u6bc1\u6307\u5b9a\u63a5\u5165\u70b9\u8d44\u6e90\u3002\n\n        Args:\n            - apid: \u63a5\u5165\u70b9ID\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpublish_ap(self, apid, args):\n        url = '{0}/v3/aps/{1}/unpublish'.format(self.host, apid)\n        return self.__post(url, args)", "response": "\u89e3\u7ed1\u81ea\u5b9a\u4e49\u57df\u540d\n\n        \u89e3\u7ed1\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u57df\u540d\uff0c\u4ec5\u5bf9\u516c\u7f51\u57df\u540d\u6a21\u5f0f\u63a5\u5165\u70b9\u751f\u6548\u3002\n\n        Args:\n            - apid: \u63a5\u5165\u70b9ID\n            - args: \u8bf7\u6c42\u53c2\u6570(json)\uff0c\u53c2\u8003 http://kirk-docs.qiniu.com/apidocs/\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u7a7adict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ap_port_healthcheck(self, apid, port):\n        url = '{0}/v3/aps/{1}/{2}/healthcheck'.format(self.host, apid, port)\n        return self.__get(url)", "response": "Get the healthcheck for a given port."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disable_ap_port(self, apid, port):\n        url = '{0}/v3/aps/{1}/{2}/disable'.format(self.host, apid, port)\n        return self.__post(url)", "response": "Disable an AP port"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_ap_port(self, apid, port):\n        url = '{0}/v3/aps/{1}/{2}/enable'.format(self.host, apid, port)\n        return self.__post(url)", "response": "Enable an AP port."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_web_proxy(self, backend):\n        url = '{0}/v3/webproxy'.format(self.host)\n        return self.__post(url, {'backend': backend})", "response": "\u83b7\u53d6\u4e00\u6b21\u6027\u4ee3\u7406\u5730\u5740\n\n        \u5bf9\u5185\u7f51\u5730\u5740\u83b7\u53d6\u4e00\u4e2a\u4e00\u6b21\u6027\u7684\u5916\u90e8\u53ef\u8bbf\u95ee\u7684\u4ee3\u7406\u5730\u5740\n\n        Args:\n            - backend: \u540e\u7aef\u5730\u5740\uff0c\u5982\uff1a\"10.128.0.1:8080\"\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u4ee3\u7406\u5730\u5740\u4fe1\u606f\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nputs data to a key in a resource.", "response": "def put_data(\n        up_token, key, data, params=None, mime_type='application/octet-stream', check_crc=False, progress_handler=None,\n        fname=None):\n    \"\"\"\u4e0a\u4f20\u4e8c\u8fdb\u5236\u6d41\u5230\u4e03\u725b\n\n    Args:\n        up_token:         \u4e0a\u4f20\u51ed\u8bc1\n        key:              \u4e0a\u4f20\u6587\u4ef6\u540d\n        data:             \u4e0a\u4f20\u4e8c\u8fdb\u5236\u6d41\n        params:           \u81ea\u5b9a\u4e49\u53d8\u91cf\uff0c\u89c4\u683c\u53c2\u8003 http://developer.qiniu.com/docs/v6/api/overview/up/response/vars.html#xvar\n        mime_type:        \u4e0a\u4f20\u6570\u636e\u7684mimeType\n        check_crc:        \u662f\u5426\u6821\u9a8ccrc32\n        progress_handler: \u4e0a\u4f20\u8fdb\u5ea6\n\n    Returns:\n        \u4e00\u4e2adict\u53d8\u91cf\uff0c\u7c7b\u4f3c {\"hash\": \"<Hash string>\", \"key\": \"<Key string>\"}\n        \u4e00\u4e2aResponseInfo\u5bf9\u8c61\n    \"\"\"\n    final_data = ''\n    if hasattr(data, 'read'):\n        while True:\n            tmp_data = data.read(config._BLOCK_SIZE)\n            if len(tmp_data) == 0:\n                break\n            else:\n                final_data += tmp_data\n    else:\n        final_data = data\n\n    crc = crc32(final_data)\n    return _form_put(up_token, key, final_data, params, mime_type, crc, progress_handler, fname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put_file(up_token, key, file_path, params=None,\n             mime_type='application/octet-stream', check_crc=False,\n             progress_handler=None, upload_progress_recorder=None, keep_last_modified=False):\n    \"\"\"\u4e0a\u4f20\u6587\u4ef6\u5230\u4e03\u725b\n\n    Args:\n        up_token:         \u4e0a\u4f20\u51ed\u8bc1\n        key:              \u4e0a\u4f20\u6587\u4ef6\u540d\n        file_path:        \u4e0a\u4f20\u6587\u4ef6\u7684\u8def\u5f84\n        params:           \u81ea\u5b9a\u4e49\u53d8\u91cf\uff0c\u89c4\u683c\u53c2\u8003 http://developer.qiniu.com/docs/v6/api/overview/up/response/vars.html#xvar\n        mime_type:        \u4e0a\u4f20\u6570\u636e\u7684mimeType\n        check_crc:        \u662f\u5426\u6821\u9a8ccrc32\n        progress_handler: \u4e0a\u4f20\u8fdb\u5ea6\n        upload_progress_recorder: \u8bb0\u5f55\u4e0a\u4f20\u8fdb\u5ea6\uff0c\u7528\u4e8e\u65ad\u70b9\u7eed\u4f20\n\n    Returns:\n        \u4e00\u4e2adict\u53d8\u91cf\uff0c\u7c7b\u4f3c {\"hash\": \"<Hash string>\", \"key\": \"<Key string>\"}\n        \u4e00\u4e2aResponseInfo\u5bf9\u8c61\n    \"\"\"\n    ret = {}\n    size = os.stat(file_path).st_size\n    # fname = os.path.basename(file_path)\n    with open(file_path, 'rb') as input_stream:\n        file_name = os.path.basename(file_path)\n        modify_time = int(os.path.getmtime(file_path))\n        if size > config._BLOCK_SIZE * 2:\n            ret, info = put_stream(up_token, key, input_stream, file_name, size, params,\n                                   mime_type, progress_handler,\n                                   upload_progress_recorder=upload_progress_recorder,\n                                   modify_time=modify_time, keep_last_modified=keep_last_modified)\n        else:\n            crc = file_crc32(file_path)\n            ret, info = _form_put(up_token, key, input_stream, params, mime_type,\n                                  crc, progress_handler, file_name,\n                                  modify_time=modify_time, keep_last_modified=keep_last_modified)\n    return ret, info", "response": "Upload a file to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload(self):\n        self.blockStatus = []\n        if config.get_default('default_zone').up_host:\n            host = config.get_default('default_zone').up_host\n        else:\n            host = config.get_default('default_zone').get_up_host_by_token(self.up_token)\n        offset = self.recovery_from_record()\n        for block in _file_iter(self.input_stream, config._BLOCK_SIZE, offset):\n            length = len(block)\n            crc = crc32(block)\n            ret, info = self.make_block(block, length, host)\n            if ret is None and not info.need_retry():\n                return ret, info\n            if info.connect_failed():\n                if config.get_default('default_zone').up_host_backup:\n                    host = config.get_default('default_zone').up_host_backup\n                else:\n                    host = config.get_default('default_zone').get_up_host_backup_by_token(self.up_token)\n            if info.need_retry() or crc != ret['crc32']:\n                ret, info = self.make_block(block, length, host)\n                if ret is None or crc != ret['crc32']:\n                    return ret, info\n            self.blockStatus.append(ret)\n            offset += length\n            self.record_upload_progress(offset)\n            if (callable(self.progress_handler)):\n                self.progress_handler(((len(self.blockStatus) - 1) * config._BLOCK_SIZE) + length, self.size)\n        return self.make_file(host)", "response": "Upload a file to the local host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_block(self, block, block_size, host):\n        url = self.block_url(host, block_size)\n        return self.post(url, block)", "response": "Create a block of data from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_file(self, host):\n        url = self.file_url(host)\n        body = ','.join([status['ctx'] for status in self.blockStatus])\n        self.upload_progress_recorder.delete_upload_record(self.file_name, self.key)\n        return self.post(url, body)", "response": "Make file on host"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_crc32(filePath):\n    crc = 0\n    with open(filePath, 'rb') as f:\n        for block in _file_iter(f, _BLOCK_SIZE):\n            crc = binascii.crc32(block, crc) & 0xFFFFFFFF\n    return crc", "response": "Returns the CRC32 of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _file_iter(input_stream, size, offset=0):\n    input_stream.seek(offset)\n    d = input_stream.read(size)\n    while d:\n        yield d\n        d = input_stream.read(size)", "response": "Iterator that returns a file in order of size bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the ETag of the file in input_stream.", "response": "def etag_stream(input_stream):\n    \"\"\"\u8ba1\u7b97\u8f93\u5165\u6d41\u7684etag:\n\n    etag\u89c4\u683c\u53c2\u8003 https://developer.qiniu.com/kodo/manual/1231/appendix#3\n\n    Args:\n        input_stream: \u5f85\u8ba1\u7b97etag\u7684\u4e8c\u8fdb\u5236\u6d41\n\n    Returns:\n        \u8f93\u5165\u6d41\u7684etag\u503c\n    \"\"\"\n    array = [_sha1(block) for block in _file_iter(input_stream, _BLOCK_SIZE)]\n    if len(array) == 0:\n        array = [_sha1(b'')]\n    if len(array) == 1:\n        data = array[0]\n        prefix = b'\\x16'\n    else:\n        sha1_str = b('').join(array)\n        data = _sha1(sha1_str)\n        prefix = b'\\x96'\n    return urlsafe_base64_encode(prefix + data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the base64 - encoded entry of a key in a bucket.", "response": "def entry(bucket, key):\n    \"\"\"\u8ba1\u7b97\u4e03\u725bAPI\u4e2d\u7684\u6570\u636e\u683c\u5f0f:\n\n    entry\u89c4\u683c\u53c2\u8003 https://developer.qiniu.com/kodo/api/1276/data-format\n\n    Args:\n        bucket: \u5f85\u64cd\u4f5c\u7684\u7a7a\u95f4\u540d\n        key:    \u5f85\u64cd\u4f5c\u7684\u6587\u4ef6\u540d\n\n    Returns:\n        \u7b26\u5408\u4e03\u725bAPI\u89c4\u683c\u7684\u6570\u636e\u683c\u5f0f\n    \"\"\"\n    if key is None:\n        return urlsafe_base64_encode('{0}'.format(bucket))\n    else:\n        return urlsafe_base64_encode('{0}:{1}'.format(bucket, key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a RFC\u683c\u5f0f from a timestamp", "response": "def rfc_from_timestamp(timestamp):\n    \"\"\"\u5c06\u65f6\u95f4\u6233\u8f6c\u6362\u4e3aHTTP RFC\u683c\u5f0f\n\n    Args:\n        timestamp: \u6574\u578bUnix\u65f6\u95f4\u6233\uff08\u5355\u4f4d\u79d2\uff09\n    \"\"\"\n    last_modified_date = datetime.utcfromtimestamp(timestamp)\n    last_modified_str = last_modified_date.strftime(\n        '%a, %d %b %Y %H:%M:%S GMT')\n    return last_modified_str"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_timestamp_anti_leech_url(host, file_name, query_string, encrypt_key, deadline):\n    if query_string:\n        url_to_sign = '{0}/{1}?{2}'.format(host, urlencode(file_name), query_string)\n    else:\n        url_to_sign = '{0}/{1}'.format(host, urlencode(file_name))\n\n    path = '/{0}'.format(urlencode(file_name))\n    expire_hex = str(hex(deadline))[2:]\n    str_to_sign = '{0}{1}{2}'.format(encrypt_key, path, expire_hex).encode()\n    sign_str = hashlib.md5(str_to_sign).hexdigest()\n\n    if query_string:\n        signed_url = '{0}&sign={1}&t={2}'.format(url_to_sign, sign_str, expire_hex)\n    else:\n        signed_url = '{0}?sign={1}&t={2}'.format(url_to_sign, sign_str, expire_hex)\n\n    return signed_url", "response": "Create an anti - leech url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refresh_urls_and_dirs(self, urls, dirs):\n        req = {}\n        if urls is not None and len(urls) > 0:\n            req.update({\"urls\": urls})\n        if dirs is not None and len(dirs) > 0:\n            req.update({\"dirs\": dirs})\n\n        body = json.dumps(req)\n        url = '{0}/v2/tune/refresh'.format(self.server)\n        return self.__post(url, body)", "response": "Refreshes the url and directories of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prefetch_urls(self, urls):\n        req = {}\n        req.update({\"urls\": urls})\n\n        body = json.dumps(req)\n        url = '{0}/v2/tune/prefetch'.format(self.server)\n        return self.__post(url, body)", "response": "This method is used to prefetch a set of urls."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bandwidth_data(self, domains, start_date, end_date, granularity):\n        req = {}\n        req.update({\"domains\": ';'.join(domains)})\n        req.update({\"startDate\": start_date})\n        req.update({\"endDate\": end_date})\n        req.update({\"granularity\": granularity})\n\n        body = json.dumps(req)\n        url = '{0}/v2/tune/bandwidth'.format(self.server)\n        return self.__post(url, body)", "response": "Get the bandwidth data for a set of domains."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_log_list_data(self, domains, log_date):\n        req = {}\n        req.update({\"domains\": ';'.join(domains)})\n        req.update({\"day\": log_date})\n\n        body = json.dumps(req)\n        url = '{0}/v2/tune/log/list'.format(self.server)\n        return self.__post(url, body)", "response": "Get log list data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new domain.", "response": "def create_domain(self, name, body):\n        \"\"\"\n        \u521b\u5efa\u57df\u540d\uff0c\u6587\u6863 https://developer.qiniu.com/fusion/api/4246/the-domain-name\n\n        Args:\n           name:     \u57df\u540d, \u5982\u679c\u662f\u6cdb\u57df\u540d\uff0c\u5fc5\u987b\u4ee5\u70b9\u53f7 . \u5f00\u5934\n           bosy:     \u521b\u5efa\u57df\u540d\u53c2\u6570\n        Returns:\n           {}\n        \"\"\"\n        url = '{0}/domain/{1}'.format(self.server, name)\n        return self.__post(url, body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a domain from the server.", "response": "def get_domain(self, name):\n        \"\"\"\n        \u83b7\u53d6\u57df\u540d\u4fe1\u606f\uff0c\u6587\u6863 https://developer.qiniu.com/fusion/api/4246/the-domain-name\n\n        Args:\n           name:     \u57df\u540d, \u5982\u679c\u662f\u6cdb\u57df\u540d\uff0c\u5fc5\u987b\u4ee5\u70b9\u53f7 . \u5f00\u5934\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56dedict{}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n        url = '{0}/domain/{1}'.format(self.server, name)\n        return self.__post(url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nput a new https configuration entry.", "response": "def put_httpsconf(self, name, certid, forceHttps):\n        \"\"\"\n        \u4fee\u6539\u8bc1\u4e66\uff0c\u6587\u6863 https://developer.qiniu.com/fusion/api/4246/the-domain-name#11\n\n        Args:\n           domains:     \u57df\u540dname\n           CertID:      \u8bc1\u4e66id\uff0c\u4ece\u4e0a\u4f20\u6216\u8005\u83b7\u53d6\u8bc1\u4e66\u5217\u8868\u91cc\u62ff\u5230\u8bc1\u4e66id\n           ForceHttps:  \u662f\u5426\u5f3a\u5236https\u8df3\u8f6c\n\n        Returns:\n           {}\n        \"\"\"\n        req = {}\n        req.update({\"certid\": certid})\n        req.update({\"forceHttps\": forceHttps})\n\n        body = json.dumps(req)\n        url = '{0}/domain/{1}/httpsconf'.format(self.server, name)\n        return self.__put(url, body)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread gbasis - formatted file data and converts it to a dictionary with the usual BSE fields we have", "response": "def read_gbasis(basis_lines, fname):\n    '''Reads gbasis-formatted file data and converts it to a dictionary with the\n       usual BSE fields\n\n       Note that the gbasis format does not store all the fields we\n       have, so some fields are left blank\n    '''\n\n    skipchars = '!#'\n    basis_lines = [l for l in basis_lines if l and not l[0] in skipchars]\n\n    bs_data = create_skel('component')\n\n    i = 0\n    bs_name = None\n    while i < len(basis_lines):\n        line = basis_lines[i]\n        lsplt = line.split(':')\n        elementsym = lsplt[0]\n\n        if bs_name is None:\n            bs_name = lsplt[1]\n        elif lsplt[1] != bs_name:\n            raise RuntimeError(\"Multiple basis sets in a file\")\n\n        element_Z = lut.element_Z_from_sym(elementsym)\n        element_Z = str(element_Z)\n\n        if not element_Z in bs_data['elements']:\n            bs_data['elements'][element_Z] = {}\n\n        element_data = bs_data['elements'][element_Z]\n\n        if not 'electron_shells' in element_data:\n            element_data['electron_shells'] = []\n\n        i += 1\n\n        max_am = int(basis_lines[i].strip())\n        i += 1\n\n        for am in range(0, max_am + 1):\n            lsplt = basis_lines[i].split()\n            shell_am = lut.amchar_to_int(lsplt[0])\n            nprim = int(lsplt[1])\n            ngen = int(lsplt[2])\n\n            if shell_am[0] != am:\n                raise RuntimeError(\"AM out of order in gbasis?\")\n\n            if max(shell_am) <= 1:\n                func_type = 'gto'\n            else:\n                func_type = 'gto_spherical'\n\n\n            shell = {\n                'function_type': func_type,\n                'region': '',\n                'angular_momentum': shell_am\n            }\n\n            exponents = []\n            coefficients = []\n\n            i += 1\n            for j in range(nprim):\n                line = basis_lines[i].replace('D', 'E')\n                line = line.replace('d', 'E')\n                lsplt = line.split()\n\n                if len(lsplt) != (ngen + 1):\n                    raise RuntimeError(\"Incorrect number of general contractions in gbasis\")\n\n                exponents.append(lsplt[0])\n                coefficients.append(lsplt[1:])\n                i += 1\n\n            shell['exponents'] = exponents\n\n            # We need to transpose the coefficient matrix\n            # (we store a matrix with primitives being the column index and\n            # general contraction being the row index)\n            shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n            element_data['electron_shells'].append(shell)\n\n    return bs_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading molcas - formatted file data and converts it to a dictionary with the usual BSE fields we", "response": "def read_molcas(basis_lines, fname):\n    '''Reads molcas-formatted file data and converts it to a dictionary with the\n       usual BSE fields\n\n       Note that the turbomole format does not store all the fields we\n       have, so some fields are left blank\n    '''\n\n    skipchars = '*#$'\n    basis_lines = [l for l in basis_lines if l and not l[0] in skipchars]\n\n    bs_data = create_skel('component')\n\n    i = 0\n    while i < len(basis_lines):\n        line = basis_lines[i]\n\n        if not line.startswith('/'):\n            raise RuntimeError(\"Expecting line starting with /\")\n\n        line_splt = line[1:].split('.')\n        elementsym = line_splt[0]\n\n        element_Z = lut.element_Z_from_sym(elementsym)\n        element_Z = str(element_Z)\n\n        if not element_Z in bs_data['elements']:\n            bs_data['elements'][element_Z] = {}\n\n        element_data = bs_data['elements'][element_Z]\n\n        if \"ecp\" in line.lower():\n            raise NotImplementedError(\"MolCAS ECPs not supported\")\n\n            #if not 'ecp_potentials' in element_data:\n            #    element_data['ecp_potentials'] = []\n\n            #i += 1\n            #line = basis_lines[i]\n\n            #lsplt = line.split('=')\n            #maxam = int(lsplt[2])\n            #n_elec = int(lsplt[1].split()[0])\n\n            #amlist = [maxam]\n            #amlist.extend(list(range(0, maxam)))\n\n            #i += 1\n            #for shell_am in amlist:\n            #    shell_am2 = lut.amchar_to_int(basis_lines[i][0])[0]\n            #    if shell_am2 != shell_am:\n            #        raise RuntimeError(\"AM not in expected order?\")\n\n            #    i += 1\n\n            #    ecp_shell = {\n            #        'ecp_type': 'scalar',\n            #        'angular_momentum': [shell_am],\n            #    }\n            #    ecp_exponents = []\n            #    ecp_rexponents = []\n            #    ecp_coefficients = []\n\n            #    while i < len(basis_lines) and basis_lines[i][0].isalpha() is False:\n            #        lsplt = basis_lines[i].split()\n            #        ecp_exponents.append(lsplt[2])\n            #        ecp_rexponents.append(int(lsplt[1]))\n            #        ecp_coefficients.append(lsplt[0])\n            #        i += 1\n\n            #    ecp_shell['r_exponents'] = ecp_rexponents\n            #    ecp_shell['gaussian_exponents'] = ecp_exponents\n            #    ecp_shell['coefficients'] = [ecp_coefficients]\n            #    element_data['ecp_potentials'].append(ecp_shell)\n\n            #element_data['ecp_electrons'] = n_elec\n\n        else:\n            if not 'electron_shells' in element_data:\n                element_data['electron_shells'] = []\n\n            # Skip two comment lines (usually ref)\n            i += 3\n\n            # Skip over an options block\n            line = basis_lines[i]\n            if line.lower() == 'options':\n                while basis_lines[i].lower() != 'endoptions':\n                    i += 1\n                i += 1\n\n            lsplt = basis_lines[i].split()\n            max_am = int(lsplt[1])\n            i += 1\n\n            for shell_am in range(max_am+1):\n                lsplt = basis_lines[i].replace(',', ' ').split()\n                nprim = int(lsplt[0])\n                ngen = int(lsplt[1])\n                i += 1\n\n                if shell_am <= 1:\n                    func_type = 'gto'\n                else:\n                    func_type = 'gto_spherical'\n\n                shell = {\n                    'function_type': func_type,\n                    'region': '',\n                    'angular_momentum': [shell_am]\n                }\n\n                exponents = []\n                coefficients = []\n\n                j = 0\n                while j < nprim:\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    exponents.extend(lsplt)\n                    i += 1\n                    j += len(lsplt)\n\n                for j in range(nprim):\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    if len(lsplt) != ngen:\n                        print(fname)\n                        print(line)\n                        raise RuntimeError(\"Unexpected number of coefficients\")\n                    coefficients.append(lsplt)\n\n\n                    i += 1\n\n                shell['exponents'] = exponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                element_data['electron_shells'].append(shell)\n\n                # Skip energies?\n                to_skip = int(basis_lines[i].strip())\n                skipped = 0\n                i += 1\n                while skipped < to_skip:\n                    skipped += len(basis_lines[i].split())\n                    i += 1\n                \n    return bs_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a JSON file and returns a dictionary of the key - value pairs.", "response": "def _read_plain_json(file_path, check_bse):\n    \"\"\"\n    Reads a JSON file\n\n    A simple wrapper around json.load that only takes the file name\n    If the file does not exist, an exception is thrown.\n\n    If the file does exist, but there is a problem with the JSON formatting,\n    the filename is added to the exception information.\n\n    If check_bse is True, this function also make sure the 'molssi_bse_schema' key\n    exists in the file.\n\n    Parameters\n    ----------\n    file_path : str\n        Full path to the file to read\n    check_bse: bool\n        If True, check to make sure the bse schema information is included.\n        If not found, an exception is raised\n    \"\"\"\n\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError('JSON file \\'{}\\' does not exist, is not '\n                                'readable, or is not a file'.format(file_path))\n\n    try:\n        if file_path.endswith('.bz2'):\n            with bz2.open(file_path, 'rt', encoding=_default_encoding) as f:\n                js = json.load(f)\n        else:\n            with open(file_path, 'r', encoding=_default_encoding) as f:\n                js = json.load(f)\n\n    except json.decoder.JSONDecodeError as ex:\n        raise RuntimeError(\"File {} contains JSON errors\".format(file_path)) from ex\n\n    if check_bse is True:\n        # Check for molssi_bse_schema key\n        if 'molssi_bse_schema' not in js:\n            raise RuntimeError('File {} does not appear to be a BSE JSON file'.format(file_path))\n\n    return js"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites information to a JSON file", "response": "def _write_plain_json(file_path, js):\n    \"\"\"\n    Write information to a JSON file\n\n    This makes sure files are created with the proper encoding and consistent indenting\n\n    Parameters\n    ----------\n    file_path : str\n        Full path to the file to write to. It will be overwritten if it exists\n    js : dict\n        JSON information to write\n    \"\"\"\n\n    # Disable ascii in the json - this prevents the json writer\n    # from escaping everything\n\n    if file_path.endswith('.bz2'):\n        with bz2.open(file_path, 'wt', encoding=_default_encoding) as f:\n            json.dump(js, f, indent=2, ensure_ascii=False)\n    else:\n        with open(file_path, 'w', encoding=_default_encoding) as f:\n            json.dump(js, f, indent=2, ensure_ascii=False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_filelist(data_dir):\n\n    all_meta = []\n    all_table = []\n    all_element = []\n    all_component = []\n\n    special = ['METADATA.json', 'REFERENCES.json']\n\n    for root, dirs, files in os.walk(data_dir):\n        for basename in files:\n            if basename in special:\n                continue\n\n            fpath = os.path.join(root, basename)\n            fpath = os.path.relpath(fpath, data_dir)\n\n            if basename.endswith('.metadata.json'):\n                all_meta.append(fpath)\n            elif basename.endswith('.table.json'):\n                all_table.append(fpath)\n            elif basename.endswith('.element.json'):\n                all_element.append(fpath)\n            elif basename.endswith('.json'):\n                all_component.append(fpath)\n\n    return (all_meta, all_table, all_element, all_component)", "response": "Returns a tuple containing the paths to all the files in the data_dir"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_notes_file(file_path):\n\n    if not os.path.isfile(file_path):\n        return None\n\n    with open(file_path, 'r', encoding=_default_encoding) as f:\n        return f.read()", "response": "Reads the contents of a notes file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _whole_basis_types(basis):\n    '''\n    Get a list of all the types of features in this basis set.\n\n    '''\n\n    all_types = set()\n\n    for v in basis['elements'].values():\n        if 'electron_shells' in v:\n            for sh in v['electron_shells']:\n                all_types.add(sh['function_type'])\n\n        if 'ecp_potentials' in v:\n            for pot in v['ecp_potentials']:\n                all_types.add(pot['ecp_type'])\n\n    return sorted(list(all_types))", "response": "Get a list of all the types of features in this basis set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compose_elemental_basis(file_relpath, data_dir):\n\n    # Do a simple read of the json\n    el_bs = fileio.read_json_basis(os.path.join(data_dir, file_relpath))\n\n    # construct a list of all files to read\n    component_files = set()\n    for k, v in el_bs['elements'].items():\n        component_files.update(set(v['components']))\n\n    # Read all the data from these files into a big dictionary\n    component_map = {k: fileio.read_json_basis(os.path.join(data_dir, k)) for k in component_files}\n\n    # Use the basis_set_description for the reference description\n    for k, v in component_map.items():\n        for el, el_data in v['elements'].items():\n            el_data['references'] = [{\n                'reference_description': v['description'],\n                'reference_keys': el_data['references']\n            }]\n\n    # Compose on a per-element basis\n    for k, v in el_bs['elements'].items():\n\n        components = v.pop('components')\n\n        # all of the component data for this element\n        el_comp_data = []\n        for c in components:\n            centry = component_map[c]['elements']\n\n            if k not in centry:\n                raise RuntimeError('File {} does not contain element {}'.format(c, k))\n\n            el_comp_data.append(centry[k])\n\n        # merge all the data\n        v = manip.merge_element_data(None, el_comp_data)\n        el_bs['elements'][k] = v\n\n    return el_bs", "response": "This function reads the elemental json file and creates a dictionary of all the elemental basis sets that are part of the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compose_table_basis(file_relpath, data_dir):\n\n    # Do a simple read of the json\n    file_path = os.path.join(data_dir, file_relpath)\n    table_bs = fileio.read_json_basis(file_path)\n\n    # construct a list of all elemental files to read\n    element_files = set(table_bs['elements'].values())\n\n    # Create a map of the elemental basis data\n    # (maps file path to data contained in that file)\n    element_map = {k: compose_elemental_basis(k, data_dir) for k in element_files}\n\n    # Replace the basis set for all elements in the table basis with the data\n    # from the elemental basis\n    for k, entry in table_bs['elements'].items():\n        data = element_map[entry]\n\n        if k not in data['elements']:\n            raise KeyError('File {} does not contain element {}'.format(entry, k))\n\n        table_bs['elements'][k] = data['elements'][k]\n\n    # Add the version to the dictionary\n    file_base = os.path.basename(file_relpath)\n    table_bs['version'] = file_base.split('.')[-3]\n\n    # Add whether the entire basis is spherical or cartesian\n    table_bs['function_types'] = _whole_basis_types(table_bs)\n\n    # Read and merge in the metadata\n    # This file must be in the same location as the table file\n    meta_dirpath, table_filename = os.path.split(file_path)\n    meta_filename = table_filename.split('.')[0] + '.metadata.json'\n    meta_filepath = os.path.join(meta_dirpath, meta_filename)\n    bs_meta = fileio.read_json_basis(meta_filepath)\n    table_bs.update(bs_meta)\n\n    # Remove the molssi schema (which isn't needed here)\n    table_bs.pop('molssi_bse_schema')\n\n    return table_bs", "response": "This function creates a dictionary containing the basis set information for all elements in the given file and the metadata file for all elements in the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_skel(role):\n    '''\n    Create the skeleton of a dictionary or JSON file\n\n    A dictionary is returned that contains the \"molssi_bse_schema\"\n    key and other required keys, depending on the role\n\n    role can be either 'component', 'element', or 'table'\n    '''\n\n    role = role.lower()\n    if not role in _skeletons:\n        raise RuntimeError(\"Role {} not found. Should be 'component', 'element', 'table', or 'metadata'\")\n\n    return copy.deepcopy(_skeletons[role])", "response": "Create the skeleton of a dictionary or JSON file based on the role"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd reference information to the bottom of a notes file", "response": "def process_notes(notes, ref_data):\n    '''Add reference information to the bottom of a notes file\n\n    `:ref:` tags are removed and the actual reference data is appended\n    '''\n\n    ref_keys = ref_data.keys()\n\n    found_refs = set()\n    for k in ref_keys:\n        if k in notes:\n            found_refs.add(k)\n\n    # The block to append\n    reference_sec = '\\n\\n'\n    reference_sec += '-------------------------------------------------\\n'\n    reference_sec += ' REFERENCES MENTIONED ABOVE\\n'\n    reference_sec += ' (not necessarily references for the basis sets)\\n'\n    reference_sec += '-------------------------------------------------\\n'\n\n    # Add reference data\n    if len(found_refs) == 0:\n        return notes\n\n    for r in sorted(found_refs):\n        rtxt = references.reference_text(ref_data[r])\n        reference_sec += r + '\\n'\n        reference_sec += textwrap.indent(rtxt, ' ' * 4)\n        reference_sec += '\\n\\n'\n\n    return notes + reference_sec"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate_extra_component(bs_data):\n    '''Extra checks for component basis files'''\n\n    assert len(bs_data['elements']) > 0\n\n    # Make sure size of the coefficient matrix matches the number of exponents\n    for el in bs_data['elements'].values():\n        if not 'electron_shells' in el:\n            continue\n\n        for s in el['electron_shells']:\n            nprim = len(s['exponents'])\n            if nprim <= 0:\n                raise RuntimeError(\"Invalid number of primitives: {}\".format(nprim))\n\n            for g in s['coefficients']:\n                if nprim != len(g):\n                    raise RuntimeError(\"Number of coefficients doesn't match number of primitives ({} vs {}\".format(\n                        len(g), nprim))\n\n            # If more than one AM is given, that should be the number of\n            # general contractions\n            nam = len(s['angular_momentum'])\n            if nam > 1:\n                ngen = len(s['coefficients'])\n                if ngen != nam:\n                    raise RuntimeError(\"Number of general contractions doesn't match combined AM ({} vs {}\".format(\n                        ngen, nam))", "response": "Extra checks for component basis files"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_data(file_type, bs_data):\n\n    if file_type not in _validate_map:\n        raise RuntimeError(\"{} is not a valid file_type\".format(file_type))\n\n    schema = api.get_schema(file_type)\n    jsonschema.validate(bs_data, schema)\n    _validate_map[file_type](bs_data)", "response": "Validates json basis set data against a schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_file(file_type, file_path):\n\n    file_data = fileio._read_plain_json(file_path, False)\n    validate_data(file_type, file_data)", "response": "Validates a file against a schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates all files in a data_dir", "response": "def validate_data_dir(data_dir):\n    \"\"\"\n    Validates all files in a data_dir\n    \"\"\"\n\n    all_meta, all_table, all_element, all_component = fileio.get_all_filelist(data_dir)\n\n    for f in all_meta:\n        full_path = os.path.join(data_dir, f)\n        validate_file('metadata', full_path)\n    for f in all_table:\n        full_path = os.path.join(data_dir, f)\n        validate_file('table', full_path)\n    for f in all_element:\n        full_path = os.path.join(data_dir, f)\n        validate_file('element', full_path)\n    for f in all_component:\n        full_path = os.path.join(data_dir, f)\n        validate_file('component', full_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_basis_dict(bs):\n\n    # yapf: disable\n    _keyorder = [\n        # Schema stuff\n        'molssi_bse_schema', 'schema_type', 'schema_version',\n\n        # Auxiliary block\n         'jkfit', 'jfit', 'rifit', 'admmfit', 'dftxfit', 'dftjfit',\n\n        # Basis set metadata\n        'name', 'names', 'aliases', 'flags', 'family', 'description', 'role', 'auxiliaries',\n        'notes', 'function_types',\n\n        # Reference stuff\n        'reference_description', 'reference_keys',\n\n        # Version metadata\n        'version', 'revision_description',\n\n        # Sources of components\n        'data_source',\n\n        # Elements and data\n        'elements', 'references', 'ecp_electrons',\n        'electron_shells', 'ecp_potentials', 'components',\n\n        # Shell information\n        'function_type', 'region', 'angular_momentum', 'exponents',\n        'coefficients',\n        'ecp_type', 'angular_momentum', 'r_exponents', 'gaussian_exponents',\n        'coefficients'\n    ]\n    # yapf: enable\n\n    # Add integers for the elements (being optimistic that element 150 will be found someday)\n    _keyorder.extend([str(x) for x in range(150)])\n\n    bs_sorted = sorted(bs.items(), key=lambda x: _keyorder.index(x[0]))\n    if _use_odict:\n        bs_sorted = OrderedDict(bs_sorted)\n    else:\n        bs_sorted = dict(bs_sorted)\n\n    for k, v in bs_sorted.items():\n        # If this is a dictionary, sort recursively\n        # If this is a list, sort each element but DO NOT sort the list itself.\n        if isinstance(v, dict):\n            bs_sorted[k] = sort_basis_dict(v)\n        elif isinstance(v, list):\n            # Note - the only nested list is with coeffs, which shouldn't be sorted\n            #        (so we don't have to recurse into lists of lists)\n            bs_sorted[k] = [sort_basis_dict(x) if isinstance(x, dict) else x for x in v]\n\n    return bs_sorted", "response": "Sorts a basis set dictionary into a standard order for cosmetic reasons."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort a basis set shell into a standard order", "response": "def sort_shell(shell, use_copy=True):\n    \"\"\"\n    Sort a basis set shell into a standard order\n\n    If use_copy is True, the input shells are not modified.\n    \"\"\"\n\n    if use_copy:\n        shell = copy.deepcopy(shell)\n\n    # Transpose of coefficients\n    tmp_c = list(map(list, zip(*shell['coefficients'])))\n\n    # For each primitive, find the index of the first nonzero coefficient\n    nonzero_idx = [next((i for i, x in enumerate(c) if float(x) != 0.0), None) for c in tmp_c]\n\n    # Zip together exponents and coeffs for sorting\n    tmp = zip(shell['exponents'], tmp_c, nonzero_idx)\n\n    # Sort by decreasing value of exponent\n    tmp = sorted(tmp, key=lambda x: -float(x[0]))\n\n    # Now (stable) sort by first non-zero coefficient\n    tmp = sorted(tmp, key=lambda x: int(x[2]))\n\n    # Unpack, and re-transpose the coefficients\n    tmp_c = [x[1] for x in tmp]\n    shell['exponents'] = [x[0] for x in tmp]\n\n    # Now sort the columns of the coefficient by index of first nonzero coefficient\n    tmp_c = list(map(list, zip(*tmp_c)))\n    nonzero_idx = [next((i for i, x in enumerate(c) if float(x) != 0.0), None) for c in tmp_c]\n\n    tmp = zip(tmp_c, nonzero_idx)\n    tmp = sorted(tmp, key=lambda x: int(x[1]))\n    tmp_c = [x[0] for x in tmp]\n\n    shell['coefficients'] = tmp_c\n\n    return shell"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting a list of basis set shells into a standard order.", "response": "def sort_shells(shells, use_copy=True):\n    \"\"\"\n    Sort a list of basis set shells into a standard order\n\n    The order within a shell is by decreasing value of the exponent.\n\n    The order of the shell list is in increasing angular momentum, and then\n    by decreasing number of primitives, then decreasing value of the largest exponent.\n\n    If use_copy is True, the input shells are not modified.\n    \"\"\"\n\n    if use_copy:\n        shells = copy.deepcopy(shells)\n\n    # Sort primitives within a shell\n    # (copying already handled above)\n    shells = [sort_shell(sh, False) for sh in shells]\n\n    # Sort the list by increasing AM, then general contraction level, then decreasing highest exponent\n    return list(\n        sorted(\n            shells,\n            key=lambda x: (max(x['angular_momentum']), -len(x['exponents']), -len(x['coefficients']), -float(\n                max(x['exponents'])))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort_potentials(potentials, use_copy=True):\n\n    if use_copy:\n        potentials = copy.deepcopy(potentials)\n\n    # Sort by increasing AM, then move the last element to the front\n    potentials = list(sorted(potentials, key=lambda x: x['angular_momentum']))\n    potentials.insert(0, potentials.pop())\n    return potentials", "response": "Sort a list of ECP potentials into a standard order"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_basis(basis, use_copy=True):\n\n    if use_copy:\n        basis = copy.deepcopy(basis)\n\n    for k, el in basis['elements'].items():\n        if 'electron_shells' in el:\n            el['electron_shells'] = sort_shells(el['electron_shells'], False)\n        if 'ecp_potentials' in el:\n            el['ecp_potentials'] = sort_potentials(el['ecp_potentials'], False)\n\n    return sort_basis_dict(basis)", "response": "Sorts the information in a basis set into a standard order."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsort a dictionary containing data for a single reference into a standard order", "response": "def sort_single_reference(ref_entry):\n    \"\"\"Sorts a dictionary containing data for a single reference into a standard order\n    \"\"\"\n\n    # yapf: disable\n    _keyorder = [\n        # Schema stuff\n        # This function gets called on the schema 'entry', too\n        'schema_type', 'schema_version',\n\n        # Type of the entry\n        'type',\n\n        # Actual publication info\n        'authors', 'title', 'booktitle', 'series', 'editors', 'journal',\n        'institution', 'volume', 'number', 'page', 'year', 'note', 'publisher',\n        'address', 'isbn', 'doi'\n    ]\n    # yapf: enable\n\n    sorted_entry = sorted(ref_entry.items(), key=lambda x: _keyorder.index(x[0]))\n\n    if _use_odict:\n        return OrderedDict(sorted_entry)\n    else:\n        return dict(sorted_entry)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sort_references_dict(refs):\n\n    if _use_odict:\n        refs_sorted = OrderedDict()\n    else:\n        refs_sorted = dict()\n\n    # We insert this first, That is ok - it will be overwritten\n    # with the sorted version later\n    refs_sorted['molssi_bse_schema'] = refs['molssi_bse_schema']\n\n    # This sorts the entries by reference key (author1985a, etc)\n    for k, v in sorted(refs.items()):\n        refs_sorted[k] = sort_single_reference(v)\n\n    return refs_sorted", "response": "Sorts a reference dictionary into a standard order."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_dalton(basis_lines, fname):\n    '''Reads Dalton-formatted file data and converts it to a dictionary with the\n       usual BSE fields\n\n       Note that the nwchem format does not store all the fields we\n       have, so some fields are left blank\n    '''\n\n    skipchars = '$'\n    basis_lines = [l for l in basis_lines if l and not l[0] in skipchars]\n\n    bs_data = create_skel('component')\n\n    i = 0\n\n    while i < len(basis_lines):\n        line = basis_lines[i]\n\n        if line.lower().startswith('a '):\n            element_Z = line.split()[1]\n            i += 1\n\n            # Shell am is strictly increasing (I hope)\n            shell_am = 0\n\n            while i < len(basis_lines) and not basis_lines[i].lower().startswith('a '):\n                line = basis_lines[i]\n                nprim, ngen = line.split()\n\n                if not element_Z in bs_data['elements']:\n                    bs_data['elements'][element_Z] = {}\n                if not 'electron_shells' in bs_data['elements'][element_Z]:\n                    bs_data['elements'][element_Z]['electron_shells'] = []\n\n                element_data = bs_data['elements'][element_Z]\n\n                if shell_am <= 1:\n                    func_type = 'gto'\n                else:\n                    func_type = 'gto_spherical'\n\n                shell = {\n                    'function_type': func_type,\n                    'region': '',\n                    'angular_momentum': [shell_am]\n                }\n\n                exponents = []\n                coefficients = []\n\n                i += 1\n                for _ in range(int(nprim)):\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    exponents.append(lsplt[0])\n                    coefficients.append(lsplt[1:])\n                    i += 1\n\n                shell['exponents'] = exponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                # Make sure the number of general contractions is >0\n                # (This error was found in some bad files)\n                if int(ngen) <= 0:\n                    raise RuntimeError(\"Number of general contractions is not greater than zero for element \" + str(element_Z))\n\n                # Make sure the number of general contractions match the heading line\n                if len(shell['coefficients']) != int(ngen):\n                    raise RuntimeError(\"Number of general contractions does not equal what was given for element \" + str(element_Z))\n\n                element_data['electron_shells'].append(shell)\n                shell_am += 1\n\n    return bs_data", "response": "Reads Dalton - formatted file data and converts it to a dictionary with the usual BSE fields we have"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_range(coeffs):\n    '''\n    Find the range in a list of coefficients where the coefficient is nonzero\n    '''\n\n    coeffs = [float(x) != 0 for x in coeffs]\n    first = coeffs.index(True)\n    coeffs.reverse()\n    last = len(coeffs) - coeffs.index(True) - 1\n    return first, last", "response": "Find the range in a list of coefficients where the coefficient is nonzero"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ref_bib(key, ref):\n    '''Convert a single reference to bibtex format\n    '''\n    s = ''\n\n    s += '@{}{{{},\\n'.format(ref['type'], key)\n\n    entry_lines = []\n    for k, v in ref.items():\n        if k == 'type':\n            continue\n\n        # Handle authors/editors\n        if k == 'authors':\n            entry_lines.append('    author = {{{}}}'.format(' and '.join(v)))\n        elif k == 'editors':\n            entry_lines.append('    editor = {{{}}}'.format(' and '.join(v)))\n        else:\n            entry_lines.append('    {} = {{{}}}'.format(k, v))\n\n    s += ',\\n'.join(entry_lines)\n    s += '\\n}'\n\n    return s", "response": "Convert a single reference to bibtex format\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert references to bibtex", "response": "def write_bib(refs):\n    '''Converts references to bibtex\n    '''\n\n    full_str = ''\n\n    lib_citation_desc, lib_citations = get_library_citation()\n\n    full_str += '%' * 80 + '\\n'\n    full_str += textwrap.indent(lib_citation_desc, '% ')\n    full_str += '%' * 80 + '\\n\\n'\n\n    for k, r in lib_citations.items():\n        full_str += _ref_bib(k, r) + '\\n\\n'\n\n    full_str += '%' * 80 + '\\n'\n    full_str += \"% References for the basis set\\n\"\n    full_str += '%' * 80 + '\\n'\n\n    # First, write out the element, description -> key mapping\n    # Also make a dict of unique reference to output\n    unique_refs = {}\n\n    for ref in refs:\n        full_str += '% {}\\n'.format(compact_elements(ref['elements']))\n\n        for ri in ref['reference_info']:\n            full_str += '%     {}\\n'.format(ri['reference_description'])\n\n            refdata = ri['reference_data']\n\n            if len(refdata) == 0:\n                full_str += '%     (...no reference...)\\n%\\n'\n            else:\n                rkeys = [x[0] for x in ri['reference_data']]\n                full_str += '%         {}\\n%\\n'.format(' '.join(rkeys))\n\n            for k, r in refdata:\n                unique_refs[k] = r\n\n    full_str += '\\n\\n'\n\n    # Go through them sorted alphabetically by key\n    for k, r in sorted(unique_refs.items(), key=lambda x: x[0]):\n        full_str += '{}\\n\\n'.format(_ref_bib(k, r))\n\n    return full_str"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading G94 - formatted file data and converts it to a dictionary with the usual BSE fields we have", "response": "def read_g94(basis_lines, fname):\n    '''Reads G94-formatted file data and converts it to a dictionary with the\n       usual BSE fields\n\n       Note that the gaussian format does not store all the fields we\n       have, so some fields are left blank\n    '''\n\n    skipchars = '!'\n    basis_lines = [l for l in basis_lines if l and not l[0] in skipchars]\n\n    bs_data = create_skel('component')\n\n    i = 0\n    if basis_lines[0] == '****':\n        i += 1  # skip initial ****\n\n    while i < len(basis_lines):\n        line = basis_lines[i]\n        elementsym = line.split()[0]\n\n        # Some gaussian files have a dash before the element\n        if elementsym[0] == '-':\n            elementsym = elementsym[1:]\n\n        element_Z = lut.element_Z_from_sym(elementsym)\n        element_Z = str(element_Z)\n\n        if not element_Z in bs_data['elements']:\n            bs_data['elements'][element_Z] = {}\n\n        element_data = bs_data['elements'][element_Z]\n\n        i += 1\n\n        # Try to guess if this is an ecp\n        # Electron basis almost always end in 1.0 (scale factor)\n        # ECP lines would end in an integer, so isdecimal() = true for ecp\n        if basis_lines[i].split()[-1].isdecimal():\n            if not 'ecp_potentials' in element_data:\n                element_data['ecp_potentials'] = []\n\n            lsplt = basis_lines[i].split()\n            maxam = int(lsplt[1])\n            n_elec = int(lsplt[2])\n            element_data['ecp_electrons'] = n_elec\n\n            # Highest AM first, then the rest in order\n            am_list = list(range(maxam + 1))\n            am_list.insert(0, am_list.pop())\n\n            i += 1\n\n            for j in range(maxam + 1):\n                i += 1  # Skip the 'title' block - unused according to gaussian docs\n                n_entries = int(basis_lines[i])\n                i += 1  # Skip title block\n\n                shell_am = am_list[j]\n                ecp_shell = {'angular_momentum': [shell_am], 'ecp_type': 'scalar_ecp'}\n                rexponents = []\n                gexponents = []\n                coefficients = []\n\n                for k in range(n_entries):\n                    lsplt = basis_lines[i].split()\n                    rexponents.append(int(lsplt[0]))\n                    gexponents.append(lsplt[1])\n                    coefficients.append(lsplt[2:])\n                    i += 1\n\n                ecp_shell['r_exponents'] = rexponents\n                ecp_shell['gaussian_exponents'] = gexponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                ecp_shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                element_data['ecp_potentials'].append(ecp_shell)\n        else:\n            if not 'electron_shells' in element_data:\n                element_data['electron_shells'] = []\n\n            while basis_lines[i] != '****':\n                lsplt = basis_lines[i].split()\n                shell_am = lut.amchar_to_int(lsplt[0])\n                nprim = int(lsplt[1])\n\n                if max(shell_am) <= 1:\n                    func_type = 'gto'\n                else:\n                    func_type = 'gto_spherical'\n\n                shell = {\n                    'function_type': func_type,\n                    'region': '',\n                    'angular_momentum': shell_am\n                }\n\n                exponents = []\n                coefficients = []\n\n                i += 1\n                for j in range(nprim):\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    exponents.append(lsplt[0])\n                    coefficients.append(lsplt[1:])\n                    i += 1\n\n                shell['exponents'] = exponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                element_data['electron_shells'].append(shell)\n\n            i += 1\n\n    return bs_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_turbomole(basis):\n    '''Converts a basis set to Gaussian format\n    '''\n\n    s = '$basis\\n'\n    s += '*\\n'\n\n    # TM basis sets are completely uncontracted\n    basis = manip.uncontract_general(basis, True)\n    basis = manip.uncontract_spdf(basis, 0, False)\n    basis = sort.sort_basis(basis, False)\n\n    # Elements for which we have electron basis\n    electron_elements = [k for k, v in basis['elements'].items() if 'electron_shells' in v]\n\n    # Elements for which we have ECP\n    ecp_elements = [k for k, v in basis['elements'].items() if 'ecp_potentials' in v]\n\n    # Electron Basis\n    if len(electron_elements) > 0:\n        for z in electron_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z, False)\n            s += '{} {}\\n'.format(sym, basis['name'])\n            s += '*\\n'\n\n            for shell in data['electron_shells']:\n                exponents = shell['exponents']\n                coefficients = shell['coefficients']\n                ncol = len(coefficients) + 1\n                nprim = len(exponents)\n\n                am = shell['angular_momentum']\n                amchar = lut.amint_to_char(am, hij=True)\n                s += '    {}   {}\\n'.format(nprim, amchar)\n\n                point_places = [8 * i + 15 * (i - 1) for i in range(1, ncol + 1)]\n                s += printing.write_matrix([exponents, *coefficients], point_places, convert_exp=True)\n\n            s += '*\\n'\n\n    # Write out ECP\n    if len(ecp_elements) > 0:\n        s += '$ecp\\n'\n        s += '*\\n'\n        for z in ecp_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z)\n            s += '{} {}-ecp\\n'.format(sym, basis['name'])\n            s += '*\\n'\n\n            max_ecp_am = max([x['angular_momentum'][0] for x in data['ecp_potentials']])\n            max_ecp_amchar = lut.amint_to_char([max_ecp_am], hij=True)\n\n            # Sort lowest->highest, then put the highest at the beginning\n            ecp_list = sorted(data['ecp_potentials'], key=lambda x: x['angular_momentum'])\n            ecp_list.insert(0, ecp_list.pop())\n\n            s += '  ncore = {}   lmax = {}\\n'.format(data['ecp_electrons'], max_ecp_am)\n\n            for pot in ecp_list:\n                rexponents = pot['r_exponents']\n                gexponents = pot['gaussian_exponents']\n                coefficients = pot['coefficients']\n\n                am = pot['angular_momentum']\n                amchar = lut.amint_to_char(am, hij=True)\n\n                if am[0] == max_ecp_am:\n                    s += '{}\\n'.format(amchar)\n                else:\n                    s += '{}-{}\\n'.format(amchar, max_ecp_amchar)\n\n                point_places = [9, 23, 32]\n                s += printing.write_matrix([*coefficients, rexponents, gexponents], point_places, convert_exp=True)\n            s += '*\\n'\n\n    s += '$end\\n'\n    return s", "response": "Converts a basis set to Gaussian format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compact_references(basis_dict, ref_data):\n\n    element_refs = []\n\n    # Create a mapping of elements -> reference information\n    # (sort by Z first, keeping in mind Z is a string)\n    sorted_el = sorted(basis_dict['elements'].items(), key=lambda x: int(x[0]))\n\n    for el, eldata in sorted_el:\n\n        # elref is a list of dict\n        # dict is { 'reference_description': str, 'reference_keys': [keys] }\n        elref = eldata['references']\n\n        for x in element_refs:\n            if x['reference_info'] == elref:\n                x['elements'].append(el)\n                break\n        else:\n            element_refs.append({'reference_info': elref, 'elements': [el]})\n\n    for item in element_refs:\n        # Loop over a list of dictionaries for this group of elements and add the\n        # actual reference data\n        # Since we store the keys with the data, we don't need it anymore\n        for elref in item['reference_info']:\n            elref['reference_data'] = [(k, ref_data[k]) for k in elref['reference_keys']]\n            elref.pop('reference_keys')\n\n    return element_refs", "response": "This function takes a dictionary containing basis set information and a dictionary containing all reference information and returns a list of dictionaries that contain the key - value pairs for each element in the basis set and the corresponding value for each element in the dictionary containing the data for the element in the basis set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reference_text(ref):\n    '''Convert a single reference to plain text format\n\n    Parameters\n    ----------\n    ref : dict\n        Information about a single reference\n    '''\n\n    ref_wrap = textwrap.TextWrapper(initial_indent='', subsequent_indent=' ' * 8)\n\n    s = ''\n    if ref['type'] == 'unpublished':\n        s += ref_wrap.fill(', '.join(ref['authors'])) + '\\n'\n        s += ref_wrap.fill(ref['title']) + '\\n'\n        s += ref_wrap.fill(ref['note']) + '\\n'\n    elif ref['type'] == 'article':\n        s += ref_wrap.fill(', '.join(ref['authors'])) + '\\n'\n        s += ref_wrap.fill(ref['title']) + '\\n'\n        s += '{}, {}, {} ({})'.format(ref['journal'], ref['volume'], ref['page'], ref['year'])\n        s += '\\n' + ref['doi']\n    elif ref['type'] == 'incollection':\n        s += ref_wrap.fill(', '.join(ref['authors']))\n        s += ref_wrap.fill('\\n{}'.format(ref['title']))\n        s += ref_wrap.fill('\\nin \\'{}\\''.format(ref['booktitle']))\n        if 'editors' in ref:\n            s += ref_wrap.fill('\\ned. ' + ', '.join(ref['editors']))\n        if 'series' in ref:\n            s += '\\n{}, {}, {} ({})'.format(ref['series'], ref['volume'], ref['page'], ref['year'])\n        if 'doi' in ref:\n            s += '\\n' + ref['doi']\n    elif ref['type'] == 'techreport':\n        s += ref_wrap.fill(', '.join(ref['authors']))\n        s += ref_wrap.fill('\\n{}'.format(ref['title']))\n        s += '\\n\\'{}\\''.format(ref['institution'])\n        s += '\\nTechnical Report {}'.format(ref['number'])\n        s += '\\n{}'.format(ref['year'])\n        if 'doi' in ref:\n            s += '\\n' + ref['doi']\n    elif ref['type'] == 'misc':\n        s += ref_wrap.fill(', '.join(ref['authors'])) + '\\n'\n        s += ref_wrap.fill(ref['title'])\n        if 'note' in ref:\n            s += '\\n' + ref['note']\n        if 'doi' in ref:\n            s += '\\n' + ref['doi']\n    else:\n        raise RuntimeError('Cannot handle reference type {}'.format(ref['type']))\n    return s", "response": "Convert a single reference to plain text format"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds how many spaces to put before a column of numbers so that all the decimal points line up This function takes a column of decimal numbers, and returns a vector containing the number of spaces to place before each number so that (when possible) the decimal points line up. Parameters ---------- column : list Numbers that will be printed as a column point_place : int Number of the character column to put the decimal point", "response": "def _determine_leftpad(column, point_place):\n    '''Find how many spaces to put before a column of numbers\n       so that all the decimal points line up\n\n    This function takes a column of decimal numbers, and returns a\n    vector containing the number of spaces to place before each number\n    so that (when possible) the decimal points line up.\n\n\n    Parameters\n    ----------\n    column : list\n        Numbers that will be printed as a column\n    point_place : int\n        Number of the character column to put the decimal point\n    '''\n\n    # Find the number of digits before the decimal\n    ndigits_left = [_find_point(x) for x in column]\n\n    # find the padding per entry, filtering negative numbers\n    return [max((point_place - 1) - x, 0) for x in ndigits_left]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representing the data for an electron shell", "response": "def electron_shell_str(shell, shellidx=None):\n    '''Return a string representing the data for an electron shell\n\n    If shellidx (index of the shell) is not None, it will also be printed\n    '''\n    am = shell['angular_momentum']\n    amchar = lut.amint_to_char(am)\n    amchar = amchar.upper()\n\n    shellidx_str = ''\n    if shellidx is not None:\n        shellidx_str = 'Index {} '.format(shellidx)\n\n    exponents = shell['exponents']\n    coefficients = shell['coefficients']\n    ncol = len(coefficients) + 1\n\n    point_places = [8 * i + 15 * (i - 1) for i in range(1, ncol + 1)]\n    s = \"Shell: {}Region: {}: AM: {}\\n\".format(shellidx_str, shell['region'], amchar)\n    s += \"Function: {}\\n\".format(shell['function_type'])\n    s += write_matrix([exponents, *coefficients], point_places)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representing the data for an ECP potential", "response": "def ecp_pot_str(pot):\n    '''Return a string representing the data for an ECP potential\n    '''\n\n    am = pot['angular_momentum']\n    amchar = lut.amint_to_char(am)\n\n    rexponents = pot['r_exponents']\n    gexponents = pot['gaussian_exponents']\n    coefficients = pot['coefficients']\n\n    point_places = [0, 10, 33]\n    s = 'Potential: {} potential\\n'.format(amchar)\n    s += 'Type: {}\\n'.format(pot['ecp_type'])\n    s += write_matrix([rexponents, gexponents, *coefficients], point_places)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string with all data for an element in the tree", "response": "def element_data_str(z, eldata):\n    '''Return a string with all data for an element\n\n    This includes shell and ECP potential data\n\n    Parameters\n    ----------\n    z : int or str\n        Element Z-number\n    eldata: dict\n        Data for the element to be printed\n    '''\n\n    sym = lut.element_sym_from_Z(z, True)\n\n    cs = contraction_string(eldata)\n    if cs == '':\n        cs = '(no electron shells)'\n    s = '\\nElement: {} : {}\\n'.format(sym, cs)\n\n    if 'electron_shells' in eldata:\n        for shellidx, shell in enumerate(eldata['electron_shells']):\n            s += electron_shell_str(shell, shellidx) + '\\n'\n\n    if 'ecp_potentials' in eldata:\n        s += 'ECP: Element: {}   Number of electrons: {}\\n'.format(sym, eldata['ecp_electrons'])\n\n        for pot in eldata['ecp_potentials']:\n            s += ecp_pot_str(pot) + '\\n'\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a component basis set as a string.", "response": "def component_basis_str(basis, elements=None):\n    '''Print a component basis set\n\n    If elements is not None, only the specified elements will be printed\n    (see :func:`bse.misc.expand_elements`)\n    '''\n\n    s = \"Description: \" + basis['description'] + '\\n'\n\n    eldata = basis['elements']\n\n    # Filter to the given elements\n    if elements is None:\n        elements = list(eldata.keys())\n    else:\n        elements = expand_elements(elements, True)\n\n    # Add the str for each element\n    for z in elements:\n        s += element_data_str(z, eldata[z]) + '\\n'\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a basis set to Molpro format", "response": "def write_molpro(basis):\n    '''Converts a basis set to Molpro format\n    '''\n\n    # Uncontract all, and make as generally-contracted as possible\n    basis = manip.uncontract_spdf(basis, 0, True)\n    basis = manip.make_general(basis, False)\n    basis = sort.sort_basis(basis, True)\n\n    s = ''\n\n    # Elements for which we have electron basis\n    electron_elements = [k for k, v in basis['elements'].items() if 'electron_shells' in v]\n\n    # Elements for which we have ECP\n    ecp_elements = [k for k, v in basis['elements'].items() if 'ecp_potentials' in v]\n\n    if len(electron_elements) > 0:\n        # basis set starts with a string\n        s += 'basis={\\n'\n\n        # Electron Basis\n        for z in electron_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z).upper()\n            s += '!\\n'\n            s += '! {:20} {}\\n'.format(lut.element_name_from_Z(z), misc.contraction_string(data))\n\n            for shell in data['electron_shells']:\n                exponents = shell['exponents']\n                coefficients = shell['coefficients']\n\n                am = shell['angular_momentum']\n                amchar = lut.amint_to_char(am).lower()\n                s += '{}, {} , {}\\n'.format(amchar, sym, ', '.join(exponents))\n                for c in coefficients:\n                    first, last = find_range(c)\n                    s += 'c, {}.{}, {}\\n'.format(first + 1, last + 1, ', '.join(c[first:last + 1]))\n        s += '}\\n'\n\n    # Write out ECP\n    if len(ecp_elements) > 0:\n        s += '\\n\\n! Effective core Potentials\\n'\n\n        for z in ecp_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z).lower()\n            max_ecp_am = max([x['angular_momentum'][0] for x in data['ecp_potentials']])\n\n            # Sort lowest->highest, then put the highest at the beginning\n            ecp_list = sorted(data['ecp_potentials'], key=lambda x: x['angular_momentum'])\n            ecp_list.insert(0, ecp_list.pop())\n\n            s += 'ECP, {}, {}, {} ;\\n'.format(sym, data['ecp_electrons'], max_ecp_am)\n\n            for pot in ecp_list:\n                rexponents = pot['r_exponents']\n                gexponents = pot['gaussian_exponents']\n                coefficients = pot['coefficients']\n\n                am = pot['angular_momentum']\n                amchar = lut.amint_to_char(am).lower()\n                s += '{};'.format(len(rexponents))\n\n                if am[0] == max_ecp_am:\n                    s += ' !  ul potential\\n'\n                else:\n                    s += ' !  {}-ul potential\\n'.format(amchar)\n\n                for p in range(len(rexponents)):\n                    s += '{},{},{};\\n'.format(rexponents[p], gexponents[p], coefficients[0][p])\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the basis set data into a string representing", "response": "def convert_basis(basis_dict, fmt, header=None):\n    '''\n    Returns the basis set data as a string representing\n    the data in the specified output format\n    '''\n\n    # make converters case insensitive\n    fmt = fmt.lower()\n    if fmt not in _converter_map:\n        raise RuntimeError('Unknown basis set format \"{}\"'.format(fmt))\n\n    converter = _converter_map[fmt]\n\n    # Determine if the converter supports all the types in the basis_dict\n    if converter['valid'] is not None:\n        ftypes = set(basis_dict['function_types'])\n        if ftypes > converter['valid']:\n            raise RuntimeError('Converter {} does not support all function types: {}'.format(fmt, str(ftypes)))\n\n    # Actually do the conversion\n    ret_str = converter['function'](basis_dict)\n\n    if header is not None and fmt != 'json':\n        comment_str = _converter_map[fmt]['comment']\n        header_str = comment_str + comment_str.join(header.splitlines(True))\n        ret_str = header_str + '\\n\\n' + ret_str\n\n    # HACK - Psi4 requires the first non-comment line be spherical/cartesian\n    #        so we have to add that before the header\n    if fmt == 'psi4':\n        types = basis_dict['function_types']\n        harm_type = 'spherical' if 'spherical_gto' in types else 'cartesian'\n        ret_str = harm_type + '\\n\\n' + ret_str\n\n    return ret_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_formats(function_types=None):\n    '''\n    Returns the available formats mapped to display name.\n\n    This is returned as an ordered dictionary, with the most common\n    at the top, followed by the rest in alphabetical order\n\n    If a list is specified for function_types, only those formats\n    supporting the given function types will be returned.\n    '''\n\n    if function_types is None:\n        return {k: v['display'] for k, v in _converter_map.items()}\n\n    ftypes = [x.lower() for x in function_types]\n    ftypes = set(ftypes)\n    ret = []\n\n    for fmt, v in _converter_map.items():\n        if v['valid'] is None or ftypes <= v['valid']:\n            ret.append(fmt)\n    return ret", "response": "Returns the available formats mapped to display name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the recommended extension for a given format", "response": "def get_format_extension(fmt):\n    '''\n    Returns the recommended extension for a given format\n    '''\n\n    if fmt is None:\n        return 'dict'\n\n    fmt = fmt.lower()\n    if fmt not in _converter_map:\n        raise RuntimeError('Unknown basis set format \"{}\"'.format(fmt))\n\n    return _converter_map[fmt]['extension']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a DOT graph file of the files included in a basis set.", "response": "def _make_graph(bsname, version=None, data_dir=None):\n    '''\n    Create a DOT graph file of the files included in a basis set\n    '''\n\n    if not graphviz_avail:\n        raise RuntimeError(\"graphviz package is not installed\")\n\n    data_dir = api.fix_data_dir(data_dir)\n\n    md = api._get_basis_metadata(bsname, data_dir)\n\n    if version is None:\n        version = md['latest_version']\n    else:\n        version = str(version)\n\n    if not version in md['versions']:\n        raise RuntimeError(\"Version {} of {} doesn't exist\".format(version, bsname))\n\n    gr = graphviz.Digraph(comment='Basis Set Graph: ' + bsname)\n\n    # Read the table file\n    table_path = os.path.join(data_dir, md['versions'][version]['file_relpath'])\n    table_data = fileio.read_json_basis(table_path)\n\n    table_edges = {}\n    for el, entry in table_data['elements'].items():\n        if entry not in table_edges:\n            table_edges[entry] = []\n        table_edges[entry].append(el)\n\n    for k, v in table_edges.items():\n        gr.edge(bsname, k, label=compact_elements(v))\n\n    # Element file\n    for elfile in table_edges.keys():\n        element_path = os.path.join(data_dir, elfile)\n        element_data = fileio.read_json_basis(element_path)\n\n        element_edges = {}\n\n        for el, components in element_data['elements'].items():\n            components = components['components']\n            components_str = '\\n'.join(components)\n\n            # skip if this element for the table basis doesn't come from this file\n            if el not in table_data['elements']:\n                continue\n            if table_data['elements'][el] != elfile:\n                continue\n\n            if components_str not in element_edges:\n                element_edges[components_str] = []\n            element_edges[components_str].append(el)\n\n        for k, v in element_edges.items():\n            if len(v):\n                gr.edge(elfile, k, label=compact_elements(v))\n\n    return gr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_library_citation():\n    '''Return a descriptive string and reference data for what users of the library should cite'''\n\n    all_ref_data = api.get_reference_data()\n    lib_refs_data = {k: all_ref_data[k] for k in _lib_refs}\n    return (_lib_refs_desc, lib_refs_data)", "response": "Return a descriptive string and reference data for what users of the library should cite"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat a list of lines into a simple column output.", "response": "def format_columns(lines, prefix=''):\n    '''\n    Create a simple column output\n\n    Parameters\n    ----------\n    lines : list\n        List of lines to format. Each line is a tuple/list with each\n        element corresponding to a column\n    prefix : str\n        Characters to insert at the beginning of each line\n\n    Returns\n    -------\n    str\n        Columnated output as one big string\n    '''\n    if len(lines) == 0:\n        return ''\n\n    ncols = 0\n    for l in lines:\n        ncols = max(ncols, len(l))\n\n    if ncols == 0:\n        return ''\n\n    # We only find the max strlen for all but the last col\n    maxlen = [0] * (ncols - 1)\n    for l in lines:\n        for c in range(ncols - 1):\n            maxlen[c] = max(maxlen[c], len(l[c]))\n\n    fmtstr = prefix + '  '.join(['{{:{x}}}'.format(x=x) for x in maxlen])\n    fmtstr += '  {}'\n    return [fmtstr.format(*l) for l in lines]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_turbomole(basis_lines, fname):\n    '''Reads turbomole-formatted file data and converts it to a dictionary with the\n       usual BSE fields\n\n       Note that the turbomole format does not store all the fields we\n       have, so some fields are left blank\n    '''\n\n    skipchars = '*#$'\n    basis_lines = [l for l in basis_lines if l and not l[0] in skipchars]\n\n    bs_data = create_skel('component')\n\n    i = 0\n    while i < len(basis_lines):\n        line = basis_lines[i]\n        elementsym = line.split()[0]\n\n        element_Z = lut.element_Z_from_sym(elementsym)\n        element_Z = str(element_Z)\n\n        if not element_Z in bs_data['elements']:\n            bs_data['elements'][element_Z] = {}\n\n        element_data = bs_data['elements'][element_Z]\n\n        if \"ecp\" in line.lower():\n            if not 'ecp_potentials' in element_data:\n                element_data['ecp_potentials'] = []\n\n            i += 1\n            line = basis_lines[i]\n\n            lsplt = line.split('=')\n            maxam = int(lsplt[2])\n            n_elec = int(lsplt[1].split()[0])\n\n            amlist = [maxam]\n            amlist.extend(list(range(0, maxam)))\n\n            i += 1\n            for shell_am in amlist:\n                shell_am2 = lut.amchar_to_int(basis_lines[i][0])[0]\n                if shell_am2 != shell_am:\n                    raise RuntimeError(\"AM not in expected order?\")\n\n                i += 1\n\n                ecp_shell = {\n                    'ecp_type': 'scalar_ecp',\n                    'angular_momentum': [shell_am],\n                }\n                ecp_exponents = []\n                ecp_rexponents = []\n                ecp_coefficients = []\n\n                while i < len(basis_lines) and basis_lines[i][0].isalpha() is False:\n                    lsplt = basis_lines[i].split()\n                    ecp_exponents.append(lsplt[2])\n                    ecp_rexponents.append(int(lsplt[1]))\n                    ecp_coefficients.append(lsplt[0])\n                    i += 1\n\n                ecp_shell['r_exponents'] = ecp_rexponents\n                ecp_shell['gaussian_exponents'] = ecp_exponents\n                ecp_shell['coefficients'] = [ecp_coefficients]\n                element_data['ecp_potentials'].append(ecp_shell)\n\n            element_data['ecp_electrons'] = n_elec\n\n        else:\n            if not 'electron_shells' in element_data:\n                element_data['electron_shells'] = []\n\n            i += 1\n            while i < len(basis_lines) and basis_lines[i][0].isalpha() == False:\n                lsplt = basis_lines[i].split()\n                shell_am = lut.amchar_to_int(lsplt[1])\n                nprim = int(lsplt[0])\n\n                if max(shell_am) <= 1:\n                    func_type = 'gto'\n                else:\n                    func_type = 'gto_spherical'\n\n                shell = {\n                    'function_type': func_type,\n                    'region': '',\n                    'angular_momentum': shell_am\n                }\n\n                exponents = []\n                coefficients = []\n\n                i += 1\n                for j in range(nprim):\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    exponents.append(lsplt[0])\n                    coefficients.append(lsplt[1:])\n                    i += 1\n\n                shell['exponents'] = exponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                element_data['electron_shells'].append(shell)\n\n    return bs_data", "response": "Reads turbomole - formatted file data and converts it to a dictionary with the usual BSE fields we have"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_nwchem(basis):\n    '''Converts a basis set to NWChem format\n    '''\n\n    # Uncontract all but SP\n    basis = manip.uncontract_spdf(basis, 1, True)\n    basis = sort.sort_basis(basis, True)\n\n    s = ''\n\n    # Elements for which we have electron basis\n    electron_elements = [k for k, v in basis['elements'].items() if 'electron_shells' in v]\n\n    # Elements for which we have ECP\n    ecp_elements = [k for k, v in basis['elements'].items() if 'ecp_potentials' in v]\n\n    if len(electron_elements) > 0:\n        # basis set starts with a string\n        s += 'BASIS \"ao basis\" PRINT\\n'\n\n        # Electron Basis\n        for z in electron_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z, True)\n            s += '#BASIS SET: {}\\n'.format(misc.contraction_string(data))\n\n            for shell in data['electron_shells']:\n                exponents = shell['exponents']\n                coefficients = shell['coefficients']\n                ncol = len(coefficients) + 1\n\n                am = shell['angular_momentum']\n                amchar = lut.amint_to_char(am).upper()\n                s += '{}    {}\\n'.format(sym, amchar)\n\n                point_places = [8 * i + 15 * (i - 1) for i in range(1, ncol + 1)]\n                s += printing.write_matrix([exponents, *coefficients], point_places)\n\n        s += 'END\\n'\n\n    # Write out ECP\n    if len(ecp_elements) > 0:\n        s += '\\n\\nECP\\n'\n\n        for z in ecp_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z, True)\n            max_ecp_am = max([x['angular_momentum'][0] for x in data['ecp_potentials']])\n\n            # Sort lowest->highest, then put the highest at the beginning\n            ecp_list = sorted(data['ecp_potentials'], key=lambda x: x['angular_momentum'])\n            ecp_list.insert(0, ecp_list.pop())\n\n            s += '{} nelec {}\\n'.format(sym, data['ecp_electrons'])\n\n            for pot in ecp_list:\n                rexponents = pot['r_exponents']\n                gexponents = pot['gaussian_exponents']\n                coefficients = pot['coefficients']\n\n                am = pot['angular_momentum']\n                amchar = lut.amint_to_char(am).upper()\n\n                if am[0] == max_ecp_am:\n                    s += '{} ul\\n'.format(sym)\n                else:\n                    s += '{} {}\\n'.format(sym, amchar)\n\n                point_places = [0, 10, 33]\n                s += printing.write_matrix([rexponents, gexponents, *coefficients], point_places)\n\n        s += 'END\\n'\n\n    return s", "response": "Converts a basis set to NWChem format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contraction_string(element):\n\n    # Does not have electron shells (ECP only?)\n    if 'electron_shells' not in element:\n        return \"\"\n\n    cont_map = dict()\n    for sh in element['electron_shells']:\n        nprim = len(sh['exponents'])\n        ngeneral = len(sh['coefficients'])\n\n        # is a combined general contraction (sp, spd, etc)\n        is_spdf = len(sh['angular_momentum']) > 1\n\n        for am in sh['angular_momentum']:\n            # If this a general contraction (and not combined am), then use that\n            ncont = ngeneral if not is_spdf else 1\n\n            if am not in cont_map:\n                cont_map[am] = (nprim, ncont)\n            else:\n                cont_map[am] = (cont_map[am][0] + nprim, cont_map[am][1] + ncont)\n\n    primstr = \"\"\n    contstr = \"\"\n    for am in sorted(cont_map.keys()):\n        nprim, ncont = cont_map[am]\n\n        if am != 0:\n            primstr += ','\n            contstr += ','\n        primstr += str(nprim) + lut.amint_to_char([am])\n        contstr += str(ncont) + lut.amint_to_char([am])\n\n    return \"({}) -> [{}]\".format(primstr, contstr)", "response": "Returns a string specifying the contractions for an element ie 4s 10p 4s and 3p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compact_elements(elements):\n\n    if len(elements) == 0:\n        return\n\n    # We have to convert to integers for this function\n    elements = [int(el) for el in elements]\n\n    # Just to be safe, sort the list\n    el = sorted(set(elements))\n\n    ranges = []\n    i = 0\n    while i < len(el):\n        start_el = el[i]\n        end_el = start_el\n\n        i += 1\n        while i < len(el):\n            if el[i] != end_el + 1:\n                break\n\n            end_el += 1\n            i += 1\n\n        if start_el == end_el:\n            ranges.append([start_el])\n        else:\n            ranges.append([start_el, end_el])\n\n    # Convert to elemental symbols\n    range_strs = []\n    for r in ranges:\n        sym = lut.element_sym_from_Z(r[0], True)\n\n        if len(r) == 1:\n            range_strs.append(sym)\n        elif len(r) == 2 and r[1] == r[0] + 1:\n            sym2 = lut.element_sym_from_Z(r[1], True)\n            range_strs.append(sym + \",\" + sym2)\n        else:\n            sym2 = lut.element_sym_from_Z(r[1], True)\n            range_strs.append(sym + \"-\" + sym2)\n\n    return \",\".join(range_strs)", "response": "Create a string that contains a list of element numbers and ranges given a list of element numbers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpanding a list of compacted elements into a single integer or a list of strings.", "response": "def expand_elements(compact_el, as_str=False):\n    \"\"\"\n    Create a list of integers given a string or list of compacted elements\n\n    This is partly the opposite of compact_elements, but is more flexible.\n\n    compact_el can be a list or a string. If compact_el is a list, each element is processed individually\n    as a string (meaning list elements can contain commas, ranges, etc)\n    If compact_el is a string, it is split by commas and then each section is processed.\n\n    In all cases, element symbols (case insensitive) and Z numbers (as integers or strings)\n    can be used interchangeably. Ranges are also allowed in both lists and strings.\n\n    Some examples:\n        \"H-Li,C-O,Ne\" will return [1, 2, 3, 6, 7, 8, 10]\n        \"H-N,8,Na-12\" will return [1, 2, 3, 4, 5, 6, 7, 8, 11, 12]\n        ['C', 'Al-15,S', 17, '18'] will return [6, 13, 14, 15, 16, 17, 18]\n\n    If as_str is True, the list will contain strings of the integers\n    (ie, the first example above will return ['1', '2', '3', '6', '7', '8', '10']\n    \"\"\"\n\n    # If an integer, just return it\n    if isinstance(compact_el, int):\n        if as_str is True:\n            return [str(compact_el)]\n        else:\n            return [compact_el]\n\n    # If compact_el is a list, make it a comma-separated string\n    if isinstance(compact_el, list):\n        compact_el = [str(x) for x in compact_el]\n        compact_el = [x for x in compact_el if len(x) > 0]\n        compact_el = ','.join(compact_el)\n\n    # Find multiple - or ,\n    # Also replace all whitespace with spaces\n    compact_el = re.sub(r',+', ',', compact_el)\n    compact_el = re.sub(r'-+', '-', compact_el)\n    compact_el = re.sub(r'\\s+', '', compact_el)\n\n    # Find starting with or ending with comma and strip them\n    compact_el = compact_el.strip(',')\n\n    # Check if I was passed an empty string or list\n    if len(compact_el) == 0:\n        return []\n\n    # Find some erroneous patterns\n    # -, and ,-\n    if '-,' in compact_el:\n        raise RuntimeError(\"Malformed element string\")\n    if ',-' in compact_el:\n        raise RuntimeError(\"Malformed element string\")\n\n    # Strings ends or begins with -\n    if compact_el.startswith('-') or compact_el.endswith('-'):\n        raise RuntimeError(\"Malformed element string\")\n\n    # x-y-z\n    if re.search(r'\\w+-\\w+-\\w+', compact_el):\n        raise RuntimeError(\"Malformed element string\")\n\n    # Split on commas\n    tmp_list = compact_el.split(',')\n\n    # Now go over each one and replace elements with ints\n    el_list = []\n    for el in tmp_list:\n        if not '-' in el:\n            el_list.append(_Z_from_str(el))\n        else:\n            begin, end = el.split('-')\n            begin = _Z_from_str(begin)\n            end = _Z_from_str(end)\n            el_list.extend(list(range(begin, end + 1)))\n\n    if as_str is True:\n        return [str(x) for x in el_list]\n    else:\n        return el_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef elements_in_files(filelist):\n    '''Get a list of what elements exist in JSON files\n\n    This works on table, element, and component data files\n\n    Parameters\n    ----------\n    filelist : list\n        A list of paths to json files\n\n    Returns\n    -------\n    dict\n        Keys are the file path, value is a compacted element string of\n        what elements are in that file \n    '''\n\n    ret = {}\n    for fpath in filelist:\n        filedata = fileio.read_json_basis(fpath)\n        els = list(filedata['elements'].keys())\n        ret[fpath] = misc.compact_elements(els)\n\n    return ret", "response": "Get a list of what elements exist in JSON files\n    This works on table element and component data files\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of what elements and refrences exist in component JSON files.", "response": "def component_file_refs(filelist):\n    '''Get a list of what elements/refrences exist in component JSON files\n\n    Parameters\n    ----------\n    filelist : list\n        A list of paths to json files\n\n    Returns\n    -------\n    dict\n        Keys are the file path, value is a list of tuples (compacted element string, refs tuple)\n    '''\n\n    ret = {}\n    for fpath in filelist:\n        filedata = fileio.read_json_basis(fpath)\n\n        refdict = {}\n        for el, eldata in filedata['elements'].items():\n            refs = tuple(eldata['references'])\n            if not refs in refdict:\n                refdict[refs] = [el]\n            else:\n                refdict[refs].append(el)\n\n        entry = []\n        for k, v in refdict.items():\n            entry.append((misc.compact_elements(v), k))\n\n        ret[fpath] = entry\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nforcing the contraction coefficient of uncontracted shells to 1. 0", "response": "def _fix_uncontracted(basis):\n    '''\n    Forces the contraction coefficient of uncontracted shells to 1.0\n    '''\n\n    for el in basis['elements'].values():\n        if 'electron_shells' not in el:\n            continue\n\n        for sh in el['electron_shells']:\n            if len(sh['coefficients']) == 1 and len(sh['coefficients'][0]) == 1:\n                sh['coefficients'][0][0] = '1.0000000'\n\n            # Some uncontracted shells don't have a coefficient\n            if len(sh['coefficients']) == 0:\n                sh['coefficients'].append(['1.0000000'])\n\n    return basis"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a basis set to BSE Debug format", "response": "def write_bsedebug(basis):\n    '''Converts a basis set to BSE Debug format\n    '''\n\n    s = ''\n\n    for el, eldata in basis['elements'].items():\n        s += element_data_str(el, eldata)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bsecurate_cli_get_reader_formats(args):\n    '''Handles the get-file-types subcommand'''\n\n    all_formats = curate.get_reader_formats()\n\n    if args.no_description:\n        liststr = all_formats.keys()\n    else:\n        liststr = format_columns(all_formats.items())\n\n    return '\\n'.join(liststr)", "response": "Handles the get - file - types subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the elements - in - files subcommand", "response": "def _bsecurate_cli_elements_in_files(args):\n    '''Handles the elements-in-files subcommand'''\n    data = curate.elements_in_files(args.files)\n    return '\\n'.join(format_columns(data.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _bsecurate_cli_component_file_refs(args):\n    '''Handles the component-file-refs subcommand'''\n    data = curate.component_file_refs(args.files)\n\n    s = ''\n\n    for cfile, cdata in data.items():\n        s += cfile + '\\n'\n        rows = []\n        for el, refs in cdata:\n            rows.append(('    ' + el, ' '.join(refs)))\n        s += '\\n'.join(format_columns(rows)) + '\\n\\n'\n\n    return s", "response": "Handles the component - file - refs subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _bsecurate_cli_print_component_file(args):\n    '''Handles the print-component-file subcommand'''\n\n    data = fileio.read_json_basis(args.file)\n    return printing.component_basis_str(data, elements=args.elements)", "response": "Handles the print - component - file subcommand"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _bsecurate_cli_compare_basis_sets(args):\n    '''Handles compare-basis-sets subcommand'''\n    ret = curate.compare_basis_sets(args.basis1, args.basis2, args.version1, args.version2, args.uncontract_general,\n          args.data_dir, args.data_dir)\n    if ret:\n        return \"No difference found\"\n    else:\n        return \"DIFFERENCES FOUND. SEE ABOVE\"", "response": "Handles compare - basis - sets subcommand"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles compare - basis - files subcommand", "response": "def _bsecurate_cli_compare_basis_files(args):\n    '''Handles compare-basis-files subcommand'''\n    ret = curate.compare_basis_files(args.file1, args.file2, args.readfmt1, args.readfmt2, args.uncontract_general)\n\n    if ret:\n        return \"No difference found\"\n    else:\n        return \"DIFFERENCES FOUND. SEE ABOVE\""}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the view - graph subcommand", "response": "def _bsecurate_cli_view_graph(args):\n    '''Handles the view-graph subcommand'''\n\n    curate.view_graph(args.basis, args.version, args.data_dir)\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bsecurate_cli_make_graph_file(args):\n    '''Handles the make-graph-file subcommand'''\n\n    curate.make_graph_file(args.basis, args.outfile, args.render, args.version, args.data_dir)\n    return ''", "response": "Handles the make - graph - file subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef element_data_from_Z(Z):\n    '''Obtain elemental data given a Z number\n\n    An exception is thrown if the Z number is not found\n    '''\n\n    # Z may be a str\n    if isinstance(Z, str) and Z.isdecimal():\n        Z = int(Z)\n\n    if Z not in _element_Z_map:\n        raise KeyError('No element data for Z = {}'.format(Z))\n    return _element_Z_map[Z]", "response": "Obtain elemental data given a Z number\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef element_data_from_sym(sym):\n    '''Obtain elemental data given an elemental symbol\n\n    The given symbol is not case sensitive\n\n    An exception is thrown if the symbol is not found\n    '''\n\n    sym_lower = sym.lower()\n    if sym_lower not in _element_sym_map:\n        raise KeyError('No element data for symbol \\'{}\\''.format(sym))\n    return _element_sym_map[sym_lower]", "response": "Obtain elemental data given an elemental symbol"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nobtains elemental data given an elemental name", "response": "def element_data_from_name(name):\n    '''Obtain elemental data given an elemental name\n\n    The given name is not case sensitive\n\n    An exception is thrown if the name is not found\n    '''\n\n    name_lower = name.lower()\n    if name_lower not in _element_name_map:\n        raise KeyError('No element data for name \\'{}\\''.format(name))\n    return _element_name_map[name_lower]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nobtain an element s name from its Z number", "response": "def element_name_from_Z(Z, normalize=False):\n    '''Obtain an element's name from its Z number\n\n    An exception is thrown if the Z number is not found\n\n    If normalize is True, the first letter will be capitalized\n    '''\n\n    r = element_data_from_Z(Z)[2]\n    if normalize:\n        return r.capitalize()\n    else:\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef element_sym_from_Z(Z, normalize=False):\n    '''Obtain an element's symbol from its Z number\n\n    An exception is thrown if the Z number is not found\n\n    If normalize is True, the first letter will be capitalized\n    '''\n\n    r = element_data_from_Z(Z)[0]\n    if normalize:\n        return r.capitalize()\n    else:\n        return r", "response": "Obtain an element s symbol from its Z number"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef amint_to_char(am, hij=False, use_L=False):\n    '''Convert an angular momentum integer to a character\n\n    The input is a list (to handle sp, spd, ... orbitals). The return\n    value is a string\n\n    For example, converts [0] to 's' and [0,1,2] to 'spd'\n\n    If hij is True, the ordering spdfghijkl is used. Otherwise, the\n    ordering will be spdfghikl (skipping j)\n\n    If use_L is True, sp shells ([0,1]) will return l instead\n    '''\n\n    if use_L and am == [0, 1]:\n        return 'l'\n    if hij:\n        amchar_map = _amchar_map_hij\n    else:\n        amchar_map = _amchar_map_hik\n\n    amchar = []\n\n    for a in am:\n        if a < 0:\n            raise IndexError('Angular momentum must be a positive integer (not {})'.format(a))\n        if a >= len(amchar_map):\n            raise IndexError('Angular momentum {} out of range. Must be less than {}'.format(a, len(amchar_map)))\n        amchar.append(amchar_map[a])\n\n    return ''.join(amchar)", "response": "Convert an angular momentum integer to a character."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an angular momentum character to an integer", "response": "def amchar_to_int(amchar, hij=False):\n    '''Convert an angular momentum integer to a character\n\n    The return value is a list of integers (to handle sp, spd, ... orbitals)\n\n    For example, converts 'p' to [1] and 'sp' to [0,1]\n\n    If hij is True, the ordering spdfghijkl is used. Otherwise, the\n    ordering will be spdfghikl (skipping j)\n    '''\n\n    if hij:\n        amchar_map = _amchar_map_hij\n    else:\n        amchar_map = _amchar_map_hik\n\n    amchar_lower = amchar.lower()\n\n    amint = []\n\n    for c in amchar_lower:\n        if c not in amchar_map:\n            raise KeyError('Angular momentum character {} is not valid'.format(c))\n\n        amint.append(amchar_map.index(c))\n\n    return amint"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_references(ref_data, fmt):\n    '''\n    Returns the basis set references as a string representing\n    the data in the specified output format\n    '''\n\n    # Make fmt case insensitive\n    fmt = fmt.lower()\n    if fmt not in _converter_map:\n        raise RuntimeError('Unknown reference format \"{}\"'.format(fmt))\n\n    # Sort the data for all references\n    for elref in ref_data:\n        for rinfo in elref['reference_info']:\n            rdata = rinfo['reference_data']\n            rinfo['reference_data'] = [(k, sort.sort_single_reference(v)) for k, v in rdata]\n\n    # Actually do the conversion\n    ret_str = _converter_map[fmt]['function'](ref_data)\n\n    return ret_str", "response": "Converts the data in the specified format to the basis set references as a string representing\n    the data in the specified output format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_nwchem(basis_lines, fname):\n    '''Reads NWChem-formatted file data and converts it to a dictionary with the\n       usual BSE fields\n\n       Note that the nwchem format does not store all the fields we\n       have, so some fields are left blank\n    '''\n\n    skipchars = '#'\n    basis_lines = [l for l in basis_lines if l and not l[0] in skipchars]\n\n    bs_data = create_skel('component')\n\n    i = 0\n\n    while i < len(basis_lines):\n        line = basis_lines[i]\n\n        if line.lower().startswith('basis'):\n            i += 1\n\n            # NWChem doesn't seem to really block by element\n            # It just has shells labeled with each element symbol\n\n            while i < len(basis_lines) and not basis_lines[i].lower().startswith('end'):\n                line = basis_lines[i]\n                lsplt = line.split()\n                elementsym = lsplt[0]\n                shell_am = lut.amchar_to_int(lsplt[1])\n\n                element_Z = lut.element_Z_from_sym(elementsym)\n                element_Z = str(element_Z)\n\n                if not element_Z in bs_data['elements']:\n                    bs_data['elements'][element_Z] = {}\n                if not 'electron_shells' in bs_data['elements'][element_Z]:\n                    bs_data['elements'][element_Z]['electron_shells'] = []\n\n                element_data = bs_data['elements'][element_Z]\n\n                if max(shell_am) <= 1:\n                    func_type = 'gto'\n                else:\n                    func_type = 'gto_spherical'\n\n                shell = {\n                    'function_type': func_type,\n                    'region': '',\n                    'angular_momentum': shell_am\n                }\n\n                exponents = []\n                coefficients = []\n\n                i += 1\n                while True:\n                    if i >= len(basis_lines) or basis_lines[i][0].isalpha():\n                        break\n\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    exponents.append(lsplt[0])\n                    coefficients.append(lsplt[1:])\n                    i += 1\n\n                shell['exponents'] = exponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                element_data['electron_shells'].append(shell)\n\n        elif line.lower().startswith('ecp'):\n            i += 1\n            while i < len(basis_lines) and not basis_lines[i].lower().startswith('end'):\n                line = basis_lines[i]\n                if 'nelec' in line.lower():\n                    lsplt = line.split()\n                    elementsym = lsplt[0]\n                    n_elec = int(lsplt[2])\n\n                    element_Z = lut.element_Z_from_sym(elementsym)\n                    element_Z = str(element_Z)\n\n                    if not element_Z in bs_data['elements']:\n                        bs_data['elements'][element_Z] = {}\n                    if not 'ecp_electrons' in bs_data['elements'][element_Z]:\n                        bs_data['elements'][element_Z]['ecp_electrons'] = n_elec\n\n                    i += 1\n                    continue\n\n                # Now parsing a shell\n                lsplt = line.split()\n                elementsym = lsplt[0]\n\n                shell_am = lsplt[1]\n                if shell_am.lower() == 'ul':\n                    shell_am = -1  # Placeholder - leave for later\n                else:\n                    shell_am = lut.amchar_to_int(lsplt[1])\n\n                element_Z = lut.element_Z_from_sym(elementsym)\n                element_Z = str(element_Z)\n\n                if not element_Z in bs_data['elements']:\n                    bs_data['elements'][element_Z] = {}\n                if not 'ecp_potentials' in bs_data['elements'][element_Z]:\n                    bs_data['elements'][element_Z]['ecp_potentials'] = []\n                element_data = bs_data['elements'][element_Z]\n\n                ecp_shell = {'angular_momentum': shell_am, 'ecp_type': 'scalar_ecp'}\n\n                rexponents = []\n                gexponents = []\n                coefficients = []\n\n                i += 1\n                while True:\n                    if i >= len(basis_lines) or basis_lines[i][0].isalpha():\n                        break\n\n                    line = basis_lines[i].replace('D', 'E')\n                    line = line.replace('d', 'E')\n                    lsplt = line.split()\n                    rexponents.append(int(lsplt[0]))\n                    gexponents.append(lsplt[1])\n                    coefficients.append(lsplt[2:])\n                    i += 1\n\n                ecp_shell['r_exponents'] = rexponents\n                ecp_shell['gaussian_exponents'] = gexponents\n\n                # We need to transpose the coefficient matrix\n                # (we store a matrix with primitives being the column index and\n                # general contraction being the row index)\n                ecp_shell['coefficients'] = list(map(list, zip(*coefficients)))\n\n                element_data['ecp_potentials'].append(ecp_shell)\n        i += 1\n\n        # Fix ecp angular momentum now that everything has been read\n        for el, v in bs_data['elements'].items():\n            if not 'ecp_potentials' in v:\n                continue\n\n            max_ecp_am = -1\n            for s in v['ecp_potentials']:\n                if s['angular_momentum'] == -1:\n                    continue\n                max_ecp_am = max(max_ecp_am, max(s['angular_momentum']))\n\n            for s in v['ecp_potentials']:\n                if s['angular_momentum'] == -1:\n                    s['angular_momentum'] = [max_ecp_am + 1]\n\n    return bs_data", "response": "Reads NWChem - formatted file data and converts it to a dictionary with the usual BSE fields we have"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_basis_metadata(name, data_dir):\n    '''Get metadata for a single basis set\n\n    If the basis doesn't exist, an exception is raised\n    '''\n\n    # Transform the name into an internal representation\n    tr_name = misc.transform_basis_name(name)\n\n    # Get the metadata for all basis sets\n    metadata = get_metadata(data_dir)\n\n    if not tr_name in metadata:\n        raise KeyError(\"Basis set {} does not exist\".format(name))\n\n    return metadata[tr_name]", "response": "Get the metadata for a single basis set"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _header_string(basis_dict):\n    '''Creates a header with information about a basis set\n\n    Information includes description, revision, etc, but not references\n    '''\n\n    tw = textwrap.TextWrapper(initial_indent='', subsequent_indent=' ' * 20)\n\n    header = '-' * 70 + '\\n'\n    header += ' Basis Set Exchange\\n'\n    header += ' Version ' + version() + '\\n'\n    header += ' ' + _main_url + '\\n'\n    header += '-' * 70 + '\\n'\n    header += '   Basis set: ' + basis_dict['name'] + '\\n'\n    header += tw.fill(' Description: ' + basis_dict['description']) + '\\n'\n    header += '        Role: ' + basis_dict['role'] + '\\n'\n    header += tw.fill('     Version: {}  ({})'.format(basis_dict['version'],\n                                                      basis_dict['revision_description'])) + '\\n'\n    header += '-' * 70 + '\\n'\n\n    return header", "response": "Creates a header string for a given basis set"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nobtains a basis set This is the main function for getting basis set information. This function reads in all the basis data and returns it either as a string or as a python dictionary. Parameters ---------- name : str Name of the basis set. This is not case sensitive. elements : str or list List of elements that you want the basis set for. Elements can be specified by Z-number (int or str) or by symbol (str). If this argument is a str (ie, '1-3,7-10'), it is expanded into a list. Z numbers and symbols (case insensitive) can be used interchangeably (see :func:`bse.misc.expand_elements`) If an empty string or list is passed, or if None is passed (the default), all elements for which the basis set is defined are included. version : int or str Obtain a specific version of this basis set. By default, the latest version is returned. fmt: str The desired output format of the basis set. By default, basis set information is returned as a python dictionary. Otherwise, if a format is specified, a string is returned. Use :func:`bse.api.get_formats` to programmatically obtain the available formats. The `fmt` argument is not case sensitive. Available formats are * nwchem * gaussian94 * psi4 * gamess_us * turbomole * json uncontract_general : bool If True, remove general contractions by duplicating the set of primitive exponents with each vector of coefficients. Primitives with zero coefficient are removed, as are duplicate shells. uncontract_spdf : bool If True, remove general contractions with combined angular momentum (sp, spd, etc) by duplicating the set of primitive exponents with each vector of coefficients. Primitives with zero coefficient are removed, as are duplicate shells. uncontract_segmented : bool If True, remove segmented contractions by duplicating each primitive into new shells. Each coefficient is set to 1.0 make_general : bool If True, make the basis set as generally-contracted as possible. There will be one shell per angular momentum (for each element) optimize_general : bool Optimize by removing general contractions that contain uncontracted functions (see :func:`bse.manip.optimize_general`) data_dir : str Data directory with all the basis set information. By default, it is in the 'data' subdirectory of this project. Returns ------- str or dict The basis set in the desired format. If `fmt` is **None**, this will be a python dictionary. Otherwise, it will be a string.", "response": "def get_basis(name,\n              elements=None,\n              version=None,\n              fmt=None,\n              uncontract_general=False,\n              uncontract_spdf=False,\n              uncontract_segmented=False,\n              make_general=False,\n              optimize_general=False,\n              data_dir=None,\n              header=True):\n    '''Obtain a basis set\n\n    This is the main function for getting basis set information.\n    This function reads in all the basis data and returns it either\n    as a string or as a python dictionary.\n\n    Parameters\n    ----------\n    name : str\n        Name of the basis set. This is not case sensitive.\n    elements : str or list\n        List of elements that you want the basis set for.\n        Elements can be specified by Z-number (int or str) or by symbol (str).\n        If this argument is a str (ie, '1-3,7-10'), it is expanded into a list.\n        Z numbers and symbols (case insensitive) can be used interchangeably\n        (see :func:`bse.misc.expand_elements`)\n\n        If an empty string or list is passed, or if None is passed (the default),\n        all elements for which the basis set is defined are included.\n    version : int or str\n        Obtain a specific version of this basis set. By default,\n        the latest version is returned.\n    fmt: str\n        The desired output format of the basis set. By default,\n        basis set information is returned as a python dictionary. Otherwise,\n        if a format is specified, a string is returned.\n        Use :func:`bse.api.get_formats` to programmatically obtain the available\n        formats.  The `fmt` argument is not case sensitive.\n\n        Available formats are\n\n            * nwchem\n            * gaussian94\n            * psi4\n            * gamess_us\n            * turbomole\n            * json\n\n    uncontract_general : bool\n        If True, remove general contractions by duplicating the set\n        of primitive exponents with each vector of coefficients.\n        Primitives with zero coefficient are removed, as are duplicate shells.\n    uncontract_spdf : bool\n        If True, remove general contractions with combined angular momentum (sp, spd, etc)\n        by duplicating the set of primitive exponents with each vector of coefficients.\n        Primitives with zero coefficient are removed, as are duplicate shells.\n    uncontract_segmented : bool\n        If True, remove segmented contractions by duplicating each primitive into new shells.\n        Each coefficient is set to 1.0\n    make_general : bool\n        If True, make the basis set as generally-contracted as possible. There will be one\n        shell per angular momentum (for each element)\n    optimize_general : bool\n        Optimize by removing general contractions that contain uncontracted\n        functions (see :func:`bse.manip.optimize_general`)\n    data_dir : str\n        Data directory with all the basis set information. By default,\n        it is in the 'data' subdirectory of this project.\n\n    Returns\n    -------\n    str or dict\n        The basis set in the desired format. If `fmt` is **None**, this will be a python\n        dictionary. Otherwise, it will be a string.\n    '''\n\n    data_dir = fix_data_dir(data_dir)\n    bs_data = _get_basis_metadata(name, data_dir)\n\n    # If version is not specified, use the latest\n    if version is None:\n        version = bs_data['latest_version']\n    else:\n        version = str(version)  # Version may be an int\n\n    if not version in bs_data['versions']:\n        raise KeyError(\"Version {} does not exist for basis {}\".format(version, name))\n\n    # Compose the entire basis set (all elements)\n    file_relpath = bs_data['versions'][version]['file_relpath']\n    basis_dict = compose.compose_table_basis(file_relpath, data_dir)\n\n    # Set the name (from the global metadata)\n    # Only the list of all names will be returned from compose_table_basis\n    basis_dict['name'] = bs_data['display_name']\n\n    # Handle optional arguments\n    if elements is not None:\n        # Convert to purely a list of strings that represent integers\n        elements = misc.expand_elements(elements, True)\n\n        # Did the user pass an empty string or empty list? If so, include\n        # all elements\n        if len(elements) != 0:\n            bs_elements = basis_dict['elements']\n\n            # Are elements part of this basis set?\n            for el in elements:\n                if not el in bs_elements:\n                    elsym = lut.element_sym_from_Z(el)\n                    raise KeyError(\"Element {} (Z={}) not found in basis {} version {}\".format(\n                        elsym, el, name, version))\n\n            # Set to only the elements we want\n            basis_dict['elements'] = {k: v for k, v in bs_elements.items() if k in elements}\n\n    # Note that from now on, the pipleline is going to modify basis_dict. That is ok,\n    # since we are returned a unique instance from compose_table_basis\n\n    needs_pruning = False\n    if optimize_general:\n        basis_dict = manip.optimize_general(basis_dict, False)\n        needs_pruning = True\n\n    # uncontract_segmented implies uncontract_general\n    if uncontract_segmented:\n        basis_dict = manip.uncontract_segmented(basis_dict, False)\n        needs_pruning = True\n\n    elif uncontract_general:\n        basis_dict = manip.uncontract_general(basis_dict, False)\n        needs_pruning = True\n\n    if uncontract_spdf:\n        basis_dict = manip.uncontract_spdf(basis_dict, 0, False)\n        needs_pruning = True\n\n    if make_general:\n        basis_dict = manip.make_general(basis_dict, False)\n        needs_pruning = True\n\n    # Remove dead and duplicate shells\n    if needs_pruning:\n        basis_dict = manip.prune_basis(basis_dict, False)\n\n    # If fmt is not specified, return as a python dict\n    if fmt is None:\n        return basis_dict\n\n    if header:\n        header_str = _header_string(basis_dict)\n    else:\n        header_str = None\n\n    return converters.convert_basis(basis_dict, fmt, header_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nobtains the metadata for all basis sets.", "response": "def get_metadata(data_dir=None):\n    '''Obtain the metadata for all basis sets\n\n    The metadata includes information such as the display name of the basis set,\n    its versions, and what elements are included in the basis set\n\n    The data is read from the METADATA.json file in the `data_dir` directory.\n\n    Parameters\n    ----------\n    data_dir : str\n        Data directory with all the basis set information. By default,\n        it is in the 'data' subdirectory of this project.\n    '''\n\n    data_dir = fix_data_dir(data_dir)\n    metadata_file = os.path.join(data_dir, \"METADATA.json\")\n    return fileio.read_metadata(metadata_file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_reference_data(data_dir=None):\n    '''Obtain information for all stored references\n\n    This is a nested dictionary with all the data for all the references\n\n    The reference data is read from the REFERENCES.json file in the given\n    `data_dir` directory.\n    '''\n\n    data_dir = fix_data_dir(data_dir)\n    reffile_path = os.path.join(data_dir, 'REFERENCES.json')\n\n    return fileio.read_references(reffile_path)", "response": "Obtain information for all stored references\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_references(basis_name, elements=None, version=None, fmt=None, data_dir=None):\n    '''Get the references/citations for a basis set\n\n    Parameters\n    ----------\n    basis_name : str\n        Name of the basis set. This is not case sensitive.\n    elements : list\n        List of element numbers that you want the basis set for. By default,\n        all elements for which the basis set is defined are included.\n    version : int\n        Obtain a specific version of this basis set. By default,\n        the latest version is returned.\n    fmt: str\n        The desired output format of the basis set references. By default,\n        basis set information is returned as a list of dictionaries. Use\n        get_reference_formats() to programmatically obtain the available formats.\n        The `fmt` argument is not case sensitive.\n\n        Available reference formats are\n\n            * bib\n            * txt\n            * json\n\n    data_dir : str\n        Data directory with all the basis set information. By default,\n        it is in the 'data' subdirectory of this project.\n\n    Returns\n    -------\n    str or dict\n        The references for the given basis set in the desired format. If `fmt` is **None**, this will be a python\n        dictionary. Otherwise, it will be a string.\n    '''\n\n    data_dir = fix_data_dir(data_dir)\n    basis_dict = get_basis(basis_name, elements=elements, version=version, data_dir=data_dir)\n\n    all_ref_data = get_reference_data(data_dir)\n    ref_data = references.compact_references(basis_dict, all_ref_data)\n\n    if fmt is None:\n        return ref_data\n\n    return refconverters.convert_references(ref_data, fmt)", "response": "Get the references and citations for a given basis set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_basis_family(basis_name, data_dir=None):\n    '''Lookup a family by a basis set name\n    '''\n\n    data_dir = fix_data_dir(data_dir)\n    bs_data = _get_basis_metadata(basis_name, data_dir)\n    return bs_data['family']", "response": "Lookup a family by a basis set name\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all basis set families", "response": "def get_families(data_dir=None):\n    '''Return a list of all basis set families'''\n    data_dir = fix_data_dir(data_dir)\n    metadata = get_metadata(data_dir)\n\n    families = set()\n    for v in metadata.values():\n        families.add(v['family'])\n\n    return sorted(list(families))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_basis_sets(substr=None, family=None, role=None, data_dir=None):\n    '''Filter basis sets by some criteria\n\n    All parameters are ANDed together and are not case sensitive.\n\n    Parameters\n    ----------\n    substr : str\n        Substring to search for in the basis set name\n    family : str\n        Family the basis set belongs to\n    role : str\n        Role of the basis set\n    data_dir : str\n        Data directory with all the basis set information. By default,\n        it is in the 'data' subdirectory of this project.\n\n    Returns\n    -------\n    dict\n        Basis set metadata that matches the search criteria\n    '''\n\n    data_dir = fix_data_dir(data_dir)\n    metadata = get_metadata(data_dir)\n\n    # family and role are required to be lowercase (via schema and validation functions)\n\n    if family:\n        family = family.lower()\n        if not family in get_families(data_dir):\n            raise RuntimeError(\"Family '{}' is not a valid family\".format(family))\n        metadata = {k: v for k, v in metadata.items() if v['family'] == family}\n    if role:\n        role = role.lower()\n        if not role in get_roles():\n            raise RuntimeError(\"Role '{}' is not a valid role\".format(role))\n        metadata = {k: v for k, v in metadata.items() if v['role'] == role}\n    if substr:\n        substr = substr.lower()\n        metadata = {k: v for k, v in metadata.items() if substr in k or substr in v['display_name']}\n\n    return metadata", "response": "Filter the basis set metadata by some criteria."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nforming a path to the notes for a family", "response": "def _family_notes_path(family, data_dir):\n    '''Form a path to the notes for a family'''\n\n    data_dir = fix_data_dir(data_dir)\n\n    family = family.lower()\n    if not family in get_families(data_dir):\n        raise RuntimeError(\"Family '{}' does not exist\".format(family))\n\n    file_name = 'NOTES.' + family.lower()\n    file_path = os.path.join(data_dir, file_name)\n    return file_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nforming a path to the notes file for a basis set", "response": "def _basis_notes_path(name, data_dir):\n    '''Form a path to the notes for a basis set'''\n\n    data_dir = fix_data_dir(data_dir)\n    bs_data = _get_basis_metadata(name, data_dir)\n\n    # the notes file is the same as the base file name, with a .notes extension\n    filebase = bs_data['basename']\n    file_path = os.path.join(data_dir, filebase + '.notes')\n    return file_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_family_notes(family, data_dir=None):\n    '''Return a string representing the notes about a basis set family\n\n    If the notes are not found, an empty string is returned\n    '''\n\n    file_path = _family_notes_path(family, data_dir)\n    notes_str = fileio.read_notes_file(file_path)\n\n    if notes_str is None:\n        notes_str = \"\"\n\n    ref_data = get_reference_data(data_dir)\n    return notes.process_notes(notes_str, ref_data)", "response": "Return a string representing the notes about a basis set family\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if notes exist for a given family Returns True if they exist False otherwise", "response": "def has_family_notes(family, data_dir=None):\n    '''Check if notes exist for a given family\n\n    Returns True if they exist, false otherwise\n    '''\n\n    file_path = _family_notes_path(family, data_dir)\n    return os.path.isfile(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representing the notes about a specific basis set", "response": "def get_basis_notes(name, data_dir=None):\n    '''Return a string representing the notes about a specific basis set\n\n    If the notes are not found, an empty string is returned\n    '''\n\n    file_path = _basis_notes_path(name, data_dir)\n    notes_str = fileio.read_notes_file(file_path)\n\n    if notes_str is None:\n        return \"\"\n\n    ref_data = get_reference_data(data_dir)\n    return notes.process_notes(notes_str, ref_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if notes exist for a given basis set Returns True if they exist False otherwise", "response": "def has_basis_notes(family, data_dir=None):\n    '''Check if notes exist for a given basis set\n\n    Returns True if they exist, false otherwise\n    '''\n\n    file_path = _basis_notes_path(family, data_dir)\n    return os.path.isfile(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_schema(schema_type):\n    '''Get a schema that can validate BSE JSON files\n\n       The schema_type represents the type of BSE JSON file to be validated,\n       and can be 'component', 'element', 'table', 'metadata', or 'references'.\n    '''\n\n    schema_file = \"{}-schema.json\".format(schema_type)\n    file_path = os.path.join(_default_schema_dir, schema_file)\n\n    if not os.path.isfile(file_path):\n        raise RuntimeError('Schema file \\'{}\\' does not exist, is not readable, or is not a file'.format(file_path))\n\n    return fileio.read_schema(file_path)", "response": "Get a schema that can validate BSE JSON files\natherMessages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cli_check_data_dir(data_dir):\n    '''Checks that the data dir exists and contains METADATA.json'''\n\n    if data_dir is None:\n        return None\n\n    data_dir = os.path.expanduser(data_dir)\n    data_dir = os.path.expandvars(data_dir)\n    if not os.path.isdir(data_dir):\n        raise RuntimeError(\"Data directory '{}' does not exist or is not a directory\".format(data_dir))\n    if not os.path.isfile(os.path.join(data_dir, 'METADATA.json')):\n        raise RuntimeError(\"Data directory '{}' does not contain a METADATA.json file\".format(data_dir))\n\n    return data_dir", "response": "Checks that the data dir exists and contains METADATA. json file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cli_check_format(fmt):\n    '''Checks that a basis set format exists and if not, raises a helpful exception'''\n\n    if fmt is None:\n        return None\n\n    fmt = fmt.lower()\n    if not fmt in api.get_formats():\n        errstr = \"Format '\" + fmt + \"' does not exist.\\n\"\n        errstr += \"For a complete list of formats, use the 'bse list-formats' command\"\n        raise RuntimeError(errstr)\n\n    return fmt", "response": "Checks that a basis set format exists and if not raises a helpful exception"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that a reference format exists and if not raises a helpful exception", "response": "def _cli_check_ref_format(fmt):\n    '''Checks that a reference format exists and if not, raises a helpful exception'''\n\n    if fmt is None:\n        return None\n\n    fmt = fmt.lower()\n    if not fmt in api.get_reference_formats():\n        errstr = \"Reference format '\" + fmt + \"' does not exist.\\n\"\n        errstr += \"For a complete list of formats, use the 'bse list-ref-formats' command\"\n        raise RuntimeError(errstr)\n\n    return fmt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that a basis set role exists and if not raises a helpful exception", "response": "def _cli_check_role(role):\n    '''Checks that a basis set role exists and if not, raises a helpful exception'''\n\n    if role is None:\n        return None\n\n    role = role.lower()\n    if not role in api.get_roles():\n        errstr = \"Role format '\" + role + \"' does not exist.\\n\"\n        errstr += \"For a complete list of roles, use the 'bse list-roles' command\"\n        raise RuntimeError(errstr)\n\n    return role"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that a basis set exists and if not raises a helpful exception", "response": "def _cli_check_basis(name, data_dir):\n    '''Checks that a basis set exists and if not, raises a helpful exception'''\n\n    if name is None:\n        return None\n\n    name = misc.transform_basis_name(name)\n    metadata = api.get_metadata(data_dir)\n    if not name in metadata:\n        errstr = \"Basis set '\" + name + \"' does not exist.\\n\"\n        errstr += \"For a complete list of basis sets, use the 'bse list-basis-sets' command\"\n        raise RuntimeError(errstr)\n\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that a basis set family exists and if not raises a helpful exception", "response": "def _cli_check_family(family, data_dir):\n    '''Checks that a basis set family exists and if not, raises a helpful exception'''\n\n    if family is None:\n        return None\n\n    family = family.lower()\n    if not family in api.get_families(data_dir):\n        errstr = \"Basis set family '\" + family + \"' does not exist.\\n\"\n        errstr += \"For a complete list of families, use the 'bse list-families' command\"\n        raise RuntimeError(errstr)\n\n    return family"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cli_check_readfmt(readfmt):\n    '''Checks that a file type exists and if not, raises a helpful exception'''\n\n    if readfmt is None:\n        return None\n\n    readfmt = readfmt.lower()\n    if not readfmt in curate.get_reader_formats():\n        errstr = \"Reader for file type '\" + readfmt + \"' does not exist.\\n\"\n        errstr += \"For a complete list of file types, use the 'bsecurate get-reader-formats' command\"\n        raise RuntimeError(errstr)\n\n    return readfmt", "response": "Checks that a file type exists and if not raises a helpful exception"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking and normalize arguments for the uggers command line", "response": "def cli_check_normalize_args(args):\n    '''Check and normalize arguments\n       This function checks that basis set names, families, roles, etc, are\n       valid (and raise an exception if they aren't)\n\n       The original data passed to this function is not modified. A modified\n       copy is returned.\n    '''\n\n    args_keys = vars(args).keys()  # What args we have\n    args_copy = copy.copy(args)\n    if 'data_dir' in args_keys:\n        args_copy.data_dir = _cli_check_data_dir(args.data_dir)\n    if 'basis' in args:\n        args_copy.basis = _cli_check_basis(args.basis, args.data_dir)\n    if 'basis1' in args_keys:\n        args_copy.basis1 = _cli_check_basis(args.basis1, args.data_dir)\n    if 'basis2' in args_keys:\n        args_copy.basis2 = _cli_check_basis(args.basis2, args.data_dir)\n    if 'fmt' in args_keys:\n        args_copy.fmt = _cli_check_format(args.fmt)\n    if 'reffmt' in args_keys:\n        args_copy.reffmt = _cli_check_ref_format(args.reffmt)\n    if 'role' in args_keys:\n        args_copy.role = _cli_check_role(args.role)\n    if 'family' in args_keys:\n        args_copy.family = _cli_check_family(args.family, args.data_dir)\n    if 'readfmt1' in args_keys:\n        args_copy.readfmt1 = _cli_check_readfmt(args.readfmt1)\n    if 'readfmt2' in args_keys:\n        args_copy.readfmt2 = _cli_check_readfmt(args.readfmt2)\n\n    return args_copy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the readme file for the bundle Returns a str representing the readme file for the bundle", "response": "def _create_readme(fmt, reffmt):\n    '''\n    Creates the readme file for the bundle\n\n    Returns a str representing the readme file\n    '''\n\n    now = datetime.datetime.utcnow()\n    timestamp = now.strftime('%Y-%m-%d %H:%M:%S UTC')\n\n    # yapf: disable\n    outstr = _readme_str.format(timestamp=timestamp,\n                                bsever=api.version(),\n                                fmt=fmt, reffmt=reffmt)\n    # yapf: enable\n\n    return outstr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _basis_data_iter(fmt, reffmt, data_dir):\n    '''Iterate over all basis set names, and return a tuple of\n       (name, data) where data is the basis set in the given format\n    '''\n    md = api.get_metadata(data_dir)\n    for bs, bs_md in md.items():\n        versions = bs_md['versions'].keys()\n\n        data = {}\n        for v in versions:\n            bsdata = api.get_basis(bs, fmt=fmt, version=v, data_dir=data_dir)\n            refdata = api.get_references(bs, fmt=reffmt, version=v, data_dir=data_dir)\n            data[v] = (bsdata, refdata)\n\n        notes = api.get_basis_notes(bs, data_dir)\n        yield (bs, data, notes)", "response": "Iterate over all basis set names and return a tuple of\n       name data and notes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds string data to a tarfile", "response": "def _add_to_tbz(tfile, filename, data_str):\n    '''\n    Adds string data to a tarfile\n    '''\n\n    # Create a bytesio object for adding to a tarfile\n    # https://stackoverflow.com/a/52724508\n    encoded_data = data_str.encode('utf-8')\n    ti = tarfile.TarInfo(name=filename)\n    ti.size = len(encoded_data)\n    tfile.addfile(tarinfo=ti, fileobj=io.BytesIO(encoded_data))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bundle_generic(bfile, addhelper, fmt, reffmt, data_dir):\n    '''\n    Loop over all basis sets and add data to an archive\n\n    Parameters\n    ----------\n    bfile : object\n        An object that gets passed through to the addhelper function\n    addhelper : function\n        A function that takes bfile and adds data to the bfile\n    fmt : str\n        Format of the basis set to create\n    reffmt : str\n        Format to use for the references\n    data_dir : str\n        Data directory with all the basis set information.\n\n    Returns\n    -------\n    None\n    '''\n\n    ext = converters.get_format_extension(fmt)\n    refext = refconverters.get_format_extension(reffmt)\n    subdir = 'basis_set_bundle-' + fmt + '-' + reffmt\n\n    readme_path = os.path.join(subdir, 'README.txt')\n    addhelper(bfile, readme_path, _create_readme(fmt, reffmt))\n\n    for name, data, notes in _basis_data_iter(fmt, reffmt, data_dir):\n        for ver, verdata in data.items():\n            filename = misc.basis_name_to_filename(name)\n            basis_filepath = os.path.join(subdir, '{}.{}{}'.format(filename, ver, ext))\n            ref_filename = os.path.join(subdir, '{}.{}.ref{}'.format(filename, ver, refext))\n\n            bsdata, refdata = verdata\n            addhelper(bfile, basis_filepath, bsdata)\n            addhelper(bfile, ref_filename, refdata)\n\n        if len(notes) > 0:\n            notes_filename = os.path.join(subdir, filename + '.notes')\n            addhelper(bfile, notes_filename, notes)\n\n    for fam in api.get_families(data_dir):\n        fam_notes = api.get_family_notes(fam, data_dir)\n\n        if len(fam_notes) > 0:\n            fam_notes_filename = os.path.join(subdir, fam + '.family_notes')\n            addhelper(bfile, fam_notes_filename, fam_notes)", "response": "Create a new archive containing all the basis sets and data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_bundle(outfile, fmt, reffmt, archive_type=None, data_dir=None):\n    '''\n    Create a single archive file containing all basis\n    sets in a given format\n\n    Parameters\n    ----------\n    outfile : str\n        Path to the file to create. Existing files will be overwritten\n    fmt : str\n        Format of the basis set to archive (nwchem, turbomole, ...)\n    reffmt : str\n        Format of the basis set references to archive (nwchem, turbomole, ...)\n    archive_type : str\n        Type of archive to create. Can be 'zip' or 'tbz'. Default is\n        None, which will autodetect based on the outfile name\n    data_dir : str\n        Data directory with all the basis set information. By default,\n        it is in the 'data' subdirectory of this project.\n\n    Returns\n    -------\n    None\n    '''\n\n    if archive_type is None:\n        outfile_lower = outfile.lower()\n\n        for k, v in _bundle_types.items():\n            if outfile_lower.endswith(v['extension']):\n                archive_type = k\n                break\n        else:\n            raise RuntimeError(\"Cannot autodetect archive type from file name: {}\".format(os.path.basename(outfile)))\n\n    else:\n        archive_type = archive_type.lower()\n        if not archive_type in _bundle_types:\n            raise RuntimeError(\"Archive type '{}' is not valid.\")\n\n    _bundle_types[archive_type]['handler'](outfile, fmt, reffmt, data_dir)", "response": "Create a single archive file containing all the basis sets in a given format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_archive_types():\n    '''\n    Return information related to the types of archives available\n    '''\n    ret = copy.deepcopy(_bundle_types)\n    for k, v in ret.items():\n        v.pop('handler')\n    return ret", "response": "Return information related to the types of archives available"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges the basis set data for an element from multiple sources into dest.", "response": "def merge_element_data(dest, sources, use_copy=True):\n    \"\"\"\n    Merges the basis set data for an element from multiple sources\n    into dest.\n\n    The destination is not modified, and a (shallow) copy of dest is returned\n    with the data from sources added.\n\n    If use_copy is True, then the data merged into dest will be a (deep)\n    copy of that found in sources. Otherwise, data may be shared between dest\n    and sources\n    \"\"\"\n\n    if dest is not None:\n        ret = dest.copy()\n    else:\n        ret = {}\n\n    if use_copy:\n        sources = copy.deepcopy(sources)\n\n    # Note that we are not copying notes/data_sources\n    for s in sources:\n        if 'electron_shells' in s:\n            if 'electron_shells' not in ret:\n                ret['electron_shells'] = []\n            ret['electron_shells'].extend(s['electron_shells'])\n        if 'ecp_potentials' in s:\n            if 'ecp_potentials' in ret:\n                raise RuntimeError('Cannot overwrite existing ECP')\n            ret['ecp_potentials'] = s['ecp_potentials']\n            ret['ecp_electrons'] = s['ecp_electrons']\n        if 'references' in s:\n            if 'references' not in ret:\n                ret['references'] = []\n            for ref in s['references']:\n                if not ref in ret['references']:\n                    ret['references'].append(ref)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving exact duplicates of primitives condenses duplicate exponents into general contractions and returns a new copy of the current shell", "response": "def prune_shell(shell, use_copy=True):\n    \"\"\"\n    Removes exact duplicates of primitives, and condenses duplicate exponents\n    into general contractions\n\n    Also removes primitives if all coefficients are zero\n    \"\"\"\n\n    new_exponents = []\n    new_coefficients = []\n\n    exponents = shell['exponents']\n    nprim = len(exponents)\n\n    # transpose of the coefficient matrix\n    coeff_t = list(map(list, zip(*shell['coefficients'])))\n\n    # Group by exponents\n    ex_groups = []\n    for i in range(nprim):\n        for ex in ex_groups:\n            if float(exponents[i]) == float(ex[0]):\n                ex[1].append(coeff_t[i])\n                break\n        else:\n            ex_groups.append((exponents[i], [coeff_t[i]]))\n\n    # Now collapse within groups\n    for ex in ex_groups:\n        if len(ex[1]) == 1:\n            # only add if there is a nonzero contraction coefficient\n            if not all([float(x) == 0.0 for x in ex[1][0]]):\n                new_exponents.append(ex[0])\n                new_coefficients.append(ex[1][0])\n            continue\n\n        # ex[1] contains rows of coefficients. The length of ex[1]\n        # is the number of times the exponent is duplicated. Columns represent general contractions.\n        # We want to find the non-zero coefficient in each column, if it exists\n        # The result is a single row with a length representing the number\n        # of general contractions\n\n        new_coeff_row = []\n\n        # so take yet another transpose.\n        ex_coeff = list(map(list, zip(*ex[1])))\n        for g in ex_coeff:\n            nonzero = [x for x in g if float(x) != 0.0]\n            if len(nonzero) > 1:\n                raise RuntimeError(\"Exponent {} is duplicated within a contraction\".format(ex[0]))\n\n            if len(nonzero) == 0:\n                new_coeff_row.append(g[0])\n            else:\n                new_coeff_row.append(nonzero[0])\n\n        # only add if there is a nonzero contraction coefficient anywhere for this exponent\n        if not all([float(x) == 0.0 for x in new_coeff_row]):\n            new_exponents.append(ex[0])\n            new_coefficients.append(new_coeff_row)\n\n    # take the transpose again, putting the general contraction\n    # as the slowest index\n    new_coefficients = list(map(list, zip(*new_coefficients)))\n\n    shell['exponents'] = new_exponents\n    shell['coefficients'] = new_coefficients\n\n    return shell"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving any non - zero coefficients and any non - zero shells and remove any duplicate primitives and shells that are not used by other manipulations.", "response": "def prune_basis(basis, use_copy=True):\n    \"\"\"\n    Removes primitives that have a zero coefficient, and\n    removes duplicate primitives and shells\n\n    This only finds EXACT duplicates, and is meant to be used\n    after other manipulations\n\n    If use_copy is True, the input basis set is not modified.\n    \"\"\"\n\n    if use_copy:\n        basis = copy.deepcopy(basis)\n\n    for k, el in basis['elements'].items():\n        if not 'electron_shells' in el:\n            continue\n\n        shells = el.pop('electron_shells')\n        shells = [prune_shell(sh, False) for sh in shells]\n\n        # Remove any duplicates\n        el['electron_shells'] = []\n\n        for sh in shells:\n            if sh not in el['electron_shells']:\n                el['electron_shells'].append(sh)\n\n    return basis"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove spd spdf etc and spdf from a basis set.", "response": "def uncontract_spdf(basis, max_am=0, use_copy=True):\n    \"\"\"\n    Removes sp, spd, spdf, etc, contractions from a basis set\n\n    The general contractions are replaced by uncontracted versions\n\n    Contractions up to max_am will be left in place. For example,\n    if max_am = 1, spd will be split into sp and d\n\n    The input basis set is not modified. The returned basis\n    may have functions with coefficients of zero and may have duplicate\n    shells.\n\n    If use_copy is True, the input basis set is not modified.\n    \"\"\"\n\n    if use_copy:\n        basis = copy.deepcopy(basis)\n\n    for k, el in basis['elements'].items():\n\n        if not 'electron_shells' in el:\n            continue\n        newshells = []\n\n        for sh in el['electron_shells']:\n\n            # am will be a list\n            am = sh['angular_momentum']\n            coeff = sh['coefficients']\n\n            # if this is an sp, spd,...  orbital\n            if len(am) > 1:\n                newsh = sh.copy()\n                newsh['angular_momentum'] = []\n                newsh['coefficients'] = []\n\n                ngen = len(sh['coefficients'])\n                for g in range(ngen):\n                    if am[g] > max_am:\n                        newsh2 = sh.copy()\n                        newsh2['angular_momentum'] = [am[g]]\n                        newsh2['coefficients'] = [coeff[g]]\n                        newshells.append(newsh2)\n                    else:\n                        newsh['angular_momentum'].append(am[g])\n                        newsh['coefficients'].append(coeff[g])\n\n                newshells.insert(0, newsh)\n\n            else:\n                newshells.append(sh)\n\n        el['electron_shells'] = newshells\n\n    return basis"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the general contractions from a basis set.", "response": "def uncontract_general(basis, use_copy=True):\n    \"\"\"\n    Removes the general contractions from a basis set\n\n    The input basis set is not modified. The returned basis\n    may have functions with coefficients of zero and may have duplicate\n    shells.\n\n    If use_copy is True, the input basis set is not modified.\n    \"\"\"\n\n    if use_copy:\n        basis = copy.deepcopy(basis)\n\n    for k, el in basis['elements'].items():\n\n        if not 'electron_shells' in el:\n            continue\n\n        newshells = []\n\n        for sh in el['electron_shells']:\n            # See if we actually have to uncontract\n            # Also, don't uncontract sp, spd,.... orbitals\n            #      (leave that to uncontract_spdf)\n            if len(sh['coefficients']) == 1 or len(sh['angular_momentum']) > 1:\n                newshells.append(sh)\n            else:\n                if len(sh['angular_momentum']) == 1:\n                    for c in sh['coefficients']:\n                        # copy, them replace 'coefficients'\n                        newsh = sh.copy()\n                        newsh['coefficients'] = [c]\n                        newshells.append(newsh)\n\n        el['electron_shells'] = newshells\n\n    # If use_basis is True, we already made our deep copy\n    return prune_basis(basis, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uncontract_segmented(basis, use_copy=True):\n\n    if use_copy:\n        basis = copy.deepcopy(basis)\n\n    for k, el in basis['elements'].items():\n\n        if not 'electron_shells' in el:\n            continue\n\n        newshells = []\n\n        for sh in el['electron_shells']:\n            exponents = sh['exponents']\n            nam = len(sh['angular_momentum'])\n\n            for i in range(len(exponents)):\n                newsh = sh.copy()\n                newsh['exponents'] = [exponents[i]]\n                newsh['coefficients'] = [[\"1.00000000\"] * nam]\n\n                # Remember to transpose the coefficients\n                newsh['coefficients'] = list(map(list, zip(*newsh['coefficients'])))\n\n                newshells.append(newsh)\n\n        el['electron_shells'] = newshells\n\n    return basis", "response": "Removes the segmented contractions from a basis set."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes one large general contraction for each angular momentum in the basis set.", "response": "def make_general(basis, use_copy=True):\n    \"\"\"\n    Makes one large general contraction for each angular momentum\n\n    If use_copy is True, the input basis set is not modified.\n\n    The output of this function is not pretty. If you want to make it nicer,\n    use sort_basis afterwards.\n    \"\"\"\n\n    zero = '0.00000000'\n\n    basis = uncontract_spdf(basis, 0, use_copy)\n\n    for k, el in basis['elements'].items():\n        if not 'electron_shells' in el:\n            continue\n\n        # See what we have\n        all_am = []\n        for sh in el['electron_shells']:\n            if not sh['angular_momentum'] in all_am:\n                all_am.append(sh['angular_momentum'])\n\n        all_am = sorted(all_am)\n\n        newshells = []\n        for am in all_am:\n            newsh = {\n                'angular_momentum': am,\n                'exponents': [],\n                'coefficients': [],\n                'region': '',\n                'function_type': None,\n            }\n\n            # Do exponents first\n            for sh in el['electron_shells']:\n                if sh['angular_momentum'] != am:\n                    continue\n                newsh['exponents'].extend(sh['exponents'])\n\n            # Number of primitives in the new shell\n            nprim = len(newsh['exponents'])\n\n            cur_prim = 0\n            for sh in el['electron_shells']:\n                if sh['angular_momentum'] != am:\n                    continue\n\n                if newsh['function_type'] is None:\n                    newsh['function_type'] = sh['function_type']\n\n                # Make sure the shells we are merging have the same function types\n                ft1 = newsh['function_type']\n                ft2 = sh['function_type']\n\n                # Check if one function type is the subset of another\n                # (should handle gto/gto_spherical, etc)\n                if ft1 not in ft2 and ft2 not in ft1:\n                    raise RuntimeError(\"Cannot make general contraction of different function types\")\n\n                ngen = len(sh['coefficients'])\n\n                for g in range(ngen):\n                    coef = [zero] * cur_prim\n                    coef.extend(sh['coefficients'][g])\n                    coef.extend([zero] * (nprim - len(coef)))\n                    newsh['coefficients'].append(coef)\n\n                cur_prim += len(sh['exponents'])\n\n            newshells.append(newsh)\n\n        el['electron_shells'] = newshells\n\n    return basis"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef optimize_general(basis, use_copy=True):\n\n    if use_copy:\n        basis = copy.deepcopy(basis)\n\n    for k, el in basis['elements'].items():\n\n        if not 'electron_shells' in el:\n            continue\n\n        elshells = el.pop('electron_shells')\n        el['electron_shells'] = []\n        for sh in elshells:\n            exponents = sh['exponents']\n            coefficients = sh['coefficients']\n            nprim = len(exponents)\n            nam = len(sh['angular_momentum'])\n\n            if nam > 1 or len(coefficients) < 2:\n                el['electron_shells'].append(sh)\n                continue\n\n            # First, find columns (general contractions) with a single non-zero value\n            single_columns = [idx for idx, c in enumerate(coefficients) if _is_single_column(c)]\n\n            # Find the corresponding rows that have a value in one of these columns\n            # Note that at this stage, the row may have coefficients in more than one\n            # column. That is ok, we are going to split it off anyway\n            single_rows = []\n            for col_idx in single_columns:\n                col = coefficients[col_idx]\n                for row_idx in range(nprim):\n                    if float(col[row_idx]) != 0.0:\n                        single_rows.append(row_idx)\n\n            # Split those out into new shells, and remove them from the\n            # original shell\n            new_shells_single = []\n            for row_idx in single_rows:\n                newsh = copy.deepcopy(sh)\n                newsh['exponents'] = [exponents[row_idx]]\n                newsh['coefficients'] = [['1.00000000000']]\n                new_shells_single.append(newsh)\n\n            exponents = [x for idx, x in enumerate(exponents) if idx not in single_rows]\n            coefficients = [x for idx, x in enumerate(coefficients) if idx not in single_columns]\n            coefficients = [[x for idx, x in enumerate(col) if not idx in single_rows] for col in coefficients]\n\n            # Remove Zero columns\n            #coefficients = [ x for x in coefficients if not _is_zero_column(x) ]\n\n            # Find contiguous rectanglar blocks\n            new_shells = []\n            while len(exponents) > 0:\n                block_rows, block_cols = _find_block(coefficients)\n\n                # add as a new shell\n                newsh = copy.deepcopy(sh)\n                newsh['exponents'] = [exponents[i] for i in block_rows]\n                newsh['coefficients'] = [[coefficients[colidx][i] for i in block_rows] for colidx in block_cols]\n                new_shells.append(newsh)\n\n                # Remove from the original exponent/coefficient set\n                exponents = [x for idx, x in enumerate(exponents) if idx not in block_rows]\n                coefficients = [x for idx, x in enumerate(coefficients) if idx not in block_cols]\n                coefficients = [[x for idx, x in enumerate(col) if not idx in block_rows] for col in coefficients]\n\n            # I do this order to mimic the output of the original BSE\n            el['electron_shells'].extend(new_shells)\n            el['electron_shells'].extend(new_shells_single)\n\n        # Fix coefficients for completely uncontracted shells to 1.0\n        for sh in el['electron_shells']:\n            if len(sh['coefficients']) == 1 and len(sh['coefficients'][0]) == 1:\n                sh['coefficients'][0][0] = '1.0000000'\n\n    return basis", "response": "Optimizes the general contractions using the method of Hashimoto et al\n                 "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reldiff(a, b):\n\n    a = float(a)\n    b = float(b)\n    aa = abs(a)\n    ba = abs(b)\n\n    if a == 0.0 and b == 0.0:\n        return 0.0\n    elif a == 0 or b == 0.0:\n        return float('inf')\n\n    return abs(a - b) / min(aa, ba)", "response": "Compute the relative difference of two floating - point numbers a and b."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compare_keys(element1, element2, key, compare_func, *args):\n    if key in element1 and key in element2:\n        if not compare_func(element1[key], element2[key], *args):\n            return False\n    elif key in element1 or key in element2:\n        return False\n\n    return True", "response": "Returns True if the key is equivalent to the key in the basis set element1 and element2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compare_vector(arr1, arr2, rel_tol):\n\n    length = len(arr1)\n    if len(arr2) != length:\n        return False\n\n    for i in range(length):\n        element_1 = float(arr1[i])\n        element_2 = float(arr2[i])\n\n        diff = abs(abs(element_1) - abs(element_2))\n        if diff != 0.0:\n            rel = _reldiff(element_1, element_2)\n\n            # For a basis set, a relatively coarse comparison\n            # should be acceptible\n            if rel > rel_tol:\n                return False\n\n    return True", "response": "Compares two arrays for approximate equality."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compare_matrix(mat1, mat2, rel_tol):\n\n    length = len(mat1)\n    if len(mat2) != length:\n        return False\n\n    for i in range(length):\n        if _compare_vector(mat1[i], mat2[i], rel_tol) is False:\n            return False\n\n    return True", "response": "Compares two matrices for approximate equality."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_electron_shells(shell1, shell2, compare_meta=False, rel_tol=0.0):\n    '''\n    Compare two electron shells for approximate equality\n    (exponents/coefficients are within a tolerance)\n\n    If compare_meta is True, the metadata is also compared for exact equality. \n    '''\n\n    if shell1['angular_momentum'] != shell2['angular_momentum']:\n        return False\n\n    # Zip together exponents and coeffs for sorting\n    # This basically creates the typical matrix with exponents\n    # being in the first column\n    tmp1 = list(zip(shell1['exponents'], *shell1['coefficients']))\n    tmp2 = list(zip(shell2['exponents'], *shell2['coefficients']))\n\n    # Now sort by first non-zero coefficient\n    tmp1 = sorted(tmp1)\n    tmp2 = sorted(tmp2)\n\n    if not _compare_matrix(tmp1, tmp2, rel_tol):\n        return False\n    if compare_meta:\n        if shell1['region'] != shell2['region']:\n            return False\n        if shell1['function_type'] != shell2['function_type']:\n            return False\n        return True\n    else:\n        return True", "response": "Compare two electron shells for approximate equality."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef electron_shells_are_subset(subset, superset, compare_meta=False, rel_tol=0.0):\n    '''\n    Determine if a list of electron shells is a subset of another\n\n    If 'subset' is a subset of the 'superset', True is returned.\n\n    The shells are compared approximately (exponents/coefficients are \n    within a tolerance)\n\n    If compare_meta is True, the metadata is also compared for exact equality. \n    '''\n\n    for item1 in subset:\n        for item2 in superset:\n            if compare_electron_shells(item1, item2, compare_meta, rel_tol):\n                break\n        else:\n            return False\n\n    return True", "response": "Determines if a list of electron shells is a subset of another list of electron shells."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if two electron shells are equal.", "response": "def electron_shells_are_equal(shells1, shells2, compare_meta=False, rel_tol=0.0):\n    '''\n    Determine if a list of electron shells is the same as another\n\n    The shells are compared approximately (exponents/coefficients are \n    within a tolerance)\n\n    If compare_meta is True, the metadata is also compared for exact equality. \n    '''\n\n    # Lists are equal if each is a subset of the other\n    # Slow but effective\n    if len(shells1) != len(shells2):\n        return False\n\n    return electron_shells_are_subset(shells1, shells2, compare_meta, rel_tol) and electron_shells_are_subset(\n        shells2, shells1, compare_meta, rel_tol)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare_ecp_pots(potential1, potential2, compare_meta=False, rel_tol=0.0):\n    '''\n    Compare two ecp potentials for approximate equality\n    (exponents/coefficients are within a tolerance)\n\n    If compare_meta is True, the metadata is also compared for exact equality. \n    '''\n\n    if potential1['angular_momentum'] != potential2['angular_momentum']:\n        return False\n\n    rexponents1 = potential1['r_exponents']\n    rexponents2 = potential2['r_exponents']\n    gexponents1 = potential1['gaussian_exponents']\n    gexponents2 = potential2['gaussian_exponents']\n    coefficients1 = potential1['coefficients']\n    coefficients2 = potential2['coefficients']\n\n    # integer comparison\n    if rexponents1 != rexponents2:\n        return False\n    if not _compare_vector(gexponents1, gexponents2, rel_tol):\n        return False\n    if not _compare_matrix(coefficients1, coefficients2, rel_tol):\n        return False\n    if compare_meta:\n        if potential1['ecp_type'] != potential2['ecp_type']:\n            return False\n        return True\n    else:\n        return True", "response": "Compare two ecp potentials for approximate equality."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if a list of ecp potentials are a subset of another list of ecp potentials.", "response": "def ecp_pots_are_subset(subset, superset, compare_meta=False, rel_tol=0.0):\n    '''\n    Determine if a list of ecp potentials is a subset of another\n\n    If 'subset' is a subset of the 'superset', True is returned.\n\n    The potentials are compared approximately (exponents/coefficients are \n    within a tolerance)\n\n    If compare_meta is True, the metadata is also compared for exact equality. \n    '''\n\n    for item1 in subset:\n        for item2 in superset:\n            if compare_ecp_pots(item1, item2, compare_meta, rel_tol):\n                break\n        else:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if two electron shells are equal.", "response": "def ecp_pots_are_equal(pots1, pots2, compare_meta=False, rel_tol=0.0):\n    '''\n    Determine if a list of electron shells is the same as another\n\n    The potentials are compared approximately (exponents/coefficients are \n    within a tolerance)\n\n    If compare_meta is True, the metadata is also compared for exact equality. \n    '''\n\n    # Lists are equal if each is a subset of the other\n    # Slow but effective\n    return ecp_pots_are_subset(pots1, pots2, compare_meta) and ecp_pots_are_subset(pots2, pots1, compare_meta)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare two dictionaries of basis information for an element and returns True if the basis information for two elements are the same as another dict of basis information for an element.", "response": "def compare_elements(element1,\n                     element2,\n                     compare_electron_shells_meta=False,\n                     compare_ecp_pots_meta=False,\n                     compare_meta=False,\n                     rel_tol=0.0):\n    '''\n    Determine if the basis information for two elements is the same as another\n\n    Exponents/coefficients are compared using a tolerance.\n\n    Parameters\n    ----------\n    element1 : dict\n        Basis information for an element\n    element2 : dict\n        Basis information for another element\n    compare_electron_shells_meta : bool\n        Compare the metadata of electron shells\n    compare_ecp_pots_meta : bool\n        Compare the metadata of ECP potentials\n    compare_meta : bool\n        Compare the overall element metadata\n    rel_tol : float\n        Maximum relative error that is considered equal\n    '''\n\n    if not _compare_keys(element1, element2, 'electron_shells', electron_shells_are_equal,\n                         compare_electron_shells_meta, rel_tol):\n        return False\n\n    if not _compare_keys(element1, element2, 'ecp_potentials', ecp_pots_are_equal, compare_ecp_pots_meta, rel_tol):\n        return False\n\n    if not _compare_keys(element1, element2, 'ecp_electrons', operator.eq):\n        return False\n\n    if compare_meta:\n        if not _compare_keys(element1, element2, 'references', operator.eq):\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare_basis(bs1,\n                  bs2,\n                  compare_electron_shells_meta=False,\n                  compare_ecp_pots_meta=False,\n                  compare_elements_meta=False,\n                  compare_meta=False,\n                  rel_tol=0.0):\n    '''\n    Determine if two basis set dictionaries are the same\n\n    bs1 : dict\n        Full basis information\n    bs2 : dict\n        Full basis information\n    compare_electron_shells_meta : bool\n        Compare the metadata of electron shells\n    compare_ecp_pots_meta : bool\n        Compare the metadata of ECP potentials\n    compare_elements_meta : bool\n        Compare the overall element metadata\n    compare_meta: bool\n        Compare the metadata for the basis set (name, description, etc)\n    rel_tol : float\n        Maximum relative error that is considered equal\n    '''\n\n    els1 = sorted(list(bs1['elements'].keys()))\n    els2 = sorted(list(bs2['elements'].keys()))\n    if not els1 == els2:\n        return False\n\n    for el in els1:\n        if not compare_elements(\n                bs1['elements'][el],\n                bs2['elements'][el],\n                compare_electron_shells_meta=compare_electron_shells_meta,\n                compare_ecp_pots_meta=compare_ecp_pots_meta,\n                compare_meta=compare_elements_meta,\n                rel_tol=rel_tol):\n            print(\"Element failed:\", el)\n            return False\n    if compare_meta:\n        for k in ['name', 'family', 'description', 'revision_description', 'role', 'auxiliaries']:\n            if not _compare_keys(bs1, bs2, k, operator.eq):\n                return False\n    return True", "response": "Compare two basis set dictionaries and return True if they are the same."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a METADATA. json file from a data directory and a data directory.", "response": "def create_metadata_file(output_path, data_dir):\n    '''Creates a METADATA.json file from a data directory\n\n    The file is written to output_path\n    '''\n\n    # Relative path to all (BASIS).metadata.json files\n    meta_filelist, table_filelist, _, _ = get_all_filelist(data_dir)\n\n    metadata = {}\n    for meta_file_relpath in meta_filelist:\n\n        # Read in the metadata for a single basis set\n        meta_file_path = os.path.join(data_dir, meta_file_relpath)\n        bs_metadata = read_json_basis(meta_file_path)\n\n        # Base of the filename for table basis sets\n        # Basename is something like '6-31G.', including the last period\n        base_relpath, meta_filename = os.path.split(meta_file_relpath)\n        base_filename = meta_filename.split('.')[0] + '.'\n\n        # All the table files that correspond to this metadata file\n        # (relative to data_dir)\n\n        this_filelist = [\n            x for x in table_filelist\n            if os.path.dirname(x) == base_relpath and os.path.basename(x).startswith(base_filename)\n        ]\n\n        # The 'versions' dict that will go into the metadata\n        version_info = {}\n\n        # Make sure function types are the same\n        function_types = None\n\n        # For each table basis, compose it\n        for table_file in this_filelist:\n            # Obtain just the filename of the table basis\n            table_filename = os.path.basename(table_file)\n\n            # Obtain the base filename and version from the filename\n            # The base filename is the part before the first period\n            # (filebase.ver.table.json)\n            table_filebase, ver, _, _ = table_filename.split('.')\n\n            # Fully compose the basis set from components\n            bs = compose_table_basis(table_file, data_dir)\n\n            # Elements for which this basis is defined\n            defined_elements = sorted(list(bs['elements'].keys()), key=lambda x: int(x))\n\n            # Determine the types of functions contained in the basis\n            # (gto, ecp, etc)\n            if function_types is None:\n                function_types = bs['function_types']\n            elif function_types != bs['function_types']:\n                raise RuntimeError(\"Differing function types across versions for \" + base_filename)\n\n            # Create the metadata for this specific version\n            # yapf: disable\n            version_info[ver] = { 'file_relpath': table_file,\n                                  'revdesc': bs['revision_description'],\n                                  'elements': defined_elements\n                                }\n            # yapf: enable\n\n        # Sort the version dicts\n        version_info = dict(sorted(version_info.items()))\n        # Find the maximum version for this basis\n        latest_ver = max(version_info.keys())\n\n        # Create the common metadata for this basis set\n        # display_name and other_names are placeholders to keep order\n        # yapf: disable\n        common_md = { 'display_name': None,\n                      'other_names': None,\n                      'description': bs['description'],\n                      'latest_version': latest_ver,\n                      'basename': base_filename[:-1], # Strip off that trailing period\n                      'relpath': base_relpath,\n                      'family': bs['family'],\n                      'role': bs['role'],\n                      'functiontypes': function_types,\n                      'auxiliaries': bs['auxiliaries'],\n                      'versions': version_info }\n        # yapf: enable\n\n        # Loop through all the common names, translate them, and then add the data\n        for bs_name in bs_metadata['names']:\n            tr_name = transform_basis_name(bs_name)\n\n            if tr_name in metadata:\n                raise RuntimeError(\"Duplicate basis set name: \" + tr_name)\n\n            # Create a new entry, with all the common metadata\n            # Also, store the other names for this basis\n            other_names = bs_metadata['names'].copy()\n            other_names.remove(bs_name)\n            metadata[tr_name] = common_md.copy()\n            metadata[tr_name]['display_name'] = bs_name\n            metadata[tr_name]['other_names'] = other_names\n\n    # Write out the metadata\n    metadata = dict(sorted(metadata.items()))\n    _write_plain_json(output_path, metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_txt(refs):\n    '''Converts references to plain text format\n    '''\n    full_str = '\\n'\n\n    lib_citation_desc, lib_citations = get_library_citation()\n\n    # Add the refs for the libarary at the top\n    full_str += '*' * 80 + '\\n'\n    full_str += lib_citation_desc\n    full_str += '*' * 80 + '\\n'\n    for r in lib_citations.values():\n        ref_txt = reference_text(r)\n        ref_txt = textwrap.indent(ref_txt, ' ' * 4)\n        full_str += '{}\\n\\n'.format(ref_txt)\n\n    full_str += '*' * 80 + '\\n'\n    full_str += \"References for the basis set\\n\"\n    full_str += '*' * 80 + '\\n'\n    for ref in refs:\n        full_str += '{}\\n'.format(compact_elements(ref['elements']))\n\n        for ri in ref['reference_info']:\n            full_str += '    ## {}\\n'.format(ri['reference_description'])\n\n            refdata = ri['reference_data']\n\n            if len(refdata) == 0:\n                full_str += '    (...no reference...)\\n\\n'\n            for k, r in refdata:\n                ref_txt = reference_text(r)\n                ref_txt = textwrap.indent(ref_txt, ' ' * 4)\n                full_str += '{}\\n\\n'.format(ref_txt)\n\n    return full_str", "response": "Converts references to plain text format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the difference between two lists of electron shells s1 and s2", "response": "def subtract_electron_shells(s1, s2, rel_tol=0.0):\n    \"\"\"\n    Returns the difference between two lists of electron shells (s1 - s2)\n\n    This will remove any shells from s1 that are also in s2, within a tolerance\n    \"\"\"\n\n    diff_shells = []\n    for sh1 in s1:\n        for sh2 in s2:\n            if compare_electron_shells(sh1, sh2, rel_tol=rel_tol):\n                break\n        else:\n            diff_shells.append(copy.deepcopy(sh1))\n\n    return diff_shells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff_basis_dict(left_list, right_list):\n    '''\n    Compute the difference between two sets of basis set dictionaries\n\n    The result is a list of dictionaries that correspond to each dictionary in\n    `left_list`. Each resulting dictionary will contain only the elements/shells\n    that exist in that entry and not in any of the dictionaries in `right_list`.\n\n    This only works on the shell level, and will only subtract entire shells\n    that are identical. ECP potentials are not affected.\n\n    The return value contains deep copies of the input data\n\n    Parameters\n    ----------\n    left_list : list of dict\n        Dictionaries to use as the base\n    right_list : list of dict\n        Dictionaries of basis data to subtract from each dictionary of `left_list`\n\n    Returns\n    ----------\n    list\n        Each object in `left_list` containing data that does not appear in `right_list`\n    '''\n\n    ret = []\n    for bs1 in left_list:\n        res = copy.deepcopy(bs1)\n        for bs2 in right_list:\n            for el in res['elements'].keys():\n                if not el in bs2['elements']:\n                    continue  # Element only exist in left\n\n                eldata1 = res['elements'][el]\n                eldata2 = bs2['elements'][el]\n\n                s1 = eldata1['electron_shells']\n                s2 = eldata2['electron_shells']\n                eldata1['electron_shells'] = subtract_electron_shells(s1, s2)\n\n        # Remove any empty elements\n        res['elements'] = {k: v for k, v in res['elements'].items() if len(v['electron_shells']) > 0}\n        ret.append(res)\n\n    return ret", "response": "Compute the difference between two sets of basis set dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff_json_files(left_files, right_files):\n    '''\n    Compute the difference between two sets of basis set JSON files\n\n    The output is a set of files that correspond to each file in\n    `left_files`. Each resulting dictionary will contain only the elements/shells\n    that exist in that entry and not in any of the files in `right_files`.\n\n    This only works on the shell level, and will only subtract entire shells\n    that are identical. ECP potentials are not affected.\n\n    `left_files` and `right_files` are lists of file paths. The output\n    is written to files with the same names as those in `left_files`,\n    but with `.diff` added to the end. If those files exist, they are overwritten.\n\n    Parameters\n    ----------\n    left_files : list of str\n        Paths to JSON files to use as the base\n    right_files : list of str\n        Paths to JSON files to subtract from each file of `left_files`\n\n    Returns\n    ----------\n    None\n    '''\n\n    left_data = [fileio.read_json_basis(x) for x in left_files]\n    right_data = [fileio.read_json_basis(x) for x in right_files]\n    d = diff_basis_dict(left_data, right_data)\n\n    for idx, diff_bs in enumerate(d):\n        fpath = left_files[idx]\n        fileio.write_json_basis(fpath + '.diff', diff_bs)", "response": "Compute the difference between two sets of basis set JSON files and return a new base base"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shells_difference(s1, s2):\n\n    max_rdiff = 0.0\n    nsh = len(s1)\n    if len(s2) != nsh:\n        print(\"Different number of shells: {} vs {}\".format(len(s1), len(s2)))\n        return float('inf')\n\n    shells1 = sort_shells(s1)\n    shells2 = sort_shells(s2)\n\n    for n in range(nsh):\n        sh1 = shells1[n]\n        sh2 = shells2[n]\n\n        if sh1['angular_momentum'] != sh2['angular_momentum']:\n            print(\"Different angular momentum for shell {}\".format(n))\n            return float('inf')\n\n        nprim = len(sh1['exponents'])\n        if len(sh2['exponents']) != nprim:\n            print(\"Different number of primitives for shell {}\".format(n))\n            return float('inf')\n\n        ngen = len(sh1['coefficients'])\n        if len(sh2['coefficients']) != ngen:\n            print(\"Different number of general contractions for shell {}\".format(n))\n            return float('inf')\n\n        for p in range(nprim):\n            e1 = sh1['exponents'][p]\n            e2 = sh2['exponents'][p]\n            r = _reldiff(e1, e2)\n            if r > 0.0:\n                print(\"   Exponent {:3}: {:20} {:20} -> {:16.8e}\".format(p, e1, e2, r))\n            max_rdiff = max(max_rdiff, r)\n\n            for g in range(ngen):\n                c1 = sh1['coefficients'][g][p]\n                c2 = sh2['coefficients'][g][p]\n                r = _reldiff(c1, c2)\n                if r > 0.0:\n                    print(\"Coefficient {:3}: {:20} {:20} -> {:16.8e}\".format(p, c1, c2, r))\n                max_rdiff = max(max_rdiff, r)\n\n    print()\n    print(\"Max relative difference for these shells: {}\".format(max_rdiff))\n    return max_rdiff", "response": "Compute the differences between two lists of shells and returns the maximum relative difference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the differences between two lists of potentials and returns the maximum relative difference.", "response": "def potentials_difference(p1, p2):\n    \"\"\"\n    Computes and prints the differences between two lists of potentials\n\n    If the shells contain a different number primitives,\n    or the lists are of different length, inf is returned.\n    Otherwise, the maximum relative difference is returned.\n    \"\"\"\n\n    max_rdiff = 0.0\n    np = len(p1)\n    if len(p2) != np:\n        print(\"Different number of potentials\")\n        return float('inf')\n\n    pots1 = sort_potentials(p1)\n    pots2 = sort_potentials(p2)\n\n    for n in range(np):\n        pot1 = pots1[n]\n        pot2 = pots2[n]\n\n        if pot1['angular_momentum'] != pot2['angular_momentum']:\n            print(\"Different angular momentum for potential {}\".format(n))\n            return float('inf')\n\n        nprim = len(pot1['gaussian_exponents'])\n        if len(pot2['gaussian_exponents']) != nprim:\n            print(\"Different number of primitives for potential {}\".format(n))\n            return float('inf')\n\n        ngen = len(pot1['coefficients'])\n        if len(pot2['coefficients']) != ngen:\n            print(\"Different number of general contractions for potential {}\".format(n))\n            return float('inf')\n\n        for p in range(nprim):\n            e1 = pot1['gaussian_exponents'][p]\n            e2 = pot2['gaussian_exponents'][p]\n            r = _reldiff(e1, e2)\n            if r > 0.0:\n                print(\"   Gaussian Exponent {:3}: {:20} {:20} -> {:16.8e}\".format(p, e1, e2, r))\n            max_rdiff = max(max_rdiff, r)\n\n            e1 = pot1['r_exponents'][p]\n            e2 = pot2['r_exponents'][p]\n            r = _reldiff(e1, e2)\n            if r > 0.0:\n                print(\"          R Exponent {:3}: {:20} {:20} -> {:16.8e}\".format(p, e1, e2, r))\n            max_rdiff = max(max_rdiff, r)\n\n            for g in range(ngen):\n                c1 = pot1['coefficients'][g][p]\n                c2 = pot2['coefficients'][g][p]\n                r = _reldiff(c1, c2)\n                if r > 0.0:\n                    print(\"         Coefficient {:3}: {:20} {:20} -> {:16.8e}\".format(p, c1, c2, r))\n                max_rdiff = max(max_rdiff, r)\n\n    print()\n    print(\"Max relative difference for these potentials: {}\".format(max_rdiff))\n    return max_rdiff"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomparing two basis set dictionaries and prints a report about their differences.", "response": "def basis_comparison_report(bs1, bs2, uncontract_general=False):\n    '''\n    Compares two basis set dictionaries and prints a report about their differences\n    '''\n\n    all_bs1 = list(bs1['elements'].keys())\n\n    if uncontract_general:\n        bs1 = manip.uncontract_general(bs1)\n        bs2 = manip.uncontract_general(bs2)\n\n    not_in_bs1 = []  # Found in bs2, not in bs1\n    not_in_bs2 = all_bs1.copy()  # Found in bs1, not in bs2\n    no_diff = []  # Elements for which there is no difference\n    some_diff = []  # Elements that are different\n    big_diff = []  # Elements that are substantially different\n\n    for k, v in bs2['elements'].items():\n        if k not in all_bs1:\n            not_in_bs1.append(k)\n            continue\n\n        print()\n        print(\"-------------------------------------\")\n        print(\" Element \", k)\n        bs1_el = bs1['elements'][k]\n\n        max_rdiff_el = 0.0\n        max_rdiff_ecp = 0.0\n\n        # Check to make sure that neither or both have ecp/electron shells\n        if 'electron_shells' in v and 'electron_shells' not in bs1_el:\n            print(\"bs2 has electron_shells, but bs1 does not\")\n            max_rdiff_el = float('inf')\n        if 'electron_shells' in bs1_el and 'electron_shells' not in v:\n            print(\"bs1 has electron_shells, but bs2 does not\")\n            max_rdiff_el = float('inf')\n        if 'ecp_potentials' in v and 'ecp_potentials' not in bs1_el:\n            print(\"bs2 has ecp_potentials, but bs1 does not\")\n            max_rdiff_ecp = float('inf')\n        if 'ecp_potentials' in bs1_el and 'ecp_potentials' not in v:\n            print(\"bs1 has ecp_potentials, but bs2 does not\")\n            max_rdiff_ecp = float('inf')\n\n        if 'electron_shells' in v and 'electron_shells' in bs1_el:\n            max_rdiff_el = max(max_rdiff_el, shells_difference(v['electron_shells'], bs1_el['electron_shells']))\n        if 'ecp_potentials' in v and 'ecp_potentials' in bs1_el:\n            nel1 = v['ecp_electrons']\n            nel2 = bs1_el['ecp_electrons']\n            if int(nel1) != int(nel2):\n                print('Different number of electrons replaced by ECP ({} vs {})'.format(nel1, nel2))\n                max_rdiff_ecp = float('inf')\n            else:\n                max_rdiff_ecp = max(max_rdiff_ecp, potentials_difference(v['ecp_potentials'],\n                                                                         bs1_el['ecp_potentials']))\n\n        max_rdiff = max(max_rdiff_el, max_rdiff_ecp)\n\n        # Handle some differences\n        if max_rdiff == float('inf'):\n            big_diff.append(k)\n        elif max_rdiff == 0.0:\n            no_diff.append(k)\n        else:\n            some_diff.append(k)\n\n        not_in_bs2.remove(k)\n\n    print()\n    print(\"     Not in bs1: \", _print_list(not_in_bs1))\n    print(\"     Not in bs2: \", _print_list(not_in_bs2))\n    print(\"  No difference: \", _print_list(no_diff))\n    print(\"Some difference: \", _print_list(some_diff))\n    print(\" BIG difference: \", _print_list(big_diff))\n    print()\n\n    return (len(not_in_bs1) == 0 and len(not_in_bs2) == 0 and len(some_diff) == 0 and len(big_diff) == 0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare_basis_against_file(basis_name,\n                               src_filepath,\n                               file_type=None,\n                               version=None,\n                               uncontract_general=False,\n                               data_dir=None):\n    '''Compare a basis set in the BSE against a reference file'''\n\n    src_data = read_formatted_basis(src_filepath, file_type)\n    bse_data = get_basis(basis_name, version=version, data_dir=data_dir)\n    return basis_comparison_report(src_data, bse_data, uncontract_general=uncontract_general)", "response": "Compare a basis set in the BSE against a reference file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compare_basis_files(file_path_1, file_path_2, file_type_1=None, file_type_2=None, uncontract_general=False):\n    '''Compare two files containing formatted basis sets'''\n\n    bs1 = read_formatted_basis(file_path_1, file_type_1)\n    bs2 = read_formatted_basis(file_path_2, file_type_2)\n    return basis_comparison_report(bs1, bs2, uncontract_general)", "response": "Compare two files containing formatted basis sets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomparing two files containing formatted basis sets", "response": "def compare_basis_sets(basis_name_1,\n                       basis_name_2,\n                       version_1=None,\n                       version_2=None,\n                       uncontract_general=False,\n                       data_dir_1=None,\n                       data_dir_2=None):\n    '''Compare two files containing formatted basis sets'''\n\n    bs1 = get_basis(basis_name_1, version=version_1, data_dir=data_dir_1)\n    bs2 = get_basis(basis_name_2, version=version_2, data_dir=data_dir_2)\n    return basis_comparison_report(bs1, bs2, uncontract_general)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bse_cli_list_basis_sets(args):\n    '''Handles the list-basis-sets subcommand'''\n    metadata = api.filter_basis_sets(args.substr, args.family, args.role, args.data_dir)\n\n    if args.no_description:\n        liststr = metadata.keys()\n    else:\n        liststr = format_columns([(k, v['description']) for k, v in metadata.items()])\n\n    return '\\n'.join(liststr)", "response": "Handles the list - basis - sets subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the list - formats subcommand", "response": "def _bse_cli_list_formats(args):\n    '''Handles the list-formats subcommand'''\n    all_formats = api.get_formats()\n\n    if args.no_description:\n        liststr = all_formats.keys()\n    else:\n        liststr = format_columns(all_formats.items())\n\n    return '\\n'.join(liststr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the list - ref - formats subcommand", "response": "def _bse_cli_list_ref_formats(args):\n    '''Handles the list-ref-formats subcommand'''\n    all_refformats = api.get_reference_formats()\n\n    if args.no_description:\n        liststr = all_refformats.keys()\n    else:\n        liststr = format_columns(all_refformats.items())\n\n    return '\\n'.join(liststr)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bse_cli_list_roles(args):\n    '''Handles the list-roles subcommand'''\n    all_roles = api.get_roles()\n\n    if args.no_description:\n        liststr = all_roles.keys()\n    else:\n        liststr = format_columns(all_roles.items())\n\n    return '\\n'.join(liststr)", "response": "Handles the list - roles subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle the lookup - by - role subcommand", "response": "def _bse_cli_lookup_by_role(args):\n    '''Handles the lookup-by-role subcommand'''\n    return api.lookup_basis_by_role(args.basis, args.role, args.data_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bse_cli_get_basis(args):\n    '''Handles the get-basis subcommand'''\n\n    return api.get_basis(\n        name=args.basis,\n        elements=args.elements,\n        version=args.version,\n        fmt=args.fmt,\n        uncontract_general=args.unc_gen,\n        uncontract_spdf=args.unc_spdf,\n        uncontract_segmented=args.unc_seg,\n        make_general=args.make_gen,\n        optimize_general=args.opt_gen,\n        data_dir=args.data_dir,\n        header=not args.noheader)", "response": "Handles the get - basis subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bse_cli_get_refs(args):\n    '''Handles the get-refs subcommand'''\n    return api.get_references(\n        basis_name=args.basis, elements=args.elements, version=args.version, fmt=args.reffmt, data_dir=args.data_dir)", "response": "Handles the get - refs subcommand"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the get - info subcommand", "response": "def _bse_cli_get_info(args):\n    '''Handles the get-info subcommand'''\n\n    bs_meta = api.get_metadata(args.data_dir)[args.basis]\n    ret = []\n    ret.append('-' * 80)\n    ret.append(args.basis)\n    ret.append('-' * 80)\n    ret.append('    Display Name: ' + bs_meta['display_name'])\n    ret.append('     Description: ' + bs_meta['description'])\n    ret.append('            Role: ' + bs_meta['role'])\n    ret.append('          Family: ' + bs_meta['family'])\n    ret.append('  Function Types: ' + ','.join(bs_meta['functiontypes']))\n    ret.append('  Latest Version: ' + bs_meta['latest_version'])\n    ret.append('')\n\n    aux = bs_meta['auxiliaries']\n    if len(aux) == 0:\n        ret.append('Auxiliary Basis Sets: None')\n    else:\n        ret.append('Auxiliary Basis Sets:')\n        ret.extend(format_columns(list(aux.items()), '    '))\n\n    ver = bs_meta['versions']\n    ret.append('')\n    ret.append('Versions:')\n\n    # Print 3 columns - version, elements, revision description\n    version_lines = format_columns([(k, compact_elements(v['elements']), v['revdesc']) for k, v in ver.items()],\n                                    '    ')\n    ret.extend(version_lines)\n\n    return '\\n'.join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling the get - versions subcommand", "response": "def _bse_cli_get_versions(args):\n    '''Handles the get-versions subcommand'''\n    name = args.basis.lower()\n    metadata = api.get_metadata(args.data_dir)\n    if not name in metadata:\n        raise KeyError(\n            \"Basis set {} does not exist. For a complete list of basis sets, use the 'list-basis-sets' command\".format(\n                name))\n\n    version_data = {k: v['revdesc'] for k, v in metadata[name]['versions'].items()}\n\n    if args.no_description:\n        liststr = version_data.keys()\n    else:\n        liststr = format_columns(version_data.items())\n\n    return '\\n'.join(liststr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the create - bundle subcommand", "response": "def _bse_cli_create_bundle(args):\n    '''Handles the create-bundle subcommand'''\n    bundle.create_bundle(args.bundle_file, args.fmt, args.reffmt, args.archive_type, args.data_dir)\n    return \"Created \" + args.bundle_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_cfour(basis):\n    '''Converts a basis set to cfour\n    '''\n\n    # March 2019\n    # Format determined from http://slater.chemie.uni-mainz.de/cfour/index.php?n=Main.NewFormatOfAnEntryInTheGENBASFile\n\n    # Uncontract all, then make general\n    basis = manip.uncontract_spdf(basis, 0, True)\n    basis = manip.make_general(basis, False)\n    basis = sort.sort_basis(basis, False)\n\n    # Elements for which we have electron basis\n    electron_elements = [k for k, v in basis['elements'].items() if 'electron_shells' in v]\n\n    # Elements for which we have ECP\n    ecp_elements = [k for k, v in basis['elements'].items() if 'ecp_potentials' in v]\n\n    s = '\\n'\n\n    if len(electron_elements) > 0:\n        # Electron Basis\n        for z in electron_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z).upper()\n            nshell = len(data['electron_shells'])\n\n            s += '{}:{}\\n'.format(sym, basis['name'])\n            s += basis['description'] + '\\n'\n            s += '\\n'\n            s += '{:>3}\\n'.format(nshell)\n\n            s_am = ''\n            s_ngen = ''\n            s_nprim = ''\n            for sh in data['electron_shells']:\n                s_am += '{:>5}'.format(sh['angular_momentum'][0])\n                s_ngen += '{:>5}'.format(len(sh['coefficients']))\n                s_nprim += '{:>5}'.format(len(sh['exponents']))\n\n            s += s_am + '\\n'\n            s += s_ngen + '\\n'\n            s += s_nprim + '\\n'\n            s += '\\n'\n\n            for shell in data['electron_shells']:\n                exponents = shell['exponents']\n                coefficients = shell['coefficients']\n                coefficients = list(map(list, zip(*coefficients)))\n\n                s += '  '.join(exponents).replace(\"E\", \"D\") + '\\n\\n'\n                for c in coefficients:\n                    s += '  '.join(c).replace(\"E\", \"D\") + '\\n'\n                s += '\\n'\n\n    # Write out ECP\n    if len(ecp_elements) > 0:\n        s += '\\n\\n! Effective core Potentials\\n'\n\n        for z in ecp_elements:\n            data = basis['elements'][z]\n            sym = lut.element_sym_from_Z(z).upper()\n            max_ecp_am = max([x['angular_momentum'][0] for x in data['ecp_potentials']])\n            max_ecp_amchar = lut.amint_to_char([max_ecp_am]).lower()\n\n            # Sort lowest->highest, then put the highest at the beginning\n            ecp_list = sorted(data['ecp_potentials'], key=lambda x: x['angular_momentum'])\n            ecp_list.insert(0, ecp_list.pop())\n\n            s += '*\\n'\n            s += '{}:{}\\n'.format(sym, basis['name'])\n            s += '# ' + basis['description'] + '\\n'\n            s += '*\\n'\n            s += '    NCORE = {}    LMAX = {}\\n'.format(data['ecp_electrons'], max_ecp_am)\n\n            for pot in ecp_list:\n                rexponents = pot['r_exponents']\n                gexponents = pot['gaussian_exponents']\n                coefficients = pot['coefficients']\n\n                am = pot['angular_momentum']\n                amchar = lut.amint_to_char(am).lower()\n\n                if am[0] == max_ecp_am:\n                    s += '{}\\n'.format(amchar)\n                else:\n                    s += '{}-{}\\n'.format(amchar, max_ecp_amchar)\n\n                point_places = [6, 18, 25]\n                s += printing.write_matrix([*coefficients, rexponents, gexponents], point_places)\n                #for p in range(len(rexponents)):\n                #    s += '{}  {}  {};\\n'.format(gexponents[p], rexponents[p], coefficients[0][p])\n            s += '*\\n'\n    return s", "response": "Converts a basis set to cfour file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_basis(bs_file,\n              data_dir,\n              subdir,\n              file_base,\n              name,\n              family,\n              role,\n              description,\n              version,\n              revision_description,\n              data_source,\n              refs=None,\n              file_fmt=None):\n    '''\n    Add a basis set to this library\n\n    This takes in a single file containing the basis set is some format, parses it, and\n    create the component, element, and table basis set files in the given data_dir (and subdir).\n    The metadata file for the basis is created if it doesn't exist, and the main metadata file is\n    also updated.\n    \n    Parameters\n    ----------\n    bs_file : str\n        Path to the file with formatted basis set information\n    data_dir : str\n        Path to the data directory to add the data to\n    subdir : str\n        Subdirectory of the data directory to add the basis set to\n    file_base : str\n        Base name for new files\n    name : str\n        Name of the basis set\n    family : str\n        Family to which this basis set belongs\n    role : str\n        Role of the basis set (orbital, etc)\n    description : str\n        Description of the basis set\n    version : str\n        Version of the basis set\n    revision_description : str\n        Description of this version of the basis set\n    data_source : str\n        Description of where this data came from\n    refs : dict or str\n        Mapping of references to elements. This can be a dictionary with a compressed\n        string of elements as keys and a list of reference strings as values.\n        For example, {'H,Li-B,Kr': ['kumar2018a']}\n\n        If a list or string is passed, then those reference(s) will be used for\n        all elements.\n\n        Elements that exist in the file but do not have a reference are given the\n        usual 'noref' extension and the references entry is empty.\n    file_fmt : str\n        Format of the input basis data (None = autodetect)\n    '''\n\n    # Read the basis set data into a component file, and add the description\n    bs_data = read_formatted_basis(bs_file, file_fmt)\n    bs_data['description'] = description\n    bs_data['data_source'] = data_source\n\n    if refs is None:\n        refs = []\n\n    # Split out the component data into files based on the reference\n    # information. We keep track of which elements we've done so that\n    # we can detect duplicates in the references (which would be an error)\n    # (and also handle elements with no reference)\n    orig_elements = bs_data['elements']\n    done_elements = []\n\n    # If a string or list of strings, use that as a reference for all elements\n    if isinstance(refs, str):\n        for k, v in bs_data['elements'].items():\n            v['references'] = [refs]\n    elif isinstance(refs, list):\n        for k, v in bs_data['elements'].items():\n            v['references'] = refs\n    else:\n        for k, v in refs.items():\n            # Expand the string a list of integers (as strings)\n            elements = expand_elements(k, True)\n\n            # Make sure we have info for the given elements\n            # and that there are no duplicates\n            for el in elements:\n                if not el in orig_elements:\n                    raise RuntimeError(\"Element {} not found in file {}\".format(el, bs_file))\n                if el in done_elements:\n                    raise RuntimeError(\"Duplicate element {} in reference string {}\".format(el, k))\n\n                if isinstance(v, str):\n                    bs_data['elements'][el]['references'] = [v]\n                else:\n                    bs_data['elements'][el]['references'] = v\n\n            done_elements.extend(elements)\n\n        # Handle elements without a reference\n        noref_elements = set(orig_elements.keys()) - set(done_elements)\n\n        if len(noref_elements) > 0:\n            for el in noref_elements:\n                bs_data['elements'][el]['references'] = []\n\n    # Start the data files for the element and table json\n    element_file_data = create_skel('element')\n    element_file_data['name'] = name\n    element_file_data['description'] = description\n    element_file_name = '{}.{}.element.json'.format(file_base, version)\n    element_file_relpath = os.path.join(subdir, element_file_name)\n    element_file_path = os.path.join(data_dir, element_file_relpath)\n\n    table_file_data = create_skel('table')\n    table_file_data['revision_description'] = revision_description\n    table_file_name = '{}.{}.table.json'.format(file_base, version)\n\n    # and the metadata file\n    meta_file_data = create_skel('metadata')\n    meta_file_data['names'] = [name]\n    meta_file_data['family'] = family\n    meta_file_data['description'] = description\n    meta_file_data['role'] = role\n    meta_file_name = '{}.metadata.json'.format(file_base)\n\n    # These get created directly in the top-level data directory\n    table_file_path = os.path.join(data_dir, table_file_name)\n    meta_file_path = os.path.join(data_dir, meta_file_name)\n\n    # Can just make all the entries for the table file pretty easily\n    table_file_entry = element_file_relpath\n    table_file_data['elements'] = {k: table_file_entry for k in bs_data['elements'].keys()}\n\n    # Create the filenames for the components\n    # Also keep track of where data for each element is (for the element and table files)\n    component_file_name = file_base + '.' + str(version) + '.json'\n    component_file_relpath = os.path.join(subdir, component_file_name)\n    component_file_path = os.path.join(data_dir, component_file_relpath)\n\n    # Add to the element file data\n    # (we add the relative path to the location of the element file,\n    # which resides in subdir)\n    for el in bs_data['elements'].keys():\n        element_file_data['elements'][el] = {'components': [component_file_relpath]}\n\n    # Verify all data using the schema\n    validate_data('component', bs_data)\n    validate_data('element', element_file_data)\n    validate_data('table', table_file_data)\n\n    ######################################################################################\n    # Before creating any files, check that all the files don't already exist.\n    # Yes, there is technically a race condition (files could be created between the\n    # check and then actually writing out), but if that happens, you are probably using\n    # this library wrong\n    #\n    # Note that the metadata file may exist already. That is ok\n    ######################################################################################\n    if os.path.exists(component_file_path):\n        raise RuntimeError(\"Component json file {} already exists\".format(component_file_path))\n    if os.path.exists(element_file_path):\n        raise RuntimeError(\"Element json file {} already exists\".format(element_file_path))\n    if os.path.exists(table_file_path):\n        raise RuntimeError(\"Table json file {} already exists\".format(table_file_path))\n\n    #############################################\n    # Actually create all the files\n    #############################################\n\n    # First, create the subdirectory\n    subdir_path = os.path.join(data_dir, subdir)\n    if not os.path.exists(subdir_path):\n        os.makedirs(subdir_path)\n\n    write_json_basis(component_file_path, bs_data)\n    write_json_basis(element_file_path, element_file_data)\n    write_json_basis(table_file_path, table_file_data)\n\n    # Create the metadata file if it doesn't exist already\n    if not os.path.exists(meta_file_path):\n        write_json_basis(meta_file_path, meta_file_data)\n\n    # Update the metadata file\n    metadata_file = os.path.join(data_dir, 'METADATA.json')\n    create_metadata_file(metadata_file, data_dir)", "response": "This function adds a basis set to the library."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of required fields for each template.", "response": "def _req(self):\n    \"\"\"\n    List of required fields for each template. Format is [tmpl_idx, \"all\"|\"any\", [req_field_1, req_field_2, ...]].\n\n    Partial reimplementation of req computing logic from Anki. We use pystache instead of Anki's custom mustache\n    implementation.\n\n    The goal is to figure out which fields are \"required\", i.e. if they are missing then the front side of the note\n    doesn't contain any meaningful content.\n    \"\"\"\n    sentinel = 'SeNtInEl'\n    field_names = [field['name'] for field in self.fields]\n\n    req = []\n    for template_ord, template in enumerate(self.templates):\n      field_values = {field: sentinel for field in field_names}\n      required_fields = []\n      for field_ord, field in enumerate(field_names):\n        fvcopy = copy(field_values)\n        fvcopy[field] = ''\n\n        rendered = pystache.render(template['qfmt'], fvcopy)\n\n        if sentinel not in rendered:\n          # when this field is missing, there is no meaningful content (no field values) in the question, so this field\n          # is required\n          required_fields.append(field_ord)\n\n      if required_fields:\n        req.append([template_ord, 'all', required_fields])\n        continue\n\n      # there are no required fields, so an \"all\" is not appropriate, switch to checking for \"any\"\n      field_values = {field: '' for field in field_names}\n      for field_ord, field in enumerate(field_names):\n        fvcopy = copy(field_values)\n        fvcopy[field] = sentinel\n\n        rendered = pystache.render(template['qfmt'], fvcopy)\n\n        if sentinel in rendered:\n          # when this field is present, there is meaningful content in the question\n          required_fields.append(field_ord)\n\n      if not required_fields:\n        raise Exception(\n          'Could not compute required fields for this template; please check the formatting of \"qfmt\": {}'.format(\n            template))\n\n      req.append([template_ord, 'any', required_fields])\n\n    return req"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_to_collection_from_addon(self):\n    from aqt import mw  # main window\n    from anki.importing.apkg import AnkiPackageImporter\n\n    tmpfilename = tempfile.NamedTemporaryFile(delete=False).name\n    self.write_to_file(tmpfilename)\n    AnkiPackageImporter(mw.col, tmpfilename).run()", "response": "Writes to local collection. Only usable when running inside an addon!"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef urljoin(*urls):\n    return reduce(_urljoin, [u.strip('/')+'/' for u in urls if u.strip('/')], '').rstrip('/')", "response": "A convenience function to join urls with a base url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_ready(self, timeout=120):\n        deadline = time.time() + timeout\n        while time.time() < deadline:\n            try:\n                self.status()\n                return True\n            except:\n                time.sleep(2)\n        return False", "response": "Wait until WDA is ready."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef source(self, format='xml', accessible=False):\n        if accessible:\n            return self.http.get('/wda/accessibleSource').value\n        return self.http.get('source?format='+format).value", "response": "Returns the source of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new session for the app.", "response": "def session(self, bundle_id=None, arguments=None, environment=None):\n        \"\"\"\n        Args:\n            - bundle_id (str): the app bundle id\n            - arguments (list): ['-u', 'https://www.google.com/ncr']\n            - enviroment (dict): {\"KEY\": \"VAL\"}\n\n        WDA Return json like\n\n        {\n            \"value\": {\n                \"sessionId\": \"69E6FDBA-8D59-4349-B7DE-A9CA41A97814\",\n                \"capabilities\": {\n                    \"device\": \"iphone\",\n                    \"browserName\": \"\u90e8\u843d\u51b2\u7a81\",\n                    \"sdkVersion\": \"9.3.2\",\n                    \"CFBundleIdentifier\": \"com.supercell.magic\"\n                }\n            },\n            \"sessionId\": \"69E6FDBA-8D59-4349-B7DE-A9CA41A97814\",\n            \"status\": 0\n        }\n\n        To create a new session, send json data like\n\n        {\n            \"desiredCapabilities\": {\n                \"bundleId\": \"your-bundle-id\",\n                \"app\": \"your-app-path\"\n                \"shouldUseCompactResponses\": (bool),\n                \"shouldUseTestManagerForVisibilityDetection\": (bool),\n                \"maxTypingFrequency\": (integer),\n                \"arguments\": (list(str)),\n                \"environment\": (dict: str->str)\n            },\n        }\n        \"\"\"\n        if bundle_id is None:\n            sid = self.status()['sessionId']\n            if not sid:\n                raise RuntimeError(\"no session created ever\")\n            http = self.http.new_client('session/'+sid)\n            return Session(http, sid)\n\n        if arguments and type(arguments) is not list:\n            raise TypeError('arguments must be a list')\n\n        if environment and type(environment) is not dict:\n            raise TypeError('environment must be a dict')\n\n        capabilities = {\n            'bundleId': bundle_id,\n            'arguments': arguments,\n            'environment': environment,\n            'shouldWaitForQuiescence': True,\n        }\n        # Remove empty value to prevent WDAError\n        for k in list(capabilities.keys()):\n            if capabilities[k] is None:\n                capabilities.pop(k)\n\n        data = json.dumps({\n            'desiredCapabilities': capabilities\n        })\n        res = self.http.post('session', data)\n        httpclient = self.http.new_client('session/'+res.sessionId)\n        return Session(httpclient, res.sessionId)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the raw data of the current user s screenshot.", "response": "def screenshot(self, png_filename=None, format='raw'):\n        \"\"\"\n        Screenshot with PNG format\n\n        Args:\n            png_filename(string): optional, save file name\n            format(string): return format, pillow or raw(default)\n        Returns:\n            raw data or PIL.Image\n        \n        Raises:\n            WDAError\n        \"\"\"\n        value = self.http.get('screenshot').value\n        raw_value = base64.b64decode(value)\n        png_header = b\"\\x89PNG\\r\\n\\x1a\\n\"\n        if not raw_value.startswith(png_header) and png_filename:\n            raise WDAError(-1, \"screenshot png format error\")\n\n        if png_filename:\n            with open(png_filename, 'wb') as f:\n                f.write(raw_value)\n\n        if format == 'raw':\n            return raw_value\n        elif format == 'pillow':\n            from PIL import Image\n            buff = io.BytesIO(raw_value)\n            return Image.open(buff)\n        else:\n            raise ValueError(\"unknown format\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the UIKit scale factor", "response": "def scale(self):\n        \"\"\"\n        UIKit scale factor\n        \n        Refs:\n            https://developer.apple.com/library/archive/documentation/DeviceInformation/Reference/iOSDeviceCompatibility/Displays/Displays.html\n        \"\"\"\n        if self.__scale:\n            return self.__scale\n        v = max(self.screenshot().size) / max(self.window_size())\n        self.__scale = round(v)\n        return self.__scale"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_alert_callback(self, callback):\n        if callable(callable):\n            self.http.alert_callback = functools.partial(callback, self)\n        else:\n            self.http.alert_callback = None", "response": "Set the alert callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef click(self, x, y):\n        if isinstance(x, float) or isinstance(y, float):\n            x, y = self._percent2pos(x, y)\n        return self.tap(x, y)", "response": "tap the mouse button x y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tap_hold(self, x, y, duration=1.0):\n        data = {'x': x, 'y': y, 'duration': duration}\n        return self.http.post('/wda/touchAndHold', data=data)", "response": "Tap and hold for a moment\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef screenshot(self):\n        b64data = self.http.get('/screenshot').value\n        raw_data = base64.b64decode(b64data)\n        from PIL import Image\n        buff = io.BytesIO(raw_data)\n        return Image.open(buff)", "response": "Take screenshot with session check\n        Returns PIL. Image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nswipe from one location to another.", "response": "def swipe(self, x1, y1, x2, y2, duration=0):\n        \"\"\"\n        Args:\n            duration (float): start coordinate press duration (seconds)\n\n        [[FBRoute POST:@\"/wda/dragfromtoforduration\"] respondWithTarget:self action:@selector(handleDragCoordinate:)],\n        \"\"\"\n        data = dict(fromX=x1, fromY=y1, toX=x2, toY=y2, duration=duration)\n        return self.http.post('/wda/dragfromtoforduration', data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a namedtuple that represents the width and height of the window.", "response": "def window_size(self):\n        \"\"\"\n        Returns:\n            namedtuple: eg\n                Size(width=320, height=568)\n        \"\"\"\n        value = self.http.get('/window/size').value\n        w = roundint(value['width'])\n        h = roundint(value['height'])\n        return namedtuple('Size', ['width', 'height'])(w, h)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_keys(self, value):\n        if isinstance(value, six.string_types):\n            value = list(value)\n        return self.http.post('/wda/keys', data={'value': value})", "response": "send keys to the user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a character to be escaped for quotes.", "response": "def _add_escape_character_for_quote_prime_character(self, text):\n        \"\"\"\n        Fix for https://github.com/openatx/facebook-wda/issues/33\n        Returns:\n            string with properly formated quotes, or non changed text\n        \"\"\"\n        if text is not None:\n          if \"'\" in text:\n            return text.replace(\"'\",\"\\\\'\")\n          elif '\"' in text:\n            return text.replace('\"','\\\\\"')\n          else:\n            return text\n        else:\n            return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all the available WDAA elements.", "response": "def _wdasearch(self, using, value):\n        \"\"\"\n        Returns:\n            element_ids (list(string)): example ['id1', 'id2']\n        \n        HTTP example response:\n        [\n            {\"ELEMENT\": \"E2FF5B2A-DBDF-4E67-9179-91609480D80A\"},\n            {\"ELEMENT\": \"597B1A1E-70B9-4CBE-ACAD-40943B0A6034\"}\n        ]\n        \"\"\"\n        element_ids = []\n        for v in self.http.post('/elements', {'using': using, 'value': value}).value:\n            element_ids.append(v['ELEMENT'])\n        return element_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all the elements in the cache", "response": "def find_elements(self):\n        \"\"\"\n        Returns:\n            Element (list): all the elements\n        \"\"\"\n        es = []\n        for element_id in self.find_element_ids():\n            e = Element(self.http.new_client(''), element_id)\n            es.append(e)\n        return es"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the next element from the list of elements in the session.", "response": "def get(self, timeout=None, raise_error=True):\n        \"\"\"\n        Args:\n            timeout (float): timeout for query element, unit seconds\n                Default 10s\n            raise_error (bool): whether to raise error if element not found\n\n        Returns:\n            Element: UI Element\n\n        Raises:\n            WDAElementNotFoundError if raise_error is True else None\n        \"\"\"\n        start_time = time.time()\n        if timeout is None:\n            timeout = self.timeout\n        while True:\n            elems = self.find_elements()\n            if len(elems) > 0:\n                return elems[0]\n            if start_time + timeout < time.time():\n                break\n            time.sleep(0.01)\n        \n        # check alert again\n        if self.session.alert.exists and self.http.alert_callback:\n            self.http.alert_callback()\n            return self.get(timeout, raise_error)\n\n        if raise_error:\n            raise WDAElementNotFoundError(\"element not found\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits element and perform click", "response": "def click_exists(self, timeout=0):\n        \"\"\"\n        Wait element and perform click\n\n        Args:\n            timeout (float): timeout for wait\n        \n        Returns:\n            bool: if successfully clicked\n        \"\"\"\n        e = self.get(timeout=timeout, raise_error=False)\n        if e is None:\n            return False\n        e.click()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait(self, timeout=None, raise_error=True):\n        return self.get(timeout=timeout, raise_error=raise_error)", "response": "alias of get\n        Args:\n            timeout (float): timeout seconds\n            raise_error (bool): default true, whether to raise error if element not found\n        \n        Raises:\n            WDAElementNotFoundError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait_gone(self, timeout=None, raise_error=True):\n        start_time = time.time()\n        if timeout is None or timeout <= 0:\n            timeout = self.timeout\n        while start_time + timeout > time.time():\n            if not self.exists:\n                return True\n        if not raise_error:\n            return False\n        raise WDAElementNotDisappearError(\"element not gone\")", "response": "Wait until the element is gone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscrolling the area of the current user s screen.", "response": "def scroll(self, direction='visible', distance=1.0):\n        \"\"\"\n        Args:\n            direction (str): one of \"visible\", \"up\", \"down\", \"left\", \"right\"\n            distance (float): swipe distance, only works when direction is not \"visible\"\n               \n        Raises:\n            ValueError\n\n        distance=1.0 means, element (width or height) multiply 1.0\n        \"\"\"\n        if direction == 'visible':\n            self._wda_req('post', '/scroll', {'toVisible': True})\n        elif direction in ['up', 'down', 'left', 'right']:\n            self._wda_req('post', '/scroll', {'direction': direction, 'distance': distance})\n        else:\n            raise ValueError(\"Invalid direction\")\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pinch(self, scale, velocity):\n        data = {'scale': scale, 'velocity': velocity}\n        return self._wda_req('post', '/pinch', data)", "response": "This function is used to pinch an entry in a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _save_feed(cls, conf, benchmarks, data, revisions, revision_to_hash):\n\n        filename = os.path.join(conf.html_dir, 'regressions.xml')\n\n        # Determine publication date as the date when the benchmark\n        # was run --- if it is missing, use the date of the commit\n        run_timestamps = {}\n        revision_timestamps = {}\n        for results in iter_results(conf.results_dir):\n            if results.commit_hash not in revisions:\n                # revisions could be filtered when specifying a range\n                # in 'asv publish'\n                continue\n            revision = revisions[results.commit_hash]\n            revision_timestamps[revision] = results.date\n\n            # Time when the benchmark was run\n            for benchmark_name, timestamp in six.iteritems(results.ended_at):\n                key = (benchmark_name, revision)\n                run_timestamps[key] = timestamp\n\n            # Fallback to commit date\n            for benchmark_name in results.get_result_keys(benchmarks):\n                key = (benchmark_name, revision)\n                run_timestamps.setdefault(key, results.date)\n\n        # Generate feed entries\n        entries = []\n\n        for name, graph_path, graph_params, idx, last_value, best_value, jumps in data:\n            if '(' in name:\n                benchmark_name = name[:name.index('(')]\n            else:\n                benchmark_name = name\n\n            benchmark = benchmarks[benchmark_name]\n\n            if idx is not None:\n                graph_params = dict(graph_params)\n\n                # Add URL parameters\n                param_values, = itertools.islice(itertools.product(*benchmark['params']),\n                                                 idx, idx + 1)\n                for k, v in zip(benchmark['param_names'], param_values):\n                    graph_params['p-' + k] = v\n\n            for rev1, rev2, value1, value2 in jumps:\n                timestamps = (run_timestamps[benchmark_name, t] for t in (rev1, rev2) if t is not None)\n                last_timestamp = max(timestamps)\n\n                updated = datetime.datetime.fromtimestamp(last_timestamp/1000)\n\n                params = dict(graph_params)\n\n                if rev1 is None:\n                    params['commits'] = '{0}'.format(revision_to_hash[rev2])\n                else:\n                    params['commits'] = '{0}-{1}'.format(revision_to_hash[rev1],\n                                                         revision_to_hash[rev2])\n\n                link = 'index.html#{0}?{1}'.format(benchmark_name, urlencode(params))\n\n                try:\n                    best_percentage = \"{0:.2f}%\".format(100 * (last_value - best_value) / best_value)\n                except ZeroDivisionError:\n                    best_percentage = \"{0:.2g} units\".format(last_value - best_value)\n\n                try:\n                    percentage = \"{0:.2f}%\".format(100 * (value2 - value1) / value1)\n                except ZeroDivisionError:\n                    percentage = \"{0:.2g} units\".format(value2 - value1)\n\n                jump_date = datetime.datetime.fromtimestamp(revision_timestamps[rev2]/1000)\n                jump_date_str = jump_date.strftime('%Y-%m-%d %H:%M:%S')\n\n                if rev1 is not None:\n                    commit_a = revision_to_hash[rev1]\n                    commit_b = revision_to_hash[rev2]\n                    if 'github.com' in conf.show_commit_url:\n                        commit_url = conf.show_commit_url + '../compare/' + commit_a + \"...\" + commit_b\n                    else:\n                        commit_url = conf.show_commit_url + commit_a\n                    commit_ref = 'in commits <a href=\"{0}\">{1}...{2}</a>'.format(commit_url,\n                                                                                 commit_a[:8],\n                                                                                 commit_b[:8])\n                else:\n                    commit_a = revision_to_hash[rev2]\n                    commit_url = conf.show_commit_url + commit_a\n                    commit_ref = 'in commit <a href=\"{0}\">{1}</a>'.format(commit_url, commit_a[:8])\n\n                unit = benchmark.get('unit', '')\n                best_value_str = util.human_value(best_value, unit)\n                last_value_str = util.human_value(last_value, unit)\n                value1_str = util.human_value(value1, unit)\n                value2_str = util.human_value(value2, unit)\n\n                title = \"{percentage} {name}\".format(**locals())\n                summary = \"\"\"\n                <a href=\"{link}\">{percentage} regression</a> on {jump_date_str} {commit_ref}.<br>\n                New value: {value2_str}, old value: {value1_str}.<br>\n                Latest value: {last_value_str} ({best_percentage} worse than best value {best_value_str}).\n                \"\"\".format(**locals()).strip()\n\n                # Information that uniquely identifies a regression\n                # --- if the values and the texts change on later\n                # runs, feed readers should is identify the regression\n                # as the same one, as long as the benchmark name and\n                # commits match.\n                id_context = [name, revision_to_hash.get(rev1, \"\"), revision_to_hash.get(rev2, \"\")]\n\n                entries.append(feed.FeedEntry(title, updated, link, summary, id_context))\n\n        entries.sort(key=lambda x: x.updated, reverse=True)\n\n        feed.write_atom(filename, entries,\n                        title='{0} performance regressions'.format(conf.project),\n                        author='Airspeed Velocity',\n                        address='{0}.asv'.format(conf.project))", "response": "Save the results of the benchmarks to an Atom feed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields the data set for the given benchmark.", "response": "def get_graph_data(self, graph, benchmark):\n        \"\"\"\n        Iterator over graph data sets\n\n        Yields\n        ------\n        param_idx\n            Flat index to parameter permutations for parameterized benchmarks.\n            None if benchmark is not parameterized.\n        entry_name\n            Name for the data set. If benchmark is non-parameterized, this is the\n            benchmark name.\n        steps\n            Steps to consider in regression detection.\n        threshold\n            User-specified threshold for regression detection.\n\n        \"\"\"\n        if benchmark.get('params'):\n            param_iter = enumerate(zip(itertools.product(*benchmark['params']),\n                                           graph.get_steps()))\n        else:\n            param_iter = [(None, (None, graph.get_steps()))]\n\n        for j, (param, steps) in param_iter:\n            if param is None:\n                entry_name = benchmark['name']\n            else:\n                entry_name = benchmark['name'] + '({0})'.format(', '.join(param))\n\n            start_revision = self._get_start_revision(graph, benchmark, entry_name)\n            threshold = self._get_threshold(graph, benchmark, entry_name)\n\n            if start_revision is None:\n                # Skip detection\n                continue\n\n            steps = [step for step in steps if step[1] >= start_revision]\n\n            yield j, entry_name, steps, threshold"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_start_revision(self, graph, benchmark, entry_name):\n        start_revision = min(six.itervalues(self.revisions))\n\n        if graph.params.get('branch'):\n            branch_suffix = '@' + graph.params.get('branch')\n        else:\n            branch_suffix = ''\n\n        for regex, start_commit in six.iteritems(self.conf.regressions_first_commits):\n            if re.match(regex, entry_name + branch_suffix):\n                if start_commit is None:\n                    # Disable regression detection completely\n                    return None\n\n                if self.conf.branches == [None]:\n                    key = (start_commit, None)\n                else:\n                    key = (start_commit, graph.params.get('branch'))\n\n                if key not in self._start_revisions:\n                    spec = self.repo.get_new_range_spec(*key)\n                    start_hash = self.repo.get_hash_from_name(start_commit)\n\n                    for commit in [start_hash] + self.repo.get_hashes_from_range(spec):\n                        rev = self.revisions.get(commit)\n                        if rev is not None:\n                            self._start_revisions[key] = rev\n                            break\n                    else:\n                        # Commit not found in the branch --- warn and ignore.\n                        log.warning((\"Commit {0} specified in `regressions_first_commits` \"\n                                  \"not found in branch\").format(start_commit))\n                        self._start_revisions[key] = -1\n\n                start_revision = max(start_revision, self._start_revisions[key] + 1)\n\n        return start_revision", "response": "Compute the first revision allowed by asv. conf. json."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_threshold(self, graph, benchmark, entry_name):\n        if graph.params.get('branch'):\n            branch_suffix = '@' + graph.params.get('branch')\n        else:\n            branch_suffix = ''\n\n        max_threshold = None\n\n        for regex, threshold in six.iteritems(self.conf.regressions_thresholds):\n            if re.match(regex, entry_name + branch_suffix):\n                try:\n                    threshold = float(threshold)\n                except ValueError:\n                    raise util.UserError(\"Non-float threshold in asv.conf.json: {!r}\".format(threshold))\n\n                if max_threshold is None:\n                    max_threshold = threshold\n                else:\n                    max_threshold = max(threshold, max_threshold)\n\n        if max_threshold is None:\n            max_threshold = 0.05\n\n        return max_threshold", "response": "Compute the regression threshold in asv. conf. json."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing current version number from the first assignment to the version number.", "response": "def get_version():\n    \"\"\"Parse current version number from __init__.py\"\"\"\n    # Grab the first assignment to __version__\n    version = None\n    init_py = os.path.join(os.path.dirname(__file__),\n                           'asv', '__init__.py')\n    with open(init_py, 'r') as f:\n        source = f.read()\n    tree = ast.parse(source)\n    for statement in tree.body:\n        if (isinstance(statement, ast.Assign) and\n            len(statement.targets) == 1 and\n            statement.targets[0].id == '__version__'):\n            version = statement.value.s\n            break\n\n    if not version:\n        raise RuntimeError(\"Failed to parse version from {}\".format(init_py))\n\n    if 'dev' in version and not version.endswith('.dev'):\n        raise RuntimeError(\"Dev version string in {} doesn't end in .dev\".format(\n            init_py))\n\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget git hash of the current version of the current asv archive", "response": "def get_git_hash():\n    \"\"\"\n    Get version from asv/__init__.py and generate asv/_version.py\n    \"\"\"\n    # Obtain git revision\n    githash = \"\"\n    if os.path.isdir(os.path.join(basedir, '.git')):\n        try:\n            proc = subprocess.Popen(\n                ['git', '-C', basedir, 'rev-parse', 'HEAD'],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            rev, err = proc.communicate()\n            if proc.returncode == 0:\n                githash = rev.strip().decode('ascii')\n        except OSError:\n            pass\n    return githash"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that the submodules are checked out and clean.", "response": "def __check_submodules(self):\n        \"\"\"\n        Verify that the submodules are checked out and clean.\n        \"\"\"\n        if not os.path.exists('.git'):\n            return\n        with open('.gitmodules') as f:\n            for l in f:\n                if 'path' in l:\n                    p = l.split('=')[-1].strip()\n                    if not os.path.exists(p):\n                        raise ValueError('Submodule %s missing' % p)\n\n        proc = subprocess.Popen(['git', 'submodule', 'status'],\n                                stdout=subprocess.PIPE)\n        status, _ = proc.communicate()\n        status = status.decode(\"ascii\", \"replace\")\n        for line in status.splitlines():\n            if line.startswith('-') or line.startswith('+'):\n                raise ValueError('Submodule not clean: %s' % line)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetecting the steps in a noisy signal.", "response": "def detect_steps(y, w=None):\n    \"\"\"\n    Detect steps in a (noisy) signal.\n\n    Parameters\n    ----------\n    y : list of float, none or nan\n        Single benchmark result series, with possible missing data\n    w : list of float, none or nan\n        Data point relative weights. Missing weights are set equal\n        to the median weight.\n\n    Returns\n    -------\n    steps : list of (left_pos, right_pos, value, min_value, err_est)\n        List containing a decomposition of the input data to a piecewise\n        constant function. Each element contains the left (inclusive) and\n        right (exclusive) bounds of a segment, the average value on \n        the segment, the minimum value in the segment, and the l1 error \n        estimate, <|Y - avg|>. Missing data points are not necessarily\n        contained in any segment; right_pos-1 is the last non-missing data\n        point.\n\n    \"\"\"\n\n    index_map = {}\n    y_filtered = []\n    for j, x in enumerate(y):\n        if x is None or x != x:\n            # None or NaN: missing data\n            continue\n        index_map[len(y_filtered)] = j\n        y_filtered.append(x)\n\n    # Weights\n    if w is None:\n        w_filtered = [1]*len(y_filtered)\n    else:\n        # Fill-in and normalize weights\n        w_valid = [ww for ww in w if ww is not None and ww == ww]\n        if w_valid:\n            w_median = median(w_valid)\n            if w_median == 0:\n                w_median = 1.0\n        else:\n            w_median = 1.0\n\n        w_filtered = [1.0]*len(y_filtered)\n        for j in range(len(w_filtered)):\n            jj = index_map[j]\n            if w[jj] is not None and w[jj] == w[jj]:\n                w_filtered[j] = w[jj] / w_median\n\n    # Find piecewise segments\n    right, values, dists, gamma = solve_potts_autogamma(y_filtered, w=w_filtered)\n\n    # Extract the steps, mapping indices back etc.\n    steps = []\n    l = 0\n    for r, v, d in zip(right, values, dists):\n        steps.append((index_map[l], index_map[r-1] + 1,\n                          v,\n                          min(y_filtered[l:r]),\n                          abs(d/(r - l))))\n        l = r\n    return steps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detect_regressions(steps, threshold=0):\n    if not steps:\n        # No data: no regressions\n        return None, None, None\n\n    regression_pos = []\n\n    last_v = steps[-1][2]\n    best_v = last_v\n    best_err = steps[-1][4]\n    prev_l = None\n\n    # Find upward steps that resulted to worsened value afterward\n    for l, r, cur_v, cur_min, cur_err in reversed(steps):\n        if best_v - cur_v > max(cur_err, best_err, threshold * cur_v):\n            regression_pos.append((r - 1, prev_l, cur_v, best_v))\n        prev_l = l\n        if cur_v < best_v:\n            best_v = cur_v\n            best_err = cur_err\n\n    regression_pos.reverse()\n\n    # Return results\n    if regression_pos:\n        return (last_v, best_v, regression_pos)\n    else:\n        return (None, None, None)", "response": "Detects regressions in a noisy signal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve_potts(y, w, gamma, min_size=1, max_size=None,\n                min_pos=None, max_pos=None, mu_dist=None):\n    \"\"\"Fit penalized stepwise constant function (Potts model) to data.\n\n    Given a time series y = {y_1, ..., y_n}, fit series x = {x_1, ..., x_n}\n    by minimizing the cost functional::\n\n        F[x] = gamma * J(x) + sum(|y - x|**p)\n\n    where J(x) is the number of jumps (x_{j+1} != x_j) in x.\n\n    The algorithm used is described in Ref. [1]_, it uses dynamic\n    programming to find an exact solution to the problem (within the\n    constraints specified).\n\n    Computation work is ~ O(n**2 log n).\n\n    Parameters\n    ----------\n    y : list of floats\n        Input data series\n    gamma : float\n        Penalty parameter.\n    min_size : int, optional\n        Minimum interval size to consider\n    max_size : int, optional\n        Maximum interval size to consider\n    mu_dist : *Dist, optional\n        Precomputed interval means/medians and cost function values\n    min_pos : int, optional\n        Start point (inclusive) for the interval grid\n    max_pos : int, optional\n        End point (exclusive) for the interval grid\n\n    Returns\n    -------\n    right : list\n        List of (exclusive) right bounds of the intervals\n    values : list\n        List of values of the intervals\n    dist : list\n        List of ``sum(|y - x|**p)`` for each interval.\n    mu_dist : *Dist\n        Precomputed interval means/medians and cost function values\n\n    References\n    ----------\n    [1] F. Friedrich et al., \"Complexity Penalized M-Estimation: Fast Computation\",\n        Journal of Computational and Graphical Statistics 17.1, 201-224 (2008).\n\n    \"\"\"\n\n    inf = float('inf')\n\n    if len(y) == 0:\n        return [], [], []\n\n    if min_pos is None:\n        min_pos = 0\n\n    if len(y) != len(w):\n        raise ValueError(\"y and w must have same size\")\n\n    if max_pos is None:\n        max_pos = len(y)\n\n    if mu_dist is None:\n        mu_dist = get_mu_dist(y, w)\n\n    if max_size is None:\n        max_size = len(y)\n\n    mu, dist = mu_dist.mu, mu_dist.dist\n\n    if min_size >= max_pos - min_pos:\n        return [len(y)], [mu(0,len(y)-1)], [dist(0,len(y)-1)]\n\n    # Perform the Bellman recursion for the optimal partition.\n    # Routine \"Find best partition\" in [1]\n    #\n    # Computes:\n    #\n    # p : list, length n\n    #     Set of intervals, represented as follows:\n    #     For interval (inclusive) right edge r in {0, ..., n-1},\n    #     the best (exclusive) left edge is at l=p[r].\n    #     Where intervals overlap, the rightmost one has priority.\n\n    if hasattr(mu_dist, 'find_best_partition'):\n        p = mu_dist.find_best_partition(gamma, min_size, max_size, min_pos, max_pos)\n    else:\n        i0 = min_pos\n        i1 = max_pos\n\n        B = [-gamma]*(i1 - i0 + 1)\n        p = [0]*(i1 - i0)\n        for r in range(i0, i1):\n            B[r+1-i0] = inf\n            a = max(r + 1 - max_size, i0)\n            b = max(r + 1 - min_size + 1, i0)\n            for l in range(a, b):\n                b = B[l-i0] + gamma + dist(l, r)\n                if b <= B[r+1-i0]:\n                    B[r+1-i0] = b\n                    p[r-i0] = l - 1\n\n            mu_dist.cleanup_cache()\n\n    # Routine \"Segmentation from partition\" in [1]\n    # Convert interval representation computed above\n    # to a list of intervals and values.\n    r = len(p) - 1 + min_pos\n    l = p[r - min_pos]\n    right = []\n    values = []\n    dists = []\n    while r >= min_pos:\n        right.append(r + 1)\n        values.append(mu((l + 1), r))\n        dists.append(dist((l + 1), r))\n        r = l\n        l = p[r - min_pos]\n    right.reverse()\n    values.reverse()\n    dists.reverse()\n\n    return right, values, dists", "response": "Solve a POTTS model on a set of time series y and a distribution of the potentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsolves Potts problem with automatically determined gamma.", "response": "def solve_potts_autogamma(y, w, beta=None, **kw):\n    \"\"\"Solve Potts problem with automatically determined gamma.\n\n    The optimal value is determined by minimizing the information measure::\n\n        f(gamma) = beta J(x(gamma)) + log sum(abs(x(gamma) - y)**p)\n\n    where x(gamma) is the solution to the Potts problem for a fixed\n    gamma. The minimization is only performed rather roughly.\n\n    Parameters\n    ----------\n    beta : float or 'bic'\n         Penalty parameter. Default is 4*ln(n)/n, similar to Bayesian\n         information criterion for gaussian model with unknown variance\n         assuming 4 DOF per breakpoint.\n\n    \"\"\"\n    n = len(y)\n\n    if n == 0:\n        return [], [], [], None\n\n    mu_dist = get_mu_dist(y, w)\n    mu, dist = mu_dist.mu, mu_dist.dist\n\n    if beta is None:\n        beta = 4 * math.log(n) / n\n\n    gamma_0 = dist(0, n-1)\n\n    if gamma_0 == 0:\n        # Zero variance\n        gamma_0 = 1.0\n\n    best_r = [None]\n    best_v = [None]\n    best_d = [None]\n    best_obj = [float('inf')]\n    best_gamma = [None]\n\n    def f(x):\n        gamma = gamma_0 * math.exp(x)\n        r, v, d = solve_potts_approx(y, w, gamma=gamma, mu_dist=mu_dist, **kw)\n\n        # MLE fit noise correlation\n        def sigma_star(rights, values, rho):\n            \"\"\"\n            |E_0| + sum_{j>0} |E_j - rho E_{j-1}|\n            \"\"\"\n            l = 1\n            E_prev = y[0] - values[0]\n            s = abs(E_prev)\n            for r, v in zip(rights, values):\n                for yv in y[l:r]:\n                    E = yv - v\n                    s += abs(E - rho*E_prev)\n                    E_prev = E\n                l = r\n            return s\n\n        rho_best = golden_search(lambda rho: sigma_star(r, v, rho), -1, 1,\n                                 xatol=0.05, expand_bounds=True)\n\n        # Measurement noise floor\n        if len(v) > 2:\n            absdiff = [abs(v[j+1] - v[j]) for j in range(len(v) - 1)]\n            sigma_0 = 0.1 * min(absdiff)\n        else:\n            absv = [abs(z) for z in v]\n            sigma_0 = 0.001 * min(absv)\n        sigma_0 = max(1e-300, sigma_0)\n\n        # Objective function\n        s = sigma_star(r, v, rho_best)\n        obj = beta*len(r) + math.log(sigma_0 + s)\n\n        # Done\n        if obj < best_obj[0]:\n            best_r[0] = r\n            best_v[0] = v\n            best_d[0] = d\n            best_gamma[0] = gamma\n            best_obj[0] = obj\n        return obj\n\n    # Try to find best gamma (golden section search on log-scale); we\n    # don't need an accurate value for it however\n    a = math.log(0.1/n)\n    b = 0.0\n    golden_search(f, a, b, xatol=abs(a)*0.1, ftol=0, expand_bounds=True)\n    return best_r[0], best_v[0], best_d[0], best_gamma[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef solve_potts_approx(y, w, gamma=None, min_size=1, **kw):\n    n = len(y)\n\n    if n == 0:\n        return [], [], []\n\n    mu_dist = kw.get('mu_dist')\n    if mu_dist is None:\n        mu_dist = get_mu_dist(y, w)\n        kw['mu_dist'] = mu_dist\n\n    if gamma is None:\n        mu, dist = mu_dist.mu, mu_dist.dist\n        gamma = 3 * dist(0,n-1) * math.log(n) / n\n\n    if min_size < 10:\n        max_size = 20\n    else:\n        max_size = min_size + 50\n\n    right, values, dists = solve_potts(y, w, gamma, min_size=min_size, max_size=max_size, **kw)\n    return merge_pieces(gamma, right, values, dists, mu_dist, max_size=max_size)", "response": "Fit penalized stepwise constant function to data\n    approximatively in linear time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_pieces(gamma, right, values, dists, mu_dist, max_size):\n    mu, dist = mu_dist.mu, mu_dist.dist\n\n    right = list(right)\n\n    # Combine consecutive intervals, if it results to decrease of cost\n    # function\n    while True:\n        min_change = 0\n        min_change_j = len(right)\n\n        l = 0\n        for j in range(1, len(right)):\n            if min_change_j < j - 2:\n                break\n\n            # Check whether merging consecutive intervals results to\n            # decrease in the cost function\n            change = dist(l, right[j]-1) - (dist(l, right[j-1]-1) + dist(right[j-1], right[j]-1) + gamma)\n            if change <= min_change:\n                min_change = change\n                min_change_j = j-1\n            l = right[j-1]\n\n        if min_change_j < len(right):\n            del right[min_change_j]\n        else:\n            break\n\n    # Check whether perturbing boundary positions leads to improvement\n    # in the cost function. The restricted Potts minimization can\n    # return sub-optimal boundaries due to the interval maximum size\n    # restriction.\n    l = 0\n    for j in range(1, len(right)):\n        prev_score = dist(l, right[j-1]-1) + dist(right[j-1], right[j]-1)\n        new_off = 0\n        for off in range(-max_size, max_size+1):\n            if right[j-1] + off - 1 <= l or right[j-1] + off >= right[j] - 1 or off == 0:\n                continue\n            new_score = dist(l, right[j-1]+off-1) + dist(right[j-1]+off, right[j]-1)\n            if new_score < prev_score:\n                new_off = off\n                prev_score = new_score\n\n        if new_off != 0:\n            right[j-1] += new_off\n\n        l = right[j-1]\n\n    # Rebuild values and dists lists\n    l = 0\n    values = []\n    dists = []\n    for j in range(len(right)):\n        dists.append(dist(l, right[j]-1))\n        values.append(mu(l, right[j]-1))\n        l = right[j]\n\n    return right, values, dists", "response": "Merge pieces of the Potts model solution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef median(items):\n    items.sort()\n    k = len(items)//2\n    if len(items) % 2 == 0:\n        return (items[k] + items[k - 1]) / 2\n    else:\n        return items[k]", "response": "Returns the median of the items in the list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rolling_median_dev(items):\n    min_heap = []\n    max_heap = []\n    min_heap_sum = 0   # equal to -sum(min_heap)\n    max_heap_sum = 0   # equal to sum(max_heap)\n    s = iter(items)\n    try:\n        while True:\n            # Odd\n            v = six.next(s)\n            min_heap_sum += v\n            v = -heapq.heappushpop(min_heap, -v)\n            min_heap_sum -= v\n            heapq.heappush(max_heap, v)\n            max_heap_sum += v\n            # Ensure d >= 0 despite rounding error\n            d = max(0, max_heap_sum - min_heap_sum - max_heap[0])\n            yield (max_heap[0], d)\n\n            # Even\n            v = six.next(s)\n            max_heap_sum += v\n            v = heapq.heappushpop(max_heap, v)\n            max_heap_sum -= v\n            heapq.heappush(min_heap, -v)\n            min_heap_sum += v\n            d = max(0, max_heap_sum - min_heap_sum)\n            yield ((max_heap[0] - min_heap[0])/2, d)\n    except StopIteration:\n        return", "response": "Compute the median of the items in a rolling fashion."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing weighted median of y with weights w.", "response": "def weighted_median(y, w):\n    \"\"\"\n    Compute weighted median of `y` with weights `w`.\n    \"\"\"\n    items = sorted(zip(y, w))\n    midpoint = sum(w) / 2\n\n    yvals = []\n    wsum = 0\n\n    for yy, ww in items:\n        wsum += ww\n        if wsum > midpoint:\n            yvals.append(yy)\n            break\n        elif wsum == midpoint:\n            yvals.append(yy)\n    else:\n        yvals = y\n\n    return sum(yvals) / len(yvals)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds minimum of a function on interval [a, b] using golden section search. If expand_bounds=True, expand the interval so that the function is first evaluated at x=a and x=b.", "response": "def golden_search(f, a, b, xatol=1e-6, ftol=1e-8, expand_bounds=False):\n    \"\"\"\n    Find minimum of a function on interval [a, b]\n    using golden section search.\n\n    If expand_bounds=True, expand the interval so that the function is\n    first evaluated at x=a and x=b.\n    \"\"\"\n\n    ratio = 2 / (1 + math.sqrt(5))\n\n    if not expand_bounds:\n        x0 = a\n        x3 = b\n    else:\n        x0 = (ratio * a - (1 - ratio) * b) / (2*ratio - 1)\n        x3 = (ratio * b - (1 - ratio) * a) / (2*ratio - 1)\n\n    x1 = ratio * x0 + (1 - ratio) * x3\n    x2 = (1 - ratio) * x0 + ratio * x3\n\n    f1 = f(x1)\n    f2 = f(x2)\n\n    f0 = max(abs(f1), abs(f2))\n\n    while True:\n        if abs(x0 - x3) < xatol or abs(f1 - f2) < ftol*f0:\n            break\n\n        if f2 < f1:\n            x0 = x1\n            x1 = x2\n            x2 = ratio * x1 + (1 - ratio) * x3\n            f1 = f2\n            f2 = f(x2)\n        else:\n            x3 = x2\n            x2 = x1\n            x1 = ratio * x2 + (1 - ratio) * x0\n            f2 = f1\n            f1 = f(x1)\n\n    if f2 < f1:\n        return x2\n    else:\n        return x1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_python(python):\n        is_pypy = python.startswith(\"pypy\")\n\n        # Parse python specifier\n        if is_pypy:\n            executable = python\n            if python == 'pypy':\n                python_version = '2'\n            else:\n                python_version = python[4:]\n        else:\n            python_version = python\n            executable = \"python{0}\".format(python_version)\n\n        # Find Python executable on path\n        try:\n            return util.which(executable)\n        except IOError:\n            pass\n\n        # Maybe the current one is correct?\n        current_is_pypy = hasattr(sys, 'pypy_version_info')\n        current_versions = ['{0[0]}'.format(sys.version_info),\n                            '{0[0]}.{0[1]}'.format(sys.version_info)]\n\n        if is_pypy == current_is_pypy and python_version in current_versions:\n            return sys.executable\n\n        return None", "response": "Find the Python executable for the given Python version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a name to uniquely identify this environment.", "response": "def name(self):\n        \"\"\"\n        Get a name to uniquely identify this environment.\n        \"\"\"\n        python = self._python\n        if self._python.startswith('pypy'):\n            # get_env_name adds py-prefix\n            python = python[2:]\n        return environment.get_env_name(self.tool_name, python, self._requirements)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup(self):\n        log.info(\"Creating virtualenv for {0}\".format(self.name))\n        util.check_call([\n            sys.executable,\n            \"-mvirtualenv\",\n            '--no-site-packages',\n            \"-p\",\n            self._executable,\n            self._path])\n\n        log.info(\"Installing requirements for {0}\".format(self.name))\n        self._install_requirements()", "response": "Setup the environment on disk using pip install."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates size of the object in bytes.", "response": "def _calcsize(fmt):\n    '''struct.calcsize() handling 'z' for Py_ssize_t.\n    '''\n     # sizeof(long) != sizeof(ssize_t) on LLP64\n    if _sizeof_Clong < _sizeof_Cvoidp: # pragma: no coverage\n        z = 'P'\n    else:\n        z = 'L'\n    return calcsize(fmt.replace('z', z))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget non - zero basicsize of type and including the header sizes.", "response": "def _basicsize(t, base=0, heap=False, obj=None):\n    '''Get non-zero basicsize of type,\n       including the header sizes.\n    '''\n    s = max(getattr(t, '__basicsize__', 0), base)\n     # include gc header size\n    if t != _Type_type:\n       h = getattr(t,   '__flags__', 0) & _Py_TPFLAGS_HAVE_GC\n    elif heap:  # type, allocated on heap\n       h = True\n    else:  # None has no __flags__ attr\n       h = getattr(obj, '__flags__', 0) & _Py_TPFLAGS_HEAPTYPE\n    if h:\n       s += _sizeof_CPyGC_Head\n     # include reference counters\n    return s + _sizeof_Crefcounts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _derive_typedef(typ):\n    '''Return single, existing super type typedef or None.\n    '''\n    v = [v for v in _values(_typedefs) if _issubclass(typ, v.type)]\n    if len(v) == 1:\n        return v[0]\n    return None", "response": "Return single existing super type typedef or None."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an attribute name object 2 - tuple for certain attributes or for the __slots__ attributes of the given object but not both.", "response": "def _dir2(obj, pref='', excl=(), slots=None, itor=''):\n    '''Return an attribute name, object 2-tuple for certain\n       attributes or for the ``__slots__`` attributes of the\n       given object, but not both.  Any iterator referent\n       objects are returned with the given name if the\n       latter is non-empty.\n    '''\n    if slots:  # __slots__ attrs\n        if hasattr(obj, slots):\n             # collect all inherited __slots__ attrs\n             # from list, tuple, or dict __slots__,\n             # while removing any duplicate attrs\n            s = {}\n            for c in type(obj).mro():\n                for a in getattr(c, slots, ()):\n                    if hasattr(obj, a):\n                        s.setdefault(a, getattr(obj, a))\n             # assume __slots__ tuple/list\n             # is holding the attr values\n            yield slots, _Slots(s)  # _keys(s)\n            for t in _items(s):\n                yield t  # attr name, value\n    elif itor:  # iterator referents\n        for o in obj:  # iter(obj)\n            yield itor, o\n    else:  # regular attrs\n        for a in dir(obj):\n            if a.startswith(pref) and a not in excl and hasattr(obj, a):\n               yield a, getattr(obj, a)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _infer_dict(obj):\n    '''Return True for likely dict object.\n    '''\n    for ats in (('__len__', 'get', 'has_key',     'items',     'keys',     'values'),\n                ('__len__', 'get', 'has_key', 'iteritems', 'iterkeys', 'itervalues')):\n        for a in ats:  # no all(<generator_expression>) in Python 2.2\n            if not _callable(getattr(obj, a, None)):\n                break\n        else:  # all True\n            return True\n    return False", "response": "Return True for likely dict object.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _isdictclass(obj):\n    '''Return True for known dict objects.\n    '''\n    c = getattr(obj, '__class__', None)\n    return c and c.__name__ in _dict_classes.get(c.__module__, ())", "response": "Return True if obj is a dict class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lengstr(obj):\n    '''Object length as a string.\n    '''\n    n = leng(obj)\n    if n is None:  # no len\n       r = ''\n    elif n > _len(obj):  # extended\n       r = ' leng %d!' % n\n    else:\n       r = ' leng %d' % n\n    return r", "response": "Object length as a string. nagenzezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezezeze"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _objs_opts(objs, all=None, **opts):\n    '''Return given or 'all' objects\n       and the remaining options.\n    '''\n    if objs:  # given objects\n        t = objs\n    elif all in (False, None):\n        t = ()\n    elif all is True:  # 'all' objects ...\n         # ... modules first, globals and stack\n         # (may contain duplicate objects)\n        t = tuple(_values(sys.modules)) + (\n            globals(), stack(sys.getrecursionlimit())[2:])\n    else:\n        raise ValueError('invalid option: %s=%r' % ('all', all))\n    return t, opts", "response": "Return given or all objects\n       and the remaining options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning percentage as string.", "response": "def _p100(part, total, prec=1):\n    '''Return percentage as string.\n    '''\n    r = float(total)\n    if r:\n        r = part * 100.0 / r\n        return '%.*f%%' % (prec, r)\n    return 'n/a'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn specific attribute objects of an object.", "response": "def _refs(obj, named, *ats, **kwds):\n    '''Return specific attribute objects of an object.\n    '''\n    if named:\n        for a in ats:  # cf. inspect.getmembers()\n            if hasattr(obj, a):\n                yield _NamedRef(a, getattr(obj, a))\n        if kwds:  # kwds are _dir2() args\n            for a, o in _dir2(obj, **kwds):\n                yield _NamedRef(a, o)\n    else:\n        for a in ats:  # cf. inspect.getmembers()\n            if hasattr(obj, a):\n                yield getattr(obj, a)\n        if kwds:  # kwds are _dir2() args\n            for _, o in _dir2(obj, **kwds):\n                yield o"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclipping long repr() string.", "response": "def _repr(obj, clip=80):\n    '''Clip long repr() string.\n    '''\n    try:  # safe repr()\n        r = repr(obj)\n    except TypeError:\n        r = 'N/A'\n    if 0 < clip < len(r):\n        h = (clip // 2) - 2\n        if h > 0:\n            r = r[:h] + '....' + r[-h:]\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _SI(size, K=1024, i='i'):\n    '''Return size as SI string.\n    '''\n    if 1 < K < size:\n        f = float(size)\n        for si in iter('KMGPTE'):\n            f /= K\n            if f < K:\n                return ' or %.1f %s%sB' % (f, si, i)\n    return ''", "response": "Return size as SI string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dict_refs(obj, named):\n    '''Return key and value objects of a dict/proxy.\n    '''\n    if named:\n        for k, v in _items(obj):\n            s = str(k)\n            yield _NamedRef('[K] ' + s, k)\n            yield _NamedRef('[V] ' + s + ': ' + _repr(v), v)\n    else:\n        for k, v in _items(obj):\n            yield k\n            yield v", "response": "Return key and value objects of a dict / proxy.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _iter_refs(obj, named):\n    '''Return the referent(s) of an iterator object.\n    '''\n    r = _getreferents(obj)  # special case\n    return _refs(r, named, itor=_nameof(obj) or 'iteref')", "response": "Return the referents of an iterator object.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn specific referents of a module object.", "response": "def _module_refs(obj, named):\n    '''Return specific referents of a module object.\n    '''\n     # ignore this very module\n    if obj.__name__ == __name__:\n        return ()\n     # module is essentially a dict\n    return _dict_refs(obj.__dict__, named)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _len_code(obj):  # see .../Lib/test/test_sys.py\n    '''Length of code object (stack and variables only).\n    '''\n    return obj.co_stacksize +      obj.co_nlocals  \\\n                            + _len(obj.co_freevars) \\\n                            + _len(obj.co_cellvars) - 1", "response": "Length of code object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _len_dict(obj):\n    '''Dict length in items (estimate).\n    '''\n    n = len(obj)  # active items\n    if n < 6:  # ma_smalltable ...\n       n = 0  # ... in basicsize\n    else:  # at least one unused\n       n = _power2(n + 1)\n    return n", "response": "Dict length in items ( estimate.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _len_int(obj):\n    '''Length of multi-precision int (aka long) in digits.\n    '''\n    if obj:\n        n, i = 1, abs(obj)\n        if i > _digitmax:\n             # no log(x[, base]) in Python 2.2\n            n += int(log(i) * _digitlog)\n    else:  # zero\n        n = 0\n    return n", "response": "Length of multi - precision int in digits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _len_iter(obj):\n    '''Length (hint) of an iterator.\n    '''\n    n = getattr(obj, '__length_hint__', None)\n    if n:\n       n = n()\n    else:  # try len()\n       n = _len(obj)\n    return n", "response": "Length of an iterator.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _len_set(obj):\n    '''Length of frozen/set (estimate).\n    '''\n    n = len(obj)\n    if n > 8:  # assume half filled\n       n = _power2(n + n - 2)\n    elif n:  # at least 8\n       n = 8\n    return n", "response": "Length of frozen / set ( estimate.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping an old - or new - style class object.", "response": "def _claskey(obj, style):\n    '''Wrap an old- or new-style class object.\n    '''\n    i =  id(obj)\n    k = _claskeys.get(i, None)\n    if not k:\n        _claskeys[i] = k = _Claskey(obj, style)\n    return k"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _typedef_both(t, base=0, item=0, leng=None, refs=None, kind=_kind_static, heap=False):\n    '''Add new typedef for both data and code.\n    '''\n    v = _Typedef(base=_basicsize(t, base=base), item=_itemsize(t, item),\n                 refs=refs, leng=leng,\n                 both=True, kind=kind, type=t)\n    v.save(t, base=base, heap=heap)\n    return v", "response": "Add new typedef for both data and code.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _typedef_code(t, base=0, refs=None, kind=_kind_static, heap=False):\n    '''Add new typedef for code only.\n    '''\n    v = _Typedef(base=_basicsize(t, base=base),\n                 refs=refs,\n                 both=False, kind=kind, type=t)\n    v.save(t, base=base, heap=heap)\n    return v", "response": "Create a new typedef for code only."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new typedef for an object.", "response": "def _typedef(obj, derive=False, infer=False):\n    '''Create a new typedef for an object.\n    '''\n    t =  type(obj)\n    v = _Typedef(base=_basicsize(t, obj=obj),\n                 kind=_kind_dynamic, type=t)\n  ##_printf('new %r %r/%r %s', t, _basicsize(t), _itemsize(t), _repr(dir(obj)))\n    if ismodule(obj):  # handle module like dict\n        v.dup(item=_dict_typedef.item + _sizeof_CPyModuleObject,\n              leng=_len_module,\n              refs=_module_refs)\n    elif isframe(obj):\n        v.set(base=_basicsize(t, base=_sizeof_CPyFrameObject, obj=obj),\n              item=_itemsize(t),\n              leng=_len_frame,\n              refs=_frame_refs)\n    elif iscode(obj):\n        v.set(base=_basicsize(t, base=_sizeof_CPyCodeObject, obj=obj),\n              item=_sizeof_Cvoidp,\n              leng=_len_code,\n              refs=_co_refs,\n              both=False)  # code only\n    elif _callable(obj):\n        if isclass(obj):  # class or type\n            v.set(refs=_class_refs,\n                  both=False)  # code only\n            if obj.__module__ in _builtin_modules:\n                v.set(kind=_kind_ignored)\n        elif isbuiltin(obj):  # function or method\n            v.set(both=False,  # code only\n                  kind=_kind_ignored)\n        elif isfunction(obj):\n            v.set(refs=_func_refs,\n                  both=False)  # code only\n        elif ismethod(obj):\n            v.set(refs=_im_refs,\n                  both=False)  # code only\n        elif isclass(t):  # callable instance, e.g. SCons,\n             # handle like any other instance further below\n            v.set(item=_itemsize(t), safe_len=True,\n                  refs=_inst_refs)  # not code only!\n        else:\n            v.set(both=False)  # code only\n    elif _issubclass(t, dict):\n        v.dup(kind=_kind_derived)\n    elif _isdictclass(obj) or (infer and _infer_dict(obj)):\n        v.dup(kind=_kind_inferred)\n    elif getattr(obj, '__module__', None) in _builtin_modules:\n        v.set(kind=_kind_ignored)\n    else:  # assume an instance of some class\n        if derive:\n            p = _derive_typedef(t)\n            if p:  # duplicate parent\n                v.dup(other=p, kind=_kind_derived)\n                return v\n        if _issubclass(t, Exception):\n            v.set(item=_itemsize(t), safe_len=True,\n                  refs=_exc_refs,\n                  kind=_kind_derived)\n        elif isinstance(obj, Exception):\n            v.set(item=_itemsize(t), safe_len=True,\n                  refs=_exc_refs)\n        else:\n            v.set(item=_itemsize(t), safe_len=True,\n                  refs=_inst_refs)\n    return v"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall one or more classes to be handled as dict.", "response": "def adict(*classes):\n    '''Install one or more classes to be handled as dict.\n    '''\n    a = True\n    for c in classes:\n         # if class is dict-like, add class\n         # name to _dict_classes[module]\n        if isclass(c) and _infer_dict(c):\n            t = _dict_classes.get(c.__module__, ())\n            if c.__name__ not in t:  # extend tuple\n                _dict_classes[c.__module__] = t + (c.__name__,)\n        else:  # not a dict-like class\n            a = False\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple containing an Asized instance for each object passed as positional argment.", "response": "def asized(*objs, **opts):\n    '''Return a tuple containing an **Asized** instance for each\n       object passed as positional argment using the following\n       options.\n\n           *align=8*       -- size alignment\n\n           *clip=80*       -- clip repr() strings\n\n           *code=False*    -- incl. (byte)code size\n\n           *derive=False*  -- derive from super type\n\n           *detail=0*      -- Asized refs level\n\n           *ignored=True*  -- ignore certain types\n\n           *infer=False*   -- try to infer types\n\n           *limit=100*     -- recursion limit\n\n           *stats=0.0*     -- print statistics\n\n       If only one object is given, the return value is the **Asized**\n       instance for that object.  Otherwise, the length of the returned\n       tuple matches the number of given objects.\n\n       Set *detail* to the desired referents level (recursion depth).\n\n       See function **asizeof** for descriptions of the other options.\n\n    '''\n    if 'all' in opts:\n        raise KeyError('invalid option: %s=%r' % ('all', opts['all']))\n    if objs:\n        _asizer.reset(**opts)\n        t = _asizer.asized(*objs)\n        _asizer.print_stats(objs, opts=opts, sized=t)  # show opts as _kwdstr\n        _asizer._clear()\n    else:\n        t = ()\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the combined size in bytes of all objects passed as positional argments.", "response": "def asizeof(*objs, **opts):\n    '''Return the combined size in bytes of all objects passed as positional argments.\n\n    The available options and defaults are the following.\n\n         *align=8*       -- size alignment\n\n         *all=False*     -- all current objects\n\n         *clip=80*       -- clip ``repr()`` strings\n\n         *code=False*    -- incl. (byte)code size\n\n         *derive=False*  -- derive from super type\n\n         *ignored=True*  -- ignore certain types\n\n         *infer=False*   -- try to infer types\n\n         *limit=100*     -- recursion limit\n\n         *stats=0.0*     -- print statistics\n\n    Set *align* to a power of 2 to align sizes.  Any value less\n    than 2 avoids size alignment.\n\n    All current module, global and stack objects are sized if\n    *all* is True and if no positional arguments are supplied.\n\n    A positive *clip* value truncates all repr() strings to at\n    most *clip* characters.\n\n    The (byte)code size of callable objects like functions,\n    methods, classes, etc. is included only if *code* is True.\n\n    If *derive* is True, new types are handled like an existing\n    (super) type provided there is one and only of those.\n\n    By default certain base types like object, super, etc. are\n    ignored.  Set *ignored* to False to include those.\n\n    If *infer* is True, new types are inferred from attributes\n    (only implemented for dict types on callable attributes\n    as get, has_key, items, keys and values).\n\n    Set *limit* to a positive value to accumulate the sizes of\n    the referents of each object, recursively up to the limit.\n    Using *limit=0* returns the sum of the flat[4] sizes of\n    the given objects.  High *limit* values may cause runtime\n    errors and miss objects for sizing.\n\n    A positive value for *stats* prints up to 8 statistics, (1)\n    a summary of the number of objects sized and seen, (2) a\n    simple profile of the sized objects by type and (3+) up to\n    6 tables showing the static, dynamic, derived, ignored,\n    inferred and dict types used, found resp. installed.  The\n    fractional part of the *stats* value (x100) is the cutoff\n    percentage for simple profiles.\n\n    [4] See the documentation of this module for the definition of flat size.\n    '''\n    t, p = _objs_opts(objs, **opts)\n    if t:\n        _asizer.reset(**p)\n        s = _asizer.asizeof(*t)\n        _asizer.print_stats(objs=t, opts=opts)  # show opts as _kwdstr\n        _asizer._clear()\n    else:\n        s = 0\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef asizesof(*objs, **opts):\n    '''Return a tuple containing the size in bytes of all objects\n       passed as positional argments using the following options.\n\n           *align=8*       -- size alignment\n\n           *clip=80*       -- clip ``repr()`` strings\n\n           *code=False*    -- incl. (byte)code size\n\n           *derive=False*  -- derive from super type\n\n           *ignored=True*  -- ignore certain types\n\n           *infer=False*   -- try to infer types\n\n           *limit=100*     -- recursion limit\n\n           *stats=0.0*     -- print statistics\n\n       See function **asizeof** for a description of the options.\n\n       The length of the returned tuple equals the number of given\n       objects.\n    '''\n    if 'all' in opts:\n        raise KeyError('invalid option: %s=%r' % ('all', opts['all']))\n    if objs:  # size given objects\n        _asizer.reset(**opts)\n        t = _asizer.asizesof(*objs)\n        _asizer.print_stats(objs, opts=opts, sizes=t)  # show opts as _kwdstr\n        _asizer._clear()\n    else:\n        t = ()\n    return t", "response": "Return a tuple containing the size in bytes of all objects in the object tree."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _typedefof(obj, save=False, **opts):\n    '''Get the typedef for an object.\n    '''\n    k = _objkey(obj)\n    v = _typedefs.get(k, None)\n    if not v:  # new typedef\n        v = _typedef(obj, **opts)\n        if save:\n            _typedefs[k] = v\n    return v", "response": "Get the typedef for an object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef basicsize(obj, **opts):\n    '''Return the basic size of an object (in bytes).\n\n       Valid options and defaults are\n\n           *derive=False*  -- derive type from super type\n\n           *infer=False*   -- try to infer types\n\n           *save=False*    -- save typedef if new\n    '''\n    v = _typedefof(obj, **opts)\n    if v:\n        v = v.base\n    return v", "response": "Return the basic size of an object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flatsize(obj, align=0, **opts):\n    '''Return the flat size of an object (in bytes),\n       optionally aligned to a given power of 2.\n\n       See function **basicsize** for a description of\n       the other options.  See the documentation of\n       this module for the definition of flat size.\n    '''\n    v = _typedefof(obj, **opts)\n    if v:\n        if align > 1:\n            m = align - 1\n            if (align & m) != 0:\n                raise ValueError('invalid option: %s=%r' % ('align', align))\n        else:\n            m = 0\n        v = v.flat(obj, m)\n    return v", "response": "Return the flat size of an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef itemsize(obj, **opts):\n    '''Return the item size of an object (in bytes).\n\n       See function **basicsize** for a description of\n       the options.\n    '''\n    v = _typedefof(obj, **opts)\n    if v:\n        v = v.item\n    return v", "response": "Return the item size of an object in bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the length of an object in items. facility", "response": "def leng(obj, **opts):\n    '''Return the length of an object (in items).\n\n       See function **basicsize** for a description of\n       the options.\n    '''\n    v = _typedefof(obj, **opts)\n    if v:\n        v = v.leng\n        if v and _callable(v):\n            v = v(obj)\n    return v"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refs(obj, **opts):\n    '''Return (a generator for) specific referents of an\n       object.\n\n       See function **basicsize** for a description of\n       the options.\n    '''\n    v = _typedefof(obj, **opts)\n    if v:\n        v = v.refs\n        if v and _callable(v):\n            v = v(obj, False)\n    return v", "response": "Return a generator for ) specific referents of an anonon objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all named referents of obj. Reuse functionality from asizeof. Does not return referents without a name.", "response": "def named_refs(obj):\n    \"\"\"\n    Return all named referents of `obj`. Reuse functionality from asizeof.\n    Does not return referents without a name, e.g. objects in a list.\n    \"\"\"\n    refs = []\n    v = _typedefof(obj)\n    if v:\n        v = v.refs\n        if v and _callable(v):\n            for ref in v(obj, True):\n                try:\n                    refs.append((ref.name, ref.ref))\n                except AttributeError:\n                    pass\n    return refs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef args(self):  # as args tuple\n        '''Return all attributes as arguments tuple.\n        '''\n        return (self.base, self.item, self.leng, self.refs,\n                self.both, self.kind, self.type)", "response": "Return all attributes as arguments tuple.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dup(self, other=None, **kwds):\n        '''Duplicate attributes of dict or other typedef.\n        '''\n        if other is None:\n            d = _dict_typedef.kwds()\n        else:\n            d =  other.kwds()\n        d.update(kwds)\n        self.reset(**d)", "response": "Duplicate attributes of dict or other typedef.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flat(self, obj, mask=0):\n        '''Return the aligned flat size.\n        '''\n        s = self.base\n        if self.leng and self.item > 0:  # include items\n            s += self.leng(obj) * self.item\n        if _getsizeof:  # _getsizeof prevails\n            s = _getsizeof(obj, s)\n        if mask:  # align\n            s = (s + mask) & ~mask\n        return s", "response": "Return the aligned flat size."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all attributes as keywords dict.", "response": "def kwds(self):\n        '''Return all attributes as keywords dict.\n        '''\n         # no dict(refs=self.refs, ..., kind=self.kind) in Python 2.0\n        return _kwds(base=self.base, item=self.item,\n                     leng=self.leng, refs=self.refs,\n                     both=self.both, kind=self.kind, type=self.type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves this typedef plus its class typedef.", "response": "def save(self, t, base=0, heap=False):\n        '''Save this typedef plus its class typedef.\n        '''\n        c, k = _keytuple(t)\n        if k and k not in _typedefs:  # instance key\n            _typedefs[k] = self\n            if c and c not in _typedefs:  # class key\n                if t.__module__ in _builtin_modules:\n                    k = _kind_ignored  # default\n                else:\n                    k = self.kind\n                _typedefs[c] = _Typedef(base=_basicsize(type(t), base=base, heap=heap),\n                                        refs=_type_refs,\n                                        both=False, kind=k, type=t)\n        elif isbuiltin(t) and t not in _typedefs:  # array, range, xrange in Python 2.x\n            _typedefs[t] = _Typedef(base=_basicsize(t, base=base),\n                                    both=False, kind=_kind_ignored, type=t)\n        else:\n            raise KeyError('asizeof typedef %r bad: %r %r' % (self, (c, k), self.both))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, safe_len=False, **kwds):\n        '''Set one or more attributes.\n        '''\n        if kwds:  # double check\n            d = self.kwds()\n            d.update(kwds)\n            self.reset(**d)\n        if safe_len and self.item:\n            self.leng = _len", "response": "Set one or more attributes.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset all specified attributes.", "response": "def reset(self, base=0, item=0, leng=None, refs=None,\n                                    both=True, kind=None, type=None):\n        '''Reset all specified attributes.\n        '''\n        if base < 0:\n            raise ValueError('invalid option: %s=%r' % ('base', base))\n        else:\n            self.base = base\n        if item < 0:\n            raise ValueError('invalid option: %s=%r' % ('item', item))\n        else:\n            self.item = item\n        if leng in _all_lengs:  # XXX or _callable(leng)\n            self.leng = leng\n        else:\n            raise ValueError('invalid option: %s=%r' % ('leng', leng))\n        if refs in _all_refs:  # XXX or _callable(refs)\n            self.refs = refs\n        else:\n            raise ValueError('invalid option: %s=%r' % ('refs', refs))\n        if both in (False, True):\n            self.both = both\n        else:\n            raise ValueError('invalid option: %s=%r' % ('both', both))\n        if kind in _all_kinds:\n            self.kind = kind\n        else:\n            raise ValueError('invalid option: %s=%r' % ('kind', kind))\n        self.type = type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, obj, size):\n        '''Update this profile.\n        '''\n        self.number += 1\n        self.total  += size\n        if self.high < size:  # largest\n           self.high = size\n           try:  # prefer using weak ref\n               self.objref, self.weak = Weakref.ref(obj), True\n           except TypeError:\n               self.objref, self.weak = obj, False", "response": "Update this profile.\n        object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting to configured stream if any is specified and the file argument is not already set.", "response": "def _printf(self, *args, **kwargs):\n        '''Print to configured stream if any is specified and the file argument\n        is not already set for this specific call.\n        '''\n        if self._stream and not kwargs.get('file'):\n            kwargs['file'] = self._stream\n        _printf(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sizer(self, obj, deep, sized):\n        '''Size an object, recursively.\n        '''\n        s, f, i = 0, 0, id(obj)\n         # skip obj if seen before\n         # or if ref of a given obj\n        if i in self._seen:\n            if deep:\n                self._seen[i] += 1\n                if sized:\n                    s = sized(s, f, name=self._nameof(obj))\n                return s\n        else:\n            self._seen[i] = 0\n        try:\n            k, rs = _objkey(obj), []\n            if k in self._excl_d:\n                self._excl_d[k] += 1\n            else:\n                v = _typedefs.get(k, None)\n                if not v:  # new typedef\n                    _typedefs[k] = v = _typedef(obj, derive=self._derive_,\n                                                     infer=self._infer_)\n                if (v.both or self._code_) and v.kind is not self._ign_d:\n                    s = f = v.flat(obj, self._mask)  # flat size\n                    if self._profile:  # profile type\n                        self._prof(k).update(obj, s)\n                     # recurse, but not for nested modules\n                    if v.refs and deep < self._limit_ and not (deep and ismodule(obj)):\n                         # add sizes of referents\n                        r, z, d = v.refs, self._sizer, deep + 1\n                        if sized and deep < self._detail_:\n                             # use named referents\n                            for o in r(obj, True):\n                                if isinstance(o, _NamedRef):\n                                    t = z(o.ref, d, sized)\n                                    t.name = o.name\n                                else:\n                                    t = z(o, d, sized)\n                                    t.name = self._nameof(o)\n                                rs.append(t)\n                                s += t.size\n                        else:  # no sum(<generator_expression>) in Python 2.2\n                            for o in r(obj, False):\n                                s += z(o, d, None)\n                         # recursion depth\n                        if self._depth < d:\n                           self._depth = d\n            self._seen[i] += 1\n        except RuntimeError:  # XXX RecursionLimitExceeded:\n            self._missed += 1\n        if sized:\n            s = sized(s, f, name=self._nameof(obj), refs=rs)\n        return s", "response": "Size an object recursively."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sizes(self, objs, sized=None):\n        '''Return the size or an **Asized** instance for each\n           given object and the total size.  The total\n           includes the size of duplicates only once.\n        '''\n        self.exclude_refs(*objs)  # skip refs to objs\n        s, t = {}, []\n        for o in objs:\n            i = id(o)\n            if i in s:  # duplicate\n                self._seen[i] += 1\n                self._duplicate += 1\n            else:\n                s[i] = self._sizer(o, 0, sized)\n            t.append(s[i])\n        if sized:\n            s = _sum([i.size for i in _values(s)])  # [] for Python 2.2\n        else:\n            s = _sum(_values(s))\n        self._total += s  # accumulate\n        return s, tuple(t)", "response": "Return the size or an Asized instance for each\n           given object and the total size."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsize each object and return an Asized instance with the given detail level.", "response": "def asized(self, *objs, **opts):\n        '''Size each object and return an **Asized** instance with\n           size information and referents up to the given detail\n           level (and with modified options, see method **set**).\n\n           If only one object is given, the return value is the\n           **Asized** instance for that object.\n        '''\n        if opts:\n            self.set(**opts)\n        _, t = self._sizes(objs, Asized)\n        if len(t) == 1:\n            t = t[0]\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the combined size of the given objects .", "response": "def asizeof(self, *objs, **opts):\n        '''Return the combined size of the given objects\n           (with modified options, see method **set**).\n        '''\n        if opts:\n            self.set(**opts)\n        s, _ = self._sizes(objs, None)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef asizesof(self, *objs, **opts):\n        '''Return the individual sizes of the given objects\n           (with modified options, see method  **set**).\n        '''\n        if opts:\n            self.set(**opts)\n        _, t = self._sizes(objs, None)\n        return t", "response": "Return the individual sizes of the given objects\n          ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exclude_refs(self, *objs):\n        '''Exclude any references to the specified objects from sizing.\n\n           While any references to the given objects are excluded, the\n           objects will be sized if specified as positional arguments\n           in subsequent calls to methods **asizeof** and **asizesof**.\n        '''\n        for o in objs:\n            self._seen.setdefault(id(o), 0)", "response": "Exclude any references to the specified objects from sizing."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexcludes the specified object instances and types from sizing.", "response": "def exclude_types(self, *objs):\n        '''Exclude the specified object instances and types from sizing.\n\n           All instances and types of the given objects are excluded,\n           even objects specified as positional arguments in subsequent\n           calls to methods **asizeof** and **asizesof**.\n        '''\n        for o in objs:\n            for t in _keytuple(o):\n                if t and t not in self._excl_d:\n                    self._excl_d[t] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_profiles(self, w=0, cutoff=0, **print3opts):\n        '''Print the profiles above *cutoff* percentage.\n\n               *w=0*           -- indentation for each line\n\n               *cutoff=0*      -- minimum percentage printed\n\n               *print3options* -- print options, ala Python 3.0\n        '''\n         # get the profiles with non-zero size or count\n        t = [(v, k) for k, v in _items(self._profs) if v.total > 0 or v.number > 1]\n        if (len(self._profs) - len(t)) < 9:  # just show all\n            t = [(v, k) for k, v in _items(self._profs)]\n        if t:\n            s = ''\n            if self._total:\n                s = ' (% of grand total)'\n                c = max(cutoff, self._cutoff)\n                c = int(c * 0.01 * self._total)\n            else:\n                c = 0\n            self._printf('%s%*d profile%s:  total%s, average, and largest flat size%s:  largest object',\n                         linesep, w, len(t), _plural(len(t)), s, self._incl, **print3opts)\n            r = len(t)\n            for v, k in _sorted(t, reverse=True):\n                s = 'object%(plural)s:  %(total)s, %(avg)s, %(high)s:  %(obj)s%(lengstr)s' % v.format(self._clip_, self._total)\n                self._printf('%*d %s %s', w, v.number, self._prepr(k), s, **print3opts)\n                r -= 1\n                if r > 1 and v.total < c:\n                    c = max(cutoff, self._cutoff)\n                    self._printf('%+*d profiles below cutoff (%.0f%%)', w, r, c)\n                    break\n            z = len(self._profs) - len(t)\n            if z > 0:\n                self._printf('%+*d %r object%s', w, z, 'zero', _plural(z), **print3opts)", "response": "Print the profiles above cutoff."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the statistics for the objects and opts.", "response": "def print_stats(self, objs=(), opts={}, sized=(), sizes=(), stats=3.0, **print3opts):\n        '''Print the statistics.\n\n               *w=0*           -- indentation for each line\n\n               *objs=()*       -- optional, list of objects\n\n               *opts={}*       -- optional, dict of options used\n\n               *sized=()*      -- optional, tuple of **Asized** instances returned\n\n               *sizes=()*      -- optional, tuple of sizes returned\n\n               *stats=0.0*     -- print stats, see function **asizeof**\n\n               *print3options* -- print options, as in Python 3.0\n        '''\n        s = min(opts.get('stats', stats) or 0, self._stats_)\n        if s > 0:  # print stats\n            t = self._total + self._missed + _sum(_values(self._seen))\n            w = len(str(t)) + 1\n            t = c = ''\n            o = _kwdstr(**opts)\n            if o and objs:\n                c = ', '\n             # print header line(s)\n            if sized and objs:\n                n = len(objs)\n                if n > 1:\n                    self._printf('%sasized(...%s%s) ...', linesep, c, o, **print3opts)\n                    for i in range(n):  # no enumerate in Python 2.2.3\n                        self._printf('%*d: %s', w-1, i, sized[i], **print3opts)\n                else:\n                    self._printf('%sasized(%s): %s', linesep, o, sized, **print3opts)\n            elif sizes and objs:\n                self._printf('%sasizesof(...%s%s) ...', linesep, c, o, **print3opts)\n                for z, o in zip(sizes, objs):\n                    self._printf('%*d bytes%s%s:  %s', w, z, _SI(z), self._incl, self._repr(o), **print3opts)\n            else:\n                if objs:\n                    t = self._repr(objs)\n                self._printf('%sasizeof(%s%s%s) ...', linesep, t, c, o, **print3opts)\n             # print summary\n            self.print_summary(w=w, objs=objs, **print3opts)\n            if s > 1:  # print profile\n                c = int(s - int(s)) * 100\n                self.print_profiles(w=w, cutoff=c, **print3opts)\n                if s > 2:  # print typedefs\n                    self.print_typedefs(w=w, **print3opts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the summary statistics.", "response": "def print_summary(self, w=0, objs=(), **print3opts):\n        '''Print the summary statistics.\n\n               *w=0*            -- indentation for each line\n\n               *objs=()*        -- optional, list of objects\n\n               *print3options*  -- print options, as in Python 3.0\n        '''\n        self._printf('%*d bytes%s%s', w, self._total, _SI(self._total), self._incl, **print3opts)\n        if self._mask:\n            self._printf('%*d byte aligned', w, self._mask + 1, **print3opts)\n        self._printf('%*d byte sizeof(void*)', w, _sizeof_Cvoidp, **print3opts)\n        n = len(objs or ())\n        if n > 0:\n            d = self._duplicate or ''\n            if d:\n                d = ', %d duplicate' % self._duplicate\n            self._printf('%*d object%s given%s', w, n, _plural(n), d, **print3opts)\n        t = _sum([1 for t in _values(self._seen) if t != 0])  # [] for Python 2.2\n        self._printf('%*d object%s sized', w, t, _plural(t), **print3opts)\n        if self._excl_d:\n            t = _sum(_values(self._excl_d))\n            self._printf('%*d object%s excluded', w, t, _plural(t), **print3opts)\n        t = _sum(_values(self._seen))\n        self._printf('%*d object%s seen', w, t, _plural(t), **print3opts)\n        if self._missed > 0:\n            self._printf('%*d object%s missed', w, self._missed, _plural(self._missed), **print3opts)\n        if self._depth > 0:\n            self._printf('%*d recursion depth', w, self._depth, **print3opts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_typedefs(self, w=0, **print3opts):\n        '''Print the types and dict tables.\n\n               *w=0*            -- indentation for each line\n\n               *print3options*  -- print options, as in Python 3.0\n        '''\n        for k in _all_kinds:\n             # XXX Python 3.0 doesn't sort type objects\n            t = [(self._prepr(a), v) for a, v in _items(_typedefs) if v.kind == k and (v.both or self._code_)]\n            if t:\n                self._printf('%s%*d %s type%s:  basicsize, itemsize, _len_(), _refs()',\n                             linesep, w, len(t), k, _plural(len(t)), **print3opts)\n                for a, v in _sorted(t):\n                    self._printf('%*s %s:  %s', w, '', a, v, **print3opts)\n         # dict and dict-like classes\n        t = _sum([len(v) for v in _values(_dict_classes)])  # [] for Python 2.2\n        if t:\n            self._printf('%s%*d dict/-like classes:', linesep, w, t, **print3opts)\n            for m, v in _items(_dict_classes):\n                self._printf('%*s %s:  %s', w, '', m, self._prepr(v), **print3opts)", "response": "Print the types and dict tables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, align=None, code=None, detail=None, limit=None, stats=None):\n        '''Set some options.  See also **reset**.\n\n               *align*   -- size alignment\n\n               *code*    -- incl. (byte)code size\n\n               *detail*  -- Asized refs level\n\n               *limit*   -- recursion limit\n\n               *stats*   -- print statistics, see function **asizeof**\n\n        Any options not set remain unchanged from the previous setting.\n        '''\n         # adjust\n        if align is not None:\n            self._align_ = align\n            if align > 1:\n                self._mask = align - 1\n                if (self._mask & align) != 0:\n                    raise ValueError('invalid option: %s=%r' % ('align', align))\n            else:\n                self._mask = 0\n        if code is not None:\n            self._code_ = code\n            if code:  # incl. (byte)code\n                self._incl = ' (incl. code)'\n        if detail is not None:\n            self._detail_ = detail\n        if limit is not None:\n            self._limit_ = limit\n        if stats is not None:\n            self._stats_ = s = int(stats)\n            self._cutoff = (stats - s) * 100\n            if s > 1:  # profile types\n                self._profile = True\n            else:\n                self._profile = False", "response": "Set some options.  See also **reset**.\n\n               *align*   -- size alignment\n\n               *code*    -- incl. (byte)code size\n\n               *detail*  -- Asized refs level\n\n               *limit*   -- recursion limit\n\n               *stats*   -- print statistics, see function **asizeof**\n\n        Any options not set remain unchanged from the previous setting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self, align=8,  clip=80,      code=False,  derive=False,\n                    detail=0, ignored=True, infer=False, limit=100,  stats=0,\n                    stream=None):\n        '''Reset options, state, etc.\n\n        The available options and default values are:\n\n             *align=8*       -- size alignment\n\n             *clip=80*       -- clip repr() strings\n\n             *code=False*    -- incl. (byte)code size\n\n             *derive=False*  -- derive from super type\n\n             *detail=0*      -- Asized refs level\n\n             *ignored=True*  -- ignore certain types\n\n             *infer=False*   -- try to infer types\n\n             *limit=100*     -- recursion limit\n\n             *stats=0.0*     -- print statistics, see function **asizeof**\n\n             *stream=None*   -- output stream for printing\n\n        See function **asizeof** for a description of the options.\n        '''\n         # options\n        self._align_  = align\n        self._clip_   = clip\n        self._code_   = code\n        self._derive_ = derive\n        self._detail_ = detail  # for Asized only\n        self._infer_  = infer\n        self._limit_  = limit\n        self._stats_  = stats\n        self._stream  = stream\n        if ignored:\n            self._ign_d = _kind_ignored\n        else:\n            self._ign_d = None\n         # clear state\n        self._clear()\n        self.set(align=align, code=code, stats=stats)", "response": "Reset the internal state of the object to the default state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the conda executable robustly across conda versions.", "response": "def _find_conda():\n    \"\"\"Find the conda executable robustly across conda versions.\n\n    Returns\n    -------\n    conda : str\n        Path to the conda executable.\n\n    Raises\n    ------\n    IOError\n        If the executable cannot be found in either the CONDA_EXE environment\n        variable or in the PATH.\n\n    Notes\n    -----\n    In POSIX platforms in conda >= 4.4, conda can be set up as a bash function\n    rather than an executable. (This is to enable the syntax\n    ``conda activate env-name``.) In this case, the environment variable\n    ``CONDA_EXE`` contains the path to the conda executable. In other cases,\n    we use standard search for the appropriate name in the PATH.\n\n    See https://github.com/airspeed-velocity/asv/issues/645 for more details.\n    \"\"\"\n    if 'CONDA_EXE' in os.environ:\n        conda = os.environ['CONDA_EXE']\n    else:\n        conda = util.which('conda')\n    return conda"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recvall(sock, size):\n    data = b\"\"\n    while len(data) < size:\n        s = sock.recv(size - len(data))\n        data += s\n        if not s:\n            raise RuntimeError(\"did not receive data from socket \"\n                               \"(size {}, got only {!r})\".format(size, data))\n    return data", "response": "Receive data from a socket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_source_code(items):\n    sources = []\n    prev_class_name = None\n\n    for func in items:\n        try:\n            lines, lineno = inspect.getsourcelines(func)\n        except TypeError:\n            continue\n\n        if not lines:\n            continue\n\n        src = \"\\n\".join(line.rstrip() for line in lines)\n        src = textwrap.dedent(src)\n\n        class_name = None\n        if inspect.ismethod(func):\n            # Add class name\n            if hasattr(func, 'im_class'):\n                class_name = func.im_class.__name__\n            elif hasattr(func, '__qualname__'):\n                names = func.__qualname__.split('.')\n                if len(names) > 1:\n                    class_name = names[-2]\n\n        if class_name and prev_class_name != class_name:\n            src = \"class {0}:\\n    {1}\".format(\n                class_name, src.replace(\"\\n\", \"\\n    \"))\n        elif class_name:\n            src = \"    {1}\".format(\n                class_name, src.replace(\"\\n\", \"\\n    \"))\n\n        sources.append(src)\n        prev_class_name = class_name\n\n    return \"\\n\\n\".join(sources).rstrip()", "response": "Extract source code of given items and concatenate and dedent it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disc_modules(module_name, ignore_import_errors=False):\n    if not ignore_import_errors:\n        module = import_module(module_name)\n    else:\n        try:\n            module = import_module(module_name)\n        except BaseException:\n            traceback.print_exc()\n            return\n\n    yield module\n\n    if getattr(module, '__path__', None):\n        for _, name, _ in pkgutil.iter_modules(module.__path__, module_name + '.'):\n            for item in disc_modules(name, ignore_import_errors=ignore_import_errors):\n                yield item", "response": "Recursively import a module and all sub - modules in the package tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndiscover all benchmarks in a given directory tree yielding Benchmark objects.", "response": "def disc_benchmarks(root, ignore_import_errors=False):\n    \"\"\"\n    Discover all benchmarks in a given directory tree, yielding Benchmark\n    objects\n\n    For each class definition, looks for any methods with a\n    special name.\n\n    For each free function, yields all functions with a special\n    name.\n    \"\"\"\n\n    root_name = os.path.basename(root)\n\n    for module in disc_modules(root_name, ignore_import_errors=ignore_import_errors):\n        for attr_name, module_attr in (\n            (k, v) for k, v in module.__dict__.items()\n            if not k.startswith('_')\n        ):\n            if inspect.isclass(module_attr):\n                for name, class_attr in inspect.getmembers(module_attr):\n                    if (inspect.isfunction(class_attr) or\n                            inspect.ismethod(class_attr)):\n                        benchmark = _get_benchmark(name, module, module_attr,\n                                                   class_attr)\n                        if benchmark is not None:\n                            yield benchmark\n            elif inspect.isfunction(module_attr):\n                benchmark = _get_benchmark(attr_name, module, None, module_attr)\n                if benchmark is not None:\n                    yield benchmark"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_benchmark_from_name(root, name, extra_params=None):\n\n    if '-' in name:\n        try:\n            name, param_idx = name.split('-', 1)\n            param_idx = int(param_idx)\n        except ValueError:\n            raise ValueError(\"Benchmark id %r is invalid\" % (name,))\n    else:\n        param_idx = None\n\n    update_sys_path(root)\n    benchmark = None\n\n    # try to directly import benchmark function by guessing its import module\n    # name\n    parts = name.split('.')\n    for i in [1, 2]:\n        path = os.path.join(root, *parts[:-i]) + '.py'\n        if not os.path.isfile(path):\n            continue\n        modname = '.'.join([os.path.basename(root)] + parts[:-i])\n        module = import_module(modname)\n        try:\n            module_attr = getattr(module, parts[-i])\n        except AttributeError:\n            break\n        if i == 1 and inspect.isfunction(module_attr):\n            benchmark = _get_benchmark(parts[-i], module, None, module_attr)\n            break\n        elif i == 2 and inspect.isclass(module_attr):\n            try:\n                class_attr = getattr(module_attr, parts[-1])\n            except AttributeError:\n                break\n            if (inspect.isfunction(class_attr) or\n                    inspect.ismethod(class_attr)):\n                benchmark = _get_benchmark(parts[-1], module, module_attr,\n                                           class_attr)\n                break\n\n    if benchmark is None:\n        for benchmark in disc_benchmarks(root):\n            if benchmark.name == name:\n                break\n        else:\n            raise ValueError(\n                \"Could not find benchmark '{0}'\".format(name))\n\n    if param_idx is not None:\n        benchmark.set_param_idx(param_idx)\n\n    if extra_params:\n        class ExtraBenchmarkAttrs:\n            pass\n        for key, value in extra_params.items():\n            setattr(ExtraBenchmarkAttrs, key, value)\n        benchmark._attr_sources.insert(0, ExtraBenchmarkAttrs)\n\n    return benchmark", "response": "Create a benchmark from a fully - qualified benchmark name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_benchmarks(root, fp):\n    update_sys_path(root)\n\n    # Streaming of JSON back out to the master process\n\n    fp.write('[')\n    first = True\n    for benchmark in disc_benchmarks(root):\n        if not first:\n            fp.write(', ')\n        clean = dict(\n            (k, v) for (k, v) in benchmark.__dict__.items()\n            if isinstance(v, (str, int, float, list, dict, bool)) and not\n               k.startswith('_'))\n        json.dump(clean, fp, skipkeys=True)\n        first = False\n    fp.write(']')", "response": "List all of the discovered benchmarks to fp as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nredirecting stdout and stderr to a file using posix dup2.", "response": "def posix_redirect_output(filename=None, permanent=True):\n    \"\"\"\n    Redirect stdout/stderr to a file, using posix dup2.\n    \"\"\"\n    sys.stdout.flush()\n    sys.stderr.flush()\n\n    stdout_fd = sys.stdout.fileno()\n    stderr_fd = sys.stderr.fileno()\n\n    if not permanent:\n        stdout_fd_copy = os.dup(stdout_fd)\n        stderr_fd_copy = os.dup(stderr_fd)\n\n    if filename is None:\n        out_fd, filename = tempfile.mkstemp()\n    else:\n        out_fd = os.open(filename, os.O_WRONLY | os.O_CREAT | os.O_TRUNC)\n\n    try:\n        # Redirect stdout and stderr to file\n        os.dup2(out_fd, stdout_fd)\n        os.dup2(out_fd, stderr_fd)\n\n        yield filename\n    finally:\n        sys.stdout.flush()\n        sys.stderr.flush()\n        os.close(out_fd)\n\n        if not permanent:\n            os.dup2(stdout_fd_copy, stdout_fd)\n            os.dup2(stderr_fd_copy, stderr_fd)\n            os.close(stdout_fd_copy)\n            os.close(stderr_fd_copy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert_param(self, param):\n        self._current_params = tuple([param] + list(self._current_params))", "response": "Insert a parameter at the front of the parameter list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_cache_dir(self, commit_hash):\n        path = os.path.join(self._path, commit_hash)\n        stamp = path + \".timestamp\"\n        return path, stamp", "response": "Get the cache dir and timestamp file corresponding to a given commit hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns list of commit hash directories in the cache sorted by decreasing timestamp", "response": "def _get_cache_contents(self):\n        \"\"\"\n        Return list of commit hash directories in the cache (containing\n        wheels or not), sorted by decreasing timestamp\n        \"\"\"\n        if not os.path.isdir(self._path):\n            return []\n\n        def sort_key(name):\n            path, stamp = self._get_cache_dir(name)\n            try:\n                return os.stat(stamp).st_mtime\n            except OSError:\n                return 0\n\n        names = os.listdir(self._path)\n        names.sort(key=sort_key, reverse=True)\n        return names"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_atom(dest, entries, author, title, address, updated=None, link=None,\n               language=\"en\"):\n    \"\"\"\n    Write an atom feed to a file.\n\n    Parameters\n    ----------\n    dest : str\n        Destination file path, or a file-like object\n    entries : list of FeedEntry\n        Feed entries.\n    author : str\n        Author of the feed.\n    title : str\n        Title for the feed.\n    address : str\n        Address (domain name or email) to be used in building unique IDs.\n    updated : datetime, optional\n        Time stamp for the feed. If not given, take from the newest entry.\n    link : str, optional\n        Link for the feed.\n    language : str, optional\n        Language of the feed. Default is 'en'.\n\n    \"\"\"\n\n    if updated is None:\n        if entries:\n            updated = max(entry.updated for entry in entries)\n        else:\n            updated = datetime.datetime.utcnow()\n\n    root = etree.Element(ATOM_NS + 'feed')\n\n    # id (obligatory)\n    el = etree.Element(ATOM_NS + 'id')\n    el.text = _get_id(address, None, [\"feed\", author, title])\n    root.append(el)\n\n    # author (obligatory)\n    el = etree.Element(ATOM_NS + 'author')\n    el2 = etree.Element(ATOM_NS + 'name')\n    el2.text = author\n    el.append(el2)\n    root.append(el)\n\n    # title (obligatory)\n    el = etree.Element(ATOM_NS + 'title')\n    el.attrib[XML_NS + 'lang'] = language\n    el.text = title\n    root.append(el)\n\n    # updated (obligatory)\n    el = etree.Element(ATOM_NS + 'updated')\n    el.text = updated.strftime('%Y-%m-%dT%H:%M:%SZ')\n    root.append(el)\n\n    # link\n    if link is not None:\n        el = etree.Element(ATOM_NS + 'link')\n        el.attrib[ATOM_NS + 'href'] = link\n        root.append(el)\n\n    # entries\n    for entry in entries:\n        root.append(entry.get_atom(address, language))\n\n    tree = etree.ElementTree(root)\n\n    def write(f):\n        if sys.version_info[:2] < (2, 7):\n            _etree_py26_write(f, tree)\n        else:\n            tree.write(f, xml_declaration=True, default_namespace=ATOM_NS[1:-1],\n                       encoding=str('utf-8'))\n\n    if hasattr(dest, 'write'):\n        write(dest)\n    else:\n        with util.long_path_open(dest, 'wb') as f:\n            write(f)", "response": "Write an atom feed to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate an unique Atom id for the given content.", "response": "def _get_id(owner, date, content):\n    \"\"\"\n    Generate an unique Atom id for the given content\n    \"\"\"\n    h = hashlib.sha256()\n    # Hash still contains the original project url, keep as is\n    h.update(\"github.com/spacetelescope/asv\".encode('utf-8'))\n    for x in content:\n        if x is None:\n            h.update(\",\".encode('utf-8'))\n        else:\n            h.update(x.encode('utf-8'))\n        h.update(\",\".encode('utf-8'))\n\n    if date is None:\n        date = datetime.datetime(1970, 1, 1)\n    return \"tag:{0},{1}:/{2}\".format(owner, date.strftime('%Y-%m-%d'), h.hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the debuggee labels from environment variables and flags.", "response": "def InitializeDebuggeeLabels(self, flags):\n    \"\"\"Initialize debuggee labels from environment variables and flags.\n\n    The caller passes all the flags that the the debuglet got. This function\n    will only use the flags used to label the debuggee. Flags take precedence\n    over environment variables.\n\n    Debuggee description is formatted from available flags.\n\n    Args:\n      flags: dictionary of debuglet command line flags.\n    \"\"\"\n    self._debuggee_labels = {}\n\n    for (label, var_names) in six.iteritems(_DEBUGGEE_LABELS):\n      # var_names is a list of possible environment variables that may contain\n      # the label value. Find the first one that is set.\n      for name in var_names:\n        value = os.environ.get(name)\n        if value:\n          # Special case for module. We omit the \"default\" module\n          # to stay consistent with AppEngine.\n          if label == labels.Debuggee.MODULE and value == 'default':\n            break\n          self._debuggee_labels[label] = value\n          break\n\n    if flags:\n      self._debuggee_labels.update(\n          {name: value for (name, value) in six.iteritems(flags)\n           if name in _DEBUGGEE_LABELS})\n\n    self._debuggee_labels['projectid'] = self._project_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets up authentication with Google APIs.", "response": "def SetupAuth(self,\n                project_id=None,\n                project_number=None,\n                service_account_json_file=None):\n    \"\"\"Sets up authentication with Google APIs.\n\n    This will use the credentials from service_account_json_file if provided,\n    falling back to application default credentials.\n    See https://cloud.google.com/docs/authentication/production.\n\n    Args:\n      project_id: GCP project ID (e.g. myproject). If not provided, will attempt\n          to retrieve it from the credentials.\n      project_number: GCP project number (e.g. 72386324623). If not provided,\n          project_id will be used in its place.\n      service_account_json_file: JSON file to use for credentials. If not\n          provided, will default to application default credentials.\n    Raises:\n      NoProjectIdError: If the project id cannot be determined.\n    \"\"\"\n    if service_account_json_file:\n      self._credentials = (\n          service_account.Credentials.from_service_account_file(\n              service_account_json_file, scopes=_CLOUD_PLATFORM_SCOPE))\n      if not project_id:\n        with open(service_account_json_file) as f:\n          project_id = json.load(f).get('project_id')\n    else:\n      self._credentials, credentials_project_id = google.auth.default(\n          scopes=_CLOUD_PLATFORM_SCOPE)\n      project_id = project_id or credentials_project_id\n\n    if not project_id:\n      raise NoProjectIdError(\n          'Unable to determine the project id from the API credentials. '\n          'Please specify the project id using the --project_id flag.')\n\n    self._project_id = project_id\n    self._project_number = project_number or project_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Start(self):\n    self._shutdown = False\n\n    self._main_thread = threading.Thread(target=self._MainThreadProc)\n    self._main_thread.name = 'Cloud Debugger main worker thread'\n    self._main_thread.daemon = True\n    self._main_thread.start()", "response": "Starts the worker thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Stop(self):\n    self._shutdown = True\n    self._new_updates.set()  # Wake up the transmission thread.\n\n    if self._main_thread is not None:\n      self._main_thread.join()\n      self._main_thread = None\n\n    if self._transmission_thread is not None:\n      self._transmission_thread.join()\n      self._transmission_thread = None", "response": "Stops the worker threads and waits until it exits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _MainThreadProc(self):\n    registration_required = True\n    while not self._shutdown:\n      if registration_required:\n        service = self._BuildService()\n        registration_required, delay = self._RegisterDebuggee(service)\n\n      if not registration_required:\n        registration_required, delay = self._ListActiveBreakpoints(service)\n\n      if self.on_idle is not None:\n        self.on_idle()\n\n      if not self._shutdown:\n        time.sleep(delay)", "response": "Entry point for the worker thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _TransmissionThreadProc(self):\n    reconnect = True\n\n    while not self._shutdown:\n      self._new_updates.clear()\n\n      if reconnect:\n        service = self._BuildService()\n        reconnect = False\n\n      reconnect, delay = self._TransmitBreakpointUpdates(service)\n\n      self._new_updates.wait(delay)", "response": "Entry point for the transmission worker thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsingles attempt to register the debuggee. If the registration succeeds, sets self._debuggee_id to the registered debuggee ID. Args: service: client to use for API calls Returns: (registration_required, delay) tuple", "response": "def _RegisterDebuggee(self, service):\n    \"\"\"Single attempt to register the debuggee.\n\n    If the registration succeeds, sets self._debuggee_id to the registered\n    debuggee ID.\n\n    Args:\n      service: client to use for API calls\n\n    Returns:\n      (registration_required, delay) tuple\n    \"\"\"\n    try:\n      request = {'debuggee': self._GetDebuggee()}\n\n      try:\n        response = service.debuggees().register(body=request).execute()\n\n        # self._project_number will refer to the project id on initialization if\n        # the project number is not available. The project field in the debuggee\n        # will always refer to the project number. Update so the server will not\n        # have to do id->number translations in the future.\n        project_number = response['debuggee'].get('project')\n        self._project_number = project_number or self._project_number\n\n        self._debuggee_id = response['debuggee']['id']\n        native.LogInfo('Debuggee registered successfully, ID: %s' % (\n            self._debuggee_id))\n        self.register_backoff.Succeeded()\n        return (False, 0)  # Proceed immediately to list active breakpoints.\n      except BaseException:\n        native.LogInfo('Failed to register debuggee: %s, %s' %\n                       (request, traceback.format_exc()))\n    except BaseException:\n      native.LogWarning('Debuggee information not available: ' +\n                        traceback.format_exc())\n\n    return (True, self.register_backoff.Failed())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ListActiveBreakpoints(self, service):\n    try:\n      response = service.debuggees().breakpoints().list(\n          debuggeeId=self._debuggee_id, waitToken=self._wait_token,\n          successOnTimeout=True).execute()\n      if not response.get('waitExpired'):\n        self._wait_token = response.get('nextWaitToken')\n        breakpoints = response.get('breakpoints') or []\n        if self._breakpoints != breakpoints:\n          self._breakpoints = breakpoints\n          native.LogInfo(\n              'Breakpoints list changed, %d active, wait token: %s' % (\n                  len(self._breakpoints), self._wait_token))\n          self.on_active_breakpoints_changed(copy.deepcopy(self._breakpoints))\n    except BaseException:\n      native.LogInfo('Failed to query active breakpoints: ' +\n                     traceback.format_exc())\n\n      # Forget debuggee ID to trigger repeated debuggee registration. Once the\n      # registration succeeds, the worker thread will retry this query\n      self._debuggee_id = None\n\n      return (True, self.list_backoff.Failed())\n\n    self.list_backoff.Succeeded()\n    return (False, 0)", "response": "Internal method to query the list of active breakpoints."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _TransmitBreakpointUpdates(self, service):\n    reconnect = False\n    retry_list = []\n\n    # There is only one consumer, so two step pop is safe.\n    while self._transmission_queue:\n      breakpoint, retry_count = self._transmission_queue.popleft()\n\n      try:\n        service.debuggees().breakpoints().update(\n            debuggeeId=self._debuggee_id, id=breakpoint['id'],\n            body={'breakpoint': breakpoint}).execute()\n\n        native.LogInfo('Breakpoint %s update transmitted successfully' % (\n            breakpoint['id']))\n      except apiclient.errors.HttpError as err:\n        # Treat 400 error codes (except timeout) as application error that will\n        # not be retried. All other errors are assumed to be transient.\n        status = err.resp.status\n        is_transient = ((status >= 500) or (status == 408))\n        if is_transient and retry_count < self.max_transmit_attempts - 1:\n          native.LogInfo('Failed to send breakpoint %s update: %s' % (\n              breakpoint['id'], traceback.format_exc()))\n          retry_list.append((breakpoint, retry_count + 1))\n        elif is_transient:\n          native.LogWarning(\n              'Breakpoint %s retry count exceeded maximum' % breakpoint['id'])\n        else:\n          # This is very common if multiple instances are sending final update\n          # simultaneously.\n          native.LogInfo('%s, breakpoint: %s' % (err, breakpoint['id']))\n      except BaseException:\n        native.LogWarning(\n            'Fatal error sending breakpoint %s update: %s' % (\n                breakpoint['id'], traceback.format_exc()))\n        reconnect = True\n\n    self._transmission_queue.extend(retry_list)\n\n    if not self._transmission_queue:\n      self.update_backoff.Succeeded()\n      # Nothing to send, wait until next breakpoint update.\n      return (reconnect, None)\n    else:\n      return (reconnect, self.update_backoff.Failed())", "response": "Sends all pending breakpoint updates to the backend."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _GetDebuggee(self):\n    major_version = 'v' + version.__version__.split('.')[0]\n    python_version = ''.join(platform.python_version().split('.')[:2])\n    agent_version = ('google.com/python%s-gcp/%s' % (python_version,\n                                                     major_version))\n\n    debuggee = {\n        'project': self._project_number,\n        'description': self._GetDebuggeeDescription(),\n        'labels': self._debuggee_labels,\n        'agentVersion': agent_version,\n    }\n\n    source_context = self._ReadAppJsonFile('source-context.json')\n    if source_context:\n      debuggee['sourceContexts'] = [source_context]\n\n    debuggee['uniquifier'] = self._ComputeUniquifier(debuggee)\n\n    return debuggee", "response": "Builds the debuggee structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetDebuggeeDescription(self):\n    return '-'.join(self._debuggee_labels[label]\n                    for label in _DESCRIPTION_LABELS\n                    if label in self._debuggee_labels)", "response": "Formats debuggee description based on debuggee labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ComputeUniquifier(self, debuggee):\n    uniquifier = hashlib.sha1()\n\n    # Compute hash of application files if we don't have source context. This\n    # way we can still distinguish between different deployments.\n    if ('minorversion' not in debuggee.get('labels', []) and\n        'sourceContexts' not in debuggee):\n      uniquifier_computer.ComputeApplicationUniquifier(uniquifier)\n\n    return uniquifier.hexdigest()", "response": "Computes the uniquifier for the debuggee."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ReadAppJsonFile(self, relative_path):\n    try:\n      with open(os.path.join(sys.path[0], relative_path), 'r') as f:\n        return json.load(f)\n    except (IOError, ValueError):\n      return None", "response": "Reads a JSON file from an application root directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef NormalizePath(path):\n  path = os.path.normpath(path)\n\n  for sys_path in sys.path:\n    if not sys_path:\n      continue\n\n    # Append '/' at the end of the path if it's not there already.\n    sys_path = os.path.join(sys_path, '')\n\n    if path.startswith(sys_path):\n      return path[len(sys_path):]\n\n  return path", "response": "Normalizes the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the type of a value returning a full path string.", "response": "def DetermineType(value):\n  \"\"\"Determines the type of val, returning a \"full path\" string.\n\n  For example:\n    DetermineType(5) -> __builtin__.int\n    DetermineType(Foo()) -> com.google.bar.Foo\n\n  Args:\n    value: Any value, the value is irrelevant as only the type metadata\n    is checked\n\n  Returns:\n    Type path string.  None if type cannot be determined.\n  \"\"\"\n\n  object_type = type(value)\n  if not hasattr(object_type, '__name__'):\n    return None\n\n  type_string = getattr(object_type, '__module__', '')\n  if type_string:\n    type_string += '.'\n\n  type_string += object_type.__name__\n  return type_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetLoggingLocation():\n  frame = inspect.currentframe()\n  this_file = frame.f_code.co_filename\n  frame = frame.f_back\n  while frame:\n    if this_file == frame.f_code.co_filename:\n      if 'cdbg_logging_location' in frame.f_locals:\n        ret = frame.f_locals['cdbg_logging_location']\n        if len(ret) != 3:\n          return (None, None, None)\n        return ret\n    frame = frame.f_back\n  return (None, None, None)", "response": "Search for and return the file and line number and function name of the logpoint location."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the logger object to use for all LOG breakpoint actions.", "response": "def SetLogger(logger):\n  \"\"\"Sets the logger object to use for all 'LOG' breakpoint actions.\"\"\"\n  global log_info_message\n  global log_warning_message\n  global log_error_message\n  log_info_message = logger.info\n  log_warning_message = logger.warning\n  log_error_message = logger.error\n  logger.addFilter(LineNoFilter())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompiling and evaluates a watched expression.", "response": "def _EvaluateExpression(frame, expression):\n  \"\"\"Compiles and evaluates watched expression.\n\n  Args:\n    frame: evaluation context.\n    expression: watched expression to compile and evaluate.\n\n  Returns:\n    (False, status) on error or (True, value) on success.\n  \"\"\"\n  try:\n    code = compile(expression, '<watched_expression>', 'eval')\n  except (TypeError, ValueError) as e:\n    # expression string contains null bytes.\n    return (False, {\n        'isError': True,\n        'refersTo': 'VARIABLE_NAME',\n        'description': {\n            'format': 'Invalid expression',\n            'parameters': [str(e)]}})\n  except SyntaxError as e:\n    return (False, {\n        'isError': True,\n        'refersTo': 'VARIABLE_NAME',\n        'description': {\n            'format': 'Expression could not be compiled: $0',\n            'parameters': [e.msg]}})\n\n  try:\n    return (True, native.CallImmutable(frame, code))\n  except BaseException as e:  # pylint: disable=broad-except\n    return (False, {\n        'isError': True,\n        'refersTo': 'VARIABLE_VALUE',\n        'description': {\n            'format': 'Exception occurred: $0',\n            'parameters': [str(e)]}})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _GetFrameCodeObjectName(frame):\n  # This functions under the assumption that member functions will name their\n  # first parameter argument 'self' but has some edge-cases.\n  if frame.f_code.co_argcount >= 1 and 'self' == frame.f_code.co_varnames[0]:\n    return (frame.f_locals['self'].__class__.__name__ +\n            '.' + frame.f_code.co_name)\n  else:\n    return frame.f_code.co_name", "response": "Gets the code object name for the frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat the message. Unescapes '$$' with '$.", "response": "def _FormatMessage(template, parameters):\n  \"\"\"Formats the message. Unescapes '$$' with '$'.\n\n  Args:\n    template: message template (e.g. 'a = $0, b = $1').\n    parameters: substitution parameters for the format.\n\n  Returns:\n    Formatted message with parameters embedded in template placeholders.\n  \"\"\"\n  def GetParameter(m):\n    try:\n      return parameters[int(m.group(0)[1:])]\n    except IndexError:\n      return INVALID_EXPRESSION_INDEX\n\n  parts = template.split('$$')\n  return '$'.join(re.sub(r'\\$\\d+', GetParameter, part) for part in parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncollect the call stack local variables and objects from the specified frame.", "response": "def Collect(self, top_frame):\n    \"\"\"Collects call stack, local variables and objects.\n\n    Starts collection from the specified frame. We don't start from the top\n    frame to exclude the frames due to debugger. Updates the content of\n    self.breakpoint.\n\n    Args:\n      top_frame: top frame to start data collection.\n    \"\"\"\n    # Evaluate call stack.\n    frame = top_frame\n    top_line = self.breakpoint['location']['line']\n    breakpoint_frames = self.breakpoint['stackFrames']\n    try:\n      # Evaluate watched expressions.\n      if 'expressions' in self.breakpoint:\n        self.breakpoint['evaluatedExpressions'] = [\n            self._CaptureExpression(top_frame, expression) for expression\n            in self.breakpoint['expressions']]\n\n      while frame and (len(breakpoint_frames) < self.max_frames):\n        line = top_line if frame == top_frame else frame.f_lineno\n        code = frame.f_code\n        if len(breakpoint_frames) < self.max_expand_frames:\n          frame_arguments, frame_locals = self.CaptureFrameLocals(frame)\n        else:\n          frame_arguments = []\n          frame_locals = []\n\n        breakpoint_frames.append({\n            'function': _GetFrameCodeObjectName(frame),\n            'location': {\n                'path': NormalizePath(code.co_filename),\n                'line': line\n            },\n            'arguments': frame_arguments,\n            'locals': frame_locals\n        })\n        frame = frame.f_back\n\n    except BaseException as e:  # pylint: disable=broad-except\n      # The variable table will get serialized even though there was a failure.\n      # The results can be useful for diagnosing the internal error.\n      self.breakpoint['status'] = {\n          'isError': True,\n          'description': {\n              'format': ('INTERNAL ERROR: Failed while capturing locals '\n                         'of frame $0: $1'),\n              'parameters': [str(len(breakpoint_frames)), str(e)]}}\n\n    # Number of entries in _var_table. Starts at 1 (index 0 is the 'buffer full'\n    # status value).\n    num_vars = 1\n\n    # Explore variables table in BFS fashion. The variables table will grow\n    # inside CaptureVariable as we encounter new references.\n    while (num_vars < len(self._var_table)) and (\n        self._total_size < self.max_size):\n      self._var_table[num_vars] = self.CaptureVariable(\n          self._var_table[num_vars], 0, self.default_capture_limits,\n          can_enqueue=False)\n\n      # Move on to the next entry in the variable table.\n      num_vars += 1\n\n    # Trim variables table and change make all references to variables that\n    # didn't make it point to var_index of 0 (\"buffer full\")\n    self.TrimVariableTable(num_vars)\n\n    self._CaptureEnvironmentLabels()\n    self._CaptureRequestLogId()\n    self._CaptureUserId()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncapturing local variables and arguments of the specified frame.", "response": "def CaptureFrameLocals(self, frame):\n    \"\"\"Captures local variables and arguments of the specified frame.\n\n    Args:\n      frame: frame to capture locals and arguments.\n\n    Returns:\n      (arguments, locals) tuple.\n    \"\"\"\n    # Capture all local variables (including method arguments).\n    variables = {n: self.CaptureNamedVariable(n, v, 1,\n                                              self.default_capture_limits)\n                 for n, v in six.viewitems(frame.f_locals)}\n\n    # Split between locals and arguments (keeping arguments in the right order).\n    nargs = frame.f_code.co_argcount\n    if frame.f_code.co_flags & inspect.CO_VARARGS: nargs += 1\n    if frame.f_code.co_flags & inspect.CO_VARKEYWORDS: nargs += 1\n\n    frame_arguments = []\n    for argname in frame.f_code.co_varnames[:nargs]:\n      if argname in variables: frame_arguments.append(variables.pop(argname))\n\n    return (frame_arguments, list(six.viewvalues(variables)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CaptureNamedVariable(self, name, value, depth, limits):\n    if not hasattr(name, '__dict__'):\n      name = str(name)\n    else:  # TODO(vlif): call str(name) with immutability verifier here.\n      name = str(id(name))\n    self._total_size += len(name)\n\n    v = (self.CheckDataVisiblity(value) or\n         self.CaptureVariable(value, depth, limits))\n    v['name'] = name\n    return v", "response": "Appends name to the product of CaptureVariable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the given value is visible in the given resource.", "response": "def CheckDataVisiblity(self, value):\n    \"\"\"Returns a status object if the given name is not visible.\n\n    Args:\n      value: The value to check.  The actual value here is not important but the\n      value's metadata (e.g. package and type) will be checked.\n\n    Returns:\n      None if the value is visible.  A variable structure with an error status\n      if the value should not be visible.\n    \"\"\"\n    if not self.data_visibility_policy:\n      return None\n\n    visible, reason = self.data_visibility_policy.IsDataVisible(\n        DetermineType(value))\n\n    if visible:\n      return None\n\n    return {\n        'status': {\n            'isError': True,\n            'refersTo': 'VARIABLE_NAME',\n            'description': {\n                'format': reason\n            }\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CaptureVariablesList(self, items, depth, empty_message, limits):\n    v = []\n    for name, value in items:\n      if (self._total_size >= self.max_size) or (\n          len(v) >= limits.max_list_items):\n        v.append({\n            'status': {\n                'refersTo': 'VARIABLE_VALUE',\n                'description': {\n                    'format':\n                        ('Only first $0 items were captured. Use in an '\n                         'expression to see all items.'),\n                    'parameters': [str(len(v))]}}})\n        break\n      v.append(self.CaptureNamedVariable(name, value, depth, limits))\n\n    if not v:\n      return [{'status': {\n          'refersTo': 'VARIABLE_NAME',\n          'description': {'format': empty_message}}}]\n\n    return v", "response": "Captures list of named items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry - Except wrapped version of CaptureVariableInternal.", "response": "def CaptureVariable(self, value, depth, limits, can_enqueue=True):\n    \"\"\"Try-Except wrapped version of CaptureVariableInternal.\"\"\"\n    try:\n      return self.CaptureVariableInternal(value, depth, limits, can_enqueue)\n    except BaseException as e:  # pylint: disable=broad-except\n      return {\n          'status': {\n              'isError': True,\n              'refersTo': 'VARIABLE_VALUE',\n              'description': {\n                  'format': ('Failed to capture variable: $0'),\n                  'parameters': [str(e)]\n              }\n          }\n      }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CaptureVariableInternal(self, value, depth, limits, can_enqueue=True):\n    if depth == limits.max_depth:\n      return {'varTableIndex': 0}  # Buffer full.\n\n    if value is None:\n      self._total_size += 4\n      return {'value': 'None'}\n\n    if isinstance(value, _PRIMITIVE_TYPES):\n      r = _TrimString(repr(value),  # Primitive type, always immutable.\n                      min(limits.max_value_len,\n                          self.max_size - self._total_size))\n      self._total_size += len(r)\n      return {'value': r, 'type': type(value).__name__}\n\n    if isinstance(value, _DATE_TYPES):\n      r = str(value)  # Safe to call str().\n      self._total_size += len(r)\n      return {'value': r, 'type': 'datetime.'+ type(value).__name__}\n\n    if isinstance(value, dict):\n      # Do not use iteritems() here. If GC happens during iteration (which it\n      # often can for dictionaries containing large variables), you will get a\n      # RunTimeError exception.\n      items = [(repr(k), v) for (k, v) in value.items()]\n      return {'members':\n              self.CaptureVariablesList(items, depth + 1,\n                                        EMPTY_DICTIONARY, limits),\n              'type': 'dict'}\n\n    if isinstance(value, _VECTOR_TYPES):\n      fields = self.CaptureVariablesList(\n          (('[%d]' % i, x) for i, x in enumerate(value)),\n          depth + 1, EMPTY_COLLECTION, limits)\n      return {'members': fields, 'type': type(value).__name__}\n\n    if isinstance(value, types.FunctionType):\n      self._total_size += len(value.__name__)\n      # TODO(vlif): set value to func_name and type to 'function'\n      return {'value': 'function ' + value.__name__}\n\n    if isinstance(value, Exception):\n      fields = self.CaptureVariablesList(\n          (('[%d]' % i, x) for i, x in enumerate(value.args)),\n          depth + 1, EMPTY_COLLECTION, limits)\n      return {'members': fields, 'type': type(value).__name__}\n\n    if can_enqueue:\n      index = self._var_table_index.get(id(value))\n      if index is None:\n        index = len(self._var_table)\n        self._var_table_index[id(value)] = index\n        self._var_table.append(value)\n      self._total_size += 4  # number of characters to accommodate a number.\n      return {'varTableIndex': index}\n\n    for pretty_printer in CaptureCollector.pretty_printers:\n      pretty_value = pretty_printer(value)\n      if not pretty_value:\n        continue\n\n      fields, object_type = pretty_value\n      return {'members':\n              self.CaptureVariablesList(fields, depth + 1, OBJECT_HAS_NO_FIELDS,\n                                        limits),\n              'type': object_type}\n\n    if not hasattr(value, '__dict__'):\n      # TODO(vlif): keep \"value\" empty and populate the \"type\" field instead.\n      r = str(type(value))\n      self._total_size += len(r)\n      return {'value': r}\n\n    # Add an additional depth for the object itself\n    items = value.__dict__.items()\n    if six.PY3:\n      # Make a list of the iterator in Python 3, to avoid 'dict changed size\n      # during iteration' errors from GC happening in the middle.\n      # Only limits.max_list_items + 1 items are copied, anything past that will\n      # get ignored by CaptureVariablesList().\n      items = list(itertools.islice(items, limits.max_list_items + 1))\n    members = self.CaptureVariablesList(items, depth + 2,\n                                        OBJECT_HAS_NO_FIELDS, limits)\n    v = {'members': members}\n\n    type_string = DetermineType(value)\n    if type_string:\n      v['type'] = type_string\n\n    return v", "response": "Captures a single nameless object into a Variable message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate the expression and captures it into a Variable object.", "response": "def _CaptureExpression(self, frame, expression):\n    \"\"\"Evalutes the expression and captures it into a Variable object.\n\n    Args:\n      frame: evaluation context.\n      expression: watched expression to compile and evaluate.\n\n    Returns:\n      Variable object (which will have error status if the expression fails\n      to evaluate).\n    \"\"\"\n    rc, value = _EvaluateExpression(frame, expression)\n    if not rc:\n      return {'name': expression, 'status': value}\n\n    return self.CaptureNamedVariable(expression, value, 0,\n                                     self.expression_capture_limits)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrims the variable table in the formatted breakpoint message.", "response": "def TrimVariableTable(self, new_size):\n    \"\"\"Trims the variable table in the formatted breakpoint message.\n\n    Removes trailing entries in variables table. Then scans the entire\n    breakpoint message and replaces references to the trimmed variables to\n    point to var_index of 0 (\"buffer full\").\n\n    Args:\n      new_size: desired size of variables table.\n    \"\"\"\n\n    def ProcessBufferFull(variables):\n      for variable in variables:\n        var_index = variable.get('varTableIndex')\n        if var_index is not None and (var_index >= new_size):\n          variable['varTableIndex'] = 0  # Buffer full.\n        members = variable.get('members')\n        if members is not None:\n          ProcessBufferFull(members)\n\n    del self._var_table[new_size:]\n    ProcessBufferFull(self.breakpoint['evaluatedExpressions'])\n    for stack_frame in self.breakpoint['stackFrames']:\n      ProcessBufferFull(stack_frame['arguments'])\n      ProcessBufferFull(stack_frame['locals'])\n    ProcessBufferFull(self._var_table)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncapturing information about the environment if possible.", "response": "def _CaptureEnvironmentLabels(self):\n    \"\"\"Captures information about the environment, if possible.\"\"\"\n    if 'labels' not in self.breakpoint:\n      self.breakpoint['labels'] = {}\n\n    if callable(breakpoint_labels_collector):\n      for (key, value) in six.iteritems(breakpoint_labels_collector()):\n        self.breakpoint['labels'][key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncapture the request log id if possible.", "response": "def _CaptureRequestLogId(self):\n    \"\"\"Captures the request log id if possible.\n\n    The request log id is stored inside the breakpoint labels.\n    \"\"\"\n    # pylint: disable=not-callable\n    if callable(request_log_id_collector):\n      request_log_id = request_log_id_collector()\n      if request_log_id:\n        # We have a request_log_id, save it into the breakpoint labels\n        self.breakpoint['labels'][\n            labels.Breakpoint.REQUEST_LOG_ID] = request_log_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaptures the user id of the end user if possible.", "response": "def _CaptureUserId(self):\n    \"\"\"Captures the user id of the end user, if possible.\"\"\"\n    user_kind, user_id = user_id_collector()\n    if user_kind and user_id:\n      self.breakpoint['evaluatedUserId'] = {'kind': user_kind, 'id': user_id}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Log(self, frame):\n    # Return error if log methods were not configured globally.\n    if not self._log_message:\n      return {'isError': True,\n              'description': {'format': LOG_ACTION_NOT_SUPPORTED}}\n\n    if self._quota_recovery_start_time:\n      ms_elapsed = (time.time() - self._quota_recovery_start_time) * 1000\n      if ms_elapsed > self.quota_recovery_ms:\n        # We are out of the recovery period, clear the time and continue\n        self._quota_recovery_start_time = None\n      else:\n        # We are in the recovery period, exit\n        return\n\n    # Evaluate watched expressions.\n    message = 'LOGPOINT: ' + _FormatMessage(\n        self._definition.get('logMessageFormat', ''),\n        self._EvaluateExpressions(frame))\n\n    line = self._definition['location']['line']\n    cdbg_logging_location = (NormalizePath(frame.f_code.co_filename), line,\n                             _GetFrameCodeObjectName(frame))\n\n    if native.ApplyDynamicLogsQuota(len(message)):\n      self._log_message(message)\n    else:\n      self._quota_recovery_start_time = time.time()\n      self._log_message(DYNAMIC_LOG_OUT_OF_QUOTA)\n    del cdbg_logging_location\n    return None", "response": "Captures the minimal application states formats it and logs the message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate watched expressions into a string form.", "response": "def _EvaluateExpressions(self, frame):\n    \"\"\"Evaluates watched expressions into a string form.\n\n    If expression evaluation fails, the error message is used as evaluated\n    expression string.\n\n    Args:\n      frame: Python stack frame of breakpoint hit.\n\n    Returns:\n      Array of strings where each string corresponds to the breakpoint\n      expression with the same index.\n    \"\"\"\n    return [self._FormatExpression(frame, expression) for expression in\n            self._definition.get('expressions') or []]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates a single watched expression and formats it into a string form.", "response": "def _FormatExpression(self, frame, expression):\n    \"\"\"Evaluates a single watched expression and formats it into a string form.\n\n    If expression evaluation fails, returns error message string.\n\n    Args:\n      frame: Python stack frame in which the expression is evaluated.\n      expression: string expression to evaluate.\n\n    Returns:\n      Formatted expression value that can be used in the log message.\n    \"\"\"\n    rc, value = _EvaluateExpression(frame, expression)\n    if not rc:\n      message = _FormatMessage(value['description']['format'],\n                               value['description'].get('parameters'))\n      return '<' + message + '>'\n\n    return self._FormatValue(value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _FormatValue(self, value, level=0):\n\n    def FormatDictItem(key_value):\n      \"\"\"Formats single dictionary item.\"\"\"\n      key, value = key_value\n      return (self._FormatValue(key, level + 1) +\n              ': ' +\n              self._FormatValue(value, level + 1))\n\n    def LimitedEnumerate(items, formatter, level=0):\n      \"\"\"Returns items in the specified enumerable enforcing threshold.\"\"\"\n      count = 0\n      limit = self.max_sublist_items if level > 0 else self.max_list_items\n      for item in items:\n        if count == limit:\n          yield '...'\n          break\n\n        yield formatter(item)\n        count += 1\n\n    def FormatList(items, formatter, level=0):\n      \"\"\"Formats a list using a custom item formatter enforcing threshold.\"\"\"\n      return ', '.join(LimitedEnumerate(items, formatter, level=level))\n\n    if isinstance(value, _PRIMITIVE_TYPES):\n      return _TrimString(repr(value),  # Primitive type, always immutable.\n                         self.max_value_len)\n\n    if isinstance(value, _DATE_TYPES):\n      return str(value)\n\n    if level > self.max_depth:\n      return str(type(value))\n\n    if isinstance(value, dict):\n      return '{' + FormatList(six.iteritems(value), FormatDictItem) + '}'\n\n    if isinstance(value, _VECTOR_TYPES):\n      return _ListTypeFormatString(value).format(FormatList(\n          value, lambda item: self._FormatValue(item, level + 1), level=level))\n\n    if isinstance(value, types.FunctionType):\n      return 'function ' + value.__name__\n\n    if hasattr(value, '__dict__') and value.__dict__:\n      return self._FormatValue(value.__dict__, level)\n\n    return str(type(value))", "response": "Pretty - print a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef OpenAndRead(relative_path='debugger-blacklist.yaml'):\n\n  # Note: This logic follows the convention established by source-context.json\n  try:\n    with open(os.path.join(sys.path[0], relative_path), 'r') as f:\n      return Read(f)\n  except IOError:\n    return None", "response": "Attempts to find the yaml configuration file then read it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread and returns a Config object from a yaml file.", "response": "def Read(f):\n  \"\"\"Reads and returns Config data from a yaml file.\n\n  Args:\n    f: Yaml file to parse.\n\n  Returns:\n    Config object as defined in this file.\n\n  Raises:\n    Error (some subclass): If there is a problem loading or parsing the file.\n  \"\"\"\n  try:\n    yaml_data = yaml.load(f)\n  except yaml.YAMLError as e:\n    raise ParseError('%s' % e)\n  except IOError as e:\n    raise YAMLLoadError('%s' % e)\n\n  _CheckData(yaml_data)\n\n  try:\n    return Config(\n        yaml_data.get('blacklist', ()),\n        yaml_data.get('whitelist', ('*')))\n  except UnicodeDecodeError as e:\n    raise YAMLLoadError('%s' % e)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CheckData(yaml_data):\n  legal_keys = set(('blacklist', 'whitelist'))\n  unknown_keys = set(yaml_data) - legal_keys\n  if unknown_keys:\n    raise UnknownConfigKeyError(\n        'Unknown keys in configuration: %s' % unknown_keys)\n\n  for key, data in six.iteritems(yaml_data):\n    _AssertDataIsList(key, data)", "response": "Checks data for illegal keys and formatting."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassert that lst contains list data and is not structured.", "response": "def _AssertDataIsList(key, lst):\n  \"\"\"Assert that lst contains list data and is not structured.\"\"\"\n\n  # list and tuple are supported.  Not supported are direct strings\n  # and dictionary; these indicate too much or two little structure.\n  if not isinstance(lst, list) and not isinstance(lst, tuple):\n    raise NotAListError('%s must be a list' % key)\n\n  # each list entry must be a string\n  for element in lst:\n    if not isinstance(element, str):\n      raise ElementNotAStringError('Unsupported list element %s found in %s',\n                                   (element, lst))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _StripCommonPathPrefix(paths):\n  # Find the longest common prefix in terms of characters.\n  common_prefix = os.path.commonprefix(paths)\n  # Truncate at last segment boundary. E.g. '/aa/bb1/x.py' and '/a/bb2/x.py'\n  # have '/aa/bb' as the common prefix, but we should strip '/aa/' instead.\n  # If there's no '/' found, returns -1+1=0.\n  common_prefix_len = common_prefix.rfind('/') + 1\n  return [path[common_prefix_len:] for path in paths]", "response": "Removes the common prefix from a list of path strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate an error message to be used when multiple modules are found.", "response": "def _MultipleModulesFoundError(path, candidates):\n  \"\"\"Generates an error message to be used when multiple matches are found.\n\n  Args:\n    path: The breakpoint location path that the user provided.\n    candidates: List of paths that match the user provided path. Must\n        contain at least 2 entries (throws AssertionError otherwise).\n\n  Returns:\n    A (format, parameters) tuple that should be used in the description\n    field of the breakpoint error status.\n  \"\"\"\n  assert len(candidates) > 1\n  params = [path] + _StripCommonPathPrefix(candidates[:2])\n  if len(candidates) == 2:\n    fmt = ERROR_LOCATION_MULTIPLE_MODULES_3\n  else:\n    fmt = ERROR_LOCATION_MULTIPLE_MODULES_4\n    params.append(str(len(candidates) - 2))\n  return fmt, params"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves surrounding whitespace leading separator and normalize.", "response": "def _NormalizePath(path):\n  \"\"\"Removes surrounding whitespace, leading separator and normalize.\"\"\"\n  # TODO(emrekultursay): Calling os.path.normpath \"may change the meaning of a\n  # path that contains symbolic links\" (e.g., \"A/foo/../B\" != \"A/B\" if foo is a\n  # symlink). This might cause trouble when matching against loaded module\n  # paths. We should try to avoid using it.\n  # Example:\n  #  > import symlink.a\n  #  > symlink.a.__file__\n  #  symlink/a.py\n  #  > import target.a\n  #  > starget.a.__file__\n  #  target/a.py\n  # Python interpreter treats these as two separate modules. So, we also need to\n  # handle them the same way.\n  return os.path.normpath(path.strip().lstrip(os.sep))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Clear(self):\n    self._RemoveImportHook()\n    if self._cookie is not None:\n      native.LogInfo('Clearing breakpoint %s' % self.GetBreakpointId())\n      native.ClearConditionalBreakpoint(self._cookie)\n      self._cookie = None\n\n    self._completed = True", "response": "Clears the breakpoint and releases all resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetExpirationTime(self):\n    # TODO(emrekultursay): Move this to a common method.\n    if '.' not in self.definition['createTime']:\n      fmt = '%Y-%m-%dT%H:%M:%S%Z'\n    else:\n      fmt = '%Y-%m-%dT%H:%M:%S.%f%Z'\n\n    create_datetime = datetime.strptime(\n        self.definition['createTime'].replace('Z', 'UTC'), fmt)\n    return create_datetime + self.expiration_period", "response": "Computes the timestamp at which this breakpoint will expire."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ActivateBreakpoint(self, module):\n\n    # First remove the import hook (if installed).\n    self._RemoveImportHook()\n\n    line = self.definition['location']['line']\n\n    # Find the code object in which the breakpoint is being set.\n    status, codeobj = module_explorer.GetCodeObjectAtLine(module, line)\n    if not status:\n      # First two parameters are common: the line of the breakpoint and the\n      # module we are trying to insert the breakpoint in.\n      # TODO(emrekultursay): Do not display the entire path of the file. Either\n      # strip some prefix, or display the path in the breakpoint.\n      params = [str(line), os.path.splitext(module.__file__)[0] + '.py']\n\n      # The next 0, 1, or 2 parameters are the alternative lines to set the\n      # breakpoint at, displayed for the user's convenience.\n      alt_lines = (str(l) for l in codeobj if l is not None)\n      params += alt_lines\n\n      if len(params) == 4:\n        fmt = ERROR_LOCATION_NO_CODE_FOUND_AT_LINE_4\n      elif len(params) == 3:\n        fmt = ERROR_LOCATION_NO_CODE_FOUND_AT_LINE_3\n      else:\n        fmt = ERROR_LOCATION_NO_CODE_FOUND_AT_LINE_2\n\n      self._CompleteBreakpoint({\n          'status': {\n              'isError': True,\n              'refersTo': 'BREAKPOINT_SOURCE_LOCATION',\n              'description': {\n                  'format': fmt,\n                  'parameters': params}}})\n      return\n\n    # Compile the breakpoint condition.\n    condition = None\n    if self.definition.get('condition'):\n      try:\n        condition = compile(self.definition.get('condition'),\n                            '<condition_expression>',\n                            'eval')\n      except (TypeError, ValueError) as e:\n        # condition string contains null bytes.\n        self._CompleteBreakpoint({\n            'status': {\n                'isError': True,\n                'refersTo': 'BREAKPOINT_CONDITION',\n                'description': {\n                    'format': 'Invalid expression',\n                    'parameters': [str(e)]}}})\n        return\n\n      except SyntaxError as e:\n        self._CompleteBreakpoint({\n            'status': {\n                'isError': True,\n                'refersTo': 'BREAKPOINT_CONDITION',\n                'description': {\n                    'format': 'Expression could not be compiled: $0',\n                    'parameters': [e.msg]}}})\n        return\n\n    native.LogInfo('Creating new Python breakpoint %s in %s, line %d' % (\n        self.GetBreakpointId(), codeobj, line))\n\n    self._cookie = native.SetConditionalBreakpoint(\n        codeobj,\n        line,\n        condition,\n        self._BreakpointEvent)", "response": "Activate the breakpoint in the loaded module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a breakpoint update and deactivates the breakpoint.", "response": "def _CompleteBreakpoint(self, data, is_incremental=True):\n    \"\"\"Sends breakpoint update and deactivates the breakpoint.\"\"\"\n    if is_incremental:\n      data = dict(self.definition, **data)\n    data['isFinalState'] = True\n\n    self._hub_client.EnqueueBreakpointUpdate(data)\n    self._breakpoints_manager.CompleteBreakpoint(self.GetBreakpointId())\n    self.Clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the breakpoint as completed.", "response": "def _SetCompleted(self):\n    \"\"\"Atomically marks the breakpoint as completed.\n\n    Returns:\n      True if the breakpoint wasn't marked already completed or False if the\n      breakpoint was already completed.\n    \"\"\"\n    with self._lock:\n      if self._completed:\n        return False\n      self._completed = True\n      return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _BreakpointEvent(self, event, frame):\n    error_status = None\n\n    if event != native.BREAKPOINT_EVENT_HIT:\n      error_status = _BREAKPOINT_EVENT_STATUS[event]\n    elif self.definition.get('action') == 'LOG':\n      error_status = self._collector.Log(frame)\n      if not error_status:\n        return  # Log action successful, no need to clear the breakpoint.\n\n    # Let only one thread capture the data and complete the breakpoint.\n    if not self._SetCompleted():\n      return\n\n    self.Clear()\n\n    if error_status:\n      self._CompleteBreakpoint({'status': error_status})\n      return\n\n    collector = capture_collector.CaptureCollector(\n        self.definition, self.data_visibility_policy)\n\n    # TODO(b/69119299): This is a temporary try/except. All exceptions should be\n    # caught inside Collect and converted into breakpoint error messages.\n    try:\n      collector.Collect(frame)\n    except BaseException as e:  # pylint: disable=broad-except\n      native.LogInfo('Internal error during data capture: %s' % repr(e))\n      error_status = {'isError': True,\n                      'description': {\n                          'format': ('Internal error while capturing data: %s' %\n                                     repr(e))}}\n      self._CompleteBreakpoint({'status': error_status})\n      return\n    except:  # pylint: disable=bare-except\n      native.LogInfo('Unknown exception raised')\n      error_status = {'isError': True,\n                      'description': {\n                          'format': 'Unknown internal error'}}\n      self._CompleteBreakpoint({'status': error_status})\n      return\n\n    self._CompleteBreakpoint(collector.breakpoint, is_incremental=False)", "response": "Callback invoked by cdbg_native when a breakpoint hits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch sys. path to find a source file that matches path.", "response": "def Search(path):\n  \"\"\"Search sys.path to find a source file that matches path.\n\n  The provided input path may have an unknown number of irrelevant outer\n  directories (e.g., /garbage1/garbage2/real1/real2/x.py').  This function\n  does multiple search iterations until an actual Python module file that\n  matches the input path is found. At each iteration, it strips one leading\n  directory from the path and searches the directories at sys.path\n  for a match.\n\n  Examples:\n    sys.path: ['/x1/x2', '/y1/y2']\n    Search order: [.pyo|.pyc|.py]\n      /x1/x2/a/b/c\n      /x1/x2/b/c\n      /x1/x2/c\n      /y1/y2/a/b/c\n      /y1/y2/b/c\n      /y1/y2/c\n    Filesystem: ['/y1/y2/a/b/c.pyc']\n\n    1) Search('a/b/c.py')\n         Returns '/y1/y2/a/b/c.pyc'\n    2) Search('q/w/a/b/c.py')\n         Returns '/y1/y2/a/b/c.pyc'\n    3) Search('q/w/c.py')\n         Returns 'q/w/c.py'\n\n    The provided input path may also be relative to an unknown directory.\n    The path may include some or all outer package names.\n\n  Examples (continued):\n\n    4) Search('c.py')\n         Returns 'c.py'\n    5) Search('b/c.py')\n         Returns 'b/c.py'\n\n  Args:\n    path: Path that describes a source file. Must contain .py file extension.\n          Must not contain any leading os.sep character.\n\n  Returns:\n    Full path to the matched source file, if a match is found. Otherwise,\n    returns the input path.\n\n  Raises:\n    AssertionError: if the provided path is an absolute path, or if it does not\n      have a .py extension.\n  \"\"\"\n  def SearchCandidates(p):\n    \"\"\"Generates all candidates for the fuzzy search of p.\"\"\"\n    while p:\n      yield p\n      (_, _, p) = p.partition(os.sep)\n\n  # Verify that the os.sep is already stripped from the input.\n  assert not path.startswith(os.sep)\n\n  # Strip the file extension, it will not be needed.\n  src_root, src_ext = os.path.splitext(path)\n  assert src_ext == '.py'\n\n  # Search longer suffixes first. Move to shorter suffixes only if longer\n  # suffixes do not result in any matches.\n  for src_part in SearchCandidates(src_root):\n    # Search is done in sys.path order, which gives higher priority to earlier\n    # entries in sys.path list.\n    for sys_path in sys.path:\n      f = os.path.join(sys_path, src_part)\n      # The order in which we search the extensions does not matter.\n      for ext in ('.pyo', '.pyc', '.py'):\n        # The os.path.exists check internally follows symlinks and flattens\n        # relative paths, so we don't have to deal with it.\n        fext = f + ext\n        if os.path.exists(fext):\n          # Once we identify a matching file in the filesystem, we should\n          # preserve the (1) potentially-symlinked and (2)\n          # potentially-non-flattened file path (f+ext), because that's exactly\n          # how we expect it to appear in sys.modules when we search the file\n          # there.\n          return fext\n\n  # A matching file was not found in sys.path directories.\n  return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure and starts the debugger.", "response": "def _StartDebugger():\n  \"\"\"Configures and starts the debugger.\"\"\"\n  global _hub_client\n  global _breakpoints_manager\n\n  cdbg_native.InitializeModule(_flags)\n\n  _hub_client = gcp_hub_client.GcpHubClient()\n  visibility_policy = _GetVisibilityPolicy()\n\n  _breakpoints_manager = breakpoints_manager.BreakpointsManager(\n      _hub_client,\n      visibility_policy)\n\n  # Set up loggers for logpoints.\n  capture_collector.SetLogger(logging.getLogger())\n\n  capture_collector.CaptureCollector.pretty_printers.append(\n      appengine_pretty_printers.PrettyPrinter)\n\n  _hub_client.on_active_breakpoints_changed = (\n      _breakpoints_manager.SetActiveBreakpoints)\n  _hub_client.on_idle = _breakpoints_manager.CheckBreakpointsExpiration\n  _hub_client.SetupAuth(\n      _flags.get('project_id'),\n      _flags.get('project_number'),\n      _flags.get('service_account_json_file'))\n  _hub_client.InitializeDebuggeeLabels(_flags)\n  _hub_client.Start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetVisibilityPolicy():\n  try:\n    visibility_config = yaml_data_visibility_config_reader.OpenAndRead()\n  except yaml_data_visibility_config_reader.Error as err:\n    return error_data_visibility_policy.ErrorDataVisibilityPolicy(\n        'Could not process debugger config: %s' % err)\n\n  if visibility_config:\n    return glob_data_visibility_policy.GlobDataVisibilityPolicy(\n        visibility_config.blacklist_patterns,\n        visibility_config.whitelist_patterns)\n\n  return None", "response": "Returns a visibility policy for the current debugger."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the debugger and runs the application with debugger attached.", "response": "def _DebuggerMain():\n  \"\"\"Starts the debugger and runs the application with debugger attached.\"\"\"\n  global _flags\n\n  # The first argument is cdbg module, which we don't care.\n  del sys.argv[0]\n\n  # Parse debugger flags until we encounter '--'.\n  _flags = {}\n  while sys.argv[0]:\n    arg = sys.argv[0]\n    del sys.argv[0]\n\n    if arg == '--':\n      break\n\n    (name, value) = arg.strip('-').split('=', 2)\n    _flags[name] = value\n\n  _StartDebugger()\n\n  # Run the app. The following code was mostly copied from pdb.py.\n  app_path = sys.argv[0]\n\n  sys.path[0] = os.path.dirname(app_path)\n\n  import __main__  # pylint: disable=g-import-not-at-top\n  __main__.__dict__.clear()\n  __main__.__dict__.update({'__name__': '__main__',\n                            '__file__': app_path,\n                            '__builtins__': __builtins__})\n  locals = globals = __main__.__dict__  # pylint: disable=redefined-builtin\n\n  sys.modules['__main__'] = __main__\n\n  with open(app_path) as f:\n    code = compile(f.read(), app_path, 'exec')\n    exec(code, globals, locals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _Matches(path, pattern_list):\n  # Note: This code does not scale to large pattern_list sizes.\n  return any(fnmatch.fnmatchcase(path, pattern) for pattern in pattern_list)", "response": "Returns True if path matches any patten found in pattern_list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef IsDataVisible(self, path):\n    if path is None:\n      return (False, RESPONSES['UNKNOWN_TYPE'])\n\n    if _Matches(path, self.blacklist_patterns):\n      return (False, RESPONSES['BLACKLISTED'])\n\n    if not _Matches(path, self.whitelist_patterns):\n      return (False, RESPONSES['NOT_WHITELISTED'])\n\n    return (True, RESPONSES['VISIBLE'])", "response": "Returns a tuple that stating if the data should be visible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the active breakpoints and removes missing ones.", "response": "def SetActiveBreakpoints(self, breakpoints_data):\n    \"\"\"Adds new breakpoints and removes missing ones.\n\n    Args:\n      breakpoints_data: updated list of active breakpoints.\n    \"\"\"\n    with self._lock:\n      ids = set([x['id'] for x in breakpoints_data])\n\n      # Clear breakpoints that no longer show up in active breakpoints list.\n      for breakpoint_id in six.viewkeys(self._active) - ids:\n        self._active.pop(breakpoint_id).Clear()\n\n      # Create new breakpoints.\n      self._active.update([\n          (x['id'],\n           python_breakpoint.PythonBreakpoint(\n               x,\n               self._hub_client,\n               self,\n               self.data_visibility_policy))\n          for x in breakpoints_data\n          if x['id'] in ids - six.viewkeys(self._active) - self._completed])\n\n      # Remove entries from completed_breakpoints_ that weren't listed in\n      # breakpoints_data vector. These are confirmed to have been removed by the\n      # hub and the debuglet can now assume that they will never show up ever\n      # again. The backend never reuses breakpoint IDs.\n      self._completed &= ids\n\n      if self._active:\n        self._next_expiration = datetime.min  # Not known.\n      else:\n        self._next_expiration = datetime.max"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks the specified breakpoint as completed.", "response": "def CompleteBreakpoint(self, breakpoint_id):\n    \"\"\"Marks the specified breaking as completed.\n\n    Appends the ID to set of completed breakpoints and clears it.\n\n    Args:\n      breakpoint_id: breakpoint ID to complete.\n    \"\"\"\n    with self._lock:\n      self._completed.add(breakpoint_id)\n      if breakpoint_id in self._active:\n        self._active.pop(breakpoint_id).Clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CheckBreakpointsExpiration(self):\n    with self._lock:\n      current_time = BreakpointsManager.GetCurrentTime()\n      if self._next_expiration > current_time:\n        return\n\n      expired_breakpoints = []\n      self._next_expiration = datetime.max\n      for breakpoint in six.itervalues(self._active):\n        expiration_time = breakpoint.GetExpirationTime()\n        if expiration_time <= current_time:\n          expired_breakpoints.append(breakpoint)\n        else:\n          self._next_expiration = min(self._next_expiration, expiration_time)\n\n    for breakpoint in expired_breakpoints:\n      breakpoint.ExpireBreakpoint()", "response": "Completes all breakpoints that have been active for too long."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef IsPathSuffix(mod_path, path):\n  return (mod_path.endswith(path) and\n          (len(mod_path) == len(path) or\n           mod_path[:-len(path)].endswith(os.sep)))", "response": "Checks whether a path is a full path suffix of a source file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching sys. modules to find a module that corresponds to the given path.", "response": "def GetLoadedModuleBySuffix(path):\n  \"\"\"Searches sys.modules to find a module with the given file path.\n\n  Args:\n    path: Path to the source file. It can be relative or absolute, as suffix\n          match can handle both. If absolute, it must have already been\n          sanitized.\n\n  Algorithm:\n    The given path must be a full suffix of a loaded module to be a valid match.\n    File extensions are ignored when performing suffix match.\n\n  Example:\n    path: 'a/b/c.py'\n    modules: {'a': 'a.py', 'a.b': 'a/b.py', 'a.b.c': 'a/b/c.pyc']\n    returns: module('a.b.c')\n\n  Returns:\n    The module that corresponds to path, or None if such module was not\n    found.\n  \"\"\"\n  root = os.path.splitext(path)[0]\n  for module in sys.modules.values():\n    mod_root = os.path.splitext(getattr(module, '__file__', None) or '')[0]\n\n    if not mod_root:\n      continue\n\n    # While mod_root can contain symlinks, we cannot eliminate them. This is\n    # because, we must perform exactly the same transformations on mod_root and\n    # path, yet path can be relative to an unknown directory which prevents\n    # identifying and eliminating symbolic links.\n    #\n    # Therefore, we only convert relative to absolute path.\n    if not os.path.isabs(mod_root):\n      mod_root = os.path.join(os.getcwd(), mod_root)\n\n    if IsPathSuffix(mod_root, root):\n      return module\n\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for a code object in the specified line in the specified module.", "response": "def GetCodeObjectAtLine(module, line):\n  \"\"\"Searches for a code object at the specified line in the specified module.\n\n  Args:\n    module: module to explore.\n    line: 1-based line number of the statement.\n\n  Returns:\n    (True, Code object) on success or (False, (prev_line, next_line)) on\n    failure, where prev_line and next_line are the closest lines with code above\n    and below the specified line, or None if they do not exist.\n  \"\"\"\n  if not hasattr(module, '__file__'):\n    return (False, (None, None))\n\n  prev_line = 0\n  next_line = six.MAXSIZE\n\n  for code_object in _GetModuleCodeObjects(module):\n    for co_line_number in _GetLineNumbers(code_object):\n      if co_line_number == line:\n        return (True, code_object)\n      elif co_line_number < line:\n        prev_line = max(prev_line, co_line_number)\n      elif co_line_number > line:\n        next_line = min(next_line, co_line_number)\n        break\n\n  prev_line = None if prev_line == 0 else prev_line\n  next_line = None if next_line == six.MAXSIZE else next_line\n  return (False, (prev_line, next_line))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _GetModuleCodeObjects(module):\n\n  visit_recorder = _VisitRecorder()\n  current = [module]\n  code_objects = set()\n  while current:\n    current = _FindCodeObjectsReferents(module, current, visit_recorder)\n    code_objects |= current\n\n    # Unfortunately Python code objects don't implement tp_traverse, so this\n    # type can't be used with gc.get_referents. The workaround is to get the\n    # relevant objects explicitly here.\n    current = [code_object.co_consts for code_object in current]\n\n  return code_objects", "response": "Returns a set of all code objects defined in the specified module."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlooks for all the code objects referenced by objects in start_objects. The traversal implemented by this function is a shallow one. In other words if the reference chain is a -> b -> co1 -> c -> co2, this function will return [co1] only. The traversal is implemented with BFS. The maximum depth is limited to avoid touching all the objects in the process. Each object is only visited once using visit_recorder. Args: module: module in which we are looking for code objects. start_objects: initial set of objects for the BFS traversal. visit_recorder: instance of _VisitRecorder class to ensure each object is visited at most once. Returns: List of code objects.", "response": "def _FindCodeObjectsReferents(module, start_objects, visit_recorder):\n  \"\"\"Looks for all the code objects referenced by objects in start_objects.\n\n  The traversal implemented by this function is a shallow one. In other words\n  if the reference chain is a -> b -> co1 -> c -> co2, this function will\n  return [co1] only.\n\n  The traversal is implemented with BFS. The maximum depth is limited to avoid\n  touching all the objects in the process. Each object is only visited once\n  using visit_recorder.\n\n  Args:\n    module: module in which we are looking for code objects.\n    start_objects: initial set of objects for the BFS traversal.\n    visit_recorder: instance of _VisitRecorder class to ensure each object is\n        visited at most once.\n\n  Returns:\n    List of code objects.\n  \"\"\"\n  def CheckIgnoreCodeObject(code_object):\n    \"\"\"Checks if the code object can be ignored.\n\n    Code objects that are not implemented in the module, or are from a lambda or\n    generator expression can be ignored.\n\n    If the module was precompiled, the code object may point to .py file, while\n    the module says that it originated from .pyc file. We just strip extension\n    altogether to work around it.\n\n    Args:\n      code_object: code object that we want to check against module.\n\n    Returns:\n      True if the code object can be ignored, False otherwise.\n    \"\"\"\n    if code_object.co_name in ('<lambda>', '<genexpr>'):\n      return True\n\n    code_object_file = os.path.splitext(code_object.co_filename)[0]\n    module_file = os.path.splitext(module.__file__)[0]\n\n    # The simple case.\n    if code_object_file == module_file:\n      return False\n\n    return True\n\n  def CheckIgnoreClass(cls):\n    \"\"\"Returns True if the class is definitely not coming from \"module\".\"\"\"\n    cls_module = sys.modules.get(cls.__module__)\n    if not cls_module:\n      return False  # We can't tell for sure, so explore this class.\n\n    return (\n        cls_module is not module and\n        getattr(cls_module, '__file__', None) != module.__file__)\n\n  code_objects = set()\n  current = start_objects\n  for obj in current:\n    visit_recorder.Record(current)\n\n  depth = 0\n  while current and depth < _MAX_REFERENTS_BFS_DEPTH:\n    new_current = []\n    for current_obj in current:\n      referents = gc.get_referents(current_obj)\n      if (current_obj is not module.__dict__ and\n          len(referents) > _MAX_OBJECT_REFERENTS):\n        continue\n\n      for obj in referents:\n        if isinstance(obj, _BFS_IGNORE_TYPES) or not visit_recorder.Record(obj):\n          continue\n\n        if isinstance(obj, types.CodeType) and CheckIgnoreCodeObject(obj):\n          continue\n\n        if isinstance(obj, six.class_types) and CheckIgnoreClass(obj):\n          continue\n\n        if isinstance(obj, types.CodeType):\n          code_objects.add(obj)\n        else:\n          new_current.append(obj)\n\n    current = new_current\n    depth += 1\n\n  return code_objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrecording the object as visited.", "response": "def Record(self, obj):\n    \"\"\"Records the object as visited.\n\n    Args:\n      obj: visited object.\n\n    Returns:\n      True if the object hasn't been previously visited or False if it has\n      already been recorded or the quota has been exhausted.\n    \"\"\"\n    if len(self._visit_recorder_objects) >= _MAX_VISIT_OBJECTS:\n      return False\n\n    obj_id = id(obj)\n    if obj_id in self._visit_recorder_objects:\n      return False\n\n    self._visit_recorder_objects[obj_id] = obj\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Failed(self):\n    interval = self._current_interval_sec\n    self._current_interval_sec = min(\n        self.max_interval_sec, self._current_interval_sec * self.multiplier)\n    return interval", "response": "Indicates that a request has failed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing application uniquifier for a particular application.", "response": "def ComputeApplicationUniquifier(hash_obj):\n  \"\"\"Computes hash of application files.\n\n  Application files can be anywhere on the disk. The application is free to\n  import a Python module from an arbitrary path ok the disk. It is also\n  impossible to distinguish application files from third party libraries.\n  Third party libraries are typically installed with \"pip\" and there is not a\n  good way to guarantee that all instances of the application are going to have\n  exactly the same version of each package. There is also a huge amount of files\n  in all sys.path directories and it will take too much time to traverse them\n  all. We therefore make an assumption that application files are only located\n  in sys.path[0].\n\n  When traversing files in sys.path, we can expect both .py and .pyc files. For\n  source deployment, we will find both .py and .pyc files. In this case we will\n  only index .py files and ignored .pyc file. In case of binary deployment, only\n  .pyc file will be there.\n\n  The naive way to hash files would be to read the file content and compute some\n  sort of a hash (e.g. SHA1). This can be expensive as well, so instead we just\n  hash file name and file size. It is a good enough heuristics to identify\n  modified files across different deployments.\n\n  Args:\n    hash_obj: hash aggregator to update with application uniquifier.\n  \"\"\"\n\n  def ProcessDirectory(path, relative_path, depth=1):\n    \"\"\"Recursively computes application uniquifier for a particular directory.\n\n    Args:\n      path: absolute path of the directory to start.\n      relative_path: path relative to sys.path[0]\n      depth: current recursion depth.\n    \"\"\"\n\n    if depth > _MAX_DEPTH:\n      return\n\n    try:\n      names = os.listdir(path)\n    except BaseException:\n      return\n\n    # Sort file names to ensure consistent hash regardless of order returned\n    # by os.listdir. This will also put .py files before .pyc and .pyo files.\n    modules = set()\n    for name in sorted(names):\n      current_path = os.path.join(path, name)\n      if not os.path.isdir(current_path):\n        file_name, ext = os.path.splitext(name)\n        if ext not in ('.py', '.pyc', '.pyo'):\n          continue  # This is not an application file.\n        if file_name in modules:\n          continue  # This is a .pyc file and we already indexed .py file.\n\n        modules.add(file_name)\n        ProcessApplicationFile(current_path, os.path.join(relative_path, name))\n      elif IsPackage(current_path):\n        ProcessDirectory(current_path,\n                         os.path.join(relative_path, name),\n                         depth + 1)\n\n  def IsPackage(path):\n    \"\"\"Checks if the specified directory is a valid Python package.\"\"\"\n    init_base_path = os.path.join(path, '__init__.py')\n    return (os.path.isfile(init_base_path) or\n            os.path.isfile(init_base_path + 'c') or\n            os.path.isfile(init_base_path + 'o'))\n\n  def ProcessApplicationFile(path, relative_path):\n    \"\"\"Updates the hash with the specified application file.\"\"\"\n    hash_obj.update(relative_path.encode())\n    hash_obj.update(':'.encode())\n    try:\n      hash_obj.update(str(os.stat(path).st_size).encode())\n    except BaseException:\n      pass\n    hash_obj.update('\\n'.encode())\n\n  ProcessDirectory(sys.path[0], '')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a callback to be invoked when a module with the given path is imported.", "response": "def AddImportCallbackBySuffix(path, callback):\n  \"\"\"Register import hook.\n\n  This function overrides the default import process. Then whenever a module\n  whose suffix matches path is imported, the callback will be invoked.\n\n  A module may be imported multiple times. Import event only means that the\n  Python code contained an \"import\" statement. The actual loading and\n  initialization of a new module normally happens only once, at which time\n  the callback will be invoked. This function does not validates the existence\n  of such a module and it's the responsibility of the caller.\n\n  TODO(erezh): handle module reload.\n\n  Args:\n    path: python module file path. It may be missing the directories for the\n          outer packages, and therefore, requires suffix comparison to match\n          against loaded modules. If it contains all outer packages, it may\n          contain the sys.path as well.\n          It might contain an incorrect file extension (e.g., py vs. pyc).\n    callback: callable to invoke upon module load.\n\n  Returns:\n    Function object to invoke to remove the installed callback.\n  \"\"\"\n\n  def RemoveCallback():\n    # This is a read-if-del operation on _import_callbacks. Lock to prevent\n    # callbacks from being inserted just before the key is deleted. Thus, it\n    # must be locked also when inserting a new entry below. On the other hand\n    # read only access, in the import hook, does not require a lock.\n    with _import_callbacks_lock:\n      callbacks = _import_callbacks.get(path)\n      if callbacks:\n        callbacks.remove(callback)\n        if not callbacks:\n          del _import_callbacks[path]\n\n  with _import_callbacks_lock:\n    _import_callbacks.setdefault(path, set()).add(callback)\n  _InstallImportHookBySuffix()\n\n  return RemoveCallback"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _InstallImportHookBySuffix():\n  global _real_import\n\n  if _real_import:\n    return  # Import hook already installed\n\n  _real_import = getattr(builtins, '__import__')\n  assert _real_import\n  builtins.__import__ = _ImportHookBySuffix\n\n  if six.PY3:\n    # In Python 2, importlib.import_module calls __import__ internally so\n    # overriding __import__ is enough. In Python 3, they are separate so it also\n    # needs to be overwritten.\n    global _real_import_module\n    _real_import_module = importlib.import_module\n    assert _real_import_module\n    importlib.import_module = _ImportModuleHookBySuffix", "response": "Lazily installs import hook."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrement the per - thread nest level of imports.", "response": "def _IncrementNestLevel():\n  \"\"\"Increments the per thread nest level of imports.\"\"\"\n  # This is the top call to import (no nesting), init the per-thread nest level\n  # and names set.\n  if getattr(_import_local, 'nest_level', None) is None:\n    _import_local.nest_level = 0\n\n  if _import_local.nest_level == 0:\n    # Re-initialize names set at each top-level import to prevent any\n    # accidental unforeseen memory leak.\n    _import_local.names = set()\n\n  _import_local.nest_level += 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ProcessImportBySuffix(name, fromlist, globals):\n  _import_local.nest_level -= 1\n\n  # To improve common code path performance, compute the loaded modules only\n  # if there are any import callbacks.\n  if _import_callbacks:\n    # Collect the names of all modules that might be newly loaded as a result\n    # of this import. Add them in a thread-local list.\n    _import_local.names |= _GenerateNames(name, fromlist, globals)\n\n    # Invoke the callbacks only on the top-level import call.\n    if _import_local.nest_level == 0:\n      _InvokeImportCallbackBySuffix(_import_local.names)\n\n  # To be safe, we clear the names set every time we exit a top level import.\n  if _import_local.nest_level == 0:\n    _import_local.names.clear()", "response": "Processes an import by the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves a relative import into an absolute path.", "response": "def _ResolveRelativeImport(name, package):\n  \"\"\"Resolves a relative import into an absolute path.\n\n  This is mostly an adapted version of the logic found in the backported\n  version of import_module in Python 2.7.\n  https://github.com/python/cpython/blob/2.7/Lib/importlib/__init__.py\n\n  Args:\n    name: relative name imported, such as '.a' or '..b.c'\n    package: absolute package path, such as 'a.b.c.d.e'\n\n  Returns:\n    The absolute path of the name to be imported, or None if it is invalid.\n    Examples:\n      _ResolveRelativeImport('.c', 'a.b') -> 'a.b.c'\n      _ResolveRelativeImport('..c', 'a.b') -> 'a.c'\n      _ResolveRelativeImport('...c', 'a.c') -> None\n  \"\"\"\n  level = sum(1 for c in itertools.takewhile(lambda c: c == '.', name))\n  if level == 1:\n    return package + name\n  else:\n    parts = package.split('.')[:-(level - 1)]\n    if not parts:\n      return None\n    parts.append(name[level:])\n    return '.'.join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ImportModuleHookBySuffix(name, package=None):\n  _IncrementNestLevel()\n\n  try:\n    # Really import modules.\n    module = _real_import_module(name, package)\n  finally:\n    if name.startswith('.'):\n      if package:\n        name = _ResolveRelativeImport(name, package)\n      else:\n        # Should not happen. Relative imports require the package argument.\n        name = None\n    if name:\n      _ProcessImportBySuffix(name, None, None)\n\n  return module", "response": "Callback when a module is imported through importlib. import_module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the names of modules that might be loaded via this import statement.", "response": "def _GenerateNames(name, fromlist, globals):\n  \"\"\"Generates the names of modules that might be loaded via this import.\n\n  Args:\n    name: Argument as passed to the importer.\n    fromlist: Argument as passed to the importer.\n    globals: Argument as passed to the importer.\n\n  Returns:\n    A set that contains the names of all modules that are loaded by the\n    currently executing import statement, as they would show up in sys.modules.\n    The returned set may contain module names that were already loaded before\n    the execution of this import statement.\n    The returned set may contain names that are not real modules.\n  \"\"\"\n  def GetCurrentPackage(globals):\n    \"\"\"Finds the name of the package for the currently executing module.\"\"\"\n    if not globals:\n      return None\n\n    # Get the name of the module/package that the current import is being\n    # executed in.\n    current = globals.get('__name__')\n    if not current:\n      return None\n\n    # Check if the current module is really a module, or a package.\n    current_file = globals.get('__file__')\n    if not current_file:\n      return None\n\n    root = os.path.splitext(os.path.basename(current_file))[0]\n    if root == '__init__':\n      # The current import happened from a package. Return the package.\n      return current\n    else:\n      # The current import happened from a module. Return the package that\n      # contains the module.\n      return current.rpartition('.')[0]\n\n  # A Python module can be addressed in two ways:\n  #   1. Using a path relative to the currently executing module's path. For\n  #   instance, module p1/p2/m3.py imports p1/p2/p3/m4.py using 'import p3.m4'.\n  #   2. Using a path relative to sys.path. For instance, module p1/p2/m3.py\n  #   imports p1/p2/p3/m4.py using 'import p1.p2.p3.m4'.\n  #\n  # The Python importer uses the 'globals' argument to identify the module that\n  # the current import is being performed in. The actual logic is very\n  # complicated, and we only approximate it here to limit the performance\n  # overhead (See import.c in the interpreter for details). Here, we only use\n  # the value of the globals['__name__'] for this purpose.\n  #\n  # Note: The Python importer prioritizes the current package over sys.path. For\n  # instance, if 'p1.p2.m3' imports 'm4', then 'p1.p2.m4' is a better match than\n  # the top level 'm4'. However, the debugger does not have to implement this,\n  # because breakpoint paths are not described relative to some other file. They\n  # are always assumed to be relative to the sys.path directories. If the user\n  # sets breakpoint inside 'm4.py', then we can map it to either the top level\n  # 'm4' or 'p1.p2.m4', i.e., both are valid matches.\n  curpkg = GetCurrentPackage(globals)\n\n  names = set()\n\n  # A Python module can be imported using two syntaxes:\n  #   1. import p1.p2.m3\n  #   2. from p1.p2 import m3\n  #\n  # When the regular 'import p1.p2.m3' syntax is used, the name of the module\n  # being imported is passed in the 'name' argument (e.g., name='p1.p2.m3',\n  # fromlist=None).\n  #\n  # When the from-import syntax is used, then fromlist contains the leaf names\n  # of the modules, and name contains the containing package. For instance, if\n  # name='a.b', fromlist=['c', 'd'], then we add ['a.b.c', 'a.b.d'].\n  #\n  # Corner cases:\n  #   1. The fromlist syntax can be used to import a function from a module.\n  #      For instance, 'from p1.p2.m3 import func'.\n  #   2. Sometimes, the importer is passed a dummy fromlist=['__doc__'] (see\n  #      import.c in the interpreter for details).\n  # Due to these corner cases, the returned set may contain entries that are not\n  # names of real modules.\n  for from_entry in fromlist or []:\n    # Name relative to sys.path.\n    # For relative imports such as 'from . import x', name will be the empty\n    # string. Thus we should not prepend a '.' to the entry.\n    entry = (name + '.' + from_entry) if name else from_entry\n    names.add(entry)\n    # Name relative to the currently executing module's package.\n    if curpkg:\n      names.add(curpkg + '.' + entry)\n\n  # Generate all names from name. For instance, if name='a.b.c', then\n  # we need to add ['a.b.c', 'a.b', 'a'].\n  while name:\n    # Name relative to sys.path.\n    names.add(name)\n    # Name relative to currently executing module's package.\n    if curpkg:\n      names.add(curpkg + '.' + name)\n    name = name.rpartition('.')[0]\n\n  return names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvoke import callbacks for newly loaded modules. Uses a path suffix match to identify whether a loaded module matches the file path provided by the user. Args: names: A set of names for modules that are loaded by the current import. The set may contain some superfluous entries that were already loaded before this import, or some entries that do not correspond to a module. The list is expected to be much smaller than the exact sys.modules so that a linear search is not as costly.", "response": "def _InvokeImportCallbackBySuffix(names):\n  \"\"\"Invokes import callbacks for newly loaded modules.\n\n  Uses a path suffix match to identify whether a loaded module matches the\n  file path provided by the user.\n\n  Args:\n    names: A set of names for modules that are loaded by the current import.\n           The set may contain some superfluous entries that were already\n           loaded before this import, or some entries that do not correspond\n           to a module. The list is expected to be much smaller than the exact\n           sys.modules so that a linear search is not as costly.\n  \"\"\"\n  def GetModuleFromName(name, path):\n    \"\"\"Returns the loaded module for this name/path, or None if not found.\n\n    Args:\n      name: A string that may represent the name of a loaded Python module.\n      path: If 'name' ends with '.*', then the last path component in 'path' is\n            used to identify what the wildcard may map to. Does not contain file\n            extension.\n\n    Returns:\n      The loaded module for the given name and path, or None if a loaded module\n      was not found.\n    \"\"\"\n    # The from-import syntax can be used as 'from p1.p2 import *'. In this case,\n    # we cannot know what modules will match the wildcard. However, we know that\n    # the wildcard can only be used to import leaf modules. So, we guess that\n    # the leaf module will have the same name as the leaf file name the user\n    # provided. For instance,\n    #   User input path = 'foo.py'\n    #   Currently executing import:\n    #     from pkg1.pkg2 import *\n    #   Then, we combine:\n    #      1. 'pkg1.pkg2' from import's outer package and\n    #      2. Add 'foo' as our guess for the leaf module name.\n    #   So, we will search for modules with name similar to 'pkg1.pkg2.foo'.\n    if name.endswith('.*'):\n      # Replace the final '*' with the name of the module we are looking for.\n      name = name.rpartition('.')[0] + '.' + path.split('/')[-1]\n\n    # Check if the module was loaded.\n    return sys.modules.get(name)\n\n  # _import_callbacks might change during iteration because RemoveCallback()\n  # might delete items. Iterate over a copy to avoid a\n  # 'dictionary changed size during iteration' error.\n  for path, callbacks in list(_import_callbacks.items()):\n    root = os.path.splitext(path)[0]\n\n    nonempty_names = (n for n in names if n)\n    modules = (GetModuleFromName(name, root) for name in nonempty_names)\n    nonempty_modules = (m for m in modules if m)\n\n    for module in nonempty_modules:\n      # TODO(emrekultursay): Write unit test to cover None case.\n      mod_file = getattr(module, '__file__', None)\n      if not mod_file:\n        continue\n\n      mod_root = os.path.splitext(mod_file)[0]\n\n      # If the module is relative, add the curdir prefix to convert it to\n      # absolute path. Note that we don't use os.path.abspath because it\n      # also normalizes the path (which has side effects we don't want).\n      if not os.path.isabs(mod_root):\n        mod_root = os.path.join(os.curdir, mod_root)\n\n      if module_utils2.IsPathSuffix(mod_root, root):\n        for callback in callbacks.copy():\n          callback(module)\n        break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a new TTS engine instance or reuses the existing instance for . If driverName is None the current driver is used.", "response": "def init(driverName=None, debug=False):\n    '''\n    Constructs a new TTS engine instance or reuses the existing instance for\n    the driver name.\n\n    @param driverName: Name of the platform specific driver to use. If\n        None, selects the default driver for the operating system.\n    @type: str\n    @param debug: Debugging output enabled or not\n    @type debug: bool\n    @return: Engine instance\n    @rtype: L{engine.Engine}\n    '''\n    try:\n        eng = _activeEngines[driverName]\n    except KeyError:\n        eng = Engine(driverName, debug)\n        _activeEngines[driverName] = eng\n    return eng"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ListVoices(voice_spec=None):\n    '''Reads the voice files from espeak-data/voices and returns a list of VOICE objects.\n\n   If voice_spec is None then all voices are listed.\n   If voice spec is given, then only the voices which are compatible with the voice_spec\n   are listed, and they are listed in preference order.'''\n    ppv = cListVoices(voice_spec)\n    res = []\n    i = 0\n    while ppv[i]:\n        res.append(ppv[i][0])\n        i += 1\n    return res", "response": "Reads the espeak - data voices and returns a list of VOICE objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef startLoop(self):\n        '''\n        Starts a blocking run loop in which driver callbacks are properly\n        invoked.\n\n        @precondition: There was no previous successful call to L{startLoop}\n            without an intervening call to L{stopLoop}.\n        '''\n        first = True\n        self._looping = True\n        while self._looping:\n            if first:\n                self._proxy.setBusy(False)\n                first = False\n            time.sleep(0.5)", "response": "Starts a blocking run loop in which driver callbacks are properly set up."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nspeaking the given text.", "response": "def say(self, text):\n        '''\n        Speaks the given text. Generates the following notifications during\n        output:\n\n        started-utterance: When speech output has started\n        started-word: When a word is about to be spoken. Includes the character\n            \"location\" of the start of the word in the original utterance text\n            and the \"length\" of the word in characters.\n        finished-utterance: When speech output has finished. Includes a flag\n            indicating if the entire utterance was \"completed\" or not.\n\n        The proxy automatically adds any \"name\" associated with the utterance\n        to the notifications on behalf of the driver.\n\n        When starting to output an utterance, the driver must inform its proxy\n        that it is busy by invoking L{driver.DriverProxy.setBusy} with a flag\n        of True. When the utterance completes or is interrupted, the driver\n        inform the proxy that it is no longer busy by invoking\n        L{driver.DriverProxy.setBusy} with a flag of False.\n\n        @param text: Unicode text to speak\n        @type text: unicode\n        '''\n        self._proxy.setBusy(True)\n        self._proxy.notify('started-utterance')\n        i = 0\n        for word in text.split(' '):\n            self._proxy.notify('started-word', location=i, length=len(word))\n            try:\n                i = text.index(' ', i+1)+1\n            except Exception:\n                pass\n        self._proxy.notify('finished-utterance', completed=True)\n        self._proxy.setBusy(False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets one of the supported properties of the speech engine listed above.", "response": "def setProperty(self, name, value):\n        '''\n        Sets one of the supported property values of the speech engine listed\n        above. If a value is invalid, attempts to clip it / coerce so it is\n        valid before giving up and firing an exception.\n\n        @param name: Property name\n        @type name: str\n        @param value: Property value\n        @type value: object\n        @raise KeyError: When the property name is unknown\n        @raise ValueError: When the value cannot be coerced to fit the property\n        '''\n        if name == 'voice':\n            v = filter(lambda v: v.id == value, self._config['voices'])\n            self._config['voice'] = v[0]\n        elif name == 'rate':\n            self._config['rate'] = value\n        elif name == 'volume':\n            self._config['volume'] = value\n        else:\n            raise KeyError('unknown property %s' % name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ _init__(self, driverName=None, debug=False):\n        self.proxy = driver.DriverProxy(weakref.proxy(self), driverName, debug)\n        # initialize other vars\n        self._connects = {}\n        self._inLoop = False\n        self._driverLoop = True\n        self._debug = debug", "response": "Initialize the TTS engine instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _notify(self, topic, **kwargs):\n        for cb in self._connects.get(topic, []):\n            try:\n                cb(**kwargs)\n            except Exception:\n                if self._debug:\n                    traceback.print_exc()", "response": "Notify callbacks for an event topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_to_file(self, text, filename, name=None):\n        '''\n        Adds an utterance to speak to the event queue.\n\n        @param text: Text to sepak\n        @type text: unicode\n        @param filename: the name of file to save.\n        @param name: Name to associate with this utterance. Included in\n            notifications about this utterance.\n        @type name: str\n        '''\n        self.proxy.save_to_file(text, filename, name)", "response": "Save a new utterance to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns an event loop until all commands queued up until this method call complete.", "response": "def runAndWait(self):\n        \"\"\"\n        Runs an event loop until all commands queued up until this method call\n        complete. Blocks during the event loop and returns when the queue is\n        cleared.\n\n        @raise RuntimeError: When the loop is already running\n        \"\"\"\n        if self._inLoop:\n            raise RuntimeError('run loop already started')\n        self._inLoop = True\n        self._driverLoop = True\n        self.proxy.runAndWait()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef startLoop(self, useDriverLoop=True):\n        if self._inLoop:\n            raise RuntimeError('run loop already started')\n        self._inLoop = True\n        self._driverLoop = useDriverLoop\n        self.proxy.startLoop(self._driverLoop)", "response": "Starts an event loop to process queued commands and callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef endLoop(self):\n        if not self._inLoop:\n            raise RuntimeError('run loop not started')\n        self.proxy.endLoop(self._driverLoop)\n        self._inLoop = False", "response": "Stops a running event loop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over the event loop.", "response": "def iterate(self):\n        \"\"\"\n        Must be called regularly when using an external event loop.\n        \"\"\"\n        if not self._inLoop:\n            raise RuntimeError('run loop not started')\n        elif self._driverLoop:\n            raise RuntimeError('iterate not valid in driver run loop')\n        self.proxy.iterate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a command to the queue.", "response": "def _push(self, mtd, args, name=None):\n        '''\n        Adds a command to the queue.\n\n        @param mtd: Method to invoke to process the command\n        @type mtd: method\n        @param args: Arguments to apply when invoking the method\n        @type args: tuple\n        @param name: Name associated with the command\n        @type name: str\n        '''\n        self._queue.append((mtd, args, name))\n        self._pump()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the next command in the queue.", "response": "def _pump(self):\n        '''\n        Attempts to process the next command in the queue if one exists and the\n        driver is not currently busy.\n        '''\n        while (not self._busy) and len(self._queue):\n            cmd = self._queue.pop(0)\n            self._name = cmd[2]\n            try:\n                cmd[0](*cmd[1])\n            except Exception as e:\n                self.notify('error', exception=e)\n                if self._debug:\n                    traceback.print_exc()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notify(self, topic, **kwargs):\n        '''\n        Sends a notification to the engine from the driver.\n\n        @param topic: Notification topic\n        @type topic: str\n        @param kwargs: Arbitrary keyword arguments\n        @type kwargs: dict\n        '''\n        kwargs['name'] = self._name\n        self._engine._notify(topic, **kwargs)", "response": "Sends a notification to the engine from the driver."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the busy flag.", "response": "def setBusy(self, busy):\n        '''\n        Called by the driver to indicate it is busy.\n\n        @param busy: True when busy, false when idle\n        @type busy: bool\n        '''\n        self._busy = busy\n        if not self._busy:\n            self._pump()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef say(self, text, name):\n        '''\n        Called by the engine to push a say command onto the queue.\n\n        @param text: Text to speak\n        @type text: unicode\n        @param name: Name to associate with the utterance\n        @type name: str\n        '''\n        self._push(self._driver.say, (text,), name)", "response": "Push a say command onto the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop the current utterance and clear the queue of commands.", "response": "def stop(self):\n        '''\n        Called by the engine to stop the current utterance and clear the queue\n        of commands.\n        '''\n        # clear queue up to first end loop command\n        while(True):\n            try:\n                mtd, args, name = self._queue[0]\n            except IndexError:\n                break\n            if(mtd == self._engine.endLoop):\n                break\n            self._queue.pop(0)\n        self._driver.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_to_file(self, text, filename, name):\n        '''\n        Called by the engine to push a say command onto the queue.\n\n        @param text: Text to speak\n        @type text: unicode\n        @param name: Name to associate with the utterance\n        @type name: str\n        '''\n        self._push(self._driver.save_to_file, (text, filename), name)", "response": "Push a say command onto the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setProperty(self, name, value):\n        '''\n        Called by the engine to set a driver property value.\n\n        @param name: Name of the property\n        @type name: str\n        @param value: Property value\n        @type value: object\n        '''\n        self._push(self._driver.setProperty, (name, value))", "response": "Set a driver property value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls by the engine to start an event loop process all commands in the queue at the start of the loop and then wait for the driver to exit.", "response": "def runAndWait(self):\n        '''\n        Called by the engine to start an event loop, process all commands in\n        the queue at the start of the loop, and then exit the loop.\n        '''\n        self._push(self._engine.endLoop, tuple())\n        self._driver.startLoop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts an event loop.", "response": "def startLoop(self, useDriverLoop):\n        '''\n        Called by the engine to start an event loop.\n        '''\n        if useDriverLoop:\n            self._driver.startLoop()\n        else:\n            self._iterator = self._driver.iterate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops the event loop.", "response": "def endLoop(self, useDriverLoop):\n        '''\n        Called by the engine to stop an event loop.\n        '''\n        self._queue = []\n        self._driver.stop()\n        if useDriverLoop:\n            self._driver.endLoop()\n        else:\n            self._iterator = None\n        self.setBusy(True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists channels & users in slack.", "response": "def list_slack():\n    \"\"\"List channels & users in slack.\"\"\"\n    try:\n        token = os.environ['SLACK_TOKEN']\n        slack = Slacker(token)\n\n        # Get channel list\n        response = slack.channels.list()\n        channels = response.body['channels']\n        for channel in channels:\n            print(channel['id'], channel['name'])\n            # if not channel['is_archived']:\n            # slack.channels.join(channel['name'])\n        print()\n\n        # Get users list\n        response = slack.users.list()\n        users = response.body['members']\n        for user in users:\n            if not user['deleted']:\n                print(user['id'], user['name'], user['is_admin'], user[\n                    'is_owner'])\n        print()\n    except KeyError as ex:\n        print('Environment variable %s not set.' % str(ex))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n\n        if self._main_loop:\n            return\n\n        self._interface_added_signal = self._bus.add_signal_receiver(\n            self._interfaces_added,\n            dbus_interface='org.freedesktop.DBus.ObjectManager',\n            signal_name='InterfacesAdded')\n\n        # TODO: Also listen to 'interfaces removed' events?\n\n        self._properties_changed_signal = self._bus.add_signal_receiver(\n            self._properties_changed,\n            dbus_interface=dbus.PROPERTIES_IFACE,\n            signal_name='PropertiesChanged',\n            arg0='org.bluez.Device1',\n            path_keyword='path')\n\n        def disconnect_signals():\n            for device in self._devices.values():\n                device.invalidate()\n            self._properties_changed_signal.remove()\n            self._interface_added_signal.remove()\n\n        self._main_loop = GObject.MainLoop()\n        try:\n            self._main_loop.run()\n            disconnect_signals()\n        except Exception:\n            disconnect_signals()\n            raise", "response": "Starts the main loop that is necessary to receive events from the Bluetooth adapter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_discovery(self, service_uuids=[]):\n\n        discovery_filter = {'Transport': 'le'}\n        if service_uuids:  # D-Bus doesn't like empty lists, it needs to guess the type\n            discovery_filter['UUIDs'] = service_uuids\n\n        try:\n            self._adapter.SetDiscoveryFilter(discovery_filter)\n            self._adapter.StartDiscovery()\n        except dbus.exceptions.DBusException as e:\n            if e.get_dbus_name() == 'org.bluez.Error.NotReady':\n                raise errors.NotReady(\n                    \"Bluetooth adapter not ready. \"\n                    \"Set `is_adapter_powered` to `True` or run 'echo \\\"power on\\\" | sudo bluetoothctl'.\")\n            if e.get_dbus_name() == 'org.bluez.Error.InProgress':\n                # Discovery was already started - ignore exception\n                pass\n            else:\n                raise _error_from_dbus_error(e)", "response": "Starts a discovery for BLE devices with given service UUIDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop_discovery(self):\n        try:\n            self._adapter.StopDiscovery()\n        except dbus.exceptions.DBusException as e:\n            if (e.get_dbus_name() == 'org.bluez.Error.Failed') and (e.get_dbus_message() == 'No discovery started'):\n                pass\n            else:\n                raise _error_from_dbus_error(e)", "response": "Stops the discovery started with start_discovery"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef alias(self):\n        try:\n            return self._properties.Get('org.bluez.Device1', 'Alias')\n        except dbus.exceptions.DBusException as e:\n            if e.get_dbus_name() == 'org.freedesktop.DBus.Error.UnknownObject':\n                # BlueZ sometimes doesn't provide an alias, we then simply return `None`.\n                # Might occur when device was deleted as the following issue points out:\n                # https://github.com/blueman-project/blueman/issues/460\n                return None\n            else:\n                raise _error_from_dbus_error(e)", "response": "Returns the alias of the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef properties_changed(self, sender, changed_properties, invalidated_properties):\n        if 'Connected' in changed_properties:\n            if changed_properties['Connected']:\n                self.connect_succeeded()\n            else:\n                self.disconnect_succeeded()\n\n        if ('ServicesResolved' in changed_properties and changed_properties['ServicesResolved'] == 1 and\n                not self.services):\n            self.services_resolved()", "response": "Called when a device property has changed or got invalidated."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall when all device s services and characteristics got resolved.", "response": "def services_resolved(self):\n        \"\"\"\n        Called when all device's services and characteristics got resolved.\n        \"\"\"\n        self._disconnect_service_signals()\n\n        services_regex = re.compile(self._device_path + '/service[0-9abcdef]{4}$')\n        managed_services = [\n            service for service in self._object_manager.GetManagedObjects().items()\n            if services_regex.match(service[0])]\n        self.services = [Service(\n            device=self,\n            path=service[0],\n            uuid=service[1]['org.bluez.GattService1']['UUID']) for service in managed_services]\n\n        self._connect_service_signals()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef characteristics_resolved(self):\n        self._disconnect_characteristic_signals()\n\n        characteristics_regex = re.compile(self._path + '/char[0-9abcdef]{4}$')\n        managed_characteristics = [\n            char for char in self._object_manager.GetManagedObjects().items()\n            if characteristics_regex.match(char[0])]\n        self.characteristics = [Characteristic(\n            service=self,\n            path=c[0],\n            uuid=c[1]['org.bluez.GattCharacteristic1']['UUID']) for c in managed_characteristics]\n\n        self._connect_characteristic_signals()", "response": "Called when all service s characteristics got resolved."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_value(self, offset=0):\n        try:\n            val = self._object.ReadValue(\n                {'offset': dbus.UInt16(offset, variant_level=1)},\n                dbus_interface='org.bluez.GattDescriptor1')\n            return val\n        except dbus.exceptions.DBusException as e:\n            error = _error_from_dbus_error(e)\n            self.service.device.descriptor_read_value_failed(self, error=error)", "response": "Reads the value of the related\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling when a characteristic property has changed.", "response": "def properties_changed(self, properties, changed_properties, invalidated_properties):\n        value = changed_properties.get('Value')\n        \"\"\"\n        Called when a Characteristic property has changed.\n        \"\"\"\n        if value is not None:\n            self.service.device.characteristic_value_updated(characteristic=self, value=bytes(value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_value(self, value, offset=0):\n        bytes = [dbus.Byte(b) for b in value]\n\n        try:\n            self._object.WriteValue(\n                bytes,\n                {'offset': dbus.UInt16(offset, variant_level=1)},\n                reply_handler=self._write_value_succeeded,\n                error_handler=self._write_value_failed,\n                dbus_interface='org.bluez.GattCharacteristic1')\n        except dbus.exceptions.DBusException as e:\n            self._write_value_failed(self, error=e)", "response": "Attempts to write a value to the characteristic."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall when the write request has failed.", "response": "def _write_value_failed(self, dbus_error):\n        \"\"\"\n        Called when the write request has failed.\n        \"\"\"\n        error = _error_from_dbus_error(dbus_error)\n        self.service.device.characteristic_write_value_failed(characteristic=self, error=error)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable or disables value change notifications.", "response": "def enable_notifications(self, enabled=True):\n        \"\"\"\n        Enables or disables value change notifications.\n\n        Success or failure will be notified by calls to `characteristic_enable_notifications_succeeded`\n        or `enable_notifications_failed` respectively.\n\n        Each time when the device notifies a new value, `characteristic_value_updated()` of the related\n        device will be called.\n        \"\"\"\n        try:\n            if enabled:\n                self._object.StartNotify(\n                    reply_handler=self._enable_notifications_succeeded,\n                    error_handler=self._enable_notifications_failed,\n                    dbus_interface='org.bluez.GattCharacteristic1')\n            else:\n                self._object.StopNotify(\n                    reply_handler=self._enable_notifications_succeeded,\n                    error_handler=self._enable_notifications_failed,\n                    dbus_interface='org.bluez.GattCharacteristic1')\n        except dbus.exceptions.DBusException as e:\n            self._enable_notifications_failed(error=e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _enable_notifications_failed(self, dbus_error):\n        if ((dbus_error.get_dbus_name() == 'org.bluez.Error.Failed') and\n            ((dbus_error.get_dbus_message() == \"Already notifying\") or\n             (dbus_error.get_dbus_message() == \"No notify session started\"))):\n            # Ignore cases where notifications where already enabled or already disabled\n            return\n        error = _error_from_dbus_error(dbus_error)\n        self.service.device.characteristic_enable_notifications_failed(characteristic=self, error=error)", "response": "Called when notifications are disabled or disabled."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsplit a string into parts at multiple characters", "response": "def _split(string, splitters):\n    \"\"\"Splits a string into parts at multiple characters\"\"\"\n    part = ''\n    for character in string:\n        if character in splitters:\n            yield part\n            part = ''\n        else:\n            part += character\n    yield part"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _hash(number, alphabet):\n    hashed = ''\n    len_alphabet = len(alphabet)\n    while True:\n        hashed = alphabet[number % len_alphabet] + hashed\n        number //= len_alphabet\n        if not number:\n            return hashed", "response": "Hashes number using the given alphabet sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring a number tuple from hashed using the given alphabet index.", "response": "def _unhash(hashed, alphabet):\n    \"\"\"Restores a number tuple from hashed using the given `alphabet` index.\"\"\"\n    number = 0\n    len_alphabet = len(alphabet)\n    for character in hashed:\n        position = alphabet.index(character)\n        number *= len_alphabet\n        number += position\n    return number"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reorder(string, salt):\n    len_salt = len(salt)\n\n    if len_salt != 0:\n        string = list(string)\n        index, integer_sum = 0, 0\n        for i in range(len(string) - 1, 0, -1):\n            integer = ord(salt[index])\n            integer_sum += integer\n            j = (integer + index + integer_sum) % i\n            string[i], string[j] = string[j], string[i]\n            index = (index + 1) % len_salt\n        string = ''.join(string)\n\n    return string", "response": "Reorders string according to salt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure the minimal hash length", "response": "def _ensure_length(encoded, min_length, alphabet, guards, values_hash):\n    \"\"\"Ensures the minimal hash length\"\"\"\n    len_guards = len(guards)\n    guard_index = (values_hash + ord(encoded[0])) % len_guards\n    encoded = guards[guard_index] + encoded\n\n    if len(encoded) < min_length:\n        guard_index = (values_hash + ord(encoded[2])) % len_guards\n        encoded += guards[guard_index]\n\n    split_at = len(alphabet) // 2\n    while len(encoded) < min_length:\n        alphabet = _reorder(alphabet, alphabet)\n        encoded = alphabet[split_at:] + encoded + alphabet[:split_at]\n        excess = len(encoded) - min_length\n        if excess > 0:\n            from_index = excess // 2\n            encoded = encoded[from_index:from_index+min_length]\n\n    return encoded"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _encode(values, salt, min_length, alphabet, separators, guards):\n\n    len_alphabet = len(alphabet)\n    len_separators = len(separators)\n    values_hash = sum(x % (i + 100) for i, x in enumerate(values))\n    encoded = lottery = alphabet[values_hash % len(alphabet)]\n\n    for i, value in enumerate(values):\n        alphabet_salt = (lottery + salt + alphabet)[:len_alphabet]\n        alphabet = _reorder(alphabet, alphabet_salt)\n        last = _hash(value, alphabet)\n        encoded += last\n        value %= ord(last[0]) + i\n        encoded += separators[value % len_separators]\n\n    encoded = encoded[:-1]  # cut off last separator\n\n    return (encoded if len(encoded) >= min_length else\n            _ensure_length(encoded, min_length, alphabet, guards, values_hash))", "response": "Helper function that does the hash building without argument checks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _decode(hashid, salt, alphabet, separators, guards):\n    parts = tuple(_split(hashid, guards))\n    hashid = parts[1] if 2 <= len(parts) <= 3 else parts[0]\n\n    if not hashid:\n        return\n\n    lottery_char = hashid[0]\n    hashid = hashid[1:]\n\n    hash_parts = _split(hashid, separators)\n    for part in hash_parts:\n        alphabet_salt = (lottery_char + salt + alphabet)[:len(alphabet)]\n        alphabet = _reorder(alphabet, alphabet_salt)\n        yield _unhash(part, alphabet)", "response": "Helper method that restores the values encoded in a hashid without\n    argument checks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _deprecated(func):\n    @wraps(func)\n    def with_warning(*args, **kwargs):\n        warnings.warn(\n            ('The %s method is deprecated and will be removed in v2.*.*' %\n             func.__name__),\n            DeprecationWarning\n        )\n        return func(*args, **kwargs)\n    return with_warning", "response": "A decorator that warns about deprecation when the passed - in function is\n    invoked."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a hash from the passed values.", "response": "def encode(self, *values):\n        \"\"\"Builds a hash from the passed `values`.\n\n        :param values The values to transform into a hashid\n\n        >>> hashids = Hashids('arbitrary salt', 16, 'abcdefghijkl0123456')\n        >>> hashids.encode(1, 23, 456)\n        '1d6216i30h53elk3'\n        \"\"\"\n        if not (values and all(_is_uint(x) for x in values)):\n            return ''\n\n        return _encode(values, self._salt, self._min_length, self._alphabet,\n                       self._separators, self._guards)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrestoring a tuple of numbers from the passed hashid.", "response": "def decode(self, hashid):\n        \"\"\"Restore a tuple of numbers from the passed `hashid`.\n\n        :param hashid The hashid to decode\n\n        >>> hashids = Hashids('arbitrary salt', 16, 'abcdefghijkl0123456')\n        >>> hashids.decode('1d6216i30h53elk3')\n        (1, 23, 456)\n        \"\"\"\n        if not hashid or not _is_str(hashid):\n            return ()\n        try:\n            numbers = tuple(_decode(hashid, self._salt, self._alphabet,\n                                    self._separators, self._guards))\n\n            return numbers if hashid == self.encode(*numbers) else ()\n        except ValueError:\n            return ()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_hex(self, hex_str):\n        numbers = (int('1' + hex_str[i:i+12], 16)\n                   for i in range(0, len(hex_str), 12))\n        try:\n            return self.encode(*numbers)\n        except ValueError:\n            return ''", "response": "Converts a hexadecimal string to a hashid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_noqa_suppressions(file_contents):\n    ignore_whole_file = False\n    ignore_lines = set()\n    for line_number, line in enumerate(file_contents):\n        if _FLAKE8_IGNORE_FILE.search(line):\n            ignore_whole_file = True\n        if _PEP8_IGNORE_LINE.search(line):\n            ignore_lines.add(line_number + 1)\n    return ignore_whole_file, ignore_lines", "response": "Finds all pep8 and flake8 suppressions in a list of file lines\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_suppressions(relative_filepaths, root, messages):\n    paths_to_ignore = set()\n    lines_to_ignore = defaultdict(set)\n    messages_to_ignore = defaultdict(lambda: defaultdict(set))\n\n    # first deal with 'noqa' style messages\n    for filepath in relative_filepaths:\n        abspath = os.path.join(root, filepath)\n\n        try:\n            file_contents = encoding.read_py_file(abspath).split('\\n')\n        except encoding.CouldNotHandleEncoding as err:\n            # TODO: this output will break output formats such as JSON\n            warnings.warn('{0}: {1}'.format(err.path, err.cause), ImportWarning)\n            continue\n\n        ignore_file, ignore_lines = get_noqa_suppressions(file_contents)\n        if ignore_file:\n            paths_to_ignore.add(filepath)\n        lines_to_ignore[filepath] |= ignore_lines\n\n    # now figure out which messages were suppressed by pylint\n    pylint_ignore_files, pylint_ignore_messages = _parse_pylint_informational(messages)\n    paths_to_ignore |= pylint_ignore_files\n    for filepath, line in pylint_ignore_messages.items():\n        for line_number, codes in line.items():\n            for code in codes:\n                messages_to_ignore[filepath][line_number].add(('pylint', code))\n                if code in _PYLINT_EQUIVALENTS:\n                    for equivalent in _PYLINT_EQUIVALENTS[code]:\n                        messages_to_ignore[filepath][line_number].add(equivalent)\n\n    return paths_to_ignore, lines_to_ignore, messages_to_ignore", "response": "Given a list of files to inspect and a list of messages to inspect create a list of files to ignore and a map of filepath - > line - number - > codes to ignore"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parser():\n    manager = cfg.build_manager()\n    source = cfg.build_command_line_source(prog='prospector', description=None)\n    return source.build_parser(manager.settings, None)", "response": "Returns an argparse parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _combine_w0614(self, messages):\n        by_loc = defaultdict(list)\n        out = []\n\n        for message in messages:\n            if message.code == 'unused-wildcard-import':\n                by_loc[message.location].append(message)\n            else:\n                out.append(message)\n\n        for location, message_list in by_loc.items():\n            names = []\n            for msg in message_list:\n                names.append(\n                    _UNUSED_WILDCARD_IMPORT_RE.match(msg.message).group(1))\n\n            msgtxt = 'Unused imports from wildcard import: %s' % ', '.join(\n                names)\n            combined_message = Message('pylint', 'unused-wildcard-import',\n                                       location, msgtxt)\n            out.append(combined_message)\n\n        return out", "response": "Combine all unused - wildcard - import messages into one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_messages(relative_filepaths, root, messages):\n    paths_to_ignore, lines_to_ignore, messages_to_ignore = get_suppressions(relative_filepaths, root, messages)\n\n    filtered = []\n    for message in messages:\n        # first get rid of the pylint informational messages\n        relative_message_path = os.path.relpath(message.location.path)\n\n        if message.source == 'pylint' and message.code in ('suppressed-message', 'file-ignored',):\n            continue\n\n        # some files are skipped entirely by messages\n        if relative_message_path in paths_to_ignore:\n            continue\n\n        # some lines are skipped entirely by messages\n        if relative_message_path in lines_to_ignore:\n            if message.location.line in lines_to_ignore[relative_message_path]:\n                continue\n\n        # and some lines have only certain messages explicitly ignored\n        if relative_message_path in messages_to_ignore:\n            if message.location.line in messages_to_ignore[relative_message_path]:\n                if message.code in messages_to_ignore[relative_message_path][message.location.line]:\n                    continue\n\n        # otherwise this message was not filtered\n        filtered.append(message)\n\n    return filtered", "response": "This method filters out all messages that are not suppressed by other tools."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_python(ignores, paths, explicit_file_mode, workdir=''):\n    if explicit_file_mode:\n        return SingleFiles(paths, workdir or os.getcwd())\n    else:\n        assert len(paths) == 1\n        files, modules, directories, packages = _find_paths(ignores, paths[0], workdir)\n        return FoundFiles(workdir, files, modules, directories, packages, ignores)", "response": "Find python files in the given paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_minimal_syspath(self, absolute_paths=True):\n        # firstly, gather a list of the minimum path to each package\n        package_list = set()\n        packages = [p[0] for p in self._packages if not p[1]]\n        for package in sorted(packages, key=len):\n            parent = os.path.split(package)[0]\n            if parent not in packages and parent not in package_list:\n                package_list.add(parent)\n\n        # now add the directory containing any modules who are not in packages\n        module_list = []\n        modules = [m[0] for m in self._modules if not m[1]]\n        for module in modules:\n            dirname = os.path.dirname(module)\n            if dirname not in packages:\n                module_list.append(dirname)\n\n        full_list = sorted(set(module_list) | package_list | {self.rootpath}, key=len)\n        if absolute_paths:\n            full_list = [os.path.join(self.rootpath, p).rstrip(os.path.sep) for p in full_list]\n        return full_list", "response": "Return a list of directories that would be added to sys. path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blend_line(messages, blend_combos=None):\n    blend_combos = blend_combos or BLEND_COMBOS\n    blend_lists = [[] for _ in range(len(blend_combos))]\n    blended = []\n\n    # first we split messages into each of the possible blendable categories\n    # so that we have a list of lists of messages which can be blended together\n    for message in messages:\n        key = (message.source, message.code)\n        found = False\n        for blend_combo_idx, blend_combo in enumerate(blend_combos):\n            if key in blend_combo:\n                found = True\n                blend_lists[blend_combo_idx].append(message)\n\n        # note: we use 'found=True' here rather than a simple break/for-else\n        # because this allows the same message to be put into more than one\n        # 'bucket'. This means that the same message from pep8 can 'subsume'\n        # two from pylint, for example.\n\n        if not found:\n            # if we get here, then this is not a message which can be blended,\n            # so by definition is already blended\n            blended.append(message)\n\n    # we should now have a list of messages which all represent the same\n    # problem on the same line, so we will sort them according to the priority\n    # in BLEND and pick the first one\n    for blend_combo_idx, blend_list in enumerate(blend_lists):\n        if len(blend_list) == 0:\n            continue\n        blend_list.sort(\n            key=lambda msg: blend_combos[blend_combo_idx].index(\n                (msg.source, msg.code),\n            ),\n        )\n        if blend_list[0] not in blended:\n            # We may have already added this message if it represents\n            # several messages in other tools which are not being run -\n            # for example, pylint missing-docstring is blended with pep257 D100, D101\n            # and D102, but should not appear 3 times!\n            blended.append(blend_list[0])\n\n        # Some messages from a tool point out an error that in another tool is handled by two\n        # different errors or more. For example, pylint emits the same warning (multiple-statements)\n        # for \"two statements on a line\" separated by a colon and a semi-colon, while pep8 has E701\n        # and E702 for those cases respectively. In this case, the pylint error will not be 'blended' as\n        # it will appear in two blend_lists. Therefore we mark anything not taken from the blend list\n        # as \"consumed\" and then filter later, to avoid such cases.\n        for now_used in blend_list[1:]:\n            now_used.used = True\n\n    return [m for m in blended if not getattr(m, 'used', False)]", "response": "Given a list of messages on the same line blend them together and return a list of all possible messages that can be blended together."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_edge_visibility(gl, v, e, f, hidden_wireframe=True):\n    gl.Clear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n\n    ec = np.arange(1, len(e)+1)\n    ec = np.tile(col(ec), (1, 3))\n    ec[:, 0] = ec[:, 0] & 255\n    ec[:, 1] = (ec[:, 1] >> 8 ) & 255\n    ec[:, 2] = (ec[:, 2] >> 16 ) & 255\n    ec = np.asarray(ec, dtype=np.uint8)\n    \n    draw_colored_primitives(gl, v, e, ec)\n    \n    if hidden_wireframe:\n        gl.Enable(GL_POLYGON_OFFSET_FILL)\n        gl.PolygonOffset(10.0, 1.0)\n        draw_colored_primitives(gl, v, f, fc=np.zeros(f.shape))\n        gl.Disable(GL_POLYGON_OFFSET_FILL)\n    \n    raw = np.asarray(gl.getImage(), np.uint32)\n    raw = raw[:,:,0] + raw[:,:,1]*256 + raw[:,:,2]*256*256 - 1\n    return raw", "response": "Draw the edge visibility of a single edge."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw_boundary_images(glf, glb, v, f, vpe, fpe, camera):\n    glf.Clear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n    glb.Clear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n\n    # Figure out which edges are on pairs of differently visible triangles\n    from opendr.geometry import TriNormals\n    tn = TriNormals(v, f).r.reshape((-1,3))\n    campos = -cv2.Rodrigues(camera.rt.r)[0].T.dot(camera.t.r)\n    rays_to_verts = v.reshape((-1,3)) - row(campos)\n    rays_to_faces = rays_to_verts[f[:,0]] + rays_to_verts[f[:,1]] + rays_to_verts[f[:,2]]\n    dps = np.sum(rays_to_faces * tn, axis=1)\n    dps = dps[fpe[:,0]] * dps[fpe[:,1]]\n    silhouette_edges = np.asarray(np.nonzero(dps<=0)[0], np.uint32)\n    non_silhouette_edges = np.nonzero(dps>0)[0]\n    lines_e = vpe[silhouette_edges]\n    lines_v = v\n\n    visibility = draw_edge_visibility(glb, lines_v, lines_e, f, hidden_wireframe=True)\n    shape = visibility.shape\n    visibility = visibility.ravel()\n    visible = np.nonzero(visibility.ravel() != 4294967295)[0]\n    visibility[visible] = silhouette_edges[visibility[visible]]\n    result = visibility.reshape(shape)\n    return result", "response": "Draw the boundary images for a GLF."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Ch object whose only attribute v represents the flattened vertices.", "response": "def TriArea(v_init, f, normalize):\n    \"\"\" Returns a Ch object whose only attribute \"v\" represents the flattened vertices.\"\"\"\n    \n    if normalize:\n        nm = lambda x : NormalizedNx3(x)\n    else:\n        nm = lambda x : x\n    result = Ch(lambda v : (Sum3xN(CrossProduct(TriEdges(f,1,0,nm(v)), TriEdges(f,2,0, nm(v)))**2.) ** 0.5) * 0.5)\n    result.v = v_init\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AcosTriAngles(v, f, normalize):\n\n    if normalize:\n        nm = lambda x : NormalizedNx3(x)\n    else:\n        nm = lambda x : x\n\n    return Ch(lambda v :\n        Sum3xN(NormalizedNx3(TriEdges(f, 1, 0, nm(v))) * NormalizedNx3(TriEdges(f, 2, 0, nm(v)))) &\n        Sum3xN(NormalizedNx3(TriEdges(f, 2, 1, nm(v))) * NormalizedNx3(TriEdges(f, 0, 1, nm(v)))) &\n        Sum3xN(NormalizedNx3(TriEdges(f, 0, 2, nm(v))) * NormalizedNx3(TriEdges(f, 1, 2, nm(v)))))", "response": "Returns a Ch object whose only attribute v represents the flattened vertices."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a stack of skew - symmetric matrices which can be multiplied by b to get the cross product.", "response": "def Ax(self):\n        \"\"\"Compute a stack of skew-symmetric matrices which can be multiplied\n        by 'b' to get the cross product. See:\n\n        http://en.wikipedia.org/wiki/Cross_product#Conversion_to_matrix_multiplication\n        \"\"\"\n        #  0         -self.a3   self.a2\n        #  self.a3    0        -self.a1\n        # -self.a2    self.a1   0        \n        m = np.zeros((len(self.a1), 3, 3))\n        m[:, 0, 1] = -self.a3\n        m[:, 0, 2] = +self.a2\n        m[:, 1, 0] = +self.a3\n        m[:, 1, 2] = -self.a1\n        m[:, 2, 0] = -self.a2\n        m[:, 2, 1] = +self.a1        \n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute a stack of skew - symmetric matrices which can be multiplied by a to get the cross product.", "response": "def Bx(self):        \n        \"\"\"Compute a stack of skew-symmetric matrices which can be multiplied\n        by 'a' to get the cross product. See:\n\n        http://en.wikipedia.org/wiki/Cross_product#Conversion_to_matrix_multiplication\n        \"\"\"\n        #  0         self.b3  -self.b2\n        # -self.b3   0         self.b1\n        #  self.b2  -self.b1   0\n        \n        \n        m = np.zeros((len(self.b1), 3, 3))\n        m[:, 0, 1] = +self.b3\n        m[:, 0, 2] = -self.b2\n        m[:, 1, 0] = -self.b3\n        m[:, 1, 2] = +self.b1\n        m[:, 2, 0] = +self.b2\n        m[:, 2, 1] = -self.b1\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lambertian_spotlight(v, vn, pos, dir, spot_exponent, camcoord=False, camera_t=None, camera_rt=None):\n\n    if camcoord: # Transform pos and dir from camera to world coordinate system\n        assert(camera_t is not None and camera_rt is not None)\n        from opendr.geometry import Rodrigues\n        rot = Rodrigues(rt=camera_rt)\n        pos = rot.T.dot(pos-camera_t)\n        dir = rot.T.dot(dir)\n\n    dir = dir / ch.sqrt(ch.sum(dir**2.))\n    v_minus_light = v - pos.reshape((1,3))\n    v_distances = ch.sqrt(ch.sum(v_minus_light**2, axis=1))\n    v_minus_light_normed = v_minus_light / v_distances.reshape((-1,1))\n    cosangle = v_minus_light_normed.dot(dir.reshape((3,1)))\n    light_dot_normal = ch.sum(vn*v_minus_light_normed, axis=1)\n    light_dot_normal.label = 'light_dot_normal'\n    cosangle.label = 'cosangle'\n    result = light_dot_normal.ravel() * cosangle.ravel()**spot_exponent\n    result = result / v_distances ** 2.\n    result = maximum(result, 0.0)\n\n    return result", "response": "Compute the lambertian spotlight."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dImage_wrt_2dVerts_bnd(observed, visible, visibility, barycentric, image_width, image_height, num_verts, f, bnd_bool):\n\n    n_channels = np.atleast_3d(observed).shape[2]\n    shape = visibility.shape\n\n    # Step 1: get the structure ready, ie the IS and the JS\n    IS = np.tile(col(visible), (1, 2*f.shape[1])).ravel()\n    JS = col(f[visibility.ravel()[visible]].ravel())\n    JS = np.hstack((JS*2, JS*2+1)).ravel()\n\n    pxs = np.asarray(visible % shape[1], np.int32)\n    pys = np.asarray(np.floor(np.floor(visible) / shape[1]), np.int32)\n\n    if n_channels > 1:\n        IS = np.concatenate([IS*n_channels+i for i in range(n_channels)])\n        JS = np.concatenate([JS for i in range(n_channels)])\n\n    # Step 2: get the data ready, ie the actual values of the derivatives\n    ksize = 1\n    bndf = bnd_bool.astype(np.float64)\n    nbndf = np.logical_not(bnd_bool).astype(np.float64)\n    sobel_normalizer = cv2.Sobel(np.asarray(np.tile(row(np.arange(10)), (10, 1)), np.float64), cv2.CV_64F, dx=1, dy=0, ksize=ksize)[5,5]\n\n    bnd_nan = bndf.reshape((observed.shape[0], observed.shape[1], -1)).copy()\n    bnd_nan.ravel()[bnd_nan.ravel()>0] = np.nan\n    bnd_nan += 1\n    obs_nonbnd = np.atleast_3d(observed) * bnd_nan\n\n    ydiffnb, xdiffnb = nangradients(obs_nonbnd)\n\n    observed = np.atleast_3d(observed)\n\n    if observed.shape[2] > 1:\n        ydiffbnd, xdiffbnd, _ = np.gradient(observed)\n    else:\n        ydiffbnd, xdiffbnd = np.gradient(observed.squeeze())\n        ydiffbnd = np.atleast_3d(ydiffbnd)\n        xdiffbnd = np.atleast_3d(xdiffbnd)\n\n    # This corrects for a bias imposed boundary differences begin spread over two pixels\n    # (by np.gradients or similar) but only counted once (since OpenGL's line\n    # drawing spans 1 pixel)\n    xdiffbnd *= 2.0\n    ydiffbnd *= 2.0\n\n    xdiffnb = -xdiffnb\n    ydiffnb = -ydiffnb\n    xdiffbnd = -xdiffbnd\n    ydiffbnd = -ydiffbnd\n    # ydiffnb *= 0\n    # xdiffnb *= 0\n\n    if False:\n        import matplotlib.pyplot as plt\n        plt.figure()\n        plt.subplot(121)\n        plt.imshow(xdiffnb)\n        plt.title('xdiffnb')\n        plt.subplot(122)\n        plt.imshow(xdiffbnd)\n        plt.title('xdiffbnd')\n        import pdb; pdb.set_trace()\n\n    idxs = np.isnan(xdiffnb.ravel())\n    xdiffnb.ravel()[idxs] = xdiffbnd.ravel()[idxs]\n\n    idxs = np.isnan(ydiffnb.ravel())\n    ydiffnb.ravel()[idxs] = ydiffbnd.ravel()[idxs]\n\n    if True: # should be right thing\n        xdiff = xdiffnb\n        ydiff = ydiffnb\n    else:  #should be old way\n        xdiff = xdiffbnd\n        ydiff = ydiffbnd\n\n\n    # TODO: NORMALIZER IS WRONG HERE\n    # xdiffnb = -cv2.Sobel(obs_nonbnd, cv2.CV_64F, dx=1, dy=0, ksize=ksize) / np.atleast_3d(cv2.Sobel(row(np.arange(obs_nonbnd.shape[1])).astype(np.float64), cv2.CV_64F, dx=1, dy=0, ksize=ksize))\n    # ydiffnb = -cv2.Sobel(obs_nonbnd, cv2.CV_64F, dx=0, dy=1, ksize=ksize) / np.atleast_3d(cv2.Sobel(col(np.arange(obs_nonbnd.shape[0])).astype(np.float64), cv2.CV_64F, dx=0, dy=1, ksize=ksize))\n    #\n    # xdiffnb.ravel()[np.isnan(xdiffnb.ravel())] = 0.\n    # ydiffnb.ravel()[np.isnan(ydiffnb.ravel())] = 0.\n    # xdiffnb.ravel()[np.isinf(xdiffnb.ravel())] = 0.\n    # ydiffnb.ravel()[np.isinf(ydiffnb.ravel())] = 0.\n\n    # xdiffnb = np.atleast_3d(xdiffnb)\n    # ydiffnb = np.atleast_3d(ydiffnb)\n    #\n    # xdiffbnd = -cv2.Sobel(observed, cv2.CV_64F, dx=1, dy=0, ksize=ksize) / sobel_normalizer\n    # ydiffbnd = -cv2.Sobel(observed, cv2.CV_64F, dx=0, dy=1, ksize=ksize) / sobel_normalizer\n    #\n    # xdiff = xdiffnb * np.atleast_3d(nbndf)\n    # xdiff.ravel()[np.isnan(xdiff.ravel())] = 0\n    # xdiff += xdiffbnd*np.atleast_3d(bndf)\n    #\n    # ydiff = ydiffnb * np.atleast_3d(nbndf)\n    # ydiff.ravel()[np.isnan(ydiff.ravel())] = 0\n    # ydiff += ydiffbnd*np.atleast_3d(bndf)\n\n    #import pdb; pdb.set_trace()\n\n    #xdiff = xdiffnb\n    #ydiff = ydiffnb\n\n    #import pdb; pdb.set_trace()\n\n    datas = []\n\n    # The data is weighted according to barycentric coordinates\n    bc0 = col(barycentric[pys, pxs, 0])\n    bc1 = col(barycentric[pys, pxs, 1])\n    bc2 = col(barycentric[pys, pxs, 2])\n    for k in range(n_channels):\n        dxs = xdiff[pys, pxs, k]\n        dys = ydiff[pys, pxs, k]\n        if f.shape[1] == 3:\n            datas.append(np.hstack((col(dxs)*bc0,col(dys)*bc0,col(dxs)*bc1,col(dys)*bc1,col(dxs)*bc2,col(dys)*bc2)).ravel())\n        else:\n            datas.append(np.hstack((col(dxs)*bc0,col(dys)*bc0,col(dxs)*bc1,col(dys)*bc1)).ravel())\n\n    data = np.concatenate(datas)\n\n    ij = np.vstack((IS.ravel(), JS.ravel()))\n    result = sp.csc_matrix((data, ij), shape=(image_width*image_height*n_channels, num_verts*2))\n\n    return result", "response": "Construct a sparse jacobian that relates 2D projected vertex positions\n    to pixel values. This can be done by two steps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dImage_wrt_2dVerts(observed, visible, visibility, barycentric, image_width, image_height, num_verts, f):\n\n    n_channels = np.atleast_3d(observed).shape[2]\n    shape = visibility.shape\n\n    # Step 1: get the structure ready, ie the IS and the JS\n    IS = np.tile(col(visible), (1, 2*f.shape[1])).ravel()\n    JS = col(f[visibility.ravel()[visible]].ravel())\n    JS = np.hstack((JS*2, JS*2+1)).ravel()\n\n    pxs = np.asarray(visible % shape[1], np.int32)\n    pys = np.asarray(np.floor(np.floor(visible) / shape[1]), np.int32)\n\n    if n_channels > 1:\n        IS = np.concatenate([IS*n_channels+i for i in range(n_channels)])\n        JS = np.concatenate([JS for i in range(n_channels)])\n\n    # Step 2: get the data ready, ie the actual values of the derivatives\n    ksize=1\n    sobel_normalizer = cv2.Sobel(np.asarray(np.tile(row(np.arange(10)), (10, 1)), np.float64), cv2.CV_64F, dx=1, dy=0, ksize=ksize)[5,5]\n    xdiff = -cv2.Sobel(observed, cv2.CV_64F, dx=1, dy=0, ksize=ksize) / sobel_normalizer\n    ydiff = -cv2.Sobel(observed, cv2.CV_64F, dx=0, dy=1, ksize=ksize) / sobel_normalizer\n\n    xdiff = np.atleast_3d(xdiff)\n    ydiff = np.atleast_3d(ydiff)\n\n    datas = []\n\n    # The data is weighted according to barycentric coordinates\n    bc0 = col(barycentric[pys, pxs, 0])\n    bc1 = col(barycentric[pys, pxs, 1])\n    bc2 = col(barycentric[pys, pxs, 2])\n    for k in range(n_channels):\n        dxs = xdiff[pys, pxs, k]\n        dys = ydiff[pys, pxs, k]\n        if f.shape[1] == 3:\n            datas.append(np.hstack((col(dxs)*bc0,col(dys)*bc0,col(dxs)*bc1,col(dys)*bc1,col(dxs)*bc2,col(dys)*bc2)).ravel())\n        else:\n            datas.append(np.hstack((col(dxs)*bc0,col(dys)*bc0,col(dxs)*bc1,col(dys)*bc1)).ravel())\n\n    data = np.concatenate(datas)\n\n    ij = np.vstack((IS.ravel(), JS.ravel()))\n    result = sp.csc_matrix((data, ij), shape=(image_width*image_height*n_channels, num_verts*2))\n\n    return result", "response": "This function is used to create a 2D projection of the 2D vertices."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_visibility_image_internal(gl, v, f):\n    gl.Clear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n\n    fc = np.arange(1, len(f)+1)\n    fc = np.tile(col(fc), (1, 3))\n    fc[:, 0] = fc[:, 0] & 255\n    fc[:, 1] = (fc[:, 1] >> 8 ) & 255\n    fc[:, 2] = (fc[:, 2] >> 16 ) & 255\n    fc = np.asarray(fc, dtype=np.uint8)\n\n    draw_colored_primitives(gl, v, f, fc)\n    raw = np.asarray(gl.getImage(), np.uint32)\n    raw = raw[:,:,0] + raw[:,:,1]*256 + raw[:,:,2]*256*256 - 1\n    return raw", "response": "Draws the visibility image of the camera."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a sparse matrix where each nonzero element in the vertex is connected to vertex 12.", "response": "def get_vert_connectivity(mesh_v, mesh_f):\n    \"\"\"Returns a sparse matrix (of size #verts x #verts) where each nonzero\n    element indicates a neighborhood relation. For example, if there is a\n    nonzero element in position (15,12), that means vertex 15 is connected\n    by an edge to vertex 12.\"\"\"\n\n    vpv = sp.csc_matrix((len(mesh_v),len(mesh_v)))\n\n    # for each column in the faces...\n    for i in range(3):\n        IS = mesh_f[:,i]\n        JS = mesh_f[:,(i+1)%3]\n        data = np.ones(len(IS))\n        ij = np.vstack((row(IS.flatten()), row(JS.flatten())))\n        mtx = sp.csc_matrix((data, ij), shape=vpv.shape)\n        vpv = vpv + mtx + mtx.T\n\n    return vpv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an Ex2 array of adjacencies between vertices where each element in the array is a vertex index and each element in the array is a edge index.", "response": "def get_vertices_per_edge(mesh_v, mesh_f):\n    \"\"\"Returns an Ex2 array of adjacencies between vertices, where\n    each element in the array is a vertex index. Each edge is included\n    only once. If output of get_faces_per_edge is provided, this is used to\n    avoid call to get_vert_connectivity()\"\"\"\n\n    vc = sp.coo_matrix(get_vert_connectivity(mesh_v, mesh_f))\n    result = np.hstack((col(vc.row), col(vc.col)))\n    result = result[result[:,0] < result[:,1]] # for uniqueness\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary from vertidx - pairs to opposites.", "response": "def get_vert_opposites_per_edge(mesh_v, mesh_f):\n    \"\"\"Returns a dictionary from vertidx-pairs to opposites.\n    For example, a key consist of [4,5)] meaning the edge between\n    vertices 4 and 5, and a value might be [10,11] which are the indices\n    of the vertices opposing this edge.\"\"\"\n    result = {}\n    for f in mesh_f:\n        for i in range(3):\n            key = [f[i], f[(i+1)%3]]\n            key.sort()\n            key = tuple(key)\n            val = f[(i+2)%3]\n\n            if key in result:\n                result[key].append(val)\n            else:\n                result[key] = [val]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts UTF - 8 strings to UTF16 - BE.", "response": "def UTF8ToUTF16BE(instr, setbom=True):\n    \"Converts UTF-8 strings to UTF16-BE.\"\n    outstr = \"\".encode()\n    if (setbom):\n        outstr += \"\\xFE\\xFF\".encode(\"latin1\")\n    if not isinstance(instr, unicode):\n        instr = instr.decode('UTF-8')\n    outstr += instr.encode('UTF-16BE')\n    # convert bytes back to fake unicode string until PEP461-like is implemented\n    if PY3K:\n        outstr = outstr.decode(\"latin1\")\n    return outstr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_elements(self, elements):\n        \"Initialize the internal element structures\"\n        self.pg_no = 0\n        self.elements = elements\n        self.keys = [v['name'].lower() for v in self.elements]", "response": "Initialize the internal element structures"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_csv(self, infile, delimiter=\",\", decimal_sep=\".\"):\n        \"Parse template format csv file and create elements dict\"\n        keys = ('name','type','x1','y1','x2','y2','font','size',\n            'bold','italic','underline','foreground','background',\n            'align','text','priority', 'multiline')\n        self.elements = []\n        self.pg_no = 0\n        if not PY3K:\n            f = open(infile, 'rb')\n        else:\n            f = open(infile)\n        for row in csv.reader(f, delimiter=delimiter):\n            kargs = {}\n            for i,v in enumerate(row):\n                if not v.startswith(\"'\") and decimal_sep!=\".\": \n                    v = v.replace(decimal_sep,\".\")\n                else:\n                    v = v\n                if v=='':\n                    v = None\n                else:\n                    v = eval(v.strip())\n                kargs[keys[i]] = v\n            self.elements.append(kargs)\n        self.keys = [v['name'].lower() for v in self.elements]", "response": "Parse template format csv file and create elements dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndivides (\\ n ) a string using a given element width", "response": "def split_multicell(self, text, element_name):\n        \"Divide (\\n) a string using a given element width\"\n        pdf = self.pdf\n        element = [element for element in self.elements\n            if element['name'].lower() == element_name.lower()][0]\n        style = \"\"\n        if element['bold']: style += \"B\"\n        if element['italic']: style += \"I\"\n        if element['underline']: style += \"U\"\n        pdf.set_font(element['font'],style,element['size'])\n        align = {'L':'L','R':'R','I':'L','D':'R','C':'C','':''}.get(element['align']) # D/I in spanish\n        if isinstance(text, unicode) and not PY3K:\n            text = text.encode(\"latin1\",\"ignore\")\n        else:\n            text = str(text)\n        return pdf.multi_cell(w=element['x2']-element['x1'],\n                             h=element['y2']-element['y1'],\n                             txt=text,align=align,split_only=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_html(self, text, image_map=None):\n        \"Parse HTML and convert it to PDF\"\n        h2p = HTML2FPDF(self, image_map)\n        text = h2p.unescape(text) # To deal with HTML entities\n        h2p.feed(text)", "response": "Parse HTML and convert it to PDF"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_page(fn):\n        \"Decorator to protect drawing methods\"\n        @wraps(fn)\n        def wrapper(self, *args, **kwargs):\n            if not self.page and not kwargs.get('split_only'):\n                self.error(\"No page open, you need to call add_page() first\")\n            else:\n                return fn(self, *args, **kwargs)\n        return wrapper", "response": "Decorator to protect drawing methods"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets left top and right margins", "response": "def set_margins(self, left,top,right=-1):\n        \"Set left, top and right margins\"\n        self.l_margin=left\n        self.t_margin=top\n        if(right==-1):\n            right=left\n        self.r_margin=right"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_auto_page_break(self, auto,margin=0):\n        \"Set auto page break mode and triggering margin\"\n        self.auto_page_break=auto\n        self.b_margin=margin\n        self.page_break_trigger=self.h-margin", "response": "Set auto page break mode and triggering margin"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_display_mode(self, zoom,layout='continuous'):\n        \n        if(zoom=='fullpage' or zoom=='fullwidth' or zoom=='real' or zoom=='default' or not isinstance(zoom,basestring)):\n            self.zoom_mode=zoom\n        else:\n            self.error('Incorrect zoom display mode: '+zoom)\n        if(layout=='single' or layout=='continuous' or layout=='two' or layout=='default'):\n            self.layout_mode=layout\n        else:\n            self.error('Incorrect layout display mode: '+layout)", "response": "Set display mode in viewer\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_page(self, orientation=''):\n        \"Start a new page\"\n        if(self.state==0):\n            self.open()\n        family=self.font_family\n        if self.underline:\n            style = self.font_style + 'U'\n        else:\n            style = self.font_style\n        size=self.font_size_pt\n        lw=self.line_width\n        dc=self.draw_color\n        fc=self.fill_color\n        tc=self.text_color\n        cf=self.color_flag\n        if(self.page>0):\n            #Page footer\n            self.in_footer=1\n            self.footer()\n            self.in_footer=0\n            #close page\n            self._endpage()\n        #Start new page\n        self._beginpage(orientation)\n        #Set line cap style to square\n        self._out('2 J')\n        #Set line width\n        self.line_width=lw\n        self._out(sprintf('%.2f w',lw*self.k))\n        #Set font\n        if(family):\n            self.set_font(family,style,size)\n        #Set colors\n        self.draw_color=dc\n        if(dc!='0 G'):\n            self._out(dc)\n        self.fill_color=fc\n        if(fc!='0 g'):\n            self._out(fc)\n        self.text_color=tc\n        self.color_flag=cf\n        #Page header\n        self.header()\n        #Restore line width\n        if(self.line_width!=lw):\n            self.line_width=lw\n            self._out(sprintf('%.2f w',lw*self.k))\n        #Restore font\n        if(family):\n            self.set_font(family,style,size)\n        #Restore colors\n        if(self.draw_color!=dc):\n            self.draw_color=dc\n            self._out(dc)\n        if(self.fill_color!=fc):\n            self.fill_color=fc\n            self._out(fc)\n        self.text_color=tc\n        self.color_flag=cf", "response": "Start a new page"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset color for all stroking operations", "response": "def set_draw_color(self, r,g=-1,b=-1):\n        \"Set color for all stroking operations\"\n        if((r==0 and g==0 and b==0) or g==-1):\n            self.draw_color=sprintf('%.3f G',r/255.0)\n        else:\n            self.draw_color=sprintf('%.3f %.3f %.3f RG',r/255.0,g/255.0,b/255.0)\n        if(self.page>0):\n            self._out(self.draw_color)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_fill_color(self,r,g=-1,b=-1):\n        \"Set color for all filling operations\"\n        if((r==0 and g==0 and b==0) or g==-1):\n            self.fill_color=sprintf('%.3f g',r/255.0)\n        else:\n            self.fill_color=sprintf('%.3f %.3f %.3f rg',r/255.0,g/255.0,b/255.0)\n        self.color_flag=(self.fill_color!=self.text_color)\n        if(self.page>0):\n            self._out(self.fill_color)", "response": "Set color for all filling operations"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_text_color(self, r,g=-1,b=-1):\n        \"Set color for text\"\n        if((r==0 and g==0 and b==0) or g==-1):\n            self.text_color=sprintf('%.3f g',r/255.0)\n        else:\n            self.text_color=sprintf('%.3f %.3f %.3f rg',r/255.0,g/255.0,b/255.0)\n        self.color_flag=(self.fill_color!=self.text_color)", "response": "Set color for text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_string_width(self, s):\n        \"Get width of a string in the current font\"\n        s = self.normalize_text(s)\n        cw=self.current_font['cw']\n        w=0\n        l=len(s)\n        if self.unifontsubset:\n            for char in s:\n                char = ord(char)\n                if len(cw) > char:\n                    w += cw[char] # ord(cw[2*char])<<8 + ord(cw[2*char+1])\n                #elif (char>0 and char<128 and isset($cw[chr($char)])) { $w += $cw[chr($char)]; }\n                elif (self.current_font['desc']['MissingWidth']) :\n                    w += self.current_font['desc']['MissingWidth']\n                #elif (isset($this->CurrentFont['MissingWidth'])) { $w += $this->CurrentFont['MissingWidth']; }\n                else:\n                    w += 500\n        else:\n            for i in range(0, l):\n                w += cw.get(s[i],0)\n        return w*self.font_size/1000.0", "response": "Get width of a string in the current font"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dashed_line(self, x1,y1,x2,y2, dash_length=1, space_length=1):\n        self._set_dash(dash_length, space_length)\n        self.line(x1, y1, x2, y2)\n        self._set_dash()", "response": "Draw a dashed line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a TrueType or Type1 font", "response": "def add_font(self, family, style='', fname='', uni=False):\n        \"Add a TrueType or Type1 font\"\n        family = family.lower()\n        if (fname == ''):\n            fname = family.replace(' ','') + style.lower() + '.pkl'\n        if (family == 'arial'):\n            family = 'helvetica'\n        style = style.upper()\n        if (style == 'IB'):\n            style = 'BI'\n        fontkey = family+style\n        if fontkey in self.fonts:\n            # Font already added!\n            return\n        if (uni):\n            global SYSTEM_TTFONTS, FPDF_CACHE_MODE, FPDF_CACHE_DIR\n            if os.path.exists(fname):\n                ttffilename = fname\n            elif (FPDF_FONT_DIR and\n                os.path.exists(os.path.join(FPDF_FONT_DIR, fname))):\n                ttffilename = os.path.join(FPDF_FONT_DIR, fname)\n            elif (SYSTEM_TTFONTS and\n                os.path.exists(os.path.join(SYSTEM_TTFONTS, fname))):\n                ttffilename = os.path.join(SYSTEM_TTFONTS, fname)\n            else:\n                raise RuntimeError(\"TTF Font file not found: %s\" % fname)\n            name = ''\n            if FPDF_CACHE_MODE == 0:\n                unifilename = os.path.splitext(ttffilename)[0] + '.pkl'\n            elif FPDF_CACHE_MODE == 2:                \n                unifilename = os.path.join(FPDF_CACHE_DIR, \\\n                    hashpath(ttffilename) + \".pkl\")\n            else:\n                unifilename = None\n            if unifilename and os.path.exists(unifilename):\n                fh = open(unifilename, \"rb\")\n                try:\n                    font_dict = pickle.load(fh)\n                finally:\n                    fh.close()\n            else:\n                ttf = TTFontFile()\n                ttf.getMetrics(ttffilename)\n                desc = {\n                    'Ascent': int(round(ttf.ascent, 0)),\n                    'Descent': int(round(ttf.descent, 0)),\n                    'CapHeight': int(round(ttf.capHeight, 0)),\n                    'Flags': ttf.flags,\n                    'FontBBox': \"[%s %s %s %s]\" % (\n                        int(round(ttf.bbox[0], 0)),\n                        int(round(ttf.bbox[1], 0)),\n                        int(round(ttf.bbox[2], 0)),\n                        int(round(ttf.bbox[3], 0))),\n                    'ItalicAngle': int(ttf.italicAngle),\n                    'StemV': int(round(ttf.stemV, 0)),\n                    'MissingWidth': int(round(ttf.defaultWidth, 0)),\n                    }\n                # Generate metrics .pkl file\n                font_dict = {\n                    'name': re.sub('[ ()]', '', ttf.fullName),\n                    'type': 'TTF',\n                    'desc': desc,\n                    'up': round(ttf.underlinePosition),\n                    'ut': round(ttf.underlineThickness),\n                    'ttffile': ttffilename,\n                    'fontkey': fontkey,\n                    'originalsize': os.stat(ttffilename).st_size,\n                    'cw': ttf.charWidths,\n                    }\n                if unifilename:\n                    try:\n                        fh = open(unifilename, \"wb\")\n                        pickle.dump(font_dict, fh)\n                        fh.close()\n                    except IOError:\n                        if not exception().errno == errno.EACCES:\n                            raise  # Not a permission error.\n                del ttf\n            if hasattr(self,'str_alias_nb_pages'):\n                sbarr = list(range(0,57))   # include numbers in the subset!\n            else:\n                sbarr = list(range(0,32))\n            self.fonts[fontkey] = {\n                'i': len(self.fonts)+1, 'type': font_dict['type'],\n                'name': font_dict['name'], 'desc': font_dict['desc'],\n                'up': font_dict['up'], 'ut': font_dict['ut'],\n                'cw': font_dict['cw'],\n                'ttffile': font_dict['ttffile'], 'fontkey': fontkey,\n                'subset': sbarr, 'unifilename': unifilename,\n                }\n            self.font_files[fontkey] = {'length1': font_dict['originalsize'],\n                                        'type': \"TTF\", 'ttffile': ttffilename}\n            self.font_files[fname] = {'type': \"TTF\"}\n        else:\n            fontfile = open(fname)\n            try:\n                font_dict = pickle.load(fontfile)\n            finally:\n                fontfile.close()\n            self.fonts[fontkey] = {'i': len(self.fonts)+1}\n            self.fonts[fontkey].update(font_dict)\n            if (diff):\n                #Search existing encodings\n                d = 0\n                nb = len(self.diffs)\n                for i in range(1, nb+1):\n                    if(self.diffs[i] == diff):\n                        d = i\n                        break\n                if (d == 0):\n                    d = nb + 1\n                    self.diffs[d] = diff\n                self.fonts[fontkey]['diff'] = d\n            filename = font_dict.get('filename')\n            if (filename):\n                if (type == 'TrueType'):\n                    self.font_files[filename]={'length1': originalsize}\n                else:\n                    self.font_files[filename]={'length1': size1,\n                                               'length2': size2}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_font(self, family,style='',size=0):\n        \"Select a font; size given in points\"\n        family=family.lower()\n        if(family==''):\n            family=self.font_family\n        if(family=='arial'):\n            family='helvetica'\n        elif(family=='symbol' or family=='zapfdingbats'):\n            style=''\n        style=style.upper()\n        if('U' in style):\n            self.underline=1\n            style=style.replace('U','')\n        else:\n            self.underline=0\n        if(style=='IB'):\n            style='BI'\n        if(size==0):\n            size=self.font_size_pt\n        #Test if font is already selected\n        if(self.font_family==family and self.font_style==style and self.font_size_pt==size):\n            return\n        #Test if used for the first time\n        fontkey=family+style\n        if fontkey not in self.fonts:\n            #Check if one of the standard fonts\n            if fontkey in self.core_fonts:\n                if fontkey not in fpdf_charwidths:\n                    #Load metric file\n                    name=os.path.join(FPDF_FONT_DIR,family)\n                    if(family=='times' or family=='helvetica'):\n                        name+=style.lower()\n                    exec(compile(open(name+'.font').read(), name+'.font', 'exec'))\n                    if fontkey not in fpdf_charwidths:\n                        self.error('Could not include font metric file for'+fontkey)\n                i=len(self.fonts)+1\n                self.fonts[fontkey]={'i':i,'type':'core','name':self.core_fonts[fontkey],'up':-100,'ut':50,'cw':fpdf_charwidths[fontkey]}\n            else:\n                self.error('Undefined font: '+family+' '+style)\n        #Select it\n        self.font_family=family\n        self.font_style=style\n        self.font_size_pt=size\n        self.font_size=size/self.k\n        self.current_font=self.fonts[fontkey]\n        self.unifontsubset = (self.fonts[fontkey]['type'] == 'TTF')\n        if(self.page>0):\n            self._out(sprintf('BT /F%d %.2f Tf ET',self.current_font['i'],self.font_size_pt))", "response": "Select a font ; size given in points"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_font_size(self, size):\n        \"Set font size in points\"\n        if(self.font_size_pt==size):\n            return\n        self.font_size_pt=size\n        self.font_size=size/self.k\n        if(self.page>0):\n            self._out(sprintf('BT /F%d %.2f Tf ET',self.current_font['i'],self.font_size_pt))", "response": "Set font size in points"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_link(self):\n        \"Create a new internal link\"\n        n=len(self.links)+1\n        self.links[n]=(0,0)\n        return n", "response": "Create a new internal link"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset destination of internal link", "response": "def set_link(self, link,y=0,page=-1):\n        \"Set destination of internal link\"\n        if(y==-1):\n            y=self.y\n        if(page==-1):\n            page=self.page\n        self.links[link]=[page,y]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nputting a link on the page", "response": "def link(self, x,y,w,h,link):\n        \"Put a link on the page\"\n        if not self.page in self.page_links:\n            self.page_links[self.page] = []\n        self.page_links[self.page] += [(x*self.k,self.h_pt-y*self.k,w*self.k,h*self.k,link),]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noutput text with automatic or explicit line breaks", "response": "def multi_cell(self, w, h, txt='', border=0, align='J', fill=0, split_only=False):\n        \"Output text with automatic or explicit line breaks\"\n        txt = self.normalize_text(txt)\n        ret = [] # if split_only = True, returns splited text cells\n        cw=self.current_font['cw']\n        if(w==0):\n            w=self.w-self.r_margin-self.x\n        wmax=(w-2*self.c_margin)*1000.0/self.font_size\n        s=txt.replace(\"\\r\",'')\n        nb=len(s)\n        if(nb>0 and s[nb-1]==\"\\n\"):\n            nb-=1\n        b=0\n        if(border):\n            if(border==1):\n                border='LTRB'\n                b='LRT'\n                b2='LR'\n            else:\n                b2=''\n                if('L' in border):\n                    b2+='L'\n                if('R' in border):\n                    b2+='R'\n                if ('T' in border):\n                    b=b2+'T'\n                else:\n                    b=b2\n        sep=-1\n        i=0\n        j=0\n        l=0\n        ns=0\n        nl=1\n        while(i<nb):\n            #Get next character\n            c=s[i]\n            if(c==\"\\n\"):\n                #Explicit line break\n                if(self.ws>0):\n                    self.ws=0\n                    if not split_only:\n                        self._out('0 Tw')\n                if not split_only:\n                    self.cell(w,h,substr(s,j,i-j),b,2,align,fill)\n                else:\n                    ret.append(substr(s,j,i-j))\n                i+=1\n                sep=-1\n                j=i\n                l=0\n                ns=0\n                nl+=1\n                if(border and nl==2):\n                    b=b2\n                continue\n            if(c==' '):\n                sep=i\n                ls=l\n                ns+=1\n            if self.unifontsubset:\n                l += self.get_string_width(c) / self.font_size*1000.0\n            else:\n                l += cw.get(c,0)\n            if(l>wmax):\n                #Automatic line break\n                if(sep==-1):\n                    if(i==j):\n                        i+=1\n                    if(self.ws>0):\n                        self.ws=0\n                        if not split_only:\n                            self._out('0 Tw')\n                    if not split_only:\n                        self.cell(w,h,substr(s,j,i-j),b,2,align,fill)\n                    else:\n                        ret.append(substr(s,j,i-j))\n                else:\n                    if(align=='J'):\n                        if ns>1:\n                            self.ws=(wmax-ls)/1000.0*self.font_size/(ns-1)\n                        else:\n                            self.ws=0\n                        if not split_only:\n                            self._out(sprintf('%.3f Tw',self.ws*self.k))\n                    if not split_only:\n                        self.cell(w,h,substr(s,j,sep-j),b,2,align,fill)\n                    else:\n                        ret.append(substr(s,j,sep-j))\n                    i=sep+1\n                sep=-1\n                j=i\n                l=0\n                ns=0\n                nl+=1\n                if(border and nl==2):\n                    b=b2\n            else:\n                i+=1\n        #Last chunk\n        if(self.ws>0):\n            self.ws=0\n            if not split_only:\n                self._out('0 Tw')\n        if(border and 'B' in border):\n            b+='B'\n        if not split_only:\n            self.cell(w,h,substr(s,j,i-j),b,2,align,fill)\n            self.x=self.l_margin\n        else:\n            ret.append(substr(s,j,i-j))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noutputs text in flowing mode", "response": "def write(self, h, txt='', link=''):\n        \"Output text in flowing mode\"\n        txt = self.normalize_text(txt)\n        cw=self.current_font['cw']\n        w=self.w-self.r_margin-self.x\n        wmax=(w-2*self.c_margin)*1000.0/self.font_size\n        s=txt.replace(\"\\r\",'')\n        nb=len(s)\n        sep=-1\n        i=0\n        j=0\n        l=0\n        nl=1\n        while(i<nb):\n            #Get next character\n            c=s[i]\n            if(c==\"\\n\"):\n                #Explicit line break\n                self.cell(w,h,substr(s,j,i-j),0,2,'',0,link)\n                i+=1\n                sep=-1\n                j=i\n                l=0\n                if(nl==1):\n                    self.x=self.l_margin\n                    w=self.w-self.r_margin-self.x\n                    wmax=(w-2*self.c_margin)*1000.0/self.font_size\n                nl+=1\n                continue\n            if(c==' '):\n                sep=i\n            if self.unifontsubset:\n                l += self.get_string_width(c) / self.font_size*1000.0\n            else:\n                l += cw.get(c,0)\n            if(l>wmax):\n                #Automatic line break\n                if(sep==-1):\n                    if(self.x>self.l_margin):\n                        #Move to next line\n                        self.x=self.l_margin\n                        self.y+=h\n                        w=self.w-self.r_margin-self.x\n                        wmax=(w-2*self.c_margin)*1000.0/self.font_size\n                        i+=1\n                        nl+=1\n                        continue\n                    if(i==j):\n                        i+=1\n                    self.cell(w,h,substr(s,j,i-j),0,2,'',0,link)\n                else:\n                    self.cell(w,h,substr(s,j,sep-j),0,2,'',0,link)\n                    i=sep+1\n                sep=-1\n                j=i\n                l=0\n                if(nl==1):\n                    self.x=self.l_margin\n                    w=self.w-self.r_margin-self.x\n                    wmax=(w-2*self.c_margin)*1000.0/self.font_size\n                nl+=1\n            else:\n                i+=1\n        #Last chunk\n        if(i!=j):\n            self.cell(l/1000.0*self.font_size,h,substr(s,j),0,0,'',0,link)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nput an image on the page", "response": "def image(self, name, x=None, y=None, w=0,h=0,type='',link=''):\n        \"Put an image on the page\"\n        if not name in self.images:\n            #First use of image, get info\n            if(type==''):\n                pos=name.rfind('.')\n                if(not pos):\n                    self.error('image file has no extension and no type was specified: '+name)\n                type=substr(name,pos+1)\n            type=type.lower()\n            if(type=='jpg' or type=='jpeg'):\n                info=self._parsejpg(name)\n            elif(type=='png'):\n                info=self._parsepng(name)\n            else:\n                #Allow for additional formats\n                #maybe the image is not showing the correct extension,\n                #but the header is OK,\n                succeed_parsing = False\n                #try all the parsing functions\n                parsing_functions = [self._parsejpg,self._parsepng,self._parsegif]\n                for pf in parsing_functions:\n                    try:\n                        info = pf(name)\n                        succeed_parsing = True\n                        break;\n                    except:\n                        pass\n                #last resource\n                if not succeed_parsing:\n                    mtd='_parse'+type\n                    if not hasattr(self,mtd):\n                        self.error('Unsupported image type: '+type)\n                    info=getattr(self, mtd)(name)\n                mtd='_parse'+type\n                if not hasattr(self,mtd):\n                    self.error('Unsupported image type: '+type)\n                info=getattr(self, mtd)(name)\n            info['i']=len(self.images)+1\n            self.images[name]=info\n        else:\n            info=self.images[name]\n        #Automatic width and height calculation if needed\n        if(w==0 and h==0):\n            #Put image at 72 dpi\n            w=info['w']/self.k\n            h=info['h']/self.k\n        elif(w==0):\n            w=h*info['w']/info['h']\n        elif(h==0):\n            h=w*info['h']/info['w']\n        # Flowing mode\n        if y is None:\n            if (self.y + h > self.page_break_trigger and not self.in_footer and self.accept_page_break()):\n                #Automatic page break\n                x = self.x\n                self.add_page(self.cur_orientation)\n                self.x = x\n            y = self.y\n            self.y += h\n        if x is None:\n            x = self.x\n        self._out(sprintf('q %.2f 0 0 %.2f %.2f %.2f cm /I%d Do Q',w*self.k,h*self.k,x*self.k,(self.h-(y+h))*self.k,info['i']))\n        if(link):\n            self.link(x,y,w,h,link)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ln(self, h=''):\n        \"Line Feed; default value is last cell height\"\n        self.x=self.l_margin\n        if(isinstance(h, basestring)):\n            self.y+=self.lasth\n        else:\n            self.y+=h", "response": "Line Feed ; default value is last cell height"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting y position and reset x", "response": "def set_y(self, y):\n        \"Set y position and reset x\"\n        self.x=self.l_margin\n        if(y>=0):\n            self.y=y\n        else:\n            self.y=self.h+y"}
