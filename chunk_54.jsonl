{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_probes():\n        try:\n            all_mbeds = ConnectHelper.get_sessions_for_all_connected_probes(blocking=False)\n            status = 0\n            error = \"\"\n        except Exception as e:\n            all_mbeds = []\n            status = 1\n            error = str(e)\n            if not self.args.output_json:\n                raise\n\n        boards = []\n        obj = {\n            'pyocd_version' : __version__,\n            'version' : { 'major' : 1, 'minor' : 0 },\n            'status' : status,\n            'boards' : boards,\n            }\n\n        if status != 0:\n            obj['error'] = error\n\n        for mbed in all_mbeds:\n            d = {\n                'unique_id' : mbed.probe.unique_id,\n                'info' : mbed.board.description,\n                'board_name' : mbed.board.name,\n                'target' : mbed.board.target_type,\n                'vendor_name' : mbed.probe.vendor_name,\n                'product_name' : mbed.probe.product_name,\n                }\n            boards.append(d)\n\n        return obj", "response": "Generate a dictionary with info about the connected debug probes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates dictionary with info about supported boards.", "response": "def list_boards():\n        \"\"\"! @brief Generate dictionary with info about supported boards.\n        \n        Output version history:\n        - 1.0, initial version\n        \"\"\"\n        boards = []\n        obj = {\n            'pyocd_version' : __version__,\n            'version' : { 'major' : 1, 'minor' : 0 },\n            'status' : 0,\n            'boards' : boards\n            }\n\n        for board_id, info in BOARD_ID_TO_INFO.items():\n            d = {\n                'id' : board_id,\n                'name' : info.name,\n                'target': info.target,\n                'binary' : info.binary,\n                }\n            boards.append(d)\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_targets():\n        targets = []\n        obj = {\n            'pyocd_version' : __version__,\n            'version' : { 'major' : 1, 'minor' : 2 },\n            'status' : 0,\n            'targets' : targets\n            }\n\n        for name in TARGET.keys():\n            s = Session(None) # Create empty session\n            t = TARGET[name](s)\n            d = {\n                'name' : name,\n                'vendor' : t.vendor,\n                'part_families' : t.part_families,\n                'part_number' : t.part_number,\n                'source': 'pack' if hasattr(t, '_pack_device') else 'builtin',\n                }\n            if t._svd_location is not None:\n                svdPath = t._svd_location.filename\n                if isinstance(svdPath, six.string_types) and os.path.exists(svdPath):\n                    d['svd_path'] = svdPath\n            targets.append(d)\n        \n        # Add targets from cmsis-pack-manager cache.\n        for dev in pack_target.ManagedPacks.get_installed_targets():\n            try:\n                targets.append({\n                    'name' : dev.part_number.lower(),\n                    'part_families' : dev.families,\n                    'part_number' : dev.part_number,\n                    'vendor' : dev.vendor,\n                    'source' : 'pack',\n                    })\n            except KeyError:\n                pass\n\n        return obj", "response": "Generate dictionary with info about all supported targets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_progress(file=None):\n    \n    if file is None:\n        file = sys.stdout\n    try:\n        istty = os.isatty(file.fileno())\n    except (OSError, AttributeError):\n        # Either the file doesn't have a fileno method, or calling it returned an\n        # error. In either case, just assume we're not connected to a TTY.\n        istty = False\n    \n    klass = ProgressReportTTY if istty else ProgressReportNoTTY\n    return klass(file)", "response": "A helper function to print a progress report for a single item."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the flash algorithm in RAM and initialize the flash algorithm in RAM.", "response": "def init(self):\n        \"\"\"\n        Download the flash algorithm in RAM\n        \"\"\"\n\n        self.target.halt()\n        self.target.reset_and_halt()\n\n        # update core register to execute the init subroutine\n\n        result = self._call_function_and_wait(self.flash_algo['pc_init'], init=True)\n\n        # check the return code\n        if result != 0:\n            logging.error('init error: %i', result)\n\n        # erase the cookie which take up one page\n        self.erase_sector(0x01000000)\n        time.sleep(.5)\n\n        #do a hardware reset which will put the pc looping in rom\n        self.target.dp.reset()\n        time.sleep(1.3)\n\n        # reconnect to the board\n        self.target.dp.init()\n        self.target.dp.power_up_debug()\n\n        self.target.halt()\n        self.target.reset_and_halt()\n\n        # update core register to execute the init subroutine\n        result = self._call_function_and_wait(self.flash_algo['pc_init'], init=True)\n\n        # check the return code\n        if result != 0:\n            logging.error('init error: %i', result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_type(value, expected_type):\n    if not isinstance(value, expected_type):\n        raise TypeError(\"Value {value!r} has unexpected type {actual_type!r}, expected {expected_type!r}\".format(\n            value=value,\n            expected_type=expected_type,\n            actual_type=type(value),\n        ))\n    return value", "response": "Perform type checking on the provided value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the volumes present on the system Ex. [ mount_point target_id_usb_id... ]", "response": "def _get_cached_mounted_points():\n    \"\"\"! Get the volumes present on the system\n    @return List of mount points and their associated target id\n      Ex. [{ 'mount_point': 'D:', 'target_id_usb_id': 'xxxx'}, ...]\n    \"\"\"\n    result = []\n    try:\n        # Open the registry key for mounted devices\n        mounted_devices_key = winreg.OpenKey(\n            winreg.HKEY_LOCAL_MACHINE, \"SYSTEM\\\\MountedDevices\"\n        )\n        for v in _iter_vals(mounted_devices_key):\n            # Valid entries have the following format: \\DosDevices\\D:\n            if \"DosDevices\" not in v[0]:\n                continue\n\n            volume_string = v[1].decode(\"utf-16le\", \"ignore\")\n            if not _is_mbed_volume(volume_string):\n                continue\n\n            mount_point_match = re.match(\".*\\\\\\\\(.:)$\", v[0])\n\n            if not mount_point_match:\n                logger.debug(\"Invalid disk pattern for entry %s, skipping\", v[0])\n                continue\n\n            mount_point = mount_point_match.group(1)\n\n            result.append({\"mount_point\": mount_point, \"volume_string\": volume_string})\n    except OSError:\n        logger.error('Failed to open \"MountedDevices\" in registry')\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _vid_pid_path_to_usb_info(vid_pid_path):\n    result = {\"vendor_id\": None, \"product_id\": None}\n\n    for component in vid_pid_path.split(\"&\"):\n        component_part = component.lower().split(\"_\")\n\n        if len(component_part) != 2:\n            logger.debug(\"Unexpected VID/PID string structure %s\", component)\n            break\n\n        if component_part[0] == \"vid\":\n            result[\"vendor_id\"] = component_part[1]\n        elif component_part[0] == \"pid\":\n            result[\"product_id\"] = component_part[1]\n\n    return result", "response": "Return the vendor ID and product ID of a device based on its entry in the registry\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iter_keys_as_str(key):\n    for i in range(winreg.QueryInfoKey(key)[0]):\n        yield winreg.EnumKey(key, i)", "response": "Iterate over subkeys returning subkey as string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating over all the keys in a keysequence.", "response": "def _iter_keys(key):\n    \"\"\"! Iterate over subkeys of a key\n    \"\"\"\n    for i in range(winreg.QueryInfoKey(key)[0]):\n        yield winreg.OpenKey(key, winreg.EnumKey(key, i))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over all values of a key in the registry.", "response": "def _iter_vals(key):\n    \"\"\"! Iterate over values of a key\n    \"\"\"\n    for i in range(winreg.QueryInfoKey(key)[1]):\n        yield winreg.EnumValue(key, i)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mount_point_ready(self, path):\n        stdout, stderr, retcode = self._run_cli_process(\"dir %s\" % path)\n        result = True if retcode == 0 else False\n\n        return result", "response": "Check if a given path exists and is ready for file operations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_pyocd_flash_algo(self, blocksize, ram_region):\n        instructions = self._FLASH_BLOB_HEADER + byte_list_to_u32le_list(self.algo_data)\n\n        offset = 0\n\n        # Stack\n        offset += FLASH_ALGO_STACK_SIZE\n        addr_stack = ram_region.start + offset\n\n        # Load address\n        addr_load = ram_region.start + offset\n        offset += len(instructions) * 4\n\n        # Data buffer 1\n        addr_data = ram_region.start + offset\n        offset += blocksize\n\n        if offset > ram_region.length:\n            # Not enough space for flash algorithm\n            LOG.warning(\"Not enough space for flash algorithm\")\n            return None\n\n        # Data buffer 2\n        addr_data2 = ram_region.start + offset\n        offset += blocksize\n\n        if offset > ram_region.length:\n            page_buffers = [addr_data]\n        else:\n            page_buffers = [addr_data, addr_data2]\n\n        # TODO - analyzer support\n\n        code_start = addr_load + self._FLASH_BLOB_HEADER_SIZE\n        flash_algo = {\n            \"load_address\": addr_load,\n            \"instructions\": instructions,\n            \"pc_init\": code_start + self.symbols[\"Init\"],\n            \"pc_unInit\": code_start + self.symbols[\"UnInit\"],\n            \"pc_eraseAll\": code_start + self.symbols[\"EraseChip\"],\n            \"pc_erase_sector\": code_start + self.symbols[\"EraseSector\"],\n            \"pc_program_page\": code_start + self.symbols[\"ProgramPage\"],\n            \"page_buffers\": page_buffers,\n            \"begin_data\": page_buffers[0],\n            \"begin_stack\": addr_stack,\n            \"static_base\": code_start + self.rw_start,\n            \"min_program_length\": self.page_size,\n            \"analyzer_supported\": False\n        }\n        return flash_algo", "response": "This method returns a dictionary representing a pyOCD flash algorithm."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _algo_check_for_section_problems(self, ro_rw_zi):\n        s_ro, s_rw, s_zi = ro_rw_zi\n        if s_ro is None:\n            return \"RO section is missing\"\n        if s_rw is None:\n            return \"RW section is missing\"\n        if s_zi is None:\n            return \"ZI section is missing\"\n        if s_ro.start != 0:\n            return \"RO section does not start at address 0\"\n        if s_ro.start + s_ro.length != s_rw.start:\n            return \"RW section does not follow RO section\"\n        if s_rw.start + s_rw.length != s_zi.start:\n            return \"ZI section does not follow RW section\"\n        return None", "response": "Checks if the layout of the RO and RW sections are well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_algo_bin(self, ro_rw_zi):\n        sect_ro, sect_rw, sect_zi = ro_rw_zi\n        algo_size = sect_ro.length + sect_rw.length + sect_zi.length\n        algo_data = bytearray(algo_size)\n        for section in (sect_ro, sect_rw):\n            start = section.start\n            size = section.length\n            data = section.data\n            assert len(data) == size\n            algo_data[start:start + size] = data\n        return algo_data", "response": "Create a binary blob of the flash algo which can execute from ram"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sector_and_sz_itr(self, elf, data_start):\n        for entry_start in itertools.count(data_start, self.FLASH_SECTORS_STRUCT_SIZE):\n            data = elf.read(entry_start, self.FLASH_SECTORS_STRUCT_SIZE)\n            size, start = struct.unpack(self.FLASH_SECTORS_STRUCT, data)\n            start_and_size = start, size\n            if start_and_size == (self.SECTOR_END, self.SECTOR_END):\n                return\n            yield start_and_size", "response": "Iterator which returns starting address and sector size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading data from the elf file.", "response": "def read(self, addr, size):\n        \"\"\"! @brief Read program data from the elf file.\n\n        @param addr Physical address (load address) to read from.\n        @param size Number of bytes to read.\n        @return Requested data or None if address is unmapped.\n        \"\"\"\n        for segment in self._elf.iter_segments():\n            seg_addr = segment[\"p_paddr\"]\n            seg_size = min(segment[\"p_memsz\"], segment[\"p_filesz\"])\n            if addr >= seg_addr + seg_size:\n                continue\n            if addr + size <= seg_addr:\n                continue\n            # There is at least some overlap\n\n            if addr >= seg_addr and addr + size <= seg_addr + seg_size:\n                # Region is fully contained\n                data = segment.data()\n                start = addr - seg_addr\n                return data[start:start + size]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the flash algo for performing operations.", "response": "def init(self, operation, address=None, clock=0, reset=True):\n        \"\"\"!\n        @brief Prepare the flash algorithm for performing operations.\n        \n        First, the target is prepared to execute flash algo operations, including loading the algo\n        to target RAM. This step is skipped if the target is already prepared, i.e., init() has been\n        called, but cleanup() not called yet.\n        \n        Next, the algo's Init() function is called with the provided parameters. If the algo does\n        not have an Init() function, this step is skipped. Calling Init() is also skipped if the\n        algo was previously inited for the same operation without an intervening uninit. If the\n        algo is already inited for a different operation, uninit() is automatically called prior\n        to intiting for the new operation.\n        \n        @exception FlashFailure\n        \"\"\"\n        if address is None:\n            address = self.get_flash_info().rom_start\n        \n        assert isinstance(operation, self.Operation)\n        assert (self._did_prepare_target) or (not self._did_prepare_target and self._active_operation is None)\n        \n        self.target.halt()\n        \n        # Handle the algo already being inited.\n        if self._active_operation is not None:\n            # Uninit if the algo was left inited for a different operation.\n            if self._active_operation != operation:\n                self.uninit()\n            # Don't need to reinit for the same operation.\n            else:\n                return\n\n        # Setup target for running the flash algo.\n        if not self._did_prepare_target:\n            if reset:\n                self.target.reset_and_halt(Target.ResetType.SW)\n            self.prepare_target()\n\n            # Load flash algo code into target RAM.\n            self.target.write_memory_block32(self.flash_algo['load_address'], self.flash_algo['instructions'])\n\n            self._did_prepare_target = True\n\n        # update core register to execute the init subroutine\n        if self._is_api_valid('pc_init'):\n            result = self._call_function_and_wait(self.flash_algo['pc_init'],\n                                              r0=address, r1=clock, r2=operation.value, init=True)\n\n            # check the return code\n            if result != 0:\n                raise FlashFailure('init error: %i' % result, result_code=result)\n        \n        self._active_operation = operation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nerasing all the flash.", "response": "def erase_all(self):\n        \"\"\"!\n        @brief Erase all the flash.\n        \n        @exception FlashEraseFailure\n        \"\"\"\n        assert self._active_operation == self.Operation.ERASE\n        assert self.is_erase_all_supported\n\n        # update core register to execute the erase_all subroutine\n        result = self._call_function_and_wait(self.flash_algo['pc_eraseAll'])\n\n        # check the return code\n        if result != 0:\n            raise FlashEraseFailure('erase_all error: %i' % result, result_code=result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef program_page(self, address, bytes):\n        assert self._active_operation == self.Operation.PROGRAM\n\n        # prevent security settings from locking the device\n        bytes = self.override_security_bits(address, bytes)\n\n        # first transfer in RAM\n        self.target.write_memory_block8(self.begin_data, bytes)\n\n        # update core register to execute the program_page subroutine\n        result = self._call_function_and_wait(self.flash_algo['pc_program_page'], address, len(bytes), self.begin_data)\n\n        # check the return code\n        if result != 0:\n            raise FlashProgramFailure('program_page(0x%x) error: %i' % (address, result), address, result)", "response": "Program the memory of the current flash page."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_program_page_with_buffer(self, buffer_number, address):\n        assert buffer_number < len(self.page_buffers), \"Invalid buffer number\"\n        assert self._active_operation == self.Operation.PROGRAM\n\n        # get info about this page\n        page_info = self.get_page_info(address)\n\n        # update core register to execute the program_page subroutine\n        result = self._call_function(self.flash_algo['pc_program_page'], address, page_info.size, self.page_buffers[buffer_number])", "response": "Start flashing one or more pages with a specific buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload data from a numbered page buffer.", "response": "def load_page_buffer(self, buffer_number, address, bytes):\n        \"\"\"!\n        @brief Load data to a numbered page buffer.\n        \n        This method is used in conjunction with start_program_page_with_buffer() to implement\n        double buffered programming.\n        \"\"\"\n        assert buffer_number < len(self.page_buffers), \"Invalid buffer number\"\n\n        # prevent security settings from locking the device\n        bytes = self.override_security_bits(address, bytes)\n\n        # transfer the buffer to device RAM\n        self.target.write_memory_block8(self.page_buffers[buffer_number], bytes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef program_phrase(self, address, bytes):\n        assert self._active_operation == self.Operation.PROGRAM\n\n        # Get min programming length. If one was not specified, use the page size.\n        if self.min_program_length:\n            min_len = self.min_program_length\n        else:\n            min_len = self.get_page_info(address).size\n\n        # Require write address and length to be aligned to min write size.\n        if flashPtr % min_len:\n            raise FlashFailure(\"unaligned flash write address\")\n        if len(bytes) % min_len:\n            raise FlashFailure(\"phrase length is unaligned or too small\")\n\n        # prevent security settings from locking the device\n        bytes = self.override_security_bits(address, bytes)\n\n        # first transfer in RAM\n        self.target.write_memory_block8(self.begin_data, bytes)\n\n        # update core register to execute the program_page subroutine\n        result = self._call_function_and_wait(self.flash_algo['pc_program_page'], address, len(bytes), self.begin_data)\n\n        # check the return code\n        if result != 0:\n            raise FlashProgramFailure('program_phrase(0x%x) error: %i' % (address, result), address, result)", "response": "Program the phrase of the flash page at the specified address and bytes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget info about the sector that contains this address.", "response": "def get_sector_info(self, addr):\n        \"\"\"!\n        @brief Get info about the sector that contains this address.\n        \"\"\"\n        assert self.region is not None\n        if not self.region.contains_address(addr):\n            return None\n\n        info = SectorInfo()\n        info.erase_weight = self.region.erase_sector_weight\n        info.size = self.region.blocksize\n        info.base_addr = addr - (addr % info.size)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_page_info(self, addr):\n        assert self.region is not None\n        if not self.region.contains_address(addr):\n            return None\n\n        info = PageInfo()\n        info.program_weight = self.region.program_page_weight\n        info.size = self.region.page_size\n        info.base_addr = addr - (addr % info.size)\n        return info", "response": "Get info about the page that contains this address."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets info about the flash.", "response": "def get_flash_info(self):\n        \"\"\"!\n        @brief Get info about the flash.\n\n        Override this method to return different values.\n        \"\"\"\n        assert self.region is not None\n\n        info = FlashInfo()\n        info.rom_start = self.region.start\n        info.erase_weight = self.region.erase_all_weight\n        info.crc_supported = self.use_analyzer\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflash a block of data.", "response": "def flash_block(self, addr, data, smart_flash=True, chip_erase=None, progress_cb=None, fast_verify=False):\n        \"\"\"!\n        @brief Flash a block of data.\n        \"\"\"\n        assert self.region is not None\n        assert self.region.contains_range(start=addr, length=len(data))\n        \n        fb = FlashBuilder(self)\n        fb.add_data(addr, data)\n        info = fb.program(chip_erase, progress_cb, smart_flash, fast_verify)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting until the breakpoint is hit.", "response": "def wait_for_completion(self):\n        \"\"\"!\n        @brief Wait until the breakpoint is hit.\n        \"\"\"\n        while self.target.get_state() == Target.TARGET_RUNNING:\n            pass\n\n        if self.flash_algo_debug:\n            regs = self.target.read_core_registers_raw(list(range(19)) + [20])\n            LOG.debug(\"Registers after flash algo: [%s]\", \" \".join(\"%08x\" % r for r in regs))\n\n            expected_fp = self.flash_algo['static_base']\n            expected_sp = self.flash_algo['begin_stack']\n            expected_pc = self.flash_algo['load_address']\n            expected_flash_algo = self.flash_algo['instructions']\n            if self.use_analyzer:\n                expected_analyzer = analyzer\n            final_ipsr = self.target.read_core_register('ipsr')\n            final_fp = self.target.read_core_register('r9')\n            final_sp = self.target.read_core_register('sp')\n            final_pc = self.target.read_core_register('pc')\n            #TODO - uncomment if Read/write and zero init sections can be moved into a separate flash algo section\n            #final_flash_algo = self.target.read_memory_block32(self.flash_algo['load_address'], len(self.flash_algo['instructions']))\n            #if self.use_analyzer:\n            #    final_analyzer = self.target.read_memory_block32(self.flash_algo['analyzer_address'], len(analyzer))\n\n            error = False\n            if final_ipsr != 0:\n                LOG.error(\"IPSR should be 0 but is 0x%x\", final_ipsr)\n                error = True\n            if final_fp != expected_fp:\n                # Frame pointer should not change\n                LOG.error(\"Frame pointer should be 0x%x but is 0x%x\" % (expected_fp, final_fp))\n                error = True\n            if final_sp != expected_sp:\n                # Stack pointer should return to original value after function call\n                LOG.error(\"Stack pointer should be 0x%x but is 0x%x\" % (expected_sp, final_sp))\n                error = True\n            if final_pc != expected_pc:\n                # PC should be pointing to breakpoint address\n                LOG.error(\"PC should be 0x%x but is 0x%x\" % (expected_pc, final_pc))\n                error = True\n            #TODO - uncomment if Read/write and zero init sections can be moved into a separate flash algo section\n            #if not _same(expected_flash_algo, final_flash_algo):\n            #    LOG.error(\"Flash algorithm overwritten!\")\n            #    error = True\n            #if self.use_analyzer and not _same(expected_analyzer, final_analyzer):\n            #    LOG.error(\"Analyzer overwritten!\")\n            #    error = True\n            assert error == False\n            self.target.set_vector_catch(self._saved_vector_catch)\n\n        return self.target.read_core_register('r0')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreduce the amount of data we have to sift through to only AttributeNames include the specified keys and children that contain the AttributeNames specified keys", "response": "def _prune(current, keys):\n    \"\"\" Reduce the amount of data we have to sift through to only\n        include the specified keys, and children that contain the\n        specified keys\n    \"\"\"\n    pruned_current = {k: current[k] for k in keys if k in current}\n    pruned_children = list(\n        filter(\n            None, [_prune(c, keys) for c in current.get(\"IORegistryEntryChildren\", [])]\n        )\n    )\n    keep_current = any(k in current for k in keys) or pruned_children\n    if keep_current:\n        if pruned_children:\n            pruned_current[\"IORegistryEntryChildren\"] = pruned_children\n        return pruned_current\n    else:\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding all of the usb info that we can from this particular IORegistry tree with depth first search (and searching the parent stack....)", "response": "def _dfs_usb_info(obj, parents):\n    \"\"\" Find all of the usb info that we can from this particular IORegistry\n        tree with depth first search (and searching the parent stack....)\n    \"\"\"\n    output = {}\n    if (\n        \"BSD Name\" in obj\n        and obj[\"BSD Name\"].startswith(\"disk\")\n        and mbed_volume_name_match.search(obj[\"IORegistryEntryName\"])\n    ):\n        disk_id = obj[\"BSD Name\"]\n        usb_info = {\"serial\": None}\n        for parent in [obj] + parents:\n            if \"USB Serial Number\" in parent:\n                usb_info[\"serial\"] = parent[\"USB Serial Number\"]\n                break\n        output[disk_id] = usb_info\n    for child in obj.get(\"IORegistryEntryChildren\", []):\n        output.update(_dfs_usb_info(child, [obj] + parents))\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mount_points(self):\n        diskutil_ls = subprocess.Popen(\n            [\"diskutil\", \"list\", \"-plist\"], stdout=subprocess.PIPE\n        )\n        disks = _plist_from_popen(diskutil_ls)\n\n        return {\n            disk[\"DeviceIdentifier\"]: disk.get(\"MountPoint\", None)\n            for disk in disks[\"AllDisksAndPartitions\"]\n        }", "response": "Returns a map of volume_id to mount_point"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _volumes(self):\n\n        # to find all the possible mbed volumes, we look for registry entries\n        # under all possible USB tree which have a \"BSD Name\" that starts with\n        # \"disk\" # (i.e. this is a USB disk), and have a IORegistryEntryName that\n        # matches /\\cmbed/\n        # Once we've found a disk, we can search up for a parent with a valid\n        # serial number, and then search down again to find a tty that's part\n        # of the same composite device\n        # ioreg -a -r -n <usb_controller_name> -l\n        usb_controllers = [\n            # Leaving these here for reference. The code nominally scanned each controller,\n            # but a bug (?) caused it to only pay attention to the last one. That seems to\n            # work fine, so the others are commented out.\n#             \"AppleUSBXHCI\",\n#             \"AppleUSBUHCI\",\n#             \"AppleUSBEHCI\",\n#             \"AppleUSBOHCI\",\n            \"IOUSBHostDevice\",\n        ]\n\n        cmp_par = \"-n\"\n        # For El Captain we need to list all the instances of (-c) rather than\n        # compare names (-n)\n        if self.mac_version >= 10.11:\n            cmp_par = \"-c\"\n\n        for usb_controller in usb_controllers:\n            ioreg_usb = subprocess.Popen(\n                [\"ioreg\", \"-a\", \"-r\", cmp_par, usb_controller, \"-l\"],\n                stdout=subprocess.PIPE,\n            )\n            usb_tree = _plist_from_popen(ioreg_usb)\n\n        r = {}\n\n        for name, obj in enumerate(usb_tree):\n            pruned_obj = _prune(\n                obj,\n                [\n                    \"USB Serial Number\",\n                    \"idVendor\",\n                    \"BSD Name\",\n                    \"IORegistryEntryName\",\n                    \"idProduct\",\n                ],\n            )\n            r.update(_dfs_usb_info(pruned_obj, []))\n\n        return r", "response": "returns a map of all the volumes under the USB tree that are part of the same composite device."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_flash(self):\n        for region in self.memory_map.get_regions_of_type(MemoryType.FLASH):\n            # If a path to an FLM file was set on the region, examine it first.\n            if region.flm is not None:\n                flmPath = self.session.find_user_file(None, [region.flm])\n                if flmPath is not None:\n                    logging.info(\"creating flash algo from: %s\", flmPath)\n                    packAlgo = PackFlashAlgo(flmPath)\n                    if self.session.options.get(\"debug.log_flm_info\", False):\n                        logging.debug(\"Flash algo info: %s\", packAlgo.flash_info)\n                    page_size = packAlgo.page_size\n                    if page_size <= 32:\n                        page_size = min(s[1] for s in packAlgo.sector_sizes)\n                    algo = packAlgo.get_pyocd_flash_algo(\n                            page_size,\n                            self.memory_map.get_first_region_of_type(MemoryType.RAM))\n                \n                    # If we got a valid algo from the FLM, set it on the region. This will then\n                    # be used below.\n                    if algo is not None:\n                        region.algo = algo\n                else:\n                    logging.warning(\"Failed to find FLM file: %s\", region.flm)\n            \n            # If the constructor of the region's flash class takes the flash_algo arg, then we\n            # need the region to have a flash algo dict to pass to it. Otherwise we assume the\n            # algo is built-in.\n            klass = region.flash_class\n            argspec = getargspec(klass.__init__)\n            if 'flash_algo' in argspec.args:\n                if region.algo is not None:\n                    obj = klass(self, region.algo)\n                else:\n                    logging.warning(\"flash region '%s' has no flash algo\" % region.name)\n                    continue\n            else:\n                obj = klass(self)\n            \n            # Set the region in the flash instance.\n            obj.region = region\n            \n            # Store the flash object back into the memory region.\n            region.flash = obj", "response": "This function creates flash objects for all memory regions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_for_cores(self):\n        if not len(self.cores):\n            # Allow the user to override the exception to enable uses like chip bringup.\n            if self.session.options.get('allow_no_cores', False):\n                logging.error(\"No cores were discovered!\")\n            else:\n                raise exceptions.DebugError(\"No cores were discovered!\")", "response": "Check that at least one core was discovered."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_reset_type(value):\n    value = value.lower()\n    if value not in RESET_TYPE_MAP:\n        raise ValueError(\"unexpected value for reset_type option ('%s')\" % value)\n    return RESET_TYPE_MAP[value]", "response": "Convert a reset_type session option value to the Target. ResetType enum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef program(self, file_or_path, file_format=None, **kwargs):\n        isPath = isinstance(file_or_path, six.string_types)\n        \n        # Check for valid path first.\n        if isPath and not os.path.isfile(file_or_path):\n            raise FileNotFoundError_(errno.ENOENT, \"No such file: '{}'\".format(file_or_path))\n        \n        # If no format provided, use the file's extension.\n        if not file_format:\n            if isPath:\n                # Extract the extension from the path.\n                file_format = os.path.splitext(file_or_path)[1][1:]\n                \n                # Explicitly check for no extension.\n                if file_format == '':\n                    raise ValueError(\"file path '{}' does not have an extension and \"\n                                        \"no format is set\".format(file_or_path))\n            else:\n                raise ValueError(\"file object provided but no format is set\")\n        \n        # Check the format is one we understand.\n        if file_format not in self._format_handlers:\n            raise ValueError(\"unknown file format '%s'\" % file_format)\n            \n        self._loader = FlashLoader(self._session,\n                                    progress=self._progress,\n                                    chip_erase=self._chip_erase,\n                                    smart_flash=self._smart_flash,\n                                    trust_crc=self._trust_crc,\n                                    keep_unwritten=self._keep_unwritten)\n        \n        file_obj = None\n        try:\n            # Open the file if a path was provided.\n            if isPath:\n                mode = 'rb'\n                if file_format == 'hex':\n                    # hex file must be read as plain text file\n                    mode = 'r'\n                file_obj = open(file_or_path, mode)\n            else:\n                file_obj = file_or_path\n\n            # Pass to the format-specific programmer.\n            self._format_handlers[file_format](file_obj, **kwargs)\n            self._loader.commit()\n        finally:\n            if isPath and file_obj is not None:\n                file_obj.close()", "response": "Program a flash file into flash."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nerase the specified sectors or sectors in the specified address range.", "response": "def erase(self, addresses=None):\n        \"\"\"! @brief Perform the type of erase operation selected when the object was created.\n        \n        For sector erase mode, an iterable of sector addresses specifications must be provided via\n        the _addresses_ parameter. The address iterable elements can be either strings, tuples,\n        or integers. Tuples must have two elements, the start and end addresses of a range to erase.\n        Integers are simply an address within the single page to erase.\n        \n        String address specifications may be in one of three formats: \"<address>\", \"<start>-<end>\",\n        or \"<start>+<length>\". Each field denoted by angled brackets is an integer literal in\n        either decimal or hex notation.\n        \n        Examples:\n        - \"0x1000\" - erase the one sector at 0x1000\n        - \"0x1000-0x4fff\" - erase sectors from 0x1000 up to but not including 0x5000\n        - \"0x8000+0x800\" - erase sectors starting at 0x8000 through 0x87ff\n        \n        @param self\n        @param addresses List of addresses or address ranges of the sectors to erase.\n        \"\"\"\n        if self._mode == self.Mode.MASS:\n            self._mass_erase()\n        elif self._mode == self.Mode.CHIP:\n            self._chip_erase()\n        elif self._mode == self.Mode.SECTOR and addresses:\n            self._sector_erase(addresses)\n        else:\n            LOG.warning(\"No operation performed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reset_state(self):\n        self._builders = {}\n        self._total_data_size = 0\n        self._progress_offset = 0\n        self._current_progress_fraction = 0", "response": "Clears all state variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a chunk of data to the memory.", "response": "def add_data(self, address, data):\n        \"\"\"! @brief Add a chunk of data to be programmed.\n        \n        The data may cross flash memory region boundaries, as long as the regions are contiguous.\n        \n        @param self\n        @param address Integer address for where the first byte of _data_ should be written.\n        @param data A list of byte values to be programmed at the given address.\n        \n        @return The FlashLoader instance is returned, to allow chaining further add_data()\n            calls or a call to commit().\n        \n        @exception ValueError Raised when the address is not within a flash memory region.\n        @exception RuntimeError Raised if the flash memory region does not have a valid Flash\n            instance associated with it, which indicates that the target connect sequence did\n            not run successfully.\n        \"\"\"\n        while len(data):\n            # Look up flash region.\n            region = self._map.get_region_for_address(address)\n            if region is None:\n                raise ValueError(\"no memory region defined for address 0x%08x\" % address)\n            if not region.is_flash:\n                raise ValueError(\"memory region at address 0x%08x is not flash\" % address)\n        \n            # Get our builder instance.\n            if region in self._builders:\n                builder = self._builders[region]\n            else:\n                if region.flash is None:\n                    raise RuntimeError(\"flash memory region at address 0x%08x has no flash instance\" % address)\n                builder = region.flash.get_flash_builder()\n                builder.log_performance = False\n                self._builders[region] = builder\n        \n            # Add as much data to the builder as is contained by this region.\n            programLength = min(len(data), region.end - address + 1)\n            assert programLength != 0\n            builder.add_data(address, data[:programLength])\n            \n            # Advance.\n            data = data[programLength:]\n            address += programLength\n            self._total_data_size += programLength\n        \n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commit(self):\n        didChipErase = False\n        perfList = []\n        \n        # Iterate over builders we've created and program the data.\n        for builder in sorted(self._builders.values(), key=lambda v: v.flash_start):\n            # Determine this builder's portion of total progress.\n            self._current_progress_fraction = builder.buffered_data_size / self._total_data_size\n            \n            # Program the data.\n            chipErase = self._chip_erase if not didChipErase else False\n            perf = builder.program(chip_erase=chipErase,\n                                    progress_cb=self._progress_cb,\n                                    smart_flash=self._smart_flash,\n                                    fast_verify=self._trust_crc,\n                                    keep_unwritten=self._keep_unwritten)\n            perfList.append(perf)\n            didChipErase = True\n            \n            self._progress_offset += self._current_progress_fraction\n\n        # Report programming statistics.\n        self._log_performance(perfList)\n        \n        # Clear state to allow reuse.\n        self._reset_state()", "response": "This method writes all collected data to flash."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _log_performance(self, perf_list):\n        # Compute overall performance numbers.\n        totalProgramTime = sum(perf.program_time for perf in perf_list)\n        program_byte_count = sum(perf.total_byte_count for perf in perf_list)\n        actual_program_byte_count = sum(perf.program_byte_count for perf in perf_list)\n        actual_program_page_count = sum(perf.program_page_count for perf in perf_list)\n        skipped_byte_count = sum(perf.skipped_byte_count for perf in perf_list)\n        skipped_page_count = sum(perf.skipped_page_count for perf in perf_list)\n        \n        # Compute kbps while avoiding a potential zero-div error.\n        if totalProgramTime == 0:\n            kbps = 0\n        else:\n            kbps = (program_byte_count/1024) / totalProgramTime\n        \n        if any(perf.program_type == FlashBuilder.FLASH_CHIP_ERASE for perf in perf_list):\n            LOG.info(\"Erased chip, programmed %d bytes (%s), skipped %d bytes (%s) at %.02f kB/s\",\n                actual_program_byte_count, get_page_count(actual_program_page_count),\n                skipped_byte_count, get_page_count(skipped_page_count),\n                kbps)\n        else:\n            erase_byte_count = sum(perf.erase_byte_count for perf in perf_list)\n            erase_sector_count = sum(perf.erase_sector_count for perf in perf_list)\n\n            LOG.info(\"Erased %d bytes (%s), programmed %d bytes (%s), skipped %d bytes (%s) at %.02f kB/s\", \n                erase_byte_count, get_sector_count(erase_sector_count),\n                actual_program_byte_count, get_page_count(actual_program_page_count),\n                skipped_byte_count, get_page_count(skipped_page_count),\n                kbps)", "response": "Log a report of programming performance numbers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef program_binary_data(cls, session, address, data):\n        mgr = cls(session)\n        mgr.add_data(address, data)\n        mgr.commit()", "response": "This is a helper routine to program a single chunk of data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert frequency suffix to frequency value string.", "response": "def convert_frequency(value):\n    \"\"\"! @brief Applies scale suffix to frequency value string.\"\"\"\n    value = value.strip()\n    suffix = value[-1].lower()\n    if suffix in ('k', 'm'):\n        value = int(value[:-1])\n        if suffix == 'k':\n            value *= 1000\n        elif suffix == 'm':\n            value *= 1000000\n        return value\n    else:\n        return int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_parser(self):\n        # Create top level argument parser.\n        parser = argparse.ArgumentParser(\n            description='PyOCD debug tools for Arm Cortex devices')\n        subparsers = parser.add_subparsers(title=\"subcommands\", metavar=\"\", dest='cmd')\n        \n        parser.add_argument('-V', '--version', action='version', version=__version__)\n        parser.add_argument('--help-options', action='store_true',\n            help=\"Display available session options.\")\n        \n        # Define logging related options.\n        loggingOptions = argparse.ArgumentParser(description='logging', add_help=False)\n        loggingOptions.add_argument('-v', '--verbose', action='count', default=0,\n            help=\"More logging. Can be specified multiple times.\")\n        loggingOptions.add_argument('-q', '--quiet', action='count', default=0,\n            help=\"Less logging. Can be specified multiple times.\")\n        \n        # Define common options for all subcommands, excluding --verbose and --quiet.\n        commonOptionsNoLogging = argparse.ArgumentParser(description='common', add_help=False)\n        commonOptionsNoLogging.add_argument('-j', '--dir', metavar=\"PATH\", dest=\"project_dir\", default=os.getcwd(),\n            help=\"Set the project directory. Defaults to the directory where pyocd was run.\")\n        commonOptionsNoLogging.add_argument('--config', metavar=\"PATH\",\n            help=\"Specify YAML configuration file. Default is pyocd.yaml or pyocd.yml.\")\n        commonOptionsNoLogging.add_argument(\"--no-config\", action=\"store_true\",\n            help=\"Do not use a configuration file.\")\n        commonOptionsNoLogging.add_argument('--script', metavar=\"PATH\",\n            help=\"Use the specified user script. Defaults to pyocd_user.py.\")\n        commonOptionsNoLogging.add_argument('-O', action='append', dest='options', metavar=\"OPTION=VALUE\",\n            help=\"Set named option.\")\n        commonOptionsNoLogging.add_argument(\"-da\", \"--daparg\", dest=\"daparg\", nargs='+',\n            help=\"Send setting to DAPAccess layer.\")\n        commonOptionsNoLogging.add_argument(\"--pack\", metavar=\"PATH\", action=\"append\",\n            help=\"Path to a CMSIS Device Family Pack.\")\n        \n        # Define common options for all subcommands with --verbose and --quiet.\n        commonOptions = argparse.ArgumentParser(description='common',\n            parents=[loggingOptions, commonOptionsNoLogging], add_help=False)\n        \n        # Common connection related options.\n        connectOptions = argparse.ArgumentParser(description='common', add_help=False)\n        connectOptions.add_argument(\"-u\", \"--uid\", dest=\"unique_id\",\n            help=\"Choose a probe by its unique ID or a substring thereof.\")\n        connectOptions.add_argument(\"-b\", \"--board\", dest=\"board_override\", metavar=\"BOARD\",\n            help=\"Set the board type (not yet implemented).\")\n        connectOptions.add_argument(\"-t\", \"--target\", dest=\"target_override\", metavar=\"TARGET\",\n            help=\"Set the target type.\")\n        connectOptions.add_argument(\"-f\", \"--frequency\", dest=\"frequency\", default=1000000, type=convert_frequency,\n            help=\"SWD/JTAG clock frequency in Hz, with optional k/K or m/M suffix for kHz or MHz.\")\n        connectOptions.add_argument(\"-W\", \"--no-wait\", action=\"store_true\",\n            help=\"Do not wait for a probe to be connected if none are available.\")\n\n        # Create *commander* subcommand parser.\n        commandOptions = argparse.ArgumentParser(description='command', add_help=False)\n        commandOptions.add_argument(\"-H\", \"--halt\", action=\"store_true\",\n            help=\"Halt core upon connect.\")\n        commandOptions.add_argument(\"-N\", \"--no-init\", action=\"store_true\",\n            help=\"Do not init debug system.\")\n        commandOptions.add_argument(\"--elf\", metavar=\"PATH\",\n            help=\"Optionally specify ELF file being debugged.\")\n        commandOptions.add_argument(\"-c\", \"--command\", dest=\"commands\", metavar=\"CMD\", action='append', nargs='+',\n            help=\"Run commands.\")\n        subparsers.add_parser('commander', parents=[commonOptions, connectOptions, commandOptions],\n            help=\"Interactive command console.\")\n        subparsers.add_parser('cmd', parents=[commonOptions, connectOptions, commandOptions],\n            help=\"Alias for 'commander'.\")\n\n        # Create *erase* subcommand parser.\n        eraseParser = subparsers.add_parser('erase', parents=[commonOptions, connectOptions],\n            help=\"Erase entire device flash or specified sectors.\",\n            epilog=\"If no position arguments are listed, then no action will be taken unless the --chip or \"\n            \"--mass-erase options are provided. Otherwise, the positional arguments should be the addresses of flash \"\n            \"sectors or address ranges. The end address of a range is exclusive, meaning that it will not be \"\n            \"erased. Thus, you should specify the address of the sector after the last one \"\n            \"to be erased. If a '+' is used instead of '-' in a range, this indicates that the \"\n            \"second value is a length rather than end address. \"\n            \"Examples: 0x1000 (erase single sector starting at 0x1000) \"\n            \"0x800-0x2000 (erase sectors starting at 0x800 up to but not including 0x2000) \"\n            \"0+8192 (erase 8 kB starting at address 0)\")\n        eraseParser.add_argument(\"-c\", \"--chip\", dest=\"erase_mode\", action=\"store_const\", const=loader.FlashEraser.Mode.CHIP,\n            help=\"Perform a chip erase.\")\n        eraseParser.add_argument(\"-s\", \"--sector\", dest=\"erase_mode\", action=\"store_const\", const=loader.FlashEraser.Mode.SECTOR,\n            help=\"Erase the sectors listed as positional arguments.\")\n        eraseParser.add_argument(\"--mass-erase\", dest=\"erase_mode\", action=\"store_const\", const=loader.FlashEraser.Mode.MASS,\n            help=\"Perform a mass erase. On some devices this is different than a chip erase.\")\n        eraseParser.add_argument(\"addresses\", metavar=\"<sector-address>\", action='append', nargs='*',\n            help=\"List of sector addresses or ranges to erase.\")\n\n        # Create *flash* subcommand parser.\n        flashParser = subparsers.add_parser('flash', parents=[commonOptions, connectOptions],\n            help=\"Program an image to device flash.\")\n        flashParser.add_argument(\"-e\", \"--erase\", choices=ERASE_OPTIONS.keys(), default='sector',\n            help=\"Choose flash erase method. Default is sector.\")\n        flashParser.add_argument(\"-a\", \"--base-address\", metavar=\"ADDR\", type=int_base_0,\n            help=\"Base address used for the address where to flash a binary. Defaults to start of flash.\")\n        flashParser.add_argument(\"--trust-crc\", action=\"store_true\",\n            help=\"Use only the CRC of each page to determine if it already has the same data.\")\n        flashParser.add_argument(\"--format\", choices=(\"bin\", \"hex\", \"elf\"),\n            help=\"File format. Default is to use the file's extension.\")\n        flashParser.add_argument(\"--skip\", metavar=\"BYTES\", default=0, type=int_base_0,\n            help=\"Skip programming the first N bytes. This can only be used with binary files.\")\n        flashParser.add_argument(\"file\", metavar=\"PATH\",\n            help=\"File to program into flash.\")\n        \n        # Create *gdbserver* subcommand parser.\n        gdbserverOptions = argparse.ArgumentParser(description='gdbserver', add_help=False)\n        gdbserverOptions.add_argument(\"-p\", \"--port\", dest=\"port_number\", type=int, default=3333,\n            help=\"Set the port number that GDB server will open (default 3333).\")\n        gdbserverOptions.add_argument(\"-T\", \"--telnet-port\", dest=\"telnet_port\", type=int, default=4444,\n            help=\"Specify the telnet port for semihosting (default 4444).\")\n        gdbserverOptions.add_argument(\"--allow-remote\", dest=\"serve_local_only\", default=True, action=\"store_false\",\n            help=\"Allow remote TCP/IP connections (default is no).\")\n        gdbserverOptions.add_argument(\"--persist\", dest=\"persist\", default=False, action=\"store_true\",\n            help=\"Keep GDB server running even after remote has detached.\")\n        gdbserverOptions.add_argument(\"--elf\", metavar=\"PATH\",\n            help=\"Optionally specify ELF file being debugged.\")\n        gdbserverOptions.add_argument(\"-e\", \"--erase\", choices=ERASE_OPTIONS.keys(), default='sector',\n            help=\"Choose flash erase method. Default is sector.\")\n        gdbserverOptions.add_argument(\"--trust-crc\", action=\"store_true\",\n            help=\"Use only the CRC of each page to determine if it already has the same data.\")\n        gdbserverOptions.add_argument(\"-C\", \"--vector-catch\", default='h',\n            help=\"Enable vector catch sources, one letter per enabled source in any order, or 'all' \"\n                \"or 'none'. (h=hard fault, b=bus fault, m=mem fault, i=irq err, s=state err, \"\n                \"c=check err, p=nocp, r=reset, a=all, n=none). Default is hard fault.\")\n        gdbserverOptions.add_argument(\"-S\", \"--semihosting\", dest=\"enable_semihosting\", action=\"store_true\",\n            help=\"Enable semihosting.\")\n        gdbserverOptions.add_argument(\"--step-into-interrupts\", dest=\"step_into_interrupt\", default=False, action=\"store_true\",\n            help=\"Allow single stepping to step into interrupts.\")\n        gdbserverOptions.add_argument(\"-c\", \"--command\", dest=\"commands\", metavar=\"CMD\", action='append', nargs='+',\n            help=\"Run command (OpenOCD compatibility).\")\n        subparsers.add_parser('gdbserver', parents=[commonOptions, connectOptions, gdbserverOptions],\n            help=\"Run the gdb remote server(s).\")\n        subparsers.add_parser('gdb', parents=[commonOptions, connectOptions, gdbserverOptions],\n            help=\"Alias for 'gdbserver'.\")\n\n        # Create *json* subcommand parser.\n        #\n        # The json subcommand does not support --verbose or --quiet since all logging is disabled.\n        jsonParser = subparsers.add_parser('json', parents=[commonOptionsNoLogging],\n            help=\"Output information as JSON.\")\n        group = jsonParser.add_mutually_exclusive_group()\n        group.add_argument('-p', '--probes', action='store_true',\n            help=\"List available probes.\")\n        group.add_argument('-t', '--targets', action='store_true',\n            help=\"List all known targets.\")\n        group.add_argument('-b', '--boards', action='store_true',\n            help=\"List all known boards.\")\n        jsonParser.set_defaults(verbose=0, quiet=0)\n\n        # Create *list* subcommand parser.\n        listParser = subparsers.add_parser('list', parents=[commonOptions],\n            help=\"List information about probes, targets, or boards.\")\n        group = listParser.add_mutually_exclusive_group()\n        group.add_argument('-p', '--probes', action='store_true',\n            help=\"List available probes.\")\n        group.add_argument('-t', '--targets', action='store_true',\n            help=\"List all known targets.\")\n        group.add_argument('-b', '--boards', action='store_true',\n            help=\"List all known boards.\")\n        listParser.add_argument('-n', '--name',\n            help=\"Restrict listing to items matching the given name.\")\n\n        # Create *pack* subcommand parser.\n        packParser = subparsers.add_parser('pack', parents=[loggingOptions],\n            help=\"Manage CMSIS-Packs for target support.\")\n        packParser.add_argument(\"-c\", \"--clean\", action='store_true',\n            help=\"Erase all stored pack information.\")\n        packParser.add_argument(\"-u\", \"--update\", action='store_true',\n            help=\"Update the pack index.\")\n        packParser.add_argument(\"-s\", \"--show\", action='store_true',\n            help=\"Show the list of installed packs.\")\n        packParser.add_argument(\"-f\", \"--find\", dest=\"find_devices\", metavar=\"GLOB\", action='append',\n            help=\"Look up a device part number in the index using a glob pattern. The pattern is \"\n                \"suffixed with '*'. Can be specified multiple times.\")\n        packParser.add_argument(\"-i\", \"--install\", dest=\"install_devices\", metavar=\"GLOB\", action='append',\n            help=\"Download and install pack(s) to support targets matching the glob pattern. \"\n                \"The pattern is suffixed with '*'. Can be specified multiple times.\")\n        packParser.add_argument(\"-n\", \"--no-download\", action='store_true',\n            help=\"Just list the pack(s) that would be downloaded, don't actually download anything.\")\n        \n        self._parser = parser\n        return parser", "response": "Builds the command line parser with all the available session options and subcommands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_logging(self):\n        self._log_level_delta = (self._args.quiet * 10) - (self._args.verbose * 10)\n        level = max(1, self._default_log_level + self._log_level_delta)\n        logging.basicConfig(level=level, format=LOG_FORMAT)", "response": "Configure the logging module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _increase_logging(self, loggers):\n        if self._log_level_delta <= 0:\n            level = max(1, self._default_log_level + self._log_level_delta - 10)\n            for logger in loggers:\n                logging.getLogger(logger).setLevel(level)", "response": "Increase the logging level for a set of loggers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, args=None):\n        try:\n            self._args = self.build_parser().parse_args(args)\n            \n            # Running without a subcommand will print usage.\n            if self._args.cmd is None:\n                if self._args.help_options:\n                    self.show_options_help()\n                else:\n                    self._parser.print_help()\n                return 1\n            \n            # The default log level differs for some subcommands.\n            self._default_log_level = DEFAULT_CMD_LOG_LEVEL[self._args.cmd]\n            self._setup_logging()\n            \n            # Pass any options to DAPAccess.\n            if hasattr(self._args, 'daparg'):\n                DAPAccess.set_args(self._args.daparg)\n\n            # Invoke subcommand.\n            self._COMMANDS[self._args.cmd](self)\n\n            # Successful exit.\n            return 0\n        except KeyboardInterrupt:\n            return 0\n        except exceptions.Error as e:\n            LOG.error(e, exc_info=Session.get_current().log_tracebacks)\n        except Exception as e:\n            LOG.error(\"uncaught exception: %s\", e, exc_info=Session.get_current().log_tracebacks)\n            return 1", "response": "! \\ ~english Main entry point for command line processing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_list(self):\n        # Default to listing probes.\n        if (self._args.probes, self._args.targets, self._args.boards) == (False, False, False):\n            self._args.probes = True\n        \n        # Create a session with no device so we load any config.\n        session = Session(None,\n                            project_dir=self._args.project_dir,\n                            config_file=self._args.config,\n                            no_config=self._args.no_config,\n                            pack=self._args.pack,\n                            **convert_session_options(self._args.options)\n                            )\n        \n        if self._args.probes:\n            ConnectHelper.list_connected_probes()\n        elif self._args.targets:\n            # Create targets from provided CMSIS pack.\n            if ('pack' in session.options) and (session.options['pack'] is not None):\n                pack_target.PackTargets.populate_targets_from_pack(session.options['pack'])\n\n            obj = ListGenerator.list_targets()\n            pt = self._get_pretty_table([\"Name\", \"Vendor\", \"Part Number\", \"Families\", \"Source\"])\n            for info in sorted(obj['targets'], key=lambda i: i['name']):\n                pt.add_row([\n                            info['name'],\n                            info['vendor'],\n                            info['part_number'],\n                            ', '.join(info['part_families']),\n                            info['source'],\n                            ])\n            print(pt)\n        elif self._args.boards:\n            obj = ListGenerator.list_boards()\n            pt = self._get_pretty_table([\"ID\", \"Name\", \"Target\", \"Test Binary\"])\n            for info in sorted(obj['boards'], key=lambda i: i['id']):\n                pt.add_row([\n                            info['id'],\n                            info['name'],\n                            info['target'],\n                            info['binary']\n                            ])\n            print(pt)", "response": "List the CMSIS related probes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_json(self):\n        # Default to listing probes.\n        if (self._args.probes, self._args.targets, self._args.boards) == (False, False, False):\n            self._args.probes = True\n        \n        # Create a session with no device so we load any config.\n        session = Session(None,\n                            project_dir=self._args.project_dir,\n                            config_file=self._args.config,\n                            no_config=self._args.no_config,\n                            pack=self._args.pack,\n                            **convert_session_options(self._args.options)\n                            )\n        \n        if self._args.probes:\n            obj = ListGenerator.list_probes()\n            print(json.dumps(obj, indent=4))\n        elif self._args.targets:\n            # Create targets from provided CMSIS pack.\n            if ('pack' in session.options) and (session.options['pack'] is not None):\n                pack_target.PackTargets.populate_targets_from_pack(session.options['pack'])\n\n            obj = ListGenerator.list_targets()\n            print(json.dumps(obj, indent=4))\n        elif self._args.boards:\n            obj = ListGenerator.list_boards()\n            print(json.dumps(obj, indent=4))", "response": "! @brief Handle 'json' subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_erase(self):\n        self._increase_logging([\"pyocd.tools.loader\", \"pyocd\"])\n        \n        session = ConnectHelper.session_with_chosen_probe(\n                            project_dir=self._args.project_dir,\n                            config_file=self._args.config,\n                            user_script=self._args.script,\n                            no_config=self._args.no_config,\n                            pack=self._args.pack,\n                            unique_id=self._args.unique_id,\n                            target_override=self._args.target_override,\n                            frequency=self._args.frequency,\n                            blocking=False,\n                            **convert_session_options(self._args.options))\n        if session is None:\n            sys.exit(1)\n        with session:\n            mode = self._args.erase_mode or loader.FlashEraser.Mode.SECTOR\n            eraser = loader.FlashEraser(session, mode)\n            \n            addresses = flatten_args(self._args.addresses)\n            eraser.erase(addresses)", "response": "! @brief Handle 'erase' subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_commands(self, commands):\n        if commands is None:\n            return\n        for cmd_list in commands:\n            try:\n                cmd_list = split_command_line(cmd_list)\n                cmd = cmd_list[0]\n                if cmd == 'gdb_port':\n                    if len(cmd_list) < 2:\n                        LOG.error(\"Missing port argument\")\n                    else:\n                        self._args.port_number = int(cmd_list[1], base=0)\n                elif cmd == 'telnet_port':\n                    if len(cmd_list) < 2:\n                        LOG.error(\"Missing port argument\")\n                    else:\n                        self._args.telnet_port = int(cmd_list[1], base=0)\n                elif cmd == 'echo':\n                    self.echo_msg = ' '.join(cmd_list[1:])\n                else:\n                    LOG.error(\"Unsupported command: %s\" % ' '.join(cmd_list))\n            except IndexError:\n                pass", "response": "Handle OpenOCD commands for compatibility."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle the gdbserver subcommand.", "response": "def do_gdbserver(self):\n        \"\"\"! @brief Handle 'gdbserver' subcommand.\"\"\"\n        self._process_commands(self._args.commands)\n\n        gdbs = []\n        try:\n            # Build dict of session options.\n            sessionOptions = convert_session_options(self._args.options)\n            sessionOptions.update({\n                'gdbserver_port' : self._args.port_number,\n                'telnet_port' : self._args.telnet_port,\n                'persist' : self._args.persist,\n                'step_into_interrupt' : self._args.step_into_interrupt,\n                'chip_erase': ERASE_OPTIONS[self._args.erase],\n                'fast_program' : self._args.trust_crc,\n                'enable_semihosting' : self._args.enable_semihosting,\n                'serve_local_only' : self._args.serve_local_only,\n                'vector_catch' : self._args.vector_catch,\n                })\n            \n            session = ConnectHelper.session_with_chosen_probe(\n                blocking=(not self._args.no_wait),\n                project_dir=self._args.project_dir,\n                user_script=self._args.script,\n                config_file=self._args.config,\n                no_config=self._args.no_config,\n                pack=self._args.pack,\n                unique_id=self._args.unique_id,\n                target_override=self._args.target_override,\n                frequency=self._args.frequency,\n                **sessionOptions)\n            if session is None:\n                LOG.error(\"No probe selected.\")\n                return\n            with session:\n                # Set ELF if provided.\n                if self._args.elf:\n                    session.board.target.elf = self._args.elf\n                for core_number, core in session.board.target.cores.items():\n                    gdb = GDBServer(session,\n                        core=core_number,\n                        server_listening_callback=self.server_listening)\n                    gdbs.append(gdb)\n                gdb = gdbs[0]\n                while gdb.isAlive():\n                    gdb.join(timeout=0.5)\n        except (KeyboardInterrupt, Exception):\n            for gdb in gdbs:\n                gdb.stop()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_commander(self):\n        # Flatten commands list then extract primary command and its arguments.\n        if self._args.commands is not None:\n            cmds = []\n            for cmd in self._args.commands:\n                cmds.append(flatten_args(split_command_line(arg) for arg in cmd))\n        else:\n            cmds = None\n\n        # Enter REPL.\n        PyOCDCommander(self._args, cmds).run()", "response": "! @brief Handle 'commander' subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the hexadecimal dump of registers as expected by GDB.", "response": "def get_register_context(self):\n        \"\"\"\n        return hexadecimal dump of registers as expected by GDB\n        \"\"\"\n        logging.debug(\"GDB getting register context\")\n        resp = b''\n        reg_num_list = [reg.reg_num for reg in self._register_list]\n        vals = self._context.read_core_registers_raw(reg_num_list)\n        #print(\"Vals: %s\" % vals)\n        for reg, regValue in zip(self._register_list, vals):\n            if reg.bitsize == 64:\n                resp += six.b(conversion.u64_to_hex16le(regValue))\n            else:\n                resp += six.b(conversion.u32_to_hex8le(regValue))\n            logging.debug(\"GDB reg: %s = 0x%X\", reg.name, regValue)\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset registers from GDB hexadecimal string.", "response": "def set_register_context(self, data):\n        \"\"\"\n        Set registers from GDB hexadecimal string.\n        \"\"\"\n        logging.debug(\"GDB setting register context\")\n        reg_num_list = []\n        reg_data_list = []\n        for reg in self._register_list:\n            if reg.bitsize == 64:\n                regValue = conversion.hex16_to_u64be(data)\n                data = data[16:]\n            else:\n                regValue = conversion.hex8_to_u32be(data)\n                data = data[8:]\n            reg_num_list.append(reg.reg_num)\n            reg_data_list.append(regValue)\n            logging.debug(\"GDB reg: %s = 0x%X\", reg.name, regValue)\n        self._context.write_core_registers_raw(reg_num_list, reg_data_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset single register from GDB hexadecimal string.", "response": "def set_register(self, reg, data):\n        \"\"\"\n        Set single register from GDB hexadecimal string.\n        reg parameter is the index of register in targetXML sent to GDB.\n        \"\"\"\n        if reg < 0:\n            return\n        elif reg < len(self._register_list):\n            regName = self._register_list[reg].name\n            regBits = self._register_list[reg].bitsize\n            if regBits == 64:\n                value = conversion.hex16_to_u64be(data)\n            else:\n                value = conversion.hex8_to_u32be(data)\n            logging.debug(\"GDB: write reg %s: 0x%X\", regName, value)\n            self._context.write_core_register_raw(regName, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_t_response(self, forceSignal=None):\n        if forceSignal is not None:\n            response = six.b('T' + conversion.byte_to_hex2(forceSignal))\n        else:\n            response = six.b('T' + conversion.byte_to_hex2(self.get_signal_value()))\n\n        # Append fp(r7), sp(r13), lr(r14), pc(r15)\n        response += self.get_reg_index_value_pairs([7, 13, 14, 15])\n\n        return response", "response": "Returns a GDB T response string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_reg_index_value_pairs(self, regIndexList):\n        str = b''\n        regList = self._context.read_core_registers_raw(regIndexList)\n        for regIndex, reg in zip(regIndexList, regList):\n            str += six.b(conversion.byte_to_hex2(regIndex) + ':' + conversion.u32_to_hex8le(reg) + ';')\n        return str", "response": "Returns a string like NN : MMMMMMMM ;... for the T response string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_memory_map_xml(self):\n        root = ElementTree.Element('memory-map')\n        for r in  self._context.core.memory_map:\n            # Look up the region type name. Regions default to ram if gdb doesn't\n            # have a concept of the region type.\n            gdbType = GDB_TYPE_MAP.get(r.type, 'ram')\n            \n            start = hex(r.start).rstrip(\"L\")\n            length = hex(r.length).rstrip(\"L\")\n            mem = ElementTree.SubElement(root, 'memory', type=gdbType, start=start, length=length)\n            if r.is_flash:\n                prop = ElementTree.SubElement(mem, 'property', name='blocksize')\n                prop.text = hex(r.blocksize).rstrip(\"L\")\n        return MAP_XML_HEADER + ElementTree.tostring(root)", "response": "Generate GDB memory map XML."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the connected USB devices.", "response": "def _get_interfaces():\n    \"\"\"Get the connected USB devices\"\"\"\n    # Get CMSIS-DAPv1 interfaces.\n    v1_interfaces = INTERFACE[USB_BACKEND].get_all_connected_interfaces()\n    \n    # Get CMSIS-DAPv2 interfaces.\n    v2_interfaces = INTERFACE[USB_BACKEND_V2].get_all_connected_interfaces()\n    \n    # Prefer v2 over v1 if a device provides both.\n    devices_in_both = [v1 for v1 in v1_interfaces for v2 in v2_interfaces\n                        if _get_unique_id(v1) == _get_unique_id(v2)]\n    for dev in devices_in_both:\n        v1_interfaces.remove(dev)\n        \n    # Return the combined list.\n    return v1_interfaces + v2_interfaces"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_response(self, data):\n        assert len(data) == self._size_bytes\n        result = []\n        for i in range(0, self._size_bytes, 4):\n            word = ((data[0 + i] << 0) | (data[1 + i] << 8) |\n                    (data[2 + i] << 16) | (data[3 + i] << 24))\n            result.append(word)\n        self._result = result", "response": "Add data read from the remote device to this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the result of this transfer.", "response": "def get_result(self):\n        \"\"\"\n        Get the result of this transfer.\n        \"\"\"\n        while self._result is None:\n            if len(self.daplink._commands_to_read) > 0:\n                self.daplink._read_packet()\n            else:\n                assert not self.daplink._crnt_cmd.get_empty()\n                self.daplink.flush()\n\n        if self._error is not None:\n            # Pylint is confused and thinks self._error is None\n            # since that is what it is initialized to.\n            # Supress warnings for this.\n            # pylint: disable=raising-bad-type\n            raise self._error\n\n        assert self._result is not None\n        return self._result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_free_words(self, blockAllowed, isRead):\n        if blockAllowed:\n            # DAP_TransferBlock request packet:\n            #   BYTE | BYTE *****| SHORT**********| BYTE *************| WORD *********|\n            # > 0x06 | DAP Index | Transfer Count | Transfer Request  | Transfer Data |\n            #  ******|***********|****************|*******************|+++++++++++++++|\n            send = self._size - 5 - 4 * self._write_count\n\n            # DAP_TransferBlock response packet:\n            #   BYTE | SHORT *********| BYTE *************| WORD *********|\n            # < 0x06 | Transfer Count | Transfer Response | Transfer Data |\n            #  ******|****************|*******************|+++++++++++++++|\n            recv = self._size - 4 - 4 * self._read_count\n\n            if isRead:\n                return recv // 4\n            else:\n                return send // 4\n        else:\n            # DAP_Transfer request packet:\n            #   BYTE | BYTE *****| BYTE **********| BYTE *************| WORD *********|\n            # > 0x05 | DAP Index | Transfer Count | Transfer Request  | Transfer Data |\n            #  ******|***********|****************|+++++++++++++++++++++++++++++++++++|\n            send = self._size - 3 - 1 * self._read_count - 5 * self._write_count\n\n            # DAP_Transfer response packet:\n            #   BYTE | BYTE **********| BYTE *************| WORD *********|\n            # < 0x05 | Transfer Count | Transfer Response | Transfer Data |\n            #  ******|****************|*******************|+++++++++++++++|\n            recv = self._size - 3 - 4 * self._read_count\n\n            if isRead:\n                # 1 request byte in request packet, 4 data bytes in response packet\n                return min(send, recv // 4)\n            else:\n                # 1 request byte + 4 data bytes\n                return send // 5", "response": "Get the number of free words in the transmit packet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, count, request, data, dap_index):\n        assert self._data_encoded is False\n        if self._dap_index is None:\n            self._dap_index = dap_index\n        assert self._dap_index == dap_index\n\n        if self._block_request is None:\n            self._block_request = request\n        elif request != self._block_request:\n            self._block_allowed = False\n        assert not self._block_allowed or self._block_request == request\n\n        if request & READ:\n            self._read_count += count\n        else:\n            self._write_count += count\n        self._data.append((count, request, data))\n\n        if LOG_PACKET_BUILDS:\n            self._logger.debug(\"add(%d, %02x:%s) -> [wc=%d, rc=%d, ba=%d]\" %\n                (count, request, 'r' if (request & READ) else 'w', self._write_count, self._read_count, self._block_allowed))", "response": "Add a single or block register transfer operation to this command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes the response from a DAP_Transfer CMSIS - DAP command into a byte array of bytes.", "response": "def _decode_transfer_data(self, data):\n        \"\"\"\n        Take a byte array and extract the data from it\n\n        Decode the response returned by a DAP_Transfer CMSIS-DAP command\n        and return it as an array of bytes.\n        \"\"\"\n        assert self.get_empty() is False\n        if data[0] != Command.DAP_TRANSFER:\n            raise ValueError('DAP_TRANSFER response error')\n\n        if data[2] != DAP_TRANSFER_OK:\n            if data[2] == DAP_TRANSFER_FAULT:\n                raise DAPAccessIntf.TransferFaultError()\n            elif data[2] == DAP_TRANSFER_WAIT:\n                raise DAPAccessIntf.TransferTimeoutError()\n            raise DAPAccessIntf.TransferError()\n\n        # Check for count mismatch after checking for DAP_TRANSFER_FAULT\n        # This allows TransferFaultError or TransferTimeoutError to get\n        # thrown instead of TransferFaultError\n        if data[1] != self._read_count + self._write_count:\n            raise DAPAccessIntf.TransferError()\n\n        return data[3:3 + 4 * self._read_count]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencoding this command into a byte array that can be sent by this function.", "response": "def _encode_transfer_block_data(self):\n        \"\"\"\n        Encode this command into a byte array that can be sent\n\n        The data returned by this function is a bytearray in\n        the format that of a DAP_TransferBlock CMSIS-DAP command.\n        \"\"\"\n        assert self.get_empty() is False\n        buf = bytearray(self._size)\n        transfer_count = self._read_count + self._write_count\n        assert not (self._read_count != 0 and self._write_count != 0)\n        assert self._block_request is not None\n        pos = 0\n        buf[pos] = Command.DAP_TRANSFER_BLOCK\n        pos += 1\n        buf[pos] = self._dap_index\n        pos += 1\n        buf[pos] = transfer_count & 0xff\n        pos += 1\n        buf[pos] = (transfer_count >> 8) & 0xff\n        pos += 1\n        buf[pos] = self._block_request\n        pos += 1\n        for count, request, write_list in self._data:\n            assert write_list is None or len(write_list) <= count\n            assert request == self._block_request\n            write_pos = 0\n            if not request & READ:\n                for _ in range(count):\n                    buf[pos] = (write_list[write_pos] >> (8 * 0)) & 0xff\n                    pos += 1\n                    buf[pos] = (write_list[write_pos] >> (8 * 1)) & 0xff\n                    pos += 1\n                    buf[pos] = (write_list[write_pos] >> (8 * 2)) & 0xff\n                    pos += 1\n                    buf[pos] = (write_list[write_pos] >> (8 * 3)) & 0xff\n                    pos += 1\n                    write_pos += 1\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode the response from a DAP_TransferBlock command into a byte array of bytes.", "response": "def _decode_transfer_block_data(self, data):\n        \"\"\"\n        Take a byte array and extract the data from it\n\n        Decode the response returned by a DAP_TransferBlock CMSIS-DAP command\n        and return it as an array of bytes.\n        \"\"\"\n        assert self.get_empty() is False\n        if data[0] != Command.DAP_TRANSFER_BLOCK:\n            raise ValueError('DAP_TRANSFER_BLOCK response error')\n\n        if data[3] != DAP_TRANSFER_OK:\n            if data[3] == DAP_TRANSFER_FAULT:\n                raise DAPAccessIntf.TransferFaultError()\n            elif data[3] == DAP_TRANSFER_WAIT:\n                raise DAPAccessIntf.TransferTimeoutError()\n            raise DAPAccessIntf.TransferError()\n\n        # Check for count mismatch after checking for DAP_TRANSFER_FAULT\n        # This allows TransferFaultError or TransferTimeoutError to get\n        # thrown instead of TransferFaultError\n        transfer_count = data[1] | (data[2] << 8)\n        if transfer_count != self._read_count + self._write_count:\n            raise DAPAccessIntf.TransferError()\n\n        return data[4:4 + 4 * self._read_count]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_data(self):\n        assert self.get_empty() is False\n        self._data_encoded = True\n        if self._block_allowed:\n            data = self._encode_transfer_block_data()\n        else:\n            data = self._encode_transfer_data()\n        return data", "response": "Encode this command into a byte array that can be sent to the client."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_data(self, data):\n        assert self.get_empty() is False\n        assert self._data_encoded is True\n        if self._block_allowed:\n            data = self._decode_transfer_block_data(data)\n        else:\n            data = self._decode_transfer_data(data)\n        return data", "response": "Decode the response data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an array of all mbed boards connected", "response": "def get_connected_devices():\n        \"\"\"\n        Return an array of all mbed boards connected\n        \"\"\"\n        all_daplinks = []\n        all_interfaces = _get_interfaces()\n        for interface in all_interfaces:\n            try:\n                new_daplink = DAPAccessCMSISDAP(None, interface=interface)\n                all_daplinks.append(new_daplink)\n            except DAPAccessIntf.TransferError:\n                logger = logging.getLogger(__name__)\n                logger.error('Failed to get unique id', exc_info=session.Session.get_current().log_tracebacks)\n        return all_daplinks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling or disables deferred transfers for the specified items.", "response": "def set_deferred_transfer(self, enable):\n        \"\"\"\n        Allow transfers to be delayed and buffered\n\n        By default deferred transfers are turned off.  All reads and\n        writes will be completed by the time the function returns.\n\n        When enabled packets are buffered and sent all at once, which\n        increases speed.  When memory is written to, the transfer\n        might take place immediately, or might take place on a future\n        memory write.  This means that an invalid write could cause an\n        exception to occur on a later, unrelated write.  To guarantee\n        that previous writes are complete call the flush() function.\n\n        The behaviour of read operations is determined by the modes\n        READ_START, READ_NOW and READ_END.  The option READ_NOW is the\n        default and will cause the read to flush all previous writes,\n        and read the data immediately.  To improve performance, multiple\n        reads can be made using READ_START and finished later with READ_NOW.\n        This allows the reads to be buffered and sent at once.  Note - All\n        READ_ENDs must be called before a call using READ_NOW can be made.\n        \"\"\"\n        if self._deferred_transfer and not enable:\n            self.flush()\n        self._deferred_transfer = enable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing or reinitalize all the deferred transfer buffers for the current entry.", "response": "def _init_deferred_buffers(self):\n        \"\"\"\n        Initialize or reinitalize all the deferred transfer buffers\n\n        Calling this method will drop all pending transactions\n        so use with care.\n        \"\"\"\n        # List of transfers that have been started, but\n        # not completed (started by write_reg, read_reg,\n        # reg_write_repeat and reg_read_repeat)\n        self._transfer_list = collections.deque()\n        # The current packet - this can contain multiple\n        # different transfers\n        self._crnt_cmd = _Command(self._packet_size)\n        # Packets that have been sent but not read\n        self._commands_to_read = collections.deque()\n        # Buffer for data returned for completed commands.\n        # This data will be added to transfers\n        self._command_response_buf = bytearray()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_packet(self):\n        # Grab command, send it and decode response\n        cmd = self._commands_to_read.popleft()\n        try:\n            raw_data = self._interface.read()\n            raw_data = bytearray(raw_data)\n            decoded_data = cmd.decode_data(raw_data)\n        except Exception as exception:\n            self._abort_all_transfers(exception)\n            raise\n\n        decoded_data = bytearray(decoded_data)\n        self._command_response_buf.extend(decoded_data)\n\n        # Attach data to transfers\n        pos = 0\n        while True:\n            size_left = len(self._command_response_buf) - pos\n            if size_left == 0:\n                # If size left is 0 then the transfer list might\n                # be empty, so don't try to access element 0\n                break\n            transfer = self._transfer_list[0]\n            size = transfer.get_data_size()\n            if size > size_left:\n                break\n\n            self._transfer_list.popleft()\n            data = self._command_response_buf[pos:pos + size]\n            pos += size\n            transfer.add_response(data)\n\n        # Remove used data from _command_response_buf\n        if pos > 0:\n            self._command_response_buf = self._command_response_buf[pos:]", "response": "Reads and decodes a single packet from the device and stores the data in the object that is stored in the _command_response_buf."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a single packet to the interface.", "response": "def _send_packet(self):\n        \"\"\"\n        Send a single packet to the interface\n\n        This function guarentees that the number of packets\n        that are stored in daplink's buffer (the number of\n        packets written but not read) does not exceed the\n        number supported by the given device.\n        \"\"\"\n        cmd = self._crnt_cmd\n        if cmd.get_empty():\n            return\n\n        max_packets = self._interface.get_packet_count()\n        if len(self._commands_to_read) >= max_packets:\n            self._read_packet()\n        data = cmd.encode_data()\n        try:\n            self._interface.write(list(data))\n        except Exception as exception:\n            self._abort_all_transfers(exception)\n            raise\n        self._commands_to_read.append(cmd)\n        self._crnt_cmd = _Command(self._packet_size)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write(self, dap_index, transfer_count,\n               transfer_request, transfer_data):\n        \"\"\"\n        Write one or more commands\n        \"\"\"\n        assert dap_index == 0  # dap index currently unsupported\n        assert isinstance(transfer_count, six.integer_types)\n        assert isinstance(transfer_request, six.integer_types)\n        assert transfer_data is None or len(transfer_data) > 0\n\n        # Create transfer and add to transfer list\n        transfer = None\n        if transfer_request & READ:\n            transfer = _Transfer(self, dap_index, transfer_count,\n                                 transfer_request, transfer_data)\n            self._transfer_list.append(transfer)\n\n        # Build physical packet by adding it to command\n        cmd = self._crnt_cmd\n        is_read = transfer_request & READ\n        size_to_transfer = transfer_count\n        trans_data_pos = 0\n        while size_to_transfer > 0:\n            # Get the size remaining in the current packet for the given request.\n            size = cmd.get_request_space(size_to_transfer, transfer_request, dap_index)\n\n            # This request doesn't fit in the packet so send it.\n            if size == 0:\n                if LOG_PACKET_BUILDS:\n                    self._logger.debug(\"_write: send packet [size==0]\")\n                self._send_packet()\n                cmd = self._crnt_cmd\n                continue\n\n            # Add request to packet.\n            if transfer_data is None:\n                data = None\n            else:\n                data = transfer_data[trans_data_pos:trans_data_pos + size]\n            cmd.add(size, transfer_request, data, dap_index)\n            size_to_transfer -= size\n            trans_data_pos += size\n\n            # Packet has been filled so send it\n            if cmd.get_full():\n                if LOG_PACKET_BUILDS:\n                    self._logger.debug(\"_write: send packet [full]\")\n                self._send_packet()\n                cmd = self._crnt_cmd\n\n        if not self._deferred_transfer:\n            self.flush()\n\n        return transfer", "response": "Write one or more commands to the physical packet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _jtag_to_swd(self):\n        data = [0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff]\n        self._protocol.swj_sequence(data)\n\n        data = [0x9e, 0xe7]\n        self._protocol.swj_sequence(data)\n\n        data = [0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff]\n        self._protocol.swj_sequence(data)\n\n        data = [0x00]\n        self._protocol.swj_sequence(data)", "response": "Send the command to switch from JTAG to SWD"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _abort_all_transfers(self, exception):\n        pending_reads = len(self._commands_to_read)\n        # invalidate _transfer_list\n        for transfer in self._transfer_list:\n            transfer.add_error(exception)\n        # clear all deferred buffers\n        self._init_deferred_buffers()\n        # finish all pending reads and ignore the data\n        # Only do this if the error is a tranfer error.\n        # Otherwise this could cause another exception\n        if isinstance(exception, DAPAccessIntf.TransferError):\n            for _ in range(pending_reads):\n                self._interface.read()", "response": "Abort any ongoing transfers and clear all buffers and abort all pending reads."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all the connected interfaces which matches PyUSB. vid or PyUSB. pid", "response": "def get_all_connected_interfaces():\n        \"\"\"\n        returns all the connected devices which matches PyUSB.vid/PyUSB.pid.\n        returns an array of PyUSB (Interface) objects\n        \"\"\"\n        # find all cmsis-dap devices\n        all_devices = usb.core.find(find_all=True, custom_match=FindDap())\n\n        # iterate on all devices found\n        boards = []\n        for board in all_devices:\n            new_board = PyUSB()\n            new_board.vid = board.idVendor\n            new_board.pid = board.idProduct\n            new_board.product_name = board.product\n            new_board.vendor_name = board.manufacturer\n            new_board.serial_number = board.serial_number\n            boards.append(new_board)\n\n        return boards"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite data on the OUT endpoint associated with the HID interface", "response": "def write(self, data):\n        \"\"\"\n        write data on the OUT endpoint associated to the HID interface\n        \"\"\"\n\n        report_size = self.packet_size\n        if self.ep_out:\n            report_size = self.ep_out.wMaxPacketSize\n\n        for _ in range(report_size - len(data)):\n            data.append(0)\n\n        self.read_sem.release()\n\n        if not self.ep_out:\n            bmRequestType = 0x21              #Host to device request of type Class of Recipient Interface\n            bmRequest = 0x09              #Set_REPORT (HID class-specific request for transferring data over EP0)\n            wValue = 0x200             #Issuing an OUT report\n            wIndex = self.intf_number  #mBed Board interface number for HID\n            self.dev.ctrl_transfer(bmRequestType, bmRequest, wValue, wIndex, data)\n            return\n            #raise ValueError('EP_OUT endpoint is NULL')\n\n        self.ep_out.write(data)\n        #logging.debug('sent: %s', data)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the next available data from the IN endpoint", "response": "def read(self):\n        \"\"\"\n        read data on the IN endpoint associated to the HID interface\n        \"\"\"\n        while len(self.rcv_data) == 0:\n            sleep(0)\n\n        if self.rcv_data[0] is None:\n            raise DAPAccessIntf.DeviceError(\"Device %s read thread exited\" %\n                                            self.serial_number)\n        return self.rcv_data.pop(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose the interface and release resources.", "response": "def close(self):\n        \"\"\"\n        close the interface\n        \"\"\"\n        assert self.closed is False\n\n        log.debug(\"closing interface\")\n        self.closed = True\n        self.read_sem.release()\n        self.thread.join()\n        assert self.rcv_data[-1] is None\n        self.rcv_data = []\n        usb.util.release_interface(self.dev, self.intf_number)\n        if self.kernel_driver_was_attached:\n            try:\n                self.dev.attach_kernel_driver(self.intf_number)\n            except Exception as exception:\n                log.warning('Exception attaching kernel driver: %s',\n                                str(exception))\n        usb.util.dispose_resources(self.dev)\n        self.ep_out = None\n        self.ep_in = None\n        self.dev = None\n        self.intf_number = None\n        self.kernel_driver_was_attached = False\n        self.thread = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resetn(self):\n        #Regular reset will kick NRF out of DBG mode\n        logging.debug(\"target_nrf51.reset: enable reset pin\")\n        self.write_memory(RESET, RESET_ENABLE)\n        #reset\n        logging.debug(\"target_nrf51.reset: trigger nRST pin\")\n        self.reset()", "response": "reset a core. After a call to this function, the core\n        is running"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_mbeds(self):\n        platform_count = {}\n        candidates = list(self.find_candidates())\n        result = []\n        for device in candidates:\n            if not device.get(\"mount_point\", None):\n                continue\n            device[\"target_id\"] = device[\"target_id_usb_id\"]\n            self._update_device_from_fs(device)\n            result.append(device)\n\n        return result", "response": "List details of connected devices\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the device information based on files from its mount_point.", "response": "def _update_device_from_fs(self, device):\n        \"\"\" Updates the device information based on files from its 'mount_point'\n            @param device Dictionary containing device information\n        \"\"\"\n        try:\n            directory_entries = listdir(device[\"mount_point\"])\n\n            # Always try to update using daplink compatible boards processself.\n            # This is done for backwards compatibility.\n            lowercase_directory_entries = [e.lower() for e in directory_entries]\n            if self.MBED_HTM_NAME.lower() in lowercase_directory_entries:\n                self._update_device_from_htm(device)\n\n        except (OSError, IOError) as e:\n            logger.warning(\n                'Marking device with mount point \"%s\" as unmounted due to the '\n                \"following error: %s\",\n                device[\"mount_point\"],\n                e,\n            )\n            device[\"mount_point\"] = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the target_id target_id_mbed_htm platform_name and daplink_info attributes of the device based on the mbed. htm on the device.", "response": "def _update_device_from_htm(self, device):\n        \"\"\"Set the 'target_id', 'target_id_mbed_htm', 'platform_name' and\n        'daplink_*' attributes by reading from mbed.htm on the device\n        \"\"\"\n        htm_target_id, daplink_info = self._read_htm_ids(device[\"mount_point\"])\n        if htm_target_id:\n            device[\"target_id\"] = htm_target_id\n        else:\n            logger.debug(\n                \"Could not read htm on from usb id %s. Falling back to usb id\",\n                device[\"target_id_usb_id\"],\n            )\n            device[\"target_id\"] = device[\"target_id_usb_id\"]\n        device[\"target_id_mbed_htm\"] = htm_target_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning scans mbed. htm to get information about TargetID.", "response": "def _read_htm_ids(self, mount_point):\n        \"\"\"! Function scans mbed.htm to get information about TargetID.\n        @param mount_point mbed mount point (disk / drive letter)\n        @return Function returns targetID, in case of failure returns None.\n        @details Note: This function should be improved to scan variety of boards'\n          mbed.htm files\n        \"\"\"\n        result = {}\n        target_id = None\n        for line in self._htm_lines(mount_point):\n            target_id = target_id or self._target_id_from_htm(line)\n        return target_id, result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _target_id_from_htm(self, line):\n        # Detecting modern mbed.htm file format\n        m = re.search(\"\\\\?code=([a-fA-F0-9]+)\", line)\n        if m:\n            result = m.groups()[0]\n            return result\n        # Last resort, we can try to see if old mbed.htm format is there\n        m = re.search(\"\\\\?auth=([a-fA-F0-9]+)\", line)\n        if m:\n            result = m.groups()[0]\n            return result\n\n        return None", "response": "Extract Target id from a mbed. htm line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_cli_process(cmd, shell=True):\n        from subprocess import Popen, PIPE\n\n        p = Popen(cmd, shell=shell, stdout=PIPE, stderr=PIPE)\n        _stdout, _stderr = p.communicate()\n        return _stdout, _stderr, p.returncode", "response": "Runs command as a process and return stdout stderr and ret code\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles a single trace event.", "response": "def receive(self, event):\n        \"\"\"! @brief Handle a single trace event.\n        \n        Passes the event through the filter() method. If one or more objects are returned, they\n        are then passed to the trace sink connected to this filter (which may be another filter).\n        \n        @param self\n        @param event An instance of TraceEvent or one of its subclasses.\n        \"\"\"\n        event = self.filter(event)\n        if (event is not None) and (self._sink is not None):\n            if isinstance(event, collections.Iterable):\n                for e in event:\n                    self._sink.receive(event)\n            else:\n                self._sink.receive(event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self, sinks):\n        if isinstance(sinks, collections.Iterable):\n            self._sinks = sinks\n        elif sinks not in self._sinks:\n            self._sinks.append(sinks)", "response": "Connect one or more downstream trace event sinks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting an XML attribute with a boolean value.", "response": "def _get_bool_attribute(elem, name, default=False):\n    \"\"\"! @brief Extract an XML attribute with a boolean value.\n    \n    Supports \"true\"/\"false\" or \"1\"/\"0\" as the attribute values. Leading and trailing whitespace\n    is stripped, and the comparison is case-insensitive.\n    \n    @param elem ElementTree.Element object.\n    @param name String for the attribute name.\n    @param default An optional default value if the attribute is missing. If not provided,\n        the default is False.\n    \"\"\"\n    if name not in elem.attrib:\n        return default\n    else:\n        value = elem.attrib[name].strip().lower()\n        if value in (\"true\", \"1\"):\n            return True\n        elif value in (\"false\", \"0\"):\n            return False\n        else:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a file - like object for a file within the pack.", "response": "def get_file(self, filename):\n        \"\"\"! @brief Return file-like object for a file within the pack.\n        \n        @param self\n        @param filename Relative path within the pack. May use forward or back slashes.\n        @return A BytesIO object is returned that contains all of the data from the file\n            in the pack. This is done to isolate the returned file from how the pack was\n            opened (due to particularities of the ZipFile implementation).\n        \"\"\"\n        filename = filename.replace('\\\\', '/')\n        return io.BytesIO(self._pack_file.read(filename))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the memory regions for the device.", "response": "def _build_memory_regions(self):\n        \"\"\"! @brief Creates memory region instances for the device.\n        \n        For each `<memory>` element in the device info, a memory region object is created and\n        added to the `_regions` attribute. IROM or non-writable memories are created as RomRegions\n        by this method. They will be converted to FlashRegions by _build_flash_regions().\n        \"\"\"\n        for elem in self._info.memories:\n            try:\n                # Get the region name, type, and access permissions.\n                if 'name' in elem.attrib:\n                    name = elem.attrib['name']\n                    access = elem.attrib['access']\n                    \n                    if ('p' in access):\n                        type = MemoryType.DEVICE\n                    elif ('w' in access):\n                        type = MemoryType.RAM\n                    else:\n                        type = MemoryType.ROM\n                elif 'id' in elem.attrib:\n                    name = elem.attrib['id']\n                    \n                    if 'RAM' in name:\n                        access = 'rwx'\n                        type = MemoryType.RAM\n                    else:\n                        access = 'rx'\n                        type = MemoryType.ROM\n                else:\n                    continue\n                \n                # Both start and size are required attributes.\n                start = int(elem.attrib['start'], base=0)\n                size = int(elem.attrib['size'], base=0)\n                \n                isDefault = _get_bool_attribute(elem, 'default')\n                isStartup = _get_bool_attribute(elem, 'startup')\n                if isStartup:\n                    self._saw_startup = True\n\n                attrs = {\n                        'name': name,\n                        'start': start,\n                        'length': size,\n                        'access': access,\n                        'is_default': isDefault,\n                        'is_boot_memory': isStartup,\n                        'is_testable': isDefault,\n                        'alias': elem.attrib.get('alias', None),\n                    }\n                \n                # Create the memory region and add to map.\n                region = MEMORY_TYPE_CLASS_MAP[type](**attrs)\n                self._regions.append(region)\n                \n                # Record the first default ram for use in flash algos.\n                if self._default_ram is None and type == MemoryType.RAM and isDefault:\n                    self._default_ram = region\n            except (KeyError, ValueError) as err:\n                # Ignore errors.\n                LOG.debug(\"ignoring error parsing memories for CMSIS-Pack devices %s: %s\",\n                    self.part_number, str(err))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_flash_regions(self):\n        # Must have a default ram.\n        if self._default_ram is None:\n            LOG.warning(\"CMSIS-Pack device %s has no default RAM defined, cannot program flash\" % self.part_number)\n            return\n        \n        # Create flash algo dicts once we have the full memory map.\n        for i, region in enumerate(self._regions):\n            # We're only interested in ROM regions here.\n            if region.type != MemoryType.ROM:\n                continue\n            \n            # Look for matching flash algo.\n            algo = self._find_matching_algo(region)\n            if algo is None:\n                # Must be a mask ROM or non-programmable flash.\n                continue\n\n            # Remove the ROM region that we'll replace with flash region(s).\n            del self._regions[i]\n\n            # Load flash algo from .FLM file.\n            algoData = self.pack.get_file(algo.attrib['name'])\n            packAlgo = PackFlashAlgo(algoData)\n            \n            # Log details of this flash algo if the debug option is enabled.\n            current_session = core.session.Session.get_current()\n            if current_session and current_session.options.get(\"debug.log_flm_info\", False):\n                LOG.debug(\"Flash algo info: %s\", packAlgo.flash_info)\n            \n            # Choose the page size. The check for <=32 is to handle some flash algos with incorrect\n            # page sizes that are too small and probably represent the phrase size.\n            page_size = packAlgo.page_size\n            if page_size <= 32:\n                page_size = min(s[1] for s in packAlgo.sector_sizes)\n            \n            # Construct the pyOCD algo using the largest sector size. We can share the same\n            # algo for all sector sizes.\n            algo = packAlgo.get_pyocd_flash_algo(page_size, self._default_ram)\n\n            # Create a separate flash region for each sector size range.\n            for i, sectorInfo in enumerate(packAlgo.sector_sizes):\n                start, sector_size = sectorInfo\n                if i + 1 >= len(packAlgo.sector_sizes):\n                    nextStart = region.length\n                else:\n                    nextStart, _ = packAlgo.sector_sizes[i + 1]\n                \n                length = nextStart - start\n                start += region.start\n                \n                # If we don't have a boot memory yet, pick the first flash.\n                if not self._saw_startup:\n                    isBoot = True\n                    self._saw_startup = True\n                else:\n                    isBoot = region.is_boot_memory\n                \n                # Construct the flash region.\n                rangeRegion = FlashRegion(name=region.name,\n                                access=region.access,\n                                start=start,\n                                length=length,\n                                sector_size=sector_size,\n                                page_size=page_size,\n                                flm=packAlgo,\n                                algo=algo,\n                                erased_byte_value=packAlgo.flash_info.value_empty,\n                                is_default=region.is_default,\n                                is_boot_memory=isBoot,\n                                is_testable=region.is_testable,\n                                alias=region.alias)\n                self._regions.append(rangeRegion)", "response": "Builds a dictionary of flash regions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_matching_algo(self, region):\n        for algo in self._info.algos:\n            # Both start and size are required attributes.\n            algoStart = int(algo.attrib['start'], base=0)\n            algoSize = int(algo.attrib['size'], base=0)\n            algoEnd = algoStart + algoSize - 1\n        \n            # Check if the region indicated by start..size fits within the algo.\n            if (algoStart <= region.start <= algoEnd) and (algoStart <= region.end <= algoEnd):\n                return algo\n        return None", "response": "Searches for a flash algo covering the regions s address range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a MemoryMap object.", "response": "def memory_map(self):\n        \"\"\"! @brief MemoryMap object.\"\"\"\n        # Lazily construct the memory map.\n        if self._memory_map is None:\n            self._build_memory_regions()\n            self._build_flash_regions()\n        \n            # Warn if there was no boot memory.\n            if not self._saw_startup:\n                LOG.warning(\"CMSIS-Pack device %s has no identifiable boot memory\", self.part_number)\n            \n            self._memory_map = MemoryMap(self._regions)\n        \n        return self._memory_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef svd(self):\n        try:\n            svdPath = self._info.debugs[0].attrib['svd']\n            return self._pack.get_file(svdPath)\n        except (KeyError, IndexError):\n            return None", "response": "! Returns the file - like object for the device s SVD file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default_reset_type(self):\n        try:\n            resetSequence = self._info.debugs[0].attrib['defaultResetSequence']\n            if resetSequence == 'ResetHardware':\n                return Target.ResetType.HW\n            elif resetSequence == 'ResetSystem':\n                return Target.ResetType.SW_SYSRESETREQ\n            elif resetSequence == 'ResetProcessor':\n                return Target.ResetType.SW_VECTRESET\n            else:\n                return Target.ResetType.SW\n        except (KeyError, IndexError):\n            return Target.ResetType.SW", "response": "Return the default reset type for a given resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bitmask(*args):\n    mask = 0\n\n    for a in args:\n        if type(a) is tuple:\n            for b in range(a[1], a[0]+1):\n                mask |= 1 << b\n        elif type(a) is list:\n            for b in a:\n                mask |= 1 << b\n        elif type(a) is int:\n            mask |= 1 << a\n\n    return mask", "response": "A function to generate a logical OR mask for the specified set of bits."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bfx(value, msb, lsb):\n    mask = bitmask((msb, lsb))\n    return (value & mask) >> lsb", "response": "! @brief Extract a value from a bitfield."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bfi(value, msb, lsb, field):\n    mask = bitmask((msb, lsb))\n    value &= ~mask\n    value |= (field << lsb) & mask\n    return value", "response": "Change a bitfield value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest whether two lists of values are the same.", "response": "def same(d1, d2):\n    \"\"\"! @brief Test whether two sequences contain the same values.\n    \n    Unlike a simple equality comparison, this function works as expected when the two sequences\n    are of different types, such as a list and bytearray. The sequences must return\n    compatible types from indexing.\n    \"\"\"\n    if len(d1) != len(d2):\n        return False\n    for i in range(len(d1)):\n        if d1[i] != d2[i]:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self, data):\n        for value in data:\n            self._parser.send(value)\n            self._bytes_parsed += 1", "response": "This method parses SWO data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _flush_events(self):\n        if self._sink is not None:\n            for event in self._pending_events:\n                self._sink.receive(event)\n        self._pending_events = []", "response": "Send all pending events to the event sink."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the event is a pair of data trace events and merge them.", "response": "def _merge_data_trace_events(self, event):\n        \"\"\"! @brief Look for pairs of data trace events and merge.\"\"\"\n        if isinstance(event, events.TraceDataTraceEvent):\n            # Record the first data trace event.\n            if self._pending_data_trace is None:\n                self._pending_data_trace = event\n            else:\n                # We've got the second in a pair. If the comparator numbers are the same, then\n                # we can merge the two events. Otherwise we just add them to the pending event\n                # queue separately.\n                if event.comparator == self._pending_data_trace.comparator:\n                    # Merge the two data trace events.\n                    ev = events.TraceDataTraceEvent(cmpn=event.comparator,\n                        pc=(event.pc or self._pending_data_trace.pc),\n                        addr=(event.address or self._pending_data_trace.address),\n                        value=(event.value or self._pending_data_trace.value),\n                        rnw=(event.is_read or self._pending_data_trace.is_read),\n                        sz=(event.transfer_size or self._pending_data_trace.transfer_size),\n                        ts=self._pending_data_trace.timestamp)\n                else:\n                    ev = self._pending_data_trace\n                self._pending_events.append(ev)\n                self._pending_data_trace = None\n            return True\n        # If we get a non-data-trace event while waiting for a second data trace event, then\n        # just place the pending data trace event in the pending event queue.\n        elif self._pending_data_trace is not None:\n            self._pending_events.append(self._pending_data_trace)\n            self._pending_data_trace = None\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_event(self, event):\n        flush = False\n        \n        # Handle merging data trace events.\n        if self._merge_data_trace_events(event):\n            return\n        \n        if isinstance(event, events.TraceTimestamp):\n            for ev in self._pending_events:\n                ev.timestamp = event.timestamp\n            flush = True\n        else:\n            self._pending_events.append(event)\n            if isinstance(event, events.TraceOverflow):\n                flush = True\n        \n        if flush:\n            self._flush_events()", "response": "Process an event object and decide when to send to the event sink."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _match_cmsis_dap_interface(interface):\n    try:\n        interface_name = usb.util.get_string(interface.device, interface.iInterface)\n        \n        # This tells us whether the interface is CMSIS-DAP, but not whether it's v1 or v2.\n        if (interface_name is None) or (\"CMSIS-DAP\" not in interface_name):\n            return False\n\n        # Now check the interface class to distinguish v1 from v2.\n        if (interface.bInterfaceClass != USB_CLASS_VENDOR_SPECIFIC) \\\n            or (interface.bInterfaceSubClass != 0):\n            return False\n\n        # Must have either 2 or 3 endpoints.\n        if interface.bNumEndpoints not in (2, 3):\n            return False\n        \n        # Endpoint 0 must be bulk out.\n        if not _check_ep(interface, 0, usb.util.ENDPOINT_OUT, usb.util.ENDPOINT_TYPE_BULK):\n            return False\n        \n        # Endpoint 1 must be bulk in.\n        if not _check_ep(interface, 1, usb.util.ENDPOINT_IN, usb.util.ENDPOINT_TYPE_BULK):\n            return False\n        \n        # Endpoint 2 is optional. If present it must be bulk in.\n        if (interface.bNumEndpoints == 3) \\\n            and not _check_ep(interface, 2, usb.util.ENDPOINT_IN, usb.util.ENDPOINT_TYPE_BULK):\n            return False\n        \n        # All checks passed, this is a CMSIS-DAPv2 interface!\n        return True\n\n    except (UnicodeDecodeError, IndexError):\n        # UnicodeDecodeError exception can be raised if the device has a corrupted interface name.\n        # Certain versions of STLinkV2 are known to have this problem. If we can't read the\n        # interface name, there's no way to tell if it's a CMSIS-DAPv2 interface.\n        #\n        # IndexError can be raised if an endpoint is missing.\n        return False", "response": "Returns true if the USB interface is a CMSIS - DAPv2 interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_connected_interfaces():\n        # find all cmsis-dap devices\n        try:\n            all_devices = usb.core.find(find_all=True, custom_match=HasCmsisDapv2Interface())\n        except usb.core.NoBackendError:\n            common.show_no_libusb_warning()\n            return []\n\n        # iterate on all devices found\n        boards = []\n        for board in all_devices:\n            new_board = PyUSBv2()\n            new_board.vid = board.idVendor\n            new_board.pid = board.idProduct\n            new_board.product_name = board.product\n            new_board.vendor_name = board.manufacturer\n            new_board.serial_number = board.serial_number\n            boards.append(new_board)\n\n        return boards", "response": "Get all the connected interfaces."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        assert self.closed is False\n\n        if self.is_swo_running:\n            self.stop_swo()\n        self.closed = True\n        self.rx_stop_event.set()\n        self.read_sem.release()\n        self.thread.join()\n        assert self.rcv_data[-1] is None\n        self.rcv_data = []\n        self.swo_data = []\n        usb.util.release_interface(self.dev, self.intf_number)\n        usb.util.dispose_resources(self.dev)\n        self.ep_out = None\n        self.ep_in = None\n        self.ep_swo = None\n        self.dev = None\n        self.intf_number = None\n        self.thread = None", "response": "! \\ ~english Close the USB interface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_mbed_detector(**kwargs):\n    host_os = platform.system()\n    if host_os == \"Windows\":\n        from .windows import StlinkDetectWindows\n\n        return StlinkDetectWindows(**kwargs)\n    elif host_os == \"Linux\":\n        from .linux import StlinkDetectLinuxGeneric\n\n        return StlinkDetectLinuxGeneric(**kwargs)\n    elif host_os == \"Darwin\":\n        from .darwin import StlinkDetectDarwin\n\n        return StlinkDetectDarwin(**kwargs)\n    else:\n        return None", "response": "Returns a MbedLsTools object that can be used to create host OS specific mbed - lstools objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the current HPROT value used for memory transactions.", "response": "def hprot(self, value):\n        \"\"\"! @brief Setter for current HPROT value used for memory transactions.\n    \n        The bits of HPROT have the following meaning. Not all bits are implemented in all\n        MEM-APs. AHB-Lite only implements HPROT[3:0].\n        \n        HPROT[0] = 1 data access, 0 instr fetch<br/>\n        HPROT[1] = 1 priviledge, 0 user<br/>\n        HPROT[2] = 1 bufferable, 0 non bufferable<br/>\n        HPROT[3] = 1 cacheable/modifable, 0 non cacheable<br/>\n        HPROT[4] = 1 lookup in cache, 0 no cache<br/>\n        HPROT[5] = 1 allocate in cache, 0 no allocate in cache<br/>\n        HPROT[6] = 1 shareable, 0 non shareable<br/>\n        \"\"\"\n        self._hprot = value & (CSW_HPROT_MASK >> CSW_HPROT_SHIFT)\n        \n        self._csw = ((self._csw & ~CSW_HPROT_MASK)\n                            | (self._hprot << CSW_HPROT_SHIFT))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value for the HNONSEC bit in the current CSW register.", "response": "def hnonsec(self, value):\n        \"\"\"! @brief Setter for current HNONSEC value used for memory transactions.\n        \n        Not all MEM-APs support control of HNONSEC. In particular, only the AHB5-AP used for\n        v8-M Cortex-M systems does. The AXI-AP for Cortex-A systems also allows this control.\n    \n        @param value 0 is secure, 1 is non-secure.\n        \"\"\"\n        self._hnonsec = value & (CSW_HNONSEC_MASK >> CSW_HNONSEC_SHIFT)\n        \n        self._csw = ((self._csw & ~CSW_HNONSEC_MASK)\n                            | (self._hnonsec << CSW_HNONSEC_SHIFT))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dev_by_id(self, device_type):\n        dir = os.path.join(\"/dev\", device_type, \"by-id\")\n        if os.path.isdir(dir):\n            to_ret = dict(\n                self._hex_ids([os.path.join(dir, f) for f in os.listdir(dir)])\n            )\n            return to_ret\n        else:\n            logger.error(\n                \"Could not get %s devices by id. \"\n                \"This could be because your Linux distribution \"\n                \"does not use udev, or does not create /dev/%s/by-id \"\n                \"symlinks. Please submit an issue to github.com/\"\n                \"armmbed/mbed-ls.\",\n                device_type,\n                device_type,\n            )\n            return {}", "response": "Get a dict of USBID - > device file in a device class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fat_mounts(self):\n        _stdout, _, retval = self._run_cli_process(\"mount\")\n        if not retval:\n            for line in _stdout.splitlines():\n                if b\"vfat\" in line:\n                    match = self.mmp.search(line.decode(\"utf-8\"))\n                    if match:\n                        yield match.group(\"dev\"), match.group(\"dir\")", "response": "Yields all vfat mounts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _hex_ids(self, dev_list):\n        for dl in dev_list:\n            match = self.nlp.search(dl)\n            if match:\n                yield match.group(\"usbid\"), _readlink(dl)", "response": "Build a USBID map for a device list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to return a text aka unicode object from the given data.", "response": "def _get_unicode(data, force=False):\n    \"\"\"Try to return a text aka unicode object from the given data.\"\"\"\n    if isinstance(data, binary_type):\n        return data.decode('utf-8')\n    elif data is None:\n        return ''\n    elif force:\n        if PY2:\n            return unicode(data)\n        else:\n            return str(data)\n    else:\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts points from given dict and returns a Unicode string containing the lines that match the line protocol introduced in InfluxDB 0. 9. 0.", "response": "def make_lines(data, precision=None):\n    \"\"\"Extract points from given dict.\n\n    Extracts the points from the given dict and returns a Unicode string\n    matching the line protocol introduced in InfluxDB 0.9.0.\n    \"\"\"\n    lines = []\n    static_tags = data.get('tags')\n    for point in data['points']:\n        elements = []\n\n        # add measurement name\n        measurement = _escape_tag(_get_unicode(\n            point.get('measurement', data.get('measurement'))))\n        key_values = [measurement]\n\n        # add tags\n        if static_tags:\n            tags = dict(static_tags)  # make a copy, since we'll modify\n            tags.update(point.get('tags') or {})\n        else:\n            tags = point.get('tags') or {}\n\n        # tags should be sorted client-side to take load off server\n        for tag_key, tag_value in sorted(iteritems(tags)):\n            key = _escape_tag(tag_key)\n            value = _escape_tag_value(tag_value)\n\n            if key != '' and value != '':\n                key_values.append(key + \"=\" + value)\n\n        elements.append(','.join(key_values))\n\n        # add fields\n        field_values = []\n        for field_key, field_value in sorted(iteritems(point['fields'])):\n            key = _escape_tag(field_key)\n            value = _escape_value(field_value)\n\n            if key != '' and value != '':\n                field_values.append(key + \"=\" + value)\n\n        elements.append(','.join(field_values))\n\n        # add timestamp\n        if 'time' in point:\n            timestamp = _get_unicode(str(int(\n                _convert_timestamp(point['time'], precision))))\n            elements.append(timestamp)\n\n        line = ' '.join(elements)\n        lines.append(line)\n\n    return '\\n'.join(lines) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the data source name provided in the data source name classmethod", "response": "def _parse_dsn(dsn):\n    \"\"\"Parse data source name.\n\n    This is a helper function to split the data source name provided in\n    the from_dsn classmethod\n    \"\"\"\n    conn_params = urlparse(dsn)\n    init_args = {}\n    scheme_info = conn_params.scheme.split('+')\n    if len(scheme_info) == 1:\n        scheme = scheme_info[0]\n        modifier = None\n    else:\n        modifier, scheme = scheme_info\n\n    if scheme != 'influxdb':\n        raise ValueError('Unknown scheme \"{0}\".'.format(scheme))\n\n    if modifier:\n        if modifier == 'udp':\n            init_args['use_udp'] = True\n        elif modifier == 'https':\n            init_args['ssl'] = True\n        else:\n            raise ValueError('Unknown modifier \"{0}\".'.format(modifier))\n\n    netlocs = conn_params.netloc.split(',')\n\n    init_args['hosts'] = []\n    for netloc in netlocs:\n        parsed = _parse_netloc(netloc)\n        init_args['hosts'].append((parsed['host'], int(parsed['port'])))\n        init_args['username'] = parsed['username']\n        init_args['password'] = parsed['password']\n\n    if conn_params.path and len(conn_params.path) > 1:\n        init_args['database'] = conn_params.path[1:]\n\n    return init_args"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating an instance of InfluxDBClient from a given data source name.", "response": "def from_dsn(cls, dsn, **kwargs):\n        r\"\"\"Generate an instance of InfluxDBClient from given data source name.\n\n        Return an instance of :class:`~.InfluxDBClient` from the provided\n        data source name. Supported schemes are \"influxdb\", \"https+influxdb\"\n        and \"udp+influxdb\". Parameters for the :class:`~.InfluxDBClient`\n        constructor may also be passed to this method.\n\n        :param dsn: data source name\n        :type dsn: string\n        :param kwargs: additional parameters for `InfluxDBClient`\n        :type kwargs: dict\n        :raises ValueError: if the provided DSN has any unexpected values\n\n        :Example:\n\n        ::\n\n            >> cli = InfluxDBClient.from_dsn('influxdb://username:password@\\\n            localhost:8086/databasename', timeout=5)\n            >> type(cli)\n            <class 'influxdb.client.InfluxDBClient'>\n            >> cli = InfluxDBClient.from_dsn('udp+influxdb://username:pass@\\\n            localhost:8086/databasename', timeout=5, udp_port=159)\n            >> print('{0._baseurl} - {0.use_udp} {0.udp_port}'.format(cli))\n            http://localhost:8086 - True 159\n\n        .. note:: parameters provided in `**kwargs` may override dsn parameters\n        .. note:: when using \"udp+influxdb\" the specified port (if any) will\n            be used for the TCP connection; specify the UDP port with the\n            additional `udp_port` parameter (cf. examples).\n        \"\"\"\n        init_args = _parse_dsn(dsn)\n        host, port = init_args.pop('hosts')[0]\n        init_args['host'] = host\n        init_args['port'] = port\n        init_args.update(kwargs)\n\n        return cls(**init_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef switch_user(self, username, password):\n        self._username = username\n        self._password = password", "response": "Change the client s username."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a HTTP request to the InfluxDB API.", "response": "def request(self, url, method='GET', params=None, data=None,\n                expected_response_code=200, headers=None):\n        \"\"\"Make a HTTP request to the InfluxDB API.\n\n        :param url: the path of the HTTP request, e.g. write, query, etc.\n        :type url: str\n        :param method: the HTTP method for the request, defaults to GET\n        :type method: str\n        :param params: additional parameters for the request, defaults to None\n        :type params: dict\n        :param data: the data of the request, defaults to None\n        :type data: str\n        :param expected_response_code: the expected response code of\n            the request, defaults to 200\n        :type expected_response_code: int\n        :param headers: headers to add to the request\n        :type headers: dict\n        :returns: the response from the request\n        :rtype: :class:`requests.Response`\n        :raises InfluxDBServerError: if the response code is any server error\n            code (5xx)\n        :raises InfluxDBClientError: if the response code is not the\n            same as `expected_response_code` and is not a server error code\n        \"\"\"\n        url = \"{0}/{1}\".format(self._baseurl, url)\n\n        if headers is None:\n            headers = self._headers\n\n        if params is None:\n            params = {}\n\n        if isinstance(data, (dict, list)):\n            data = json.dumps(data)\n\n        # Try to send the request more than once by default (see #103)\n        retry = True\n        _try = 0\n        while retry:\n            try:\n                response = self._session.request(\n                    method=method,\n                    url=url,\n                    auth=(self._username, self._password),\n                    params=params,\n                    data=data,\n                    headers=headers,\n                    proxies=self._proxies,\n                    verify=self._verify_ssl,\n                    timeout=self._timeout\n                )\n                break\n            except (requests.exceptions.ConnectionError,\n                    requests.exceptions.HTTPError,\n                    requests.exceptions.Timeout):\n                _try += 1\n                if self._retries != 0:\n                    retry = _try < self._retries\n                if method == \"POST\":\n                    time.sleep((2 ** _try) * random.random() / 100.0)\n                if not retry:\n                    raise\n        # if there's not an error, there must have been a successful response\n        if 500 <= response.status_code < 600:\n            raise InfluxDBServerError(response.content)\n        elif response.status_code == expected_response_code:\n            return response\n        else:\n            raise InfluxDBClientError(response.content, response.status_code)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, data, params=None, expected_response_code=204,\n              protocol='json'):\n        \"\"\"Write data to InfluxDB.\n\n        :param data: the data to be written\n        :type data: (if protocol is 'json') dict\n                    (if protocol is 'line') sequence of line protocol strings\n                                            or single string\n        :param params: additional parameters for the request, defaults to None\n        :type params: dict\n        :param expected_response_code: the expected response code of the write\n            operation, defaults to 204\n        :type expected_response_code: int\n        :param protocol: protocol of input data, either 'json' or 'line'\n        :type protocol: str\n        :returns: True, if the write operation is successful\n        :rtype: bool\n        \"\"\"\n        headers = self._headers\n        headers['Content-Type'] = 'application/octet-stream'\n\n        if params:\n            precision = params.get('precision')\n        else:\n            precision = None\n\n        if protocol == 'json':\n            data = make_lines(data, precision).encode('utf-8')\n        elif protocol == 'line':\n            if isinstance(data, str):\n                data = [data]\n            data = ('\\n'.join(data) + '\\n').encode('utf-8')\n\n        self.request(\n            url=\"write\",\n            method='POST',\n            params=params,\n            data=data,\n            expected_response_code=expected_response_code,\n            headers=headers\n        )\n        return True", "response": "Write data to InfluxDB."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a query to InfluxDB.", "response": "def query(self,\n              query,\n              params=None,\n              bind_params=None,\n              epoch=None,\n              expected_response_code=200,\n              database=None,\n              raise_errors=True,\n              chunked=False,\n              chunk_size=0,\n              method=\"GET\"):\n        \"\"\"Send a query to InfluxDB.\n\n        .. danger::\n            In order to avoid injection vulnerabilities (similar to `SQL\n            injection <https://www.owasp.org/index.php/SQL_Injection>`_\n            vulnerabilities), do not directly include untrusted data into the\n            ``query`` parameter, use ``bind_params`` instead.\n\n        :param query: the actual query string\n        :type query: str\n\n        :param params: additional parameters for the request,\n            defaults to {}\n        :type params: dict\n\n        :param bind_params: bind parameters for the query:\n            any variable in the query written as ``'$var_name'`` will be\n            replaced with ``bind_params['var_name']``. Only works in the\n            ``WHERE`` clause and takes precedence over ``params['params']``\n        :type bind_params: dict\n\n        :param epoch: response timestamps to be in epoch format either 'h',\n            'm', 's', 'ms', 'u', or 'ns',defaults to `None` which is\n            RFC3339 UTC format with nanosecond precision\n        :type epoch: str\n\n        :param expected_response_code: the expected status code of response,\n            defaults to 200\n        :type expected_response_code: int\n\n        :param database: database to query, defaults to None\n        :type database: str\n\n        :param raise_errors: Whether or not to raise exceptions when InfluxDB\n            returns errors, defaults to True\n        :type raise_errors: bool\n\n        :param chunked: Enable to use chunked responses from InfluxDB.\n            With ``chunked`` enabled, one ResultSet is returned per chunk\n            containing all results within that chunk\n        :type chunked: bool\n\n        :param chunk_size: Size of each chunk to tell InfluxDB to use.\n        :type chunk_size: int\n\n        :param method: the HTTP method for the request, defaults to GET\n        :type method: str\n\n        :returns: the queried data\n        :rtype: :class:`~.ResultSet`\n        \"\"\"\n        if params is None:\n            params = {}\n\n        if bind_params is not None:\n            params_dict = json.loads(params.get('params', '{}'))\n            params_dict.update(bind_params)\n            params['params'] = json.dumps(params_dict)\n\n        params['q'] = query\n        params['db'] = database or self._database\n\n        if epoch is not None:\n            params['epoch'] = epoch\n\n        if chunked:\n            params['chunked'] = 'true'\n            if chunk_size > 0:\n                params['chunk_size'] = chunk_size\n\n        if query.lower().startswith(\"select \") and \" into \" in query.lower():\n            method = \"POST\"\n\n        response = self.request(\n            url=\"query\",\n            method=method,\n            params=params,\n            data=None,\n            expected_response_code=expected_response_code\n        )\n\n        if chunked:\n            return self._read_chunked_response(response)\n\n        data = response.json()\n\n        results = [\n            ResultSet(result, raise_errors=raise_errors)\n            for result\n            in data.get('results', [])\n        ]\n\n        # TODO(aviau): Always return a list. (This would be a breaking change)\n        if len(results) == 1:\n            return results[0]\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a list of points to a single time series name.", "response": "def write_points(self,\n                     points,\n                     time_precision=None,\n                     database=None,\n                     retention_policy=None,\n                     tags=None,\n                     batch_size=None,\n                     protocol='json',\n                     consistency=None\n                     ):\n        \"\"\"Write to multiple time series names.\n\n        :param points: the list of points to be written in the database\n        :type points: list of dictionaries, each dictionary represents a point\n        :type points: (if protocol is 'json') list of dicts, where each dict\n                                            represents a point.\n                    (if protocol is 'line') sequence of line protocol strings.\n        :param time_precision: Either 's', 'm', 'ms' or 'u', defaults to None\n        :type time_precision: str\n        :param database: the database to write the points to. Defaults to\n            the client's current database\n        :type database: str\n        :param tags: a set of key-value pairs associated with each point. Both\n            keys and values must be strings. These are shared tags and will be\n            merged with point-specific tags, defaults to None\n        :type tags: dict\n        :param retention_policy: the retention policy for the points. Defaults\n            to None\n        :type retention_policy: str\n        :param batch_size: value to write the points in batches\n            instead of all at one time. Useful for when doing data dumps from\n            one database to another or when doing a massive write operation,\n            defaults to None\n        :type batch_size: int\n        :param protocol: Protocol for writing data. Either 'line' or 'json'.\n        :type protocol: str\n        :param consistency: Consistency for the points.\n            One of {'any','one','quorum','all'}.\n        :type consistency: str\n        :returns: True, if the operation is successful\n        :rtype: bool\n\n        .. note:: if no retention policy is specified, the default retention\n            policy for the database is used\n        \"\"\"\n        if batch_size and batch_size > 0:\n            for batch in self._batches(points, batch_size):\n                self._write_points(points=batch,\n                                   time_precision=time_precision,\n                                   database=database,\n                                   retention_policy=retention_policy,\n                                   tags=tags, protocol=protocol,\n                                   consistency=consistency)\n            return True\n\n        return self._write_points(points=points,\n                                  time_precision=time_precision,\n                                  database=database,\n                                  retention_policy=retention_policy,\n                                  tags=tags, protocol=protocol,\n                                  consistency=consistency)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks connectivity to InfluxDB.", "response": "def ping(self):\n        \"\"\"Check connectivity to InfluxDB.\n\n        :returns: The version of the InfluxDB the client is connected to\n        \"\"\"\n        response = self.request(\n            url=\"ping\",\n            method='GET',\n            expected_response_code=204\n        )\n\n        return response.headers['X-Influxdb-Version']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a retention policy for a database.", "response": "def create_retention_policy(self, name, duration, replication,\n                                database=None,\n                                default=False, shard_duration=\"0s\"):\n        \"\"\"Create a retention policy for a database.\n\n        :param name: the name of the new retention policy\n        :type name: str\n        :param duration: the duration of the new retention policy.\n            Durations such as 1h, 90m, 12h, 7d, and 4w, are all supported\n            and mean 1 hour, 90 minutes, 12 hours, 7 day, and 4 weeks,\n            respectively. For infinite retention - meaning the data will\n            never be deleted - use 'INF' for duration.\n            The minimum retention period is 1 hour.\n        :type duration: str\n        :param replication: the replication of the retention policy\n        :type replication: str\n        :param database: the database for which the retention policy is\n            created. Defaults to current client's database\n        :type database: str\n        :param default: whether or not to set the policy as default\n        :type default: bool\n        :param shard_duration: the shard duration of the retention policy.\n            Durations such as 1h, 90m, 12h, 7d, and 4w, are all supported and\n            mean 1 hour, 90 minutes, 12 hours, 7 day, and 4 weeks,\n            respectively. Infinite retention is not supported. As a workaround,\n            specify a \"1000w\" duration to achieve an extremely long shard group\n            duration. Defaults to \"0s\", which is interpreted by the database\n            to mean the default value given the duration.\n            The minimum shard group duration is 1 hour.\n        :type shard_duration: str\n        \"\"\"\n        query_string = \\\n            \"CREATE RETENTION POLICY {0} ON {1} \" \\\n            \"DURATION {2} REPLICATION {3} SHARD DURATION {4}\".format(\n                quote_ident(name), quote_ident(database or self._database),\n                duration, replication, shard_duration)\n\n        if default is True:\n            query_string += \" DEFAULT\"\n\n        self.query(query_string, method=\"POST\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef alter_retention_policy(self, name, database=None,\n                               duration=None, replication=None,\n                               default=None, shard_duration=None):\n        \"\"\"Modify an existing retention policy for a database.\n\n        :param name: the name of the retention policy to modify\n        :type name: str\n        :param database: the database for which the retention policy is\n            modified. Defaults to current client's database\n        :type database: str\n        :param duration: the new duration of the existing retention policy.\n            Durations such as 1h, 90m, 12h, 7d, and 4w, are all supported\n            and mean 1 hour, 90 minutes, 12 hours, 7 day, and 4 weeks,\n            respectively. For infinite retention, meaning the data will\n            never be deleted, use 'INF' for duration.\n            The minimum retention period is 1 hour.\n        :type duration: str\n        :param replication: the new replication of the existing\n            retention policy\n        :type replication: int\n        :param default: whether or not to set the modified policy as default\n        :type default: bool\n        :param shard_duration: the shard duration of the retention policy.\n            Durations such as 1h, 90m, 12h, 7d, and 4w, are all supported and\n            mean 1 hour, 90 minutes, 12 hours, 7 day, and 4 weeks,\n            respectively. Infinite retention is not supported. As a workaround,\n            specify a \"1000w\" duration to achieve an extremely long shard group\n            duration.\n            The minimum shard group duration is 1 hour.\n        :type shard_duration: str\n\n        .. note:: at least one of duration, replication, or default flag\n            should be set. Otherwise the operation will fail.\n        \"\"\"\n        query_string = (\n            \"ALTER RETENTION POLICY {0} ON {1}\"\n        ).format(quote_ident(name),\n                 quote_ident(database or self._database), shard_duration)\n        if duration:\n            query_string += \" DURATION {0}\".format(duration)\n        if shard_duration:\n            query_string += \" SHARD DURATION {0}\".format(shard_duration)\n        if replication:\n            query_string += \" REPLICATION {0}\".format(replication)\n        if default is True:\n            query_string += \" DEFAULT\"\n\n        self.query(query_string, method=\"POST\")", "response": "Modify an existing retention policy for a database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping an existing retention policy for a database.", "response": "def drop_retention_policy(self, name, database=None):\n        \"\"\"Drop an existing retention policy for a database.\n\n        :param name: the name of the retention policy to drop\n        :type name: str\n        :param database: the database for which the retention policy is\n            dropped. Defaults to current client's database\n        :type database: str\n        \"\"\"\n        query_string = (\n            \"DROP RETENTION POLICY {0} ON {1}\"\n        ).format(quote_ident(name), quote_ident(database or self._database))\n        self.query(query_string, method=\"POST\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_list_retention_policies(self, database=None):\n        if not (database or self._database):\n            raise InfluxDBClientError(\n                \"get_list_retention_policies() requires a database as a \"\n                \"parameter or the client to be using a database\")\n\n        rsp = self.query(\n            \"SHOW RETENTION POLICIES ON {0}\".format(\n                quote_ident(database or self._database))\n        )\n        return list(rsp.get_points())", "response": "Get the list of retention policies for a database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new user in InfluxDB.", "response": "def create_user(self, username, password, admin=False):\n        \"\"\"Create a new user in InfluxDB.\n\n        :param username: the new username to create\n        :type username: str\n        :param password: the password for the new user\n        :type password: str\n        :param admin: whether the user should have cluster administration\n            privileges or not\n        :type admin: boolean\n        \"\"\"\n        text = \"CREATE USER {0} WITH PASSWORD {1}\".format(\n            quote_ident(username), quote_literal(password))\n        if admin:\n            text += ' WITH ALL PRIVILEGES'\n        self.query(text, method=\"POST\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndrop a user from InfluxDB.", "response": "def drop_user(self, username):\n        \"\"\"Drop a user from InfluxDB.\n\n        :param username: the username to drop\n        :type username: str\n        \"\"\"\n        text = \"DROP USER {0}\".format(quote_ident(username), method=\"POST\")\n        self.query(text, method=\"POST\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_user_password(self, username, password):\n        text = \"SET PASSWORD FOR {0} = {1}\".format(\n            quote_ident(username), quote_literal(password))\n        self.query(text)", "response": "Change the password of an existing user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting all series from a database.", "response": "def delete_series(self, database=None, measurement=None, tags=None):\n        \"\"\"Delete series from a database.\n\n        Series must be filtered by either measurement and tags.\n        This method cannot be used to delete all series, use\n        `drop_database` instead.\n\n        :param database: the database from which the series should be\n            deleted, defaults to client's current database\n        :type database: str\n        :param measurement: Delete all series from a measurement\n        :type measurement: str\n        :param tags: Delete all series that match given tags\n        :type tags: dict\n        \"\"\"\n        database = database or self._database\n        query_str = 'DROP SERIES'\n        if measurement:\n            query_str += ' FROM {0}'.format(quote_ident(measurement))\n\n        if tags:\n            tag_eq_list = [\"{0}={1}\".format(quote_ident(k), quote_literal(v))\n                           for k, v in tags.items()]\n            query_str += ' WHERE ' + ' AND '.join(tag_eq_list)\n        self.query(query_str, database=database, method=\"POST\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grant_admin_privileges(self, username):\n        text = \"GRANT ALL PRIVILEGES TO {0}\".format(quote_ident(username))\n        self.query(text, method=\"POST\")", "response": "Grant administration privileges to a user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef revoke_admin_privileges(self, username):\n        text = \"REVOKE ALL PRIVILEGES FROM {0}\".format(quote_ident(username))\n        self.query(text, method=\"POST\")", "response": "Revoke all administration privileges from a user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef grant_privilege(self, privilege, database, username):\n        text = \"GRANT {0} ON {1} TO {2}\".format(privilege,\n                                                quote_ident(database),\n                                                quote_ident(username))\n        self.query(text, method=\"POST\")", "response": "Grant a privilege to a database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_list_privileges(self, username):\n        text = \"SHOW GRANTS FOR {0}\".format(quote_ident(username))\n        return list(self.query(text).get_points())", "response": "Get the list of all privileges granted to given user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the list of continuous queries in InfluxDB.", "response": "def get_list_continuous_queries(self):\n        \"\"\"Get the list of continuous queries in InfluxDB.\n\n        :return: all CQs in InfluxDB\n        :rtype: list of dictionaries\n\n        :Example:\n\n        ::\n\n            >> cqs = client.get_list_cqs()\n            >> cqs\n            [\n                {\n                    u'db1': []\n                },\n                {\n                    u'db2': [\n                        {\n                            u'name': u'vampire',\n                            u'query': u'CREATE CONTINUOUS QUERY vampire ON '\n                                       'mydb BEGIN SELECT count(dracula) INTO '\n                                       'mydb.autogen.all_of_them FROM '\n                                       'mydb.autogen.one GROUP BY time(5m) END'\n                        }\n                    ]\n                }\n            ]\n        \"\"\"\n        query_string = \"SHOW CONTINUOUS QUERIES\"\n        return [{sk[0]: list(p)} for sk, p in self.query(query_string).items()]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_continuous_query(self, name, select, database=None,\n                                resample_opts=None):\n        r\"\"\"Create a continuous query for a database.\n\n        :param name: the name of continuous query to create\n        :type name: str\n        :param select: select statement for the continuous query\n        :type select: str\n        :param database: the database for which the continuous query is\n            created. Defaults to current client's database\n        :type database: str\n        :param resample_opts: resample options\n        :type resample_opts: str\n\n        :Example:\n\n        ::\n\n            >> select_clause = 'SELECT mean(\"value\") INTO \"cpu_mean\" ' \\\n            ...                 'FROM \"cpu\" GROUP BY time(1m)'\n            >> client.create_continuous_query(\n            ...     'cpu_mean', select_clause, 'db_name', 'EVERY 10s FOR 2m'\n            ... )\n            >> client.get_list_continuous_queries()\n            [\n                {\n                    'db_name': [\n                        {\n                            'name': 'cpu_mean',\n                            'query': 'CREATE CONTINUOUS QUERY \"cpu_mean\" '\n                                    'ON \"db_name\" '\n                                    'RESAMPLE EVERY 10s FOR 2m '\n                                    'BEGIN SELECT mean(\"value\") '\n                                    'INTO \"cpu_mean\" FROM \"cpu\" '\n                                    'GROUP BY time(1m) END'\n                        }\n                    ]\n                }\n            ]\n        \"\"\"\n        query_string = (\n            \"CREATE CONTINUOUS QUERY {0} ON {1}{2} BEGIN {3} END\"\n        ).format(quote_ident(name), quote_ident(database or self._database),\n                 ' RESAMPLE ' + resample_opts if resample_opts else '', select)\n        self.query(query_string)", "response": "r Create a continuous query for a database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop_continuous_query(self, name, database=None):\n        query_string = (\n            \"DROP CONTINUOUS QUERY {0} ON {1}\"\n        ).format(quote_ident(name), quote_ident(database or self._database))\n        self.query(query_string)", "response": "Drop an existing continuous query for a database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_packet(self, packet, protocol='json', time_precision=None):\n        if protocol == 'json':\n            data = make_lines(packet, time_precision).encode('utf-8')\n        elif protocol == 'line':\n            data = ('\\n'.join(packet) + '\\n').encode('utf-8')\n        self.udp_socket.sendto(data, (self._host, self._udp_port))", "response": "Send a packet to the UDP socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loads(s):\n    _decoder = json.JSONDecoder()\n\n    while s:\n        s = s.strip()\n        obj, pos = _decoder.raw_decode(s)\n        if not pos:\n            raise ValueError('no JSON object found at %i' % pos)\n        yield obj\n        s = s[pos:]", "response": "Generate a sequence of JSON values from a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef commit(cls, client=None):\n        if not client:\n            client = cls._client\n        rtn = client.write_points(cls._json_body_())\n        cls._reset_()\n        return rtn", "response": "Commit all datapoints from datapoints via the client."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the JSON body of the datapoints.", "response": "def _json_body_(cls):\n        \"\"\"Return JSON body of the datapoints.\n\n        :return: JSON body of the datapoints.\n        \"\"\"\n        json = []\n        for series_name, data in six.iteritems(cls._datapoints):\n            json.append({'name': series_name,\n                         'columns': cls._fields,\n                         'points': [[getattr(point, k) for k in cls._fields]\n                                    for point in data]\n                         })\n        return json"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite data points to multiple time series names.", "response": "def write_points(self,\n                     dataframe,\n                     measurement,\n                     tags=None,\n                     tag_columns=None,\n                     field_columns=None,\n                     time_precision=None,\n                     database=None,\n                     retention_policy=None,\n                     batch_size=None,\n                     protocol='line',\n                     numeric_precision=None):\n        \"\"\"Write to multiple time series names.\n\n        :param dataframe: data points in a DataFrame\n        :param measurement: name of measurement\n        :param tags: dictionary of tags, with string key-values\n        :param time_precision: [Optional, default None] Either 's', 'ms', 'u'\n            or 'n'.\n        :param batch_size: [Optional] Value to write the points in batches\n            instead of all at one time. Useful for when doing data dumps from\n            one database to another or when doing a massive write operation\n        :type batch_size: int\n        :param protocol: Protocol for writing data. Either 'line' or 'json'.\n        :param numeric_precision: Precision for floating point values.\n            Either None, 'full' or some int, where int is the desired decimal\n            precision. 'full' preserves full precision for int and float\n            datatypes. Defaults to None, which preserves 14-15 significant\n            figures for float and all significant figures for int datatypes.\n        \"\"\"\n        if tag_columns is None:\n            tag_columns = []\n\n        if field_columns is None:\n            field_columns = []\n\n        if batch_size:\n            number_batches = int(math.ceil(len(dataframe) / float(batch_size)))\n\n            for batch in range(number_batches):\n                start_index = batch * batch_size\n                end_index = (batch + 1) * batch_size\n\n                if protocol == 'line':\n                    points = self._convert_dataframe_to_lines(\n                        dataframe.iloc[start_index:end_index].copy(),\n                        measurement=measurement,\n                        global_tags=tags,\n                        time_precision=time_precision,\n                        tag_columns=tag_columns,\n                        field_columns=field_columns,\n                        numeric_precision=numeric_precision)\n                else:\n                    points = self._convert_dataframe_to_json(\n                        dataframe.iloc[start_index:end_index].copy(),\n                        measurement=measurement,\n                        tags=tags,\n                        time_precision=time_precision,\n                        tag_columns=tag_columns,\n                        field_columns=field_columns)\n\n                super(DataFrameClient, self).write_points(\n                    points,\n                    time_precision,\n                    database,\n                    retention_policy,\n                    protocol=protocol)\n\n            return True\n\n        if protocol == 'line':\n            points = self._convert_dataframe_to_lines(\n                dataframe,\n                measurement=measurement,\n                global_tags=tags,\n                tag_columns=tag_columns,\n                field_columns=field_columns,\n                time_precision=time_precision,\n                numeric_precision=numeric_precision)\n        else:\n            points = self._convert_dataframe_to_json(\n                dataframe,\n                measurement=measurement,\n                tags=tags,\n                time_precision=time_precision,\n                tag_columns=tag_columns,\n                field_columns=field_columns)\n\n        super(DataFrameClient, self).write_points(\n            points,\n            time_precision,\n            database,\n            retention_policy,\n            protocol=protocol)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self,\n              query,\n              params=None,\n              bind_params=None,\n              epoch=None,\n              expected_response_code=200,\n              database=None,\n              raise_errors=True,\n              chunked=False,\n              chunk_size=0,\n              method=\"GET\",\n              dropna=True):\n        \"\"\"\n        Query data into a DataFrame.\n\n        .. danger::\n            In order to avoid injection vulnerabilities (similar to `SQL\n            injection <https://www.owasp.org/index.php/SQL_Injection>`_\n            vulnerabilities), do not directly include untrusted data into the\n            ``query`` parameter, use ``bind_params`` instead.\n\n        :param query: the actual query string\n        :param params: additional parameters for the request, defaults to {}\n        :param bind_params: bind parameters for the query:\n            any variable in the query written as ``'$var_name'`` will be\n            replaced with ``bind_params['var_name']``. Only works in the\n            ``WHERE`` clause and takes precedence over ``params['params']``\n        :param epoch: response timestamps to be in epoch format either 'h',\n            'm', 's', 'ms', 'u', or 'ns',defaults to `None` which is\n            RFC3339 UTC format with nanosecond precision\n        :param expected_response_code: the expected status code of response,\n            defaults to 200\n        :param database: database to query, defaults to None\n        :param raise_errors: Whether or not to raise exceptions when InfluxDB\n            returns errors, defaults to True\n        :param chunked: Enable to use chunked responses from InfluxDB.\n            With ``chunked`` enabled, one ResultSet is returned per chunk\n            containing all results within that chunk\n        :param chunk_size: Size of each chunk to tell InfluxDB to use.\n        :param dropna: drop columns where all values are missing\n        :returns: the queried data\n        :rtype: :class:`~.ResultSet`\n        \"\"\"\n        query_args = dict(params=params,\n                          bind_params=bind_params,\n                          epoch=epoch,\n                          expected_response_code=expected_response_code,\n                          raise_errors=raise_errors,\n                          chunked=chunked,\n                          database=database,\n                          method=method,\n                          chunk_size=chunk_size)\n        results = super(DataFrameClient, self).query(query, **query_args)\n        if query.strip().upper().startswith(\"SELECT\"):\n            if len(results) > 0:\n                return self._to_dataframe(results, dropna)\n            else:\n                return {}\n        else:\n            return results", "response": "Query the InfluxDB for the set of missing entries in the specified database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(host='localhost', port=8086):\n    user = 'root'\n    password = 'root'\n    dbname = 'example'\n    dbuser = 'smly'\n    dbuser_password = 'my_secret_password'\n    query = 'select Float_value from cpu_load_short;'\n    query_where = 'select Int_value from cpu_load_short where host=$host;'\n    bind_params = {'host': 'server01'}\n    json_body = [\n        {\n            \"measurement\": \"cpu_load_short\",\n            \"tags\": {\n                \"host\": \"server01\",\n                \"region\": \"us-west\"\n            },\n            \"time\": \"2009-11-10T23:00:00Z\",\n            \"fields\": {\n                \"Float_value\": 0.64,\n                \"Int_value\": 3,\n                \"String_value\": \"Text\",\n                \"Bool_value\": True\n            }\n        }\n    ]\n\n    client = InfluxDBClient(host, port, user, password, dbname)\n\n    print(\"Create database: \" + dbname)\n    client.create_database(dbname)\n\n    print(\"Create a retention policy\")\n    client.create_retention_policy('awesome_policy', '3d', 3, default=True)\n\n    print(\"Switch user: \" + dbuser)\n    client.switch_user(dbuser, dbuser_password)\n\n    print(\"Write points: {0}\".format(json_body))\n    client.write_points(json_body)\n\n    print(\"Querying data: \" + query)\n    result = client.query(query)\n\n    print(\"Result: {0}\".format(result))\n\n    print(\"Querying data: \" + query_where)\n    result = client.query(query_where, bind_params=bind_params)\n\n    print(\"Result: {0}\".format(result))\n\n    print(\"Switch user: \" + user)\n    client.switch_user(user, password)\n\n    print(\"Drop database: \" + dbname)\n    client.drop_database(dbname)", "response": "Instantiate a connection to the InfluxDB server and create a retention policy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the JSON body of given datapoints.", "response": "def _json_body_(cls):\n        \"\"\"Return the JSON body of given datapoints.\n\n        :return: JSON body of these datapoints.\n        \"\"\"\n        json = []\n        for series_name, data in six.iteritems(cls._datapoints):\n            for point in data:\n                json_point = {\n                    \"measurement\": series_name,\n                    \"fields\": {},\n                    \"tags\": {},\n                    \"time\": getattr(point, \"time\")\n                }\n\n                for field in cls._fields:\n                    value = getattr(point, field)\n                    if value is not None:\n                        json_point['fields'][field] = value\n\n                for tag in cls._tags:\n                    json_point['tags'][tag] = getattr(point, tag)\n\n                json.append(json_point)\n        return json"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main(host='localhost', port=8086):\n    now = datetime.datetime.today()\n    points = []\n\n    for angle in range(0, 360):\n        y = 10 + math.sin(math.radians(angle)) * 10\n\n        point = {\n            \"measurement\": 'foobar',\n            \"time\": int(now.strftime('%s')) + angle,\n            \"fields\": {\n                \"value\": y\n            }\n        }\n        points.append(point)\n\n    client = InfluxDBClient(host, port, USER, PASSWORD, DBNAME)\n\n    print(\"Create database: \" + DBNAME)\n    client.create_database(DBNAME)\n    client.switch_database(DBNAME)\n\n    # Write points\n    client.write_points(points)\n\n    time.sleep(3)\n\n    query = 'SELECT * FROM foobar'\n    print(\"Querying data: \" + query)\n    result = client.query(query, database=DBNAME)\n    print(\"Result: {0}\".format(result))\n\n    \"\"\"\n    You might want to comment the delete and plot the result on InfluxDB\n    Interface. Connect on InfluxDB Interface at http://127.0.0.1:8083/\n    Select the database tutorial -> Explore Data\n\n    Then run the following query:\n\n        SELECT * from foobar\n    \"\"\"\n\n    print(\"Delete database: \" + DBNAME)\n    client.drop_database(DBNAME)", "response": "Define function to generate the sin wave."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_points(self, measurement=None, tags=None):\n        # Raise error if measurement is not str or bytes\n        if not isinstance(measurement,\n                          (bytes, type(b''.decode()), type(None))):\n            raise TypeError('measurement must be an str or None')\n\n        for series in self._get_series():\n            series_name = series.get('measurement',\n                                     series.get('name', 'results'))\n            if series_name is None:\n                # this is a \"system\" query or a query which\n                # doesn't return a name attribute.\n                # like 'show retention policies' ..\n                if tags is None:\n                    for item in self._get_points_for_series(series):\n                        yield item\n\n            elif measurement in (None, series_name):\n                # by default if no tags was provided then\n                # we will matches every returned series\n                series_tags = series.get('tags', {})\n                for item in self._get_points_for_series(series):\n                    if tags is None or \\\n                            self._tag_matches(item, tags) or \\\n                            self._tag_matches(series_tags, tags):\n                        yield item", "response": "Return a generator for all the points that match the given filters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tag_matches(tags, filter):\n        for tag_name, tag_value in filter.items():\n            # using _sentinel as I'm not sure that \"None\"\n            # could be used, because it could be a valid\n            # series_tags value : when a series has no such tag\n            # then I think it's set to /null/None/.. TBC..\n            series_tag_value = tags.get(tag_name, _sentinel)\n            if series_tag_value != tag_value:\n                return False\n\n        return True", "response": "Check if all key value in filter match in tags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of keys in the ResultSet.", "response": "def keys(self):\n        \"\"\"Return the list of keys in the ResultSet.\n\n        :return: List of keys. Keys are tuples (series_name, tags)\n        \"\"\"\n        keys = []\n        for series in self._get_series():\n            keys.append(\n                (series.get('measurement',\n                            series.get('name', 'results')),\n                 series.get('tags', None))\n            )\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef items(self):\n        items = []\n        for series in self._get_series():\n            series_key = (series.get('measurement',\n                                     series.get('name', 'results')),\n                          series.get('tags', None))\n            items.append(\n                (series_key, self._get_points_for_series(series))\n            )\n        return items", "response": "Return the set of items from the ResultSet."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns generator of dict from columns and values of a series.", "response": "def _get_points_for_series(self, series):\n        \"\"\"Return generator of dict from columns and values of a series.\n\n        :param series: One series\n        :return: Generator of dicts\n        \"\"\"\n        for point in series.get('values', []):\n            yield self.point_from_cols_vals(\n                series['columns'],\n                point\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a dict from columns and values lists.", "response": "def point_from_cols_vals(cols, vals):\n        \"\"\"Create a dict from columns and values lists.\n\n        :param cols: List of columns\n        :param vals: List of values\n        :return: Dict where keys are columns.\n        \"\"\"\n        point = {}\n        for col_index, col_name in enumerate(cols):\n            point[col_name] = vals[col_index]\n\n        return point"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_dsn(dsn, **kwargs):\n        init_args = {}\n        conn_params = urlparse(dsn)\n        scheme_info = conn_params.scheme.split('+')\n\n        if len(scheme_info) == 1:\n            scheme = scheme_info[0]\n            modifier = None\n        else:\n            modifier, scheme = scheme_info\n\n        if scheme != 'influxdb':\n            raise ValueError('Unknown scheme \"{0}\".'.format(scheme))\n\n        if modifier:\n            if modifier == 'udp':\n                init_args['use_udp'] = True\n            elif modifier == 'https':\n                init_args['ssl'] = True\n            else:\n                raise ValueError('Unknown modifier \"{0}\".'.format(modifier))\n\n        if conn_params.hostname:\n            init_args['host'] = conn_params.hostname\n        if conn_params.port:\n            init_args['port'] = conn_params.port\n        if conn_params.username:\n            init_args['username'] = conn_params.username\n        if conn_params.password:\n            init_args['password'] = conn_params.password\n        if conn_params.path and len(conn_params.path) > 1:\n            init_args['database'] = conn_params.path[1:]\n\n        init_args.update(kwargs)\n\n        return InfluxDBClient(**init_args)", "response": "r Return an instance of InfluxDBClient from the provided DSN."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a http request to the InfluxDB API.", "response": "def request(self, url, method='GET', params=None, data=None,\n                expected_response_code=200):\n        \"\"\"Make a http request to API.\"\"\"\n        url = \"{0}/{1}\".format(self._baseurl, url)\n\n        if params is None:\n            params = {}\n\n        auth = {\n            'u': self._username,\n            'p': self._password\n        }\n\n        params.update(auth)\n\n        if data is not None and not isinstance(data, str):\n            data = json.dumps(data)\n\n        retry = True\n        _try = 0\n        # Try to send the request more than once by default (see #103)\n        while retry:\n            try:\n                response = session.request(\n                    method=method,\n                    url=url,\n                    params=params,\n                    data=data,\n                    headers=self._headers,\n                    verify=self._verify_ssl,\n                    timeout=self._timeout\n                )\n                break\n            except (requests.exceptions.ConnectionError,\n                    requests.exceptions.Timeout):\n                _try += 1\n                if self._retries != 0:\n                    retry = _try < self._retries\n        else:\n            raise requests.exceptions.ConnectionError\n\n        if response.status_code == expected_response_code:\n            return response\n        else:\n            raise InfluxDBClientError(response.content, response.status_code)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting data to the current object.", "response": "def write(self, data):\n        \"\"\"Provide as convenience for influxdb v0.9.0, this may change.\"\"\"\n        self.request(\n            url=\"write\",\n            method='POST',\n            params=None,\n            data=data,\n            expected_response_code=200\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_points(self, data, time_precision='s', *args, **kwargs):\n        def list_chunks(l, n):\n            \"\"\"Yield successive n-sized chunks from l.\"\"\"\n            for i in xrange(0, len(l), n):\n                yield l[i:i + n]\n\n        batch_size = kwargs.get('batch_size')\n        if batch_size and batch_size > 0:\n            for item in data:\n                name = item.get('name')\n                columns = item.get('columns')\n                point_list = item.get('points', [])\n\n                for batch in list_chunks(point_list, batch_size):\n                    item = [{\n                        \"points\": batch,\n                        \"name\": name,\n                        \"columns\": columns\n                    }]\n                    self._write_points(\n                        data=item,\n                        time_precision=time_precision)\n            return True\n\n        return self._write_points(data=data,\n                                  time_precision=time_precision)", "response": "Write to multiple time series names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite to multiple time series names.", "response": "def write_points_with_precision(self, data, time_precision='s'):\n        \"\"\"Write to multiple time series names.\n\n        DEPRECATED.\n        \"\"\"\n        warnings.warn(\n            \"write_points_with_precision is deprecated, and will be removed \"\n            \"in future versions. Please use \"\n            \"``InfluxDBClient.write_points(time_precision='..')`` instead.\",\n            FutureWarning)\n        return self._write_points(data=data, time_precision=time_precision)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an entire series.", "response": "def delete_points(self, name):\n        \"\"\"Delete an entire series.\"\"\"\n        url = \"db/{0}/series/{1}\".format(self._database, name)\n\n        self.request(\n            url=url,\n            method='DELETE',\n            expected_response_code=204\n        )\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(self, query, time_precision='s', chunked=False):\n        return self._query(query, time_precision=time_precision,\n                           chunked=chunked)", "response": "Query the data from the influxdb v0. 8 database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_database(self, database):\n        url = \"db\"\n\n        data = {'name': database}\n\n        self.request(\n            url=url,\n            method='POST',\n            data=data,\n            expected_response_code=201\n        )\n\n        return True", "response": "Create a database on the InfluxDB server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_database(self, database):\n        url = \"db/{0}\".format(database)\n\n        self.request(\n            url=url,\n            method='DELETE',\n            expected_response_code=204\n        )\n\n        return True", "response": "Drop a database on the InfluxDB server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the list of databases.", "response": "def get_list_database(self):\n        \"\"\"Get the list of databases.\"\"\"\n        url = \"db\"\n\n        response = self.request(\n            url=url,\n            method='GET',\n            expected_response_code=200\n        )\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndrop a series on the InfluxDB server.", "response": "def delete_series(self, series):\n        \"\"\"Drop a series on the InfluxDB server.\n\n        :param series: the name of the series to delete\n        :type series: string\n        :rtype: boolean\n        \"\"\"\n        url = \"db/{0}/series/{1}\".format(\n            self._database,\n            series\n        )\n\n        self.request(\n            url=url,\n            method='DELETE',\n            expected_response_code=204\n        )\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_list_cluster_admins(self):\n        response = self.request(\n            url=\"cluster_admins\",\n            method='GET',\n            expected_response_code=200\n        )\n\n        return response.json()", "response": "Get list of cluster admins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_cluster_admin_password(self, username, new_password):\n        url = \"cluster_admins/{0}\".format(username)\n\n        data = {\n            'password': new_password\n        }\n\n        self.request(\n            url=url,\n            method='POST',\n            data=data,\n            expected_response_code=200\n        )\n\n        return True", "response": "Update cluster admin password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef alter_database_admin(self, username, is_admin):\n        url = \"db/{0}/users/{1}\".format(self._database, username)\n\n        data = {'admin': is_admin}\n\n        self.request(\n            url=url,\n            method='POST',\n            data=data,\n            expected_response_code=200\n        )\n\n        return True", "response": "Alter the database admin."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_database_users(self):\n        url = \"db/{0}/users\".format(self._database)\n\n        response = self.request(\n            url=url,\n            method='GET',\n            expected_response_code=200\n        )\n\n        return response.json()", "response": "Get list of database users."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_database_user(self, new_username, new_password, permissions=None):\n        url = \"db/{0}/users\".format(self._database)\n\n        data = {\n            'name': new_username,\n            'password': new_password\n        }\n\n        if permissions:\n            try:\n                data['readFrom'], data['writeTo'] = permissions\n            except (ValueError, TypeError):\n                raise TypeError(\n                    \"'permissions' must be (readFrom, writeTo) tuple\"\n                )\n\n        self.request(\n            url=url,\n            method='POST',\n            data=data,\n            expected_response_code=200\n        )\n\n        return True", "response": "Adds a database user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nalters a database user and their permissions.", "response": "def alter_database_user(self, username, password=None, permissions=None):\n        \"\"\"Alter a database user and/or their permissions.\n\n        :param permissions: A ``(readFrom, writeTo)`` tuple\n        :raise TypeError: if permissions cannot be read.\n        :raise ValueError: if neither password nor permissions provided.\n        \"\"\"\n        url = \"db/{0}/users/{1}\".format(self._database, username)\n\n        if not password and not permissions:\n            raise ValueError(\"Nothing to alter for user {0}.\".format(username))\n\n        data = {}\n\n        if password:\n            data['password'] = password\n\n        if permissions:\n            try:\n                data['readFrom'], data['writeTo'] = permissions\n            except (ValueError, TypeError):\n                raise TypeError(\n                    \"'permissions' must be (readFrom, writeTo) tuple\"\n                )\n\n        self.request(\n            url=url,\n            method='POST',\n            data=data,\n            expected_response_code=200\n        )\n\n        if username == self._username:\n            self._password = password\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a UDP packet along the wire.", "response": "def send_packet(self, packet):\n        \"\"\"Send a UDP packet along the wire.\"\"\"\n        data = json.dumps(packet)\n        byte = data.encode('utf-8')\n        self.udp_socket.sendto(byte, (self._host, self._udp_port))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_points(self, data, *args, **kwargs):\n        batch_size = kwargs.get('batch_size')\n        time_precision = kwargs.get('time_precision', 's')\n        if batch_size:\n            kwargs.pop('batch_size')  # don't hand over to InfluxDBClient\n            for key, data_frame in data.items():\n                number_batches = int(math.ceil(\n                    len(data_frame) / float(batch_size)))\n                for batch in range(number_batches):\n                    start_index = batch * batch_size\n                    end_index = (batch + 1) * batch_size\n                    outdata = [\n                        self._convert_dataframe_to_json(\n                            name=key,\n                            dataframe=data_frame\n                            .iloc[start_index:end_index].copy(),\n                            time_precision=time_precision)]\n                    InfluxDBClient.write_points(self, outdata, *args, **kwargs)\n            return True\n\n        outdata = [\n            self._convert_dataframe_to_json(name=key, dataframe=dataframe,\n                                            time_precision=time_precision)\n            for key, dataframe in data.items()]\n        return InfluxDBClient.write_points(self, outdata, *args, **kwargs)", "response": "Write to multiple time series names."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_points_with_precision(self, data, time_precision='s'):\n        warnings.warn(\n            \"write_points_with_precision is deprecated, and will be removed \"\n            \"in future versions. Please use \"\n            \"``DataFrameClient.write_points(time_precision='..')`` instead.\",\n            FutureWarning)\n        return self.write_points(data, time_precision='s')", "response": "Write to multiple time series names."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries data into DataFrames.", "response": "def query(self, query, time_precision='s', chunked=False):\n        \"\"\"Query data into DataFrames.\n\n        Returns a DataFrame for a single time series and a map for multiple\n        time series with the time series as value and its name as key.\n\n        :param time_precision: [Optional, default 's'] Either 's', 'm', 'ms'\n            or 'u'.\n        :param chunked: [Optional, default=False] True if the data shall be\n            retrieved in chunks, False otherwise.\n        \"\"\"\n        result = InfluxDBClient.query(self, query=query,\n                                      time_precision=time_precision,\n                                      chunked=chunked)\n        if len(result) == 0:\n            return result\n        elif len(result) == 1:\n            return self._to_dataframe(result[0], time_precision)\n        else:\n            ret = {}\n            for time_series in result:\n                ret[time_series['name']] = self._to_dataframe(time_series,\n                                                              time_precision)\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a list of query information and headers for the listview.", "response": "def _build_queries_and_headers(self):\n        \"\"\"\n        Build a list of query information and headers (pseudo-folders) for consumption by the template.\n\n        Strategy: Look for queries with titles of the form \"something - else\" (eg. with a ' - ' in the middle)\n                  and split on the ' - ', treating the left side as a \"header\" (or folder). Interleave the\n                  headers into the ListView's object_list as appropriate. Ignore headers that only have one\n                  child. The front end uses bootstrap's JS Collapse plugin, which necessitates generating CSS\n                  classes to map the header onto the child rows, hence the collapse_target variable.\n\n                  To make the return object homogeneous, convert the object_list models into dictionaries for\n                  interleaving with the header \"objects\". This necessitates special handling of 'created_at'\n                  and 'created_by_user' because model_to_dict doesn't include non-editable fields (created_at)\n                  and will give the int representation of the user instead of the string representation.\n\n        :return: A list of model dictionaries representing all the query objects, interleaved with header dictionaries.\n        \"\"\"\n\n        dict_list = []\n        rendered_headers = []\n        pattern = re.compile('[\\W_]+')\n\n        headers = Counter([q.title.split(' - ')[0] for q in self.object_list])\n\n        for q in self.object_list:\n            model_dict = model_to_dict(q)\n            header = q.title.split(' - ')[0]\n            collapse_target = pattern.sub('', header)\n\n            if headers[header] > 1 and header not in rendered_headers:\n                dict_list.append({'title': header,\n                                  'is_header': True,\n                                  'is_in_category': False,\n                                  'collapse_target': collapse_target,\n                                  'count': headers[header]})\n                rendered_headers.append(header)\n\n            model_dict.update({'is_in_category': headers[header] > 1,\n                               'collapse_target': collapse_target,\n                               'created_at': q.created_at,\n                               'is_header': False,\n                               'run_count': q.run_count,\n                               'created_by_user': six.text_type(q.created_by_user) if q.created_by_user else None})\n            dict_list.append(model_dict)\n        return dict_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the SQL for this Query model passes the blacklist.", "response": "def validate(self, value):\n        \"\"\"\n        Ensure that the SQL passes the blacklist.\n\n        :param value: The SQL for this Query model.\n        \"\"\"\n\n        query = Query(sql=value)\n\n        passes_blacklist, failing_words = query.passes_blacklist()\n\n        error = MSG_FAILED_BLACKLIST % ', '.join(failing_words) if not passes_blacklist else None\n\n        if error:\n            raise ValidationError(\n                error,\n                code=\"InvalidSql\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_schema_info(connection_alias):\n    connection = get_valid_connection(connection_alias)\n    ret = []\n    with connection.cursor() as cursor:\n        tables_to_introspect = connection.introspection.table_names(cursor, include_views=_include_views())\n\n        for table_name in tables_to_introspect:\n            if not _include_table(table_name):\n                continue\n            td = []\n            table_description = connection.introspection.get_table_description(cursor, table_name)\n            for row in table_description:\n                column_name = row[0]\n                try:\n                    field_type = connection.introspection.get_field_type(row[1], row)\n                except KeyError as e:\n                    field_type = 'Unknown'\n                td.append((column_name, field_type))\n            ret.append((table_name, td))\n    return ret", "response": "Construct schema information via engine - specific queries of the tables in the DB."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the addition of the test connection would create a cycle.", "response": "def creates_cycle(connections, test):\n    \"\"\"\n    Returns true if the addition of the 'test' connection would create a cycle,\n    assuming that no cycle already exists in the graph represented by 'connections'.\n    \"\"\"\n    i, o = test\n    if i == o:\n        return True\n\n    visited = {o}\n    while True:\n        num_added = 0\n        for a, b in connections:\n            if a in visited and b not in visited:\n                if b == i:\n                    return True\n\n                visited.add(b)\n                num_added += 1\n\n        if num_added == 0:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of input and output nodes and a list of connections return the set of nodes that require the final network output.", "response": "def required_for_output(inputs, outputs, connections):\n    \"\"\"\n    Collect the nodes whose state is required to compute the final network output(s).\n    :param inputs: list of the input identifiers\n    :param outputs: list of the output node identifiers\n    :param connections: list of (input, output) connections in the network.\n    NOTE: It is assumed that the input identifier set and the node identifier set are disjoint.\n    By convention, the output node ids are always the same as the output index.\n\n    Returns a set of identifiers of required nodes.\n    \"\"\"\n\n    required = set(outputs)\n    s = set(outputs)\n    while 1:\n        # Find nodes not in S whose output is consumed by a node in s.\n        t = set(a for (a, b) in connections if b in s and a not in s)\n\n        if not t:\n            break\n\n        layer_nodes = set(x for x in t if x not in inputs)\n        if not layer_nodes:\n            break\n\n        required = required.union(layer_nodes)\n        s = s.union(t)\n\n    return required"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of nodes and a list of outputs and a list of connections return the layers that can be evaluated in parallel in a feed - forward network.", "response": "def feed_forward_layers(inputs, outputs, connections):\n    \"\"\"\n    Collect the layers whose members can be evaluated in parallel in a feed-forward network.\n    :param inputs: list of the network input nodes\n    :param outputs: list of the output node identifiers\n    :param connections: list of (input, output) connections in the network.\n\n    Returns a list of layers, with each layer consisting of a set of node identifiers.\n    Note that the returned layers do not contain nodes whose output is ultimately\n    never used to compute the final network output.\n    \"\"\"\n\n    required = required_for_output(inputs, outputs, connections)\n\n    layers = []\n    s = set(inputs)\n    while 1:\n        # Find candidate nodes c for the next layer.  These nodes should connect\n        # a node in s to a node not in s.\n        c = set(b for (a, b) in connections if a in s and b not in s)\n        # Keep only the used nodes whose entire input set is contained in s.\n        t = set()\n        for n in c:\n            if n in required and all(a in s for (a, b) in connections if b == n):\n                t.add(n)\n\n        if not t:\n            break\n\n        layers.append(t)\n        s = s.union(t)\n\n    return layers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(config_file):\n\t# Load configuration.\n\tconfig = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n\t\t\t\t\t\t neat.DefaultSpeciesSet, neat.DefaultStagnation,\n\t\t\t\t\t\t config_file)\n\n\t# Create the population, which is the top-level object for a NEAT run.\n\tp = neat.Population(config)\n\n\t# Add a stdout reporter to show progress in the terminal.\n\tp.add_reporter(neat.StdOutReporter(True))\n\tstats = neat.StatisticsReporter()\n\tp.add_reporter(stats)\n\n\t# Run for up to 300 generations.\n\tpe = neat.ThreadedEvaluator(4, eval_genome)\n\twinner = p.run(pe.evaluate, 300)\n\tpe.stop()\n\n\t# Display the winning genome.\n\tprint('\\nBest genome:\\n{!s}'.format(winner))\n\n\t# Show output of the most fit genome against training data.\n\tprint('\\nOutput:')\n\twinner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n\tfor xi, xo in zip(xor_inputs, xor_outputs):\n\t\toutput = winner_net.activate(xi)\n\t\tprint(\n\t\t\t\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output)\n\t\t\t)\n\n\tif visualize is not None:\n\t\tnode_names = {-1: 'A', -2: 'B', 0: 'A XOR B'}\n\t\tvisualize.draw_net(config, winner, True, node_names=node_names)\n\t\tvisualize.plot_stats(stats, ylog=False, view=True)\n\t\tvisualize.plot_species(stats, view=True)", "response": "Load the config create a population evolve and show the result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the average and best fitness of the most fit genomes.", "response": "def plot_stats(statistics, ylog=False, view=False, filename='avg_fitness.svg'):\n    \"\"\" Plots the population's average and best fitness. \"\"\"\n    if plt is None:\n        warnings.warn(\"This display is not available due to a missing optional dependency (matplotlib)\")\n        return\n\n    generation = range(len(statistics.most_fit_genomes))\n    best_fitness = [c.fitness for c in statistics.most_fit_genomes]\n    avg_fitness = np.array(statistics.get_fitness_mean())\n    stdev_fitness = np.array(statistics.get_fitness_stdev())\n\n    plt.plot(generation, avg_fitness, 'b-', label=\"average\")\n    plt.plot(generation, avg_fitness - stdev_fitness, 'g-.', label=\"-1 sd\")\n    plt.plot(generation, avg_fitness + stdev_fitness, 'g-.', label=\"+1 sd\")\n    plt.plot(generation, best_fitness, 'r-', label=\"best\")\n\n    plt.title(\"Population's average and best fitness\")\n    plt.xlabel(\"Generations\")\n    plt.ylabel(\"Fitness\")\n    plt.grid()\n    plt.legend(loc=\"best\")\n    if ylog:\n        plt.gca().set_yscale('symlog')\n\n    plt.savefig(filename)\n    if view:\n        plt.show()\n\n    plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvisualizes speciation throughout evolution.", "response": "def plot_species(statistics, view=False, filename='speciation.svg'):\n    \"\"\" Visualizes speciation throughout evolution. \"\"\"\n    if plt is None:\n        warnings.warn(\"This display is not available due to a missing optional dependency (matplotlib)\")\n        return\n\n    species_sizes = statistics.get_species_sizes()\n    num_generations = len(species_sizes)\n    curves = np.array(species_sizes).T\n\n    fig, ax = plt.subplots()\n    ax.stackplot(range(num_generations), *curves)\n\n    plt.title(\"Speciation\")\n    plt.ylabel(\"Size per Species\")\n    plt.xlabel(\"Generations\")\n\n    plt.savefig(filename)\n\n    if view:\n        plt.show()\n\n    plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the softmax of the given value set.", "response": "def softmax(values):\n    \"\"\"\n    Compute the softmax of the given value set, v_i = exp(v_i) / s,\n    where s = sum(exp(v_0), exp(v_1), ..).\"\"\"\n    e_values = list(map(exp, values))\n    s = sum(e_values)\n    inv_s = 1.0 / s\n    return [ev * inv_s for ev in e_values]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the current simulation state.", "response": "def save_checkpoint(self, config, population, species_set, generation):\n        \"\"\" Save the current simulation state. \"\"\"\n        filename = '{0}{1}'.format(self.filename_prefix, generation)\n        print(\"Saving checkpoint to {0}\".format(filename))\n\n        with gzip.open(filename, 'w', compresslevel=5) as f:\n            data = (generation, config, population, species_set, random.getstate())\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restore_checkpoint(filename):\n        with gzip.open(filename) as f:\n            generation, config, population, species_set, rndstate = pickle.load(f)\n            random.setstate(rndstate)\n            return Population(config, (population, species_set, generation))", "response": "Resumes the simulation from a previous saved point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the given hostname points to the localhost otherwise False.", "response": "def host_is_local(hostname, port=22): # no port specified, just use the ssh port\n    \"\"\"\n    Returns True if the hostname points to the localhost, otherwise False.\n    \"\"\"\n    hostname = socket.getfqdn(hostname)\n    if hostname in (\"localhost\", \"0.0.0.0\", \"127.0.0.1\", \"1.0.0.127.in-addr.arpa\",\n                    \"1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa\"):\n        return True\n    localhost = socket.gethostname()\n    if hostname == localhost:\n        return True\n    localaddrs = socket.getaddrinfo(localhost, port)\n    targetaddrs = socket.getaddrinfo(hostname, port)\n    for (ignored_family, ignored_socktype, ignored_proto, ignored_canonname,\n         sockaddr) in localaddrs:\n        for (ignored_rfamily, ignored_rsocktype, ignored_rproto,\n             ignored_rcanonname, rsockaddr) in targetaddrs:\n            if rsockaddr[0] == sockaddr[0]:\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines what mode is used for the given address.", "response": "def _determine_mode(addr, mode):\n    \"\"\"\n    Returns the mode which should be used.\n    If mode is MODE_AUTO, this is determined by checking if 'addr' points to the\n    local host. If it does, return MODE_PRIMARY, else return MODE_SECONDARY.\n    If mode is either MODE_PRIMARY or MODE_SECONDARY,\n    return the 'mode' argument. Otherwise, a ValueError is raised.\n    \"\"\"\n    if isinstance(addr, tuple):\n        host = addr[0]\n    elif type(addr) == type(b\"binary_string\"):\n        host = addr\n    else:\n        raise TypeError(\"'addr' needs to be a tuple or an bytestring!\")\n    if mode == MODE_AUTO:\n        if host_is_local(host):\n            return MODE_PRIMARY\n\n        return MODE_SECONDARY\n    elif mode in (MODE_SECONDARY, MODE_PRIMARY):\n        return mode\n    else:\n        raise ValueError(\"Invalid mode {!r}!\".format(mode))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chunked(data, chunksize):\n    if chunksize < 1:\n        raise ValueError(\"Chunksize must be at least 1!\")\n    if int(chunksize) != chunksize:\n        raise ValueError(\"Chunksize needs to be an integer\")\n    res = []\n    cur = []\n    for e in data:\n        cur.append(e)\n        if len(cur) >= chunksize:\n            res.append(cur)\n            cur = []\n    if cur:\n        res.append(cur)\n    return res", "response": "Returns a list of chunks containing at most chunksize elements of data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting or connects to the manager.", "response": "def start(self):\n        \"\"\"Starts or connects to the manager.\"\"\"\n        if self.mode == MODE_PRIMARY:\n            i = self._start()\n        else:\n            i = self._connect()\n        self.manager = i"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_secondary_state(self, value):\n        if value not in (_STATE_RUNNING, _STATE_SHUTDOWN, _STATE_FORCED_SHUTDOWN):\n            raise ValueError(\n                \"State {!r} is invalid - needs to be one of _STATE_RUNNING, _STATE_SHUTDOWN, or _STATE_FORCED_SHUTDOWN\".format(\n                    value)\n                )\n        if self.manager is None:\n            raise RuntimeError(\"Manager not started\")\n        self.manager.set_state(value)", "response": "Sets the value for secondary_state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_manager_class(self, register_callables=False):\n\n        class _EvaluatorSyncManager(managers.BaseManager):\n            \"\"\"\n            A custom BaseManager.\n            Please see the documentation of `multiprocessing` for more\n            information.\n            \"\"\"\n            pass\n\n        inqueue = queue.Queue()\n        outqueue = queue.Queue()\n        namespace = Namespace()\n\n        if register_callables:\n            _EvaluatorSyncManager.register(\n                \"get_inqueue\",\n                callable=lambda: inqueue,\n                )\n            _EvaluatorSyncManager.register(\n                \"get_outqueue\",\n                callable=lambda: outqueue,\n                )\n            _EvaluatorSyncManager.register(\n                \"get_state\",\n                callable=self._get_secondary_state,\n                )\n            _EvaluatorSyncManager.register(\n                \"set_state\",\n                callable=lambda v: self._secondary_state.set(v),\n                )\n            _EvaluatorSyncManager.register(\n                \"get_namespace\",\n                callable=lambda: namespace,\n                )\n\n\n        else:\n            _EvaluatorSyncManager.register(\n                \"get_inqueue\",\n                )\n            _EvaluatorSyncManager.register(\n                \"get_outqueue\",\n                )\n            _EvaluatorSyncManager.register(\n                \"get_state\",\n                )\n            _EvaluatorSyncManager.register(\n                \"set_state\",\n                )\n            _EvaluatorSyncManager.register(\n                \"get_namespace\",\n                )\n        return _EvaluatorSyncManager", "response": "Returns a new manager class with registered methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _connect(self):\n        cls = self._get_manager_class(register_callables=False)\n        ins = cls(address=self.addr, authkey=self.authkey)\n        ins.connect()\n        return ins", "response": "Connects to the manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(self, exit_on_stop=True, secondary_wait=0, reconnect=False):\n        if self.started:\n            raise RuntimeError(\"DistributedEvaluator already started!\")\n        self.started = True\n        if self.mode == MODE_PRIMARY:\n            self._start_primary()\n        elif self.mode == MODE_SECONDARY:\n            time.sleep(secondary_wait)\n            self._start_secondary()\n            self._secondary_loop(reconnect=reconnect)\n            if exit_on_stop:\n                sys.exit(0)\n        else:\n            raise ValueError(\"Invalid mode {!r}!\".format(self.mode))", "response": "Starts the DistributedEvaluator process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping all secondaries and all the other nodes.", "response": "def stop(self, wait=1, shutdown=True, force_secondary_shutdown=False):\n        \"\"\"\n        Stops all secondaries.\n        'wait' specifies the time (in seconds) to wait before shutting down the\n        manager or returning.\n        If 'shutdown', shutdown the manager.\n        If 'force_secondary_shutdown', shutdown the secondary nodes even if\n        they are started with 'reconnect=True'.\n        \"\"\"\n        if self.mode != MODE_PRIMARY:\n            raise ModeError(\"Not in primary mode!\")\n        if not self.started:\n            raise RuntimeError(\"Not yet started!\")\n        if force_secondary_shutdown:\n            state = _STATE_FORCED_SHUTDOWN\n        else:\n            state = _STATE_SHUTDOWN\n        self.em.set_secondary_state(state)\n        time.sleep(wait)\n        if shutdown:\n            self.em.stop()\n        self.started = False\n        self.inqueue = self.outqueue = self.namespace = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts as the primary", "response": "def _start_primary(self):\n        \"\"\"Start as the primary\"\"\"\n        self.em.start()\n        self.em.set_secondary_state(_STATE_RUNNING)\n        self._set_shared_instances()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_shared_instances(self):\n        self.inqueue = self.em.get_inqueue()\n        self.outqueue = self.em.get_outqueue()\n        self.namespace = self.em.get_namespace()", "response": "Sets attributes from the shared instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset self. em and the shared instances.", "response": "def _reset_em(self):\n        \"\"\"Resets self.em and the shared instances.\"\"\"\n        self.em = _ExtendedManager(self.addr, self.authkey, mode=self.mode, start=False)\n        self.em.start()\n        self._set_shared_instances()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _secondary_loop(self, reconnect=False):\n        if self.num_workers > 1:\n            pool = multiprocessing.Pool(self.num_workers)\n        else:\n            pool = None\n        should_reconnect = True\n        while should_reconnect:\n            i = 0\n            running = True\n            try:\n                self._reset_em()\n            except (socket.error, EOFError, IOError, OSError, socket.gaierror, TypeError):\n                continue\n            while running:\n                i += 1\n                if i % 5 == 0:\n                    # for better performance, only check every 5 cycles\n                    try:\n                        state = self.em.secondary_state\n                    except (socket.error, EOFError, IOError, OSError, socket.gaierror, TypeError):\n                        if not reconnect:\n                            raise\n                        else:\n                            break\n                    if state == _STATE_FORCED_SHUTDOWN:\n                        running = False\n                        should_reconnect = False\n                    elif state == _STATE_SHUTDOWN:\n                        running = False\n                    if not running:\n                        continue\n                try:\n                    tasks = self.inqueue.get(block=True, timeout=0.2)\n                except queue.Empty:\n                    continue\n                except (socket.error, EOFError, IOError, OSError, socket.gaierror, TypeError):\n                    break\n                except (managers.RemoteError, multiprocessing.ProcessError) as e:\n                    if ('Empty' in repr(e)) or ('TimeoutError' in repr(e)):\n                        continue\n                    if (('EOFError' in repr(e)) or ('PipeError' in repr(e)) or\n                        ('AuthenticationError' in repr(e))): # Second for Python 3.X, Third for 3.6+\n                        break\n                    raise\n                if pool is None:\n                    res = []\n                    for genome_id, genome, config in tasks:\n                        fitness = self.eval_function(genome, config)\n                        res.append((genome_id, fitness))\n                else:\n                    genome_ids = []\n                    jobs = []\n                    for genome_id, genome, config in tasks:\n                        genome_ids.append(genome_id)\n                        jobs.append(\n                            pool.apply_async(\n                                self.eval_function, (genome, config)\n                                )\n                            )\n                    results = [\n                        job.get(timeout=self.worker_timeout) for job in jobs\n                        ]\n                    res = zip(genome_ids, results)\n                try:\n                    self.outqueue.put(res)\n                except (socket.error, EOFError, IOError, OSError, socket.gaierror, TypeError):\n                    break\n                except (managers.RemoteError, multiprocessing.ProcessError) as e:\n                    if ('Empty' in repr(e)) or ('TimeoutError' in repr(e)):\n                        continue\n                    if (('EOFError' in repr(e)) or ('PipeError' in repr(e)) or\n                        ('AuthenticationError' in repr(e))): # Second for Python 3.X, Third for 3.6+\n                        break\n                    raise\n\n            if not reconnect:\n                should_reconnect = False\n                break\n        if pool is not None:\n            pool.terminate()", "response": "The main loop for the secondary nodes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating the genomes. This method raises a ModeError if the DistributedEvaluator is not in primary mode.", "response": "def evaluate(self, genomes, config):\n        \"\"\"\n        Evaluates the genomes.\n        This method raises a ModeError if the\n        DistributedEvaluator is not in primary mode.\n        \"\"\"\n        if self.mode != MODE_PRIMARY:\n            raise ModeError(\"Not in primary mode!\")\n        tasks = [(genome_id, genome, config) for genome_id, genome in genomes]\n        id2genome = {genome_id: genome for genome_id, genome in genomes}\n        tasks = chunked(tasks, self.secondary_chunksize)\n        n_tasks = len(tasks)\n        for task in tasks:\n            self.inqueue.put(task)\n        tresults = []\n        while len(tresults) < n_tasks:\n            try:\n                sr = self.outqueue.get(block=True, timeout=0.2)\n            except (queue.Empty, managers.RemoteError):\n                continue\n            tresults.append(sr)\n        results = []\n        for sr in tresults:\n            results += sr\n        for genome_id, fitness in results:\n            genome = id2genome[genome_id]\n            genome.fitness = fitness"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef step(self, force):\n        '''\n        Update the system state using leapfrog integration.\n            x_{i+1} = x_i + v_i * dt + 0.5 * a_i * dt^2\n            v_{i+1} = v_i + 0.5 * (a_i + a_{i+1}) * dt\n        '''\n        # Locals for readability.\n        g = self.gravity\n        mp = self.mpole\n        mc = self.mcart\n        mt = mp + mc\n        L = self.lpole\n        dt = self.time_step\n\n        # Remember acceleration from previous step.\n        tacc0 = self.tacc\n        xacc0 = self.xacc\n\n        # Update position/angle.\n        self.x += dt * self.dx + 0.5 * xacc0 * dt ** 2\n        self.theta += dt * self.dtheta + 0.5 * tacc0 * dt ** 2\n\n        # Compute new accelerations as given in \"Correct equations for the dynamics of the cart-pole system\"\n        # by Razvan V. Florian (http://florian.io).\n        # http://coneural.org/florian/papers/05_cart_pole.pdf\n        st = sin(self.theta)\n        ct = cos(self.theta)\n        tacc1 = (g * st + ct * (-force - mp * L * self.dtheta ** 2 * st) / mt) / (L * (4.0 / 3 - mp * ct ** 2 / mt))\n        xacc1 = (force + mp * L * (self.dtheta ** 2 * st - tacc1 * ct)) / mt\n\n        # Update velocities.\n        self.dx += 0.5 * (xacc0 + xacc1) * dt\n        self.dtheta += 0.5 * (tacc0 + tacc1) * dt\n\n        # Remember current acceleration for next step.\n        self.tacc = tacc1\n        self.xacc = xacc1\n        self.t += dt", "response": "Update the state of the current system by a given amount of time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_scaled_state(self):\n        '''Get full state, scaled into (approximately) [0, 1].'''\n        return [0.5 * (self.x + self.position_limit) / self.position_limit,\n                (self.dx + 0.75) / 1.5,\n                0.5 * (self.theta + self.angle_limit_radians) / self.angle_limit_radians,\n                (self.dtheta + 1.0) / 2.0]", "response": "Get full state scaled into ( approximimately ) [ 0 1 )."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the proper number of offspring per species.", "response": "def compute_spawn(adjusted_fitness, previous_sizes, pop_size, min_species_size):\n        \"\"\"Compute the proper number of offspring per species (proportional to fitness).\"\"\"\n        af_sum = sum(adjusted_fitness)\n\n        spawn_amounts = []\n        for af, ps in zip(adjusted_fitness, previous_sizes):\n            if af_sum > 0:\n                s = max(min_species_size, af / af_sum * pop_size)\n            else:\n                s = min_species_size\n\n            d = (s - ps) * 0.5\n            c = int(round(d))\n            spawn = ps\n            if abs(c) > 0:\n                spawn += c\n            elif d > 0:\n                spawn += 1\n            elif d < 0:\n                spawn -= 1\n\n            spawn_amounts.append(spawn)\n\n        # Normalize the spawn amounts so that the next generation is roughly\n        # the population size requested by the user.\n        total_spawn = sum(spawn_amounts)\n        norm = pop_size / total_spawn\n        spawn_amounts = [max(min_species_size, int(round(n * norm))) for n in spawn_amounts]\n\n        return spawn_amounts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreproducing genomes from scratch or by sexual or by parents.", "response": "def reproduce(self, config, species, pop_size, generation):\n        \"\"\"\n        Handles creation of genomes, either from scratch or by sexual or\n        asexual reproduction from parents.\n        \"\"\"\n        # TODO: I don't like this modification of the species and stagnation objects,\n        # because it requires internal knowledge of the objects.\n\n        # Filter out stagnated species, collect the set of non-stagnated\n        # species members, and compute their average adjusted fitness.\n        # The average adjusted fitness scheme (normalized to the interval\n        # [0, 1]) allows the use of negative fitness values without\n        # interfering with the shared fitness scheme.\n        all_fitnesses = []\n        remaining_species = []\n        for stag_sid, stag_s, stagnant in self.stagnation.update(species, generation):\n            if stagnant:\n                self.reporters.species_stagnant(stag_sid, stag_s)\n            else:\n                all_fitnesses.extend(m.fitness for m in itervalues(stag_s.members))\n                remaining_species.append(stag_s)\n        # The above comment was not quite what was happening - now getting fitnesses\n        # only from members of non-stagnated species.\n\n        # No species left.\n        if not remaining_species:\n            species.species = {}\n            return {} # was []\n\n        # Find minimum/maximum fitness across the entire population, for use in\n        # species adjusted fitness computation.\n        min_fitness = min(all_fitnesses)\n        max_fitness = max(all_fitnesses)\n        # Do not allow the fitness range to be zero, as we divide by it below.\n        # TODO: The ``1.0`` below is rather arbitrary, and should be configurable.\n        fitness_range = max(1.0, max_fitness - min_fitness)\n        for afs in remaining_species:\n            # Compute adjusted fitness.\n            msf = mean([m.fitness for m in itervalues(afs.members)])\n            af = (msf - min_fitness) / fitness_range\n            afs.adjusted_fitness = af\n\n        adjusted_fitnesses = [s.adjusted_fitness for s in remaining_species]\n        avg_adjusted_fitness = mean(adjusted_fitnesses) # type: float\n        self.reporters.info(\"Average adjusted fitness: {:.3f}\".format(avg_adjusted_fitness))\n\n        # Compute the number of new members for each species in the new generation.\n        previous_sizes = [len(s.members) for s in remaining_species]\n        min_species_size = self.reproduction_config.min_species_size\n        # Isn't the effective min_species_size going to be max(min_species_size,\n        # self.reproduction_config.elitism)? That would probably produce more accurate tracking\n        # of population sizes and relative fitnesses... doing. TODO: document.\n        min_species_size = max(min_species_size,self.reproduction_config.elitism)\n        spawn_amounts = self.compute_spawn(adjusted_fitnesses, previous_sizes,\n                                           pop_size, min_species_size)\n\n        new_population = {}\n        species.species = {}\n        for spawn, s in zip(spawn_amounts, remaining_species):\n            # If elitism is enabled, each species always at least gets to retain its elites.\n            spawn = max(spawn, self.reproduction_config.elitism)\n\n            assert spawn > 0\n\n            # The species has at least one member for the next generation, so retain it.\n            old_members = list(iteritems(s.members))\n            s.members = {}\n            species.species[s.key] = s\n\n            # Sort members in order of descending fitness.\n            old_members.sort(reverse=True, key=lambda x: x[1].fitness)\n\n            # Transfer elites to new generation.\n            if self.reproduction_config.elitism > 0:\n                for i, m in old_members[:self.reproduction_config.elitism]:\n                    new_population[i] = m\n                    spawn -= 1\n\n            if spawn <= 0:\n                continue\n\n            # Only use the survival threshold fraction to use as parents for the next generation.\n            repro_cutoff = int(math.ceil(self.reproduction_config.survival_threshold *\n                                         len(old_members)))\n            # Use at least two parents no matter what the threshold fraction result is.\n            repro_cutoff = max(repro_cutoff, 2)\n            old_members = old_members[:repro_cutoff]\n\n            # Randomly choose parents and produce the number of offspring allotted to the species.\n            while spawn > 0:\n                spawn -= 1\n\n                parent1_id, parent1 = random.choice(old_members)\n                parent2_id, parent2 = random.choice(old_members)\n\n                # Note that if the parents are not distinct, crossover will produce a\n                # genetically identical clone of the parent (but with a different ID).\n                gid = next(self.genome_indexer)\n                child = config.genome_type(gid)\n                child.configure_crossover(parent1, parent2, config.genome_config)\n                child.mutate(config.genome_config)\n                new_population[gid] = child\n                self.ancestors[gid] = (parent1_id, parent2_id)\n\n        return new_population"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a feedforward network from a genome and a config.", "response": "def create(genome, config):\n        \"\"\" Receives a genome and returns its phenotype (a FeedForwardNetwork). \"\"\"\n\n        # Gather expressed connections.\n        connections = [cg.key for cg in itervalues(genome.connections) if cg.enabled]\n\n        layers = feed_forward_layers(config.genome_config.input_keys, config.genome_config.output_keys, connections)\n        node_evals = []\n        for layer in layers:\n            for node in layer:\n                inputs = []\n                node_expr = [] # currently unused\n                for conn_key in connections:\n                    inode, onode = conn_key\n                    if onode == node:\n                        cg = genome.connections[conn_key]\n                        inputs.append((inode, cg.weight))\n                        node_expr.append(\"v[{}] * {:.7e}\".format(inode, cg.weight))\n\n\n                ng = genome.nodes[node]\n                aggregation_function = config.genome_config.aggregation_function_defs.get(ng.aggregation)\n                activation_function = config.genome_config.activation_defs.get(ng.activation)\n                node_evals.append((node, activation_function, aggregation_function, ng.bias, ng.response, inputs))\n\n        return FeedForwardNetwork(config.genome_config.input_keys, config.genome_config.output_keys, node_evals)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, fitness_function, n=None):\n\n        if self.config.no_fitness_termination and (n is None):\n            raise RuntimeError(\"Cannot have no generational limit with no fitness termination\")\n\n        k = 0\n        while n is None or k < n:\n            k += 1\n\n            self.reporters.start_generation(self.generation)\n\n            # Evaluate all genomes using the user-provided function.\n            fitness_function(list(iteritems(self.population)), self.config)\n\n            # Gather and report statistics.\n            best = None\n            for g in itervalues(self.population):\n                if best is None or g.fitness > best.fitness:\n                    best = g\n            self.reporters.post_evaluate(self.config, self.population, self.species, best)\n\n            # Track the best genome ever seen.\n            if self.best_genome is None or best.fitness > self.best_genome.fitness:\n                self.best_genome = best\n\n            if not self.config.no_fitness_termination:\n                # End if the fitness threshold is reached.\n                fv = self.fitness_criterion(g.fitness for g in itervalues(self.population))\n                if fv >= self.config.fitness_threshold:\n                    self.reporters.found_solution(self.config, self.generation, best)\n                    break\n\n            # Create the next generation from the current generation.\n            self.population = self.reproduction.reproduce(self.config, self.species,\n                                                          self.config.pop_size, self.generation)\n\n            # Check for complete extinction.\n            if not self.species.species:\n                self.reporters.complete_extinction()\n\n                # If requested by the user, create a completely new population,\n                # otherwise raise an exception.\n                if self.config.reset_on_extinction:\n                    self.population = self.reproduction.create_new(self.config.genome_type,\n                                                                   self.config.genome_config,\n                                                                   self.config.pop_size)\n                else:\n                    raise CompleteExtinctionException()\n\n            # Divide the new population into species.\n            self.species.speciate(self.config, self.population, self.generation)\n\n            self.reporters.end_generation(self.config, self.population, self.species)\n\n            self.generation += 1\n\n        if self.config.no_fitness_termination:\n            self.reporters.found_solution(self.config, self.generation, self.best_genome)\n\n        return self.best_genome", "response": "Runs the NEAT s genetic algorithm for at most n generations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef crossover(self, gene2):\n        assert self.key == gene2.key\n\n        # Note: we use \"a if random() > 0.5 else b\" instead of choice((a, b))\n        # here because `choice` is substantially slower.\n        new_gene = self.__class__(self.key)\n        for a in self._gene_attributes:\n            if random() > 0.5:\n                setattr(new_gene, a.name, getattr(self, a.name))\n            else:\n                setattr(new_gene, a.name, getattr(gene2, a.name))\n\n        return new_gene", "response": "Creates a new gene randomly inheriting attributes from its parents."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mutate_add_connection(self, config):\n        '''\n        Attempt to add a new connection, the only restriction being that the output\n        node cannot be one of the network input pins.\n        '''\n        possible_outputs = list(iterkeys(self.nodes))\n        out_node = choice(possible_outputs)\n\n        possible_inputs = possible_outputs + config.input_keys\n        in_node = choice(possible_inputs)\n\n        if in_node == out_node:\n            return\n\n        # # Don't duplicate connections.\n        # key = (in_node, out_node)\n        # if key in self.connections:\n        #     return\n\n        cg = self.create_connection(config, in_node, out_node)\n        self.connections[cg.key] = cg", "response": "Add a new connection to the network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the genetic distance between this genome and the other. This distance value is used to compute genome compatibility for speciation. This distance value is used to compute genome compatibility for speciation.", "response": "def distance(self, other, config):\n        \"\"\"\n        Returns the genetic distance between this genome and the other. This distance value\n        is used to compute genome compatibility for speciation.\n        \"\"\"\n\n        # Compute node gene distance component.\n        node_distance = 0.0\n        if self.nodes or other.nodes:\n            disjoint_nodes = 0\n            for k2 in iterkeys(other.nodes):\n                if k2 not in self.nodes:\n                    disjoint_nodes += 1\n\n            for k1, n1 in iteritems(self.nodes):\n                n2 = other.nodes.get(k1)\n                if n2 is None:\n                    disjoint_nodes += 1\n                else:\n                    # Homologous genes compute their own distance value.\n                    node_distance += n1.distance(n2, config)\n\n            max_nodes = max(len(self.nodes), len(other.nodes))\n            node_distance = (node_distance + config.compatibility_disjoint_coefficient * disjoint_nodes) / max_nodes\n\n        # Compute connection gene differences.\n        connection_distance = 0.0\n        if self.connections or other.connections:\n            disjoint_connections = 0\n            for k2 in iterkeys(other.connections):\n                if k2 not in self.connections:\n                    disjoint_connections += 1\n\n            for k1, c1 in iteritems(self.connections):\n                c2 = other.connections.get(k1)\n                if c2 is None:\n                    disjoint_connections += 1\n                else:\n                    # Homologous genes compute their own distance value.\n                    connection_distance += c1.distance(c2, config)\n\n            max_conn = max(len(self.connections), len(other.connections))\n            connection_distance = (connection_distance + config.compatibility_disjoint_coefficient * disjoint_connections) / max_conn\n\n        distance = node_distance + connection_distance\n\n        return distance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef size(self):\n        '''Returns genome 'complexity', taken to be (number of nodes, number of enabled connections)'''\n        num_enabled_connections = sum([1 for cg in self.connections.values() if cg.enabled is True])\n        return len(self.nodes), num_enabled_connections", "response": "Returns genome complexity taken to be ( number of nodes number of enabled connections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadvance the simulation by the given amount of time assuming that inputs are are constant at the given values during the simulated time.", "response": "def advance(self, inputs, advance_time, time_step=None):\n        \"\"\"\n        Advance the simulation by the given amount of time, assuming that inputs are\n        constant at the given values during the simulated time.\n        \"\"\"\n        final_time_seconds = self.time_seconds + advance_time\n\n        # Use half of the max allowed time step if none is given.\n        if time_step is None:  # pragma: no cover\n            time_step = 0.5 * self.get_max_time_step()\n\n        if len(self.input_nodes) != len(inputs):\n            raise RuntimeError(\"Expected {0} inputs, got {1}\".format(len(self.input_nodes), len(inputs)))\n\n        while self.time_seconds < final_time_seconds:\n            dt = min(time_step, final_time_seconds - self.time_seconds)\n\n            ivalues = self.values[self.active]\n            ovalues = self.values[1 - self.active]\n            self.active = 1 - self.active\n\n            for i, v in zip(self.input_nodes, inputs):\n                ivalues[i] = v\n                ovalues[i] = v\n\n            for node_key, ne in iteritems(self.node_evals):\n                node_inputs = [ivalues[i] * w for i, w in ne.links]\n                s = ne.aggregation(node_inputs)\n                z = ne.activation(ne.bias + ne.response * s)\n                ovalues[node_key] += dt / ne.time_constant * (-ovalues[node_key] + z)\n\n            self.time_seconds += dt\n\n        ovalues = self.values[1 - self.active]\n        return [ovalues[i] for i in self.output_nodes]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new CTRNN instance.", "response": "def create(genome, config, time_constant):\n        \"\"\" Receives a genome and returns its phenotype (a CTRNN). \"\"\"\n        genome_config = config.genome_config\n        required = required_for_output(genome_config.input_keys, genome_config.output_keys, genome.connections)\n\n        # Gather inputs and expressed connections.\n        node_inputs = {}\n        for cg in itervalues(genome.connections):\n            if not cg.enabled:\n                continue\n\n            i, o = cg.key\n            if o not in required and i not in required:\n                continue\n\n            if o not in node_inputs:\n                node_inputs[o] = [(i, cg.weight)]\n            else:\n                node_inputs[o].append((i, cg.weight))\n\n        node_evals = {}\n        for node_key, inputs in iteritems(node_inputs):\n            node = genome.nodes[node_key]\n            activation_function = genome_config.activation_defs.get(node.activation)\n            aggregation_function = genome_config.aggregation_function_defs.get(node.aggregation)\n            node_evals[node_key] = CTRNNNodeEval(time_constant,\n                                                 activation_function,\n                                                 aggregation_function,\n                                                 node.bias,\n                                                 node.response,\n                                                 inputs)\n\n        return CTRNN(genome_config.input_keys, genome_config.output_keys, node_evals)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addr_tuple(s):\n    if \"[\" in s:\n        # ip v6\n        if (s.count(\"[\") != 1) or (s.count(\"]\") != 1):\n            raise ValueError(\"Invalid IPv6 address!\")\n        end = s.index(\"]\")\n        if \":\" not in s[end:]:\n            raise ValueError(\"IPv6 address does specify a port to use!\")\n        host, port = s[1:end], s[end+1:]\n        port = int(port)\n        return (host, port)\n    else:\n        host, port = s.split(\":\")\n        port = int(port)\n        return (host, port)", "response": "converts a string into a tuple of host and port"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate species fitness history information for all the species in the species_set.", "response": "def update(self, species_set, generation):\n        \"\"\"\n        Required interface method. Updates species fitness history information,\n        checking for ones that have not improved in max_stagnation generations,\n        and - unless it would result in the number of species dropping below the configured\n        species_elitism parameter if they were removed,\n        in which case the highest-fitness species are spared -\n        returns a list with stagnant species marked for removal.\n        \"\"\"\n        species_data = []\n        for sid, s in iteritems(species_set.species):\n            if s.fitness_history:\n                prev_fitness = max(s.fitness_history)\n            else:\n                prev_fitness = -sys.float_info.max\n\n            s.fitness = self.species_fitness_func(s.get_fitnesses())\n            s.fitness_history.append(s.fitness)\n            s.adjusted_fitness = None\n            if prev_fitness is None or s.fitness > prev_fitness:\n                s.last_improved = generation\n\n            species_data.append((sid, s))\n\n        # Sort in ascending fitness order.\n        species_data.sort(key=lambda x: x[1].fitness)\n\n        result = []\n        species_fitnesses = []\n        num_non_stagnant = len(species_data)\n        for idx, (sid, s) in enumerate(species_data):\n            # Override stagnant state if marking this species as stagnant would\n            # result in the total number of species dropping below the limit.\n            # Because species are in ascending fitness order, less fit species\n            # will be marked as stagnant first.\n            stagnant_time = generation - s.last_improved\n            is_stagnant = False\n            if num_non_stagnant > self.stagnation_config.species_elitism:\n                is_stagnant = stagnant_time >= self.stagnation_config.max_stagnation\n\n            if (len(species_data) - idx) <= self.stagnation_config.species_elitism:\n                is_stagnant = False\n\n            if is_stagnant:\n                num_non_stagnant -= 1\n\n            result.append((sid, s, is_stagnant))\n            species_fitnesses.append(s.fitness)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the worker threads", "response": "def start(self):\n        \"\"\"Starts the worker threads\"\"\"\n        if self.working:\n            return\n        self.working = True\n        for i in range(self.num_workers):\n            w = threading.Thread(\n                name=\"Worker Thread #{i}\".format(i=i),\n                target=self._worker,\n                )\n            w.daemon = True\n            w.start()\n            self.workers.append(w)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops the worker threads and waits for them to finish", "response": "def stop(self):\n        \"\"\"Stops the worker threads and waits for them to finish\"\"\"\n        self.working = False\n        for w in self.workers:\n            w.join()\n        self.workers = []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate the genomes and assign the fitness back to each genome.", "response": "def evaluate(self, genomes, config):\n        \"\"\"Evaluate the genomes\"\"\"\n        if not self.working:\n            self.start()\n        p = 0\n        for genome_id, genome in genomes:\n            p += 1\n            self.inqueue.put((genome_id, genome, config))\n\n        # assign the fitness back to each genome\n        while p > 0:\n            p -= 1\n            ignored_genome_id, genome, fitness = self.outqueue.get()\n            genome.fitness = fitness"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef advance(self, dt_msec):\n        # TODO: Make the time step adjustable, and choose an appropriate\n        # numerical integration method to maintain stability.\n        # TODO: The need to catch overflows indicates that the current method is\n        # not stable for all possible network configurations and states.\n        try:\n            self.v += 0.5 * dt_msec * (0.04 * self.v ** 2 + 5 * self.v + 140 - self.u + self.current)\n            self.v += 0.5 * dt_msec * (0.04 * self.v ** 2 + 5 * self.v + 140 - self.u + self.current)\n            self.u += dt_msec * self.a * (self.b * self.v - self.u)\n        except OverflowError:\n            # Reset without producing a spike.\n            self.v = self.c\n            self.u = self.b * self.v\n\n        self.fired = 0.0\n        if self.v > 30.0:\n            # Output spike and reset.\n            self.fired = 1.0\n            self.v = self.c\n            self.u += self.d", "response": "Advance the simulation time by the given time step in milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting all state variables to their initial state.", "response": "def reset(self):\n        \"\"\"Resets all state variables.\"\"\"\n        self.v = self.c\n        self.u = self.b * self.v\n        self.fired = 0.0\n        self.current = self.bias"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new IZNN object from a genome and a config.", "response": "def create(genome, config):\n        \"\"\" Receives a genome and returns its phenotype (a neural network). \"\"\"\n        genome_config = config.genome_config\n        required = required_for_output(genome_config.input_keys, genome_config.output_keys, genome.connections)\n\n        # Gather inputs and expressed connections.\n        node_inputs = {}\n        for cg in itervalues(genome.connections):\n            if not cg.enabled:\n                continue\n\n            i, o = cg.key\n            if o not in required and i not in required:\n                continue\n\n            if o not in node_inputs:\n                node_inputs[o] = [(i, cg.weight)]\n            else:\n                node_inputs[o].append((i, cg.weight))\n\n        neurons = {}\n        for node_key in required:\n            ng = genome.nodes[node_key]\n            inputs = node_inputs.get(node_key, [])\n            neurons[node_key] = IZNeuron(ng.bias, ng.a, ng.b, ng.c, ng.d, inputs)\n\n        genome_config = config.genome_config\n        return IZNN(neurons, genome_config.input_keys, genome_config.output_keys)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the network s output based on the time to first spike of the two output neurons.", "response": "def compute_output(t0, t1):\n    '''Compute the network's output based on the \"time to first spike\" of the two output neurons.'''\n    if t0 is None or t1 is None:\n        # If neither of the output neurons fired within the allotted time,\n        # give a response which produces a large error.\n        return -1.0\n    else:\n        # If the output neurons fire within 1.0 milliseconds of each other,\n        # the output is 1, and if they fire more than 11 milliseconds apart,\n        # the output is 0, with linear interpolation between 1 and 11 milliseconds.\n        response = 1.1 - 0.1 * abs(t0 - t1)\n        return max(0.0, min(1.0, response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplacing genomes into species by genetic similarity. Note that this method assumes the current representatives of the species are from the old generation, and that after speciation has been performed, the old representatives should be dropped and replaced with representatives from the new generation. If you violate this assumption, you should make sure other necessary parts of the code are updated to reflect the new behavior.", "response": "def speciate(self, config, population, generation):\n        \"\"\"\n        Place genomes into species by genetic similarity.\n\n        Note that this method assumes the current representatives of the species are from the old\n        generation, and that after speciation has been performed, the old representatives should be\n        dropped and replaced with representatives from the new generation.  If you violate this\n        assumption, you should make sure other necessary parts of the code are updated to reflect\n        the new behavior.\n        \"\"\"\n        assert isinstance(population, dict)\n\n        compatibility_threshold = self.species_set_config.compatibility_threshold\n\n        # Find the best representatives for each existing species.\n        unspeciated = set(iterkeys(population))\n        distances = GenomeDistanceCache(config.genome_config)\n        new_representatives = {}\n        new_members = {}\n        for sid, s in iteritems(self.species):\n            candidates = []\n            for gid in unspeciated:\n                g = population[gid]\n                d = distances(s.representative, g)\n                candidates.append((d, g))\n\n            # The new representative is the genome closest to the current representative.\n            ignored_rdist, new_rep = min(candidates, key=lambda x: x[0])\n            new_rid = new_rep.key\n            new_representatives[sid] = new_rid\n            new_members[sid] = [new_rid]\n            unspeciated.remove(new_rid)\n\n        # Partition population into species based on genetic similarity.\n        while unspeciated:\n            gid = unspeciated.pop()\n            g = population[gid]\n\n            # Find the species with the most similar representative.\n            candidates = []\n            for sid, rid in iteritems(new_representatives):\n                rep = population[rid]\n                d = distances(rep, g)\n                if d < compatibility_threshold:\n                    candidates.append((d, sid))\n\n            if candidates:\n                ignored_sdist, sid = min(candidates, key=lambda x: x[0])\n                new_members[sid].append(gid)\n            else:\n                # No species is similar enough, create a new species, using\n                # this genome as its representative.\n                sid = next(self.indexer)\n                new_representatives[sid] = gid\n                new_members[sid] = [gid]\n\n        # Update species collection based on new speciation.\n        self.genome_to_species = {}\n        for sid, rid in iteritems(new_representatives):\n            s = self.species.get(sid)\n            if s is None:\n                s = Species(sid, generation)\n                self.species[sid] = s\n\n            members = new_members[sid]\n            for gid in members:\n                self.genome_to_species[gid] = sid\n\n            member_dict = dict((gid, population[gid]) for gid in members)\n            s.update(population[rid], member_dict)\n\n        gdmean = mean(itervalues(distances.distances))\n        gdstdev = stdev(itervalues(distances.distances))\n        self.reporters.info(\n            'Mean genetic distance {0:.3f}, standard deviation {1:.3f}'.format(gdmean, gdstdev))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the per - generation average cross_validation fitness.", "response": "def get_average_cross_validation_fitness(self): # pragma: no cover\n        \"\"\"Get the per-generation average cross_validation fitness.\"\"\"\n        avg_cross_validation_fitness = []\n        for stats in self.generation_cross_validation_statistics:\n            scores = []\n            for fitness in stats.values():\n                scores.extend(fitness)\n            avg_cross_validation_fitness.append(mean(scores))\n\n        return avg_cross_validation_fitness"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef best_unique_genomes(self, n):\n        best_unique = {}\n        for g in self.most_fit_genomes:\n            best_unique[g.key] = g\n        best_unique_list = list(best_unique.values())\n\n        def key(genome):\n            return genome.fitness\n\n        return sorted(best_unique_list, key=key, reverse=True)[:n]", "response": "Returns the most n fit genomes with no duplication."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the n most fit genomes ever seen.", "response": "def best_genomes(self, n):\n        \"\"\"Returns the n most fit genomes ever seen.\"\"\"\n        def key(g):\n            return g.fitness\n\n        return sorted(self.most_fit_genomes, key=key, reverse=True)[:n]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_genome_fitness(self,\n                            delimiter=' ',\n                            filename='fitness_history.csv',\n                            with_cross_validation=False):\n        \"\"\" Saves the population's best and average fitness. \"\"\"\n        with open(filename, 'w') as f:\n            w = csv.writer(f, delimiter=delimiter)\n\n            best_fitness = [c.fitness for c in self.most_fit_genomes]\n            avg_fitness = self.get_fitness_mean()\n\n            if with_cross_validation: # pragma: no cover\n                cv_best_fitness = [c.cross_fitness for c in self.most_fit_genomes]\n                cv_avg_fitness = self.get_average_cross_validation_fitness()\n                for best, avg, cv_best, cv_avg in zip(best_fitness,\n                                                      avg_fitness,\n                                                      cv_best_fitness,\n                                                      cv_avg_fitness):\n                    w.writerow([best, avg, cv_best, cv_avg])\n            else:\n                for best, avg in zip(best_fitness, avg_fitness):\n                    w.writerow([best, avg])", "response": "Saves the best and average fitness of the most fit genomes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the species count throughout evolution.", "response": "def save_species_count(self, delimiter=' ', filename='speciation.csv'):\n        \"\"\" Log speciation throughout evolution. \"\"\"\n        with open(filename, 'w') as f:\n            w = csv.writer(f, delimiter=delimiter)\n            for s in self.get_species_sizes():\n                w.writerow(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_species_fitness(self, delimiter=' ', null_value='NA', filename='species_fitness.csv'):\n        with open(filename, 'w') as f:\n            w = csv.writer(f, delimiter=delimiter)\n            for s in self.get_species_fitness(null_value):\n                w.writerow(s)", "response": "Save species fitness throughout evolution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure_new(self, config):\n\n        # Create node genes for the output pins.\n        for node_key in config.output_keys:\n            self.nodes[node_key] = self.create_node(config, node_key)\n\n        # Add hidden nodes if requested.\n        if config.num_hidden > 0:\n            for i in range(config.num_hidden):\n                node_key = config.get_new_node_key(self.nodes)\n                assert node_key not in self.nodes\n                node = self.create_node(config, node_key)\n                self.nodes[node_key] = node\n\n        # Add connections based on initial connectivity type.\n\n        if 'fs_neat' in config.initial_connection:\n            if config.initial_connection == 'fs_neat_nohidden':\n                self.connect_fs_neat_nohidden(config)\n            elif config.initial_connection == 'fs_neat_hidden':\n                self.connect_fs_neat_hidden(config)\n            else:\n                if config.num_hidden > 0:\n                    print(\n                        \"Warning: initial_connection = fs_neat will not connect to hidden nodes;\",\n                        \"\\tif this is desired, set initial_connection = fs_neat_nohidden;\",\n                        \"\\tif not, set initial_connection = fs_neat_hidden\",\n                        sep='\\n', file=sys.stderr);\n                self.connect_fs_neat_nohidden(config)\n        elif 'full' in config.initial_connection:\n            if config.initial_connection == 'full_nodirect':\n                self.connect_full_nodirect(config)\n            elif config.initial_connection == 'full_direct':\n                self.connect_full_direct(config)\n            else:\n                if config.num_hidden > 0:\n                    print(\n                        \"Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\",\n                        \"\\tif this is desired, set initial_connection = full_nodirect;\",\n                        \"\\tif not, set initial_connection = full_direct\",\n                        sep='\\n', file=sys.stderr);\n                self.connect_full_nodirect(config)\n        elif 'partial' in config.initial_connection:\n            if config.initial_connection == 'partial_nodirect':\n                self.connect_partial_nodirect(config)\n            elif config.initial_connection == 'partial_direct':\n                self.connect_partial_direct(config)\n            else:\n                if config.num_hidden > 0:\n                    print(\n                        \"Warning: initial_connection = partial with hidden nodes will not do direct input-output connections;\",\n                        \"\\tif this is desired, set initial_connection = partial_nodirect {0};\".format(\n                            config.connection_fraction),\n                        \"\\tif not, set initial_connection = partial_direct {0}\".format(\n                            config.connection_fraction),\n                        sep='\\n', file=sys.stderr);\n                self.connect_partial_nodirect(config)", "response": "Configure a new genome based on the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures a new genome by crossover from two parent genomes.", "response": "def configure_crossover(self, genome1, genome2, config):\n        \"\"\" Configure a new genome by crossover from two parent genomes. \"\"\"\n        assert isinstance(genome1.fitness, (int, float))\n        assert isinstance(genome2.fitness, (int, float))\n        if genome1.fitness > genome2.fitness:\n            parent1, parent2 = genome1, genome2\n        else:\n            parent1, parent2 = genome2, genome1\n\n        # Inherit connection genes\n        for key, cg1 in iteritems(parent1.connections):\n            cg2 = parent2.connections.get(key)\n            if cg2 is None:\n                # Excess or disjoint gene: copy from the fittest parent.\n                self.connections[key] = cg1.copy()\n            else:\n                # Homologous gene: combine genes from both parents.\n                self.connections[key] = cg1.crossover(cg2)\n\n        # Inherit node genes\n        parent1_set = parent1.nodes\n        parent2_set = parent2.nodes\n\n        for key, ng1 in iteritems(parent1_set):\n            ng2 = parent2_set.get(key)\n            assert key not in self.nodes\n            if ng2 is None:\n                # Extra gene: copy from the fittest parent\n                self.nodes[key] = ng1.copy()\n            else:\n                # Homologous gene: combine genes from both parents.\n                self.nodes[key] = ng1.crossover(ng2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new connection to the network.", "response": "def mutate_add_connection(self, config):\n        \"\"\"\n        Attempt to add a new connection, the only restriction being that the output\n        node cannot be one of the network input pins.\n        \"\"\"\n        possible_outputs = list(iterkeys(self.nodes))\n        out_node = choice(possible_outputs)\n\n        possible_inputs = possible_outputs + config.input_keys\n        in_node = choice(possible_inputs)\n\n        # Don't duplicate connections.\n        key = (in_node, out_node)\n        if key in self.connections:\n            # TODO: Should this be using mutation to/from rates? Hairy to configure...\n            if config.check_structural_mutation_surer():\n                self.connections[key].enabled = True\n            return\n\n        # Don't allow connections between two output nodes\n        if in_node in config.output_keys and out_node in config.output_keys:\n            return\n\n        # No need to check for connections between input nodes:\n        # they cannot be the output end of a connection (see above).\n\n        # For feed-forward networks, avoid creating cycles.\n        if config.feed_forward and creates_cycle(list(iterkeys(self.connections)), key):\n            return\n\n        cg = self.create_connection(config, in_node, out_node)\n        self.connections[cg.key] = cg"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect_fs_neat_hidden(self, config):\n        input_id = choice(config.input_keys)\n        others = [i for i in iterkeys(self.nodes) if i not in config.input_keys]\n        for output_id in others:\n            connection = self.create_connection(config, input_id, output_id)\n            self.connections[connection.key] = connection", "response": "Connect one input to all hidden and all output nodes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_full_connections(self, config, direct):\n        hidden = [i for i in iterkeys(self.nodes) if i not in config.output_keys]\n        output = [i for i in iterkeys(self.nodes) if i in config.output_keys]\n        connections = []\n        if hidden:\n            for input_id in config.input_keys:\n                for h in hidden:\n                    connections.append((input_id, h))\n            for h in hidden:\n                for output_id in output:\n                    connections.append((h, output_id))\n        if direct or (not hidden):\n            for input_id in config.input_keys:\n                for output_id in output:\n                    connections.append((input_id, output_id))\n\n        # For recurrent genomes, include node self-connections.\n        if not config.feed_forward:\n            for i in iterkeys(self.nodes):\n                connections.append((i, i))\n\n        return connections", "response": "Compute all the full - connected feed - forward genome - each input connected to all hidden nodes and each output connected to all hidden nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect_full_nodirect(self, config):\n        for input_id, output_id in self.compute_full_connections(config, False):\n            connection = self.create_connection(config, input_id, output_id)\n            self.connections[connection.key] = connection", "response": "Create a fully - connected genomeCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect_full_direct(self, config):\n        for input_id, output_id in self.compute_full_connections(config, True):\n            connection = self.create_connection(config, input_id, output_id)\n            self.connections[connection.key] = connection", "response": "Create a fully - connected genome including direct input - output connections."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect_partial_nodirect(self, config):\n        assert 0 <= config.connection_fraction <= 1\n        all_connections = self.compute_full_connections(config, False)\n        shuffle(all_connections)\n        num_to_add = int(round(len(all_connections) * config.connection_fraction))\n        for input_id, output_id in all_connections[:num_to_add]:\n            connection = self.create_connection(config, input_id, output_id)\n            self.connections[connection.key] = connection", "response": "Create a partially - connected genome and\n            with no direct input - output connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninterprets the config_parser output into the proper type and returns the result.", "response": "def interpret(self, config_dict):\n        \"\"\"\n        Converts the config_parser output into the proper type,\n        supplies defaults if available and needed, and checks for some errors.\n        \"\"\"\n        value = config_dict.get(self.name)\n        if value is None:\n            if self.default is None:\n                raise RuntimeError('Missing configuration item: ' + self.name)\n            else:\n                warnings.warn(\"Using default {!r} for '{!s}'\".format(self.default, self.name),\n                              DeprecationWarning)\n                if (str != self.value_type) and isinstance(self.default, self.value_type):\n                    return self.default\n                else:\n                    value = self.default\n\n        try:\n            if str == self.value_type:\n                return str(value)\n            if int == self.value_type:\n                return int(value)\n            if bool == self.value_type:\n                if value.lower() == \"true\":\n                    return True\n                elif value.lower() == \"false\":\n                    return False\n                else:\n                    raise RuntimeError(self.name + \" must be True or False\")\n            if float == self.value_type:\n                return float(value)\n            if list == self.value_type:\n                return value.split(\" \")\n        except Exception:\n            raise RuntimeError(\"Error interpreting config item '{}' with value {!r} and type {}\".format(\n                self.name, value, self.value_type))\n\n        raise RuntimeError(\"Unexpected configuration type: \" + repr(self.value_type))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_spikes(spikes, view=False, filename=None, title=None):\n    t_values = [t for t, I, v, u, f in spikes]\n    v_values = [v for t, I, v, u, f in spikes]\n    u_values = [u for t, I, v, u, f in spikes]\n    I_values = [I for t, I, v, u, f in spikes]\n    f_values = [f for t, I, v, u, f in spikes]\n\n    fig = plt.figure()\n    plt.subplot(4, 1, 1)\n    plt.ylabel(\"Potential (mv)\")\n    plt.xlabel(\"Time (in ms)\")\n    plt.grid()\n    plt.plot(t_values, v_values, \"g-\")\n\n    if title is None:\n        plt.title(\"Izhikevich's spiking neuron model\")\n    else:\n        plt.title(\"Izhikevich's spiking neuron model ({0!s})\".format(title))\n\n    plt.subplot(4, 1, 2)\n    plt.ylabel(\"Fired\")\n    plt.xlabel(\"Time (in ms)\")\n    plt.grid()\n    plt.plot(t_values, f_values, \"r-\")\n\n    plt.subplot(4, 1, 3)\n    plt.ylabel(\"Recovery (u)\")\n    plt.xlabel(\"Time (in ms)\")\n    plt.grid()\n    plt.plot(t_values, u_values, \"r-\")\n\n    plt.subplot(4, 1, 4)\n    plt.ylabel(\"Current (I)\")\n    plt.xlabel(\"Time (in ms)\")\n    plt.grid()\n    plt.plot(t_values, I_values, \"r-o\")\n\n    if filename is not None:\n        plt.savefig(filename)\n\n    if view:\n        plt.show()\n        plt.close()\n        fig = None\n\n    return fig", "response": "Plots the trains for a single spiking neuron."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_net(config, genome, view=False, filename=None, node_names=None, show_disabled=True, prune_unused=False,\n             node_colors=None, fmt='svg'):\n    \"\"\" Receives a genome and draws a neural network with arbitrary topology. \"\"\"\n    # Attributes for network nodes.\n    if graphviz is None:\n        warnings.warn(\"This display is not available due to a missing optional dependency (graphviz)\")\n        return\n\n    if node_names is None:\n        node_names = {}\n\n    assert type(node_names) is dict\n\n    if node_colors is None:\n        node_colors = {}\n\n    assert type(node_colors) is dict\n\n    node_attrs = {\n        'shape': 'circle',\n        'fontsize': '9',\n        'height': '0.2',\n        'width': '0.2'}\n\n    dot = graphviz.Digraph(format=fmt, node_attr=node_attrs)\n\n    inputs = set()\n    for k in config.genome_config.input_keys:\n        inputs.add(k)\n        name = node_names.get(k, str(k))\n        input_attrs = {'style': 'filled', 'shape': 'box', 'fillcolor': node_colors.get(k, 'lightgray')}\n        dot.node(name, _attributes=input_attrs)\n\n    outputs = set()\n    for k in config.genome_config.output_keys:\n        outputs.add(k)\n        name = node_names.get(k, str(k))\n        node_attrs = {'style': 'filled', 'fillcolor': node_colors.get(k, 'lightblue')}\n\n        dot.node(name, _attributes=node_attrs)\n\n    if prune_unused:\n        connections = set()\n        for cg in genome.connections.values():\n            if cg.enabled or show_disabled:\n                connections.add((cg.in_node_id, cg.out_node_id))\n\n        used_nodes = copy.copy(outputs)\n        pending = copy.copy(outputs)\n        while pending:\n            new_pending = set()\n            for a, b in connections:\n                if b in pending and a not in used_nodes:\n                    new_pending.add(a)\n                    used_nodes.add(a)\n            pending = new_pending\n    else:\n        used_nodes = set(genome.nodes.keys())\n\n    for n in used_nodes:\n        if n in inputs or n in outputs:\n            continue\n\n        attrs = {'style': 'filled',\n                 'fillcolor': node_colors.get(n, 'white')}\n        dot.node(str(n), _attributes=attrs)\n\n    for cg in genome.connections.values():\n        if cg.enabled or show_disabled:\n            #if cg.input not in used_nodes or cg.output not in used_nodes:\n            #    continue\n            input, output = cg.key\n            a = node_names.get(input, str(input))\n            b = node_names.get(output, str(output))\n            style = 'solid' if cg.enabled else 'dotted'\n            color = 'green' if cg.weight > 0 else 'red'\n            width = str(0.1 + abs(cg.weight / 5.0))\n            dot.edge(a, b, _attributes={'style': style, 'color': color, 'penwidth': width})\n\n    dot.render(filename, view=view)\n\n    return dot", "response": "Draws a neural network from a genome."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generatePayload(self, query):\n        payload = self._payload_default.copy()\n        if query:\n            payload.update(query)\n        payload[\"__req\"] = str_base(self._req_counter, 36)\n        payload[\"seq\"] = self._seq\n        self._req_counter += 1\n        return payload", "response": "Generate the payload for the current request"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfix errors in fb_window and fb_postLogin errors", "response": "def _fix_fb_errors(self, error_code):\n        \"\"\"\n        This fixes \"Please try closing and re-opening your browser window\" errors (1357004)\n        This error usually happens after 1-2 days of inactivity\n        It may be a bad idea to do this in an exception handler, if you have a better method, please suggest it!\n        \"\"\"\n        if error_code == \"1357004\":\n            log.warning(\"Got error #1357004. Doing a _postLogin, and resending request\")\n            self._postLogin()\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef graphql_requests(self, *queries):\n        data = {\n            \"method\": \"GET\",\n            \"response_format\": \"json\",\n            \"queries\": graphql_queries_to_json(*queries),\n        }\n        return tuple(\n            self._post(self.req_url.GRAPHQL, data, fix_request=True, as_graphql=True)\n        )", "response": "This method is used to make a GraphQL request to get the set of objects in the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isLoggedIn(self):\n        # Send a request to the login url, to see if we're directed to the home page\n        r = self._cleanGet(self.req_url.LOGIN, allow_redirects=False)\n        return \"Location\" in r.headers and \"home\" in r.headers[\"Location\"]", "response": "Checks if the client is logged in"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setSession(self, session_cookies):\n        # Quick check to see if session_cookies is formatted properly\n        if not session_cookies or \"c_user\" not in session_cookies:\n            return False\n\n        try:\n            # Load cookies into current session\n            self._session.cookies = requests.cookies.merge_cookies(\n                self._session.cookies, session_cookies\n            )\n            self._postLogin()\n        except Exception as e:\n            log.exception(\"Failed loading session\")\n            self._resetValues()\n            return False\n        return True", "response": "Loads session cookies into current session object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses `email` and `password` to login the user (If the user is already logged in, this will do a re-login) :param email: Facebook `email` or `id` or `phone number` :param password: Facebook account password :param max_tries: Maximum number of times to try logging in :type max_tries: int :raises: FBchatException on failed login", "response": "def login(self, email, password, max_tries=5):\n        \"\"\"\n        Uses `email` and `password` to login the user (If the user is already logged in, this will do a re-login)\n\n        :param email: Facebook `email` or `id` or `phone number`\n        :param password: Facebook account password\n        :param max_tries: Maximum number of times to try logging in\n        :type max_tries: int\n        :raises: FBchatException on failed login\n        \"\"\"\n        self.onLoggingIn(email=email)\n\n        if max_tries < 1:\n            raise FBchatUserError(\"Cannot login: max_tries should be at least one\")\n\n        if not (email and password):\n            raise FBchatUserError(\"Email and password not set\")\n\n        for i in range(1, max_tries + 1):\n            login_successful, login_url = self._login(email, password)\n            if not login_successful:\n                log.warning(\n                    \"Attempt #{} failed{}\".format(\n                        i, {True: \", retrying\"}.get(i < max_tries, \"\")\n                    )\n                )\n                time.sleep(1)\n                continue\n            else:\n                self.onLoggedIn(email=email)\n                break\n        else:\n            raise FBchatUserError(\n                \"Login failed. Check email/password. \"\n                \"(Failed on url: {})\".format(login_url)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getThread(self, given_thread_id=None, given_thread_type=None):\n        if given_thread_id is None:\n            if self._default_thread_id is not None:\n                return self._default_thread_id, self._default_thread_type\n            else:\n                raise ValueError(\"Thread ID is not set\")\n        else:\n            return given_thread_id, given_thread_type", "response": "Returns the thread ID and thread type from the given thread ID and thread type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the default thread to send messages to", "response": "def setDefaultThread(self, thread_id, thread_type):\n        \"\"\"\n        Sets default thread to send messages to\n\n        :param thread_id: User/Group ID to default to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type thread_type: models.ThreadType\n        \"\"\"\n        self._default_thread_id = thread_id\n        self._default_thread_type = thread_type"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetchThreads(self, thread_location, before=None, after=None, limit=None):\n        threads = []\n\n        last_thread_timestamp = None\n        while True:\n            # break if limit is exceeded\n            if limit and len(threads) >= limit:\n                break\n\n            # fetchThreadList returns at max 20 threads before last_thread_timestamp (included)\n            candidates = self.fetchThreadList(\n                before=last_thread_timestamp, thread_location=thread_location\n            )\n\n            if len(candidates) > 1:\n                threads += candidates[1:]\n            else:  # End of threads\n                break\n\n            last_thread_timestamp = threads[-1].last_message_timestamp\n\n            # FB returns a sorted list of threads\n            if (before is not None and int(last_thread_timestamp) > before) or (\n                after is not None and int(last_thread_timestamp) < after\n            ):\n                break\n\n        # Return only threads between before and after (if set)\n        if before is not None or after is not None:\n            for t in threads:\n                last_message_timestamp = int(t.last_message_timestamp)\n                if (before is not None and last_message_timestamp > before) or (\n                    after is not None and last_message_timestamp < after\n                ):\n                    threads.remove(t)\n\n        if limit and len(threads) > limit:\n            return threads[:limit]\n\n        return threads", "response": "Fetch all threads in thread_location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetchAllUsersFromThreads(self, threads):\n        users = []\n        users_to_fetch = []  # It's more efficient to fetch all users in one request\n        for thread in threads:\n            if thread.type == ThreadType.USER:\n                if thread.uid not in [user.uid for user in users]:\n                    users.append(thread)\n            elif thread.type == ThreadType.GROUP:\n                for user_id in thread.participants:\n                    if (\n                        user_id not in [user.uid for user in users]\n                        and user_id not in users_to_fetch\n                    ):\n                        users_to_fetch.append(user_id)\n        for user_id, user in self.fetchUserInfo(*users_to_fetch).items():\n            users.append(user)\n        return users", "response": "Get all users involved in threads."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetchAllUsers(self):\n        data = {\"viewer\": self._uid}\n        j = self._post(\n            self.req_url.ALL_USERS, query=data, fix_request=True, as_json=True\n        )\n        if j.get(\"payload\") is None:\n            raise FBchatException(\"Missing payload while fetching users: {}\".format(j))\n\n        users = []\n        for data in j[\"payload\"].values():\n            if data[\"type\"] in [\"user\", \"friend\"]:\n                if data[\"id\"] in [\"0\", 0]:\n                    # Skip invalid users\n                    continue\n                users.append(User._from_all_fetch(data))\n        return users", "response": "Gets all users in the current user group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding and get user by his name", "response": "def searchForUsers(self, name, limit=10):\n        \"\"\"\n        Find and get user by his/her name\n\n        :param name: Name of the user\n        :param limit: The max. amount of users to fetch\n        :return: :class:`models.User` objects, ordered by relevance\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        params = {\"search\": name, \"limit\": limit}\n        j = self.graphql_request(GraphQL(query=GraphQL.SEARCH_USER, params=params))\n\n        return [User._from_graphql(node) for node in j[name][\"users\"][\"nodes\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding and get a page by its name", "response": "def searchForPages(self, name, limit=10):\n        \"\"\"\n        Find and get page by its name\n\n        :param name: Name of the page\n        :return: :class:`models.Page` objects, ordered by relevance\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        params = {\"search\": name, \"limit\": limit}\n        j = self.graphql_request(GraphQL(query=GraphQL.SEARCH_PAGE, params=params))\n\n        return [Page._from_graphql(node) for node in j[name][\"pages\"][\"nodes\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind and get group by its name", "response": "def searchForGroups(self, name, limit=10):\n        \"\"\"\n        Find and get group thread by its name\n\n        :param name: Name of the group thread\n        :param limit: The max. amount of groups to fetch\n        :return: :class:`models.Group` objects, ordered by relevance\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        params = {\"search\": name, \"limit\": limit}\n        j = self.graphql_request(GraphQL(query=GraphQL.SEARCH_GROUP, params=params))\n\n        return [Group._from_graphql(node) for node in j[\"viewer\"][\"groups\"][\"nodes\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef searchForThreads(self, name, limit=10):\n        params = {\"search\": name, \"limit\": limit}\n        j = self.graphql_request(GraphQL(query=GraphQL.SEARCH_THREAD, params=params))\n\n        rtn = []\n        for node in j[name][\"threads\"][\"nodes\"]:\n            if node[\"__typename\"] == \"User\":\n                rtn.append(User._from_graphql(node))\n            elif node[\"__typename\"] == \"MessageThread\":\n                # MessageThread => Group thread\n                rtn.append(Group._from_graphql(node))\n            elif node[\"__typename\"] == \"Page\":\n                rtn.append(Page._from_graphql(node))\n            elif node[\"__typename\"] == \"Group\":\n                # We don't handle Facebook \"Groups\"\n                pass\n            else:\n                log.warning(\n                    \"Unknown type {} in {}\".format(repr(node[\"__typename\"]), node)\n                )\n\n        return rtn", "response": "Find and get a thread by its name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch and get message IDs by query", "response": "def searchForMessageIDs(self, query, offset=0, limit=5, thread_id=None):\n        \"\"\"\n        Find and get message IDs by query\n\n        :param query: Text to search for\n        :param offset: Number of messages to skip\n        :param limit: Max. number of messages to retrieve\n        :param thread_id: User/Group ID to search in. See :ref:`intro_threads`\n        :type offset: int\n        :type limit: int\n        :return: Found Message IDs\n        :rtype: generator\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\n            \"query\": query,\n            \"snippetOffset\": offset,\n            \"snippetLimit\": limit,\n            \"identifier\": \"thread_fbid\",\n            \"thread_fbid\": thread_id,\n        }\n        j = self._post(\n            self.req_url.SEARCH_MESSAGES, data, fix_request=True, as_json=True\n        )\n\n        result = j[\"payload\"][\"search_snippets\"][query]\n        snippets = result[thread_id][\"snippets\"] if result.get(thread_id) else []\n        for snippet in snippets:\n            yield snippet[\"message_id\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch and get messages by query", "response": "def searchForMessages(self, query, offset=0, limit=5, thread_id=None):\n        \"\"\"\n        Find and get :class:`models.Message` objects by query\n\n        .. warning::\n            This method sends request for every found message ID.\n\n        :param query: Text to search for\n        :param offset: Number of messages to skip\n        :param limit: Max. number of messages to retrieve\n        :param thread_id: User/Group ID to search in. See :ref:`intro_threads`\n        :type offset: int\n        :type limit: int\n        :return: Found :class:`models.Message` objects\n        :rtype: generator\n        :raises: FBchatException if request failed\n        \"\"\"\n        message_ids = self.searchForMessageIDs(\n            query, offset=offset, limit=limit, thread_id=thread_id\n        )\n        for mid in message_ids:\n            yield self.fetchMessageInfo(mid, thread_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(self, query, fetch_messages=False, thread_limit=5, message_limit=5):\n        data = {\"query\": query, \"snippetLimit\": thread_limit}\n        j = self._post(\n            self.req_url.SEARCH_MESSAGES, data, fix_request=True, as_json=True\n        )\n        result = j[\"payload\"][\"search_snippets\"][query]\n\n        if fetch_messages:\n            search_method = self.searchForMessages\n        else:\n            search_method = self.searchForMessageIDs\n\n        return {\n            thread_id: search_method(query, limit=message_limit, thread_id=thread_id)\n            for thread_id in result\n        }", "response": "Search for messages in all threads"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching users info from IDs unordered Sends two requests to fetch all available info!", "response": "def fetchUserInfo(self, *user_ids):\n        \"\"\"\n        Get users' info from IDs, unordered\n\n        .. warning::\n            Sends two requests, to fetch all available info!\n\n        :param user_ids: One or more user ID(s) to query\n        :return: :class:`models.User` objects, labeled by their ID\n        :rtype: dict\n        :raises: FBchatException if request failed\n        \"\"\"\n        threads = self.fetchThreadInfo(*user_ids)\n        users = {}\n        for id_, thread in threads.items():\n            if thread.type == ThreadType.USER:\n                users[id_] = thread\n            else:\n                raise FBchatUserError(\"Thread {} was not a user\".format(thread))\n\n        return users"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetchPageInfo(self, *page_ids):\n        threads = self.fetchThreadInfo(*page_ids)\n        pages = {}\n        for id_, thread in threads.items():\n            if thread.type == ThreadType.PAGE:\n                pages[id_] = thread\n            else:\n                raise FBchatUserError(\"Thread {} was not a page\".format(thread))\n\n        return pages", "response": "Fetch pages info from IDs unordered\n\n        Sends two requests to fetch all available info!"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching group info from IDs unordered", "response": "def fetchGroupInfo(self, *group_ids):\n        \"\"\"\n        Get groups' info from IDs, unordered\n\n        :param group_ids: One or more group ID(s) to query\n        :return: :class:`models.Group` objects, labeled by their ID\n        :rtype: dict\n        :raises: FBchatException if request failed\n        \"\"\"\n        threads = self.fetchThreadInfo(*group_ids)\n        groups = {}\n        for id_, thread in threads.items():\n            if thread.type == ThreadType.GROUP:\n                groups[id_] = thread\n            else:\n                raise FBchatUserError(\"Thread {} was not a group\".format(thread))\n\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetchThreadInfo(self, *thread_ids):\n        queries = []\n        for thread_id in thread_ids:\n            params = {\n                \"id\": thread_id,\n                \"message_limit\": 0,\n                \"load_messages\": False,\n                \"load_read_receipts\": False,\n                \"before\": None,\n            }\n            queries.append(GraphQL(doc_id=\"2147762685294928\", params=params))\n\n        j = self.graphql_requests(*queries)\n\n        for i, entry in enumerate(j):\n            if entry.get(\"message_thread\") is None:\n                # If you don't have an existing thread with this person, attempt to retrieve user data anyways\n                j[i][\"message_thread\"] = {\n                    \"thread_key\": {\"other_user_id\": thread_ids[i]},\n                    \"thread_type\": \"ONE_TO_ONE\",\n                }\n\n        pages_and_user_ids = [\n            k[\"message_thread\"][\"thread_key\"][\"other_user_id\"]\n            for k in j\n            if k[\"message_thread\"].get(\"thread_type\") == \"ONE_TO_ONE\"\n        ]\n        pages_and_users = {}\n        if len(pages_and_user_ids) != 0:\n            pages_and_users = self._fetchInfo(*pages_and_user_ids)\n\n        rtn = {}\n        for i, entry in enumerate(j):\n            entry = entry[\"message_thread\"]\n            if entry.get(\"thread_type\") == \"GROUP\":\n                _id = entry[\"thread_key\"][\"thread_fbid\"]\n                rtn[_id] = Group._from_graphql(entry)\n            elif entry.get(\"thread_type\") == \"ONE_TO_ONE\":\n                _id = entry[\"thread_key\"][\"other_user_id\"]\n                if pages_and_users.get(_id) is None:\n                    raise FBchatException(\"Could not fetch thread {}\".format(_id))\n                entry.update(pages_and_users[_id])\n                if entry[\"type\"] == ThreadType.USER:\n                    rtn[_id] = User._from_graphql(entry)\n                else:\n                    rtn[_id] = Page._from_graphql(entry)\n            else:\n                raise FBchatException(\n                    \"{} had an unknown thread type: {}\".format(thread_ids[i], entry)\n                )\n\n        return rtn", "response": "Fetch thread info from the given list of thread IDs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the last 20 messages in a thread.", "response": "def fetchThreadMessages(self, thread_id=None, limit=20, before=None):\n        \"\"\"\n        Get the last messages in a thread\n\n        :param thread_id: User/Group ID to get messages from. See :ref:`intro_threads`\n        :param limit: Max. number of messages to retrieve\n        :param before: A timestamp, indicating from which point to retrieve messages\n        :type limit: int\n        :type before: int\n        :return: :class:`models.Message` objects\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        params = {\n            \"id\": thread_id,\n            \"message_limit\": limit,\n            \"load_messages\": True,\n            \"load_read_receipts\": True,\n            \"before\": before,\n        }\n        j = self.graphql_request(GraphQL(doc_id=\"1860982147341344\", params=params))\n\n        if j.get(\"message_thread\") is None:\n            raise FBchatException(\"Could not fetch thread {}: {}\".format(thread_id, j))\n\n        messages = [\n            Message._from_graphql(message)\n            for message in j[\"message_thread\"][\"messages\"][\"nodes\"]\n        ]\n        messages.reverse()\n\n        read_receipts = j[\"message_thread\"][\"read_receipts\"][\"nodes\"]\n\n        for message in messages:\n            for receipt in read_receipts:\n                if int(receipt[\"watermark\"]) >= int(message.timestamp):\n                    message.read_by.append(receipt[\"actor\"][\"id\"])\n\n        return messages"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetchThreadList(\n        self, offset=None, limit=20, thread_location=ThreadLocation.INBOX, before=None\n    ):\n        \"\"\"Get thread list of your facebook account\n\n        :param offset: Deprecated. Do not use!\n        :param limit: Max. number of threads to retrieve. Capped at 20\n        :param thread_location: models.ThreadLocation: INBOX, PENDING, ARCHIVED or OTHER\n        :param before: A timestamp (in milliseconds), indicating from which point to retrieve threads\n        :type limit: int\n        :type before: int\n        :return: :class:`models.Thread` objects\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        if offset is not None:\n            log.warning(\n                \"Using `offset` in `fetchThreadList` is no longer supported, \"\n                \"since Facebook migrated to the use of GraphQL in this request. \"\n                \"Use `before` instead.\"\n            )\n\n        if limit > 20 or limit < 1:\n            raise FBchatUserError(\"`limit` should be between 1 and 20\")\n\n        if thread_location in ThreadLocation:\n            loc_str = thread_location.value\n        else:\n            raise FBchatUserError('\"thread_location\" must be a value of ThreadLocation')\n\n        params = {\n            \"limit\": limit,\n            \"tags\": [loc_str],\n            \"before\": before,\n            \"includeDeliveryReceipts\": True,\n            \"includeSeqID\": False,\n        }\n        j = self.graphql_request(GraphQL(doc_id=\"1349387578499440\", params=params))\n\n        rtn = []\n        for node in j[\"viewer\"][\"message_threads\"][\"nodes\"]:\n            _type = node.get(\"thread_type\")\n            if _type == \"GROUP\":\n                rtn.append(Group._from_graphql(node))\n            elif _type == \"ONE_TO_ONE\":\n                rtn.append(User._from_thread_fetch(node))\n            else:\n                raise FBchatException(\n                    \"Unknown thread type: {}, with data: {}\".format(_type, node)\n                )\n        return rtn", "response": "Fetch the thread list of your facebook account."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the unread thread list", "response": "def fetchUnread(self):\n        \"\"\"\n        Get the unread thread list\n\n        :return: List of unread thread ids\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        form = {\n            \"folders[0]\": \"inbox\",\n            \"client\": \"mercury\",\n            \"last_action_timestamp\": now() - 60 * 1000\n            # 'last_action_timestamp': 0\n        }\n        j = self._post(\n            self.req_url.UNREAD_THREADS, form, fix_request=True, as_json=True\n        )\n\n        payload = j[\"payload\"][\"unread_thread_fbids\"][0]\n        return payload[\"thread_fbids\"] + payload[\"other_user_fbids\"]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the unseen thread ids", "response": "def fetchUnseen(self):\n        \"\"\"\n        Get the unseen (new) thread list\n\n        :return: List of unseen thread ids\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        j = self._post(\n            self.req_url.UNSEEN_THREADS, None, fix_request=True, as_json=True\n        )\n\n        payload = j[\"payload\"][\"unseen_thread_fbids\"][0]\n        return payload[\"thread_fbids\"] + payload[\"other_user_fbids\"]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetchImageUrl(self, image_id):\n        image_id = str(image_id)\n        data = {\"photo_id\": str(image_id)}\n        j = self._get(\n            ReqUrl.ATTACHMENT_PHOTO, query=data, fix_request=True, as_json=True\n        )\n\n        url = get_jsmods_require(j, 3)\n        if url is None:\n            raise FBchatException(\"Could not fetch image url from: {}\".format(j))\n        return url", "response": "Fetches the url to the original image from an image attachment ID\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetchMessageInfo(self, mid, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        message_info = self._forcedFetch(thread_id, mid).get(\"message\")\n        return Message._from_graphql(message_info)", "response": "Fetches the message info from the given message id and thread_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch a list of PollOption objects from the poll idpoll_id", "response": "def fetchPollOptions(self, poll_id):\n        \"\"\"\n        Fetches list of :class:`models.PollOption` objects from the poll id\n\n        :param poll_id: Poll ID to fetch from\n        :rtype: list\n        :raises: FBchatException if request failed\n        \"\"\"\n        data = {\"question_id\": poll_id}\n        j = self._post(\n            self.req_url.GET_POLL_OPTIONS, data, fix_request=True, as_json=True\n        )\n        return [PollOption._from_graphql(m) for m in j[\"payload\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch a : class : models. Plan object from the plan id", "response": "def fetchPlanInfo(self, plan_id):\n        \"\"\"\n        Fetches a :class:`models.Plan` object from the plan id\n\n        :param plan_id: Plan ID to fetch from\n        :return: :class:`models.Plan` object\n        :rtype: models.Plan\n        :raises: FBchatException if request failed\n        \"\"\"\n        data = {\"event_reminder_id\": plan_id}\n        j = self._post(self.req_url.PLAN_INFO, data, fix_request=True, as_json=True)\n        return Plan._from_fetch(j[\"payload\"])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getSendData(self, message=None, thread_id=None, thread_type=ThreadType.USER):\n        messageAndOTID = generateOfflineThreadingID()\n        timestamp = now()\n        data = {\n            \"client\": \"mercury\",\n            \"author\": \"fbid:{}\".format(self._uid),\n            \"timestamp\": timestamp,\n            \"source\": \"source:chat:web\",\n            \"offline_threading_id\": messageAndOTID,\n            \"message_id\": messageAndOTID,\n            \"threading_id\": generateMessageID(self._client_id),\n            \"ephemeral_ttl_mode:\": \"0\",\n        }\n\n        # Set recipient\n        if thread_type in [ThreadType.USER, ThreadType.PAGE]:\n            data[\"other_user_fbid\"] = thread_id\n        elif thread_type == ThreadType.GROUP:\n            data[\"thread_fbid\"] = thread_id\n\n        if message is None:\n            message = Message()\n\n        if message.text or message.sticker or message.emoji_size:\n            data[\"action_type\"] = \"ma-type:user-generated-message\"\n\n        if message.text:\n            data[\"body\"] = message.text\n\n        for i, mention in enumerate(message.mentions):\n            data[\"profile_xmd[{}][id]\".format(i)] = mention.thread_id\n            data[\"profile_xmd[{}][offset]\".format(i)] = mention.offset\n            data[\"profile_xmd[{}][length]\".format(i)] = mention.length\n            data[\"profile_xmd[{}][type]\".format(i)] = \"p\"\n\n        if message.emoji_size:\n            if message.text:\n                data[\"tags[0]\"] = \"hot_emoji_size:\" + message.emoji_size.name.lower()\n            else:\n                data[\"sticker_id\"] = message.emoji_size.value\n\n        if message.sticker:\n            data[\"sticker_id\"] = message.sticker.uid\n\n        if message.quick_replies:\n            xmd = {\"quick_replies\": []}\n            for quick_reply in message.quick_replies:\n                q = dict()\n                q[\"content_type\"] = quick_reply._type\n                q[\"payload\"] = quick_reply.payload\n                q[\"external_payload\"] = quick_reply.external_payload\n                q[\"data\"] = quick_reply.data\n                if quick_reply.is_response:\n                    q[\"ignore_for_webhook\"] = False\n                if isinstance(quick_reply, QuickReplyText):\n                    q[\"title\"] = quick_reply.title\n                if not isinstance(quick_reply, QuickReplyLocation):\n                    q[\"image_url\"] = quick_reply.image_url\n                xmd[\"quick_replies\"].append(q)\n            if len(message.quick_replies) == 1 and message.quick_replies[0].is_response:\n                xmd[\"quick_replies\"] = xmd[\"quick_replies\"][0]\n            data[\"platform_xmd\"] = json.dumps(xmd)\n\n        if message.reply_to_id:\n            data[\"replied_to_message_id\"] = message.reply_to_id\n\n        return data", "response": "Returns the data needed to send a message to the SendURL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending the data to SendURL and returns the message ID or None on failure", "response": "def _doSendRequest(self, data, get_thread_id=False):\n        \"\"\"Sends the data to `SendURL`, and returns the message ID or None on failure\"\"\"\n        j = self._post(self.req_url.SEND, data, fix_request=True, as_json=True)\n\n        # update JS token if received in response\n        fb_dtsg = get_jsmods_require(j, 2)\n        if fb_dtsg is not None:\n            self._payload_default[\"fb_dtsg\"] = fb_dtsg\n\n        try:\n            message_ids = [\n                (action[\"message_id\"], action[\"thread_fbid\"])\n                for action in j[\"payload\"][\"actions\"]\n                if \"message_id\" in action\n            ]\n            if len(message_ids) != 1:\n                log.warning(\"Got multiple message ids' back: {}\".format(message_ids))\n            if get_thread_id:\n                return message_ids[0]\n            else:\n                return message_ids[0][0]\n        except (KeyError, IndexError, TypeError) as e:\n            raise FBchatException(\n                \"Error when sending message: \"\n                \"No message IDs could be found: {}\".format(j)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to a thread", "response": "def send(self, message, thread_id=None, thread_type=ThreadType.USER):\n        \"\"\"\n        Sends a message to a thread\n\n        :param message: Message to send\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type message: models.Message\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent message\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, thread_type)\n        data = self._getSendData(\n            message=message, thread_id=thread_id, thread_type=thread_type\n        )\n        return self._doSendRequest(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to a thread.", "response": "def sendMessage(self, message, thread_id=None, thread_type=ThreadType.USER):\n        \"\"\"\n        Deprecated. Use :func:`fbchat.Client.send` instead\n        \"\"\"\n        return self.send(\n            Message(text=message), thread_id=thread_id, thread_type=thread_type\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends an emoji to a user or a group.", "response": "def sendEmoji(\n        self,\n        emoji=None,\n        size=EmojiSize.SMALL,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n    ):\n        \"\"\"\n        Deprecated. Use :func:`fbchat.Client.send` instead\n        \"\"\"\n        return self.send(\n            Message(text=emoji, emoji_size=size),\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wave(self, wave_first=True, thread_id=None, thread_type=None):\n        thread_id, thread_type = self._getThread(thread_id, thread_type)\n        data = self._getSendData(thread_id=thread_id, thread_type=thread_type)\n        data[\"action_type\"] = \"ma-type:user-generated-message\"\n        data[\"lightweight_action_attachment[lwa_state]\"] = (\n            \"INITIATED\" if wave_first else \"RECIPROCATED\"\n        )\n        data[\"lightweight_action_attachment[lwa_type]\"] = \"WAVE\"\n        if thread_type == ThreadType.USER:\n            data[\"specific_to_list[0]\"] = \"fbid:{}\".format(thread_id)\n        return self._doSendRequest(data)", "response": "Send a wave to a thread!"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a quick reply to a thread", "response": "def quickReply(self, quick_reply, payload=None, thread_id=None, thread_type=None):\n        \"\"\"\n        Replies to a chosen quick reply\n\n        :param quick_reply: Quick reply to reply to\n        :param payload: Optional answer to the quick reply\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type quick_reply: models.QuickReply\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent message\n        :raises: FBchatException if request failed\n        \"\"\"\n        quick_reply.is_response = True\n        if isinstance(quick_reply, QuickReplyText):\n            return self.send(\n                Message(text=quick_reply.title, quick_replies=[quick_reply])\n            )\n        elif isinstance(quick_reply, QuickReplyLocation):\n            if not isinstance(payload, LocationAttachment):\n                raise ValueError(\n                    \"Payload must be an instance of `fbchat.models.LocationAttachment`\"\n                )\n            return self.sendLocation(\n                payload, thread_id=thread_id, thread_type=thread_type\n            )\n        elif isinstance(quick_reply, QuickReplyEmail):\n            if not payload:\n                payload = self.getEmails()[0]\n            quick_reply.external_payload = quick_reply.payload\n            quick_reply.payload = payload\n            return self.send(Message(text=payload, quick_replies=[quick_reply]))\n        elif isinstance(quick_reply, QuickReplyPhoneNumber):\n            if not payload:\n                payload = self.getPhoneNumbers()[0]\n            quick_reply.external_payload = quick_reply.payload\n            quick_reply.payload = payload\n            return self.send(Message(text=payload, quick_replies=[quick_reply]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unsend(self, mid):\n        data = {\"message_id\": mid}\n        r = self._post(self.req_url.UNSEND, data)\n        r.raise_for_status()", "response": "Unsends a message from the current user"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendLocation(self, location, message=None, thread_id=None, thread_type=None):\n        self._sendLocation(\n            location=location,\n            current=True,\n            message=message,\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )", "response": "Send a given location to a thread as the current location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sendPinnedLocation(\n        self, location, message=None, thread_id=None, thread_type=None\n    ):\n        \"\"\"\n        Sends a given location to a thread as a pinned location\n\n        :param location: Location to send\n        :param message: Additional message\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type location: models.LocationAttachment\n        :type message: models.Message\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent message\n        :raises: FBchatException if request failed\n        \"\"\"\n        self._sendLocation(\n            location=location,\n            current=False,\n            message=message,\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )", "response": "Send a given location to a thread as pinned"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading files to Facebook and returns a list of tuples with a file s ID and mimetype", "response": "def _upload(self, files, voice_clip=False):\n        \"\"\"\n        Uploads files to Facebook\n\n        `files` should be a list of files that requests can upload, see:\n        http://docs.python-requests.org/en/master/api/#requests.request\n\n        Returns a list of tuples with a file's ID and mimetype\n        \"\"\"\n        file_dict = {\"upload_{}\".format(i): f for i, f in enumerate(files)}\n\n        data = {\"voice_clip\": voice_clip}\n\n        j = self._postFile(\n            self.req_url.UPLOAD,\n            files=file_dict,\n            query=data,\n            fix_request=True,\n            as_json=True,\n        )\n\n        if len(j[\"payload\"][\"metadata\"]) != len(files):\n            raise FBchatException(\n                \"Some files could not be uploaded: {}, {}\".format(j, files)\n            )\n\n        return [\n            (data[mimetype_to_key(data[\"filetype\"])], data[\"filetype\"])\n            for data in j[\"payload\"][\"metadata\"]\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sendFiles(\n        self, files, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Sends files from file IDs to a thread\n\n        `files` should be a list of tuples, with a file's ID and mimetype\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, thread_type)\n        data = self._getSendData(\n            message=self._oldMessage(message),\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )\n\n        data[\"action_type\"] = \"ma-type:user-generated-message\"\n        data[\"has_attachment\"] = True\n\n        for i, (file_id, mimetype) in enumerate(files):\n            data[\"{}s[{}]\".format(mimetype_to_key(mimetype), i)] = file_id\n\n        return self._doSendRequest(data)", "response": "Send files from file IDs to a thread"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending files from URLs to a thread.", "response": "def sendRemoteFiles(\n        self, file_urls, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Sends files from URLs to a thread\n\n        :param file_urls: URLs of files to upload and send\n        :param message: Additional message\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent files\n        :raises: FBchatException if request failed\n        \"\"\"\n        file_urls = require_list(file_urls)\n        files = self._upload(get_files_from_urls(file_urls))\n        return self._sendFiles(\n            files=files, message=message, thread_id=thread_id, thread_type=thread_type\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sendLocalFiles(\n        self, file_paths, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Sends local files to a thread\n\n        :param file_paths: Paths of files to upload and send\n        :param message: Additional message\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent files\n        :raises: FBchatException if request failed\n        \"\"\"\n        file_paths = require_list(file_paths)\n        with get_files_from_paths(file_paths) as x:\n            files = self._upload(x)\n        return self._sendFiles(\n            files=files, message=message, thread_id=thread_id, thread_type=thread_type\n        )", "response": "Sends local files to a thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending voice clips from URLs to a thread", "response": "def sendRemoteVoiceClips(\n        self, clip_urls, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Sends voice clips from URLs to a thread\n\n        :param clip_urls: URLs of clips to upload and send\n        :param message: Additional message\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent files\n        :raises: FBchatException if request failed\n        \"\"\"\n        clip_urls = require_list(clip_urls)\n        files = self._upload(get_files_from_urls(clip_urls), voice_clip=True)\n        return self._sendFiles(\n            files=files, message=message, thread_id=thread_id, thread_type=thread_type\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sendLocalVoiceClips(\n        self, clip_paths, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Sends local voice clips to a thread\n\n        :param clip_paths: Paths of clips to upload and send\n        :param message: Additional message\n        :param thread_id: User/Group ID to send to. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type thread_type: models.ThreadType\n        :return: :ref:`Message ID <intro_message_ids>` of the sent files\n        :raises: FBchatException if request failed\n        \"\"\"\n        clip_paths = require_list(clip_paths)\n        with get_files_from_paths(clip_paths) as x:\n            files = self._upload(x, voice_clip=True)\n        return self._sendFiles(\n            files=files, message=message, thread_id=thread_id, thread_type=thread_type\n        )", "response": "Sends local voice clips to a thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sendImage(\n        self,\n        image_id,\n        message=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        is_gif=False,\n    ):\n        \"\"\"\n        Deprecated. Use :func:`fbchat.Client._sendFiles` instead\n        \"\"\"\n        if is_gif:\n            mimetype = \"image/gif\"\n        else:\n            mimetype = \"image/png\"\n        return self._sendFiles(\n            files=[(image_id, mimetype)],\n            message=message,\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )", "response": "Send an image to a thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendRemoteImage(\n        self, image_url, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Deprecated. Use :func:`fbchat.Client.sendRemoteFiles` instead\n        \"\"\"\n        return self.sendRemoteFiles(\n            file_urls=[image_url],\n            message=message,\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )", "response": "Send an image to a thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending local image to the specified thread.", "response": "def sendLocalImage(\n        self, image_path, message=None, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Deprecated. Use :func:`fbchat.Client.sendLocalFiles` instead\n        \"\"\"\n        return self.sendLocalFiles(\n            file_paths=[image_path],\n            message=message,\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forwardAttachment(self, attachment_id, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        data = {\n            \"attachment_id\": attachment_id,\n            \"recipient_map[{}]\".format(generateOfflineThreadingID()): thread_id,\n        }\n        j = self._post(\n            self.req_url.FORWARD_ATTACHMENT, data, fix_request=True, as_json=True\n        )", "response": "Forwards an attachment to a thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef createGroup(self, message, user_ids):\n        data = self._getSendData(message=self._oldMessage(message))\n\n        if len(user_ids) < 2:\n            raise FBchatUserError(\"Error when creating group: Not enough participants\")\n\n        for i, user_id in enumerate(user_ids + [self._uid]):\n            data[\"specific_to_list[{}]\".format(i)] = \"fbid:{}\".format(user_id)\n\n        message_id, thread_id = self._doSendRequest(data, get_thread_id=True)\n        if not thread_id:\n            raise FBchatException(\n                \"Error when creating group: No thread_id could be found\"\n            )\n        return thread_id", "response": "Creates a group with the given list of users."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addUsersToGroup(self, user_ids, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        data = self._getSendData(thread_id=thread_id, thread_type=ThreadType.GROUP)\n\n        data[\"action_type\"] = \"ma-type:log-message\"\n        data[\"log_message_type\"] = \"log:subscribe\"\n\n        user_ids = require_list(user_ids)\n\n        for i, user_id in enumerate(user_ids):\n            if user_id == self._uid:\n                raise FBchatUserError(\n                    \"Error when adding users: Cannot add self to group thread\"\n                )\n            else:\n                data[\n                    \"log_message_data[added_participants][{}]\".format(i)\n                ] = \"fbid:{}\".format(user_id)\n\n        return self._doSendRequest(data)", "response": "Adds users to a group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving users from a group.", "response": "def removeUserFromGroup(self, user_id, thread_id=None):\n        \"\"\"\n        Removes users from a group.\n\n        :param user_id: User ID to remove\n        :param thread_id: Group ID to remove people from. See :ref:`intro_threads`\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\"uid\": user_id, \"tid\": thread_id}\n        j = self._post(self.req_url.REMOVE_USER, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef changeGroupApprovalMode(self, require_admin_approval, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\"set_mode\": int(require_admin_approval), \"thread_fbid\": thread_id}\n        j = self._post(self.req_url.APPROVAL_MODE, data, fix_request=True, as_json=True)", "response": "Changes the group s approval mode"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _changeGroupImage(self, image_id, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\"thread_image_id\": image_id, \"thread_id\": thread_id}\n\n        j = self._post(self.req_url.THREAD_IMAGE, data, fix_request=True, as_json=True)\n        return image_id", "response": "Changes a thread image from an image id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef changeGroupImageRemote(self, image_url, thread_id=None):\n        (image_id, mimetype), = self._upload(get_files_from_urls([image_url]))\n        return self._changeGroupImage(image_id, thread_id)", "response": "Changes a thread image from a URL"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef changeGroupImageLocal(self, image_path, thread_id=None):\n        with get_files_from_paths([image_path]) as files:\n            (image_id, mimetype), = self._upload(files)\n\n        return self._changeGroupImage(image_id, thread_id)", "response": "Changes a thread image from a local path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef changeThreadTitle(self, title, thread_id=None, thread_type=ThreadType.USER):\n        thread_id, thread_type = self._getThread(thread_id, thread_type)\n\n        if thread_type == ThreadType.USER:\n            # The thread is a user, so we change the user's nickname\n            return self.changeNickname(\n                title, thread_id, thread_id=thread_id, thread_type=thread_type\n            )\n\n        data = {\"thread_name\": title, \"thread_id\": thread_id}\n        j = self._post(self.req_url.THREAD_NAME, data, fix_request=True, as_json=True)", "response": "Changes the title of a thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the nickname of a user in a thread.", "response": "def changeNickname(\n        self, nickname, user_id, thread_id=None, thread_type=ThreadType.USER\n    ):\n        \"\"\"\n        Changes the nickname of a user in a thread\n\n        :param nickname: New nickname\n        :param user_id: User that will have their nickname changed\n        :param thread_id: User/Group ID to change color of. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type thread_type: models.ThreadType\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, thread_type)\n\n        data = {\n            \"nickname\": nickname,\n            \"participant_id\": user_id,\n            \"thread_or_other_fbid\": thread_id,\n        }\n        j = self._post(\n            self.req_url.THREAD_NICKNAME, data, fix_request=True, as_json=True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef changeThreadColor(self, color, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\n            \"color_choice\": color.value if color != ThreadColor.MESSENGER_BLUE else \"\",\n            \"thread_or_other_fbid\": thread_id,\n        }\n        j = self._post(self.req_url.THREAD_COLOR, data, fix_request=True, as_json=True)", "response": "Changes thread color\n\n        :param color: New thread color\n        :param thread_id: User/Group ID to change color of. See :ref:`intro_threads`\n        :type color: models.ThreadColor\n        :raises: FBchatException if request failed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the color of a thread s emoji", "response": "def changeThreadEmoji(self, emoji, thread_id=None):\n        \"\"\"\n        Changes thread color\n\n        Trivia: While changing the emoji, the Facebook web client actually sends multiple different requests, though only this one is required to make the change\n\n        :param color: New thread emoji\n        :param thread_id: User/Group ID to change emoji of. See :ref:`intro_threads`\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\"emoji_choice\": emoji, \"thread_or_other_fbid\": thread_id}\n        j = self._post(self.req_url.THREAD_EMOJI, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreacts to a message or removes reaction", "response": "def reactToMessage(self, message_id, reaction):\n        \"\"\"\n        Reacts to a message, or removes reaction\n\n        :param message_id: :ref:`Message ID <intro_message_ids>` to react to\n        :param reaction: Reaction emoji to use, if None removes reaction\n        :type reaction: models.MessageReaction or None\n        :raises: FBchatException if request failed\n        \"\"\"\n        data = {\n            \"action\": \"ADD_REACTION\" if reaction else \"REMOVE_REACTION\",\n            \"client_mutation_id\": \"1\",\n            \"actor_id\": self._uid,\n            \"message_id\": str(message_id),\n            \"reaction\": reaction.value if reaction else None,\n        }\n        data = {\"doc_id\": 1491398900900362, \"variables\": json.dumps({\"data\": data})}\n        self._post(self.req_url.MESSAGE_REACTION, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createPlan(self, plan, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\n            \"event_type\": \"EVENT\",\n            \"event_time\": plan.time,\n            \"title\": plan.title,\n            \"thread_id\": thread_id,\n            \"location_id\": plan.location_id or \"\",\n            \"location_name\": plan.location or \"\",\n            \"acontext\": ACONTEXT,\n        }\n        j = self._post(self.req_url.PLAN_CREATE, data, fix_request=True, as_json=True)", "response": "Sets a new plan in the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nediting a plan :param plan: Plan to edit :param new_plan: New plan :type plan: models.Plan :raises: FBchatException if request failed", "response": "def editPlan(self, plan, new_plan):\n        \"\"\"\n        Edits a plan\n\n        :param plan: Plan to edit\n        :param new_plan: New plan\n        :type plan: models.Plan\n        :raises: FBchatException if request failed\n        \"\"\"\n        data = {\n            \"event_reminder_id\": plan.uid,\n            \"delete\": \"false\",\n            \"date\": new_plan.time,\n            \"location_name\": new_plan.location or \"\",\n            \"location_id\": new_plan.location_id or \"\",\n            \"title\": new_plan.title,\n            \"acontext\": ACONTEXT,\n        }\n        j = self._post(self.req_url.PLAN_CHANGE, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a specific plan", "response": "def deletePlan(self, plan):\n        \"\"\"\n        Deletes a plan\n\n        :param plan: Plan to delete\n        :raises: FBchatException if request failed\n        \"\"\"\n        data = {\"event_reminder_id\": plan.uid, \"delete\": \"true\", \"acontext\": ACONTEXT}\n        j = self._post(self.req_url.PLAN_CHANGE, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef changePlanParticipation(self, plan, take_part=True):\n        data = {\n            \"event_reminder_id\": plan.uid,\n            \"guest_state\": \"GOING\" if take_part else \"DECLINED\",\n            \"acontext\": ACONTEXT,\n        }\n        j = self._post(\n            self.req_url.PLAN_PARTICIPATION, data, fix_request=True, as_json=True\n        )", "response": "Changes participation in a plan"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eventReminder(self, thread_id, time, title, location=\"\", location_id=\"\"):\n        plan = Plan(time=time, title=title, location=location, location_id=location_id)\n        self.createPlan(plan=plan, thread_id=thread_id)", "response": "Create a new event reminder for a thread."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a poll in a group thread.", "response": "def createPoll(self, poll, thread_id=None):\n        \"\"\"\n        Creates poll in a group thread\n\n        :param poll: Poll to create\n        :param thread_id: User/Group ID to create poll in. See :ref:`intro_threads`\n        :type poll: models.Poll\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, None)\n\n        # We're using ordered dicts, because the Facebook endpoint that parses the POST\n        # parameters is badly implemented, and deals with ordering the options wrongly.\n        # This also means we had to change `client._payload_default` to an ordered dict,\n        # since that's being copied in between this point and the `requests` call\n        #\n        # If you can find a way to fix this for the endpoint, or if you find another\n        # endpoint, please do suggest it ;)\n        data = OrderedDict([(\"question_text\", poll.title), (\"target_id\", thread_id)])\n\n        for i, option in enumerate(poll.options):\n            data[\"option_text_array[{}]\".format(i)] = option.text\n            data[\"option_is_selected_array[{}]\".format(i)] = str(int(option.vote))\n\n        j = self._post(self.req_url.CREATE_POLL, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef updatePollVote(self, poll_id, option_ids=[], new_options=[]):\n        data = {\"question_id\": poll_id}\n\n        for i, option_id in enumerate(option_ids):\n            data[\"selected_options[{}]\".format(i)] = option_id\n\n        for i, option_text in enumerate(new_options):\n            data[\"new_options[{}]\".format(i)] = option_text\n\n        j = self._post(self.req_url.UPDATE_VOTE, data, fix_request=True, as_json=True)", "response": "Updates a poll s vote"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the typing status in a thread", "response": "def setTypingStatus(self, status, thread_id=None, thread_type=None):\n        \"\"\"\n        Sets users typing status in a thread\n\n        :param status: Specify the typing status\n        :param thread_id: User/Group ID to change status in. See :ref:`intro_threads`\n        :param thread_type: See :ref:`intro_threads`\n        :type status: models.TypingStatus\n        :type thread_type: models.ThreadType\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_id, thread_type = self._getThread(thread_id, thread_type)\n\n        data = {\n            \"typ\": status.value,\n            \"thread\": thread_id,\n            \"to\": thread_id if thread_type == ThreadType.USER else \"\",\n            \"source\": \"mercury-chat\",\n        }\n        j = self._post(self.req_url.TYPING, data, fix_request=True, as_json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef markAsDelivered(self, thread_id, message_id):\n        data = {\n            \"message_ids[0]\": message_id,\n            \"thread_ids[%s][0]\" % thread_id: message_id,\n        }\n\n        r = self._post(self.req_url.DELIVERED, data)\n        return r.ok", "response": "Mark a message as delivered."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef markAsSeen(self):\n        r = self._post(self.req_url.MARK_SEEN, {\"seen_timestamp\": now()})\n        return r.ok", "response": ".. todo::\n            Documenting this"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef friendConnect(self, friend_id):\n        data = {\"to_friend\": friend_id, \"action\": \"confirm\"}\n\n        r = self._post(self.req_url.CONNECT, data)\n        return r.ok", "response": "Connect to a user s domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a specifed friend from your friend list.", "response": "def removeFriend(self, friend_id=None):\n        \"\"\"\n        Removes a specifed friend from your friend list\n\n        :param friend_id: The ID of the friend that you want to remove\n        :return: Returns error if the removing was unsuccessful, returns True when successful.\n        \"\"\"\n        payload = {\"friend_id\": friend_id, \"unref\": \"none\", \"confirm\": \"Confirm\"}\n        r = self._post(self.req_url.REMOVE_FRIEND, payload)\n        query = parse_qs(urlparse(r.url).query)\n        if \"err\" not in query:\n            log.debug(\"Remove was successful!\")\n            return True\n        else:\n            log.warning(\"Error while removing friend\")\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blockUser(self, user_id):\n        data = {\"fbid\": user_id}\n        r = self._post(self.req_url.BLOCK_USER, data)\n        return r.ok", "response": "Blocks messages from a specifed user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unblockUser(self, user_id):\n        data = {\"fbid\": user_id}\n        r = self._post(self.req_url.UNBLOCK_USER, data)\n        return r.ok", "response": "Unblocks messages from a blocked user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove threads to specified location", "response": "def moveThreads(self, location, thread_ids):\n        \"\"\"\n        Moves threads to specifed location\n\n        :param location: models.ThreadLocation: INBOX, PENDING, ARCHIVED or OTHER\n        :param thread_ids: Thread IDs to move. See :ref:`intro_threads`\n        :return: Whether the request was successful\n        :raises: FBchatException if request failed\n        \"\"\"\n        thread_ids = require_list(thread_ids)\n\n        if location == ThreadLocation.PENDING:\n            location = ThreadLocation.OTHER\n\n        if location == ThreadLocation.ARCHIVED:\n            data_archive = dict()\n            data_unpin = dict()\n            for thread_id in thread_ids:\n                data_archive[\"ids[{}]\".format(thread_id)] = \"true\"\n                data_unpin[\"ids[{}]\".format(thread_id)] = \"false\"\n            r_archive = self._post(self.req_url.ARCHIVED_STATUS, data_archive)\n            r_unpin = self._post(self.req_url.PINNED_STATUS, data_unpin)\n            return r_archive.ok and r_unpin.ok\n        else:\n            data = dict()\n            for i, thread_id in enumerate(thread_ids):\n                data[\"{}[{}]\".format(location.name.lower(), i)] = thread_id\n            r = self._post(self.req_url.MOVE_THREAD, data)\n            return r.ok"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deleteThreads(self, thread_ids):\n        thread_ids = require_list(thread_ids)\n\n        data_unpin = dict()\n        data_delete = dict()\n        for i, thread_id in enumerate(thread_ids):\n            data_unpin[\"ids[{}]\".format(thread_id)] = \"false\"\n            data_delete[\"ids[{}]\".format(i)] = thread_id\n        r_unpin = self._post(self.req_url.PINNED_STATUS, data_unpin)\n        r_delete = self._post(self.req_url.DELETE_THREAD, data_delete)\n        return r_unpin.ok and r_delete.ok", "response": "Deletes threads from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef markAsSpam(self, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        r = self._post(self.req_url.MARK_SPAM, {\"id\": thread_id})\n        return r.ok", "response": "Mark a thread as spam and delete it"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the specified messages from the cache.", "response": "def deleteMessages(self, message_ids):\n        \"\"\"\n        Deletes specifed messages\n\n        :param message_ids: Message IDs to delete\n        :return: Whether the request was successful\n        :raises: FBchatException if request failed\n        \"\"\"\n        message_ids = require_list(message_ids)\n        data = dict()\n        for i, message_id in enumerate(message_ids):\n            data[\"message_ids[{}]\".format(i)] = message_id\n        r = self._post(self.req_url.DELETE_MESSAGES, data)\n        return r.ok"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef muteThread(self, mute_time=-1, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        data = {\"mute_settings\": str(mute_time), \"thread_fbid\": thread_id}\n        content = self._post(self.req_url.MUTE_THREAD, data, fix_request=True)", "response": "Mutes thread\n\n        :param mute_time: Mute time in seconds, leave blank to mute forever\n        :param thread_id: User/Group ID to mute. See :ref:`intro_threads`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef muteThreadReactions(self, mute=True, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        data = {\"reactions_mute_mode\": int(mute), \"thread_fbid\": thread_id}\n        r = self._post(self.req_url.MUTE_REACTIONS, data, fix_request=True)", "response": "Mutes the reactions of a thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef muteThreadMentions(self, mute=True, thread_id=None):\n        thread_id, thread_type = self._getThread(thread_id, None)\n        data = {\"mentions_mute_mode\": int(mute), \"thread_fbid\": thread_id}\n        r = self._post(self.req_url.MUTE_MENTIONS, data, fix_request=True)", "response": "Mutes the mentions of a thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pullMessage(self):\n        data = {\n            \"msgs_recv\": 0,\n            \"sticky_token\": self._sticky,\n            \"sticky_pool\": self._pool,\n            \"clientid\": self._client_id,\n            \"state\": \"active\" if self._markAlive else \"offline\",\n        }\n        return self._get(self.req_url.STICKY, data, fix_request=True, as_json=True)", "response": "Call pull api with seq value to get message data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parseMessage(self, content):\n        self._seq = content.get(\"seq\", \"0\")\n\n        if \"lb_info\" in content:\n            self._sticky = content[\"lb_info\"][\"sticky\"]\n            self._pool = content[\"lb_info\"][\"pool\"]\n\n        if \"batches\" in content:\n            for batch in content[\"batches\"]:\n                self._parseMessage(batch)\n\n        if \"ms\" not in content:\n            return\n\n        for m in content[\"ms\"]:\n            mtype = m.get(\"type\")\n            try:\n                # Things that directly change chat\n                if mtype == \"delta\":\n                    self._parseDelta(m)\n                # Inbox\n                elif mtype == \"inbox\":\n                    self.onInbox(\n                        unseen=m[\"unseen\"],\n                        unread=m[\"unread\"],\n                        recent_unread=m[\"recent_unread\"],\n                        msg=m,\n                    )\n\n                # Typing\n                elif mtype == \"typ\" or mtype == \"ttyp\":\n                    author_id = str(m.get(\"from\"))\n                    thread_id = m.get(\"thread_fbid\")\n                    if thread_id:\n                        thread_type = ThreadType.GROUP\n                        thread_id = str(thread_id)\n                    else:\n                        thread_type = ThreadType.USER\n                        if author_id == self._uid:\n                            thread_id = m.get(\"to\")\n                        else:\n                            thread_id = author_id\n                    typing_status = TypingStatus(m.get(\"st\"))\n                    self.onTyping(\n                        author_id=author_id,\n                        status=typing_status,\n                        thread_id=thread_id,\n                        thread_type=thread_type,\n                        msg=m,\n                    )\n\n                # Delivered\n\n                # Seen\n                # elif mtype == \"m_read_receipt\":\n                #\n                #     self.onSeen(m.get('realtime_viewer_fbid'), m.get('reader'), m.get('time'))\n\n                elif mtype in [\"jewel_requests_add\"]:\n                    from_id = m[\"from\"]\n                    self.onFriendRequest(from_id=from_id, msg=m)\n\n                # Happens on every login\n                elif mtype == \"qprimer\":\n                    self.onQprimer(ts=m.get(\"made\"), msg=m)\n\n                # Is sent before any other message\n                elif mtype == \"deltaflow\":\n                    pass\n\n                # Chat timestamp\n                elif mtype == \"chatproxy-presence\":\n                    statuses = dict()\n                    for id_, data in m.get(\"buddyList\", {}).items():\n                        statuses[id_] = ActiveStatus._from_chatproxy_presence(id_, data)\n                        self._buddylist[id_] = statuses[id_]\n\n                    self.onChatTimestamp(buddylist=statuses, msg=m)\n\n                # Buddylist overlay\n                elif mtype == \"buddylist_overlay\":\n                    statuses = dict()\n                    for id_, data in m.get(\"overlay\", {}).items():\n                        old_in_game = None\n                        if id_ in self._buddylist:\n                            old_in_game = self._buddylist[id_].in_game\n\n                        statuses[id_] = ActiveStatus._from_buddylist_overlay(\n                            data, old_in_game\n                        )\n                        self._buddylist[id_] = statuses[id_]\n\n                    self.onBuddylistOverlay(statuses=statuses, msg=m)\n\n                # Unknown message type\n                else:\n                    self.onUnknownMesssageType(msg=m)\n\n            except Exception as e:\n                self.onMessageError(exception=e, msg=m)", "response": "Parses the message and author name from the content."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndoes one cycle of the listening loop. This method is useful if you want to control fbchat from an external event loop .. warning:: `markAlive` parameter is deprecated now, use :func:`fbchat.Client.setActiveStatus` or `markAlive` parameter in :func:`fbchat.Client.listen` instead. :return: Whether the loop should keep running :rtype: bool", "response": "def doOneListen(self, markAlive=None):\n        \"\"\"\n        Does one cycle of the listening loop.\n        This method is useful if you want to control fbchat from an external event loop\n\n        .. warning::\n            `markAlive` parameter is deprecated now, use :func:`fbchat.Client.setActiveStatus`\n            or `markAlive` parameter in :func:`fbchat.Client.listen` instead.\n\n        :return: Whether the loop should keep running\n        :rtype: bool\n        \"\"\"\n        if markAlive is not None:\n            self._markAlive = markAlive\n        try:\n            if self._markAlive:\n                self._ping()\n            content = self._pullMessage()\n            if content:\n                self._parseMessage(content)\n        except KeyboardInterrupt:\n            return False\n        except requests.Timeout:\n            pass\n        except requests.ConnectionError:\n            # If the client has lost their internet connection, keep trying every 30 seconds\n            time.sleep(30)\n        except FBchatFacebookError as e:\n            # Fix 502 and 503 pull errors\n            if e.request_status_code in [502, 503]:\n                self.req_url.change_pull_channel()\n                self.startListening()\n            else:\n                raise e\n        except Exception as e:\n            return self.onListenError(exception=e)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef listen(self, markAlive=None):\n        if markAlive is not None:\n            self.setActiveStatus(markAlive)\n\n        self.startListening()\n        self.onListening()\n\n        while self.listening and self.doOneListen():\n            pass\n\n        self.stopListening()", "response": "Starts the listening loop continually"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef onMessage(\n        self,\n        mid=None,\n        author_id=None,\n        message=None,\n        message_object=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody sends a message\n\n        :param mid: The message ID\n        :param author_id: The ID of the author\n        :param message: (deprecated. Use `message_object.text` instead)\n        :param message_object: The message (As a `Message` object)\n        :param thread_id: Thread ID that the message was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the message was sent to. See :ref:`intro_threads`\n        :param ts: The timestamp of the message\n        :param metadata: Extra metadata about the message\n        :param msg: A full set of the data recieved\n        :type message_object: models.Message\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\"{} from {} in {}\".format(message_object, thread_id, thread_type.name))", "response": "Called when the client is listening and somebody sends a message to the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onColorChange(\n        self,\n        mid=None,\n        author_id=None,\n        new_color=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody changes a thread's color\n\n        :param mid: The action ID\n        :param author_id: The ID of the person who changed the color\n        :param new_color: The new color\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type new_color: models.ThreadColor\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Color change from {} in {} ({}): {}\".format(\n                author_id, thread_id, thread_type.name, new_color\n            )\n        )", "response": "Called when somebody changes a color of a thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when somebody changes an internal emoji of a thread.", "response": "def onEmojiChange(\n        self,\n        mid=None,\n        author_id=None,\n        new_emoji=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody changes a thread's emoji\n\n        :param mid: The action ID\n        :param author_id: The ID of the person who changed the emoji\n        :param new_emoji: The new emoji\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Emoji change from {} in {} ({}): {}\".format(\n                author_id, thread_id, thread_type.name, new_emoji\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onTitleChange(\n        self,\n        mid=None,\n        author_id=None,\n        new_title=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody changes the title of a thread\n\n        :param mid: The action ID\n        :param author_id: The ID of the person who changed the title\n        :param new_title: The new title\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Title change from {} in {} ({}): {}\".format(\n                author_id, thread_id, thread_type.name, new_title\n            )\n        )", "response": "Called when somebody changes the title of a thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef onImageChange(\n        self,\n        mid=None,\n        author_id=None,\n        new_image=None,\n        thread_id=None,\n        thread_type=ThreadType.GROUP,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody changes the image of a thread\n\n        :param mid: The action ID\n        :param author_id: The ID of the person who changed the image\n        :param new_image: The ID of the new image\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\"{} changed thread image in {}\".format(author_id, thread_id))", "response": "Called when the client is listening and somebody changes the image of a thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef onNicknameChange(\n        self,\n        mid=None,\n        author_id=None,\n        changed_for=None,\n        new_nickname=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody changes the nickname of a person\n\n        :param mid: The action ID\n        :param author_id: The ID of the person who changed the nickname\n        :param changed_for: The ID of the person whom got their nickname changed\n        :param new_nickname: The new nickname\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Nickname change from {} in {} ({}) for {}: {}\".format(\n                author_id, thread_id, thread_type.name, changed_for, new_nickname\n            )\n        )", "response": "Called when the client is listening and somebody changes the nickname of a person."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when somebody adds an admin to a group thread", "response": "def onAdminAdded(\n        self,\n        mid=None,\n        added_id=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=ThreadType.GROUP,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody adds an admin to a group thread\n\n        :param mid: The action ID\n        :param added_id: The ID of the admin who got added\n        :param author_id: The ID of the person who added the admins\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        \"\"\"\n        log.info(\"{} added admin: {} in {}\".format(author_id, added_id, thread_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when somebody removes an admin from a group thread", "response": "def onAdminRemoved(\n        self,\n        mid=None,\n        removed_id=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=ThreadType.GROUP,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody removes an admin from a group thread\n\n        :param mid: The action ID\n        :param removed_id: The ID of the admin who got removed\n        :param author_id: The ID of the person who removed the admins\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        \"\"\"\n        log.info(\"{} removed admin: {} in {}\".format(author_id, removed_id, thread_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef onApprovalModeChange(\n        self,\n        mid=None,\n        approval_mode=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=ThreadType.GROUP,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody changes approval mode in a group thread\n\n        :param mid: The action ID\n        :param approval_mode: True if approval mode is activated\n        :param author_id: The ID of the person who changed approval mode\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        \"\"\"\n        if approval_mode:\n            log.info(\"{} activated approval mode in {}\".format(author_id, thread_id))\n        else:\n            log.info(\"{} disabled approval mode in {}\".format(author_id, thread_id))", "response": "Called when the client is listening and somebody changes approval mode in a group thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when a message is seen by somebody.", "response": "def onMessageSeen(\n        self,\n        seen_by=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        seen_ts=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody marks a message as seen\n\n        :param seen_by: The ID of the person who marked the message as seen\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param seen_ts: A timestamp of when the person saw the message\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Messages seen by {} in {} ({}) at {}s\".format(\n                seen_by, thread_id, thread_type.name, seen_ts / 1000\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef onMessageDelivered(\n        self,\n        msg_ids=None,\n        delivered_for=None,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody marks messages as delivered\n\n        :param msg_ids: The messages that are marked as delivered\n        :param delivered_for: The person that marked the messages as delivered\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Messages {} delivered to {} in {} ({}) at {}s\".format(\n                msg_ids, delivered_for, thread_id, thread_type.name, ts / 1000\n            )\n        )", "response": "Called when the client is listening and somebody marks the messages as delivered to the specified thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef onMarkedSeen(\n        self, threads=None, seen_ts=None, ts=None, metadata=None, msg=None\n    ):\n        \"\"\"\n        Called when the client is listening, and the client has successfully marked threads as seen\n\n        :param threads: The threads that were marked\n        :param author_id: The ID of the person who changed the emoji\n        :param seen_ts: A timestamp of when the threads were seen\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"Marked messages as seen in threads {} at {}s\".format(\n                [(x[0], x[1].name) for x in threads], seen_ts / 1000\n            )\n        )", "response": "Called when the client is listening and the client has successfully marked the threads as seen in the emoji."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onMessageUnsent(\n        self,\n        mid=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and someone unsends (deletes for everyone) a message\n\n        :param mid: ID of the unsent message\n        :param author_id: The ID of the person who unsent the message\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} unsent the message {} in {} ({}) at {}s\".format(\n                author_id, repr(mid), thread_id, thread_type.name, ts / 1000\n            )\n        )", "response": "Called when the client is listening and somebody unsends a message from the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling when somebody adds people to a group thread", "response": "def onPeopleAdded(\n        self,\n        mid=None,\n        added_ids=None,\n        author_id=None,\n        thread_id=None,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody adds people to a group thread\n\n        :param mid: The action ID\n        :param added_ids: The IDs of the people who got added\n        :param author_id: The ID of the person who added the people\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        \"\"\"\n        log.info(\n            \"{} added: {} in {}\".format(author_id, \", \".join(added_ids), thread_id)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef onPersonRemoved(\n        self,\n        mid=None,\n        removed_id=None,\n        author_id=None,\n        thread_id=None,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody removes a person from a group thread\n\n        :param mid: The action ID\n        :param removed_id: The ID of the person who got removed\n        :param author_id: The ID of the person who removed the person\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        \"\"\"\n        log.info(\"{} removed: {} in {}\".format(author_id, removed_id, thread_id))", "response": "Called when the client is listening and somebody removes a person from a group thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onInbox(self, unseen=None, unread=None, recent_unread=None, msg=None):\n        log.info(\"Inbox event: {}, {}, {}\".format(unseen, unread, recent_unread))", "response": "Called when the user is listening and somebody is listening and somebody is listening and somebody is listening"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when the client is listening and somebody plays a game in the thread.", "response": "def onGamePlayed(\n        self,\n        mid=None,\n        author_id=None,\n        game_id=None,\n        game_name=None,\n        score=None,\n        leaderboard=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody plays a game\n\n        :param mid: The action ID\n        :param author_id: The ID of the person who played the game\n        :param game_id: The ID of the game\n        :param game_name: Name of the game\n        :param score: Score obtained in the game\n        :param leaderboard: Actual leaderboard of the game in the thread\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            '{} played \"{}\" in {} ({})'.format(\n                author_id, game_name, thread_id, thread_type.name\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef onReactionAdded(\n        self,\n        mid=None,\n        reaction=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody reacts to a message\n\n        :param mid: Message ID, that user reacted to\n        :param reaction: Reaction\n        :param add_reaction: Whether user added or removed reaction\n        :param author_id: The ID of the person who reacted to the message\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        :type reaction: models.MessageReaction\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} reacted to message {} with {} in {} ({})\".format(\n                author_id, mid, reaction.name, thread_id, thread_type.name\n            )\n        )", "response": "Called when the client is listening and somebody reacts to a message in the reaction tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef onReactionRemoved(\n        self,\n        mid=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody removes reaction from a message\n\n        :param mid: Message ID, that user reacted to\n        :param author_id: The ID of the person who removed reaction\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} removed reaction from {} message in {} ({})\".format(\n                author_id, mid, thread_id, thread_type\n            )\n        )", "response": "Called when the client is listening and somebody removes reaction from a message in a specific thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef onBlock(\n        self, author_id=None, thread_id=None, thread_type=None, ts=None, msg=None\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody blocks client\n\n        :param author_id: The ID of the person who blocked\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} blocked {} ({}) thread\".format(author_id, thread_id, thread_type.name)\n        )", "response": "Called when somebody blocks a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling when somebody sends live location info", "response": "def onLiveLocation(\n        self,\n        mid=None,\n        location=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening and somebody sends live location info\n\n        :param mid: The action ID\n        :param location: Sent location info\n        :param author_id: The ID of the person who sent location info\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param msg: A full set of the data recieved\n        :type location: models.LiveLocationAttachment\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} sent live location info in {} ({}) with latitude {} and longitude {}\".format(\n                author_id, thread_id, thread_type, location.latitude, location.longitude\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef onCallStarted(\n        self,\n        mid=None,\n        caller_id=None,\n        is_video_call=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        .. todo::\n            Make this work with private calls\n\n        Called when the client is listening, and somebody starts a call in a group\n\n        :param mid: The action ID\n        :param caller_id: The ID of the person who started the call\n        :param is_video_call: True if it's video call\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} started call in {} ({})\".format(caller_id, thread_id, thread_type.name)\n        )", "response": "Called when the client is listening and somebody starts a call in a group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef onUserJoinedCall(\n        self,\n        mid=None,\n        joined_id=None,\n        is_video_call=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody joins a group call\n\n        :param mid: The action ID\n        :param joined_id: The ID of the person who joined the call\n        :param is_video_call: True if it's video call\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} joined call in {} ({})\".format(joined_id, thread_id, thread_type.name)\n        )", "response": "Called when the client is listening and somebody joins a call in a group"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls when somebody creates a poll in a group", "response": "def onPollCreated(\n        self,\n        mid=None,\n        poll=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody creates a group poll\n\n        :param mid: The action ID\n        :param poll: Created poll\n        :param author_id: The ID of the person who created the poll\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type poll: models.Poll\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} created poll {} in {} ({})\".format(\n                author_id, poll, thread_id, thread_type.name\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onPollVoted(\n        self,\n        mid=None,\n        poll=None,\n        added_options=None,\n        removed_options=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody votes in a group poll\n\n        :param mid: The action ID\n        :param poll: Poll, that user voted in\n        :param author_id: The ID of the person who voted in the poll\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type poll: models.Poll\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} voted in poll {} in {} ({})\".format(\n                author_id, poll, thread_id, thread_type.name\n            )\n        )", "response": "Called when the client is listening and somebody votes in a poll in a group"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef onPlanDeleted(\n        self,\n        mid=None,\n        plan=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody deletes a plan\n\n        :param mid: The action ID\n        :param plan: Deleted plan\n        :param author_id: The ID of the person who deleted the plan\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type plan: models.Plan\n        :type thread_type: models.ThreadType\n        \"\"\"\n        log.info(\n            \"{} deleted plan {} in {} ({})\".format(\n                author_id, plan, thread_id, thread_type.name\n            )\n        )", "response": "Called when the client is listening and somebody deletes a specific resource in the log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when the client is listening and somebody takes part in a plan or notater in a thread.", "response": "def onPlanParticipation(\n        self,\n        mid=None,\n        plan=None,\n        take_part=None,\n        author_id=None,\n        thread_id=None,\n        thread_type=None,\n        ts=None,\n        metadata=None,\n        msg=None,\n    ):\n        \"\"\"\n        Called when the client is listening, and somebody takes part in a plan or not\n\n        :param mid: The action ID\n        :param plan: Plan\n        :param take_part: Whether the person takes part in the plan or not\n        :param author_id: The ID of the person who will participate in the plan or not\n        :param thread_id: Thread ID that the action was sent to. See :ref:`intro_threads`\n        :param thread_type: Type of thread that the action was sent to. See :ref:`intro_threads`\n        :param ts: A timestamp of the action\n        :param metadata: Extra metadata about the action\n        :param msg: A full set of the data recieved\n        :type plan: models.Plan\n        :type take_part: bool\n        :type thread_type: models.ThreadType\n        \"\"\"\n        if take_part:\n            log.info(\n                \"{} will take part in {} in {} ({})\".format(\n                    author_id, plan, thread_id, thread_type.name\n                )\n            )\n        else:\n            log.info(\n                \"{} won't take part in {} in {} ({})\".format(\n                    author_id, plan, thread_id, thread_type.name\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert GraphQL queries to JSON", "response": "def graphql_queries_to_json(*queries):\n    \"\"\"\n    Queries should be a list of GraphQL objects\n    \"\"\"\n    rtn = {}\n    for i, query in enumerate(queries):\n        rtn[\"q{}\".format(i)] = query.value\n    return json.dumps(rtn)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate app with config", "response": "def create_app(config_filename=None):\n    \"\"\"create app.\"\"\"\n    app = FlaskAPI(__name__)\n    per_page = int(os.getenv('BUKUSERVER_PER_PAGE', str(views.DEFAULT_PER_PAGE)))\n    per_page = per_page if per_page > 0 else views.DEFAULT_PER_PAGE\n    app.config['BUKUSERVER_PER_PAGE'] = per_page\n    url_render_mode = os.getenv('BUKUSERVER_URL_RENDER_MODE', views.DEFAULT_URL_RENDER_MODE)\n    if url_render_mode not in ('full', 'netloc'):\n        url_render_mode = views.DEFAULT_URL_RENDER_MODE\n    app.config['BUKUSERVER_URL_RENDER_MODE'] = url_render_mode\n    app.config['SECRET_KEY'] = os.getenv('BUKUSERVER_SECRET_KEY') or os.urandom(24)\n    app.config['BUKUSERVER_DB_FILE'] = os.getenv('BUKUSERVER_DB_FILE')\n    bukudb = BukuDb(dbfile=app.config['BUKUSERVER_DB_FILE'])\n    app.app_context().push()\n    setattr(flask.g, 'bukudb', bukudb)\n\n    @app.shell_context_processor\n    def shell_context():\n        \"\"\"Shell context definition.\"\"\"\n        return {'app': app, 'bukudb': bukudb}\n\n    app.jinja_env.filters['netloc'] = lambda x: urlparse(x).netloc  # pylint: disable=no-member\n\n    Bootstrap(app)\n    admin = Admin(\n        app, name='Buku Server', template_mode='bootstrap3',\n        index_view=views.CustomAdminIndexView(\n            template='bukuserver/home.html', url='/'\n        )\n    )\n    # routing\n    #  api\n    app.add_url_rule('/api/tags', 'get_tags', tag_list, methods=['GET'])\n    app.add_url_rule('/api/tags/<tag>', 'update_tag', tag_detail, methods=['GET', 'PUT'])\n    app.add_url_rule(\n        '/api/network_handle',\n        'networkk_handle',\n        network_handle_detail,\n        methods=['POST'])\n    app.add_url_rule('/api/bookmarks', 'bookmarks', bookmarks, methods=['GET', 'POST', 'DELETE'])\n    app.add_url_rule(\n        '/api/bookmarks/refresh',\n        'refresh_bookmarks',\n        refresh_bookmarks,\n        methods=['POST'])\n    app.add_url_rule(\n        '/api/bookmarks/<id>',\n        'bookmark_api',\n        bookmark_api,\n        methods=['GET', 'PUT', 'DELETE'])\n    app.add_url_rule(\n        '/api/bookmarks/<id>/refresh',\n        'refresh_bookmark',\n        refresh_bookmark,\n        methods=['POST'])\n    app.add_url_rule('/api/bookmarks/<id>/tiny', 'get_tiny_url', get_tiny_url, methods=['GET'])\n    app.add_url_rule('/api/bookmarks/<id>/long', 'get_long_url', get_long_url, methods=['GET'])\n    app.add_url_rule(\n        '/api/bookmarks/<starting_id>/<ending_id>',\n        'bookmark_range_operations', bookmark_range_operations, methods=['GET', 'PUT', 'DELETE'])\n    app.add_url_rule(\n        '/api/bookmarks/search',\n        'search_bookmarks',\n        search_bookmarks,\n        methods=['GET', 'DELETE'])\n    #  non api\n    admin.add_view(views.BookmarkModelView(\n        bukudb, 'Bookmarks', page_size=per_page, url_render_mode=url_render_mode))\n    admin.add_view(views.TagModelView(\n        bukudb, 'Tags', page_size=per_page))\n    admin.add_view(views.StatisticView(\n        bukudb, 'Statistic', endpoint='statistic'))\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self):\n        \"redirect to bookmark search\"\n        form = forms.HomeForm()\n        bbm_filter = bs_filters.BookmarkBukuFilter(\n            all_keywords=False, deep=form.deep.data, regex=form.regex.data)\n        op_text = bbm_filter.operation()\n        values_combi = sorted(itertools.product([True, False], repeat=3))\n        for idx, (all_keywords, deep, regex) in enumerate(values_combi):\n            if deep == form.deep.data and regex == form.regex.data and not all_keywords:\n                choosen_idx = idx\n        url_op_text = op_text.replace(', ', '_').replace('  ', ' ').replace(' ', '_')\n        key = ''.join(['flt', str(choosen_idx), '_buku_', url_op_text])\n        kwargs = {key: form.keyword.data}\n        url = url_for('bookmark.index_view', **kwargs)\n        return redirect(url)", "response": "redirect to bookmark search"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering a given link or email address.", "response": "def autolink(self, link, is_email=False):\n        \"\"\"Rendering a given link or email address.\n\n        :param link: link content or email address.\n        :param is_email: whether this is an email or not.\n        \"\"\"\n        text = link = escape_link(link)\n        if is_email:\n            link = 'mailto:%s' % link\n        return '<a href=\"%s\">%s</a>' % (link, text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the SDK and optionally integrations. This takes the same arguments as the client constructor.", "response": "def init(*args, **kwargs):\n    \"\"\"Initializes the SDK and optionally integrations.\n\n    This takes the same arguments as the client constructor.\n    \"\"\"\n    global _initial_client\n    client = Client(*args, **kwargs)\n    Hub.current.bind_client(client)\n    rv = _InitGuard(client)\n    if client is not None:\n        _initial_client = weakref.ref(client)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current(self):\n        # type: () -> Hub\n        \"\"\"Returns the current instance of the hub.\"\"\"\n        rv = _local.get(None)\n        if rv is None:\n            rv = Hub(GLOBAL_HUB)\n            _local.set(rv)\n        return rv", "response": "Returns the current instance of the hub."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_integration(self, name_or_class):\n        # type: (Union[str, Integration]) -> Any\n        \"\"\"Returns the integration for this hub by name or class.  If there\n        is no client bound or the client does not have that integration\n        then `None` is returned.\n\n        If the return value is not `None` the hub is guaranteed to have a\n        client attached.\n        \"\"\"\n        if isinstance(name_or_class, str):\n            integration_name = name_or_class\n        elif name_or_class.identifier is not None:\n            integration_name = name_or_class.identifier\n        else:\n            raise ValueError(\"Integration has no name\")\n\n        client = self._stack[-1][0]\n        if client is not None:\n            rv = client.integrations.get(integration_name)\n            if rv is not None:\n                return rv\n\n        initial_client = _initial_client\n        if initial_client is not None:\n            initial_client = initial_client()\n\n        if (\n            initial_client is not None\n            and initial_client is not client\n            and initial_client.integrations.get(name_or_class) is not None\n        ):\n            warning = (\n                \"Integration %r attempted to run but it was only \"\n                \"enabled on init() but not the client that \"\n                \"was bound to the current flow.  Earlier versions of \"\n                \"the SDK would consider these integrations enabled but \"\n                \"this is no longer the case.\" % (name_or_class,)\n            )\n            warn(Warning(warning), stacklevel=3)\n            logger.warning(warning)", "response": "Returns the integration for this hub by name or class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind_client(self, new):\n        top = self._stack[-1]\n        self._stack[-1] = (new, top[1])", "response": "Binds a new client to the hub."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef capture_event(self, event, hint=None):\n        # type: (Dict[str, Any], Dict[str, Any]) -> Optional[str]\n        \"\"\"Captures an event.  The return value is the ID of the event.\n\n        The event is a dictionary following the Sentry v7/v8 protocol\n        specification.  Optionally an event hint dict can be passed that\n        is used by processors to extract additional information from it.\n        Typically the event hint object would contain exception information.\n        \"\"\"\n        client, scope = self._stack[-1]\n        if client is not None:\n            rv = client.capture_event(event, hint, scope)\n            if rv is not None:\n                self._last_event_id = rv\n            return rv\n        return None", "response": "Captures an event.  The return value is the ID of the event.\n\n        The event is a dictionary following the Sentry v7/v8 protocol\n        specification.  Optionally an event hint dict can be passed that\n        is used by processors to extract additional information from it.\n        Typically the event hint object would contain exception information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef capture_message(self, message, level=None):\n        # type: (str, Optional[Any]) -> Optional[str]\n        \"\"\"Captures a message.  The message is just a string.  If no level\n        is provided the default level is `info`.\n        \"\"\"\n        if self.client is None:\n            return None\n        if level is None:\n            level = \"info\"\n        return self.capture_event({\"message\": message, \"level\": level})", "response": "Captures a message.  The message is just a string.  If no level\n        is provided the default level is `info`."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncapturing an exception. The argument passed can be `None` in which case the last exception will be reported, otherwise an exception object or an `exc_info` tuple.", "response": "def capture_exception(self, error=None):\n        # type: (Optional[BaseException]) -> Optional[str]\n        \"\"\"Captures an exception.\n\n        The argument passed can be `None` in which case the last exception\n        will be reported, otherwise an exception object or an `exc_info`\n        tuple.\n        \"\"\"\n        client = self.client\n        if client is None:\n            return None\n        if error is None:\n            exc_info = sys.exc_info()\n        else:\n            exc_info = exc_info_from_error(error)\n\n        event, hint = event_from_exception(exc_info, client_options=client.options)\n        try:\n            return self.capture_event(event, hint=hint)\n        except Exception:\n            self._capture_internal_exception(sys.exc_info())\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_breadcrumb(self, crumb=None, hint=None, **kwargs):\n        # type: (Dict[str, Any], Dict[str, Any], **Any) -> None\n        \"\"\"Adds a breadcrumb.  The breadcrumbs are a dictionary with the\n        data as the sentry v7/v8 protocol expects.  `hint` is an optional\n        value that can be used by `before_breadcrumb` to customize the\n        breadcrumbs that are emitted.\n        \"\"\"\n        client, scope = self._stack[-1]\n        if client is None:\n            logger.info(\"Dropped breadcrumb because no client bound\")\n            return\n\n        crumb = dict(crumb or ())  # type: Dict[str, Any]\n        crumb.update(kwargs)\n        if not crumb:\n            return\n\n        hint = dict(hint or ())\n\n        if crumb.get(\"timestamp\") is None:\n            crumb[\"timestamp\"] = datetime.utcnow()\n        if crumb.get(\"type\") is None:\n            crumb[\"type\"] = \"default\"\n\n        original_crumb = crumb\n        if client.options[\"before_breadcrumb\"] is not None:\n            crumb = client.options[\"before_breadcrumb\"](crumb, hint)\n\n        if crumb is not None:\n            scope._breadcrumbs.append(crumb)\n        else:\n            logger.info(\"before breadcrumb dropped breadcrumb (%s)\", original_crumb)\n\n        max_breadcrumbs = client.options[\"max_breadcrumbs\"]  # type: int\n        while len(scope._breadcrumbs) > max_breadcrumbs:\n            scope._breadcrumbs.popleft()", "response": "Adds a breadcrumb to the current context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npush a new layer on the scope stack. Returns a new context manager that can be used to pop the scope again.", "response": "def push_scope(self, callback=None):  # noqa\n        \"\"\"Pushes a new layer on the scope stack. Returns a context manager\n        that should be used to pop the scope again.  Alternatively a callback\n        can be provided that is executed in the context of the scope.\n        \"\"\"\n\n        if callback is not None:\n            with self.push_scope() as scope:\n                callback(scope)\n            return None\n\n        client, scope = self._stack[-1]\n        new_layer = (client, copy.copy(scope))\n        self._stack.append(new_layer)\n\n        return _ScopeManager(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaptures an event. This takes the ready made event and an optoinal hint and scope. The hint is internally used to further customize the representation of the error. When provided it's a dictionary of optional information such as exception info. If the transport is not set nothing happens, otherwise the return value of this function will be the ID of the captured event.", "response": "def capture_event(self, event, hint=None, scope=None):\n        # type: (Dict[str, Any], Any, Scope) -> Optional[str]\n        \"\"\"Captures an event.\n\n        This takes the ready made event and an optoinal hint and scope.  The\n        hint is internally used to further customize the representation of the\n        error.  When provided it's a dictionary of optional information such\n        as exception info.\n\n        If the transport is not set nothing happens, otherwise the return\n        value of this function will be the ID of the captured event.\n        \"\"\"\n        if self.transport is None:\n            return None\n        if hint is None:\n            hint = {}\n        rv = event.get(\"event_id\")\n        if rv is None:\n            event[\"event_id\"] = rv = uuid.uuid4().hex\n        if not self._should_capture(event, hint, scope):\n            return None\n        event = self._prepare_event(event, hint, scope)  # type: ignore\n        if event is None:\n            return None\n        self.transport.capture_event(event)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose the client and shut down the transport.", "response": "def close(self, timeout=None, callback=None):\n        \"\"\"\n        Close the client and shut down the transport. Arguments have the same\n        semantics as `self.flush()`.\n        \"\"\"\n        if self.transport is not None:\n            self.flush(timeout=timeout, callback=callback)\n            self.transport.kill()\n            self.transport = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflush the pending events to the local cache.", "response": "def flush(self, timeout=None, callback=None):\n        \"\"\"\n        Wait `timeout` seconds for the current events to be sent. If no\n        `timeout` is provided, the `shutdown_timeout` option value is used.\n\n        The `callback` is invoked with two arguments: the number of pending\n        events and the configured timeout.  For instance the default atexit\n        integration will use this to render out a message on stderr.\n        \"\"\"\n        if self.transport is not None:\n            if timeout is None:\n                timeout = self.options[\"shutdown_timeout\"]\n            self.transport.flush(timeout=timeout, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a list of integrations this installs them all.", "response": "def setup_integrations(integrations, with_defaults=True):\n    # type: (List[Integration], bool) -> Dict[str, Integration]\n    \"\"\"Given a list of integration instances this installs them all.  When\n    `with_defaults` is set to `True` then all default integrations are added\n    unless they were already provided before.\n    \"\"\"\n    integrations = dict(\n        (integration.identifier, integration) for integration in integrations or ()\n    )\n\n    logger.debug(\"Setting up integrations (with default = %s)\", with_defaults)\n\n    if with_defaults:\n        for integration_cls in iter_default_integrations():\n            if integration_cls.identifier not in integrations:\n                instance = integration_cls()\n                integrations[instance.identifier] = instance\n\n    for identifier, integration in iteritems(integrations):\n        with _installer_lock:\n            if identifier not in _installed_integrations:\n                logger.debug(\n                    \"Setting up previously not enabled integration %s\", identifier\n                )\n                try:\n                    type(integration).setup_once()\n                except NotImplementedError:\n                    if getattr(integration, \"install\", None) is not None:\n                        logger.warn(\n                            \"Integration %s: The install method is \"\n                            \"deprecated. Use `setup_once`.\",\n                            identifier,\n                        )\n                        integration.install()\n                    else:\n                        raise\n                _installed_integrations.add(identifier)\n\n    for identifier in integrations:\n        logger.debug(\"Enabling integration %s\", identifier)\n\n    return integrations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self):\n        # type: () -> None\n        \"\"\"Clears the entire scope.\"\"\"\n        self._level = None\n        self._fingerprint = None\n        self._transaction = None\n        self._user = None\n\n        self._tags = {}  # type: Dict[str, Any]\n        self._contexts = {}  # type: Dict[str, Dict]\n        self._extras = {}  # type: Dict[str, Any]\n\n        self.clear_breadcrumbs()\n        self._should_capture = True\n\n        self._span = None", "response": "Clears the entire scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a scope local error processor on the scope.", "response": "def add_error_processor(self, func, cls=None):\n        # type: (Callable, Optional[type]) -> None\n        \"\"\"\"Register a scope local error processor on the scope.\n\n        The error processor works similar to an event processor but is\n        invoked with the original exception info triple as second argument.\n        \"\"\"\n        if cls is not None:\n            real_func = func\n\n            def func(event, exc_info):\n                try:\n                    is_inst = isinstance(exc_info[1], cls)\n                except Exception:\n                    is_inst = False\n                if is_inst:\n                    return real_func(event, exc_info)\n                return event\n\n        self._error_processors.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the information contained on the scope to the given event.", "response": "def apply_to_event(self, event, hint=None):\n        # type: (Dict[str, Any], Dict[str, Any]) -> Optional[Dict[str, Any]]\n        \"\"\"Applies the information contained on the scope to the given event.\"\"\"\n\n        def _drop(event, cause, ty):\n            # type: (Dict[str, Any], Callable, str) -> Optional[Any]\n            logger.info(\"%s (%s) dropped event (%s)\", ty, cause, event)\n            return None\n\n        if self._level is not None:\n            event[\"level\"] = self._level\n\n        event.setdefault(\"breadcrumbs\", []).extend(self._breadcrumbs)\n        if event.get(\"user\") is None and self._user is not None:\n            event[\"user\"] = self._user\n\n        if event.get(\"transaction\") is None and self._transaction is not None:\n            event[\"transaction\"] = self._transaction\n\n        if event.get(\"fingerprint\") is None and self._fingerprint is not None:\n            event[\"fingerprint\"] = self._fingerprint\n\n        if self._extras:\n            event.setdefault(\"extra\", {}).update(object_to_json(self._extras))\n\n        if self._tags:\n            event.setdefault(\"tags\", {}).update(self._tags)\n\n        if self._contexts:\n            event.setdefault(\"contexts\", {}).update(self._contexts)\n\n        if self._span is not None:\n            event.setdefault(\"contexts\", {})[\"trace\"] = {\n                \"trace_id\": self._span.trace_id,\n                \"span_id\": self._span.span_id,\n            }\n\n        exc_info = hint.get(\"exc_info\") if hint is not None else None\n        if exc_info is not None:\n            for processor in self._error_processors:\n                new_event = processor(event, exc_info)\n                if new_event is None:\n                    return _drop(event, processor, \"error processor\")\n                event = new_event\n\n        for processor in chain(global_event_processors, self._event_processors):\n            new_event = event\n            with capture_internal_exceptions():\n                new_event = processor(event, hint)\n            if new_event is None:\n                return _drop(event, processor, \"event processor\")\n            event = new_event\n\n        return event"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef default_callback(pending, timeout):\n\n    def echo(msg):\n        sys.stderr.write(msg + \"\\n\")\n\n    echo(\"Sentry is attempting to send %i pending error messages\" % pending)\n    echo(\"Waiting up to %s seconds\" % timeout)\n    echo(\"Press Ctrl-%s to quit\" % (os.name == \"nt\" and \"Break\" or \"C\"))\n    sys.stderr.flush()", "response": "This is the default shutdown callback that is set on the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install_sql_hook():\n    # type: () -> None\n    \"\"\"If installed this causes Django's queries to be captured.\"\"\"\n    try:\n        from django.db.backends.utils import CursorWrapper  # type: ignore\n    except ImportError:\n        from django.db.backends.util import CursorWrapper  # type: ignore\n\n    try:\n        real_execute = CursorWrapper.execute\n        real_executemany = CursorWrapper.executemany\n    except AttributeError:\n        # This won't work on Django versions < 1.6\n        return\n\n    def record_many_sql(sql, param_list, cursor):\n        for params in param_list:\n            record_sql(sql, params, cursor)\n\n    def execute(self, sql, params=None):\n        try:\n            return real_execute(self, sql, params)\n        finally:\n            record_sql(sql, params, self.cursor)\n\n    def executemany(self, sql, param_list):\n        try:\n            return real_executemany(self, sql, param_list)\n        finally:\n            record_many_sql(sql, param_list, self.cursor)\n\n    CursorWrapper.execute = execute\n    CursorWrapper.executemany = executemany\n    ignore_logger(\"django.db.backends\")", "response": "Installs the SQL hook that will be executed when the database is executed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_host(environ):\n    # type: (Dict[str, str]) -> str\n    \"\"\"Return the host for the given WSGI environment. Yanked from Werkzeug.\"\"\"\n    if environ.get(\"HTTP_HOST\"):\n        rv = environ[\"HTTP_HOST\"]\n        if environ[\"wsgi.url_scheme\"] == \"http\" and rv.endswith(\":80\"):\n            rv = rv[:-3]\n        elif environ[\"wsgi.url_scheme\"] == \"https\" and rv.endswith(\":443\"):\n            rv = rv[:-4]\n    elif environ.get(\"SERVER_NAME\"):\n        rv = environ[\"SERVER_NAME\"]\n        if (environ[\"wsgi.url_scheme\"], environ[\"SERVER_PORT\"]) not in (\n            (\"https\", \"443\"),\n            (\"http\", \"80\"),\n        ):\n            rv += \":\" + environ[\"SERVER_PORT\"]\n    else:\n        # In spite of the WSGI spec, SERVER_NAME might not be present.\n        rv = \"unknown\"\n\n    return rv", "response": "Return the host for the given WSGI environment. Yanked from Werkzeug."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_request_url(environ):\n    # type: (Dict[str, str]) -> str\n    \"\"\"Return the absolute URL without query string for the given WSGI\n    environment.\"\"\"\n    return \"%s://%s/%s\" % (\n        environ.get(\"wsgi.url_scheme\"),\n        get_host(environ),\n        wsgi_decoding_dance(environ.get(\"PATH_INFO\") or \"\").lstrip(\"/\"),\n    )", "response": "Return the absolute URL without query string for the given WSGI\n    environment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_environ(environ):\n    # type: (Dict[str, str]) -> Iterator[Tuple[str, str]]\n    \"\"\"\n    Returns our whitelisted environment variables.\n    \"\"\"\n    keys = [\"SERVER_NAME\", \"SERVER_PORT\"]\n    if _should_send_default_pii():\n        # Add all three headers here to make debugging of proxy setup easier.\n        keys += [\"REMOTE_ADDR\", \"HTTP_X_FORWARDED_FOR\", \"HTTP_X_REAL_IP\"]\n\n    for key in keys:\n        if key in environ:\n            yield key, environ[key]", "response": "Returns our whitelisted environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn only proper HTTP headers.", "response": "def _get_headers(environ):\n    # type: (Dict[str, str]) -> Iterator[Tuple[str, str]]\n    \"\"\"\n    Returns only proper HTTP headers.\n\n    \"\"\"\n    for key, value in environ.items():\n        key = str(key)\n        if key.startswith(\"HTTP_\") and key not in (\n            \"HTTP_CONTENT_TYPE\",\n            \"HTTP_CONTENT_LENGTH\",\n        ):\n            yield key[5:].replace(\"_\", \"-\").title(), value\n        elif key in (\"CONTENT_TYPE\", \"CONTENT_LENGTH\"):\n            yield key.replace(\"_\", \"-\").title(), value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_client_ip(environ):\n    # type: (Dict[str, str]) -> Optional[Any]\n    \"\"\"\n    Infer the user IP address from various headers. This cannot be used in\n    security sensitive situations since the value may be forged from a client,\n    but it's good enough for the event payload.\n    \"\"\"\n    try:\n        return environ[\"HTTP_X_FORWARDED_FOR\"].split(\",\")[0].strip()\n    except (KeyError, IndexError):\n        pass\n\n    try:\n        return environ[\"HTTP_X_REAL_IP\"]\n    except KeyError:\n        pass\n\n    return environ.get(\"REMOTE_ADDR\")", "response": "Infer the client IP address from various headers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a hint with the exc info filled in.", "response": "def event_hint_with_exc_info(exc_info=None):\n    # type: (ExcInfo) -> Dict[str, Optional[ExcInfo]]\n    \"\"\"Creates a hint with the exc info filled in.\"\"\"\n    if exc_info is None:\n        exc_info = sys.exc_info()\n    else:\n        exc_info = exc_info_from_error(exc_info)\n    if exc_info[0] is None:\n        exc_info = None\n    return {\"exc_info\": exc_info}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat a string containing %s for placeholders and call strip_string on each parameter.", "response": "def format_and_strip(template, params, strip_string=strip_string):\n    \"\"\"Format a string containing %s for placeholders and call `strip_string`\n    on each parameter. The string template itself does not have a maximum\n    length.\n\n    TODO: handle other placeholders, not just %s\n    \"\"\"\n    chunks = template.split(u\"%s\")\n    if not chunks:\n        raise ValueError(\"No formatting placeholders found\")\n\n    params = list(reversed(params))\n    rv_remarks = []\n    rv_original_length = 0\n    rv_length = 0\n    rv = []\n\n    def realign_remark(remark):\n        return [\n            (rv_length + x if isinstance(x, int_types) and i < 4 else x)\n            for i, x in enumerate(remark)\n        ]\n\n    for chunk in chunks[:-1]:\n        rv.append(chunk)\n        rv_length += len(chunk)\n        rv_original_length += len(chunk)\n        if not params:\n            raise ValueError(\"Not enough params.\")\n        param = params.pop()\n\n        stripped_param = strip_string(param)\n        if isinstance(stripped_param, AnnotatedValue):\n            rv_remarks.extend(\n                realign_remark(remark) for remark in stripped_param.metadata[\"rem\"]\n            )\n            stripped_param = stripped_param.value\n\n        rv_original_length += len(param)\n        rv_length += len(stripped_param)\n        rv.append(stripped_param)\n\n    rv.append(chunks[-1])\n    rv_length += len(chunks[-1])\n    rv_original_length += len(chunks[-1])\n\n    rv = u\"\".join(rv)\n    assert len(rv) == rv_length\n\n    if not rv_remarks:\n        return rv\n\n    return AnnotatedValue(\n        value=rv, metadata={\"len\": rv_original_length, \"rem\": rv_remarks}\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store_api_url(self):\n        return \"%s://%s%sapi/%s/store/\" % (\n            self.scheme,\n            self.host,\n            self.path,\n            self.project_id,\n        )", "response": "Returns the API url for storing events."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_header(self, timestamp=None):\n        rv = [(\"sentry_key\", self.public_key), (\"sentry_version\", self.version)]\n        if timestamp is not None:\n            rv.append((\"sentry_timestamp\", str(to_timestamp(timestamp))))\n        if self.client is not None:\n            rv.append((\"sentry_client\", self.client))\n        if self.secret_key is not None:\n            rv.append((\"sentry_secret\", self.secret_key))\n        return u\"Sentry \" + u\", \".join(\"%s=%s\" % (key, value) for key, value in rv)", "response": "Returns the auth header a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _simplify(self, pattern):\n        # type: (str) -> str\n        r\"\"\"\n        Clean up urlpattern regexes into something readable by humans:\n\n        From:\n        > \"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n\n        To:\n        > \"{sport_slug}/athletes/{athlete_slug}/\"\n        \"\"\"\n        # remove optional params\n        # TODO(dcramer): it'd be nice to change these into [%s] but it currently\n        # conflicts with the other rules because we're doing regexp matches\n        # rather than parsing tokens\n        result = self._optional_group_matcher.sub(lambda m: \"%s\" % m.group(1), pattern)\n\n        # handle named groups first\n        result = self._named_group_matcher.sub(lambda m: \"{%s}\" % m.group(1), result)\n\n        # handle non-named groups\n        result = self._non_named_group_matcher.sub(\"{var}\", result)\n\n        # handle optional params\n        result = self._either_option_matcher.sub(lambda m: m.group(1), result)\n\n        # clean up any outstanding regex-y characters.\n        result = (\n            result.replace(\"^\", \"\")\n            .replace(\"$\", \"\")\n            .replace(\"?\", \"\")\n            .replace(\"//\", \"/\")\n            .replace(\"\\\\\", \"\")\n        )\n\n        return result", "response": "Simplify a urlpattern into something readable by humans."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _register_bounds_validator_if_needed(parser, name, flag_values):\n  if parser.lower_bound is not None or parser.upper_bound is not None:\n\n    def checker(value):\n      if value is not None and parser.is_outside_bounds(value):\n        message = '%s is not %s' % (value, parser.syntactic_help)\n        raise _exceptions.ValidationError(message)\n      return True\n\n    _validators.register_validator(name, checker, flag_values=flag_values)", "response": "Register a validator that checks that the value is within the bounds of the flag_values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DEFINE(parser, name, default, help, flag_values=_flagvalues.FLAGS,  # pylint: disable=redefined-builtin,invalid-name\n           serializer=None, module_name=None, **args):\n  \"\"\"Registers a generic Flag object.\n\n  NOTE: in the docstrings of all DEFINE* functions, \"registers\" is short\n  for \"creates a new flag and registers it\".\n\n  Auxiliary function: clients should use the specialized DEFINE_<type>\n  function instead.\n\n  Args:\n    parser: ArgumentParser, used to parse the flag arguments.\n    name: str, the flag name.\n    default: The default value of the flag.\n    help: str, the help message.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    serializer: ArgumentSerializer, the flag serializer instance.\n    module_name: str, the name of the Python module declaring this flag.\n        If not provided, it will be computed using the stack trace of this call.\n    **args: dict, the extra keyword args that are passed to Flag __init__.\n  \"\"\"\n  DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n              flag_values, module_name)", "response": "Registers a generic Flag object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a Flag object with a FlagValues object.", "response": "def DEFINE_flag(flag, flag_values=_flagvalues.FLAGS, module_name=None):  # pylint: disable=invalid-name\n  \"\"\"Registers a 'Flag' object with a 'FlagValues' object.\n\n  By default, the global FLAGS 'FlagValue' object is used.\n\n  Typical users will use one of the more specialized DEFINE_xxx\n  functions, such as DEFINE_string or DEFINE_integer.  But developers\n  who need to create Flag objects themselves should use this function\n  to register their flags.\n\n  Args:\n    flag: Flag, a flag that is key to the module.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    module_name: str, the name of the Python module declaring this flag.\n        If not provided, it will be computed using the stack trace of this call.\n  \"\"\"\n  # Copying the reference to flag_values prevents pychecker warnings.\n  fv = flag_values\n  fv[flag.name] = flag\n  # Tell flag_values who's defining the flag.\n  if module_name:\n    module = sys.modules.get(module_name)\n  else:\n    module, module_name = _helpers.get_calling_module_object_and_name()\n  flag_values.register_flag_by_module(module_name, flag)\n  flag_values.register_flag_by_module_id(id(module), flag)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _internal_declare_key_flags(\n    flag_names, flag_values=_flagvalues.FLAGS, key_flag_values=None):\n  \"\"\"Declares a flag as key for the calling module.\n\n  Internal function.  User code should call declare_key_flag or\n  adopt_module_key_flags instead.\n\n  Args:\n    flag_names: [str], a list of strings that are names of already-registered\n        Flag objects.\n    flag_values: FlagValues, the FlagValues instance with which the flags listed\n        in flag_names have registered (the value of the flag_values\n        argument from the DEFINE_* calls that defined those flags).\n        This should almost never need to be overridden.\n    key_flag_values: FlagValues, the FlagValues instance that (among possibly\n        many other things) keeps track of the key flags for each module.\n        Default None means \"same as flag_values\".  This should almost\n        never need to be overridden.\n\n  Raises:\n    UnrecognizedFlagError: Raised when the flag is not defined.\n  \"\"\"\n  key_flag_values = key_flag_values or flag_values\n\n  module = _helpers.get_calling_module()\n\n  for flag_name in flag_names:\n    flag = flag_values[flag_name]\n    key_flag_values.register_key_flag_for_module(module, flag)", "response": "Internal function that declares a flag as key for the calling module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef declare_key_flag(flag_name, flag_values=_flagvalues.FLAGS):\n  if flag_name in _helpers.SPECIAL_FLAGS:\n    # Take care of the special flags, e.g., --flagfile, --undefok.\n    # These flags are defined in SPECIAL_FLAGS, and are treated\n    # specially during flag parsing, taking precedence over the\n    # user-defined flags.\n    _internal_declare_key_flags([flag_name],\n                                flag_values=_helpers.SPECIAL_FLAGS,\n                                key_flag_values=flag_values)\n    return\n  try:\n    _internal_declare_key_flags([flag_name], flag_values=flag_values)\n  except KeyError:\n    raise ValueError('Flag --%s is undefined. To set a flag as a key flag '\n                     'first define it in Python.' % flag_name)", "response": "Declare a flag as a key to the current module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adopt_module_key_flags(module, flag_values=_flagvalues.FLAGS):\n  if not isinstance(module, types.ModuleType):\n    raise _exceptions.Error('Expected a module object, not %r.' % (module,))\n  _internal_declare_key_flags(\n      [f.name for f in flag_values.get_key_flags_for_module(module.__name__)],\n      flag_values=flag_values)\n  # If module is this flag module, take _helpers.SPECIAL_FLAGS into account.\n  if module == _helpers.FLAGS_MODULE:\n    _internal_declare_key_flags(\n        # As we associate flags with get_calling_module_object_and_name(), the\n        # special flags defined in this module are incorrectly registered with\n        # a different module.  So, we can't use get_key_flags_for_module.\n        # Instead, we take all flags from _helpers.SPECIAL_FLAGS (a private\n        # FlagValues, where no other module should register flags).\n        [_helpers.SPECIAL_FLAGS[name].name for name in _helpers.SPECIAL_FLAGS],\n        flag_values=_helpers.SPECIAL_FLAGS,\n        key_flag_values=flag_values)", "response": "Declares that all flags for a module are declared as key flags to the current module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeclaring that the current module will not define any more key flags.", "response": "def disclaim_key_flags():\n  \"\"\"Declares that the current module will not define any more key flags.\n\n  Normally, the module that calls the DEFINE_xxx functions claims the\n  flag to be its key flag.  This is undesirable for modules that\n  define additional DEFINE_yyy functions with its own flag parsers and\n  serializers, since that module will accidentally claim flags defined\n  by DEFINE_yyy as its key flags.  After calling this function, the\n  module disclaims flag definitions thereafter, so the key flags will\n  be correctly attributed to the caller of DEFINE_yyy.\n\n  After calling this function, the module will not be able to define\n  any more flags.  This function will affect all FlagValues objects.\n  \"\"\"\n  globals_for_caller = sys._getframe(1).f_globals  # pylint: disable=protected-access\n  module, _ = _helpers.get_module_object_and_name(globals_for_caller)\n  _helpers.disclaim_module_ids.add(id(module))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a flag whose value can be any string.", "response": "def DEFINE_string(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, flag_values=_flagvalues.FLAGS, **args):\n  \"\"\"Registers a flag whose value can be any string.\"\"\"\n  parser = _argument_parser.ArgumentParser()\n  serializer = _argument_parser.ArgumentSerializer()\n  DEFINE(parser, name, default, help, flag_values, serializer, **args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a boolean flag.", "response": "def DEFINE_boolean(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, flag_values=_flagvalues.FLAGS, module_name=None,\n    **args):\n  \"\"\"Registers a boolean flag.\n\n  Such a boolean flag does not take an argument.  If a user wants to\n  specify a false value explicitly, the long option beginning with 'no'\n  must be used: i.e. --noflag\n\n  This flag will have a value of None, True or False.  None is possible\n  if default=None and the user does not specify the flag on the command\n  line.\n\n  Args:\n    name: str, the flag name.\n    default: bool|str|None, the default value of the flag.\n    help: str, the help message.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    module_name: str, the name of the Python module declaring this flag.\n        If not provided, it will be computed using the stack trace of this call.\n    **args: dict, the extra keyword args that are passed to Flag __init__.\n  \"\"\"\n  DEFINE_flag(_flag.BooleanFlag(name, default, help, **args),\n              flag_values, module_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DEFINE_float(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, lower_bound=None, upper_bound=None,\n    flag_values=_flagvalues.FLAGS, **args):   # pylint: disable=invalid-name\n  \"\"\"Registers a flag whose value must be a float.\n\n  If lower_bound or upper_bound are set, then this flag must be\n  within the given range.\n\n  Args:\n    name: str, the flag name.\n    default: float|str|None, the default value of the flag.\n    help: str, the help message.\n    lower_bound: float, min value of the flag.\n    upper_bound: float, max value of the flag.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    **args: dict, the extra keyword args that are passed to DEFINE.\n  \"\"\"\n  parser = _argument_parser.FloatParser(lower_bound, upper_bound)\n  serializer = _argument_parser.ArgumentSerializer()\n  DEFINE(parser, name, default, help, flag_values, serializer, **args)\n  _register_bounds_validator_if_needed(parser, name, flag_values=flag_values)", "response": "Registers a flag whose value must be a float."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a flag whose value must be an integer.", "response": "def DEFINE_integer(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, lower_bound=None, upper_bound=None,\n    flag_values=_flagvalues.FLAGS, **args):\n  \"\"\"Registers a flag whose value must be an integer.\n\n  If lower_bound, or upper_bound are set, then this flag must be\n  within the given range.\n\n  Args:\n    name: str, the flag name.\n    default: int|str|None, the default value of the flag.\n    help: str, the help message.\n    lower_bound: int, min value of the flag.\n    upper_bound: int, max value of the flag.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    **args: dict, the extra keyword args that are passed to DEFINE.\n  \"\"\"\n  parser = _argument_parser.IntegerParser(lower_bound, upper_bound)\n  serializer = _argument_parser.ArgumentSerializer()\n  DEFINE(parser, name, default, help, flag_values, serializer, **args)\n  _register_bounds_validator_if_needed(parser, name, flag_values=flag_values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DEFINE_enum_class(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, enum_class, help, flag_values=_flagvalues.FLAGS,\n    module_name=None, **args):\n  \"\"\"Registers a flag whose value can be the name of enum members.\n\n  Args:\n    name: str, the flag name.\n    default: Enum|str|None, the default value of the flag.\n    enum_class: class, the Enum class with all the possible values for the flag.\n    help: str, the help message.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    module_name: str, the name of the Python module declaring this flag.\n        If not provided, it will be computed using the stack trace of this call.\n    **args: dict, the extra keyword args that are passed to Flag __init__.\n  \"\"\"\n  DEFINE_flag(_flag.EnumClassFlag(name, default, help, enum_class, **args),\n              flag_values, module_name)", "response": "Registers a flag whose value can be the name of enum members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a flag whose value is a whitespace - separated list of strings.", "response": "def DEFINE_spaceseplist(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, comma_compat=False, flag_values=_flagvalues.FLAGS,\n    **args):\n  \"\"\"Registers a flag whose value is a whitespace-separated list of strings.\n\n  Any whitespace can be used as a separator.\n\n  Args:\n    name: str, the flag name.\n    default: list|str|None, the default value of the flag.\n    help: str, the help message.\n    comma_compat: bool - Whether to support comma as an additional separator.\n        If false then only whitespace is supported.  This is intended only for\n        backwards compatibility with flags that used to be comma-separated.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = _argument_parser.WhitespaceSeparatedListParser(\n      comma_compat=comma_compat)\n  serializer = _argument_parser.ListSerializer(' ')\n  DEFINE(parser, name, default, help, flag_values, serializer, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a flag whose value can be a list of any string.", "response": "def DEFINE_multi_string(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, flag_values=_flagvalues.FLAGS, **args):\n  \"\"\"Registers a flag whose value can be a list of any strings.\n\n  Use the flag on the command line multiple times to place multiple\n  string values into the list.  The 'default' may be a single string\n  (which will be converted into a single-element list) or a list of\n  strings.\n\n\n  Args:\n    name: str, the flag name.\n    default: Union[Iterable[Text], Text, None], the default value of the flag;\n        see `DEFINE_multi`.\n    help: str, the help message.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = _argument_parser.ArgumentParser()\n  serializer = _argument_parser.ArgumentSerializer()\n  DEFINE_multi(parser, serializer, name, default, help, flag_values, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DEFINE_multi_integer(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, help, lower_bound=None, upper_bound=None,\n    flag_values=_flagvalues.FLAGS, **args):\n  \"\"\"Registers a flag whose value can be a list of arbitrary integers.\n\n  Use the flag on the command line multiple times to place multiple\n  integer values into the list.  The 'default' may be a single integer\n  (which will be converted into a single-element list) or a list of\n  integers.\n\n  Args:\n    name: str, the flag name.\n    default: Union[Iterable[int], Text, None], the default value of the flag;\n        see `DEFINE_multi`.\n    help: str, the help message.\n    lower_bound: int, min values of the flag.\n    upper_bound: int, max values of the flag.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = _argument_parser.IntegerParser(lower_bound, upper_bound)\n  serializer = _argument_parser.ArgumentSerializer()\n  DEFINE_multi(parser, serializer, name, default, help, flag_values, **args)", "response": "Registers a flag whose value can be a list of arbitrary integers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a flag whose value can be a list of strings from enum_values.", "response": "def DEFINE_multi_enum(  # pylint: disable=invalid-name,redefined-builtin\n    name, default, enum_values, help, flag_values=_flagvalues.FLAGS,\n    case_sensitive=True, **args):\n  \"\"\"Registers a flag whose value can be a list strings from enum_values.\n\n  Use the flag on the command line multiple times to place multiple\n  enum values into the list.  The 'default' may be a single string\n  (which will be converted into a single-element list) or a list of\n  strings.\n\n  Args:\n    name: str, the flag name.\n    default: Union[Iterable[Text], Text, None], the default value of the flag;\n        see `DEFINE_multi`.\n    enum_values: [str], a non-empty list of strings with the possible values for\n        the flag.\n    help: str, the help message.\n    flag_values: FlagValues, the FlagValues instance with which the flag will\n        be registered. This should almost never need to be overridden.\n    case_sensitive: Whether or not the enum is to be case-sensitive.\n    **args: Dictionary with extra keyword args that are passed to the\n        Flag __init__.\n  \"\"\"\n  parser = _argument_parser.EnumParser(enum_values, case_sensitive)\n  serializer = _argument_parser.ArgumentSerializer()\n  DEFINE_multi(parser, serializer, name, default, help, flag_values, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DEFINE_multi_enum_class(  # pylint: disable=invalid-name,redefined-builtin\n    name,\n    default,\n    enum_class,\n    help,\n    flag_values=_flagvalues.FLAGS,\n    module_name=None,\n    **args):\n  \"\"\"Registers a flag whose value can be a list of enum members.\n\n  Use the flag on the command line multiple times to place multiple\n  enum values into the list.\n\n  Args:\n    name: str, the flag name.\n    default: Union[Iterable[Enum], Iterable[Text], Enum, Text, None], the\n        default value of the flag; see\n        `DEFINE_multi`; only differences are documented here. If the value is\n        a single Enum, it is treated as a single-item list of that Enum value.\n        If it is an iterable, text values within the iterable will be converted\n        to the equivalent Enum objects.\n    enum_class: class, the Enum class with all the possible values for the flag.\n        help: str, the help message.\n    flag_values: FlagValues, the FlagValues instance with which the flag will be\n      registered. This should almost never need to be overridden.\n    module_name: A string, the name of the Python module declaring this flag. If\n      not provided, it will be computed using the stack trace of this call.\n    **args: Dictionary with extra keyword args that are passed to the Flag\n      __init__.\n  \"\"\"\n  DEFINE_flag(\n      _flag.MultiEnumClassFlag(name, default, help, enum_class),\n      flag_values, module_name, **args)", "response": "Registers a flag whose value can be a list of enum members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the kernel s process name on Linux.", "response": "def set_kernel_process_name(name):\n  \"\"\"Changes the Kernel's /proc/self/status process name on Linux.\n\n  The kernel name is NOT what will be shown by the ps or top command.\n  It is a 15 character string stored in the kernel's process table that\n  is included in the kernel log when a process is OOM killed.\n  The first 15 bytes of name are used.  Non-ASCII unicode is replaced with '?'.\n\n  Does nothing if /proc/self/comm cannot be written or prctl() fails.\n\n  Args:\n    name: bytes|unicode, the Linux kernel's command name to set.\n  \"\"\"\n  if not isinstance(name, bytes):\n    name = name.encode('ascii', 'replace')\n  try:\n    # This is preferred to using ctypes to try and call prctl() when possible.\n    with open('/proc/self/comm', 'wb') as proc_comm:\n      proc_comm.write(name[:15])\n  except EnvironmentError:\n    try:\n      import ctypes\n    except ImportError:\n      return  # No ctypes.\n    try:\n      libc = ctypes.CDLL('libc.so.6')\n    except EnvironmentError:\n      return  # No libc.so.6.\n    pr_set_name = ctypes.c_ulong(15)  # linux/prctl.h PR_SET_NAME value.\n    zero = ctypes.c_ulong(0)\n    try:\n      libc.prctl(pr_set_name, name, zero, zero, zero)\n      # Ignore the prctl return value.  Nothing we can do if it errored.\n    except AttributeError:\n      return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, argument):\n    if not isinstance(argument, six.string_types):\n      raise TypeError('flag value must be a string, found \"{}\"'.format(\n          type(argument)))\n    return argument", "response": "Parses the string argument and returns the native value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_outside_bounds(self, val):\n    return ((self.lower_bound is not None and val < self.lower_bound) or\n            (self.upper_bound is not None and val > self.upper_bound))", "response": "Returns whether the value is outside the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a single argument into a sequence of values.", "response": "def parse(self, argument):\n    \"\"\"See base class.\"\"\"\n    val = self.convert(argument)\n    if self.is_outside_bounds(val):\n      raise ValueError('%s is not %s' % (val, self.syntactic_help))\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the float value of argument.", "response": "def convert(self, argument):\n    \"\"\"Returns the float value of argument.\"\"\"\n    if (_is_integer_type(argument) or isinstance(argument, float) or\n        isinstance(argument, six.string_types)):\n      return float(argument)\n    else:\n      raise TypeError(\n          'Expect argument to be a string, int, or float, found {}'.format(\n              type(argument)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the int value of argument.", "response": "def convert(self, argument):\n    \"\"\"Returns the int value of argument.\"\"\"\n    if _is_integer_type(argument):\n      return argument\n    elif isinstance(argument, six.string_types):\n      base = 10\n      if len(argument) > 2 and argument[0] == '0':\n        if argument[1] == 'o':\n          base = 8\n        elif argument[1] == 'x':\n          base = 16\n      return int(argument, base)\n    else:\n      raise TypeError('Expect argument to be a string or int, found {}'.format(\n          type(argument)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the argument into a sequence of objects.", "response": "def parse(self, argument):\n    \"\"\"See base class.\"\"\"\n    if isinstance(argument, six.string_types):\n      if argument.lower() in ('true', 't', '1'):\n        return True\n      elif argument.lower() in ('false', 'f', '0'):\n        return False\n      else:\n        raise ValueError('Non-boolean argument to boolean flag', argument)\n    elif isinstance(argument, six.integer_types):\n      # Only allow bool or integer 0, 1.\n      # Note that float 1.0 == True, 0.0 == False.\n      bool_value = bool(argument)\n      if argument == bool_value:\n        return bool_value\n      else:\n        raise ValueError('Non-boolean argument to boolean flag', argument)\n\n    raise TypeError('Non-boolean argument to boolean flag', argument)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self, argument):\n    if isinstance(argument, self.enum_class):\n      return argument\n    if argument not in self.enum_class.__members__:\n      raise ValueError('value should be one of <%s>' %\n                       '|'.join(self.enum_class.__members__.keys()))\n    else:\n      return self.enum_class[argument]", "response": "Determines validity of argument and returns the correct element of enum."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize a list of values.", "response": "def serialize(self, value):\n    \"\"\"See base class.\"\"\"\n    return self.list_sep.join([_helpers.str_or_unicode(x) for x in value])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize a single object into a string.", "response": "def serialize(self, value):\n    \"\"\"See base class.\"\"\"\n    if isinstance(value, list):\n      return self.list_sep.join(_helpers.str_or_unicode(x.name) for x in value)\n    else:\n      return _helpers.str_or_unicode(value.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, argument):\n    if isinstance(argument, list):\n      return argument\n    elif not argument:\n      return []\n    else:\n      return [s.strip() for s in argument.split(self._token)]", "response": "Parse the argument into a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse argument as comma - separated list of strings.", "response": "def parse(self, argument):\n    \"\"\"Parses argument as comma-separated list of strings.\"\"\"\n    if isinstance(argument, list):\n      return argument\n    elif not argument:\n      return []\n    else:\n      try:\n        return [s.strip() for s in list(csv.reader([argument], strict=True))[0]]\n      except csv.Error as e:\n        # Provide a helpful report for case like\n        #   --listflag=\"$(printf 'hello,\\nworld')\"\n        # IOW, list flag values containing naked newlines.  This error\n        # was previously \"reported\" by allowing csv.Error to\n        # propagate.\n        raise ValueError('Unable to parse the value %r as a %s: %s'\n                         % (argument, self.flag_type(), e))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the argument as whitespace - separated list of strings.", "response": "def parse(self, argument):\n    \"\"\"Parses argument as whitespace-separated list of strings.\n\n    It also parses argument as comma-separated list of strings if requested.\n\n    Args:\n      argument: string argument passed in the commandline.\n\n    Returns:\n      [str], the parsed flag value.\n    \"\"\"\n    if isinstance(argument, list):\n      return argument\n    elif not argument:\n      return []\n    else:\n      if self._comma_compat:\n        argument = argument.replace(',', ' ')\n      return argument.split()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting whether or not to use GNU style scanning.", "response": "def set_gnu_getopt(self, gnu_getopt=True):\n    \"\"\"Sets whether or not to use GNU style scanning.\n\n    GNU style allows mixing of flag and non-flag arguments. See\n    http://docs.python.org/library/getopt.html#getopt.gnu_getopt\n\n    Args:\n      gnu_getopt: bool, whether or not to use GNU style scanning.\n    \"\"\"\n    self.__dict__['__use_gnu_getopt'] = gnu_getopt\n    self.__dict__['__use_gnu_getopt_explicitly_set'] = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_flag_by_module(self, module_name, flag):\n    flags_by_module = self.flags_by_module_dict()\n    flags_by_module.setdefault(module_name, []).append(flag)", "response": "Register a flag instance by module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrecording the module that defines a specific flag.", "response": "def register_flag_by_module_id(self, module_id, flag):\n    \"\"\"Records the module that defines a specific flag.\n\n    Args:\n      module_id: int, the ID of the Python module.\n      flag: Flag, the Flag instance that is key to the module.\n    \"\"\"\n    flags_by_module_id = self.flags_by_module_id_dict()\n    flags_by_module_id.setdefault(module_id, []).append(flag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nspecifying that a flag is a key flag for a Python module.", "response": "def register_key_flag_for_module(self, module_name, flag):\n    \"\"\"Specifies that a flag is a key flag for a module.\n\n    Args:\n      module_name: str, the name of a Python module.\n      flag: Flag, the Flag instance that is key to the module.\n    \"\"\"\n    key_flags_by_module = self.key_flags_by_module_dict()\n    # The list of key flags for the module named module_name.\n    key_flags = key_flags_by_module.setdefault(module_name, [])\n    # Add flag, but avoid duplicates.\n    if flag not in key_flags:\n      key_flags.append(flag)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _flag_is_registered(self, flag_obj):\n    flag_dict = self._flags()\n    # Check whether flag_obj is registered under its long name.\n    name = flag_obj.name\n    if flag_dict.get(name, None) == flag_obj:\n      return True\n    # Check whether flag_obj is registered under its short name.\n    short_name = flag_obj.short_name\n    if (short_name is not None and\n        flag_dict.get(short_name, None) == flag_obj):\n      return True\n    return False", "response": "Checks whether a Flag object is registered under long name or short name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans up unregistered flags from all module - level dictionaries.", "response": "def _cleanup_unregistered_flag_from_module_dicts(self, flag_obj):\n    \"\"\"Cleans up unregistered flags from all module -> [flags] dictionaries.\n\n    If flag_obj is registered under either its long name or short name, it\n    won't be removed from the dictionaries.\n\n    Args:\n      flag_obj: Flag, the Flag instance to clean up for.\n    \"\"\"\n    if self._flag_is_registered(flag_obj):\n      return\n    for flags_by_module_dict in (self.flags_by_module_dict(),\n                                 self.flags_by_module_id_dict(),\n                                 self.key_flags_by_module_dict()):\n      for flags_in_module in six.itervalues(flags_by_module_dict):\n        # While (as opposed to if) takes care of multiple occurrences of a\n        # flag in the list for the same module.\n        while flag_obj in flags_in_module:\n          flags_in_module.remove(flag_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_flags_defined_by_module(self, module):\n    if not isinstance(module, str):\n      module = module.__name__\n\n    return list(self.flags_by_module_dict().get(module, []))", "response": "Returns the list of flags defined by a module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_key_flags_for_module(self, module):\n    if not isinstance(module, str):\n      module = module.__name__\n\n    # Any flag is a key flag for the module that defined it.  NOTE:\n    # key_flags is a fresh list: we can update it without affecting the\n    # internals of this FlagValues object.\n    key_flags = self._get_flags_defined_by_module(module)\n\n    # Take into account flags explicitly declared as key for a module.\n    for flag in self.key_flags_by_module_dict().get(module, []):\n      if flag not in key_flags:\n        key_flags.append(flag)\n    return key_flags", "response": "Returns the list of key flags defined by a module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_module_defining_flag(self, flagname, default=None):\n    registered_flag = self._flags().get(flagname)\n    if registered_flag is None:\n      return default\n    for module, flags in six.iteritems(self.flags_by_module_dict()):\n      for flag in flags:\n        # It must compare the flag with the one in _flags. This is because a\n        # flag might be overridden only for its long name (or short name),\n        # and only its short name (or long name) is considered registered.\n        if (flag.name == registered_flag.name and\n            flag.short_name == registered_flag.short_name):\n          return module\n    return default", "response": "Returns the name of the module which defines this flag or default if flagname is not defined."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_module_id_defining_flag(self, flagname, default=None):\n    registered_flag = self._flags().get(flagname)\n    if registered_flag is None:\n      return default\n    for module_id, flags in six.iteritems(self.flags_by_module_id_dict()):\n      for flag in flags:\n        # It must compare the flag with the one in _flags. This is because a\n        # flag might be overridden only for its long name (or short name),\n        # and only its short name (or long name) is considered registered.\n        if (flag.name == registered_flag.name and\n            flag.short_name == registered_flag.short_name):\n          return module_id\n    return default", "response": "Returns the ID of the module which defines this flag or default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value of the unknown flag.", "response": "def _set_unknown_flag(self, name, value):\n    \"\"\"Returns value if setting flag |name| to |value| returned True.\n\n    Args:\n      name: str, name of the flag to set.\n      value: Value to set.\n\n    Returns:\n      Flag value on successful call.\n\n    Raises:\n      UnrecognizedFlagError\n      IllegalFlagValueError\n    \"\"\"\n    setter = self.__dict__['__set_unknown']\n    if setter:\n      try:\n        setter(name, value)\n        return value\n      except (TypeError, ValueError):  # Flag value is not valid.\n        raise _exceptions.IllegalFlagValueError(\n            '\"{1}\" is not valid for --{0}' .format(name, value))\n      except NameError:  # Flag name is not valid.\n        pass\n    raise _exceptions.UnrecognizedFlagError(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends flags registered in another FlagValues instance to the current FlagValues instance.", "response": "def append_flag_values(self, flag_values):\n    \"\"\"Appends flags registered in another FlagValues instance.\n\n    Args:\n      flag_values: FlagValues, the FlagValues instance from which to copy flags.\n    \"\"\"\n    for flag_name, flag in six.iteritems(flag_values._flags()):  # pylint: disable=protected-access\n      # Each flags with short_name appears here twice (once under its\n      # normal name, and again with its short name).  To prevent\n      # problems (DuplicateFlagError) with double flag registration, we\n      # perform a check to make sure that the entry we're looking at is\n      # for its normal name.\n      if flag_name == flag.name:\n        try:\n          self[flag_name] = flag\n        except _exceptions.DuplicateFlagError:\n          raise _exceptions.DuplicateFlagError.from_flag(\n              flag_name, self, other_flag_values=flag_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasserts if all validators in the list satisfy the flag.", "response": "def _assert_validators(self, validators):\n    \"\"\"Asserts if all validators in the list are satisfied.\n\n    It asserts validators in the order they were created.\n\n    Args:\n      validators: Iterable(validators.Validator), validators to be\n          verified.\n    Raises:\n      AttributeError: Raised if validators work with a non-existing flag.\n      IllegalFlagValueError: Raised if validation fails for at least one\n          validator.\n    \"\"\"\n    for validator in sorted(\n        validators, key=lambda validator: validator.insertion_index):\n      try:\n        validator.verify(self)\n      except _exceptions.ValidationError as e:\n        message = validator.print_flags_with_values(self)\n        raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_default(self, name, value):\n    fl = self._flags()\n    if name not in fl:\n      self._set_unknown_flag(name, value)\n      return\n    fl[name]._set_default(value)  # pylint: disable=protected-access\n    self._assert_validators(fl[name].validators)", "response": "Changes the default value of the named flag object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_args(self, args, known_only):\n    unparsed_names_and_args = []  # A list of (flag name or None, arg).\n    undefok = set()\n    retired_flag_func = self.__dict__['__is_retired_flag_func']\n\n    flag_dict = self._flags()\n    args = iter(args)\n    for arg in args:\n      value = None\n\n      def get_value():\n        # pylint: disable=cell-var-from-loop\n        try:\n          return next(args) if value is None else value\n        except StopIteration:\n          raise _exceptions.Error('Missing value for flag ' + arg)  # pylint: disable=undefined-loop-variable\n\n      if not arg.startswith('-'):\n        # A non-argument: default is break, GNU is skip.\n        unparsed_names_and_args.append((None, arg))\n        if self.is_gnu_getopt():\n          continue\n        else:\n          break\n\n      if arg == '--':\n        if known_only:\n          unparsed_names_and_args.append((None, arg))\n        break\n\n      # At this point, arg must start with '-'.\n      if arg.startswith('--'):\n        arg_without_dashes = arg[2:]\n      else:\n        arg_without_dashes = arg[1:]\n\n      if '=' in arg_without_dashes:\n        name, value = arg_without_dashes.split('=', 1)\n      else:\n        name, value = arg_without_dashes, None\n\n      if not name:\n        # The argument is all dashes (including one dash).\n        unparsed_names_and_args.append((None, arg))\n        if self.is_gnu_getopt():\n          continue\n        else:\n          break\n\n      # --undefok is a special case.\n      if name == 'undefok':\n        value = get_value()\n        undefok.update(v.strip() for v in value.split(','))\n        undefok.update('no' + v.strip() for v in value.split(','))\n        continue\n\n      flag = flag_dict.get(name)\n      if flag:\n        if flag.boolean and value is None:\n          value = 'true'\n        else:\n          value = get_value()\n      elif name.startswith('no') and len(name) > 2:\n        # Boolean flags can take the form of --noflag, with no value.\n        noflag = flag_dict.get(name[2:])\n        if noflag and noflag.boolean:\n          if value is not None:\n            raise ValueError(arg + ' does not take an argument')\n          flag = noflag\n          value = 'false'\n\n      if retired_flag_func and not flag:\n        is_retired, is_bool = retired_flag_func(name)\n\n        # If we didn't recognize that flag, but it starts with\n        # \"no\" then maybe it was a boolean flag specified in the\n        # --nofoo form.\n        if not is_retired and name.startswith('no'):\n          is_retired, is_bool = retired_flag_func(name[2:])\n          is_retired = is_retired and is_bool\n\n        if is_retired:\n          if not is_bool and value is None:\n            # This happens when a non-bool retired flag is specified\n            # in format of \"--flag value\".\n            get_value()\n          logging.error('Flag \"%s\" is retired and should no longer '\n                        'be specified. See go/totw/90.', name)\n          continue\n\n      if flag:\n        flag.parse(value)\n        flag.using_default_value = False\n      else:\n        unparsed_names_and_args.append((name, arg))\n\n    unknown_flags = []\n    unparsed_args = []\n    for name, arg in unparsed_names_and_args:\n      if name is None:\n        # Positional arguments.\n        unparsed_args.append(arg)\n      elif name in undefok:\n        # Remove undefok flags.\n        continue\n      else:\n        # This is an unknown flag.\n        if known_only:\n          unparsed_args.append(arg)\n        else:\n          unknown_flags.append((name, arg))\n\n    unparsed_args.extend(list(args))\n    return unknown_flags, unparsed_args", "response": "This function parses the arguments of the flag value and returns a list of tuples with the names of the flags that are found in the flag dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unparse_flags(self):\n    for f in self._flags().values():\n      f.unparse()\n    # We log this message before marking flags as unparsed to avoid a\n    # problem when the logging library causes flags access.\n    logging.info('unparse_flags() called; flags access will now raise errors.')\n    self.__dict__['__flags_parsed'] = False\n    self.__dict__['__unparse_flags_called'] = True", "response": "Unparses all flags to the point before any FLAGS was called."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary that maps flag names to flag values.", "response": "def flag_values_dict(self):\n    \"\"\"Returns a dictionary that maps flag names to flag values.\"\"\"\n    return {name: flag.value for name, flag in six.iteritems(self._flags())}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_help(self, prefix='', include_special_flags=True):\n    flags_by_module = self.flags_by_module_dict()\n    if flags_by_module:\n      modules = sorted(flags_by_module)\n      # Print the help for the main module first, if possible.\n      main_module = sys.argv[0]\n      if main_module in modules:\n        modules.remove(main_module)\n        modules = [main_module] + modules\n      return self._get_help_for_modules(modules, prefix, include_special_flags)\n    else:\n      output_lines = []\n      # Just print one long list of flags.\n      values = six.itervalues(self._flags())\n      if include_special_flags:\n        values = itertools.chain(\n            values, six.itervalues(_helpers.SPECIAL_FLAGS._flags()))  # pylint: disable=protected-access\n      self._render_flag_list(values, output_lines, prefix)\n      return '\\n'.join(output_lines)", "response": "Returns a string that contains all known flags and their help."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_help_for_modules(self, modules, prefix, include_special_flags):\n    output_lines = []\n    for module in modules:\n      self._render_our_module_flags(module, output_lines, prefix)\n    if include_special_flags:\n      self._render_module_flags(\n          'absl.flags',\n          six.itervalues(_helpers.SPECIAL_FLAGS._flags()),  # pylint: disable=protected-access\n          output_lines,\n          prefix)\n    return '\\n'.join(output_lines)", "response": "Private to absl. flags package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a help string for a given module.", "response": "def _render_module_flags(self, module, flags, output_lines, prefix=''):\n    \"\"\"Returns a help string for a given module.\"\"\"\n    if not isinstance(module, str):\n      module = module.__name__\n    output_lines.append('\\n%s%s:' % (prefix, module))\n    self._render_flag_list(flags, output_lines, prefix + '  ')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _render_our_module_flags(self, module, output_lines, prefix=''):\n    flags = self._get_flags_defined_by_module(module)\n    if flags:\n      self._render_module_flags(module, flags, output_lines, prefix)", "response": "Returns a help string for a given module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a help string for the key flags of a given module.", "response": "def _render_our_module_key_flags(self, module, output_lines, prefix=''):\n    \"\"\"Returns a help string for the key flags of a given module.\n\n    Args:\n      module: module|str, the module to render key flags for.\n      output_lines: [str], a list of strings.  The generated help message\n          lines will be appended to this list.\n      prefix: str, a string that is prepended to each generated help line.\n    \"\"\"\n    key_flags = self.get_key_flags_for_module(module)\n    if key_flags:\n      self._render_module_flags(module, key_flags, output_lines, prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndescribing the key flags of a module.", "response": "def module_help(self, module):\n    \"\"\"Describes the key flags of a module.\n\n    Args:\n      module: module|str, the module to describe the key flags for.\n\n    Returns:\n      str, describing the key flags of a module.\n    \"\"\"\n    helplist = []\n    self._render_our_module_key_flags(module, helplist)\n    return '\\n'.join(helplist)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_flag_value(self, name, default):  # pylint: disable=invalid-name\n\n    value = self.__getattr__(name)\n    if value is not None:  # Can't do if not value, b/c value might be '0' or \"\"\n      return value\n    else:\n      return default", "response": "Returns the value of a flag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_flag_file_directive(self, flag_string):\n    if isinstance(flag_string, type('')):\n      if flag_string.startswith('--flagfile='):\n        return 1\n      elif flag_string == '--flagfile':\n        return 1\n      elif flag_string.startswith('-flagfile='):\n        return 1\n      elif flag_string == '-flagfile':\n        return 1\n      else:\n        return 0\n    return 0", "response": "Checks whether flag_string contains a flagfile=<foo > directive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_filename(self, flagfile_str):\n    if flagfile_str.startswith('--flagfile='):\n      return os.path.expanduser((flagfile_str[(len('--flagfile=')):]).strip())\n    elif flagfile_str.startswith('-flagfile='):\n      return os.path.expanduser((flagfile_str[(len('-flagfile=')):]).strip())\n    else:\n      raise _exceptions.Error(\n          'Hit illegal --flagfile type: %s' % flagfile_str)", "response": "Extracts the filename from a flagfile_str of form - [ flagfile = filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_flag_file_lines(self, filename, parsed_file_stack=None):\n    # For consistency with the cpp version, ignore empty values.\n    if not filename:\n      return []\n    if parsed_file_stack is None:\n      parsed_file_stack = []\n    # We do a little safety check for reparsing a file we've already encountered\n    # at a previous depth.\n    if filename in parsed_file_stack:\n      sys.stderr.write('Warning: Hit circular flagfile dependency. Ignoring'\n                       ' flagfile: %s\\n' % (filename,))\n      return []\n    else:\n      parsed_file_stack.append(filename)\n\n    line_list = []  # All line from flagfile.\n    flag_line_list = []  # Subset of lines w/o comments, blanks, flagfile= tags.\n    try:\n      file_obj = open(filename, 'r')\n    except IOError as e_msg:\n      raise _exceptions.CantOpenFlagFileError(\n          'ERROR:: Unable to open flagfile: %s' % e_msg)\n\n    with file_obj:\n      line_list = file_obj.readlines()\n\n    # This is where we check each line in the file we just read.\n    for line in line_list:\n      if line.isspace():\n        pass\n      # Checks for comment (a line that starts with '#').\n      elif line.startswith('#') or line.startswith('//'):\n        pass\n      # Checks for a nested \"--flagfile=<bar>\" flag in the current file.\n      # If we find one, recursively parse down into that file.\n      elif self._is_flag_file_directive(line):\n        sub_filename = self._extract_filename(line)\n        included_flags = self._get_flag_file_lines(\n            sub_filename, parsed_file_stack=parsed_file_stack)\n        flag_line_list.extend(included_flags)\n      else:\n        # Any line that's not a comment or a nested flagfile should get\n        # copied into 2nd position.  This leaves earlier arguments\n        # further back in the list, thus giving them higher priority.\n        flag_line_list.append(line.strip())\n\n    parsed_file_stack.pop()\n    return flag_line_list", "response": "Returns the useful lines from a flag file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads all flags from a list of command line arguments.", "response": "def read_flags_from_files(self, argv, force_gnu=True):\n    \"\"\"Processes command line args, but also allow args to be read from file.\n\n    Args:\n      argv: [str], a list of strings, usually sys.argv[1:], which may contain\n          one or more flagfile directives of the form --flagfile=\"./filename\".\n          Note that the name of the program (sys.argv[0]) should be omitted.\n      force_gnu: bool, if False, --flagfile parsing obeys the\n          FLAGS.is_gnu_getopt() value. If True, ignore the value and always\n          follow gnu_getopt semantics.\n\n    Returns:\n      A new list which has the original list combined with what we read\n      from any flagfile(s).\n\n    Raises:\n      IllegalFlagValueError: Raised when --flagfile is provided with no\n          argument.\n\n    This function is called by FLAGS(argv).\n    It scans the input list for a flag that looks like:\n    --flagfile=<somefile>. Then it opens <somefile>, reads all valid key\n    and value pairs and inserts them into the input list in exactly the\n    place where the --flagfile arg is found.\n\n    Note that your application's flags are still defined the usual way\n    using absl.flags DEFINE_flag() type functions.\n\n    Notes (assuming we're getting a commandline of some sort as our input):\n    --> For duplicate flags, the last one we hit should \"win\".\n    --> Since flags that appear later win, a flagfile's settings can be \"weak\"\n        if the --flagfile comes at the beginning of the argument sequence,\n        and it can be \"strong\" if the --flagfile comes at the end.\n    --> A further \"--flagfile=<otherfile.cfg>\" CAN be nested in a flagfile.\n        It will be expanded in exactly the spot where it is found.\n    --> In a flagfile, a line beginning with # or // is a comment.\n    --> Entirely blank lines _should_ be ignored.\n    \"\"\"\n    rest_of_args = argv\n    new_argv = []\n    while rest_of_args:\n      current_arg = rest_of_args[0]\n      rest_of_args = rest_of_args[1:]\n      if self._is_flag_file_directive(current_arg):\n        # This handles the case of -(-)flagfile foo.  In this case the\n        # next arg really is part of this one.\n        if current_arg == '--flagfile' or current_arg == '-flagfile':\n          if not rest_of_args:\n            raise _exceptions.IllegalFlagValueError(\n                '--flagfile with no argument')\n          flag_filename = os.path.expanduser(rest_of_args[0])\n          rest_of_args = rest_of_args[1:]\n        else:\n          # This handles the case of (-)-flagfile=foo.\n          flag_filename = self._extract_filename(current_arg)\n        new_argv.extend(self._get_flag_file_lines(flag_filename))\n      else:\n        new_argv.append(current_arg)\n        # Stop parsing after '--', like getopt and gnu_getopt.\n        if current_arg == '--':\n          break\n        # Stop parsing after a non-flag, like getopt.\n        if not current_arg.startswith('-'):\n          if not force_gnu and not self.__dict__['__use_gnu_getopt']:\n            break\n        else:\n          if ('=' not in current_arg and\n              rest_of_args and not rest_of_args[0].startswith('-')):\n            # If this is an occurrence of a legitimate --x y, skip the value\n            # so that it won't be mistaken for a standalone arg.\n            fl = self._flags()\n            name = current_arg.lstrip('-')\n            if name in fl and not fl[name].boolean:\n              current_arg = rest_of_args[0]\n              rest_of_args = rest_of_args[1:]\n              new_argv.append(current_arg)\n\n    if rest_of_args:\n      new_argv.extend(rest_of_args)\n\n    return new_argv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string with the flags assignments from this FlagValues object.", "response": "def flags_into_string(self):\n    \"\"\"Returns a string with the flags assignments from this FlagValues object.\n\n    This function ignores flags whose value is None.  Each flag\n    assignment is separated by a newline.\n\n    NOTE: MUST mirror the behavior of the C++ CommandlineFlagsIntoString\n    from https://github.com/gflags/gflags.\n\n    Returns:\n      str, the string with the flags assignments from this FlagValues object.\n      The flags are ordered by (module_name, flag_name).\n    \"\"\"\n    module_flags = sorted(self.flags_by_module_dict().items())\n    s = ''\n    for unused_module_name, flags in module_flags:\n      flags = sorted(flags, key=lambda f: f.name)\n      for flag in flags:\n        if flag.value is not None:\n          s += flag.serialize() + '\\n'\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append_flags_into_file(self, filename):\n    with open(filename, 'a') as out_file:\n      out_file.write(self.flags_into_string())", "response": "Appends all flags assignments from this FlagInfo object to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noutput flag documentation in XML format.", "response": "def write_help_in_xml_format(self, outfile=None):\n    \"\"\"Outputs flag documentation in XML format.\n\n    NOTE: We use element names that are consistent with those used by\n    the C++ command-line flag library, from\n    https://github.com/gflags/gflags.\n    We also use a few new elements (e.g., <key>), but we do not\n    interfere / overlap with existing XML elements used by the C++\n    library.  Please maintain this consistency.\n\n    Args:\n      outfile: File object we write to.  Default None means sys.stdout.\n    \"\"\"\n    doc = minidom.Document()\n    all_flag = doc.createElement('AllFlags')\n    doc.appendChild(all_flag)\n\n    all_flag.appendChild(_helpers.create_xml_dom_element(\n        doc, 'program', os.path.basename(sys.argv[0])))\n\n    usage_doc = sys.modules['__main__'].__doc__\n    if not usage_doc:\n      usage_doc = '\\nUSAGE: %s [flags]\\n' % sys.argv[0]\n    else:\n      usage_doc = usage_doc.replace('%s', sys.argv[0])\n    all_flag.appendChild(_helpers.create_xml_dom_element(\n        doc, 'usage', usage_doc))\n\n    # Get list of key flags for the main module.\n    key_flags = self.get_key_flags_for_module(sys.argv[0])\n\n    # Sort flags by declaring module name and next by flag name.\n    flags_by_module = self.flags_by_module_dict()\n    all_module_names = list(flags_by_module.keys())\n    all_module_names.sort()\n    for module_name in all_module_names:\n      flag_list = [(f.name, f) for f in flags_by_module[module_name]]\n      flag_list.sort()\n      for unused_flag_name, flag in flag_list:\n        is_key = flag in key_flags\n        all_flag.appendChild(flag._create_xml_dom_element(  # pylint: disable=protected-access\n            doc, module_name, is_key=is_key))\n\n    outfile = outfile or sys.stdout\n    if six.PY2:\n      outfile.write(doc.toprettyxml(indent='  ', encoding='utf-8'))\n    else:\n      outfile.write(\n          doc.toprettyxml(indent='  ', encoding='utf-8').decode('utf-8'))\n    outfile.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef absl_to_cpp(level):\n  if not isinstance(level, int):\n    raise TypeError('Expect an int level, found {}'.format(type(level)))\n  if level >= 0:\n    # C++ log levels must be >= 0\n    return 0\n  else:\n    return -level", "response": "Converts an absl log level to a cpp log level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef absl_to_standard(level):\n  if not isinstance(level, int):\n    raise TypeError('Expect an int level, found {}'.format(type(level)))\n  if level < ABSL_FATAL:\n    level = ABSL_FATAL\n  if level <= ABSL_DEBUG:\n    return ABSL_TO_STANDARD[level]\n  # Maps to vlog levels.\n  return STANDARD_DEBUG - level + 1", "response": "Converts an integer level from the absl value to the standard value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an integer level from the standard value to the absl value.", "response": "def standard_to_absl(level):\n  \"\"\"Converts an integer level from the standard value to the absl value.\n\n  Args:\n    level: int, a Python standard logging level.\n\n  Raises:\n    TypeError: Raised when level is not an integer.\n\n  Returns:\n    The corresponding integer level for use in absl logging.\n  \"\"\"\n  if not isinstance(level, int):\n    raise TypeError('Expect an int level, found {}'.format(type(level)))\n  if level < 0:\n    level = 0\n  if level < STANDARD_DEBUG:\n    # Maps to vlog levels.\n    return STANDARD_DEBUG - level + 1\n  elif level < STANDARD_INFO:\n    return ABSL_DEBUG\n  elif level < STANDARD_WARNING:\n    return ABSL_INFO\n  elif level < STANDARD_ERROR:\n    return ABSL_WARNING\n  elif level < STANDARD_CRITICAL:\n    return ABSL_ERROR\n  else:\n    return ABSL_FATAL"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_flags_with_usage(args):\n  try:\n    return FLAGS(args)\n  except flags.Error as error:\n    sys.stderr.write('FATAL Flags parsing error: %s\\n' % error)\n    sys.stderr.write('Pass --helpshort or --helpfull to see help on flags.\\n')\n    sys.exit(1)", "response": "Tries to parse the flags print usage and exit if unparseable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef define_help_flags():\n  # Use a global to ensure idempotence.\n  global _define_help_flags_called\n\n  if not _define_help_flags_called:\n    flags.DEFINE_flag(HelpFlag())\n    flags.DEFINE_flag(HelpshortFlag())  # alias for --help\n    flags.DEFINE_flag(HelpfullFlag())\n    flags.DEFINE_flag(HelpXMLFlag())\n    _define_help_flags_called = True", "response": "Registers help flags. Idempotent."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters help flags parses arguments and shows usage if appropriate.", "response": "def _register_and_parse_flags_with_usage(\n    argv=None,\n    flags_parser=parse_flags_with_usage,\n):\n  \"\"\"Registers help flags, parses arguments and shows usage if appropriate.\n\n  This also calls sys.exit(0) if flag --only_check_args is True.\n\n  Args:\n    argv: [str], a non-empty list of the command line arguments including\n        program name, sys.argv is used if None.\n    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.\n        The return value of this function is passed to `main` untouched.\n        It must guarantee FLAGS is parsed after this function is called.\n\n  Returns:\n    The return value of `flags_parser`. When using the default `flags_parser`,\n    it returns the following:\n    [str], a non-empty list of remaining command line arguments after parsing\n    flags, including program name.\n\n  Raises:\n    Error: Raised when flags_parser is called, but FLAGS is not parsed.\n    SystemError: Raised when it's called more than once.\n  \"\"\"\n  if _register_and_parse_flags_with_usage.done:\n    raise SystemError('Flag registration can be done only once.')\n\n  define_help_flags()\n\n  original_argv = sys.argv if argv is None else argv\n  args_to_main = flags_parser(original_argv)\n  if not FLAGS.is_parsed():\n    raise Error('FLAGS must be parsed after flags_parser is called.')\n\n  # Exit when told so.\n  if FLAGS.only_check_args:\n    sys.exit(0)\n  # Immediately after flags are parsed, bump verbosity to INFO if the flag has\n  # not been set.\n  if FLAGS['verbosity'].using_default_value:\n    FLAGS.verbosity = 0\n  _register_and_parse_flags_with_usage.done = True\n\n  return args_to_main"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_main(main, argv):\n  if FLAGS.run_with_pdb:\n    sys.exit(pdb.runcall(main, argv))\n  elif FLAGS.run_with_profiling or FLAGS.profile_file:\n    # Avoid import overhead since most apps (including performance-sensitive\n    # ones) won't be run with profiling.\n    import atexit\n    if FLAGS.use_cprofile_for_profiling:\n      import cProfile as profile\n    else:\n      import profile\n    profiler = profile.Profile()\n    if FLAGS.profile_file:\n      atexit.register(profiler.dump_stats, FLAGS.profile_file)\n    else:\n      atexit.register(profiler.print_stats)\n    retval = profiler.runcall(main, argv)\n    sys.exit(retval)\n  else:\n    sys.exit(main(argv))", "response": "Calls main optionally with pdb or profiler."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling any installed exception handlers.", "response": "def _call_exception_handlers(exception):\n  \"\"\"Calls any installed exception handlers.\"\"\"\n  for handler in EXCEPTION_HANDLERS:\n    try:\n      if handler.wants(exception):\n        handler.handle(exception)\n    except:  # pylint: disable=bare-except\n      try:\n        # We don't want to stop for exceptions in the exception handlers but\n        # we shouldn't hide them either.\n        logging.error(traceback.format_exc())\n      except:  # pylint: disable=bare-except\n        # In case even the logging statement fails, ignore.\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts executing the program.", "response": "def run(\n    main,\n    argv=None,\n    flags_parser=parse_flags_with_usage,\n):\n  \"\"\"Begins executing the program.\n\n  Args:\n    main: The main function to execute. It takes an single argument \"argv\",\n        which is a list of command line arguments with parsed flags removed.\n        If it returns an integer, it is used as the process's exit code.\n    argv: A non-empty list of the command line arguments including program name,\n        sys.argv is used if None.\n    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.\n        The return value of this function is passed to `main` untouched.\n        It must guarantee FLAGS is parsed after this function is called.\n  - Parses command line flags with the flag module.\n  - If there are any errors, prints usage().\n  - Calls main() with the remaining arguments.\n  - If main() raises a UsageError, prints usage and the error message.\n  \"\"\"\n  try:\n    args = _run_init(\n        sys.argv if argv is None else argv,\n        flags_parser,\n    )\n    while _init_callbacks:\n      callback = _init_callbacks.popleft()\n      callback()\n    try:\n      _run_main(main, args)\n    except UsageError as error:\n      usage(shorthelp=True, detailed_error=error, exitcode=error.exitcode)\n    except:\n      if FLAGS.pdb_post_mortem:\n        traceback.print_exc()\n        pdb.post_mortem()\n      raise\n  except Exception as e:\n    _call_exception_handlers(e)\n    raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_init(\n    argv,\n    flags_parser,\n):\n  \"\"\"Does one-time initialization and re-parses flags on rerun.\"\"\"\n  if _run_init.done:\n    return flags_parser(argv)\n  command_name.make_process_name_useful()\n  # Set up absl logging handler.\n  logging.use_absl_handler()\n  args = _register_and_parse_flags_with_usage(\n      argv=argv,\n      flags_parser=flags_parser,\n  )\n  if faulthandler:\n    try:\n      faulthandler.enable()\n    except Exception:  # pylint: disable=broad-except\n      # Some tests verify stderr output very closely, so don't print anything.\n      # Disabled faulthandler is a low-impact error.\n      pass\n  _run_init.done = True\n  return args", "response": "Does one - time initialization and re - parses flags on rerun."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef usage(shorthelp=False, writeto_stdout=False, detailed_error=None,\n          exitcode=None):\n  \"\"\"Writes __main__'s docstring to stderr with some help text.\n\n  Args:\n    shorthelp: bool, if True, prints only flags from the main module,\n        rather than all flags.\n    writeto_stdout: bool, if True, writes help message to stdout,\n        rather than to stderr.\n    detailed_error: str, additional detail about why usage info was presented.\n    exitcode: optional integer, if set, exits with this status code after\n        writing help.\n  \"\"\"\n  if writeto_stdout:\n    stdfile = sys.stdout\n  else:\n    stdfile = sys.stderr\n\n  doc = sys.modules['__main__'].__doc__\n  if not doc:\n    doc = '\\nUSAGE: %s [flags]\\n' % sys.argv[0]\n    doc = flags.text_wrap(doc, indent='       ', firstline_indent='')\n  else:\n    # Replace all '%s' with sys.argv[0], and all '%%' with '%'.\n    num_specifiers = doc.count('%') - 2 * doc.count('%%')\n    try:\n      doc %= (sys.argv[0],) * num_specifiers\n    except (OverflowError, TypeError, ValueError):\n      # Just display the docstring as-is.\n      pass\n  if shorthelp:\n    flag_str = FLAGS.main_module_help()\n  else:\n    flag_str = FLAGS.get_help()\n  try:\n    stdfile.write(doc)\n    if flag_str:\n      stdfile.write('\\nflags:\\n')\n      stdfile.write(flag_str)\n    stdfile.write('\\n')\n    if detailed_error is not None:\n      stdfile.write('\\n%s\\n' % detailed_error)\n  except IOError as e:\n    # We avoid printing a huge backtrace if we get EPIPE, because\n    # \"foo.par --help | less\" is a frequent use case.\n    if e.errno != errno.EPIPE:\n      raise\n  if exitcode is not None:\n    sys.exit(exitcode)", "response": "Prints the usage text of the flags module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls an exception handler.", "response": "def install_exception_handler(handler):\n  \"\"\"Installs an exception handler.\n\n  Args:\n    handler: ExceptionHandler, the exception handler to install.\n\n  Raises:\n    TypeError: Raised when the handler was not of the correct type.\n\n  All installed exception handlers will be called if main() exits via\n  an abnormal exception, i.e. not one of SystemExit, KeyboardInterrupt,\n  FlagsError or UsageError.\n  \"\"\"\n  if not isinstance(handler, ExceptionHandler):\n    raise TypeError('handler of type %s does not inherit from ExceptionHandler'\n                    % type(handler))\n  EXCEPTION_HANDLERS.append(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self, arguments):\n    new_values = self._parse(arguments)\n    if self.present:\n      self.value.extend(new_values)\n    else:\n      self.value = new_values\n    self.present += len(new_values)", "response": "Parses one or more arguments with the installed parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes value for XML.", "response": "def _serialize_value_for_xml(self, value):\n    \"\"\"See base class.\"\"\"\n    if value is not None:\n      value_serialized = self.serializer.serialize(value)\n    else:\n      value_serialized = ''\n    return value_serialized"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_validator(flag_name,\n                       checker,\n                       message='Flag validation failed',\n                       flag_values=_flagvalues.FLAGS):\n  \"\"\"Adds a constraint, which will be enforced during program execution.\n\n  The constraint is validated when flags are initially parsed, and after each\n  change of the corresponding flag's value.\n  Args:\n    flag_name: str, name of the flag to be checked.\n    checker: callable, a function to validate the flag.\n        input - A single positional argument: The value of the corresponding\n            flag (string, boolean, etc.  This value will be passed to checker\n            by the library).\n        output - bool, True if validator constraint is satisfied.\n            If constraint is not satisfied, it should either return False or\n            raise flags.ValidationError(desired_error_message).\n    message: str, error text to be shown to the user if checker returns False.\n        If checker raises flags.ValidationError, message from the raised\n        error will be shown.\n    flag_values: flags.FlagValues, optional FlagValues instance to validate\n        against.\n  Raises:\n    AttributeError: Raised when flag_name is not registered as a valid flag\n        name.\n  \"\"\"\n  v = SingleFlagValidator(flag_name, checker, message)\n  _add_validator(flag_values, v)", "response": "Registers a constraint that will be enforced when each flag value of the corresponding flag is set to True."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validator(flag_name, message='Flag validation failed',\n              flag_values=_flagvalues.FLAGS):\n  \"\"\"A function decorator for defining a flag validator.\n\n  Registers the decorated function as a validator for flag_name, e.g.\n\n  @flags.validator('foo')\n  def _CheckFoo(foo):\n    ...\n\n  See register_validator() for the specification of checker function.\n\n  Args:\n    flag_name: str, name of the flag to be checked.\n    message: str, error text to be shown to the user if checker returns False.\n        If checker raises flags.ValidationError, message from the raised\n        error will be shown.\n    flag_values: flags.FlagValues, optional FlagValues instance to validate\n        against.\n  Returns:\n    A function decorator that registers its function argument as a validator.\n  Raises:\n    AttributeError: Raised when flag_name is not registered as a valid flag\n        name.\n  \"\"\"\n\n  def decorate(function):\n    register_validator(flag_name, function,\n                       message=message,\n                       flag_values=flag_values)\n    return function\n  return decorate", "response": "A decorator that registers a function as a validator for flag_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that all flags in flag_names are set to None during program execution.", "response": "def mark_flags_as_required(flag_names, flag_values=_flagvalues.FLAGS):\n  \"\"\"Ensures that flags are not None during program execution.\n\n  Recommended usage:\n\n    if __name__ == '__main__':\n      flags.mark_flags_as_required(['flag1', 'flag2', 'flag3'])\n      app.run()\n\n  Args:\n    flag_names: Sequence[str], names of the flags.\n    flag_values: flags.FlagValues, optional FlagValues instance where the flags\n        are defined.\n  Raises:\n    AttributeError: If any of flag name has not already been defined as a flag.\n  \"\"\"\n  for flag_name in flag_names:\n    mark_flag_as_required(flag_name, flag_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mark_flags_as_mutual_exclusive(flag_names, required=False,\n                                   flag_values=_flagvalues.FLAGS):\n  \"\"\"Ensures that only one flag among flag_names is not None.\n\n  Important note: This validator checks if flag values are None, and it does not\n  distinguish between default and explicit values. Therefore, this validator\n  does not make sense when applied to flags with default values other than None,\n  including other false values (e.g. False, 0, '', []). That includes multi\n  flags with a default value of [] instead of None.\n\n  Args:\n    flag_names: [str], names of the flags.\n    required: bool. If true, exactly one of the flags must have a value other\n        than None. Otherwise, at most one of the flags can have a value other\n        than None, and it is valid for all of the flags to be None.\n    flag_values: flags.FlagValues, optional FlagValues instance where the flags\n        are defined.\n  \"\"\"\n  for flag_name in flag_names:\n    if flag_values[flag_name].default is not None:\n      warnings.warn(\n          'Flag --{} has a non-None default value. That does not make sense '\n          'with mark_flags_as_mutual_exclusive, which checks whether the '\n          'listed flags have a value other than None.'.format(flag_name))\n\n  def validate_mutual_exclusion(flags_dict):\n    flag_count = sum(1 for val in flags_dict.values() if val is not None)\n    if flag_count == 1 or (not required and flag_count == 0):\n      return True\n    raise _exceptions.ValidationError(\n        '{} one of ({}) must have a value other than None.'.format(\n            'Exactly' if required else 'At most', ', '.join(flag_names)))\n\n  register_multi_flags_validator(\n      flag_names, validate_mutual_exclusion, flag_values=flag_values)", "response": "Ensures that only one flag among flag_names is not None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that only one flag among flag_names is True.", "response": "def mark_bool_flags_as_mutual_exclusive(flag_names, required=False,\n                                        flag_values=_flagvalues.FLAGS):\n  \"\"\"Ensures that only one flag among flag_names is True.\n\n  Args:\n    flag_names: [str], names of the flags.\n    required: bool. If true, exactly one flag must be True. Otherwise, at most\n        one flag can be True, and it is valid for all flags to be False.\n    flag_values: flags.FlagValues, optional FlagValues instance where the flags\n        are defined.\n  \"\"\"\n  for flag_name in flag_names:\n    if not flag_values[flag_name].boolean:\n      raise _exceptions.ValidationError(\n          'Flag --{} is not Boolean, which is required for flags used in '\n          'mark_bool_flags_as_mutual_exclusive.'.format(flag_name))\n\n  def validate_boolean_mutual_exclusion(flags_dict):\n    flag_count = sum(bool(val) for val in flags_dict.values())\n    if flag_count == 1 or (not required and flag_count == 0):\n      return True\n    raise _exceptions.ValidationError(\n        '{} one of ({}) must be True.'.format(\n            'Exactly' if required else 'At most', ', '.join(flag_names)))\n\n  register_multi_flags_validator(\n      flag_names, validate_boolean_mutual_exclusion, flag_values=flag_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister new flags validator to be checked.", "response": "def _add_validator(fv, validator_instance):\n  \"\"\"Register new flags validator to be checked.\n\n  Args:\n    fv: flags.FlagValues, the FlagValues instance to add the validator.\n    validator_instance: validators.Validator, the validator to add.\n  Raises:\n    KeyError: Raised when validators work with a non-existing flag.\n  \"\"\"\n  for flag_name in validator_instance.get_flags_names():\n    fv[flag_name].validators.append(validator_instance)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying that the constraint is satisfied.", "response": "def verify(self, flag_values):\n    \"\"\"Verifies that constraint is satisfied.\n\n    flags library calls this method to verify Validator's constraint.\n\n    Args:\n      flag_values: flags.FlagValues, the FlagValues instance to get flags from.\n    Raises:\n      Error: Raised if constraint is not satisfied.\n    \"\"\"\n    param = self._get_input_to_checker_function(flag_values)\n    if not self.checker(param):\n      raise _exceptions.ValidationError(self.message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of flag values returns the input to be given to checker.", "response": "def _get_input_to_checker_function(self, flag_values):\n    \"\"\"Given flag values, returns the input to be given to checker.\n\n    Args:\n      flag_values: flags.FlagValues, the FlagValues instance to get flags from.\n    Returns:\n      dict, with keys() being self.lag_names, and value for each key\n      being the value of the corresponding flag (string, boolean, etc).\n    \"\"\"\n    return dict([key, flag_values[key].value] for key in self.flag_names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a DuplicateFlagError by providing a flag name and values.", "response": "def from_flag(cls, flagname, flag_values, other_flag_values=None):\n    \"\"\"Creates a DuplicateFlagError by providing flag name and values.\n\n    Args:\n      flagname: str, the name of the flag being redefined.\n      flag_values: FlagValues, the FlagValues instance containing the first\n          definition of flagname.\n      other_flag_values: FlagValues, if it is not None, it should be the\n          FlagValues object where the second definition of flagname occurs.\n          If it is None, we assume that we're being called when attempting\n          to create the flag a second time, and we use the module calling\n          this one as the source of the second definition.\n\n    Returns:\n      An instance of DuplicateFlagError.\n    \"\"\"\n    first_module = flag_values.find_module_defining_flag(\n        flagname, default='<unknown>')\n    if other_flag_values is None:\n      second_module = _helpers.get_calling_module()\n    else:\n      second_module = other_flag_values.find_module_defining_flag(\n          flagname, default='<unknown>')\n    flag_summary = flag_values[flagname].help\n    msg = (\"The flag '%s' is defined twice. First from %s, Second from %s.  \"\n           \"Description from first occurrence: %s\") % (\n               flagname, first_module, second_module, flag_summary)\n    return cls(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_verbosity(v):\n  try:\n    new_level = int(v)\n  except ValueError:\n    new_level = converter.ABSL_NAMES[v.upper()]\n  FLAGS.verbosity = new_level", "response": "Sets the logging verbosity."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_stderrthreshold(s):\n  if s in converter.ABSL_LEVELS:\n    FLAGS.stderrthreshold = converter.ABSL_LEVELS[s]\n  elif isinstance(s, str) and s.upper() in converter.ABSL_NAMES:\n    FLAGS.stderrthreshold = s\n  else:\n    raise ValueError(\n        'set_stderrthreshold only accepts integer absl logging level '\n        'from -3 to 1, or case-insensitive string values '\n        \"'debug', 'info', 'warning', 'error', and 'fatal'. \"\n        'But found \"{}\" ({}).'.format(s, type(s)))", "response": "Sets the stderr threshold to the value passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_every_n(level, msg, n, *args):\n  count = _get_next_log_count_per_token(get_absl_logger().findCaller())\n  log_if(level, msg, not (count % n), *args)", "response": "Logs msg % args at level once per n times."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _seconds_have_elapsed(token, num_seconds):\n  now = timeit.default_timer()\n  then = _log_timer_per_token.get(token, None)\n  if then is None or (now - then) >= num_seconds:\n    _log_timer_per_token[token] = now\n    return True\n  else:\n    return False", "response": "Tests if num_seconds have elapsed since token was requested."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog msg % args at level iff n_seconds elapsed since last call.", "response": "def log_every_n_seconds(level, msg, n_seconds, *args):\n  \"\"\"Logs 'msg % args' at level 'level' iff 'n_seconds' elapsed since last call.\n\n  Logs the first call, logs subsequent calls if 'n' seconds have elapsed since\n  the last logging call from the same call site (file + line). Not thread-safe.\n\n  Args:\n    level: int, the absl logging level at which to log.\n    msg: str, the message to be logged.\n    n_seconds: float or int, seconds which should elapse before logging again.\n    *args: The args to be substitued into the msg.\n  \"\"\"\n  should_log = _seconds_have_elapsed(get_absl_logger().findCaller(), n_seconds)\n  log_if(level, msg, should_log, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging msg % args at level only if condition is fulfilled.", "response": "def log_if(level, msg, condition, *args):\n  \"\"\"Logs 'msg % args' at level 'level' only if condition is fulfilled.\"\"\"\n  if condition:\n    log(level, msg, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog msg % args at absl logging level level.", "response": "def log(level, msg, *args, **kwargs):\n  \"\"\"Logs 'msg % args' at absl logging level 'level'.\n\n  If no args are given just print msg, ignoring any interpolation specifiers.\n\n  Args:\n    level: int, the absl logging level at which to log the message\n        (logging.DEBUG|INFO|WARNING|ERROR|FATAL). While some C++ verbose logging\n        level constants are also supported, callers should prefer explicit\n        logging.vlog() calls for such purpose.\n\n    msg: str, the message to be logged.\n    *args: The args to be substitued into the msg.\n    **kwargs: May contain exc_info to add exception traceback to message.\n  \"\"\"\n  if level > converter.ABSL_DEBUG:\n    # Even though this function supports level that is greater than 1, users\n    # should use logging.vlog instead for such cases.\n    # Treat this as vlog, 1 is equivalent to DEBUG.\n    standard_level = converter.STANDARD_DEBUG - (level - 1)\n  else:\n    if level < converter.ABSL_FATAL:\n      level = converter.ABSL_FATAL\n    standard_level = converter.absl_to_standard(level)\n\n  _absl_logger.log(standard_level, msg, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vlog_is_on(level):\n\n  if level > converter.ABSL_DEBUG:\n    # Even though this function supports level that is greater than 1, users\n    # should use logging.vlog instead for such cases.\n    # Treat this as vlog, 1 is equivalent to DEBUG.\n    standard_level = converter.STANDARD_DEBUG - (level - 1)\n  else:\n    if level < converter.ABSL_FATAL:\n      level = converter.ABSL_FATAL\n    standard_level = converter.absl_to_standard(level)\n  return _absl_logger.isEnabledFor(standard_level)", "response": "Checks if vlog is enabled for the given level in caller s source file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_log_file_name(level=INFO):\n  if level not in converter.ABSL_LEVELS:\n    raise ValueError('Invalid absl.logging level {}'.format(level))\n  stream = get_absl_handler().python_handler.stream\n  if (stream == sys.stderr or stream == sys.stdout or\n      not hasattr(stream, 'name')):\n    return ''\n  else:\n    return stream.name", "response": "Returns the name of the log file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_log_dir_and_names(program_name=None, log_dir=None):\n  if not program_name:\n    # Strip the extension (foobar.par becomes foobar, and\n    # fubar.py becomes fubar). We do this so that the log\n    # file names are similar to C++ log file names.\n    program_name = os.path.splitext(os.path.basename(sys.argv[0]))[0]\n\n    # Prepend py_ to files so that python code gets a unique file, and\n    # so that C++ libraries do not try to write to the same log files as us.\n    program_name = 'py_%s' % program_name\n\n  actual_log_dir = find_log_dir(log_dir=log_dir)\n\n  try:\n    username = getpass.getuser()\n  except KeyError:\n    # This can happen, e.g. when running under docker w/o passwd file.\n    if hasattr(os, 'getuid'):\n      # Windows doesn't have os.getuid\n      username = str(os.getuid())\n    else:\n      username = 'unknown'\n  hostname = socket.gethostname()\n  file_prefix = '%s.%s.%s.log' % (program_name, hostname, username)\n\n  return actual_log_dir, file_prefix, program_name", "response": "Find the log directory and file names for a given program name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_log_dir(log_dir=None):\n  # Get a list of possible log dirs (will try to use them in order).\n  if log_dir:\n    # log_dir was explicitly specified as an arg, so use it and it alone.\n    dirs = [log_dir]\n  elif FLAGS['log_dir'].value:\n    # log_dir flag was provided, so use it and it alone (this mimics the\n    # behavior of the same flag in logging.cc).\n    dirs = [FLAGS['log_dir'].value]\n  else:\n    dirs = ['/tmp/', './']\n\n  # Find the first usable log dir.\n  for d in dirs:\n    if os.path.isdir(d) and os.access(d, os.W_OK):\n      return d\n  _absl_logger.fatal(\"Can't find a writable directory for logs, tried %s\", dirs)", "response": "Returns the most suitable directory to put log files into."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_absl_log_prefix(record):\n  created_tuple = time.localtime(record.created)\n  created_microsecond = int(record.created % 1.0 * 1e6)\n\n  critical_prefix = ''\n  level = record.levelno\n  if _is_non_absl_fatal_record(record):\n    # When the level is FATAL, but not logged from absl, lower the level so\n    # it's treated as ERROR.\n    level = logging.ERROR\n    critical_prefix = _CRITICAL_PREFIX\n  severity = converter.get_initial_for_level(level)\n\n  return '%c%02d%02d %02d:%02d:%02d.%06d %5d %s:%d] %s' % (\n      severity,\n      created_tuple.tm_mon,\n      created_tuple.tm_mday,\n      created_tuple.tm_hour,\n      created_tuple.tm_min,\n      created_tuple.tm_sec,\n      created_microsecond,\n      _get_thread_id(),\n      record.filename,\n      record.lineno,\n      critical_prefix)", "response": "Returns the absl log prefix for the given logging. LogRecord."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nskips reporting the prefix of a given function or name by ABSLLogger.", "response": "def skip_log_prefix(func):\n  \"\"\"Skips reporting the prefix of a given function or name by ABSLLogger.\n\n  This is a convenience wrapper function / decorator for\n  `ABSLLogger.register_frame_to_skip`.\n\n  If a callable function is provided, only that function will be skipped.\n  If a function name is provided, all functions with the same name in the\n  file that this is called in will be skipped.\n\n  This can be used as a decorator of the intended function to be skipped.\n\n  Args:\n    func: Callable function or its name as a string.\n\n  Returns:\n    func (the input, unchanged).\n\n  Raises:\n    ValueError: The input is callable but does not have a function code object.\n    TypeError: The input is neither callable nor a string.\n  \"\"\"\n  if callable(func):\n    func_code = getattr(func, '__code__', None)\n    if func_code is None:\n      raise ValueError('Input callable does not have a function code object.')\n    file_name = func_code.co_filename\n    func_name = func_code.co_name\n    func_lineno = func_code.co_firstlineno\n  elif isinstance(func, six.string_types):\n    file_name = get_absl_logger().findCaller()[0]\n    func_name = func\n    func_lineno = None\n  else:\n    raise TypeError('Input is neither callable nor a string.')\n  ABSLLogger.register_frame_to_skip(file_name, func_name, func_lineno)\n  return func"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef use_absl_handler():\n  absl_handler = get_absl_handler()\n  if absl_handler not in logging.root.handlers:\n    logging.root.addHandler(absl_handler)\n    FLAGS['verbosity']._update_logging_levels()", "response": "Uses the ABSL logging handler for logging if not yet configured."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _initialize():\n  global _absl_logger, _absl_handler\n\n  if _absl_logger:\n    return\n\n  original_logger_class = logging.getLoggerClass()\n  logging.setLoggerClass(ABSLLogger)\n  _absl_logger = logging.getLogger('absl')\n  logging.setLoggerClass(original_logger_class)\n\n  python_logging_formatter = PythonFormatter()\n  _absl_handler = ABSLHandler(python_logging_formatter)\n\n  # The absl handler logs to stderr by default. To prevent double logging to\n  # stderr, the following code tries its best to remove other handlers that emit\n  # to stderr. Those handlers are most commonly added when logging.info/debug is\n  # called before importing this module.\n  handlers = [\n      h for h in logging.root.handlers\n      if isinstance(h, logging.StreamHandler) and h.stream == sys.stderr]\n  for h in handlers:\n    logging.root.removeHandler(h)\n\n  # The absl handler will always be attached to root, not the absl logger.\n  if not logging.root.handlers:\n    # Attach the absl handler at import time when there are no other handlers.\n    # Otherwise it means users have explicitly configured logging, and the absl\n    # handler will only be attached later in app.run(). For App Engine apps,\n    # the absl handler is not used.\n    logging.root.addHandler(_absl_handler)", "response": "Initializes loggers and handlers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_logging_levels(self):\n    if not _absl_logger:\n      return\n\n    if self._value <= converter.ABSL_DEBUG:\n      standard_verbosity = converter.absl_to_standard(self._value)\n    else:\n      # --verbosity is set to higher than 1 for vlog.\n      standard_verbosity = logging.DEBUG - (self._value - 1)\n\n    # Also update root level when absl_handler is used.\n    if _absl_handler in logging.root.handlers:\n      logging.root.setLevel(standard_verbosity)", "response": "Updates the logging levels to the current verbosity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_logging_to_file(self, program_name=None, log_dir=None):\n    FLAGS.logtostderr = False\n\n    actual_log_dir, file_prefix, symlink_prefix = find_log_dir_and_names(\n        program_name=program_name, log_dir=log_dir)\n\n    basename = '%s.INFO.%s.%d' % (\n        file_prefix,\n        time.strftime('%Y%m%d-%H%M%S', time.localtime(time.time())),\n        os.getpid())\n    filename = os.path.join(actual_log_dir, basename)\n\n    if six.PY2:\n      self.stream = open(filename, 'a')\n    else:\n      self.stream = open(filename, 'a', encoding='utf-8')\n\n    # os.symlink is not available on Windows Python 2.\n    if getattr(os, 'symlink', None):\n      # Create a symlink to the log file with a canonical name.\n      symlink = os.path.join(actual_log_dir, symlink_prefix + '.INFO')\n      try:\n        if os.path.islink(symlink):\n          os.unlink(symlink)\n        os.symlink(os.path.basename(filename), symlink)\n      except EnvironmentError:\n        # If it fails, we're sad but it's no error.  Commonly, this\n        # fails because the symlink was created by another user and so\n        # we can't modify it\n        pass", "response": "Starts logging messages to files instead of standard error."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(self):\n    self.acquire()\n    try:\n      self.stream.flush()\n    except (EnvironmentError, ValueError):\n      # A ValueError is thrown if we try to flush a closed file.\n      pass\n    finally:\n      self.release()", "response": "Flushes all log files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _log_to_stderr(self, record):\n    # emit() is protected by a lock in logging.Handler, so we don't need to\n    # protect here again.\n    old_stream = self.stream\n    self.stream = sys.stderr\n    try:\n      super(PythonHandler, self).emit(record)\n    finally:\n      self.stream = old_stream", "response": "Emits the record to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint a record out to some streams.", "response": "def emit(self, record):\n    \"\"\"Prints a record out to some streams.\n\n    If FLAGS.logtostderr is set, it will print to sys.stderr ONLY.\n    If FLAGS.alsologtostderr is set, it will print to sys.stderr.\n    If FLAGS.logtostderr is not set, it will log to the stream\n      associated with the current thread.\n\n    Args:\n      record: logging.LogRecord, the record to emit.\n    \"\"\"\n    # People occasionally call logging functions at import time before\n    # our flags may have even been defined yet, let alone even parsed, as we\n    # rely on the C++ side to define some flags for us and app init to\n    # deal with parsing.  Match the C++ library behavior of notify and emit\n    # such messages to stderr.  It encourages people to clean-up and does\n    # not hide the message.\n    level = record.levelno\n    if not FLAGS.is_parsed():  # Also implies \"before flag has been defined\".\n      global _warn_preinit_stderr\n      if _warn_preinit_stderr:\n        sys.stderr.write(\n            'WARNING: Logging before flag parsing goes to stderr.\\n')\n        _warn_preinit_stderr = False\n      self._log_to_stderr(record)\n    elif FLAGS['logtostderr'].value:\n      self._log_to_stderr(record)\n    else:\n      super(PythonHandler, self).emit(record)\n      stderr_threshold = converter.string_to_standard(\n          FLAGS['stderrthreshold'].value)\n      if ((FLAGS['alsologtostderr'].value or level >= stderr_threshold) and\n          self.stream != sys.stderr):\n        self._log_to_stderr(record)\n    # Die when the record is created from ABSLLogger and level is FATAL.\n    if _is_absl_fatal_record(record):\n      self.flush()  # Flush the log before dying.\n\n      # In threaded python, sys.exit() from a non-main thread only\n      # exits the thread in question.\n      os.abort()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose the stream to which we are writing.", "response": "def close(self):\n    \"\"\"Closes the stream to which we are writing.\"\"\"\n    self.acquire()\n    try:\n      self.flush()\n      try:\n        # Do not close the stream if it's sys.stderr|stdout. They may be\n        # redirected or overridden to files, which should be managed by users\n        # explicitly.\n        if self.stream not in (sys.stderr, sys.stdout) and (\n            not hasattr(self.stream, 'isatty') or not self.stream.isatty()):\n          self.stream.close()\n      except ValueError:\n        # A ValueError is thrown if we try to run isatty() on a closed file.\n        pass\n      super(PythonHandler, self).close()\n    finally:\n      self.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nappends the message from the record to the results of the prefix.", "response": "def format(self, record):\n    \"\"\"Appends the message from the record to the results of the prefix.\n\n    Args:\n      record: logging.LogRecord, the record to be formatted.\n\n    Returns:\n      The formatted string representing the record.\n    \"\"\"\n    if (not FLAGS['showprefixforinfo'].value and\n        FLAGS['verbosity'].value == converter.ABSL_INFO and\n        record.levelno == logging.INFO and\n        _absl_handler.python_handler.stream == sys.stderr):\n      prefix = ''\n    else:\n      prefix = get_absl_log_prefix(record)\n    return prefix + super(PythonFormatter, self).format(record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef findCaller(self, stack_info=False):\n    f_to_skip = ABSLLogger._frames_to_skip\n    # Use sys._getframe(2) instead of logging.currentframe(), it's slightly\n    # faster because there is one less frame to traverse.\n    frame = sys._getframe(2)  # pylint: disable=protected-access\n\n    while frame:\n      code = frame.f_code\n      if (_LOGGING_FILE_PREFIX not in code.co_filename and\n          (code.co_filename, code.co_name,\n           code.co_firstlineno) not in f_to_skip and\n          (code.co_filename, code.co_name) not in f_to_skip):\n        if six.PY2 and not stack_info:\n          return (code.co_filename, frame.f_lineno, code.co_name)\n        else:\n          sinfo = None\n          if stack_info:\n            out = io.StringIO()\n            out.write(u'Stack (most recent call last):\\n')\n            traceback.print_stack(frame, file=out)\n            sinfo = out.getvalue().rstrip(u'\\n')\n          return (code.co_filename, frame.f_lineno, code.co_name, sinfo)\n      frame = frame.f_back", "response": "Finds the frame of the calling method on the stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fatal(self, msg, *args, **kwargs):\n    self.log(logging.FATAL, msg, *args, **kwargs)", "response": "Logs msg % args with severity FATAL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs msg % args with severity WARN.", "response": "def warn(self, msg, *args, **kwargs):\n    \"\"\"Logs 'msg % args' with severity 'WARN'.\"\"\"\n    if six.PY3:\n      warnings.warn(\"The 'warn' method is deprecated, use 'warning' instead\",\n                    DeprecationWarning, 2)\n    self.log(logging.WARN, msg, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log(self, level, msg, *args, **kwargs):\n    if level >= logging.FATAL:\n      # Add property to the LogRecord created by this logger.\n      # This will be used by the ABSLHandler to determine whether it should\n      # treat CRITICAL/FATAL logs as really FATAL.\n      extra = kwargs.setdefault('extra', {})\n      extra[_ABSL_LOG_FATAL] = True\n    super(ABSLLogger, self).log(level, msg, *args, **kwargs)", "response": "Logs a message at a cetain level substituting in the supplied arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_frame_to_skip(cls, file_name, function_name, line_number=None):\n    if line_number is not None:\n      cls._frames_to_skip.add((file_name, function_name, line_number))\n    else:\n      cls._frames_to_skip.add((file_name, function_name))", "response": "Registers a function name to skip when walking the stack."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _strip_undefok_args(undefok, args):\n  if undefok:\n    undefok_names = set(name.strip() for name in undefok.split(','))\n    undefok_names |= set('no' + name for name in undefok_names)\n    # Remove undefok flags.\n    args = [arg for arg in args if not _is_undefok(arg, undefok_names)]\n  return args", "response": "Returns a new list of args after removing flags in undefok."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_undefok(arg, undefok_names):\n  if not arg.startswith('-'):\n    return False\n  if arg.startswith('--'):\n    arg_without_dash = arg[2:]\n  else:\n    arg_without_dash = arg[1:]\n  if '=' in arg_without_dash:\n    name, _ = arg_without_dash.split('=', 1)\n  else:\n    name = arg_without_dash\n  if name in undefok_names:\n    return True\n  return False", "response": "Returns whether we can ignore arg based on a set of undefok flag names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _define_absl_flags(self, absl_flags):\n    key_flags = set(absl_flags.get_key_flags_for_module(sys.argv[0]))\n    for name in absl_flags:\n      if name in _BUILT_IN_FLAGS:\n        # Do not inherit built-in flags.\n        continue\n      flag_instance = absl_flags[name]\n      # Each flags with short_name appears in FLAGS twice, so only define\n      # when the dictionary key is equal to the regular name.\n      if name == flag_instance.name:\n        # Suppress the flag in the help short message if it's not a main\n        # module's key flag.\n        suppress = flag_instance not in key_flags\n        self._define_absl_flag(flag_instance, suppress)", "response": "Defines flags from absl_flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _define_absl_flag(self, flag_instance, suppress):\n    flag_name = flag_instance.name\n    short_name = flag_instance.short_name\n    argument_names = ['--' + flag_name]\n    if short_name:\n      argument_names.insert(0, '-' + short_name)\n    if suppress:\n      helptext = argparse.SUPPRESS\n    else:\n      # argparse help string uses %-formatting. Escape the literal %'s.\n      helptext = flag_instance.help.replace('%', '%%')\n    if flag_instance.boolean:\n      # Only add the `no` form to the long name.\n      argument_names.append('--no' + flag_name)\n      self.add_argument(\n          *argument_names, action=_BooleanFlagAction, help=helptext,\n          metavar=flag_instance.name.upper(),\n          flag_instance=flag_instance)\n    else:\n      self.add_argument(\n          *argument_names, action=_FlagAction, help=helptext,\n          metavar=flag_instance.name.upper(),\n          flag_instance=flag_instance)", "response": "Defines a flag from the flag_instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the module that defines a global environment and its name.", "response": "def get_module_object_and_name(globals_dict):\n  \"\"\"Returns the module that defines a global environment, and its name.\n\n  Args:\n    globals_dict: A dictionary that should correspond to an environment\n      providing the values of the globals.\n\n  Returns:\n    _ModuleObjectAndName - pair of module object & module name.\n    Returns (None, None) if the module could not be identified.\n  \"\"\"\n  name = globals_dict.get('__name__', None)\n  module = sys.modules.get(name, None)\n  # Pick a more informative name for the main module.\n  return _ModuleObjectAndName(module,\n                              (sys.argv[0] if name == '__main__' else name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_calling_module_object_and_name():\n  for depth in range(1, sys.getrecursionlimit()):\n    # sys._getframe is the right thing to use here, as it's the best\n    # way to walk up the call stack.\n    globals_for_frame = sys._getframe(depth).f_globals  # pylint: disable=protected-access\n    module, module_name = get_module_object_and_name(globals_for_frame)\n    if id(module) not in disclaim_module_ids and module_name is not None:\n      return _ModuleObjectAndName(module, module_name)\n  raise AssertionError('No module was found')", "response": "Returns the module that s calling into this module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an XML DOM element with name and text value.", "response": "def create_xml_dom_element(doc, name, value):\n  \"\"\"Returns an XML DOM element with name and text value.\n\n  Args:\n    doc: minidom.Document, the DOM document it should create nodes from.\n    name: str, the tag of XML element.\n    value: object, whose string representation will be used\n        as the value of the XML element. Illegal or highly discouraged xml 1.0\n        characters are stripped.\n\n  Returns:\n    An instance of minidom.Element.\n  \"\"\"\n  s = str_or_unicode(value)\n  if six.PY2 and not isinstance(s, unicode):\n    # Get a valid unicode string.\n    s = s.decode('utf-8', 'ignore')\n  if isinstance(value, bool):\n    # Display boolean values as the C++ flag library does: no caps.\n    s = s.lower()\n  # Remove illegal xml characters.\n  s = _ILLEGAL_XML_CHARS_REGEX.sub(u'', s)\n\n  e = doc.createElement(name)\n  e.appendChild(doc.createTextNode(s))\n  return e"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_help_width():\n  if not sys.stdout.isatty() or termios is None or fcntl is None:\n    return _DEFAULT_HELP_WIDTH\n  try:\n    data = fcntl.ioctl(sys.stdout, termios.TIOCGWINSZ, '1234')\n    columns = struct.unpack('hh', data)[1]\n    # Emacs mode returns 0.\n    # Here we assume that any value below 40 is unreasonable.\n    if columns >= _MIN_HELP_WIDTH:\n      return columns\n    # Returning an int as default is fine, int(int) just return the int.\n    return int(os.getenv('COLUMNS', _DEFAULT_HELP_WIDTH))\n\n  except (TypeError, IOError, struct.error):\n    return _DEFAULT_HELP_WIDTH", "response": "Returns the integer width of help lines that is used in TextWrap."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_flag_suggestions(attempt, longopt_list):\n  # Don't suggest on very short strings, or if no longopts are specified.\n  if len(attempt) <= 2 or not longopt_list:\n    return []\n\n  option_names = [v.split('=')[0] for v in longopt_list]\n\n  # Find close approximations in flag prefixes.\n  # This also handles the case where the flag is spelled right but ambiguous.\n  distances = [(_damerau_levenshtein(attempt, option[0:len(attempt)]), option)\n               for option in option_names]\n  # t[0] is distance, and sorting by t[1] allows us to have stable output.\n  distances.sort()\n\n  least_errors, _ = distances[0]\n  # Don't suggest excessively bad matches.\n  if least_errors >= _SUGGESTION_ERROR_RATE_THRESHOLD * len(attempt):\n    return []\n\n  suggestions = []\n  for errors, name in distances:\n    if errors == least_errors:\n      suggestions.append(name)\n    else:\n      break\n  return suggestions", "response": "Returns helpful similar matches for an invalid flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the Damerau - Levenshtein edit distance from a to b.", "response": "def _damerau_levenshtein(a, b):\n  \"\"\"Returns Damerau-Levenshtein edit distance from a to b.\"\"\"\n  memo = {}\n\n  def distance(x, y):\n    \"\"\"Recursively defined string distance with memoization.\"\"\"\n    if (x, y) in memo:\n      return memo[x, y]\n    if not x:\n      d = len(y)\n    elif not y:\n      d = len(x)\n    else:\n      d = min(\n          distance(x[1:], y) + 1,  # correct an insertion error\n          distance(x, y[1:]) + 1,  # correct a deletion error\n          distance(x[1:], y[1:]) + (x[0] != y[0]))  # correct a wrong character\n      if len(x) >= 2 and len(y) >= 2 and x[0] == y[1] and x[1] == y[0]:\n        # Correct a transposition.\n        t = distance(x[2:], y[2:]) + 1\n        if d > t:\n          d = t\n\n    memo[x, y] = d\n    return d\n  return distance(a, b)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef text_wrap(text, length=None, indent='', firstline_indent=None):\n  # Get defaults where callee used None\n  if length is None:\n    length = get_help_width()\n  if indent is None:\n    indent = ''\n  if firstline_indent is None:\n    firstline_indent = indent\n\n  if len(indent) >= length:\n    raise ValueError('Length of indent exceeds length')\n  if len(firstline_indent) >= length:\n    raise ValueError('Length of first line indent exceeds length')\n\n  text = text.expandtabs(4)\n\n  result = []\n  # Create one wrapper for the first paragraph and one for subsequent\n  # paragraphs that does not have the initial wrapping.\n  wrapper = textwrap.TextWrapper(\n      width=length, initial_indent=firstline_indent, subsequent_indent=indent)\n  subsequent_wrapper = textwrap.TextWrapper(\n      width=length, initial_indent=indent, subsequent_indent=indent)\n\n  # textwrap does not have any special treatment for newlines. From the docs:\n  # \"...newlines may appear in the middle of a line and cause strange output.\n  # For this reason, text should be split into paragraphs (using\n  # str.splitlines() or similar) which are wrapped separately.\"\n  for paragraph in (p.strip() for p in text.splitlines()):\n    if paragraph:\n      result.extend(wrapper.wrap(paragraph))\n    else:\n      result.append('')  # Keep empty lines.\n    # Replace initial wrapper with wrapper for subsequent paragraphs.\n    wrapper = subsequent_wrapper\n\n  return '\\n'.join(result)", "response": "Wraps a given text to a maximum line length and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim_docstring(docstring):\n  if not docstring:\n    return ''\n\n  # If you've got a line longer than this you have other problems...\n  max_indent = 1 << 29\n\n  # Convert tabs to spaces (following the normal Python rules)\n  # and split into a list of lines:\n  lines = docstring.expandtabs().splitlines()\n\n  # Determine minimum indentation (first line doesn't count):\n  indent = max_indent\n  for line in lines[1:]:\n    stripped = line.lstrip()\n    if stripped:\n      indent = min(indent, len(line) - len(stripped))\n  # Remove indentation (first line is special):\n  trimmed = [lines[0].strip()]\n  if indent < max_indent:\n    for line in lines[1:]:\n      trimmed.append(line[indent:].rstrip())\n  # Strip off trailing and leading blank lines:\n  while trimmed and not trimmed[-1]:\n    trimmed.pop()\n  while trimmed and not trimmed[0]:\n    trimmed.pop(0)\n  # Return a single string:\n  return '\\n'.join(trimmed)", "response": "Removes indentation from triple - quoted strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a __doc__ string and reformats it as help.", "response": "def doc_to_help(doc):\n  \"\"\"Takes a __doc__ string and reformats it as help.\"\"\"\n\n  # Get rid of starting and ending white space. Using lstrip() or even\n  # strip() could drop more than maximum of first line and right space\n  # of last line.\n  doc = doc.strip()\n\n  # Get rid of all empty lines.\n  whitespace_only_line = re.compile('^[ \\t]+$', re.M)\n  doc = whitespace_only_line.sub('', doc)\n\n  # Cut out common space at line beginnings.\n  doc = trim_docstring(doc)\n\n  # Just like this module's comment, comments tend to be aligned somehow.\n  # In other words they all start with the same amount of white space.\n  # 1) keep double new lines;\n  # 2) keep ws after new lines if not empty line;\n  # 3) all other new lines shall be changed to a space;\n  # Solution: Match new lines between non white space and replace with space.\n  doc = re.sub(r'(?<=\\S)\\n(?=\\S)', ' ', doc, flags=re.M)\n\n  return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvokes the dispatcher to handle the request envelope and return a response envelope.", "response": "def invoke(self, request_envelope, context):\n        # type: (RequestEnvelope, Any) -> ResponseEnvelope\n        \"\"\"Invokes the dispatcher, to handle the request envelope and\n        return a response envelope.\n\n        :param request_envelope: Request Envelope instance containing\n            request information\n        :type request_envelope: RequestEnvelope\n        :param context: Context passed during invocation\n        :type context: Any\n        :return: Response Envelope generated by handling the request\n        :rtype: ResponseEnvelope\n        \"\"\"\n        if (self.skill_id is not None and\n                request_envelope.context.system.application.application_id !=\n                self.skill_id):\n            raise AskSdkException(\"Skill ID Verification failed!!\")\n\n        if self.api_client is not None:\n            api_token = request_envelope.context.system.api_access_token\n            api_endpoint = request_envelope.context.system.api_endpoint\n            api_configuration = ApiConfiguration(\n                serializer=self.serializer, api_client=self.api_client,\n                authorization_value=api_token,\n                api_endpoint=api_endpoint)\n            factory = ServiceClientFactory(api_configuration=api_configuration)\n        else:\n            factory = None\n\n        attributes_manager = AttributesManager(\n            request_envelope=request_envelope,\n            persistence_adapter=self.persistence_adapter)\n\n        handler_input = HandlerInput(\n            request_envelope=request_envelope,\n            attributes_manager=attributes_manager,\n            context=context,\n            service_client_factory=factory)\n\n        response = self.request_dispatcher.dispatch(\n            handler_input=handler_input)\n        session_attributes = None\n\n        if request_envelope.session is not None:\n            session_attributes = (\n                handler_input.attributes_manager.session_attributes)\n\n        return ResponseEnvelope(\n            response=response, version=RESPONSE_FORMAT_VERSION,\n            session_attributes=session_attributes,\n            user_agent=user_agent_info(\n                sdk_version=__version__,\n                custom_user_agent=self.custom_user_agent))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dispatch(self, request, *args, **kwargs):\n        # type: (HttpRequest, object, object) -> HttpResponse\n        \"\"\"Inspect the HTTP method and delegate to the view method.\n\n        This is the default implementation of the\n        :py:class:`django.views.View` method, which will inspect the\n        HTTP method in the input request and delegate it to the\n        corresponding method in the view. The only allowed method on\n        this view is ``post``.\n\n        :param request: The input request sent to the view\n        :type request: django.http.HttpRequest\n        :return: The response from the view\n        :rtype: django.http.HttpResponse\n        :raises: :py:class:`django.http.HttpResponseNotAllowed` if the\n            method is invoked for other than HTTP POST request.\n            :py:class:`django.http.HttpResponseBadRequest` if the\n            request verification fails.\n            :py:class:`django.http.HttpResponseServerError` for any\n            internal exception.\n        \"\"\"\n        return super(SkillAdapter, self).dispatch(request)", "response": "Inspect the HTTP method and delegate to the view method."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets viewport orientation from given width and height.", "response": "def get_orientation(width, height):\n    # type: (int, int) -> Orientation\n    \"\"\"Get viewport orientation from given width and height.\n\n    :type width: int\n    :type height: int\n    :return: viewport orientation enum\n    :rtype: Orientation\n    \"\"\"\n    if width > height:\n        return Orientation.LANDSCAPE\n    elif width < height:\n        return Orientation.PORTRAIT\n    else:\n        return Orientation.EQUAL"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_size(size):\n    # type: (int) -> Size\n    \"\"\"Get viewport size from given size.\n\n    :type size: int\n    :return: viewport size enum\n    :rtype: Size\n    \"\"\"\n    if size in range(0, 600):\n        return Size.XSMALL\n    elif size in range(600, 960):\n        return Size.SMALL\n    elif size in range(960, 1280):\n        return Size.MEDIUM\n    elif size in range(1280, 1920):\n        return Size.LARGE\n    elif size >= 1920:\n        return Size.XLARGE\n\n    raise AskSdkException(\"Unknown size group value: {}\".format(size))", "response": "Get viewport size from given size."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets viewport density group from given dpi.", "response": "def get_dpi_group(dpi):\n    # type: (int) -> Density\n    \"\"\"Get viewport density group from given dpi.\n\n    :type dpi: int\n    :return: viewport density group enum\n    :rtype: Density\n    \"\"\"\n    if dpi in range(0, 121):\n        return Density.XLOW\n    elif dpi in range(121, 161):\n        return Density.LOW\n    elif dpi in range(161, 241):\n        return Density.MEDIUM\n    elif dpi in range(241, 321):\n        return Density.HIGH\n    elif dpi in range(321, 481):\n        return Density.XHIGH\n    elif dpi >= 481:\n        return Density.XXHIGH\n\n    raise AskSdkException(\"Unknown dpi group value: {}\".format(dpi))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the skill configuration object using the registered components.", "response": "def skill_configuration(self):\n        # type: () -> SkillConfiguration\n        \"\"\"Create the skill configuration object using the registered\n        components.\n        \"\"\"\n        skill_config = super(StandardSkillBuilder, self).skill_configuration\n        skill_config.api_client = DefaultApiClient()\n\n        if self.table_name is not None:\n            kwargs = {\"table_name\": self.table_name}  # type: Dict[str, Any]\n            if self.auto_create_table:\n                kwargs[\"create_table\"] = self.auto_create_table\n\n            if self.partition_keygen:\n                kwargs[\"partition_keygen\"] = self.partition_keygen\n\n            if self.dynamodb_client:\n                kwargs[\"dynamodb_resource\"] = self.dynamodb_client\n\n            skill_config.persistence_adapter = DynamoDbAdapter(**kwargs)\n        return skill_config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_handler(self, can_handle_func):\n        # type: (Callable[[Input], bool]) -> Callable\n        \"\"\"Decorator that can be used to add request handlers easily to\n        the builder.\n\n        The can_handle_func has to be a Callable instance, which takes\n        a single parameter and no varargs or kwargs. This is because\n        of the RequestHandler class signature restrictions. The\n        returned wrapper function can be applied as a decorator on any\n        function that returns a response object by the skill. The\n        function should follow the signature of the handle function in\n        :py:class:`ask_sdk_runtime.dispatch_components.request_components.AbstractRequestHandler`\n        class.\n\n        :param can_handle_func: The function that validates if the\n            request can be handled.\n        :type can_handle_func: Callable[[Input], bool]\n        :return: Wrapper function that can be decorated on a handle\n            function.\n        \"\"\"\n        def wrapper(handle_func):\n            if not callable(can_handle_func) or not callable(handle_func):\n                raise SkillBuilderException(\n                    \"Request Handler can_handle_func and handle_func \"\n                    \"input parameters should be callable\")\n\n            class_attributes = {\n                \"can_handle\": lambda self, handler_input: can_handle_func(\n                    handler_input),\n                \"handle\": lambda self, handler_input: handle_func(\n                    handler_input)\n            }\n\n            request_handler_class = type(\n                \"RequestHandler{}\".format(\n                    handle_func.__name__.title().replace(\"_\", \"\")),\n                (AbstractRequestHandler,), class_attributes)\n\n            self.add_request_handler(request_handler=request_handler_class())\n        return wrapper", "response": "Decorator that can be used to add request handlers easily to the skill."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef global_response_interceptor(self):\n        # type: () -> Callable\n        \"\"\"Decorator that can be used to add global\n        response interceptors easily to the builder.\n\n        The returned wrapper function can be applied as a decorator\n        on any function that processes the input and the response\n        generated by the request handler. The function should follow\n        the signature of the process function in\n        :py:class:`ask_sdk_runtime.dispatch_components.request_components.AbstractResponseInterceptor`\n        class.\n\n        :return: Wrapper function that can be decorated on a\n            interceptor process function.\n        \"\"\"\n        def wrapper(process_func):\n            if not callable(process_func):\n                raise SkillBuilderException(\n                    \"Global Response Interceptor process_func input \"\n                    \"parameter should be callable\")\n\n            class_attributes = {\n                \"process\": (\n                    lambda self, handler_input, response: process_func(\n                        handler_input, response))\n            }\n\n            response_interceptor = type(\n                \"ResponseInterceptor{}\".format(\n                    process_func.__name__.title().replace(\"_\", \"\")),\n                (AbstractResponseInterceptor,), class_attributes)\n\n            self.add_global_response_interceptor(\n                response_interceptor=response_interceptor())\n        return wrapper", "response": "Decorator that can be used to add global response interceptors easily to the builder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_intent_name(handler_input):\n    # type: (HandlerInput) -> AnyStr\n    \"\"\"Return the name of the intent request.\n\n    The method retrieves the intent ``name`` from the input request, only if\n    the input request is an\n    :py:class:`ask_sdk_model.intent_request.IntentRequest`. If the input\n    is not an IntentRequest, a :py:class:`TypeError` is raised.\n\n    :param handler_input: The handler input instance that is generally\n        passed in the sdk's request and exception components\n    :type handler_input: ask_sdk_core.handler_input.HandlerInput\n    :return: Name of the intent request\n    :rtype: str\n    :raises: TypeError\n    \"\"\"\n    request = handler_input.request_envelope.request\n    if isinstance(request, IntentRequest):\n        return request.intent.name\n\n    raise TypeError(\"The provided request is not an IntentRequest\")", "response": "Returns the name of the intent request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_device_id(handler_input):\n    # type: (HandlerInput) -> Optional[AnyStr]\n    \"\"\"Return the device id from the input request.\n\n    The method retrieves the `deviceId` property from the input request.\n    This value uniquely identifies the device and is generally used as\n    input for some Alexa-specific API calls. More information about this\n    can be found here :\n    https://developer.amazon.com/docs/custom-skills/request-and-response-json-reference.html#system-object\n\n    If there is no device information in the input request, then a ``None``\n    is returned.\n\n    :param handler_input: The handler input instance that is generally\n        passed in the sdk's request and exception components\n    :type handler_input: ask_sdk_core.handler_input.HandlerInput\n    :return: Unique device id of the device used to send the alexa request\n        or `None` if device information is not present\n    :rtype: Optional[str]\n    \"\"\"\n    device = handler_input.request_envelope.context.system.device\n    if device:\n        return device.device_id\n    else:\n        return None", "response": "Returns the unique device id from the input request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_dialog_state(handler_input):\n    # type: (HandlerInput) -> Optional[DialogState]\n    \"\"\"Return the dialog state enum from the intent request.\n\n    The method retrieves the `dialogState` from the intent request, if\n    the skill's interaction model includes a dialog model. This can be\n    used to determine the current status of user conversation and return\n    the appropriate dialog directives if the conversation is not yet complete.\n    More information on dialog management can be found here :\n    https://developer.amazon.com/docs/custom-skills/define-the-dialog-to-collect-and-confirm-required-information.html\n\n    The method returns a ``None`` if there is no dialog model added or\n    if the intent doesn't have dialog management. The method raises a\n    :py:class:`TypeError` if the input is not an `IntentRequest`.\n\n    :param handler_input: The handler input instance that is generally\n        passed in the sdk's request and exception components.\n    :type handler_input: ask_sdk_core.handler_input.HandlerInput\n    :return: State of the dialog model from the intent request.\n    :rtype: Optional[ask_sdk_model.dialog_state.DialogState]\n    :raises: TypeError if the input is not an IntentRequest\n    \"\"\"\n    request = handler_input.request_envelope.request\n    if isinstance(request, IntentRequest):\n        return request.dialog_state\n\n    raise TypeError(\"The provided request is not an IntentRequest\")", "response": "Returns the dialog state enum from the intent request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the slot information from intent request.", "response": "def get_slot(handler_input, slot_name):\n    # type: (HandlerInput, str) -> Optional[Slot]\n    \"\"\"Return the slot information from intent request.\n\n    The method retrieves the slot information\n    :py:class:`ask_sdk_model.slot.Slot` from the input intent request\n    for the given ``slot_name``. More information on the slots can be\n    found here :\n    https://developer.amazon.com/docs/custom-skills/request-types-reference.html#slot-object\n\n    If there is no such slot, then a ``None``\n    is returned. If the input request is not an\n    :py:class:`ask_sdk_model.intent_request.IntentRequest`, a\n    :py:class:`TypeError` is raised.\n\n    :param handler_input: The handler input instance that is generally\n        passed in the sdk's request and exception components\n    :type handler_input: ask_sdk_core.handler_input.HandlerInput\n    :param slot_name: Name of the slot that needs to be retrieved\n    :type slot_name: str\n    :return: Slot information for the provided slot name if it exists,\n        or a `None` value\n    :rtype: Optional[ask_sdk_model.slot.Slot]\n    :raises: TypeError if the input is not an IntentRequest\n    \"\"\"\n    request = handler_input.request_envelope.request\n    if isinstance(request, IntentRequest):\n        if request.intent.slots is not None:\n            return request.intent.slots.get(slot_name, None)\n        else:\n            return None\n\n    raise TypeError(\"The provided request is not an IntentRequest\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value of the slot with the given name from the input intent request.", "response": "def get_slot_value(handler_input, slot_name):\n    # type: (HandlerInput, str) -> AnyStr\n    \"\"\"Return the slot value from intent request.\n\n    The method retrieves the slot value from the input intent request\n    for the given ``slot_name``. More information on the slots can be\n    found here :\n    https://developer.amazon.com/docs/custom-skills/request-types-reference.html#slot-object\n\n    If there is no such slot, then a :py:class:`ValueError` is raised.\n    If the input request is not an\n    :py:class:`ask_sdk_model.intent_request.IntentRequest`, a\n    :py:class:`TypeError` is raised.\n\n    :param handler_input: The handler input instance that is generally\n        passed in the sdk's request and exception components\n    :type handler_input: ask_sdk_core.handler_input.HandlerInput\n    :param slot_name: Name of the slot for which the value has to be retrieved\n    :type slot_name: str\n    :return: Slot value for the provided slot if it exists\n    :rtype: str\n    :raises: TypeError if the input is not an IntentRequest. ValueError is\n        slot doesn't exist\n    \"\"\"\n    slot = get_slot(handler_input=handler_input, slot_name=slot_name)\n\n    if slot is not None:\n        return slot.value\n\n    raise ValueError(\n        \"Provided slot {} doesn't exist in the input request\".format(\n            slot_name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning if the session is new for the input request.", "response": "def is_new_session(handler_input):\n    # type: (HandlerInput) -> bool\n    \"\"\"Return if the session is new for the input request.\n\n    The method retrieves the ``new`` value from the input request's\n    session, which indicates if it's a new session or not. The\n    :py:class:`ask_sdk_model.session.Session` is only included on all\n    standard requests except ``AudioPlayer``, ``VideoApp`` and\n    ``PlaybackController`` requests. More information can be found here :\n    https://developer.amazon.com/docs/custom-skills/request-and-response-json-reference.html#session-object\n\n    A :py:class:`TypeError` is raised if the input request doesn't have\n    the ``session`` information.\n\n    :param handler_input: The handler input instance that is generally\n        passed in the sdk's request and exception components\n    :type handler_input: ask_sdk_core.handler_input.HandlerInput\n    :return: Boolean if the session is new for the input request\n    :rtype: bool\n    :raises: TypeError if the input request doesn't have a session\n    \"\"\"\n    session = handler_input.request_envelope.session\n\n    if session is not None:\n        return session.new\n\n    raise TypeError(\"The provided request doesn't have a session\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_plain_text_content(primary_text=None, secondary_text=None, tertiary_text=None):\n    # type: (str, str, str) -> TextContent\n    \"\"\"Responsible for building plain text content object using\n    ask-sdk-model in Alexa skills kit display interface.\n    https://developer.amazon.com/docs/custom-skills/display-interface-reference.html#textcontent-object-specifications.\n\n    :param primary_text: Text for primary_text field\n    :type primary_text: (optional) str\n    :param secondary_text: Text for secondary_text field\n    :type secondary_text: (optional) str\n    :param tertiary_text: Text for tertiary_text field\n    :type tertiary_text: (optional) str\n    :return: Text Content instance with primary, secondary and tertiary\n        text set as Plain Text objects.\n    :rtype: TextContent\n    :raises: ValueError\n    \"\"\"\n    return get_text_content(\n        primary_text=primary_text, primary_text_type=PLAIN_TEXT_TYPE,\n        secondary_text=secondary_text, secondary_text_type=PLAIN_TEXT_TYPE,\n        tertiary_text=tertiary_text, tertiary_text_type=PLAIN_TEXT_TYPE)", "response": "Responsible for building a plain text content object using the custom skills kit display interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_rich_text_content(primary_text=None, secondary_text=None, tertiary_text=None):\n    # type: (str, str, str) -> TextContent\n    \"\"\"Responsible for building plain text content object using\n    ask-sdk-model in Alexa skills kit display interface.\n    https://developer.amazon.com/docs/custom-skills/display-interface-reference.html#textcontent-object-specifications.\n\n    :param primary_text: Text for primary_text field\n    :type primary_text: (optional) str\n    :param secondary_text: Text for secondary_text field\n    :type secondary_text: (optional) str\n    :param tertiary_text: Text for tertiary_text field\n    :type tertiary_text: (optional) str\n    :return: Text Content instance with primary, secondary and tertiary\n        text set as Rich Text objects.\n    :rtype: TextContent\n    :raises: ValueError\n    \"\"\"\n    return get_text_content(\n        primary_text=primary_text, primary_text_type=RICH_TEXT_TYPE,\n        secondary_text=secondary_text, secondary_text_type=RICH_TEXT_TYPE,\n        tertiary_text=tertiary_text, tertiary_text_type=RICH_TEXT_TYPE)", "response": "Responsible for building plain text content object using a random text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new object with the specified primary secondary and tertiary text fields set.", "response": "def get_text_content(\n        primary_text=None, primary_text_type=PLAIN_TEXT_TYPE,\n        secondary_text=None, secondary_text_type=PLAIN_TEXT_TYPE,\n        tertiary_text=None, tertiary_text_type=PLAIN_TEXT_TYPE):\n    # type: (str, str, str, str, str, str) -> TextContent\n    \"\"\"Responsible for building text content object using ask-sdk-model\n    in Alexa skills kit display interface.\n    https://developer.amazon.com/docs/custom-skills/display-interface-reference.html#textcontent-object-specifications.\n\n    :param primary_text: Text for primary_text field\n    :type primary_text: (optional) str\n    :param primary_text_type: Type of the primary text field. Allowed\n        values are `PlainText` and `RichText`.\n        Defaulted to `PlainText`.\n    :param secondary_text: Text for secondary_text field\n    :type primary_text_type: (optional) str\n    :type secondary_text: (optional) str\n    :param secondary_text_type: Type of the secondary text field.\n        Allowed values are `PlainText` and `RichText`.\n        Defaulted to `PlainText`.\n    :param tertiary_text: Text for tertiary_text field\n    :type tertiary_text: (optional) str\n    :param tertiary_text_type: Type of the tertiary text field.\n        Allowed values are `PlainText` and `RichText`.\n        Defaulted to `PlainText`.\n    :return: Text Content instance with primary, secondary and tertiary\n        text set.\n    :rtype: TextContent\n    :raises: ValueError\n    \"\"\"\n    text_content = TextContent()\n    if primary_text:\n        text_content.primary_text = __set_text_field(\n            primary_text, primary_text_type)\n    if secondary_text:\n        text_content.secondary_text = __set_text_field(\n            secondary_text, secondary_text_type)\n    if tertiary_text:\n        text_content.tertiary_text = __set_text_field(\n            tertiary_text, tertiary_text_type)\n    return text_content"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef speak(self, speech, play_behavior=None):\n        # type: (str, PlayBehavior) -> 'ResponseFactory'\n        \"\"\"Say the provided speech to the user.\n\n        :param speech: the output speech sent back to the user.\n        :type speech: str\n        :param play_behavior: attribute to control alexa's speech\n            interruption\n        :type play_behavior: ask_sdk_model.ui.play_behavior.PlayBehavior\n        :return: response factory with partial response being built and\n            access from self.response.\n        :rtype: ResponseFactory\n        \"\"\"\n        ssml = \"<speak>{}</speak>\".format(self.__trim_outputspeech(\n            speech_output=speech))\n        self.response.output_speech = SsmlOutputSpeech(\n            ssml=ssml, play_behavior=play_behavior)\n        return self", "response": "Say the provided speech to the user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide reprompt speech to the user and optionally end session.", "response": "def ask(self, reprompt, play_behavior=None):\n        # type: (str, PlayBehavior) -> 'ResponseFactory'\n        \"\"\"Provide reprompt speech to the user, if no response for\n        8 seconds.\n\n        The should_end_session value will be set to false except when\n        the video app launch directive is present in directives.\n\n        :param reprompt: the output speech to reprompt.\n        :type reprompt: str\n        :param play_behavior: attribute to control alexa's speech\n            interruption\n        :type play_behavior: ask_sdk_model.ui.play_behavior.PlayBehavior\n        :return: response factory with partial response being built and\n            access from self.response.\n        :rtype: ResponseFactory\n        \"\"\"\n        ssml = \"<speak>{}</speak>\".format(self.__trim_outputspeech(\n            speech_output=reprompt))\n        output_speech = SsmlOutputSpeech(\n            ssml=ssml, play_behavior=play_behavior)\n        self.response.reprompt = Reprompt(output_speech=output_speech)\n        if not self.__is_video_app_launch_directive_present():\n            self.response.should_end_session = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a directive to response.", "response": "def add_directive(self, directive):\n        # type: (Directive) -> 'ResponseFactory'\n        \"\"\"Adds directive to response.\n\n        :param directive: the directive sent back to Alexa device.\n        :type directive: ask_sdk_model.directive.Directive\n        :return: response factory with partial response being built and\n            access from self.response.\n        :rtype: ResponseFactory\n        \"\"\"\n        if self.response.directives is None:\n            self.response.directives = []\n\n        if (directive is not None and\n                directive.object_type == \"VideoApp.Launch\"):\n            self.response.should_end_session = None\n        self.response.directives.append(directive)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the response should_end_session property of the response factory.", "response": "def set_should_end_session(self, should_end_session):\n        # type: (bool) -> 'ResponseFactory'\n        \"\"\"Sets shouldEndSession value to null/false/true.\n\n        :param should_end_session: value to show if the session should\n            be ended or not.\n        :type should_end_session: bool\n        :return: response factory with partial response being built and\n            access from self.response.\n        :rtype: ResponseFactory\n        \"\"\"\n        if not self.__is_video_app_launch_directive_present():\n            self.response.should_end_session = should_end_session\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrimming the output speech if it already has the <speak></speak > tag.", "response": "def __trim_outputspeech(self, speech_output=None):\n        # type: (Union[str, None]) -> str\n        \"\"\"Trims the output speech if it already has the\n        <speak></speak> tag.\n\n        :param speech_output: the output speech sent back to user.\n        :type speech_output: str\n        :return: the trimmed output speech.\n        :rtype: Union[bool, None]\n        \"\"\"\n        if speech_output is None:\n            return \"\"\n        speech = speech_output.strip()\n        if speech.startswith(\"<speak>\") and speech.endswith(\"</speak>\"):\n            return speech[7:-8].strip()\n        return speech"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __is_video_app_launch_directive_present(self):\n        # type: () -> bool\n        \"\"\"Checks if the video app launch directive is present or not.\n\n        :return: boolean to show if video app launch directive is\n            present or not.\n        :rtype: bool\n        \"\"\"\n        if self.response.directives is None:\n            return False\n\n        for directive in self.response.directives:\n            if (directive is not None and\n                    directive.object_type == \"VideoApp.Launch\"):\n                return True\n        return False", "response": "Checks if the video app launch directive is present or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dispatch(self, handler_input):\n        # type: (Input) -> Union[Output, None]\n        \"\"\"Dispatches an incoming request to the appropriate\n        request handler and returns the output.\n\n        Before running the request on the appropriate request handler,\n        dispatcher runs any predefined global request interceptors.\n        On successful response returned from request handler, dispatcher\n        runs predefined global response interceptors, before returning\n        the response.\n\n        :param handler_input: generic input to the dispatcher\n        :type handler_input: Input\n        :return: generic output handled by the handler, optionally\n            containing a response\n        :rtype: Union[None, Output]\n        :raises: :py:class:`ask_sdk_runtime.exceptions.DispatchException`\n        \"\"\"\n        try:\n            for request_interceptor in self.request_interceptors:\n                request_interceptor.process(handler_input=handler_input)\n\n            output = self.__dispatch_request(handler_input)  # type: Union[Output, None]\n\n            for response_interceptor in self.response_interceptors:\n                response_interceptor.process(\n                    handler_input=handler_input, dispatch_output=output)\n\n            return output\n        except Exception as e:\n            if self.exception_mapper is not None:\n                exception_handler = self.exception_mapper.get_handler(\n                    handler_input, e)\n                if exception_handler is None:\n                    raise e\n                return exception_handler.handle(handler_input, e)\n            else:\n                raise e", "response": "Dispatches an incoming request to the appropriate request handler and returns the output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __dispatch_request(self, handler_input):\n        # type: (Input) -> Union[Output, None]\n        \"\"\"Process the request and return handler output.\n\n        When the method is invoked, using the registered list of\n        :py:class:`RequestMapper`, a Handler Chain is found that can\n        handle the request. The handler invocation is delegated to the\n        supported :py:class:`HandlerAdapter`. The registered\n        request interceptors in the handler chain are processed before\n        executing the handler. The registered response interceptors in\n        the handler chain are processed after executing the handler.\n\n        :param handler_input: generic input to the dispatcher containing\n            incoming request and other context.\n        :type handler_input: Input\n        :return: Output from the 'handle' method execution of the\n            supporting handler.\n        :rtype: Union[None, Output]\n        :raises DispatchException if there is no supporting\n            handler chain or adapter\n        \"\"\"\n        request_handler_chain = None\n        for mapper in self.request_mappers:\n            request_handler_chain = mapper.get_request_handler_chain(\n                handler_input)\n            if request_handler_chain is not None:\n                break\n\n        if request_handler_chain is None:\n            raise DispatchException(\n                \"Unable to find a suitable request handler\")\n\n        request_handler = request_handler_chain.request_handler\n        supported_handler_adapter = None\n        for adapter in self.handler_adapters:\n            if adapter.supports(request_handler):\n                supported_handler_adapter = adapter\n                break\n\n        if supported_handler_adapter is None:\n            raise DispatchException(\n                \"Unable to find a suitable request adapter\")\n\n        local_request_interceptors = request_handler_chain.request_interceptors\n        for interceptor in local_request_interceptors:\n            interceptor.process(handler_input=handler_input)\n\n        output = supported_handler_adapter.execute(\n            handler_input=handler_input, handler=request_handler)  # type: Union[Output, None]\n\n        local_response_interceptors = (\n            request_handler_chain.response_interceptors)\n        for response_interceptor in local_response_interceptors:\n            response_interceptor.process(\n                handler_input=handler_input, dispatch_output=output)\n\n        return output", "response": "Process the incoming request and return the handler output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef skill_configuration(self):\n        # type: () -> SkillConfiguration\n        \"\"\"Create the skill configuration object using the\n        registered components.\n        \"\"\"\n        self.runtime_configuration = self.runtime_configuration_builder.get_runtime_configuration()\n        self.runtime_configuration.custom_user_agent = self.custom_user_agent\n        self.runtime_configuration.skill_id = self.skill_id\n        self.runtime_configuration = self.__populate_missing_attributes(\n            self.runtime_configuration)\n\n        return self.runtime_configuration", "response": "Create the skill configuration object using the available components."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a handler function that can be used as handler inAWS Lambda console.", "response": "def lambda_handler(self):\n        # type: () -> Callable[[RequestEnvelope, T], Dict[str, T]]\n        \"\"\"Create a handler function that can be used as handler in\n        AWS Lambda console.\n\n        The lambda handler provides a handler function, that acts as\n        an entry point to the AWS Lambda console. Users can set the\n        lambda_handler output to a variable and set the variable as\n        AWS Lambda Handler on the console.\n\n        :return: Handler function to tag on AWS Lambda console.\n        \"\"\"\n        def wrapper(event, context):\n            # type: (RequestEnvelope, T) -> Dict[str, T]\n            skill = CustomSkill(skill_configuration=self.skill_configuration)\n            request_envelope = skill.serializer.deserialize(\n                payload=json.dumps(event), obj_type=RequestEnvelope)\n            response_envelope = skill.invoke(\n                request_envelope=request_envelope, context=context)\n            return skill.serializer.serialize(response_envelope)  # type:ignore\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef skill_configuration(self):\n        # type: () -> SkillConfiguration\n        \"\"\"Create the skill configuration object using the\n        registered components.\n        \"\"\"\n        skill_config = super(CustomSkillBuilder, self).skill_configuration\n        skill_config.persistence_adapter = self.persistence_adapter\n        skill_config.api_client = self.api_client\n        return skill_config", "response": "Create the skill configuration object using the available components."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_app(self, app):\n        # type: (Flask) -> None\n        \"\"\"Register the extension on the given Flask application.\n\n        Use this function only when no Flask application was provided\n        in the ``app`` keyword argument to the constructor of this class.\n\n        The function sets ``True`` defaults for\n        :py:const:`VERIFY_SIGNATURE_APP_CONFIG` and\n        :py:const:`VERIFY_TIMESTAMP_APP_CONFIG` configurations. It adds\n        the skill id: self instance mapping to the application\n        extensions, and creates a\n        :py:class:`ask_sdk_webservice_support.webservice_handler.WebserviceHandler`\n        instance, for request verification and dispatch.\n\n        :param app: A :py:class:`flask.Flask` application instance\n        :type app: flask.Flask\n        :rtype: None\n        \"\"\"\n        app.config.setdefault(VERIFY_SIGNATURE_APP_CONFIG, True)\n        app.config.setdefault(VERIFY_TIMESTAMP_APP_CONFIG, True)\n\n        if EXTENSION_NAME not in app.extensions:\n            app.extensions[EXTENSION_NAME] = {}\n\n        app.extensions[EXTENSION_NAME][self._skill_id] = self\n\n        with app.app_context():\n            self._create_webservice_handler(self._skill, self._verifiers)", "response": "Register the extension on the given Flask application."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_webservice_handler(self, skill, verifiers):\n        # type: (CustomSkill, List[AbstractVerifier]) -> None\n        \"\"\"Create the handler for request verification and dispatch.\n\n        :param skill: A :py:class:`ask_sdk_core.skill.CustomSkill`\n            instance. If you are using the skill builder from ask-sdk,\n            then you can use the ``create`` method under it, to create a\n            skill instance\n        :type skill: ask_sdk_core.skill.CustomSkill\n        :param verifiers: A list of verifiers, that needs to\n            be applied on the input request, before invoking the\n            request handlers.\n        :type verifiers:\n            list[ask_sdk_webservice_support.verifier.AbstractVerifier]\n        :rtype: None\n        \"\"\"\n        if verifiers is None:\n            verifiers = []\n\n        self._webservice_handler = WebserviceSkillHandler(\n            skill=skill,\n            verify_signature=current_app.config.get(\n                VERIFY_SIGNATURE_APP_CONFIG, True),\n            verify_timestamp=current_app.config.get(\n                VERIFY_TIMESTAMP_APP_CONFIG, True),\n            verifiers=verifiers)", "response": "Create the webservice handler for request verification and dispatch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dispatch_request(self):\n        # type: () -> Response\n        \"\"\"Method that handles request verification and routing.\n\n        This method can be used as a function to register on the URL\n        rule. The request is verified through the registered list of\n        verifiers, before invoking the request handlers. The method\n        returns a JSON response for the Alexa service to respond to the\n        request.\n\n        :return: The skill response for the input request\n        :rtype: flask.Response\n        :raises: :py:class:`werkzeug.exceptions.MethodNotAllowed` if the\n            method is invoked for other than HTTP POST request.\n            :py:class:`werkzeug.exceptions.BadRequest` if the\n            verification fails.\n            :py:class:`werkzeug.exceptions.InternalServerError` for any\n            internal exception.\n        \"\"\"\n        if flask_request.method != \"POST\":\n            raise exceptions.MethodNotAllowed()\n\n        try:\n            content = flask_request.data.decode(\n                verifier_constants.CHARACTER_ENCODING)\n            response = self._webservice_handler.verify_request_and_dispatch(\n                http_request_headers=flask_request.headers,\n                http_request_body=content)\n\n            return jsonify(response)\n        except VerificationException:\n            current_app.logger.error(\n                \"Request verification failed\", exc_info=True)\n            raise exceptions.BadRequest(\n                description=\"Incoming request failed verification\")\n        except AskSdkException:\n            current_app.logger.error(\n                \"Skill dispatch exception\", exc_info=True)\n            raise exceptions.InternalServerError(\n                description=\"Exception occurred during skill dispatch\")", "response": "Method that handles the request verification and routing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(self, app, route, endpoint=None):\n        # type: (Flask, str, str) -> None\n        \"\"\"Method to register the routing on the app at provided route.\n\n        This is a utility method, that can be used for registering the\n        ``dispatch_request`` on the provided :py:class:`flask.Flask`\n        application at the provided URL ``route``.\n\n        :param app: A :py:class:`flask.Flask` application instance\n        :type app: flask.Flask\n        :param route: The URL rule where the skill dispatch has to be\n            registered\n        :type route: str\n        :param endpoint: The endpoint for the registered URL rule.\n            This can be used to set multiple skill endpoints on same app.\n        :type endpoint: str\n        :rtype: None\n        :raises: :py:class:`TypeError` if ``app`` or `route`` is not\n            provided or is of an invalid type\n        \"\"\"\n        if app is None or not isinstance(app, Flask):\n            raise TypeError(\"Expected a valid Flask instance\")\n\n        if route is None or not isinstance(route, str):\n            raise TypeError(\"Expected a valid URL rule string\")\n\n        app.add_url_rule(\n            route, view_func=self.dispatch_request, methods=[\"POST\"],\n            endpoint=endpoint)", "response": "Method to register the routing on the provided app at the provided route."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing the object into a dictionary of the most common types and values.", "response": "def serialize(self, obj):  # type: ignore\n        # type: (Any) -> Union[Dict[str, Any], List, Tuple, str, int, float, None]\n        \"\"\"Builds a serialized object.\n\n        * If obj is None, return None.\n        * If obj is str, int, long, float, bool, return directly.\n        * If obj is datetime.datetime, datetime.date convert to\n          string in iso8601 format.\n        * If obj is list, serialize each element in the list.\n        * If obj is dict, return the dict with serialized values.\n        * If obj is ask sdk model, return the dict with keys resolved\n          from the union of model's ``attribute_map`` and\n          ``deserialized_types`` and values serialized based on\n          ``deserialized_types``.\n        * If obj is a generic class instance, return the dict with keys\n          from instance's ``deserialized_types`` and values serialized\n          based on ``deserialized_types``.\n\n        :param obj: The data to serialize.\n        :type obj: object\n        :return: The serialized form of data.\n        :rtype: Union[Dict[str, Any], List, Tuple, str, int, float, None]\n        \"\"\"\n        if obj is None:\n            return None\n        elif isinstance(obj, self.PRIMITIVE_TYPES):\n            return obj\n        elif isinstance(obj, list):\n            return [self.serialize(sub_obj) for sub_obj in obj]\n        elif isinstance(obj, tuple):\n            return tuple(self.serialize(sub_obj) for sub_obj in obj)\n        elif isinstance(obj, (datetime, date)):\n            return obj.isoformat()\n        elif isinstance(obj, Enum):\n            return obj.value\n        elif isinstance(obj, decimal.Decimal):\n            if obj % 1 == 0:\n                return int(obj)\n            else:\n                return float(obj)\n\n        if isinstance(obj, dict):\n            obj_dict = obj\n        else:\n            # Convert model obj to dict\n            # All the non null attributes under `deserialized_types`\n            # map are considered for serialization.\n            # The `attribute_map` provides the key names to be used\n            # in the dict. In case of missing `attribute_map` mapping,\n            # the original attribute name is retained as the key name.\n            class_attribute_map = getattr(obj, 'attribute_map', {})\n            class_attribute_map.update(\n                {\n                    k: k for k in obj.deserialized_types.keys()\n                    if k not in class_attribute_map\n                }\n            )\n\n            obj_dict = {\n                class_attribute_map[attr]: getattr(obj, attr)\n                for attr, _ in iteritems(obj.deserialized_types)\n                if getattr(obj, attr) is not None\n            }\n\n        return {key: self.serialize(val) for key, val in iteritems(obj_dict)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the class from the class_name provided.", "response": "def __load_class_from_name(self, class_name):\n        # type: (str) -> T\n        \"\"\"Load the class from the ``class_name`` provided.\n\n        Resolve the class name from the ``class_name`` provided, load\n        the class on path and return the resolved class. If the module\n        information is not provided in the ``class_name``, then look\n        for the class on sys ``modules``.\n\n        :param class_name: absolute class name to be loaded\n        :type class_name: str\n        :return: Resolved class reference\n        :rtype: object\n        :raises: :py:class:`ask_sdk_core.exceptions.SerializationException`\n        \"\"\"\n        try:\n            module_class_list = class_name.rsplit(\".\", 1)\n            if len(module_class_list) > 1:\n                module_name = module_class_list[0]\n                resolved_class_name = module_class_list[1]\n                module = __import__(\n                    module_name, fromlist=[resolved_class_name])\n                resolved_class = getattr(module, resolved_class_name)\n            else:\n                resolved_class_name = module_class_list[0]\n                resolved_class = getattr(\n                    sys.modules[__name__], resolved_class_name)\n            return resolved_class\n        except Exception as e:\n            raise SerializationException(\n                \"Unable to resolve class {} from installed \"\n                \"modules: {}\".format(class_name, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __deserialize_primitive(self, payload, obj_type):\n        # type: (str, Union[T, str]) -> Any\n        \"\"\"Deserialize primitive datatypes.\n\n        :param payload: data to be deserialized\n        :type payload: str\n        :param obj_type: primitive datatype str\n        :type obj_type: Union[object, str]\n        :return: deserialized primitive datatype object\n        :rtype: object\n        :raises: :py:class:`ask_sdk_core.exceptions.SerializationException`\n        \"\"\"\n        obj_cast = cast(Any, obj_type)\n        try:\n            return obj_cast(payload)\n        except UnicodeEncodeError:\n            return unicode_type(payload)\n        except TypeError:\n            return payload\n        except ValueError:\n            raise SerializationException(\n                \"Failed to parse {} into '{}' object\".format(\n                    payload, obj_cast.__name__))", "response": "Deserialize the object into the specified object type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __deserialize_datetime(self, payload, obj_type):\n        # type: (str, Union[T, str]) -> Any\n        \"\"\"Deserialize datetime instance in ISO8601 format to\n        date/datetime object.\n\n        :param payload: data to be deserialized in ISO8601 format\n        :type payload: str\n        :param obj_type: primitive datatype str\n        :type obj_type: Union[object, str]\n        :return: deserialized primitive datatype object\n        :rtype: object\n        :raises: :py:class:`ask_sdk_core.exceptions.SerializationException`\n        \"\"\"\n        obj_cast = cast(Any, obj_type)\n        try:\n            from dateutil.parser import parse\n            parsed_datetime = parse(payload)\n            if obj_type is date:\n                return parsed_datetime.date()\n            else:\n                return parsed_datetime\n        except ImportError:\n            return payload\n        except ValueError:\n            raise SerializationException(\n                \"Failed to parse {} into '{}' object\".format(\n                    payload, obj_cast.__name__))", "response": "Deserialize a date or datetime object into the object type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __deserialize_model(self, payload, obj_type):\n        # type: (str, Union[T, str]) -> Any\n        \"\"\"Deserialize instance to model object.\n\n        :param payload: data to be deserialized\n        :type payload: str\n        :param obj_type: sdk model class\n        :type obj_type: Union[object, str]\n        :return: deserialized sdk model object\n        :rtype: object\n        :raises: :py:class:`ask_sdk_core.exceptions.SerializationException`\n        \"\"\"\n        try:\n            obj_cast = cast(Any, obj_type)\n            if issubclass(obj_cast, Enum):\n                return obj_cast(payload)\n\n            if hasattr(obj_cast, 'deserialized_types'):\n                if hasattr(obj_cast, 'get_real_child_model'):\n                    obj_cast = self.__get_obj_by_discriminator(\n                        payload, obj_cast)\n\n                class_deserialized_types = obj_cast.deserialized_types\n                class_attribute_map = getattr(obj_cast, 'attribute_map', {})\n                class_attribute_map.update(\n                    {\n                        k: k for k in obj_cast.deserialized_types.keys()\n                        if k not in class_attribute_map\n                    }\n                )\n\n                deserialized_model = obj_cast()\n                for class_param_name, payload_param_name in iteritems(\n                        class_attribute_map):\n                    if payload_param_name in payload:\n                        setattr(\n                            deserialized_model,\n                            class_param_name,\n                            self.__deserialize(\n                                payload[payload_param_name],\n                                class_deserialized_types[class_param_name]))\n\n                additional_params = [\n                    param for param in payload\n                    if param not in class_attribute_map.values()]\n\n                for add_param in additional_params:\n                    setattr(deserialized_model, add_param, payload[cast(Any,add_param)])\n                return deserialized_model\n            else:\n                return payload\n        except Exception as e:\n            raise SerializationException(str(e))", "response": "Deserialize instance to model object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget correct subclass instance using the discriminator in the payload.", "response": "def __get_obj_by_discriminator(self, payload, obj_type):\n        # type: (str, Union[T, str]) -> T\n        \"\"\"Get correct subclass instance using the discriminator in\n        payload.\n\n        :param payload: Payload for deserialization\n        :type payload: str\n        :param obj_type: parent class for deserializing payload into\n        :type obj_type: Union[object, str]\n        :return: Subclass of provided parent class, that resolves to\n            the discriminator in payload.\n        :rtype: object\n        :raises: :py:class:`ask_sdk_core.exceptions.SerializationException`\n        \"\"\"\n        obj_cast = cast(Any, obj_type)\n        namespaced_class_name = obj_cast.get_real_child_model(payload)\n        if not namespaced_class_name:\n            raise SerializationException(\n                \"Couldn't resolve object by discriminator type \"\n                \"for {} class\".format(obj_type))\n\n        return self.__load_class_from_name(namespaced_class_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef persistent_attributes(self):\n        # type: () -> Dict[str, object]\n        \"\"\"Attributes stored at the Persistence level of the skill lifecycle.\n\n        :return: persistent_attributes retrieved from persistence adapter\n        :rtype: Dict[str, object]\n        :raises: :py:class:`ask_sdk_core.exceptions.AttributesManagerException`\n            if trying to get persistent attributes without persistence adapter\n        \"\"\"\n        if not self._persistence_adapter:\n            raise AttributesManagerException(\n                \"Cannot get PersistentAttributes without Persistence adapter\")\n        if not self._persistent_attributes_set:\n            self._persistence_attributes = (\n                self._persistence_adapter.get_attributes(\n                    request_envelope=self._request_envelope))\n            self._persistent_attributes_set = True\n        return self._persistence_attributes", "response": "Retrieves the persistent attributes for the current skill."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nover-writes and caches the persistent attributes value.", "response": "def persistent_attributes(self, persistent_attributes):\n        # type: (Dict[str, object]) -> None\n        \"\"\"Overwrites and caches the persistent attributes value.\n\n        Note that the persistent attributes will not be saved to\n        persistence layer until the save_persistent_attributes method\n        is called.\n\n        :param persistent_attributes: attributes in persistence layer\n        :type persistent_attributes: Dict[str, object]\n        :raises: :py:class:`ask_sdk_core.exceptions.AttributesManagerException`\n            if trying to set persistent attributes without persistence adapter\n        \"\"\"\n        if not self._persistence_adapter:\n            raise AttributesManagerException(\n                \"Cannot set PersistentAttributes without persistence adapter!\")\n        self._persistence_attributes = persistent_attributes\n        self._persistent_attributes_set = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_persistent_attributes(self):\n        # type: () -> None\n        \"\"\"Save persistent attributes to the persistence layer if a\n        persistence adapter is provided.\n\n        :rtype: None\n        :raises: :py:class:`ask_sdk_core.exceptions.AttributesManagerException`\n            if trying to save persistence attributes without persistence adapter\n        \"\"\"\n        if not self._persistence_adapter:\n            raise AttributesManagerException(\n                \"Cannot save PersistentAttributes without \"\n                \"persistence adapter!\")\n        if self._persistent_attributes_set:\n            self._persistence_adapter.save_attributes(\n                request_envelope=self._request_envelope,\n                attributes=self._persistence_attributes)", "response": "Saves persistent attributes to the persistence layer if the persistence adapter is provided."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_persistent_attributes(self):\n        # type: () -> None\n        \"\"\"Deletes the persistent attributes from the persistence layer.\n        :rtype: None\n        :raises: :py:class: `ask_sdk_core.exceptions.AttributesManagerException`\n            if trying to delete persistence attributes without persistence adapter\n        \"\"\"\n        if not self._persistence_adapter:\n            raise AttributesManagerException(\n                \"Cannot delete PersistentAttributes without \"\n                \"persistence adapter!\")\n        if self._persistent_attributes_set:\n            self._persistence_adapter.delete_attributes(\n                request_envelope=self._request_envelope)\n            self._persistence_attributes = {}\n            self._persistent_attributes_set = False", "response": "Deletes the persistent attributes from the persistence layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_agent_info(sdk_version, custom_user_agent):\n    # type: (str, str) -> str\n    \"\"\"Return the user agent info along with the SDK and Python\n    Version information.\n\n    :param sdk_version: Version of the SDK being used.\n    :type sdk_version: str\n    :param custom_user_agent: Custom User Agent string provided by\n        the developer.\n    :type custom_user_agent: str\n    :return: User Agent Info string\n    :rtype: str\n    \"\"\"\n    python_version = \".\".join(str(x) for x in sys.version_info[0:3])\n    user_agent = \"ask-python/{} Python/{}\".format(\n        sdk_version, python_version)\n    if custom_user_agent is None:\n        return user_agent\n    else:\n        return user_agent + \" {}\".format(custom_user_agent)", "response": "Return the user agent info along with the SDK and PythonVersion information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_attributes(self, request_envelope):\n        # type: (RequestEnvelope) -> Dict[str, object]\n        \"\"\"Get attributes from table in Dynamodb resource.\n\n        Retrieves the attributes from Dynamodb table. If the table\n        doesn't exist, returns an empty dict if the\n        ``create_table`` variable is set as True, else it raises\n        PersistenceException. Raises PersistenceException if `get_item`\n        fails on the table.\n\n        :param request_envelope: Request Envelope passed during skill\n            invocation\n        :type request_envelope: ask_sdk_model.RequestEnvelope\n        :return: Attributes stored under the partition keygen mapping\n            in the table\n        :rtype: Dict[str, object]\n        :raises: :py:class:`ask_sdk_core.exceptions.PersistenceException`\n        \"\"\"\n        try:\n            table = self.dynamodb.Table(self.table_name)\n            partition_key_val = self.partition_keygen(request_envelope)\n            response = table.get_item(\n                Key={self.partition_key_name: partition_key_val},\n                ConsistentRead=True)\n            if \"Item\" in response:\n                return response[\"Item\"][self.attribute_name]\n            else:\n                return {}\n        except ResourceNotExistsError:\n            raise PersistenceException(\n                \"DynamoDb table {} doesn't exist or in the process of \"\n                \"being created. Failed to get attributes from \"\n                \"DynamoDb table.\".format(self.table_name))\n        except Exception as e:\n            raise PersistenceException(\n                \"Failed to retrieve attributes from DynamoDb table. \"\n                \"Exception of type {} occurred: {}\".format(\n                    type(e).__name__, str(e)))", "response": "Get the attributes from the table in Dynamodb."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_attributes(self, request_envelope, attributes):\n        # type: (RequestEnvelope, Dict[str, object]) -> None\n        \"\"\"Saves attributes to table in Dynamodb resource.\n\n        Saves the attributes into Dynamodb table. Raises\n        PersistenceException if table doesn't exist or ``put_item`` fails\n        on the table.\n\n        :param request_envelope: Request Envelope passed during skill\n            invocation\n        :type request_envelope: ask_sdk_model.RequestEnvelope\n        :param attributes: Attributes stored under the partition keygen\n            mapping in the table\n        :type attributes: Dict[str, object]\n        :rtype: None\n        :raises: :py:class:`ask_sdk_core.exceptions.PersistenceException`\n        \"\"\"\n        try:\n            table = self.dynamodb.Table(self.table_name)\n            partition_key_val = self.partition_keygen(request_envelope)\n            table.put_item(\n                Item={self.partition_key_name: partition_key_val,\n                      self.attribute_name: attributes})\n        except ResourceNotExistsError:\n            raise PersistenceException(\n                \"DynamoDb table {} doesn't exist. Failed to save attributes \"\n                \"to DynamoDb table.\".format(\n                    self.table_name))\n        except Exception as e:\n            raise PersistenceException(\n                \"Failed to save attributes to DynamoDb table. Exception of \"\n                \"type {} occurred: {}\".format(\n                    type(e).__name__, str(e)))", "response": "Saves the attributes to table in Dynamodb. Raises PersistenceException if table doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the attributes from table in Dynamodb table. Raises PersistenceException if table doesn t exist.", "response": "def delete_attributes(self, request_envelope):\n        # type: (RequestEnvelope) -> None\n        \"\"\"Deletes attributes from table in Dynamodb resource.\n\n        Deletes the attributes from Dynamodb table. Raises\n        PersistenceException if table doesn't exist or ``delete_item`` fails\n        on the table.\n\n        :param request_envelope: Request Envelope passed during skill\n            invocation\n        :type request_envelope: ask_sdk_model.RequestEnvelope\n        :rtype: None\n        :raises: :py:class:`ask_sdk_core.exceptions.PersistenceException`\n        \"\"\"\n        try:\n            table = self.dynamodb.Table(self.table_name)\n            partition_key_val = self.partition_keygen(request_envelope)\n            table.delete_item(\n                Key={self.partition_key_name: partition_key_val})\n        except ResourceNotExistsError:\n            raise PersistenceException(\n                \"DynamoDb table {} doesn't exist. Failed to delete attributes \"\n                \"from DynamoDb table.\".format(\n                    self.table_name))\n        except Exception as e:\n            raise PersistenceException(\n                \"Failed to delete attributes in DynamoDb table. Exception of \"\n                \"type {} occurred: {}\".format(\n                    type(e).__name__, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __create_table_if_not_exists(self):\n        # type: () -> None\n        \"\"\"Creates table in Dynamodb resource if it doesn't exist and\n        create_table is set as True.\n\n        :rtype: None\n        :raises: PersistenceException: When `create_table` fails on\n            dynamodb resource.\n        \"\"\"\n        if self.create_table:\n            try:\n                self.dynamodb.create_table(\n                    TableName=self.table_name,\n                    KeySchema=[\n                        {\n                            'AttributeName': self.partition_key_name,\n                            'KeyType': 'HASH'\n                        }\n                    ],\n                    AttributeDefinitions=[\n                        {\n                            'AttributeName': self.partition_key_name,\n                            'AttributeType': 'S'\n                        }\n\n                    ],\n                    ProvisionedThroughput={\n                        'ReadCapacityUnits': 5,\n                        'WriteCapacityUnits': 5\n                    }\n                )\n            except Exception as e:\n                if e.__class__.__name__ != \"ResourceInUseException\":\n                    raise PersistenceException(\n                        \"Create table if not exists request \"\n                        \"failed: Exception of type {} \"\n                        \"occurred: {}\".format(\n                            type(e).__name__, str(e)))", "response": "Creates table in Dynamodb resource if it doesn t exist and create_table is set as True."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister input to the request handlers list.", "response": "def add_request_handler(self, request_handler):\n        # type: (AbstractRequestHandler) -> None\n        \"\"\"Register input to the request handlers list.\n\n        :param request_handler: Request Handler instance to be\n            registered.\n        :type request_handler: AbstractRequestHandler\n        :return: None\n        \"\"\"\n        if request_handler is None:\n            raise RuntimeConfigException(\n                \"Valid Request Handler instance to be provided\")\n\n        if not isinstance(request_handler, AbstractRequestHandler):\n            raise RuntimeConfigException(\n                \"Input should be a RequestHandler instance\")\n\n        self.request_handler_chains.append(GenericRequestHandlerChain(\n            request_handler=request_handler))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering input to the exception handlers list.", "response": "def add_exception_handler(self, exception_handler):\n        # type: (AbstractExceptionHandler) -> None\n        \"\"\"Register input to the exception handlers list.\n\n        :param exception_handler: Exception Handler instance to be\n            registered.\n        :type exception_handler: AbstractExceptionHandler\n        :return: None\n        \"\"\"\n        if exception_handler is None:\n            raise RuntimeConfigException(\n                \"Valid Exception Handler instance to be provided\")\n\n        if not isinstance(exception_handler, AbstractExceptionHandler):\n            raise RuntimeConfigException(\n                \"Input should be an ExceptionHandler instance\")\n\n        self.exception_handlers.append(exception_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering input to the global request interceptors list.", "response": "def add_global_request_interceptor(self, request_interceptor):\n        # type: (AbstractRequestInterceptor) -> None\n        \"\"\"Register input to the global request interceptors list.\n\n        :param request_interceptor: Request Interceptor instance to be\n            registered.\n        :type request_interceptor: AbstractRequestInterceptor\n        :return: None\n        \"\"\"\n        if request_interceptor is None:\n            raise RuntimeConfigException(\n                \"Valid Request Interceptor instance to be provided\")\n\n        if not isinstance(request_interceptor, AbstractRequestInterceptor):\n            raise RuntimeConfigException(\n                \"Input should be a RequestInterceptor instance\")\n\n        self.global_request_interceptors.append(request_interceptor)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_global_response_interceptor(self, response_interceptor):\n        # type: (AbstractResponseInterceptor) -> None\n        \"\"\"Register input to the global response interceptors list.\n\n        :param response_interceptor: Response Interceptor instance to\n            be registered.\n        :type response_interceptor: AbstractResponseInterceptor\n        :return: None\n        \"\"\"\n        if response_interceptor is None:\n            raise RuntimeConfigException(\n                \"Valid Response Interceptor instance to be provided\")\n\n        if not isinstance(response_interceptor, AbstractResponseInterceptor):\n            raise RuntimeConfigException(\n                \"Input should be a ResponseInterceptor instance\")\n\n        self.global_response_interceptors.append(response_interceptor)", "response": "Register input to the global response interceptors list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_runtime_configuration(self):\n        # type: () -> RuntimeConfiguration\n        \"\"\"Build the runtime configuration object from the registered\n        components.\n\n        :return: Runtime Configuration Object\n        :rtype: RuntimeConfiguration\n        \"\"\"\n        request_mapper = GenericRequestMapper(\n            request_handler_chains=self.request_handler_chains)\n        exception_mapper = GenericExceptionMapper(\n            exception_handlers=self.exception_handlers)\n        handler_adapter = GenericHandlerAdapter()\n\n        runtime_configuration = RuntimeConfiguration(\n            request_mappers=[request_mapper],\n            handler_adapters=[handler_adapter],\n            exception_mapper=exception_mapper,\n            request_interceptors=self.global_request_interceptors,\n            response_interceptors=self.global_response_interceptors)\n\n        return runtime_configuration", "response": "Build the runtime configuration object from the registered\n        components."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_id_partition_keygen(request_envelope):\n    # type: (RequestEnvelope) -> str\n    \"\"\"Retrieve user id from request envelope, to use as partition key.\n\n    :param request_envelope: Request Envelope passed during skill\n        invocation\n    :type request_envelope: ask_sdk_model.RequestEnvelope\n    :return: User Id retrieved from request envelope\n    :rtype: str\n    :raises: :py:class:`ask_sdk_core.exceptions.PersistenceException`\n    \"\"\"\n    try:\n        user_id = request_envelope.context.system.user.user_id\n        return user_id\n    except AttributeError:\n        raise PersistenceException(\"Couldn't retrieve user id from request \"\n                                   \"envelope, for partition key use\")", "response": "Retrieve user id from request envelope to use as partition key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef device_id_partition_keygen(request_envelope):\n    # type: (RequestEnvelope) -> str\n    \"\"\"Retrieve device id from request envelope, to use as partition key.\n\n    :param request_envelope: Request Envelope passed during skill\n        invocation\n    :type request_envelope: ask_sdk_model.RequestEnvelope\n    :return: Device Id retrieved from request envelope\n    :rtype: str\n    :raises: :py:class:`ask_sdk_core.exceptions.PersistenceException`\n    \"\"\"\n    try:\n        device_id = request_envelope.context.system.device.device_id\n        return device_id\n    except AttributeError:\n        raise PersistenceException(\"Couldn't retrieve device id from \"\n                                   \"request envelope, for partition key use\")", "response": "Retrieve device id from request envelope to use as partition key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_canfulfill_intent_name(name):\n    # type: (str) -> Callable[[HandlerInput], bool]\n    \"\"\"A predicate function returning a boolean, when name matches the\n    intent name in a CanFulfill Intent Request.\n\n    The function can be applied on a\n    :py:class:`ask_sdk_core.handler_input.HandlerInput`, to\n    check if the input is of\n    :py:class:`ask_sdk_model.intent_request.CanFulfillIntentRequest` type and if the\n    name of the request matches with the passed name.\n\n    :param name: Name to be matched with the CanFulfill Intent Request Name\n    :type name: str\n    :return: Predicate function that can be used to check name of the\n        request\n    :rtype: Callable[[HandlerInput], bool]\n    \"\"\"\n    def can_handle_wrapper(handler_input):\n        # type: (HandlerInput) -> bool\n        return (isinstance(\n            handler_input.request_envelope.request, CanFulfillIntentRequest) and\n                handler_input.request_envelope.request.intent.name == name)\n    return can_handle_wrapper", "response": "A predicate function returning a boolean when name matches the intent name in a CanFulfill Intent Request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_intent_name(name):\n    # type: (str) -> Callable[[HandlerInput], bool]\n    \"\"\"A predicate function returning a boolean, when name matches the\n    name in Intent Request.\n\n    The function can be applied on a\n    :py:class:`ask_sdk_core.handler_input.HandlerInput`, to\n    check if the input is of\n    :py:class:`ask_sdk_model.intent_request.IntentRequest` type and if the\n    name of the request matches with the passed name.\n\n    :param name: Name to be matched with the Intent Request Name\n    :type name: str\n    :return: Predicate function that can be used to check name of the\n        request\n    :rtype: Callable[[HandlerInput], bool]\n    \"\"\"\n    def can_handle_wrapper(handler_input):\n        # type: (HandlerInput) -> bool\n        return (isinstance(\n            handler_input.request_envelope.request, IntentRequest) and\n                handler_input.request_envelope.request.intent.name == name)\n    return can_handle_wrapper", "response": "A predicate function returning a boolean when name matches the name in Intent Request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_request_type(request_type):\n    # type: (str) -> Callable[[HandlerInput], bool]\n    \"\"\"A predicate function returning a boolean, when request type is\n    the passed-in type.\n\n    The function can be applied on a\n    :py:class:`ask_sdk_core.handler_input.HandlerInput`, to check\n    if the input request type is the passed in request type.\n\n    :param request_type: request type to be matched with the input's request\n    :type request_type: str\n    :return: Predicate function that can be used to check the type of\n        the request\n    :rtype: Callable[[HandlerInput], bool]\n    \"\"\"\n    def can_handle_wrapper(handler_input):\n        # type: (HandlerInput) -> bool\n        return (handler_input.request_envelope.request.object_type ==\n                request_type)\n    return can_handle_wrapper", "response": "A predicate function returning a boolean when request type is\n    the passed - in type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndispatch a request to an API endpoint described in the request object and returns the response from the client call.", "response": "def invoke(self, request):\n        # type: (ApiClientRequest) -> ApiClientResponse\n        \"\"\"Dispatches a request to an API endpoint described in the\n        request.\n\n        Resolves the method from input request object, converts the\n        list of header tuples to the required format (dict) for the\n        `requests` lib call and invokes the method with corresponding\n        parameters on `requests` library. The response from the call is\n        wrapped under the `ApiClientResponse` object and the\n        responsibility of translating a response code and response/\n        error lies with the caller.\n\n        :param request: Request to dispatch to the ApiClient\n        :type request: ApiClientRequest\n        :return: Response from the client call\n        :rtype: ApiClientResponse\n        :raises: :py:class:`ask_sdk_core.exceptions.ApiClientException`\n        \"\"\"\n        try:\n            http_method = self._resolve_method(request)\n            http_headers = self._convert_list_tuples_to_dict(\n                headers_list=request.headers)\n\n            parsed_url = parse_url(request.url)\n            if parsed_url.scheme is None or parsed_url.scheme != \"https\":\n                raise ApiClientException(\n                    \"Requests against non-HTTPS endpoints are not allowed.\")\n\n            if request.body:\n                body_content_type = http_headers.get(\"Content-type\", None)\n                if (body_content_type is not None and\n                        \"json\" in body_content_type):\n                    raw_data = json.dumps(request.body)\n                else:\n                    raw_data = request.body\n            else:\n                raw_data = None\n\n            http_response = http_method(\n                url=request.url, headers=http_headers, data=raw_data)\n\n            return ApiClientResponse(\n                headers=self._convert_dict_to_list_tuples(\n                    http_response.headers),\n                status_code=http_response.status_code,\n                body=http_response.text)\n        except Exception as e:\n            raise ApiClientException(\n                \"Error executing the request: {}\".format(str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving the method from request object to requests http call.", "response": "def _resolve_method(self, request):\n        # type: (ApiClientRequest) -> Callable\n        \"\"\"Resolve the method from request object to `requests` http\n        call.\n\n        :param request: Request to dispatch to the ApiClient\n        :type request: ApiClientRequest\n        :return: The HTTP method that maps to the request call.\n        :rtype: Callable\n        :raises :py:class:`ask_sdk_core.exceptions.ApiClientException`\n            if invalid http request method is being called\n        \"\"\"\n        try:\n            return getattr(requests, request.method.lower())\n        except AttributeError:\n            raise ApiClientException(\n                \"Invalid request method: {}\".format(request.method))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts list of tuples from ApiClientRequest headers variable to dictionary format.", "response": "def _convert_list_tuples_to_dict(self, headers_list):\n        # type: (List[Tuple[str, str]]) -> Dict[str, str]\n        \"\"\"Convert list of tuples from headers of request object to\n        dictionary format.\n\n        :param headers_list: List of tuples made up of two element\n            strings from `ApiClientRequest` headers variable\n        :type headers_list: List[Tuple[str, str]]\n        :return: Dictionary of headers in keys as strings and values\n            as comma separated strings\n        :rtype: Dict[str, str]\n        \"\"\"\n        headers_dict = {}  # type: Dict\n        if headers_list is not None:\n            for header_tuple in headers_list:\n                key, value = header_tuple[0], header_tuple[1]\n                if key in headers_dict:\n                    headers_dict[key] = \"{}, {}\".format(\n                        headers_dict[key], value)\n                else:\n                    headers_dict[header_tuple[0]] = value\n        return headers_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_dict_to_list_tuples(self, headers_dict):\n        # type: (Dict[str, str]) -> List[Tuple[str, str]]\n        \"\"\"Convert headers dict to list of string tuples format for\n        `ApiClientResponse` headers variable.\n\n        :param headers_dict: Dictionary of headers in keys as strings\n            and values as comma separated strings\n        :type headers_dict: Dict[str, str]\n        :return: List of tuples made up of two element strings from\n            headers of client response\n        :rtype: List[Tuple[str, str]]\n        \"\"\"\n        headers_list = []\n        if headers_dict is not None:\n            for key, values in six.iteritems(headers_dict):\n                for value in values.split(\",\"):\n                    value = value.strip()\n                    if value is not None and value is not '':\n                        headers_list.append((key, value.strip()))\n        return headers_list", "response": "Convert a dictionary of headers into list of tuples format for\n            APIClientResponse headers variable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_request_handler_chain(self, request_handler_chain):\n        # type: (GenericRequestHandlerChain) -> None\n        \"\"\"Checks the type before adding it to the\n        request_handler_chains instance variable.\n\n        :param request_handler_chain:  Request Handler Chain instance.\n        :type request_handler_chain: RequestHandlerChain\n        :raises: :py:class:`ask_sdk_runtime.exceptions.DispatchException`\n            if a null input is provided or if the input is of invalid type\n        \"\"\"\n        if request_handler_chain is None or not isinstance(\n                request_handler_chain, GenericRequestHandlerChain):\n            raise DispatchException(\n                \"Request Handler Chain is not a GenericRequestHandlerChain \"\n                \"instance\")\n        self._request_handler_chains.append(request_handler_chain)", "response": "Adds the given request handler chain instance to the internal list of the available request handlers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_request_handler_chain(self, handler_input):\n        # type: (Input) -> Union[GenericRequestHandlerChain, None]\n        \"\"\"Get the request handler chain that can handle the dispatch\n        input.\n\n        :param handler_input: Generic input passed to the\n            dispatcher.\n        :type handler_input: Input\n        :return: Handler Chain that can handle the input.\n        :rtype: Union[None, GenericRequestHandlerChain]\n        \"\"\"\n        for chain in self.request_handler_chains:\n            handler = chain.request_handler  # type: AbstractRequestHandler\n            if handler.can_handle(handler_input=handler_input):\n                return chain\n        return None", "response": "Returns the request handler chain that can handle the input."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an exception handler instance to the internal list of exception_handlers.", "response": "def add_exception_handler(self, exception_handler):\n        # type: (AbstractExceptionHandler) -> None\n        \"\"\"Checks the type before adding it to the exception_handlers\n        instance variable.\n\n        :param exception_handler: Exception Handler instance.\n        :type exception_handler: ask_sdk_runtime.dispatch_components.exception_components.AbstractExceptionHandler\n        :raises: :py:class:`ask_sdk_runtime.exceptions.DispatchException` if a\n            null input is provided or if the input is of invalid type\n        \"\"\"\n        if exception_handler is None or not isinstance(\n                exception_handler, AbstractExceptionHandler):\n            raise DispatchException(\n                \"Input is not an AbstractExceptionHandler instance\")\n        self._exception_handlers.append(exception_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the exception handler that can handle the input and exception.", "response": "def get_handler(self, handler_input, exception):\n        # type: (Input, Exception) -> Union[AbstractExceptionHandler, None]\n        \"\"\"Get the exception handler that can handle the input and\n        exception.\n\n        :param handler_input: Generic input passed to the\n            dispatcher.\n        :type handler_input: Input\n        :param exception: Exception thrown by\n            :py:class:`ask_sdk_runtime.dispatch.GenericRequestDispatcher`\n            dispatch method.\n        :type exception: Exception\n        :return: Exception Handler that can handle the input or None.\n        :rtype: Union[None, ask_sdk_runtime.dispatch_components.exception_components.AbstractExceptionHandler]\n        \"\"\"\n        for handler in self.exception_handlers:\n            if handler.can_handle(\n                    handler_input=handler_input, exception=exception):\n                return handler\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify if the signature and the body matches.", "response": "def verify(\n            self, headers, serialized_request_env, deserialized_request_env):\n        # type: (Dict[str, Any], str, RequestEnvelope) -> None\n        \"\"\"Verify if the input request signature and the body matches.\n\n        The verify method retrieves the Signature Certificate Chain URL,\n        validates the URL, retrieves the chain from the URL, validates\n        the signing certificate, extract the public key, base64 decode\n        the Signature and verifies if the hash value of the request body\n        matches with the decrypted signature.\n\n        :param headers: headers of the input POST request\n        :type headers: Dict[str, Any]\n        :param serialized_request_env: raw request envelope in the\n            input POST request\n        :type serialized_request_env: str\n        :param deserialized_request_env: deserialized request envelope\n            instance of the input POST request\n        :type deserialized_request_env:\n            :py:class:`ask_sdk_model.request_envelope.RequestEnvelope`\n        :raises: :py:class:`VerificationException` if headers doesn't\n            exist or verification fails\n        \"\"\"\n        cert_url = headers.get(self._signature_cert_chain_url_key)\n        signature = headers.get(self._signature_key)\n\n        if cert_url is None or signature is None:\n            raise VerificationException(\n                \"Missing Signature/Certificate for the skill request\")\n\n        cert_chain = self._retrieve_and_validate_certificate_chain(cert_url)\n\n        self._valid_request_body(\n            cert_chain, signature, serialized_request_env)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve and validate certificate chain.", "response": "def _retrieve_and_validate_certificate_chain(self, cert_url):\n        # type: (str) -> Certificate\n        \"\"\"Retrieve and validate certificate chain.\n\n        This method validates if the URL is valid and loads and\n        validates the certificate chain, before returning it.\n\n        :param cert_url: URL for retrieving certificate chain\n        :type cert_url: str\n        :return The certificate chain loaded from the URL\n        :rtype cryptography.x509.Certificate\n        :raises: :py:class:`VerificationException` if the URL is invalid,\n            if the loaded certificate chain is invalid\n        \"\"\"\n        self._validate_certificate_url(cert_url)\n\n        cert_chain = self._load_cert_chain(cert_url)\n        self._validate_cert_chain(cert_chain)\n        return cert_chain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the URL containing the certificate chain.", "response": "def _validate_certificate_url(self, cert_url):\n        # type: (str) -> None\n        \"\"\"Validate the URL containing the certificate chain.\n\n        This method validates if the URL provided adheres to the format\n        mentioned here :\n        https://developer.amazon.com/docs/custom-skills/host-a-custom-skill-as-a-web-service.html#cert-verify-signature-certificate-url\n\n        :param cert_url: URL for retrieving certificate chain\n        :type cert_url: str\n        :raises: :py:class:`VerificationException` if the URL is invalid\n        \"\"\"\n        parsed_url = urlparse(cert_url)\n\n        protocol = parsed_url.scheme\n        if protocol.lower() != CERT_CHAIN_URL_PROTOCOL.lower():\n            raise VerificationException(\n                \"Signature Certificate URL has invalid protocol: {}. \"\n                \"Expecting {}\".format(protocol, CERT_CHAIN_URL_PROTOCOL))\n\n        hostname = parsed_url.hostname\n        if (hostname is None or\n                hostname.lower() != CERT_CHAIN_URL_HOSTNAME.lower()):\n            raise VerificationException(\n                \"Signature Certificate URL has invalid hostname: {}. \"\n                \"Expecting {}\".format(hostname, CERT_CHAIN_URL_HOSTNAME))\n\n        normalized_path = os.path.normpath(parsed_url.path)\n        if not normalized_path.startswith(CERT_CHAIN_URL_STARTPATH):\n            raise VerificationException(\n                \"Signature Certificate URL has invalid path: {}. \"\n                \"Expecting the path to start with {}\".format(\n                    normalized_path, CERT_CHAIN_URL_STARTPATH))\n\n        port = parsed_url.port\n        if port is not None and port != CERT_CHAIN_URL_PORT:\n            raise VerificationException(\n                \"Signature Certificate URL has invalid port: {}. \"\n                \"Expecting {}\".format(str(port), str(CERT_CHAIN_URL_PORT)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the certificate chain from the URL.", "response": "def _load_cert_chain(self, cert_url, x509_backend=default_backend()):\n        # type: (str, X509Backend) -> Certificate\n        \"\"\"Loads the certificate chain from the URL.\n\n        This method loads the certificate chain from the certificate\n        cache. If there is a cache miss, the certificate chain is\n        loaded from the certificate URL using the\n        :py:func:`cryptography.x509.load_pem_x509_certificate` method.\n        The x509 backend is set as default to the\n        :py:class:`cryptography.hazmat.backends.default_backend`\n        instance. A :py:class:`VerificationException` is raised if the\n        certificate cannot be loaded.\n\n        :param cert_url: URL for retrieving certificate chain\n        :type cert_url: str\n        :param x509_backend: Backend to be used, for loading pem x509\n            certificate\n        :type x509_backend:\n            cryptography.hazmat.backends.interfaces.X509Backend\n        :return: Certificate chain loaded from cache or URL\n        :rtype cryptography.x509.Certificate\n        :raises: :py:class:`VerificationException` if unable to load the\n            certificate\n        \"\"\"\n        try:\n            if cert_url in self._cert_cache:\n                return self._cert_cache.get(cert_url)\n            else:\n                with urlopen(cert_url) as cert_response:\n                    cert_data = cert_response.read()\n                    x509_certificate = load_pem_x509_certificate(\n                        cert_data, x509_backend)\n                    self._cert_cache[cert_url] = x509_certificate\n                    return x509_certificate\n        except ValueError as e:\n            raise VerificationException(\n                \"Unable to load certificate from URL\", e)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_cert_chain(self, cert_chain):\n        # type: (Certificate) -> None\n        \"\"\"Validate the certificate chain.\n\n        This method checks if the passed in certificate chain is valid,\n        i.e it is not expired and the Alexa domain is present in the\n        SAN extensions of the certificate chain. A\n        :py:class:`VerificationException` is raised if the certificate\n        chain is not valid.\n\n        :param cert_chain: Certificate chain to be validated\n        :type cert_chain: cryptography.x509.Certificate\n        :return: None\n        :raises: :py:class:`VerificationException` if certificated is\n            not valid\n        \"\"\"\n        now = datetime.utcnow()\n        if not (cert_chain.not_valid_before <= now <=\n                cert_chain.not_valid_after):\n            raise VerificationException(\"Signing Certificate expired\")\n\n        ext = cert_chain.extensions.get_extension_for_oid(\n            ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        if CERT_CHAIN_DOMAIN not in ext.value.get_values_for_type(\n                DNSName):\n            raise VerificationException(\n                \"{} domain missing in Signature Certificate Chain\".format(\n                    CERT_CHAIN_DOMAIN))", "response": "Validates the certificate chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates the request body hash with the signature.", "response": "def _valid_request_body(\n            self, cert_chain, signature, serialized_request_env):\n        # type: (Certificate, str, str) -> None\n        \"\"\"Validate the request body hash with signature.\n\n        This method checks if the hash value of the request body\n        matches with the hash value of the signature, decrypted using\n        certificate chain. A\n        :py:class:`VerificationException` is raised if there is a\n        mismatch.\n\n        :param cert_chain: Certificate chain to be validated\n        :type cert_chain: cryptography.x509.Certificate\n        :param signature: Encrypted signature of the request\n        :type: str\n        :param serialized_request_env: Raw request body\n        :type: str\n        :raises: :py:class:`VerificationException` if certificate is\n            not valid\n        \"\"\"\n        decoded_signature = base64.b64decode(signature)\n        public_key = cert_chain.public_key()\n        request_env_bytes = serialized_request_env.encode(CHARACTER_ENCODING)\n\n        try:\n            public_key.verify(\n                decoded_signature, request_env_bytes,\n                self._padding, self._hash_algorithm)\n        except InvalidSignature as e:\n            raise VerificationException(\"Request body is not valid\", e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies if the input request timestamp is in tolerated limits.", "response": "def verify(\n            self, headers, serialized_request_env, deserialized_request_env):\n        # type: (Dict[str, Any], str, RequestEnvelope) -> None\n        \"\"\"Verify if the input request timestamp is in tolerated limits.\n\n        The verify method retrieves the request timestamp and check if\n        it falls in the limit set by the tolerance, by checking with\n        the current timestamp in UTC.\n\n        :param headers: headers of the input POST request\n        :type headers: Dict[str, Any]\n        :param serialized_request_env: raw request envelope in the\n            input POST request\n        :type serialized_request_env: str\n        :param deserialized_request_env: deserialized request envelope\n            instance of the input POST request\n        :type deserialized_request_env:\n            :py:class:`ask_sdk_model.request_envelope.RequestEnvelope`\n        :raises: :py:class:`VerificationException` if difference between\n            local timestamp and input request timestamp is more than\n            specific tolerance limit\n        \"\"\"\n        local_now = datetime.now(tz.tzutc())\n        request_timestamp = deserialized_request_env.request.timestamp\n        if (abs((local_now - request_timestamp).seconds) >\n                (self._tolerance_in_millis / 1000)):\n            raise VerificationException(\"Timestamp verification failed\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef escape(identifier, ansi_quotes, should_quote):\n    if not should_quote(identifier):\n        return identifier\n\n    quote = '\"' if ansi_quotes else '`'\n    identifier = identifier.replace(quote, 2*quote)\n    return '{0}{1}{2}'.format(quote, identifier, quote)", "response": "Escape identifiers.\n\n    ANSI uses single quotes, but many databases use back quotes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates moving average bias ratio between two dates", "response": "def ma_bias_ratio(self, day1, day2):\n        \"\"\"Calculate moving average bias ratio\"\"\"\n        data1 = self.moving_average(self.price, day1)\n        data2 = self.moving_average(self.price, day2)\n        result = [data1[-i] - data2[-i] for i in range(1, min(len(data1), len(data2)) + 1)]\n\n        return result[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates pivot point for ma bias ratio", "response": "def ma_bias_ratio_pivot(self, data, sample_size=5, position=False):\n        \"\"\"Calculate pivot point\"\"\"\n        sample = data[-sample_size:]\n\n        if position is True:\n            check_value = max(sample)\n            pre_check_value = max(sample) > 0\n        elif position is False:\n            check_value = min(sample)\n            pre_check_value = max(sample) < 0\n\n        return ((sample_size - sample.index(check_value) < 4 and\n                 sample.index(check_value) != sample_size - 1 and pre_check_value),\n                sample_size - sample.index(check_value) - 1,\n                check_value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cal_continue(self, list_data):\n        diff_data = []\n        for i in range(1, len(list_data)):\n            if list_data[-i] > list_data[-i - 1]:\n                diff_data.append(1)\n            else:\n                diff_data.append(-1)\n        cont = 0\n        for value in diff_data:\n            if value == diff_data[0]:\n                cont += 1\n            else:\n                break\n        return cont * diff_data[0]", "response": "Return the number of times the user has to continue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef moving_average(self, data, days):\n        result = []\n        data = data[:]\n        for dummy in range(len(data) - int(days) + 1):\n            result.append(round(sum(data[-days:]) / days, 2))\n            data.pop()\n        result.reverse()\n        return result", "response": "\u8a08\u7b97\u79fb\u52d5\u5e73\u5747\u6578\n\n            :rtype: \u5e8f\u5217 \u820a\u2192\u65b0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ma_bias_ratio(self, date1, date2, data):\n        data1 = self.moving_average(data, date1)\n        data2 = self.moving_average(data, date2)\n        cal_list = []\n        for i in range(1, min(len(data1), len(data2)) + 1):\n            cal_list.append(data1[-i] - data2[-i])\n        cal_list.reverse()\n        return cal_list", "response": "ma_bias_ratio - \u8a08\u7b97\u4e56\u96e2\u7387\uff08\u5747\u50f9\uff09 date1 - date2 - data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef best_buy_1(self):\n        result = self.data.capacity[-1] > self.data.capacity[-2] and \\\n                 self.data.price[-1] > self.data.open[-1]\n        return result", "response": "return true if the best buy 1 is available"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef best_buy_2(self):\n        result = self.data.capacity[-1] < self.data.capacity[-2] and \\\n                 self.data.price[-1] > self.data.price[-2]\n        return result", "response": "return true if the best buy 2 is available"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef best_sell_1(self):\n        result = self.data.capacity[-1] > self.data.capacity[-2] and \\\n                 self.data.price[-1] < self.data.open[-1]\n        return result", "response": "return true if the best sell 1 is"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn true if the best sell 2 is available", "response": "def best_sell_2(self):\n        \"\"\"\u91cf\u7e2e\u50f9\u8dcc\n        \"\"\"\n        result = self.data.capacity[-1] < self.data.capacity[-2] and \\\n                 self.data.price[-1] < self.data.price[-2]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef best_four_point_to_sell(self):\n        result = []\n        if self.check_plus_bias_ratio() and \\\n            (self.best_sell_1() or self.best_sell_2() or self.best_sell_3() or \\\n             self.best_sell_4()):\n            if self.best_sell_1():\n                result.append(self.best_sell_1.__doc__.strip())\n            if self.best_sell_2():\n                result.append(self.best_sell_2.__doc__.strip())\n            if self.best_sell_3():\n                result.append(self.best_sell_3.__doc__.strip())\n            if self.best_sell_4():\n                result.append(self.best_sell_4.__doc__.strip())\n            result = ', '.join(result)\n        else:\n            result = False\n        return result", "response": "returns a list of the most likely few point to the best sell"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef best_four_point(self):\n        buy = self.best_four_point_to_buy()\n        sell = self.best_four_point_to_sell()\n\n        if buy:\n            return True, buy\n        elif sell:\n            return False, sell\n\n        return None", "response": "returns tuple containing buy and sell"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_date(self, date):\n        return '/'.join([str(int(date.split('/')[0]) + 1911)] + date.split('/')[1:])", "response": "Convert '106/05/01' to '2017/05/01"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches year month data", "response": "def fetch(self, year: int, month: int):\n        \"\"\"Fetch year month data\"\"\"\n        self.raw_data = [self.fetcher.fetch(year, month, self.sid)]\n        self.data = self.raw_data[0]['data']\n        return self.data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching data from year month to current year month data", "response": "def fetch_from(self, year: int, month: int):\n        \"\"\"Fetch data from year, month to current year month data\"\"\"\n        self.raw_data = []\n        self.data = []\n        today = datetime.datetime.today()\n        for year, month in self._month_year_iter(month, year, today.month, today.year):\n            self.raw_data.append(self.fetcher.fetch(year, month, self.sid))\n            self.data.extend(self.raw_data[-1]['data'])\n        return self.data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch 31 days data", "response": "def fetch_31(self):\n        \"\"\"Fetch 31 days data\"\"\"\n        today = datetime.datetime.today()\n        before = today - datetime.timedelta(days=60)\n        self.fetch_from(before.year, before.month)\n        self.data = self.data[-31:]\n        return self.data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dict(self):\n        return dict(\n            addr=self.addr,\n            protocol=self.protocol,\n            weight=self.weight,\n            last_checked=self.last_checked)", "response": "convert detailed proxy info into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef proxy_num(self, protocol=None):\n        http_num = len(self.proxies['http'])\n        https_num = len(self.proxies['https'])\n        if protocol == 'http':\n            return http_num\n        elif protocol == 'https':\n            return https_num\n        else:\n            return http_num + https_num", "response": "Get the number of proxies in the pool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the next proxy in the list of proxies.", "response": "def get_next(self, protocol='http', format=False, policy='loop'):\n        \"\"\"Get the next proxy\n\n        Args:\n            protocol (str): 'http' or 'https'. (default 'http')\n            format (bool): Whether to format the proxy. (default False)\n            policy (str): Either 'loop' or 'random', indicating the policy of\n                getting the next proxy. If set to 'loop', will return proxies\n                in turn, otherwise will return a proxy randomly.\n\n        Returns:\n            Proxy or dict: If format is true, then return the formatted proxy\n                which is compatible with requests.Session parameters,\n                otherwise a Proxy object.\n        \"\"\"\n        if not self.proxies[protocol]:\n            return None\n        if policy == 'loop':\n            idx = self.idx[protocol]\n            self.idx[protocol] = (idx + 1) % len(self.proxies[protocol])\n        elif policy == 'random':\n            idx = random.randint(0, self.proxy_num(protocol) - 1)\n        else:\n            self.logger.error('Unsupported get_next policy: {}'.format(policy))\n            exit()\n        proxy = self.proxies[protocol][self.addr_list[protocol][idx]]\n        if proxy.weight < random.random():\n            return self.get_next(protocol, format, policy)\n        if format:\n            return proxy.format()\n        else:\n            return proxy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave proxies to file", "response": "def save(self, filename):\n        \"\"\"Save proxies to file\"\"\"\n        proxies = {'http': [], 'https': []}\n        for protocol in ['http', 'https']:\n            for proxy in self.proxies[protocol]:\n                serializable_proxy = self.proxies[protocol][proxy].to_dict()\n                proxies[protocol].append(serializable_proxy)\n        with open(filename, 'w') as fout:\n            json.dump(proxies, fout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading proxies from file", "response": "def load(self, filename):\n        \"\"\"Load proxies from file\"\"\"\n        with open(filename, 'r') as fin:\n            proxies = json.load(fin)\n        for protocol in proxies:\n            for proxy in proxies[protocol]:\n                self.proxies[protocol][proxy['addr']] = Proxy(\n                    proxy['addr'], proxy['protocol'], proxy['weight'],\n                    proxy['last_checked'])\n                self.addr_list[protocol].append(proxy['addr'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_proxy(self, proxy):\n        protocol = proxy.protocol\n        addr = proxy.addr\n        if addr in self.proxies:\n            self.proxies[protocol][addr].last_checked = proxy.last_checked\n        else:\n            self.proxies[protocol][addr] = proxy\n            self.addr_list[protocol].append(addr)", "response": "Add a valid proxy into the pool"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_proxy(self, proxy):\n        del self.search_flag[proxy.protocol][proxy.addr]\n        del self.addr_list[proxy.protocol][proxy.addr]", "response": "Remove a proxy from the pool"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef increase_weight(self, proxy):\n        new_weight = proxy.weight * self.inc_ratio\n        if new_weight < 1.0:\n            proxy.weight = new_weight\n        else:\n            proxy.weight = 1.0", "response": "Increase the weight of a proxy by multiplying inc_ratio and setting weight to 1. 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecrease the weight of a proxy by multiplying dec_ratio and removing the proxy if it is below weight_thr.", "response": "def decrease_weight(self, proxy):\n        \"\"\"Decreasing the weight of a proxy by multiplying dec_ratio\"\"\"\n        new_weight = proxy.weight * self.dec_ratio\n        if new_weight < self.weight_thr:\n            self.remove_proxy(proxy)\n        else:\n            proxy.weight = new_weight"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid(self, addr, protocol='http', timeout=5):\n        start = time.time()\n        try:\n            r = requests.get(self.test_url[protocol],\n                             timeout=timeout,\n                             proxies={protocol: 'http://' + addr})\n        except KeyboardInterrupt:\n            raise\n        except requests.exceptions.Timeout:\n            return {'valid': False, 'msg': 'timeout'}\n        except:\n            return {'valid': False, 'msg': 'exception'}\n        else:\n            if r.status_code == 200:\n                response_time = time.time() - start\n                return {'valid': True, 'response_time': response_time}\n            else:\n                return {\n                    'valid': False,\n                    'msg': 'status code: {}'.format(r.status_code)\n                }", "response": "Check if a proxy is valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self,\n                 proxy_scanner,\n                 expected_num=20,\n                 queue_timeout=3,\n                 val_timeout=5):\n        \"\"\"Target function of validation threads\n\n        Args:\n            proxy_scanner: A ProxyScanner object.\n            expected_num: Max number of valid proxies to be scanned.\n            queue_timeout: Timeout for getting a proxy from the queue.\n            val_timeout: An integer passed to `is_valid` as argument `timeout`.\n        \"\"\"\n        while self.proxy_num() < expected_num:\n            try:\n                candidate_proxy = proxy_scanner.proxy_queue.get(\n                    timeout=queue_timeout)\n            except queue.Empty:\n                if proxy_scanner.is_scanning():\n                    continue\n                else:\n                    break\n            addr = candidate_proxy['addr']\n            protocol = candidate_proxy['protocol']\n            ret = self.is_valid(addr, protocol, val_timeout)\n            if self.proxy_num() >= expected_num:\n                self.logger.info('Enough valid proxies, thread {} exit.'\n                                 .format(threading.current_thread().name))\n                break\n            if ret['valid']:\n                self.add_proxy(Proxy(addr, protocol))\n                self.logger.info('{} ok, {:.2f}s'.format(addr, ret[\n                    'response_time']))\n            else:\n                self.logger.info('{} invalid, {}'.format(addr, ret['msg']))", "response": "This function checks the number of proxies in the queue and validates the number of proxies in the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscan and validate proxies using multiple threads.", "response": "def scan(self,\n             proxy_scanner,\n             expected_num=20,\n             val_thr_num=4,\n             queue_timeout=3,\n             val_timeout=5,\n             out_file='proxies.json'):\n        \"\"\"Scan and validate proxies\n\n        Firstly, call the `scan` method of `proxy_scanner`, then using multiple\n        threads to validate them.\n\n        Args:\n            proxy_scanner: A ProxyScanner object.\n            expected_num: Max number of valid proxies to be scanned.\n            val_thr_num: Number of threads used for validating proxies.\n            queue_timeout: Timeout for getting a proxy from the queue.\n            val_timeout: An integer passed to `is_valid` as argument `timeout`.\n            out_file: A string or None. If not None, the proxies will be saved\n                      into `out_file`.\n        \"\"\"\n        try:\n            proxy_scanner.scan()\n            self.logger.info('starting {} threads to validating proxies...'\n                             .format(val_thr_num))\n            val_threads = []\n            for i in range(val_thr_num):\n                t = threading.Thread(\n                    name='val-{:0>2d}'.format(i + 1),\n                    target=self.validate,\n                    kwargs=dict(\n                        proxy_scanner=proxy_scanner,\n                        expected_num=expected_num,\n                        queue_timeout=queue_timeout,\n                        val_timeout=val_timeout))\n                t.daemon = True\n                val_threads.append(t)\n                t.start()\n            for t in val_threads:\n                t.join()\n            self.logger.info('Proxy scanning done!')\n        except:\n            raise\n        finally:\n            if out_file is not None:\n                self.save(out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default_scan(self,\n                     region='mainland',\n                     expected_num=20,\n                     val_thr_num=4,\n                     queue_timeout=3,\n                     val_timeout=5,\n                     out_file='proxies.json',\n                     src_files=None):\n        \"\"\"Default scan method, to simplify the usage of `scan` method.\n\n        It will register following scan functions:\n        1. scan_file\n        2. scan_cnproxy (if region is mainland)\n        3. scan_free_proxy_list (if region is overseas)\n        4. scan_ip84\n        5. scan_mimiip\n        After scanning, all the proxy info will be saved in out_file.\n\n        Args:\n            region: Either 'mainland' or 'overseas'\n            expected_num: An integer indicating the expected number of proxies,\n                          if this argument is set too great, it may take long to\n                          finish scanning process.\n            val_thr_num: Number of threads used for validating proxies.\n            queue_timeout: An integer indicating the timeout for getting a\n                           candidate proxy from the queue.\n            val_timeout: An integer indicating the timeout when connecting the\n                         test url using a candidate proxy.\n            out_file: the file name of the output file saving all the proxy info\n            src_files: A list of file names to scan\n        \"\"\"\n        if expected_num > 30:\n            self.logger.warn('The more proxy you expect, the more time it '\n                             'will take. It is highly recommended to limit the'\n                             ' expected num under 30.')\n        proxy_scanner = ProxyScanner()\n        if src_files is None:\n            src_files = []\n        elif isinstance(src_files, str):\n            src_files = [src_files]\n        for filename in src_files:\n            proxy_scanner.register_func(proxy_scanner.scan_file,\n                                        {'src_file': filename})\n        if region == 'mainland':\n            proxy_scanner.register_func(proxy_scanner.scan_cnproxy, {})\n        elif region == 'overseas':\n            proxy_scanner.register_func(proxy_scanner.scan_free_proxy_list, {})\n        proxy_scanner.register_func(proxy_scanner.scan_ip84,\n                                    {'region': region,\n                                     'page': 5})\n        proxy_scanner.register_func(proxy_scanner.scan_mimiip,\n                                    {'region': region,\n                                     'page': 5})\n        self.scan(proxy_scanner, expected_num, val_thr_num, queue_timeout,\n                  val_timeout, out_file)", "response": "Default scan method, to simplify the usage of `scan` method.\n\n        It will register following scan functions:\n        1. scan_file\n        2. scan_cnproxy (if region is mainland)\n        3. scan_free_proxy_list (if region is overseas)\n        4. scan_ip84\n        5. scan_mimiip\n        After scanning, all the proxy info will be saved in out_file.\n\n        Args:\n            region: Either 'mainland' or 'overseas'\n            expected_num: An integer indicating the expected number of proxies,\n                          if this argument is set too great, it may take long to\n                          finish scanning process.\n            val_thr_num: Number of threads used for validating proxies.\n            queue_timeout: An integer indicating the timeout for getting a\n                           candidate proxy from the queue.\n            val_timeout: An integer indicating the timeout when connecting the\n                         test url using a candidate proxy.\n            out_file: the file name of the output file saving all the proxy info\n            src_files: A list of file names to scan"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a scan function in the list of functions that are available to the user.", "response": "def register_func(self, func_name, func_kwargs):\n        \"\"\"Register a scan function\n\n        Args:\n            func_name: The function name of a scan function.\n            func_kwargs: A dict containing arguments of the scan function.\n        \"\"\"\n        self.scan_funcs.append(func_name)\n        self.scan_kwargs.append(func_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scan_ip84(self, region='mainland', page=1):\n        self.logger.info('start scanning http://ip84.com for proxy list...')\n        for i in range(1, page + 1):\n            if region == 'mainland':\n                url = 'http://ip84.com/dlgn/{}'.format(i)\n            elif region == 'overseas':\n                url = 'http://ip84.com/gwgn/{}'.format(i)\n            else:\n                url = 'http://ip84.com/gn/{}'.format(i)\n            response = requests.get(url)\n            soup = BeautifulSoup(response.content, 'lxml')\n            table = soup.find('table', class_='list')\n            for tr in table.find_all('tr'):\n                if tr.th is not None:\n                    continue\n                info = tr.find_all('td')\n                protocol = info[4].string.lower()\n                addr = '{}:{}'.format(info[0].string, info[1].string)\n                self.proxy_queue.put({'addr': addr, 'protocol': protocol})", "response": "Scan candidate proxies from http://ip84. com"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscan candidate proxies from http://cn - proxy. com and add them to the queue.", "response": "def scan_cnproxy(self):\n        \"\"\"Scan candidate (mainland) proxies from http://cn-proxy.com\"\"\"\n        self.logger.info(\n            'start scanning http://cn-proxy.com for proxy list...')\n        response = requests.get('http://cn-proxy.com')\n        soup = BeautifulSoup(response.content, 'lxml')\n        tables = soup.find_all('table', class_='sortable')\n        for table in tables:\n            for tr in table.tbody.find_all('tr'):\n                info = tr.find_all('td')\n                addr = '{}:{}'.format(info[0].string, info[1].string)\n                self.proxy_queue.put({'addr': addr, 'protocol': 'http'})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scan_free_proxy_list(self):\n        self.logger.info('start scanning http://free-proxy-list.net '\n                         'for proxy list...')\n        response = requests.get('http://free-proxy-list.net')\n        soup = BeautifulSoup(response.content, 'lxml')\n        table = soup.find('table', id='proxylisttable')\n        for tr in table.tbody.find_all('tr'):\n            info = tr.find_all('td')\n            if info[4].string != 'elite proxy':\n                continue\n            if info[6].string == 'yes':\n                protocol = 'https'\n            else:\n                protocol = 'http'\n            addr = '{}:{}'.format(info[0].string, info[1].string)\n            self.proxy_queue.put({'addr': addr, 'protocol': protocol})", "response": "Scan candidate proxies from http://free - proxy - list. net and add them to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scan_file(self, src_file):\n        self.logger.info('start scanning file {} for proxy list...'\n                         .format(src_file))\n        with open(src_file, 'r') as fin:\n            proxies = json.load(fin)\n        for protocol in proxies.keys():\n            for proxy in proxies[protocol]:\n                self.proxy_queue.put({\n                    'addr': proxy['addr'],\n                    'protocol': protocol\n                })", "response": "Scan candidate proxies from an existing file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scan(self):\n        self.logger.info('{0} registered scan functions, starting {0} threads '\n                         'to scan candidate proxy lists...'\n                         .format(len(self.scan_funcs)))\n        for i in range(len(self.scan_funcs)):\n            t = threading.Thread(\n                name=self.scan_funcs[i].__name__,\n                target=self.scan_funcs[i],\n                kwargs=self.scan_kwargs[i])\n            t.daemon = True\n            self.scan_threads.append(t)\n            t.start()", "response": "Start a thread for each registered scan function to scan proxy lists"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_duplicated(self, item):\n        if isinstance(item, dict):\n            hashable_item = json.dumps(item, sort_keys=True)\n        elif isinstance(item, list):\n            hashable_item = frozenset(item)\n        else:\n            hashable_item = item\n        if hashable_item in self._cache:\n            return True\n        else:\n            if self.cache_capacity > 0 and len(\n                    self._cache) >= self.cache_capacity:\n                self._cache.popitem(False)\n            self._cache[hashable_item] = 1\n            return False", "response": "Checks whether the item has been duplicated in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs an item to the queue if it is not duplicated.", "response": "def put(self, item, block=True, timeout=None, dup_callback=None):\n        \"\"\"Put an item to queue if it is not duplicated.\n        \"\"\"\n        if not self.is_duplicated(item):\n            super(CachedQueue, self).put(item, block, timeout)\n        else:\n            if dup_callback:\n                dup_callback(item)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, **signals):\n        for name in signals:\n            if name not in self._signals:\n                self._init_status[name] = signals[name]\n            self._signals[name] = signals[name]", "response": "Sets the internal state of all the neccesary objects in the sequence of all the related objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef worker_exec(self,\n                    queue_timeout=2,\n                    req_timeout=5,\n                    max_retry=3,\n                    **kwargs):\n        \"\"\"Target method of workers.\n\n        Firstly download the page and then call the :func:`parse` method.\n        A parser thread will exit in either of the following cases:\n\n        1. All feeder threads have exited and the ``url_queue`` is empty.\n        2. Downloaded image number has reached required number.\n\n        Args:\n            queue_timeout (int): Timeout of getting urls from ``url_queue``.\n            req_timeout (int): Timeout of making requests for downloading pages.\n            max_retry (int): Max retry times if the request fails.\n            **kwargs: Arguments to be passed to the :func:`parse` method.\n        \"\"\"\n        while True:\n            if self.signal.get('reach_max_num'):\n                self.logger.info('downloaded image reached max num, thread %s '\n                                 'is ready to exit', current_thread().name)\n                break\n            # get the page url\n            try:\n                url = self.in_queue.get(timeout=queue_timeout)\n            except queue.Empty:\n                if self.signal.get('feeder_exited'):\n                    self.logger.info(\n                        'no more page urls for thread %s to parse',\n                        current_thread().name)\n                    break\n                else:\n                    self.logger.info('%s is waiting for new page urls',\n                                     current_thread().name)\n                    continue\n            except:\n                self.logger.error('exception in thread %s',\n                                  current_thread().name)\n                continue\n            else:\n                self.logger.debug('start fetching page {}'.format(url))\n            # fetch and parse the page\n            retry = max_retry\n            while retry > 0:\n                try:\n                    base_url = '{0.scheme}://{0.netloc}'.format(urlsplit(url))\n                    response = self.session.get(url,\n                                                timeout=req_timeout,\n                                                headers={'Referer': base_url})\n                except Exception as e:\n                    self.logger.error(\n                        'Exception caught when fetching page %s, '\n                        'error: %s, remaining retry times: %d', url, e,\n                        retry - 1)\n                else:\n                    self.logger.info('parsing result page {}'.format(url))\n                    for task in self.parse(response, **kwargs):\n                        while not self.signal.get('reach_max_num'):\n                            try:\n                                if isinstance(task, dict):\n                                    self.output(task, timeout=1)\n                                elif isinstance(task, str):\n                                    # this case only work for GreedyCrawler,\n                                    # which need to feed the url back to\n                                    # url_queue, dirty implementation\n                                    self.input(task, timeout=1)\n                            except queue.Full:\n                                time.sleep(1)\n                            except Exception as e:\n                                self.logger.error(\n                                    'Exception caught when put task %s into '\n                                    'queue, error: %s', task, url)\n                            else:\n                                break\n                        if self.signal.get('reach_max_num'):\n                            break\n                    self.in_queue.task_done()\n                    break\n                finally:\n                    retry -= 1\n        self.logger.info('thread {} exit'.format(current_thread().name))", "response": "This method is called by the worker threads to download and parse the page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef worker_exec(self, **kwargs):\n        self.feed(**kwargs)\n        self.logger.info('thread {} exit'.format(current_thread().name))", "response": "Target function of workers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef feed(self, url_template, keyword, offset, max_num, page_step):\n        for i in range(offset, offset + max_num, page_step):\n            url = url_template.format(keyword, i)\n            self.out_queue.put(url)\n            self.logger.debug('put url to url_queue: {}'.format(url))", "response": "Feed urls once\n\n        Args:\n            url_template: A string with parameters replaced with \"{}\".\n            keyword: A string indicating the searching keyword.\n            offset: An integer indicating the starting index.\n            max_num: An integer indicating the max number of images to be crawled.\n            page_step: An integer added to offset after each iteration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting two ThreadPools. The ``in_queue`` of the second pool will be set as the ``out_queue`` of the current pool, thus all the output will be input to the second pool. Args: component (ThreadPool): the ThreadPool to be connected. Returns: ThreadPool: the modified second ThreadPool.", "response": "def connect(self, component):\n        \"\"\"Connect two ThreadPools.\n\n        The ``in_queue`` of the second pool will be set as the ``out_queue`` of\n        the current pool, thus all the output will be input to the second pool.\n\n        Args:\n            component (ThreadPool): the ThreadPool to be connected.\n        Returns:\n            ThreadPool: the modified second ThreadPool.\n        \"\"\"\n        if not isinstance(component, ThreadPool):\n            raise TypeError('\"component\" must be a ThreadPool object')\n        component.in_queue = self.out_queue\n        return component"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_file_idx_offset(self, file_idx_offset=0):\n        if isinstance(file_idx_offset, int):\n            self.file_idx_offset = file_idx_offset\n        elif file_idx_offset == 'auto':\n            self.file_idx_offset = self.storage.max_file_idx()\n        else:\n            raise ValueError('\"file_idx_offset\" must be an integer or `auto`')", "response": "Set offset of file index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the filename of the image from the task dict.", "response": "def get_filename(self, task, default_ext):\n        \"\"\"Set the path where the image will be saved.\n\n        The default strategy is to use an increasing 6-digit number as\n        the filename. You can override this method if you want to set custom\n        naming rules. The file extension is kept if it can be obtained from\n        the url, otherwise ``default_ext`` is used as extension.\n\n        Args:\n            task (dict): The task dict got from ``task_queue``.\n\n        Output:\n            Filename with extension.\n        \"\"\"\n        url_path = urlparse(task['file_url'])[2]\n        extension = url_path.split('.')[-1] if '.' in url_path else default_ext\n        file_idx = self.fetched_num + self.file_idx_offset\n        return '{:06d}.{}'.format(file_idx, extension)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reach_max_num(self):\n        if self.signal.get('reach_max_num'):\n            return True\n        if self.max_num > 0 and self.fetched_num >= self.max_num:\n            return True\n        else:\n            return False", "response": "Check if downloaded images reached max num."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload the image and save it to the corresponding path.", "response": "def download(self,\n                 task,\n                 default_ext,\n                 timeout=5,\n                 max_retry=3,\n                 overwrite=False,\n                 **kwargs):\n        \"\"\"Download the image and save it to the corresponding path.\n\n        Args:\n            task (dict): The task dict got from ``task_queue``.\n            timeout (int): Timeout of making requests for downloading images.\n            max_retry (int): the max retry times if the request fails.\n            **kwargs: reserved arguments for overriding.\n        \"\"\"\n        file_url = task['file_url']\n        task['success'] = False\n        task['filename'] = None\n        retry = max_retry\n\n        if not overwrite:\n            with self.lock:\n                self.fetched_num += 1\n                filename = self.get_filename(task, default_ext)\n                if self.storage.exists(filename):\n                    self.logger.info('skip downloading file %s', filename)\n                    return\n                self.fetched_num -= 1\n\n        while retry > 0 and not self.signal.get('reach_max_num'):\n            try:\n                response = self.session.get(file_url, timeout=timeout)\n            except Exception as e:\n                self.logger.error('Exception caught when downloading file %s, '\n                                  'error: %s, remaining retry times: %d',\n                                  file_url, e, retry - 1)\n            else:\n                if self.reach_max_num():\n                    self.signal.set(reach_max_num=True)\n                    break\n                elif response.status_code != 200:\n                    self.logger.error('Response status code %d, file %s',\n                                      response.status_code, file_url)\n                    break\n                elif not self.keep_file(task, response, **kwargs):\n                    break\n                with self.lock:\n                    self.fetched_num += 1\n                    filename = self.get_filename(task, default_ext)\n                self.logger.info('image #%s\\t%s', self.fetched_num, file_url)\n                self.storage.write(filename, response.content)\n                task['success'] = True\n                task['filename'] = filename\n                break\n            finally:\n                retry -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntarget method of workers. Get task from ``task_queue`` and then download files and process meta data. A downloader thread will exit in either of the following cases: 1. All parser threads have exited and the task_queue is empty. 2. Downloaded image number has reached required number(max_num). Args: queue_timeout (int): Timeout of getting tasks from ``task_queue``. req_timeout (int): Timeout of making requests for downloading pages. **kwargs: Arguments passed to the :func:`download` method.", "response": "def worker_exec(self,\n                    max_num,\n                    default_ext='',\n                    queue_timeout=5,\n                    req_timeout=5,\n                    **kwargs):\n        \"\"\"Target method of workers.\n\n        Get task from ``task_queue`` and then download files and process meta\n        data. A downloader thread will exit in either of the following cases:\n\n        1. All parser threads have exited and the task_queue is empty.\n        2. Downloaded image number has reached required number(max_num).\n\n        Args:\n            queue_timeout (int): Timeout of getting tasks from ``task_queue``.\n            req_timeout (int): Timeout of making requests for downloading pages.\n            **kwargs: Arguments passed to the :func:`download` method.\n        \"\"\"\n        self.max_num = max_num\n        while True:\n            if self.signal.get('reach_max_num'):\n                self.logger.info('downloaded images reach max num, thread %s'\n                                 ' is ready to exit',\n                                 current_thread().name)\n                break\n            try:\n                task = self.in_queue.get(timeout=queue_timeout)\n            except queue.Empty:\n                if self.signal.get('parser_exited'):\n                    self.logger.info('no more download task for thread %s',\n                                     current_thread().name)\n                    break\n                else:\n                    self.logger.info('%s is waiting for new download tasks',\n                                     current_thread().name)\n            except:\n                self.logger.error('exception in thread %s',\n                                  current_thread().name)\n            else:\n                self.download(task, default_ext, req_timeout, **kwargs)\n                self.process_meta(task)\n                self.in_queue.task_done()\n        self.logger.info('thread {} exit'.format(current_thread().name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keep_file(self, task, response, min_size=None, max_size=None):\n        try:\n            img = Image.open(BytesIO(response.content))\n        except (IOError, OSError):\n            return False\n        task['img_size'] = img.size\n        if min_size and not self._size_gt(img.size, min_size):\n            return False\n        if max_size and not self._size_lt(img.size, max_size):\n            return False\n        return True", "response": "Decide whether to keep the image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_logger(self, log_level=logging.INFO):\n        logging.basicConfig(\n            format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n            level=log_level,\n            stream=sys.stderr)\n        self.logger = logging.getLogger(__name__)\n        logging.getLogger('requests').setLevel(logging.WARNING)", "response": "Configure the logger with log_level."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_signal(self):\n        self.signal = Signal()\n        self.signal.set(\n            feeder_exited=False, parser_exited=False, reach_max_num=False)", "response": "Init signal\n\n        3 signals are added: ``feeder_exited``, ``parser_exited`` and\n        ``reach_max_num``."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the storage for downloader .", "response": "def set_storage(self, storage):\n        \"\"\"Set storage backend for downloader\n\n        For full list of storage backend supported, please see :mod:`storage`.\n\n        Args:\n            storage (dict or BaseStorage): storage backend configuration or instance\n\n        \"\"\"\n        if isinstance(storage, BaseStorage):\n            self.storage = storage\n        elif isinstance(storage, dict):\n            if 'backend' not in storage and 'root_dir' in storage:\n                storage['backend'] = 'FileSystem'\n            try:\n                backend_cls = getattr(storage_package, storage['backend'])\n            except AttributeError:\n                try:\n                    backend_cls = import_module(storage['backend'])\n                except ImportError:\n                    self.logger.error('cannot find backend module %s',\n                                      storage['backend'])\n                    sys.exit()\n            kwargs = storage.copy()\n            del kwargs['backend']\n            self.storage = backend_cls(**kwargs)\n        else:\n            raise TypeError('\"storage\" must be a storage object or dict')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_session(self, headers=None):\n        if headers is None:\n            headers = {\n                'User-Agent':\n                ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3)'\n                 ' AppleWebKit/537.36 (KHTML, like Gecko) '\n                 'Chrome/48.0.2564.116 Safari/537.36')\n            }\n        elif not isinstance(headers, dict):\n            raise TypeError('\"headers\" must be a dict object')\n\n        self.session = Session(self.proxy_pool)\n        self.session.headers.update(headers)", "response": "Init session with default or custom headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts the crawling thread and wait until all threads exit.", "response": "def crawl(self,\n              feeder_kwargs=None,\n              parser_kwargs=None,\n              downloader_kwargs=None):\n        \"\"\"Start crawling\n\n        This method will start feeder, parser and download and wait\n        until all threads exit.\n\n        Args:\n            feeder_kwargs (dict, optional): Arguments to be passed to ``feeder.start()``\n            parser_kwargs (dict, optional): Arguments to be passed to ``parser.start()``\n            downloader_kwargs (dict, optional): Arguments to be passed to\n                ``downloader.start()``\n        \"\"\"\n        self.signal.reset()\n        self.logger.info('start crawling...')\n\n        feeder_kwargs = {} if feeder_kwargs is None else feeder_kwargs\n        parser_kwargs = {} if parser_kwargs is None else parser_kwargs\n        downloader_kwargs = {} if downloader_kwargs is None else downloader_kwargs\n\n        self.logger.info('starting %d feeder threads...',\n                         self.feeder.thread_num)\n        self.feeder.start(**feeder_kwargs)\n\n        self.logger.info('starting %d parser threads...',\n                         self.parser.thread_num)\n        self.parser.start(**parser_kwargs)\n\n        self.logger.info('starting %d downloader threads...',\n                         self.downloader.thread_num)\n        self.downloader.start(**downloader_kwargs)\n\n        while True:\n            if not self.feeder.is_alive():\n                self.signal.set(feeder_exited=True)\n            if not self.parser.is_alive():\n                self.signal.set(parser_exited=True)\n            if not self.downloader.is_alive():\n                break\n            time.sleep(1)\n\n        if not self.feeder.in_queue.empty():\n            self.feeder.clear_buffer()\n        if not self.parser.in_queue.empty():\n            self.parser.clear_buffer()\n        if not self.downloader.in_queue.empty():\n            self.downloader.clear_buffer(True)\n\n        self.logger.info('Crawling task done!')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the indicator to use integer positions for this entry.", "response": "def use_integer_positions(self, integer_positions):\n        \"\"\"\n        Set indicator to use (or not) integer positions for a given strategy or\n        security.\n\n        By default all positions in number of stocks should be integer.\n        However this may lead to unexpected results when working with adjusted\n        prices of stocks. Because of series of reverse splits of stocks, the\n        adjusted prices back in time might be high. Thus rounding of desired\n        amount of stocks to buy may lead to having 0, and thus ignoring this\n        stock from backtesting.\n        \"\"\"\n        self.integer_positions = integer_positions\n        for c in self._childrenv:\n            c.use_integer_positions(integer_positions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the current value of the node.", "response": "def value(self):\n        \"\"\"\n        Current value of the Node\n        \"\"\"\n        if self.root.stale:\n            self.root.update(self.root.now, None)\n        return self._value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the weight of the node.", "response": "def weight(self):\n        \"\"\"\n        Current weight of the Node (with respect to the parent).\n        \"\"\"\n        if self.root.stale:\n            self.root.update(self.root.now, None)\n        return self._weight"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all the members of this node including current node and all its children.", "response": "def members(self):\n        \"\"\"\n        Node members. Members include current node as well as Node's\n        children.\n        \"\"\"\n        res = [self]\n        for c in list(self.children.values()):\n            res.extend(c.members)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the current price.", "response": "def price(self):\n        \"\"\"\n        Current price.\n        \"\"\"\n        if self.root.stale:\n            self.root.update(self.now, None)\n        return self._price"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prices(self):\n        if self.root.stale:\n            self.root.update(self.now, None)\n        return self._prices.loc[:self.now]", "response": "Returns a list of all the prices in the current time frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef values(self):\n        if self.root.stale:\n            self.root.update(self.now, None)\n        return self._values.loc[:self.now]", "response": "Return a list of all the values in the current time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the universe of the current time.", "response": "def universe(self):\n        \"\"\"\n        Data universe available at the current time.\n        Universe contains the data passed in when creating a Backtest.\n        Use this data to determine strategy logic.\n        \"\"\"\n        # avoid windowing every time\n        # if calling and on same date return\n        # cached value\n        if self.now == self._last_chk:\n            return self._funiverse\n        else:\n            self._last_chk = self.now\n            self._funiverse = self._universe.loc[:self.now]\n            return self._funiverse"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef outlays(self):\n        return pd.DataFrame({x.name: x.outlays for x in self.securities})", "response": "Returns a DataFrame of outlays for each child SecurityBase"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef positions(self):\n        # if accessing and stale - update first\n        if self.root.stale:\n            self.root.update(self.root.now, None)\n\n        vals = pd.DataFrame({x.name: x.positions for x in self.members\n                             if isinstance(x, SecurityBase)})\n        self._positions = vals\n        return vals", "response": "Returns a DataFrame of all the available positions for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the internal state of the object.", "response": "def update(self, date, data=None, inow=None):\n        \"\"\"\n        Update strategy. Updates prices, values, weight, etc.\n        \"\"\"\n        # resolve stale state\n        self.root.stale = False\n\n        # update helpers on date change\n        # also set newpt flag\n        newpt = False\n        if self.now == 0:\n            newpt = True\n        elif date != self.now:\n            self._net_flows = 0\n            self._last_price = self._price\n            self._last_value = self._value\n            self._last_fee = 0.0\n            newpt = True\n\n        # update now\n        self.now = date\n        if inow is None:\n            if self.now == 0:\n                inow = 0\n            else:\n                inow = self.data.index.get_loc(date)\n\n        # update children if any and calculate value\n        val = self._capital  # default if no children\n\n        if self.children is not None:\n            for c in self._childrenv:\n                # avoid useless update call\n                if c._issec and not c._needupdate:\n                    continue\n                c.update(date, data, inow)\n                val += c.value\n\n        if self.root == self:\n            if (val < 0) and not self.bankrupt:\n                # Declare a bankruptcy\n                self.bankrupt = True\n                self.flatten()\n\n        # update data if this value is different or\n        # if now has changed - avoid all this if not since it\n        # won't change\n        if newpt or self._value != val:\n            self._value = val\n            self._values.values[inow] = val\n\n            bottom = self._last_value + self._net_flows\n            if bottom != 0:\n                ret = self._value / (self._last_value + self._net_flows) - 1\n            else:\n                if self._value == 0:\n                    ret = 0\n                else:\n                    raise ZeroDivisionError(\n                        'Could not update %s. Last value '\n                        'was %s and net flows were %s. Current'\n                        'value is %s. Therefore, '\n                        'we are dividing by zero to obtain the return '\n                        'for the period.' % (self.name,\n                                             self._last_value,\n                                             self._net_flows,\n                                             self._value))\n\n            self._price = self._last_price * (1 + ret)\n            self._prices.values[inow] = self._price\n\n        # update children weights\n        if self.children is not None:\n            for c in self._childrenv:\n                # avoid useless update call\n                if c._issec and not c._needupdate:\n                    continue\n\n                if val != 0:\n                    c._weight = c.value / val\n                else:\n                    c._weight = 0.0\n\n        # if we have strategy children, we will need to update them in universe\n        if self._has_strat_children:\n            for c in self._strat_children:\n                # TODO: optimize \".loc\" here as well\n                self._universe.loc[date, c] = self.children[c].price\n\n        # Cash should track the unallocated capital at the end of the day, so\n        # we should update it every time we call \"update\".\n        # Same for fees\n        self._cash.values[inow] = self._capital\n        self._fees.values[inow] = self._last_fee\n\n        # update paper trade if necessary\n        if newpt and self._paper_trade:\n            self._paper.update(date)\n            self._paper.run()\n            self._paper.update(date)\n            # update price\n            self._price = self._paper.price\n            self._prices.values[inow] = self._price"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef adjust(self, amount, update=True, flow=True, fee=0.0):\n        # adjust capital\n        self._capital += amount\n        self._last_fee += fee\n\n        # if flow - increment net_flows - this will not affect\n        # performance. Commissions and other fees are not flows since\n        # they have a performance impact\n        if flow:\n            self._net_flows += amount\n\n        if update:\n            # indicates that data is now stale and must\n            # be updated before access\n            self.root.stale = True", "response": "Adjust the capital and amount of an anatomical entry in the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rebalance(self, weight, child, base=np.nan, update=True):\n        # if weight is 0 - we want to close child\n        if weight == 0:\n            if child in self.children:\n                return self.close(child)\n            else:\n                return\n\n        # if no base specified use self's value\n        if np.isnan(base):\n            base = self.value\n\n        # else make sure we have child\n        if child not in self.children:\n            c = SecurityBase(child)\n            c.setup(self._universe)\n            # update child to bring up to speed\n            c.update(self.now)\n            self._add_child(c)\n\n        # allocate to child\n        # figure out weight delta\n        c = self.children[child]\n        delta = weight - c.weight\n        c.allocate(delta * base)", "response": "Rebalance a child to a given weight."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose a child position - alias for rebalance(0, child). This will also flatten (close out all) the child's children. Args: * child (str): Child, specified by name.", "response": "def close(self, child):\n        \"\"\"\n        Close a child position - alias for rebalance(0, child). This will also\n        flatten (close out all) the child's children.\n\n        Args:\n            * child (str): Child, specified by name.\n        \"\"\"\n        c = self.children[child]\n        # flatten if children not None\n        if c.children is not None and len(c.children) != 0:\n            c.flatten()\n\n        if c.value != 0. and not np.isnan(c.value):\n            c.allocate(-c.value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all child identifiers in the tree.", "response": "def flatten(self):\n        \"\"\"\n        Close all child positions.\n        \"\"\"\n        # go right to base alloc\n        [c.allocate(-c.value) for c in self._childrenv if c.value != 0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the commission function for this object.", "response": "def set_commissions(self, fn):\n        \"\"\"\n        Set commission (transaction fee) function.\n\n        Args:\n            fn (fn(quantity, price)): Function used to determine commission\n            amount.\n\n        \"\"\"\n        self.commission_fn = fn\n\n        for c in self._childrenv:\n            if isinstance(c, StrategyBase):\n                c.set_commissions(fn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef price(self):\n        # if accessing and stale - update first\n        if self._needupdate or self.now != self.parent.now:\n            self.update(self.root.now)\n        return self._price", "response": "Returns the current price of the current item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of all the prices for this instance.", "response": "def prices(self):\n        \"\"\"\n        TimeSeries of prices.\n        \"\"\"\n        # if accessing and stale - update first\n        if self._needupdate or self.now != self.parent.now:\n            self.update(self.root.now)\n        return self._prices.loc[:self.now]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef values(self):\n        # if accessing and stale - update first\n        if self._needupdate or self.now != self.parent.now:\n            self.update(self.root.now)\n        if self.root.stale:\n            self.root.update(self.root.now, None)\n        return self._values.loc[:self.now]", "response": "Return a list of all the values in this instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef positions(self):\n        # if accessing and stale - update first\n        if self._needupdate:\n            self.update(self.root.now)\n        if self.root.stale:\n            self.root.update(self.root.now, None)\n        return self._positions.loc[:self.now]", "response": "Get a list of all the time series of the current set of entries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup(self, universe):\n        # if we already have all the prices, we will store them to speed up\n        # future updates\n        try:\n            prices = universe[self.name]\n        except KeyError:\n            prices = None\n\n        # setup internal data\n        if prices is not None:\n            self._prices = prices\n            self.data = pd.DataFrame(index=universe.index,\n                                     columns=['value', 'position'],\n                                     data=0.0)\n            self._prices_set = True\n        else:\n            self.data = pd.DataFrame(index=universe.index,\n                                     columns=['price', 'value', 'position'])\n            self._prices = self.data['price']\n            self._prices_set = False\n\n        self._values = self.data['value']\n        self._positions = self.data['position']\n\n        # add _outlay\n        self.data['outlay'] = 0.\n        self._outlays = self.data['outlay']", "response": "Setup Security with universe. Speeds up future runs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the security with a given date and optionally some data.", "response": "def update(self, date, data=None, inow=None):\n        \"\"\"\n        Update security with a given date and optionally, some data.\n        This will update price, value, weight, etc.\n        \"\"\"\n        # filter for internal calls when position has not changed - nothing to\n        # do. Internal calls (stale root calls) have None data. Also want to\n        # make sure date has not changed, because then we do indeed want to\n        # update.\n        if date == self.now and self._last_pos == self._position:\n            return\n\n        if inow is None:\n            if date == 0:\n                inow = 0\n            else:\n                inow = self.data.index.get_loc(date)\n\n        # date change - update price\n        if date != self.now:\n            # update now\n            self.now = date\n\n            if self._prices_set:\n                self._price = self._prices.values[inow]\n            # traditional data update\n            elif data is not None:\n                prc = data[self.name]\n                self._price = prc\n                self._prices.values[inow] = prc\n\n        self._positions.values[inow] = self._position\n        self._last_pos = self._position\n\n        if np.isnan(self._price):\n            if self._position == 0:\n                self._value = 0\n            else:\n                raise Exception(\n                    'Position is open (non-zero) and latest price is NaN '\n                    'for security %s. Cannot update node value.' % self.name)\n        else:\n            self._value = self._position * self._price * self.multiplier\n\n        self._values.values[inow] = self._value\n\n        if self._weight == 0 and self._position == 0:\n            self._needupdate = False\n\n        # save outlay to outlays\n        if self._outlay != 0:\n            self._outlays.values[inow] = self._outlay\n            # reset outlay back to 0\n            self._outlay = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef outlay(self, q):\n        fee = self.commission(q, self._price * self.multiplier)\n        outlay = q * self._price * self.multiplier\n        return outlay + fee, outlay, fee", "response": "Determines the complete cash outlay including commission itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the name of the object.", "response": "def name(self):\n        \"\"\"\n        Algo name.\n        \"\"\"\n        if self._name is None:\n            self._name = self.__class__.__name__\n        return self._name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_notebooks():\n    convert_status = call(['ipython', 'nbconvert', '--to', 'rst', '*.ipynb'])\n    if convert_status != 0:\n        raise SystemError('Conversion failed! Status was %s' % convert_status)\n\n    notebooks = [x for x in os.listdir('.') if '.ipynb'\n                 in x and os.path.isfile(x)]\n    names = [os.path.splitext(x)[0] for x in notebooks]\n\n    for i in range(len(notebooks)):\n        name = names[i]\n        notebook = notebooks[i]\n\n        print('processing %s (%s)' % (name, notebook))\n\n        # move static files\n        sdir = '%s_files' % name\n        statics = os.listdir(sdir)\n        statics = [os.path.join(sdir, x) for x in statics]\n        [shutil.copy(x, '_static/') for x in statics]\n        shutil.rmtree(sdir)\n\n        # rename static dir in rst file\n        rst_file = '%s.rst' % name\n        print('REsT file is %s' % rst_file)\n        data = None\n        with open(rst_file, 'r') as f:\n            data = f.read()\n\n        if data is not None:\n            with open(rst_file, 'w') as f:\n                data = re.sub('%s' % sdir, '_static', data)\n                f.write(data)\n\n        # add special tags\n        lines = None\n        with open(rst_file, 'r') as f:\n            lines = f.readlines()\n\n        if lines is not None:\n            n = len(lines)\n            i = 0\n            rawWatch = False\n\n            while i < n:\n                line = lines[i]\n                # add class tags to images for css formatting\n                if 'image::' in line:\n                    lines.insert(i + 1, '    :class: pynb\\n')\n                    n += 1\n                elif 'parsed-literal::' in line:\n                    lines.insert(i + 1, '    :class: pynb-result\\n')\n                    n += 1\n                elif 'raw:: html' in line:\n                    rawWatch = True\n\n                if rawWatch:\n                    if '<div' in line:\n                        line = line.replace('<div', '<div class=\"pynb-result\"')\n                        lines[i] = line\n                        rawWatch = False\n\n                i += 1\n\n            with open(rst_file, 'w') as f:\n                f.writelines(lines)", "response": "Converts IPython Notebooks to proper. rst files and moves static files to the _static directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _spectrogram_mono(self, x):\n        '''x.shape : (None, 1, len_src),\n        returns 2D batch of a mono power-spectrogram'''\n        x = K.permute_dimensions(x, [0, 2, 1])\n        x = K.expand_dims(x, 3)  # add a dummy dimension (channel axis)\n        subsample = (self.n_hop, 1)\n        output_real = K.conv2d(x, self.dft_real_kernels,\n                               strides=subsample,\n                               padding=self.padding,\n                               data_format='channels_last')\n        output_imag = K.conv2d(x, self.dft_imag_kernels,\n                               strides=subsample,\n                               padding=self.padding,\n                               data_format='channels_last')\n        output = output_real ** 2 + output_imag ** 2\n        # now shape is (batch_sample, n_frame, 1, freq)\n        if self.image_data_format == 'channels_last':\n            output = K.permute_dimensions(output, [0, 3, 1, 2])\n        else:\n            output = K.permute_dimensions(output, [0, 2, 3, 1])\n        return output", "response": "This function computes the mono power - spectrogram of a single image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mel(sr, n_dft, n_mels=128, fmin=0.0, fmax=None, htk=False, norm=1):\n    return librosa.filters.mel(sr=sr, n_fft=n_dft, n_mels=n_mels,\n                               fmin=fmin, fmax=fmax,\n                               htk=htk, norm=norm).astype(K.floatx())", "response": "create a filterbank matrix to combine stft bins into mel - frequency bins"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the kernels for real and imaginary parts assuming the input. is real.", "response": "def get_stft_kernels(n_dft):\n    \"\"\"[np] Return dft kernels for real/imagnary parts assuming\n        the input . is real.\n    An asymmetric hann window is used (scipy.signal.hann).\n\n    Parameters\n    ----------\n    n_dft : int > 0 and power of 2 [scalar]\n        Number of dft components.\n\n    Returns\n    -------\n        |  dft_real_kernels : np.ndarray [shape=(nb_filter, 1, 1, n_win)]\n        |  dft_imag_kernels : np.ndarray [shape=(nb_filter, 1, 1, n_win)]\n\n    * nb_filter = n_dft/2 + 1\n    * n_win = n_dft\n\n    \"\"\"\n    assert n_dft > 1 and ((n_dft & (n_dft - 1)) == 0), \\\n        ('n_dft should be > 1 and power of 2, but n_dft == %d' % n_dft)\n\n    nb_filter = int(n_dft // 2 + 1)\n\n    # prepare DFT filters\n    timesteps = np.array(range(n_dft))\n    w_ks = np.arange(nb_filter) * 2 * np.pi / float(n_dft)\n    dft_real_kernels = np.cos(w_ks.reshape(-1, 1) * timesteps.reshape(1, -1))\n    dft_imag_kernels = -np.sin(w_ks.reshape(-1, 1) * timesteps.reshape(1, -1))\n\n    # windowing DFT filters\n    dft_window = librosa.filters.get_window('hann', n_dft, fftbins=True)  # _hann(n_dft, sym=False)\n    dft_window = dft_window.astype(K.floatx())\n    dft_window = dft_window.reshape((1, -1))\n    dft_real_kernels = np.multiply(dft_real_kernels, dft_window)\n    dft_imag_kernels = np.multiply(dft_imag_kernels, dft_window)\n\n    dft_real_kernels = dft_real_kernels.transpose()\n    dft_imag_kernels = dft_imag_kernels.transpose()\n    dft_real_kernels = dft_real_kernels[:, np.newaxis, np.newaxis, :]\n    dft_imag_kernels = dft_imag_kernels[:, np.newaxis, np.newaxis, :]\n\n    return dft_real_kernels.astype(K.floatx()), dft_imag_kernels.astype(K.floatx())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filterbank_log(sr, n_freq, n_bins=84, bins_per_octave=12,\n                   fmin=None, spread=0.125):  # pragma: no cover\n    \"\"\"[np] Approximate a constant-Q filter bank for a fixed-window STFT.\n\n    Each filter is a log-normal window centered at the corresponding frequency.\n\n    Note: `logfrequency` in librosa 0.4 (deprecated), so copy-and-pasted,\n        `tuning` was removed, `n_freq` instead of `n_fft`.\n\n    Parameters\n    ----------\n    sr : number > 0 [scalar]\n        audio sampling rate\n\n    n_freq : int > 0 [scalar]\n        number of frequency bins\n\n    n_bins : int > 0 [scalar]\n        Number of bins.  Defaults to 84 (7 octaves).\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave. Defaults to 12 (semitones).\n\n    fmin : float > 0 [scalar]\n        Minimum frequency bin. Defaults to `C1 ~= 32.70`\n\n    spread : float > 0 [scalar]\n        Spread of each filter, as a fraction of a bin.\n\n    Returns\n    -------\n    C : np.ndarray [shape=(n_bins, 1 + n_fft/2)]\n        log-frequency filter bank.\n    \"\"\"\n\n    if fmin is None:\n        fmin = 32.70319566\n\n    # What's the shape parameter for our log-normal filters?\n    sigma = float(spread) / bins_per_octave\n\n    # Construct the output matrix\n    basis = np.zeros((n_bins, n_freq))\n\n    # Get log frequencies of bins\n    log_freqs = np.log2(librosa.fft_frequencies(sr, (n_freq - 1) * 2)[1:])\n\n    for i in range(n_bins):\n        # What's the center (median) frequency of this filter?\n        c_freq = fmin * (2.0 ** (float(i) / bins_per_octave))\n\n        # Place a log-normal window around c_freq\n        basis[i, 1:] = np.exp(-0.5 * ((log_freqs - np.log2(c_freq)) / sigma) ** 2\n                              - np.log2(sigma) - log_freqs)\n\n    # Normalize the filters\n    basis = librosa.util.normalize(basis, norm=1, axis=1)\n\n    return basis.astype(K.floatx())", "response": "Return a constant - Q filter bank for a fixed - window STFT."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef amplitude_to_decibel(x, amin=1e-10, dynamic_range=80.0):\n    log_spec = 10 * K.log(K.maximum(x, amin)) / np.log(10).astype(K.floatx())\n    if K.ndim(x) > 1:\n        axis = tuple(range(K.ndim(x))[1:])\n    else:\n        axis = None\n\n    log_spec = log_spec - K.max(log_spec, axis=axis, keepdims=True)  # [-?, 0]\n    log_spec = K.maximum(log_spec, -1 * dynamic_range)  # [-80, 0]\n    return log_spec", "response": "Convert linear amplitude to decibel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the settings dictionary", "response": "def load(self, local='localsettings.py', default='settings.py'):\n        '''\n        Load the settings dict\n\n        @param local: The local settings filename to use\n        @param default: The default settings module to read\n        @return: A dict of the loaded settings\n        '''\n        self._load_defaults(default)\n        self._load_custom(local)\n\n        return self.settings()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_from_string(self, settings_string='', module_name='customsettings'):\n        '''\n        Loads settings from a settings_string. Expects an escaped string like\n        the following:\n            \"NAME=\\'stuff\\'\\nTYPE=[\\'item\\']\\n\"\n\n        @param settings_string: The string with your settings\n        @return: A dict of loaded settings\n        '''\n        try:\n            mod = imp.new_module(module_name)\n            exec(settings_string, mod.__dict__)\n        except TypeError:\n            log.warning(\"Could not import settings\")\n        self.my_settings = {}\n        try:\n            self.my_settings = self._convert_to_dict(mod)\n        except ImportError:\n            log.warning(\"Settings unable to be loaded\")\n\n        return self.settings()", "response": "Loads settings from a string. Expects an escaped string like\n            NAME = stuff TYPE = item \\ n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the default settings", "response": "def _load_defaults(self, default='settings.py'):\n        '''\n        Load the default settings\n        '''\n        if default[-3:] == '.py':\n            default = default[:-3]\n\n        self.my_settings = {}\n        try:\n            settings = importlib.import_module(default)\n            self.my_settings = self._convert_to_dict(settings)\n        except ImportError:\n            log.warning(\"No default settings found\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_custom(self, settings_name='localsettings.py'):\n        '''\n        Load the user defined settings, overriding the defaults\n\n        '''\n        if settings_name[-3:] == '.py':\n            settings_name = settings_name[:-3]\n\n        new_settings = {}\n        try:\n            settings = importlib.import_module(settings_name)\n            new_settings = self._convert_to_dict(settings)\n        except ImportError:\n            log.info(\"No override settings found\")\n\n        for key in new_settings:\n            if key in self.my_settings:\n                item = new_settings[key]\n                if isinstance(item, dict) and \\\n                        isinstance(self.my_settings[key], dict):\n                    for key2 in item:\n                        self.my_settings[key][key2] = item[key2]\n                else:\n                    self.my_settings[key] = item\n            else:\n                self.my_settings[key] = new_settings[key]", "response": "Load the user defined settings overriding the defaults"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a settings file into a dictionary ignoring python defaults", "response": "def _convert_to_dict(self, setting):\n        '''\n        Converts a settings file into a dictionary, ignoring python defaults\n\n        @param setting: A loaded setting module\n        '''\n        the_dict = {}\n        set = dir(setting)\n        for key in set:\n            if key in self.ignore:\n                continue\n            value = getattr(setting, key)\n            the_dict[key] = value\n\n        return the_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_domain_config(self, loaded_config):\n        '''\n        Loads the domain_config and sets up queue_dict\n        @param loaded_config: the yaml loaded config dict from zookeeper\n        '''\n        self.domain_config = {}\n        # vetting process to ensure correct configs\n        if loaded_config:\n            if 'domains' in loaded_config:\n                for domain in loaded_config['domains']:\n                    item = loaded_config['domains'][domain]\n                    # check valid\n                    if 'window' in item and 'hits' in item:\n                        self.logger.debug(\"Added domain {dom} to loaded config\"\n                                          .format(dom=domain))\n                        self.domain_config[domain] = item\n            if 'blacklist' in loaded_config:\n                self.black_domains = loaded_config['blacklist']\n\n        self.config_flag = True", "response": "Loads the domain_config and sets up queue_dict\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the queues in memory with the new ones from the domain config.", "response": "def update_domain_queues(self):\n        '''\n        Check to update existing queues already in memory\n        new queues are created elsewhere\n        '''\n        for key in self.domain_config:\n            final_key = \"{name}:{domain}:queue\".format(\n                    name=self.spider.name,\n                    domain=key)\n            # we already have a throttled queue for this domain, update it to new settings\n            if final_key in self.queue_dict:\n                self.queue_dict[final_key][0].window = float(self.domain_config[key]['window'])\n                self.logger.debug(\"Updated queue {q} with new config\"\n                                  .format(q=final_key))\n                # if scale is applied, scale back; otherwise use updated hits\n                if 'scale' in self.domain_config[key]:\n                    # round to int\n                    hits = int(self.domain_config[key]['hits'] * self.fit_scale(\n                               self.domain_config[key]['scale']))\n                    self.queue_dict[final_key][0].limit = float(hits)\n                else:\n                    self.queue_dict[final_key][0].limit = float(self.domain_config[key]['hits'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the in memory list of the redis queues that are not already in memory and adds new throttled queues if it does not have them", "response": "def create_queues(self):\n        '''\n        Updates the in memory list of the redis queues\n        Creates new throttled queue instances if it does not have them\n        '''\n        # new config could have loaded between scrapes\n        newConf = self.check_config()\n\n        self.queue_keys = self.redis_conn.keys(self.spider.name + \":*:queue\")\n\n        for key in self.queue_keys:\n            # build final queue key, depending on type and ip bools\n            throttle_key = \"\"\n\n            if self.add_type:\n                throttle_key = self.spider.name + \":\"\n            if self.add_ip:\n                throttle_key = throttle_key + self.my_ip + \":\"\n\n            # add the tld from the key `type:tld:queue`\n            the_domain = re.split(':', key)[1]\n            throttle_key = throttle_key + the_domain\n\n            if key not in self.queue_dict or newConf:\n                self.logger.debug(\"Added new Throttled Queue {q}\"\n                                  .format(q=key))\n                q = RedisPriorityQueue(self.redis_conn, key, encoding=ujson)\n\n                # use default window and hits\n                if the_domain not in self.domain_config:\n                    # this is now a tuple, all access needs to use [0] to get\n                    # the object, use [1] to get the time\n                    self.queue_dict[key] = [RedisThrottledQueue(self.redis_conn,\n                    q, self.window, self.hits, self.moderated, throttle_key,\n                    throttle_key, True), time.time()]\n                # use custom window and hits\n                else:\n                    window = self.domain_config[the_domain]['window']\n                    hits = self.domain_config[the_domain]['hits']\n\n                    # adjust the crawl rate based on the scale if exists\n                    if 'scale' in self.domain_config[the_domain]:\n                        hits = int(hits * self.fit_scale(self.domain_config[the_domain]['scale']))\n\n                    self.queue_dict[key] = [RedisThrottledQueue(self.redis_conn,\n                    q, window, hits, self.moderated, throttle_key,\n                    throttle_key, True), time.time()]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexpire old queue_dict keys that have not been used in a long time.", "response": "def expire_queues(self):\n        '''\n        Expires old queue_dict keys that have not been used in a long time.\n        Prevents slow memory build up when crawling lots of different domains\n        '''\n        curr_time = time.time()\n        for key in list(self.queue_dict):\n            diff = curr_time - self.queue_dict[key][1]\n            if diff > self.queue_timeout:\n                self.logger.debug(\"Expiring domain queue key \" + key)\n                del self.queue_dict[key]\n                if key in self.queue_keys:\n                    self.queue_keys.remove(key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_ipaddress(self):\n        '''\n        Updates the scheduler so it knows its own ip address\n        '''\n        # assign local ip in case of exception\n        self.old_ip = self.my_ip\n        self.my_ip = '127.0.0.1'\n        try:\n            obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL',\n                                  'http://ip.42.pl/raw'))\n            results = self.ip_regex.findall(obj.read())\n            if len(results) > 0:\n                self.my_ip = results[0]\n            else:\n                raise IOError(\"Could not get valid IP Address\")\n            obj.close()\n            self.logger.debug(\"Current public ip: {ip}\".format(ip=self.my_ip))\n        except IOError:\n            self.logger.error(\"Could not reach out to get public ip\")\n            pass\n\n        if self.old_ip != self.my_ip:\n            self.logger.info(\"Changed Public IP: {old} -> {new}\".format(\n                             old=self.old_ip, new=self.my_ip))", "response": "Updates the scheduler so it knows its own ip address\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report_self(self):\n        '''\n        Reports the crawler uuid to redis\n        '''\n        self.logger.debug(\"Reporting self id\", extra={'uuid':self.my_uuid})\n        key = \"stats:crawler:{m}:{s}:{u}\".format(\n            m=socket.gethostname(),\n            s=self.spider.name,\n            u=self.my_uuid)\n        self.redis_conn.set(key, time.time())\n        self.redis_conn.expire(key, self.ip_update_interval * 2)", "response": "Reports the crawler uuid to redis\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the redis blacklist for crawls that should not be propagated either from expiring or stopped", "response": "def is_blacklisted(self, appid, crawlid):\n        '''\n        Checks the redis blacklist for crawls that should not be propagated\n        either from expiring or stopped\n        @return: True if the appid crawlid combo is blacklisted\n        '''\n        key_check = '{appid}||{crawlid}'.format(appid=appid,\n                                                crawlid=crawlid)\n        redis_key = self.spider.name + \":blacklist\"\n        return self.redis_conn.sismember(redis_key, key_check)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a request into the throttled queue", "response": "def enqueue_request(self, request):\n        '''\n        Pushes a request from the spider into the proper throttled queue\n        '''\n        if not request.dont_filter and self.dupefilter.request_seen(request):\n            self.logger.debug(\"Request not added back to redis\")\n            return\n        req_dict = self.request_to_dict(request)\n\n        if not self.is_blacklisted(req_dict['meta']['appid'],\n                                   req_dict['meta']['crawlid']):\n            # grab the tld of the request\n            ex_res = self.extract(req_dict['url'])\n            key = \"{sid}:{dom}.{suf}:queue\".format(\n                sid=req_dict['meta']['spiderid'],\n                dom=ex_res.domain,\n                suf=ex_res.suffix)\n\n            curr_time = time.time()\n\n            domain = \"{d}.{s}\".format(d=ex_res.domain, s=ex_res.suffix)\n\n            # allow only if we want all requests or we want\n            # everything but blacklisted domains\n            # insert if crawl never expires (0) or time < expires\n            if (self.backlog_blacklist or\n                    (not self.backlog_blacklist and\n                    domain not in self.black_domains)) and \\\n                    (req_dict['meta']['expires'] == 0 or\n                    curr_time < req_dict['meta']['expires']):\n                # we may already have the queue in memory\n                if key in self.queue_keys:\n                    self.queue_dict[key][0].push(req_dict,\n                                              req_dict['meta']['priority'])\n                else:\n                    # shoving into a new redis queue, negative b/c of sorted sets\n                    # this will populate ourself and other schedulers when\n                    # they call create_queues\n                    self.redis_conn.zadd(key, ujson.dumps(req_dict),\n                                        -req_dict['meta']['priority'])\n                self.logger.debug(\"Crawlid: '{id}' Appid: '{appid}' added to queue\"\n                    .format(appid=req_dict['meta']['appid'],\n                            id=req_dict['meta']['crawlid']))\n            else:\n                self.logger.debug(\"Crawlid: '{id}' Appid: '{appid}' expired\"\n                                  .format(appid=req_dict['meta']['appid'],\n                                          id=req_dict['meta']['crawlid']))\n        else:\n            self.logger.debug(\"Crawlid: '{id}' Appid: '{appid}' blacklisted\"\n                              .format(appid=req_dict['meta']['appid'],\n                                      id=req_dict['meta']['crawlid']))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_to_dict(self, request):\n        '''\n        Convert Request object to a dict.\n        modified from scrapy.utils.reqser\n        '''\n        req_dict = {\n            # urls should be safe (safe_string_url)\n            'url': to_unicode(request.url),\n            'method': request.method,\n            'headers': dict(request.headers),\n            'body': request.body,\n            'cookies': request.cookies,\n            'meta': request.meta,\n            '_encoding': request._encoding,\n            'priority': request.priority,\n            'dont_filter': request.dont_filter,\n             #  callback/errback are assumed to be a bound instance of the spider\n            'callback': None if request.callback is None else request.callback.__name__,\n            'errback': None if request.errback is None else request.errback.__name__,\n        }\n        return req_dict", "response": "Convert a request object to a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_item(self):\n        '''\n        Finds an item from the throttled queues\n        '''\n        random.shuffle(self.queue_keys)\n        count = 0\n\n        while count <= self.item_retries:\n            for key in self.queue_keys:\n                # skip if the whole domain has been blacklisted in zookeeper\n                if key.split(':')[1] in self.black_domains:\n                    continue\n                # the throttled queue only returns an item if it is allowed\n                item = self.queue_dict[key][0].pop()\n\n                if item:\n                    # update timeout and return\n                    self.queue_dict[key][1] = time.time()\n                    return item\n\n            count = count + 1\n\n        return None", "response": "Finds an item from the throttled queues\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next_request(self):\n        '''\n        Logic to handle getting a new url request, from a bunch of\n        different queues\n        '''\n        t = time.time()\n        # update the redis queues every so often\n        if t - self.update_time > self.update_interval:\n            self.update_time = t\n            self.create_queues()\n            self.expire_queues()\n\n        # update the ip address every so often\n        if t - self.update_ip_time > self.ip_update_interval:\n            self.update_ip_time = t\n            self.update_ipaddress()\n            self.report_self()\n\n        item = self.find_item()\n        if item:\n            self.logger.debug(\"Found url to crawl {url}\" \\\n                    .format(url=item['url']))\n            try:\n                req = Request(item['url'])\n            except ValueError:\n                # need absolute url\n                # need better url validation here\n                req = Request('http://' + item['url'])\n\n            try:\n                if 'callback' in item and item['callback'] is not None:\n                    req.callback = getattr(self.spider, item['callback'])\n            except AttributeError:\n                self.logger.warn(\"Unable to find callback method\")\n\n            try:\n                if 'errback' in item and item['errback'] is not None:\n                    req.errback = getattr(self.spider, item['errback'])\n            except AttributeError:\n                self.logger.warn(\"Unable to find errback method\")\n\n            if 'meta' in item:\n                item = item['meta']\n\n            # defaults not in schema\n            if 'curdepth' not in item:\n                item['curdepth'] = 0\n            if \"retry_times\" not in item:\n                item['retry_times'] = 0\n\n            for key in list(item.keys()):\n                req.meta[key] = item[key]\n\n            # extra check to add items to request\n            if 'useragent' in item and item['useragent'] is not None:\n                req.headers['User-Agent'] = item['useragent']\n            if 'cookie' in item and item['cookie'] is not None:\n                if isinstance(item['cookie'], dict):\n                    req.cookies = item['cookie']\n                elif isinstance(item['cookie'], basestring):\n                    req.cookies = self.parse_cookie(item['cookie'])\n\n            return req\n\n        return None", "response": "This method handles getting a new url request from a bunch of queues and creates a new Redis queue and updates the ip address every so often\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a cookie string like returned in a Set - Cookie header and returns a dictionary of the key - value pairs.", "response": "def parse_cookie(self, string):\n        '''\n        Parses a cookie string like returned in a Set-Cookie header\n        @param string: The cookie string\n        @return: the cookie dict\n        '''\n        results = re.findall('([^=]+)=([^\\;]+);?\\s?', string)\n        my_dict = {}\n        for item in results:\n            my_dict[item[0]] = item[1]\n\n        return my_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean the item to be logged", "response": "def _clean_item(self, item):\n        '''\n        Cleans the item to be logged\n        '''\n        item_copy = dict(item)\n        del item_copy['body']\n        del item_copy['links']\n        del item_copy['response_headers']\n        del item_copy['request_headers']\n        del item_copy['status_code']\n        del item_copy['status_msg']\n        item_copy['action'] = 'ack'\n        item_copy['logger'] = self.logger.name\n        item_copy\n\n        return item_copy"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _kafka_success(self, item, spider, response):\n        '''\n        Callback for successful send\n        '''\n        item['success'] = True\n        item = self._clean_item(item)\n        item['spiderid'] = spider.name\n        self.logger.info(\"Sent page to Kafka\", item)", "response": "Callback for successful send\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npurposing of this method is to reconstruct the headers dictionary that is normally passed in with a \"response\" object from scrapy. Args: response: A scrapy response object Returns: A dictionary that mirrors the \"response.headers\" dictionary that is normally within a response object Raises: None Reason: Originally, there was a bug where the json.dumps() did not properly serialize the headers. This method is a way to circumvent the known issue", "response": "def reconstruct_headers(self, response):\n        \"\"\"\n        Purpose of this method is to reconstruct the headers dictionary that\n        is normally passed in with a \"response\" object from scrapy.\n\n        Args:\n            response: A scrapy response object\n\n        Returns: A dictionary that mirrors the \"response.headers\" dictionary\n        that is normally within a response object\n\n        Raises: None\n        Reason: Originally, there was a bug where the json.dumps() did not\n        properly serialize the headers. This method is a way to circumvent\n        the known issue\n        \"\"\"\n\n        header_dict = {}\n        # begin reconstructing headers from scratch...\n        for key in list(response.headers.keys()):\n            key_item_list = []\n            key_list = response.headers.getlist(key)\n            for item in key_list:\n                key_item_list.append(item)\n            header_dict[key] = key_item_list\n        return header_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a vaild crawl request and add it to the redis queue", "response": "def handle(self, dict):\n        '''\n        Processes a vaild crawl request\n\n        @param dict: a valid dictionary object\n        '''\n        # format key\n        ex_res = self.extract(dict['url'])\n        key = \"{sid}:{dom}.{suf}:queue\".format(\n            sid=dict['spiderid'],\n            dom=ex_res.domain,\n            suf=ex_res.suffix)\n\n        val = ujson.dumps(dict)\n\n        # shortcut to shove stuff into the priority queue\n        self.redis_conn.zadd(key, val, -dict['priority'])\n\n        # if timeout crawl, add value to redis\n        if 'expires' in dict and dict['expires'] != 0:\n            key = \"timeout:{sid}:{appid}:{crawlid}\".format(\n                            sid=dict['spiderid'],\n                            appid=dict['appid'],\n                            crawlid=dict['crawlid'])\n            self.redis_conn.set(key, dict['expires'])\n\n        # log success\n        dict['parsed'] = True\n        dict['valid'] = True\n        self.logger.info('Added crawl to Redis', extra=dict)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef timeout(timeout_time, default):\n        '''\n        Decorate a method so it is required to execute in a given time period,\n        or return a default value.\n        '''\n        def timeout_function(f):\n            def f2(*args):\n                def timeout_handler(signum, frame):\n                    raise MethodTimer.DecoratorTimeout()\n\n                old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n                # triger alarm in timeout_time seconds\n                signal.alarm(timeout_time)\n                try:\n                    retval = f(*args)\n                except MethodTimer.DecoratorTimeout:\n                    return default\n                finally:\n                    signal.signal(signal.SIGALRM, old_handler)\n                signal.alarm(0)\n                return retval\n            return f2\n        return timeout_function", "response": "Decorator for creating a method so it is required to execute in a given time period."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset up the instance variables for the scraper handler.", "response": "def setup(self, settings):\n        '''\n        Does the actual setup of the middleware\n        '''\n        # set up the default sc logger\n        my_level = settings.get('SC_LOG_LEVEL', 'INFO')\n        my_name = settings.get('SClogger_NAME', 'sc-logger')\n        my_output = settings.get('SC_LOG_STDOUT', True)\n        my_json = settings.get('SC_LOG_JSON', False)\n        my_dir = settings.get('SC_LOG_DIR', 'logs')\n        my_bytes = settings.get('SC_LOG_MAX_BYTES', '10MB')\n        my_file = settings.get('SC_LOG_FILE', 'main.log')\n        my_backups = settings.get('SC_LOG_BACKUPS', 5)\n\n        self.logger = LogFactory.get_instance(json=my_json,\n                                         name=my_name,\n                                         stdout=my_output,\n                                         level=my_level,\n                                         dir=my_dir,\n                                         file=my_file,\n                                         bytes=my_bytes,\n                                         backups=my_backups)\n\n        self.settings = settings\n        self.stats_dict = {}\n\n        # set up redis\n        self.redis_conn = redis.Redis(host=settings.get('REDIS_HOST'),\n            port=settings.get('REDIS_PORT'),\n            db=settings.get('REDIS_DB'))\n\n        try:\n            self.redis_conn.info()\n            self.logger.debug(\"Connected to Redis in ScraperHandler\")\n        except ConnectionError:\n            self.logger.error(\"Failed to connect to Redis in Stats Middleware\")\n            # plugin is essential to functionality\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets up the status codes stats collectors and the stats collectors for the given spider.", "response": "def _setup_stats_status_codes(self, spider_name):\n        '''\n        Sets up the status code stats collectors\n        '''\n        self.stats_dict[spider_name] = {\n            'status_codes': {}\n        }\n        self.stats_dict[spider_name]['status_codes'] = {}\n        hostname = self._get_hostname()\n        # we chose to handle 504's here as well as in the middleware\n        # in case the middleware is disabled\n        for status_code in self.settings['STATS_RESPONSE_CODES']:\n            temp_key = 'stats:crawler:{h}:{n}:{s}'.format(\n                h=hostname, n=spider_name, s=status_code)\n            self.stats_dict[spider_name]['status_codes'][status_code] = {}\n            for item in self.settings['STATS_TIMES']:\n                try:\n                    time = getattr(StatsCollector, item)\n\n                    self.stats_dict[spider_name]['status_codes'][status_code][time] = StatsCollector \\\n                            .get_rolling_time_window(\n                                    redis_conn=self.redis_conn,\n                                    key='{k}:{t}'.format(k=temp_key, t=time),\n                                    window=time,\n                                    cycle_time=self.settings['STATS_CYCLE'])\n                    self.logger.debug(\"Set up status code {s}, {n} spider,\"\\\n                        \" host {h} Stats Collector '{i}'\"\\\n                        .format(h=hostname, n=spider_name, s=status_code, i=item))\n                except AttributeError as e:\n                    self.logger.warning(\"Unable to find Stats Time '{s}'\"\\\n                            .format(s=item))\n            total = StatsCollector.get_hll_counter(redis_conn=self.redis_conn,\n                            key='{k}:lifetime'.format(k=temp_key),\n                            cycle_time=self.settings['STATS_CYCLE'],\n                            roll=False)\n            self.logger.debug(\"Set up status code {s}, {n} spider,\"\\\n                        \"host {h} Stats Collector 'lifetime'\"\\\n                            .format(h=hostname, n=spider_name, s=status_code))\n            self.stats_dict[spider_name]['status_codes'][status_code]['lifetime'] = total"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the spider input and update the stats dictionary with the stats from the response.", "response": "def process_spider_input(self, response, spider):\n        '''\n        Ensures the meta data from the response is passed\n        through in any Request's generated from the spider\n        '''\n        self.logger.debug(\"processing redis stats middleware\")\n        if self.settings['STATS_STATUS_CODES']:\n            if spider.name not in self.stats_dict:\n                self._setup_stats_status_codes(spider.name)\n\n            if 'status_codes' in self.stats_dict[spider.name]:\n                code = response.status\n                if code in self.settings['STATS_RESPONSE_CODES']:\n                    for key in self.stats_dict[spider.name]['status_codes'][code]:\n                        try:\n                            if key == 'lifetime':\n                                unique = response.url + str(response.status)\\\n                                    + str(time.time())\n                                self.stats_dict[spider.name]['status_codes'][code][key].increment(unique)\n                            else:\n                                self.stats_dict[spider.name]['status_codes'][code][key].increment()\n                        except Exception as e:\n                            self.logger.warn(\"Error in spider redis stats\")\n                    self.logger.debug(\"Incremented status_code '{c}' stats\"\\\n                            .format(c=code))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self, settings):\n        '''\n        Setup the handler\n\n        @param settings: The loaded settings file\n        '''\n        self.producer = self._create_producer(settings)\n        self.topic_prefix = settings['KAFKA_TOPIC_PREFIX']\n\n        self.use_appid_topics = settings['KAFKA_APPID_TOPICS']\n\n        self.logger.debug(\"Successfully connected to Kafka in {name}\"\n                              .format(name=self.__class__.__name__))", "response": "Setup the handler\n\n        @param settings: The loaded settings file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to establish a Kafka consumer connection", "response": "def _create_producer(self, settings):\n        \"\"\"Tries to establish a Kafka consumer connection\"\"\"\n        try:\n            brokers = settings['KAFKA_HOSTS']\n            self.logger.debug(\"Creating new kafka producer using brokers: \" +\n                               str(brokers))\n\n            return KafkaProducer(bootstrap_servers=brokers,\n                                 value_serializer=lambda m: json.dumps(m),\n                                 retries=3,\n                                 linger_ms=settings['KAFKA_PRODUCER_BATCH_LINGER_MS'],\n                                 buffer_memory=settings['KAFKA_PRODUCER_BUFFER_BYTES'])\n        except KeyError as e:\n            self.logger.error('Missing setting named ' + str(e),\n                               {'ex': traceback.format_exc()})\n        except:\n            self.logger.error(\"Couldn't initialize kafka producer in plugin.\",\n                               {'ex': traceback.format_exc()})\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending the message back to Kafka", "response": "def _send_to_kafka(self, master):\n        '''\n        Sends the message back to Kafka\n        @param master: the final dict to send\n        @returns: True if successfully sent to kafka\n        '''\n        appid_topic = \"{prefix}.outbound_{appid}\".format(\n                                                    prefix=self.topic_prefix,\n                                                    appid=master['appid'])\n        firehose_topic = \"{prefix}.outbound_firehose\".format(\n                                                    prefix=self.topic_prefix)\n        try:\n            # dont want logger in outbound kafka message\n            if self.use_appid_topics:\n                f1 = self.producer.send(appid_topic, master)\n                f1.add_callback(self._kafka_success)\n                f1.add_errback(self._kafka_failure)\n            f2 = self.producer.send(firehose_topic, master)\n            f2.add_callback(self._kafka_success)\n            f2.add_errback(self._kafka_failure)\n\n            return True\n        except Exception as ex:\n            message = \"An exception '{0}' occured while sending a message \" \\\n                \"to kafka. Arguments:\\n{1!r}\" \\\n                .format(type(ex).__name__, ex.args)\n            self.logger.error(message)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload all plugins and setups the settings. py", "response": "def _load_plugins(self):\n        '''\n        Sets up all plugins, defaults and settings.py\n        '''\n        plugins = self.settings['PLUGINS']\n\n        self.plugins_dict = {}\n        for key in plugins:\n            # skip loading the plugin if its value is None\n            if plugins[key] is None:\n                continue\n            # valid plugin, import and setup\n            self.logger.debug(\"Trying to load plugin {cls}\".format(cls=key))\n            the_class = self._import_class(key)\n            instance = the_class()\n            instance._set_logger(self.logger)\n            if not self.unit_test:\n                instance.setup(self.settings)\n            the_schema = None\n            with open(self.settings['PLUGIN_DIR'] + instance.schema) as the_file:\n                the_schema = json.load(the_file)\n\n            mini = {}\n            mini['instance'] = instance\n            mini['schema'] = the_schema\n            self.logger.debug(\"Successfully loaded plugin {cls}\".format(cls=key))\n            self.plugins_dict[plugins[key]] = mini\n\n        self.plugins_dict = OrderedDict(sorted(list(self.plugins_dict.items()),\n                                               key=lambda t: t[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup(self, level=None, log_file=None, json=None):\n        '''\n        Load everything up. Note that any arg here will override both\n        default and custom settings\n\n        @param level: the log level\n        @param log_file: boolean t/f whether to log to a file, else stdout\n        @param json: boolean t/f whether to write the logs in json\n        '''\n        self.settings = self.wrapper.load(self.settings_name)\n\n        my_level = level if level else self.settings['LOG_LEVEL']\n        # negate because logger wants True for std out\n        my_output = not log_file if log_file else self.settings['LOG_STDOUT']\n        my_json = json if json else self.settings['LOG_JSON']\n        self.logger = LogFactory.get_instance(json=my_json, stdout=my_output,\n                                              level=my_level,\n                                              name=self.settings['LOGGER_NAME'],\n                                              dir=self.settings['LOG_DIR'],\n                                              file=self.settings['LOG_FILE'],\n                                              bytes=self.settings['LOG_MAX_BYTES'],\n                                              backups=self.settings['LOG_BACKUPS'])\n\n        self.validator = self.extend_with_default(Draft4Validator)", "response": "Load everything up and initialize the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up the stats collection", "response": "def _setup_stats(self):\n        '''\n        Sets up the stats collection\n        '''\n        self.stats_dict = {}\n\n        redis_conn = redis.Redis(host=self.settings['REDIS_HOST'],\n                                 port=self.settings['REDIS_PORT'],\n                                 db=self.settings.get('REDIS_DB'))\n\n        try:\n            redis_conn.info()\n            self.logger.debug(\"Connected to Redis in StatsCollector Setup\")\n            self.redis_conn = redis_conn\n        except ConnectionError:\n            self.logger.warn(\"Failed to connect to Redis in StatsCollector\"\n                             \" Setup, no stats will be collected\")\n            return\n\n        if self.settings['STATS_TOTAL']:\n            self._setup_stats_total(redis_conn)\n\n        if self.settings['STATS_PLUGINS']:\n            self._setup_stats_plugins(redis_conn)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting up the total stats collectors for the current resource.", "response": "def _setup_stats_total(self, redis_conn):\n        '''\n        Sets up the total stats collectors\n\n        @param redis_conn: the redis connection\n        '''\n        self.stats_dict['total'] = {}\n        self.stats_dict['fail'] = {}\n        temp_key1 = 'stats:kafka-monitor:total'\n        temp_key2 = 'stats:kafka-monitor:fail'\n        for item in self.settings['STATS_TIMES']:\n            try:\n                time = getattr(StatsCollector, item)\n                self.stats_dict['total'][time] = StatsCollector \\\n                        .get_rolling_time_window(\n                                redis_conn=redis_conn,\n                                key='{k}:{t}'.format(k=temp_key1, t=time),\n                                window=time,\n                                cycle_time=self.settings['STATS_CYCLE'])\n                self.stats_dict['fail'][time] = StatsCollector \\\n                        .get_rolling_time_window(\n                                redis_conn=redis_conn,\n                                key='{k}:{t}'.format(k=temp_key2, t=time),\n                                window=time,\n                                cycle_time=self.settings['STATS_CYCLE'])\n                self.logger.debug(\"Set up total/fail Stats Collector '{i}'\"\\\n                        .format(i=item))\n            except AttributeError as e:\n                self.logger.warning(\"Unable to find Stats Time '{s}'\"\\\n                        .format(s=item))\n        total1 = StatsCollector.get_hll_counter(redis_conn=redis_conn,\n                                                key='{k}:lifetime'.format(k=temp_key1),\n                                                cycle_time=self.settings['STATS_CYCLE'],\n                                                roll=False)\n        total2 = StatsCollector.get_hll_counter(redis_conn=redis_conn,\n                                                key='{k}:lifetime'.format(k=temp_key2),\n                                                cycle_time=self.settings['STATS_CYCLE'],\n                                                roll=False)\n        self.logger.debug(\"Set up total/fail Stats Collector 'lifetime'\")\n        self.stats_dict['total']['lifetime'] = total1\n        self.stats_dict['fail']['lifetime'] = total2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend_with_default(self, validator_class):\n        '''\n        Method to add default fields to our schema validation\n        ( From the docs )\n        '''\n        validate_properties = validator_class.VALIDATORS[\"properties\"]\n\n        def set_defaults(validator, properties, instance, schema):\n            for error in validate_properties(\n                validator, properties, instance, schema,\n            ):\n                yield error\n\n            for property, subschema in list(properties.items()):\n                if \"default\" in subschema:\n                    instance.setdefault(property, subschema[\"default\"])\n\n        return validators.extend(\n            validator_class, {\"properties\": set_defaults},\n        )", "response": "Extend the validator_class with default fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _increment_fail_stat(self, item):\n        '''\n        Increments the total stat counters\n\n        @param item: the loaded message object for HLL counter\n        '''\n        if isinstance(item, dict):\n            item['ts'] = time.time()\n        elif isinstance(item, str):\n            item = item + str(time.time())\n\n        if 'fail' in self.stats_dict:\n            self.logger.debug(\"Incremented fail stats\")\n            for key in self.stats_dict['fail']:\n                if key == 'lifetime':\n                    self.stats_dict['fail'][key].increment(item)\n                else:\n                    self.stats_dict['fail'][key].increment()", "response": "Increments the total stat counters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _increment_plugin_stat(self, name, item):\n        '''\n        Increments the total stat counters\n\n        @param name: The formal name of the plugin\n        @param dict: the loaded message object for HLL counter\n        '''\n        item['ts'] = time.time()\n        if 'plugins' in self.stats_dict:\n            self.logger.debug(\"Incremented plugin '{p}' plugin stats\"\\\n                    .format(p=name))\n            for key in self.stats_dict['plugins'][name]:\n                if key == 'lifetime':\n                    self.stats_dict['plugins'][name][key].increment(item)\n                else:\n                    self.stats_dict['plugins'][name][key].increment()", "response": "Increment the total stat counters for the HLL counter\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndump the stats out", "response": "def _dump_stats(self):\n        '''\n        Dumps the stats out\n        '''\n        extras = {}\n        if 'total' in self.stats_dict:\n            self.logger.debug(\"Compiling total/fail dump stats\")\n            for key in self.stats_dict['total']:\n                final = 'total_{t}'.format(t=key)\n                extras[final] = self.stats_dict['total'][key].value()\n            for key in self.stats_dict['fail']:\n                final = 'fail_{t}'.format(t=key)\n                extras[final] = self.stats_dict['fail'][key].value()\n\n        if 'plugins' in self.stats_dict:\n            self.logger.debug(\"Compiling plugin dump stats\")\n            for name in self.stats_dict['plugins']:\n                for key in self.stats_dict['plugins'][name]:\n                    final = 'plugin_{n}_{t}'.format(n=name, t=key)\n                    extras[final] = self.stats_dict['plugins'][name][key].value()\n\n        if not self.logger.json:\n            self.logger.info('Kafka Monitor Stats Dump:\\n{0}'.format(\n                    json.dumps(extras, indent=4, sort_keys=True)))\n        else:\n            self.logger.info('Kafka Monitor Stats Dump', extra=extras)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        '''\n        Set up and run\n        '''\n        self._setup_kafka()\n        self._load_plugins()\n        self._setup_stats()\n        self._main_loop()", "response": "Set up and run\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _report_self(self):\n        '''\n        Reports the kafka monitor uuid to redis\n        '''\n        key = \"stats:kafka-monitor:self:{m}:{u}\".format(\n            m=socket.gethostname(),\n            u=self.my_uuid)\n        self.redis_conn.set(key, time.time())\n        self.redis_conn.expire(key, self.settings['HEARTBEAT_TIMEOUT'])", "response": "Reports the kafka monitor uuid to redis\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfeed a json item into the Kafka topic omento", "response": "def feed(self, json_item):\n        '''\n        Feeds a json item into the Kafka topic\n\n        @param json_item: The loaded json object\n        '''\n        @MethodTimer.timeout(self.settings['KAFKA_FEED_TIMEOUT'], False)\n        def _feed(json_item):\n            producer = self._create_producer()\n            topic = self.settings['KAFKA_INCOMING_TOPIC']\n            if not self.logger.json:\n                self.logger.info('Feeding JSON into {0}\\n{1}'.format(\n                    topic, json.dumps(json_item, indent=4)))\n            else:\n                self.logger.info('Feeding JSON into {0}\\n'.format(topic),\n                                 extra={'value': json_item})\n\n            if producer is not None:\n                producer.send(topic, json_item)\n                producer.flush()\n                producer.close(timeout=10)\n                return True\n            else:\n                return False\n\n        result = _feed(json_item)\n\n        if result:\n            self.logger.info(\"Successfully fed item to Kafka\")\n        else:\n            self.logger.error(\"Failed to feed item into Kafka\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding to check for timeout and return True if the current time is greater than the timeout.", "response": "def check_precondition(self, key, value):\n        '''\n        Override to check for timeout\n        '''\n        timeout = float(value)\n        curr_time = self.get_current_time()\n        if curr_time > timeout:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle(self, key, value):\n        '''\n        Processes a vaild action info request\n\n        @param key: The key that matched the request\n        @param value: The value associated with the key\n        '''\n        # very similar to stop\n        # break down key\n        elements = key.split(\":\")\n        spiderid = elements[1]\n        appid = elements[2]\n        crawlid = elements[3]\n\n        # log ack of expire\n        extras = self.get_log_dict('expire', appid,\n                                   spiderid, crawlid=crawlid)\n        self.logger.info(\"Expiring crawl found\", extra=extras)\n\n        # add crawl to blacklist so it doesnt propagate\n        redis_key = spiderid + \":blacklist\"\n        value = '{appid}||{crawlid}'.format(appid=appid,\n                                            crawlid=crawlid)\n        # add this to the blacklist set\n        self.redis_conn.sadd(redis_key, value)\n\n        # everything stored in the queue is now expired\n        result = self._purge_crawl(spiderid, appid, crawlid)\n\n        # add result to our dict\n        master = {}\n        master['server_time'] = int(self.get_current_time())\n        master['crawlid'] = crawlid\n        master['spiderid'] = spiderid\n        master['appid'] = appid\n        master['total_expired'] = result\n        master['action'] = 'expired'\n\n        if self._send_to_kafka(master):\n            master['success'] = True\n            self.logger.info('Sent expired ack to kafka', extra=master)\n        else:\n            master['success'] = False\n            self.logger.error('Failed to send expired ack to kafka',\n                              extra=master)", "response": "Process a vaild action info request and return the key and value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_time_window(self, redis_conn=None, host='localhost', port=6379,\n                        key='time_window_counter', cycle_time=5,\n                        start_time=None, window=SECONDS_1_HOUR, roll=True,\n                        keep_max=12):\n        '''\n        Generate a new TimeWindow\n        Useful for collecting number of hits generated between certain times\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        @param key: the key for your stats collection\n        @param cycle_time: how often to check for expiring counts\n        @param start_time: the time to start valid collection\n        @param window: how long to collect data for in seconds (if rolling)\n        @param roll: Roll the window after it expires, to continue collecting\n            on a new date based key.\n        @keep_max: If rolling the static window, the max number of prior\n            windows to keep\n        '''\n        counter = TimeWindow(key=key, cycle_time=cycle_time,\n                             start_time=start_time, window=window, roll=roll,\n                             keep_max=keep_max)\n        counter.setup(redis_conn=redis_conn, host=host, port=port)\n        return counter", "response": "Generate a new TimeWindow that will collect data for a specific key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a new rolling time window for the current time of the log entry", "response": "def get_rolling_time_window(self, redis_conn=None, host='localhost',\n                                port=6379, key='rolling_time_window_counter',\n                                cycle_time=5, window=SECONDS_1_HOUR):\n        '''\n        Generate a new RollingTimeWindow\n        Useful for collect data about the number of hits in the past X seconds\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        @param key: the key for your stats collection\n        @param cycle_time: how often to check for expiring counts\n        @param window: the number of seconds behind now() to keep data for\n        '''\n        counter = RollingTimeWindow(key=key, cycle_time=cycle_time,\n                                    window=window)\n        counter.setup(redis_conn=redis_conn, host=host, port=port)\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a Counter object for the given key.", "response": "def get_counter(self, redis_conn=None, host='localhost', port=6379,\n                    key='counter', cycle_time=5, start_time=None,\n                    window=SECONDS_1_HOUR, roll=True, keep_max=12, start_at=0):\n        '''\n        Generate a new Counter\n        Useful for generic distributed counters\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        @param key: the key for your stats collection\n        @param cycle_time: how often to check for expiring counts\n        @param start_time: the time to start valid collection\n        @param window: how long to collect data for in seconds (if rolling)\n        @param roll: Roll the window after it expires, to continue collecting\n            on a new date based key.\n        @keep_max: If rolling the static window, the max number of prior\n            windows to keep\n        @param start_at: The integer to start counting at\n        '''\n        counter = Counter(key=key, cycle_time=cycle_time,\n                          start_time=start_time, window=window, roll=roll,\n                          keep_max=keep_max)\n        counter.setup(redis_conn=redis_conn, host=host, port=port)\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a unique counter for the object with the given key.", "response": "def get_unique_counter(self, redis_conn=None, host='localhost', port=6379,\n                           key='unique_counter', cycle_time=5, start_time=None,\n                           window=SECONDS_1_HOUR, roll=True, keep_max=12):\n        '''\n        Generate a new UniqueCounter.\n        Useful for exactly counting unique objects\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        @param key: the key for your stats collection\n        @param cycle_time: how often to check for expiring counts\n        @param start_time: the time to start valid collection\n        @param window: how long to collect data for in seconds (if rolling)\n        @param roll: Roll the window after it expires, to continue collecting\n            on a new date based key.\n        @keep_max: If rolling the static window, the max number of prior\n            windows to keep\n        '''\n        counter = UniqueCounter(key=key, cycle_time=cycle_time,\n                                start_time=start_time, window=window,\n                                roll=roll, keep_max=keep_max)\n        counter.setup(redis_conn=redis_conn, host=host, port=port)\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a new HyperLogLogCounter object for the given key.", "response": "def get_hll_counter(self, redis_conn=None, host='localhost', port=6379,\n                        key='hyperloglog_counter', cycle_time=5,\n                        start_time=None, window=SECONDS_1_HOUR, roll=True,\n                        keep_max=12):\n        '''\n        Generate a new HyperLogLogCounter.\n        Useful for approximating extremely large counts of unique items\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        @param key: the key for your stats collection\n        @param cycle_time: how often to check for expiring counts\n        @param start_time: the time to start valid collection\n        @param window: how long to collect data for in seconds (if rolling)\n        @param roll: Roll the window after it expires, to continue collecting\n            on a new date based key.\n        @keep_max: If rolling the static window, the max number of prior\n            windows to keep\n        '''\n        counter = HyperLogLogCounter(key=key, cycle_time=cycle_time,\n                                     start_time=start_time, window=window,\n                                     roll=roll, keep_max=keep_max)\n        counter.setup(redis_conn=redis_conn, host=host, port=port)\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a new bitmap counter for the items in the current user and items.", "response": "def get_bitmap_counter(self, redis_conn=None, host='localhost', port=6379,\n                           key='bitmap_counter', cycle_time=5, start_time=None,\n                           window=SECONDS_1_HOUR, roll=True, keep_max=12):\n        '''\n        Generate a new BitMapCounter\n        Useful for creating different bitsets about users/items\n        that have unique indices\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        @param key: the key for your stats collection\n        @param cycle_time: how often to check for expiring counts\n        @param start_time: the time to start valid collection\n        @param window: how long to collect data for in seconds (if rolling)\n        @param roll: Roll the window after it expires, to continue collecting\n            on a new date based key.\n        @keep_max: If rolling the static window, the max number of prior\n            windows to keep\n        '''\n        counter = BitMapCounter(key=key, cycle_time=cycle_time,\n                                start_time=start_time, window=window,\n                                roll=roll, keep_max=keep_max)\n        counter.setup(redis_conn=redis_conn, host=host, port=port)\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset up the redis connection", "response": "def setup(self, redis_conn=None, host='localhost', port=6379):\n        '''\n        Set up the redis connection\n        '''\n        if redis_conn is None:\n            if host is not None and port is not None:\n                self.redis_conn = redis.Redis(host=host, port=port)\n            else:\n                raise Exception(\"Please specify some form of connection \"\n                                    \"to Redis\")\n        else:\n            self.redis_conn = redis_conn\n\n        self.redis_conn.info()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup(self, redis_conn=None, host='localhost', port=6379):\n        '''\n        Set up the counting manager class\n\n        @param redis_conn: A premade redis connection (overrides host and port)\n        @param host: the redis host\n        @param port: the redis port\n        '''\n        AbstractCounter.setup(self, redis_conn=redis_conn, host=host,\n                              port=port)\n\n        self._threaded_start()", "response": "Setup the count manager class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nspawning a worker thread to do the expiration checks", "response": "def _threaded_start(self):\n        '''\n        Spawns a worker thread to do the expiration checks\n        '''\n        self.active = True\n        self.thread = Thread(target=self._main_loop)\n        self.thread.setDaemon(True)\n        self.thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the final key to be used currently", "response": "def _set_key(self):\n        '''\n        sets the final key to be used currently\n        '''\n        if self.roll:\n            self.date = time.strftime(self.date_format,\n                                      time.gmtime(self.start_time))\n\n            self.final_key = '{}:{}'.format(self.key, self.date)\n        else:\n            self.final_key = self.key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning true if the time is beyond the window", "response": "def is_expired(self):\n        '''\n        Returns true if the time is beyond the window\n        '''\n        if self.window is not None:\n            return (self._time() - self.start_time) >= self.window\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes an item object into a string.", "response": "def _encode_item(self, item):\n        '''\n        Encode an item object\n\n        @requires: The object be serializable\n        '''\n        if self.encoding.__name__ == 'pickle':\n            return self.encoding.dumps(item, protocol=-1)\n        else:\n            return self.encoding.dumps(item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npop an item from the cache.", "response": "def pop(self, timeout=0):\n        '''\n        Pop an item\n        '''\n        if timeout > 0:\n            data = self.server.brpop(self.key, timeout)\n            if isinstance(data, tuple):\n                data = data[1]\n        else:\n            data = self.server.rpop(self.key)\n        if data:\n            return self._decode_item(data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push(self, item, priority):\n        '''\n        Push an item\n\n        @param priority: the priority of the item\n        '''\n        data = self._encode_item(item)\n        pairs = {data: -priority}\n        self.server.zadd(self.key, **pairs)", "response": "Push an item into the cache"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush an item onto the end of the list.", "response": "def push(self, item):\n        '''\n        Push an item\n        '''\n        self.server.lpush(self.key, self._encode_item(item))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopping an item from the cache.", "response": "def pop(self, timeout=0):\n        '''\n        Pop an item\n        '''\n        if timeout > 0:\n            data = self.server.blpop(self.key, timeout)\n            if isinstance(data, tuple):\n                data = data[1]\n        else:\n            data = self.server.lpop(self.key)\n\n        if data:\n            return self._decode_item(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nspawns a worker thread to set up the zookeeper connection", "response": "def threaded_start(self, no_init=False):\n        '''\n        Spawns a worker thread to set up the zookeeper connection\n        '''\n        thread = Thread(target=self.init_connections, kwargs={\n                        'no_init': no_init})\n        thread.setDaemon(True)\n        thread.start()\n        thread.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_connections(self, no_init=False):\n        '''\n        Sets up the initial Kazoo Client and watches\n        '''\n        success = False\n        self.set_valid(False)\n\n        if not no_init:\n            if self.zoo_client:\n                self.zoo_client.remove_listener(self.state_listener)\n                self.old_data = ''\n                self.old_pointed = ''\n\n            while not success:\n                try:\n                    if self.zoo_client is None:\n                        self.zoo_client = KazooClient(hosts=self.hosts)\n                        self.zoo_client.start()\n                    else:\n                        # self.zoo_client.stop()\n                        self.zoo_client._connection.connection_stopped.set()\n                        self.zoo_client.close()\n                        self.zoo_client = KazooClient(hosts=self.hosts)\n                        self.zoo_client.start()\n                except Exception as e:\n                    log.error(\"ZKWatcher Exception: \" + e.message)\n                    sleep(1)\n                    continue\n\n                self.setup()\n                success = self.update_file(self.my_file)\n                sleep(5)\n        else:\n            self.setup()\n            self.update_file(self.my_file)", "response": "Initializes the initial Kazoo Client and watches for new entries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure the path to the watched file exists and we have a state listener", "response": "def setup(self):\n        '''\n        Ensures the path to the watched file exists and we have a state\n        listener\n        '''\n        self.zoo_client.add_listener(self.state_listener)\n\n        if self.ensure:\n            self.zoo_client.ensure_path(self.my_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall when the state of the kazoo is changed.", "response": "def state_listener(self, state):\n        '''\n        Restarts the session if we get anything besides CONNECTED\n        '''\n        if state == KazooState.SUSPENDED:\n            self.set_valid(False)\n            self.call_error(self.BAD_CONNECTION)\n        elif state == KazooState.LOST and not self.do_not_restart:\n            self.threaded_start()\n        elif state == KazooState.CONNECTED:\n            # This is going to throw a SUSPENDED kazoo error\n            # which will cause the sessions to be wiped and re established.\n            # Used b/c of massive connection pool issues\n            self.zoo_client.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosing the kazoo client.", "response": "def close(self, kill_restart=True):\n        '''\n        Use when you would like to close everything down\n        @param kill_restart= Prevent kazoo restarting from occurring\n        '''\n        self.do_not_restart = kill_restart\n        self.zoo_client.stop()\n        self.zoo_client.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting any file contents you care about. Defaults to the main file.", "response": "def get_file_contents(self, pointer=False):\n        '''\n        Gets any file contents you care about. Defaults to the main file\n        @param pointer: The the contents of the file pointer, not the pointed\n        at file\n        @return: A string of the contents\n        '''\n        if self.pointer:\n            if pointer:\n                return self.old_pointed\n            else:\n                return self.old_data\n        else:\n            return self.old_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the file watcher and calls the appropriate method for results", "response": "def update_file(self, path):\n        '''\n        Updates the file watcher and calls the appropriate method for results\n        @return: False if we need to keep trying the connection\n        '''\n        try:\n            # grab the file\n            result, stat = self.zoo_client.get(path, watch=self.watch_file)\n        except ZookeeperError:\n            self.set_valid(False)\n            self.call_error(self.INVALID_GET)\n            return False\n\n        if self.pointer:\n            if result is not None and len(result) > 0:\n                self.pointed_at_expired = False\n                # file is a pointer, go update and watch other file\n                self.point_path = result\n                if self.compare_pointer(result):\n                    self.update_pointed()\n            else:\n                self.pointed_at_expired = True\n                self.old_pointed = ''\n                self.old_data = ''\n                self.set_valid(False)\n                self.call_error(self.INVALID_PATH)\n        else:\n            # file is not a pointer, return contents\n            if self.compare_data(result):\n                self.call_config(result)\n            self.set_valid(True)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_pointed(self):\n        '''\n        Grabs the latest file contents based on the pointer uri\n        '''\n        # only grab file if our pointer is still good (not None)\n        if not self.pointed_at_expired:\n            try:\n                conf_string, stat2 = self.zoo_client.get(self.point_path,\n                                                    watch=self.watch_pointed)\n            except ZookeeperError:\n                self.old_data = ''\n                self.set_valid(False)\n                self.pointed_at_expired = True\n                self.call_error(self.INVALID_PATH)\n                return\n\n            if self.compare_data(conf_string):\n                self.call_config(conf_string)\n            self.set_valid(True)", "response": "Grabs the latest file contents based on the pointer uri\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the state of the object to boolean if needed.", "response": "def set_valid(self, boolean):\n        '''\n        Sets the state and calls the change if needed\n        @param bool: The state (true or false)\n        '''\n        old_state = self.is_valid()\n        self.valid_file = boolean\n\n        if old_state != self.valid_file:\n            self.call_valid(self.valid_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompares the string data with the old data and returns True if the data is different False otherwise", "response": "def compare_data(self, data):\n        '''\n        Compares the string data\n        @return: True if the data is different\n        '''\n        if self.old_data != data:\n            self.old_data = data\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_pointer(self, data):\n        '''\n        Compares the string data\n        @return: True if the data is different\n        '''\n        if self.old_pointed != data:\n            self.old_pointed = data\n            return True\n        return False", "response": "Compares the string data\n            and returns True if the string data is different\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a vaild stats request from the server", "response": "def handle(self, dict):\n        '''\n        Processes a vaild stats request\n\n        @param dict: a valid dictionary object\n        '''\n        # format key\n        key = \"statsrequest:{stats}:{appid}\".format(\n                stats=dict['stats'],\n                appid=dict['appid'])\n\n        self.redis_conn.set(key, dict['uuid'])\n\n        dict['parsed'] = True\n        dict['valid'] = True\n        self.logger.info('Added stat request to Redis', extra=dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(self, settings):\n        '''\n        Does the actual setup of the middleware\n        '''\n        # set up the default sc logger\n        my_level = settings.get('SC_LOG_LEVEL', 'INFO')\n        my_name = settings.get('SC_LOGGER_NAME', 'sc-logger')\n        my_output = settings.get('SC_LOG_STDOUT', True)\n        my_json = settings.get('SC_LOG_JSON', False)\n        my_dir = settings.get('SC_LOG_DIR', 'logs')\n        my_bytes = settings.get('SC_LOG_MAX_BYTES', '10MB')\n        my_file = settings.get('SC_LOG_FILE', 'main.log')\n        my_backups = settings.get('SC_LOG_BACKUPS', 5)\n\n        self.logger = LogFactory.get_instance(json=my_json,\n                                         name=my_name,\n                                         stdout=my_output,\n                                         level=my_level,\n                                         dir=my_dir,\n                                         file=my_file,\n                                         bytes=my_bytes,\n                                         backups=my_backups)", "response": "Does the actual setup of the middleware\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_spider_output(self, response, result, spider):\n        '''\n        Ensures the meta data from the response is passed\n        through in any Request's generated from the spider\n        '''\n        self.logger.debug(\"processing meta passthrough middleware\")\n        for x in result:\n            # only operate on requests\n            if isinstance(x, Request):\n                self.logger.debug(\"found request\")\n                # pass along all known meta fields, only if\n                # they were not already set in the spider's new request\n                for key in list(response.meta.keys()):\n                    if key not in x.meta:\n                        x.meta[key] = response.meta[key]\n            yield x", "response": "Process the result of the spider s request processing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle(self, key, value):\n        '''\n        Processes a vaild zookeeper request\n\n        @param key: The key that matched the request\n        @param value: The value associated with the key\n        '''\n        # break down key\n        elements = key.split(\":\")\n        dict = {}\n        dict['action'] = elements[1]\n        dict['domain'] = elements[2]\n        dict['appid'] = elements[3]\n        value = ujson.loads(value)\n\n        # the master dict to return\n        master = {}\n        master['uuid'] = value['uuid']\n        master['server_time'] = int(self.get_current_time())\n        master['action'] = dict['action']\n        master['domain'] = dict['domain']\n        master['appid'] = dict['appid']\n\n        # log we received the info message\n        extras = self.get_log_dict(dict['action'], appid=dict['appid'],\n                                   uuid=master['uuid'])\n\n        self.logger.info('Received zookeeper request', extra=extras)\n\n        # get the current zk configuration\n        data = None\n        try:\n            data = self.zoo_client.get(self.path)[0]\n        except ZookeeperError:\n            e = \"Unable to load Zookeeper config\"\n            self.logger.error(e)\n            master['error'] = e\n        the_dict = {}\n        if data is not None and len(data) > 0:\n            the_dict = yaml.safe_load(data)\n\n        # update the configuration\n        if \"domains\" not in the_dict:\n            the_dict['domains'] = {}\n\n        if \"blacklist\" not in the_dict:\n            the_dict['blacklist'] = []\n\n        if dict['action'] == 'domain-update':\n            the_dict['domains'][dict['domain']] = {\n                \"window\": value['window'],\n                \"hits\": value['hits'],\n                \"scale\": value['scale']\n            }\n        elif dict['action'] == 'domain-remove':\n            if dict['domain'] in the_dict['domains']:\n                del the_dict['domains'][dict['domain']]\n        elif dict['action'] == 'blacklist-update':\n            the_dict['blacklist'].append(dict['domain'])\n            the_dict['blacklist'] = list(set(the_dict['blacklist']))\n        elif dict['action'] == 'blacklist-remove':\n            if dict['domain'] in the_dict['blacklist']:\n                the_dict['blacklist'].remove(dict['domain'])\n        else:\n            self.logger.warn(\"Unknown command given to Zookeeper Monitor\")\n\n        # write the configuration back to zookeeper\n        the_string = yaml.dump(the_dict, default_flow_style=False)\n        try:\n            self.zoo_client.set(self.path, the_string)\n        except ZookeeperError:\n            e = \"Unable to store Zookeeper config\"\n            self.logger.error(e)\n            master['error'] = e\n\n        # ack the data back to kafka\n        if self._send_to_kafka(master):\n            extras['success'] = True\n            self.logger.info('Sent zookeeper update to kafka', extra=extras)\n        else:\n            extras['success'] = False\n            self.logger.error('Failed to send zookeeper update to kafka',\n                              extra=extras)", "response": "Processes a vaild zookeeper request and returns the key value pair"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle(self, dict):\n        '''\n        Processes a vaild zookeeper request\n\n        @param dict: a valid dictionary object\n        '''\n        # format key\n        key = \"zk:{action}:{domain}:{appid}\".format(\n                action=dict['action'],\n                appid=dict['appid'],\n                domain=dict['domain'])\n\n        d = {\"domain\": dict['domain'], \"uuid\": dict['uuid']}\n\n        if dict['action'] == 'domain-update':\n            if dict['hits'] != 0 and dict['window'] != 0:\n                d[\"hits\"] = dict['hits']\n                d['scale'] = dict['scale']\n                d['window'] = dict['window']\n            else:\n                self.logger.error(\"'hits' and 'window' not set for domain update\")\n                return None\n\n        value = ujson.dumps(d)\n\n        self.redis_conn.set(key, value)\n\n        dict['parsed'] = True\n        dict['valid'] = True\n        self.logger.info('Added zookeeper action to Redis', extra=dict)", "response": "Processes a vaild zookeeper request and stores the result in the Redis table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract links from the response.", "response": "def _extract_links(self, selector, response_url, response_encoding, base_url):\n        '''\n        Pretty much the same function, just added 'ignore' to to_native_str()\n        '''\n        links = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            # added 'ignore' to encoding errors\n            url = to_native_str(url, encoding=response_encoding,\n                                errors='ignore')\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(url, _collect_string_content(el) or u'',\n                        nofollow=rel_has_nofollow(el.get('rel')))\n            links.append(link)\n        return self._deduplicate_if_needed(links)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_log_dict(self, action, appid, spiderid=None, uuid=None,\n                     crawlid=None):\n        '''\n        Returns a basic dictionary for logging\n        @param action: the action taken by the redis monitor\n        @param spiderid: the spider id\n        @param appid: the application id\n        @param uuid: a unique id of the request\n        @param crawlid: a unique crawl id of the request\n        '''\n        extras = {}\n        extras['action'] = action\n        extras['appid'] = appid\n        if spiderid is not None:\n            extras['spiderid'] = spiderid\n        if uuid is not None:\n            extras['uuid'] = uuid\n        if crawlid is not None:\n            extras['crawlid'] = crawlid\n\n        return extras", "response": "Returns a basic dictionary for logging related data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload all settings and setup the instance variables.", "response": "def setup(self, level=None, log_file=None, json=None):\n        '''\n        Load everything up. Note that any arg here will override both\n        default and custom settings\n\n        @param level: the log level\n        @param log_file: boolean t/f whether to log to a file, else stdout\n        @param json: boolean t/f whether to write the logs in json\n        '''\n        self.settings = self.wrapper.load(self.settings_name)\n\n        my_level = level if level else self.settings['LOG_LEVEL']\n        # negate because logger wants True for std out\n        my_output = not log_file if log_file else self.settings['LOG_STDOUT']\n        my_json = json if json else self.settings['LOG_JSON']\n        self.logger = LogFactory.get_instance(json=my_json,\n                                              stdout=my_output, level=my_level,\n                                              name=self.settings['LOGGER_NAME'],\n                                              dir=self.settings['LOG_DIR'],\n                                              file=self.settings['LOG_FILE'],\n                                              bytes=self.settings['LOG_MAX_BYTES'],\n                                              backups=self.settings['LOG_BACKUPS'])\n\n        self.redis_conn = redis.StrictRedis(host=self.settings['REDIS_HOST'],\n                                      port=self.settings['REDIS_PORT'],\n                                      db=self.settings['REDIS_DB'])\n\n        try:\n            self.redis_conn.info()\n            self.logger.debug(\"Successfully connected to Redis\")\n        except ConnectionError:\n            self.logger.error(\"Failed to connect to Redis\")\n            # essential to functionality\n            sys.exit(1)\n\n        self._load_plugins()\n        self._setup_stats()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads all plugins and setup the instance and regex", "response": "def _load_plugins(self):\n        '''\n        Sets up all plugins and defaults\n        '''\n        plugins = self.settings['PLUGINS']\n\n        self.plugins_dict = {}\n        for key in plugins:\n            # skip loading the plugin if its value is None\n            if plugins[key] is None:\n                continue\n            # valid plugin, import and setup\n            self.logger.debug(\"Trying to load plugin {cls}\"\n                              .format(cls=key))\n            the_class = self.import_class(key)\n            instance = the_class()\n            instance.redis_conn = self.redis_conn\n            instance._set_logger(self.logger)\n            if not self.unit_test:\n                instance.setup(self.settings)\n            the_regex = instance.regex\n\n            mini = {}\n            mini['instance'] = instance\n            if the_regex is None:\n                raise ImportError()\n                # continue\n            mini['regex'] = the_regex\n\n            self.plugins_dict[plugins[key]] = mini\n\n        self.plugins_dict = OrderedDict(sorted(list(self.plugins_dict.items()),\n                                               key=lambda t: t[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _main_loop(self):\n        '''\n        The internal while true main loop for the redis monitor\n        '''\n        self.logger.debug(\"Running main loop\")\n        old_time = 0\n        while True:\n            for plugin_key in self.plugins_dict:\n                obj = self.plugins_dict[plugin_key]\n                self._process_plugin(obj)\n\n            if self.settings['STATS_DUMP'] != 0:\n                new_time = int(old_div(time.time(), self.settings['STATS_DUMP']))\n                # only log every X seconds\n                if new_time != old_time:\n                    self._dump_stats()\n\n                    if self.settings['STATS_DUMP_CRAWL']:\n                        self._dump_crawl_stats()\n\n                    if self.settings['STATS_DUMP_QUEUE']:\n                        self._dump_queue_stats()\n\n                    old_time = new_time\n            self._report_self()\n            time.sleep(self.settings['SLEEP_TIME'])", "response": "Main loop for the monitor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_plugin(self, plugin):\n        '''\n        Logic to handle each plugin that is active\n\n        @param plugin: a plugin dict object\n        '''\n        instance = plugin['instance']\n        regex = plugin['regex']\n        for key in self.redis_conn.scan_iter(match=regex):\n            # acquire lock\n            lock = self._create_lock_object(key)\n\n            try:\n                if lock.acquire(blocking=False):\n                    val = self.redis_conn.get(key)\n                    self._process_key_val(instance, key, val)\n            except Exception:\n                self.logger.error(traceback.format_exc())\n                self._increment_fail_stat('{k}:{v}'.format(k=key, v=val))\n\n                self._process_failures(key)\n\n            # remove lock regardless of if exception or was handled ok\n            if lock._held:\n                self.logger.debug(\"releasing lock\")\n                lock.release()", "response": "Logic to handle each plugin that is active\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a lock object split for testing Returns a lock object split for testing", "response": "def _create_lock_object(self, key):\n        '''\n        Returns a lock object, split for testing\n        '''\n        return redis_lock.Lock(self.redis_conn, key,\n                               expire=self.settings['REDIS_LOCK_EXPIRATION'],\n                               auto_renewal=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_failures(self, key):\n        '''\n        Handles the retrying of the failed key\n        '''\n        if self.settings['RETRY_FAILURES']:\n            self.logger.debug(\"going to retry failure\")\n            # get the current failure count\n            failkey = self._get_fail_key(key)\n            current = self.redis_conn.get(failkey)\n            if current is None:\n                current = 0\n            else:\n                current = int(current)\n            if current < self.settings['RETRY_FAILURES_MAX']:\n                self.logger.debug(\"Incr fail key\")\n                current += 1\n                self.redis_conn.set(failkey, current)\n            else:\n                self.logger.error(\"Could not process action within\"\n                                  \" failure limit\")\n                self.redis_conn.delete(failkey)\n                self.redis_conn.delete(key)", "response": "Handles the retrying of the failed key\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_key_val(self, instance, key, val):\n        '''\n        Logic to let the plugin instance process the redis key/val\n        Split out for unit testing\n\n        @param instance: the plugin instance\n        @param key: the redis key\n        @param val: the key value from redis\n        '''\n        if instance.check_precondition(key, val):\n            combined = '{k}:{v}'.format(k=key, v=val)\n            self._increment_total_stat(combined)\n            self._increment_plugin_stat(\n                instance.__class__.__name__,\n                combined)\n            instance.handle(key, val)\n            self.redis_conn.delete(key)\n            failkey = self._get_fail_key(key)\n            if self.redis_conn.exists(failkey):\n                self.redis_conn.delete(failkey)", "response": "This method is called by the plugin instance when a key value is received from the plugin instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_stats(self):\n        '''\n        Sets up the stats\n        '''\n        # stats setup\n        self.stats_dict = {}\n\n        if self.settings['STATS_TOTAL']:\n            self._setup_stats_total()\n\n        if self.settings['STATS_PLUGINS']:\n            self._setup_stats_plugins()", "response": "Sets up the stats dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting up the stats collectors for the current instance of the Stats Collector.", "response": "def _setup_stats_plugins(self):\n        '''\n        Sets up the plugin stats collectors\n        '''\n        self.stats_dict['plugins'] = {}\n        for key in self.plugins_dict:\n            plugin_name = self.plugins_dict[key]['instance'].__class__.__name__\n            temp_key = 'stats:redis-monitor:{p}'.format(p=plugin_name)\n            self.stats_dict['plugins'][plugin_name] = {}\n            for item in self.settings['STATS_TIMES']:\n                try:\n                    time = getattr(StatsCollector, item)\n\n                    self.stats_dict['plugins'][plugin_name][time] = StatsCollector \\\n                            .get_rolling_time_window(\n                                    redis_conn=self.redis_conn,\n                                    key='{k}:{t}'.format(k=temp_key, t=time),\n                                    window=time,\n                                    cycle_time=self.settings['STATS_CYCLE'])\n                    self.logger.debug(\"Set up {p} plugin Stats Collector '{i}'\"\\\n                            .format(p=plugin_name, i=item))\n                except AttributeError as e:\n                    self.logger.warning(\"Unable to find Stats Time '{s}'\"\\\n                            .format(s=item))\n            total = StatsCollector.get_hll_counter(redis_conn=self.redis_conn,\n                            key='{k}:lifetime'.format(k=temp_key),\n                            cycle_time=self.settings['STATS_CYCLE'],\n                            roll=False)\n            self.logger.debug(\"Set up {p} plugin Stats Collector 'lifetime'\"\\\n                            .format(p=plugin_name))\n            self.stats_dict['plugins'][plugin_name]['lifetime'] = total"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps flattened crawling stats so the spiders do not have to have to", "response": "def _dump_crawl_stats(self):\n        '''\n        Dumps flattened crawling stats so the spiders do not have to\n        '''\n        extras = {}\n        spiders = {}\n\n        spider_set = set()\n        total_spider_count = 0\n\n        keys = self.redis_conn.keys('stats:crawler:*:*:*')\n        for key in keys:\n            # we only care about the spider\n            elements = key.split(\":\")\n            spider = elements[3]\n\n            if spider not in spiders:\n                spiders[spider] = 0\n\n            if len(elements) == 6:\n                # got a time based stat\n                response = elements[4]\n                end = elements[5]\n\n                final = '{s}_{r}_{e}'.format(s=spider, r=response, e=end)\n\n                if end == 'lifetime':\n                    value = self.redis_conn.execute_command(\"PFCOUNT\", key)\n                else:\n                    value = self.redis_conn.zcard(key)\n\n                extras[final] = value\n\n            elif len(elements) == 5:\n                # got a spider identifier\n                spiders[spider] += 1\n                total_spider_count += 1\n                spider_set.add(spider)\n\n            else:\n                self.logger.warn(\"Unknown crawler stat key\", {\"key\":key})\n\n        # simple counts\n        extras['unique_spider_count'] = len(spider_set)\n        extras['total_spider_count'] = total_spider_count\n\n        for spider in spiders:\n            extras['{k}_spider_count'.format(k=spider)] = spiders[spider]\n\n        if not self.logger.json:\n            self.logger.info('Crawler Stats Dump:\\n{0}'.format(\n                    json.dumps(extras, indent=4, sort_keys=True)))\n        else:\n            self.logger.info('Crawler Stats Dump', extra=extras)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n        '''\n        Closes the Redis Monitor and plugins\n        '''\n        for plugin_key in self.plugins_dict:\n            obj = self.plugins_dict[plugin_key]\n            instance = obj['instance']\n            instance.close()", "response": "Closes the Redis Monitor and plugins\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a vaild stats request and sends it to kafka", "response": "def handle(self, key, value):\n        '''\n        Processes a vaild stats request\n\n        @param key: The key that matched the request\n        @param value: The value associated with the key\n        '''\n        # break down key\n        elements = key.split(\":\")\n\n        stats = elements[1]\n        appid = elements[2]\n        uuid = value\n\n        # log we received the stats request\n        extras = self.get_log_dict('stats', appid, uuid=uuid)\n        self.logger.info('Received {s} stats request'.format(s=stats),\n            extra=extras)\n\n        extras = {}\n        if stats == 'all':\n            extras = self.get_all_stats()\n        elif stats == 'kafka-monitor':\n            extras = self.get_kafka_monitor_stats()\n        elif stats == 'redis-monitor':\n            extras = self.get_redis_monitor_stats()\n        elif stats == 'crawler':\n            extras = self.get_crawler_stats()\n        elif stats == 'spider':\n            extras = self.get_spider_stats()\n        elif stats == 'machine':\n            extras = self.get_machine_stats()\n        elif stats == 'queue':\n            extras = self.get_queue_stats()\n        elif stats == 'rest':\n            extras = self.get_rest_stats()\n        else:\n            self.logger.warn('Received invalid stats request: {s}'\\\n                .format(s=stats),\n            extra=extras)\n            return\n\n        extras['stats'] = stats\n        extras['appid'] = appid\n        extras['uuid'] = uuid\n        extras['server_time'] = int(self.get_current_time())\n\n        if self._send_to_kafka(extras):\n            extras['success'] = True\n            self.logger.info('Sent stats to kafka', extra=extras)\n        else:\n            extras['success'] = False\n            self.logger.error('Failed to send stats to kafka', extra=extras)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngathers all stats objects", "response": "def get_all_stats(self):\n        '''\n        Gather all stats objects\n        '''\n        self.logger.debug(\"Gathering all stats\")\n        the_dict = {}\n        the_dict['kafka-monitor'] = self.get_kafka_monitor_stats()\n        the_dict['redis-monitor'] = self.get_redis_monitor_stats()\n        the_dict['crawler'] = self.get_crawler_stats()\n        the_dict['rest'] = self.get_rest_stats()\n\n        return the_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_plugin_stats(self, name):\n        '''\n        Used for getting stats for Plugin based stuff, like Kafka Monitor\n        and Redis Monitor\n\n        @param name: the main class stats name\n        @return: A formatted dict of stats\n        '''\n        the_dict = {}\n\n        keys = self.redis_conn.keys('stats:{n}:*'.format(n=name))\n\n        for key in keys:\n            # break down key\n            elements = key.split(\":\")\n            main = elements[2]\n            end = elements[3]\n\n            if main == 'total' or main == 'fail':\n                if main not in the_dict:\n                    the_dict[main] = {}\n                the_dict[main][end] = self._get_key_value(key, end == 'lifetime')\n            elif main == 'self':\n                if 'nodes' not in the_dict:\n                    # main is self, end is machine, true_tail is uuid\n                    the_dict['nodes'] = {}\n                true_tail = elements[4]\n                if end not in the_dict['nodes']:\n                    the_dict['nodes'][end] = []\n                the_dict['nodes'][end].append(true_tail)\n            else:\n                if 'plugins' not in the_dict:\n                    the_dict['plugins'] = {}\n                if main not in the_dict['plugins']:\n                    the_dict['plugins'][main] = {}\n                the_dict['plugins'][main][end] = self._get_key_value(key, end == 'lifetime')\n\n        return the_dict", "response": "Used for getting stats for Plugin based stuff like Kafka Monitor\n        and Redis Monitor\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_key_value(self, key, is_hll=False):\n        '''\n        Returns the proper key value for the stats\n\n        @param key: the redis key\n        @param is_hll: the key is a HyperLogLog, else is a sorted set\n        '''\n        if is_hll:\n            # get hll value\n            return self.redis_conn.execute_command(\"PFCOUNT\", key)\n        else:\n            # get zcard value\n            return self.redis_conn.zcard(key)", "response": "Returns the proper key value for the stats\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_spider_stats(self):\n        '''\n        Gather spider based stats\n        '''\n        self.logger.debug(\"Gathering spider stats\")\n        the_dict = {}\n        spider_set = set()\n        total_spider_count = 0\n\n        keys = self.redis_conn.keys('stats:crawler:*:*:*')\n        for key in keys:\n            # we only care about the spider\n            elements = key.split(\":\")\n            spider = elements[3]\n\n            if spider not in the_dict:\n                the_dict[spider] = {}\n                the_dict[spider]['count'] = 0\n\n            if len(elements) == 6:\n                # got a time based stat\n                response = elements[4]\n                end = elements[5]\n\n                if response not in the_dict[spider]:\n                    the_dict[spider][response] = {}\n\n                the_dict[spider][response][end] = self._get_key_value(key, end == 'lifetime')\n\n            elif len(elements) == 5:\n                # got a spider identifier\n                the_dict[spider]['count'] += 1\n                total_spider_count += 1\n                spider_set.add(spider)\n\n            else:\n                self.logger.warn(\"Unknown crawler stat key\", {\"key\":key})\n\n        # simple counts\n        the_dict['unique_spider_count'] = len(spider_set)\n        the_dict['total_spider_count'] = total_spider_count\n\n        ret_dict = {}\n        ret_dict['spiders'] = the_dict\n\n        return ret_dict", "response": "Gather spider based stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_machine_stats(self):\n        '''\n        Gather spider based stats\n        '''\n        self.logger.debug(\"Gathering machine stats\")\n        the_dict = {}\n        keys = self.redis_conn.keys('stats:crawler:*:*:*:*')\n\n        for key in keys:\n            # break down key\n            elements = key.split(\":\")\n            machine = elements[2]\n            spider = elements[3]\n            response = elements[4]\n            end = elements[5]\n\n            # we only care about the machine, not spider type\n            if machine not in the_dict:\n                the_dict[machine] = {}\n\n            if response not in the_dict[machine]:\n                the_dict[machine][response] = {}\n\n            if end in the_dict[machine][response]:\n                the_dict[machine][response][end] = the_dict[machine][response][end] + \\\n                    self._get_key_value(key, end == 'lifetime')\n            else:\n                the_dict[machine][response][end] = self._get_key_value(key, end == 'lifetime')\n\n        # simple count\n        the_dict['count'] = len(list(the_dict.keys()))\n\n        ret_dict = {}\n        ret_dict['machines'] = the_dict\n\n        return ret_dict", "response": "Gather spider based stats\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_crawler_stats(self):\n        '''\n        Gather crawler stats\n\n        @return: A dict of stats\n        '''\n        self.logger.debug(\"Gathering crawler stats\")\n        the_dict = {}\n\n        the_dict['spiders'] = self.get_spider_stats()['spiders']\n        the_dict['machines'] = self.get_machine_stats()['machines']\n        the_dict['queue'] = self.get_queue_stats()['queues']\n\n        return the_dict", "response": "Gather crawler stats\n\n        @return: A dict of stats"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngathering queue based stats", "response": "def get_queue_stats(self):\n        '''\n        Gather queue stats\n\n        @return: A dict of stats\n        '''\n        self.logger.debug(\"Gathering queue based stats\")\n\n        the_dict = {}\n        keys = self.redis_conn.keys('*:*:queue')\n        total_backlog = 0\n        for key in keys:\n            elements = key.split(\":\")\n            spider = elements[0]\n            domain = elements[1]\n            spider = 'queue_' + spider\n\n            if spider not in the_dict:\n                the_dict[spider] = {\n                    'spider_backlog': 0,\n                    'num_domains': 0,\n                    'domains': []\n                }\n\n            count = self.redis_conn.zcard(key)\n            total_backlog += count\n            the_dict[spider]['spider_backlog'] += count\n            the_dict[spider]['num_domains'] += 1\n            the_dict[spider]['domains'].append({'domain': domain,\n                                                'backlog': count})\n\n        the_dict['total_backlog'] = total_backlog\n        ret_dict = {\n            'queues': the_dict\n        }\n\n        return ret_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if a is a subdict of b.", "response": "def is_subdict(self, a,b):\n        '''\n        Return True if a is a subdict of b\n        '''\n        return all((k in b and b[k]==v) for k,v in a.iteritems())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_log_level(self, level):\n        '''\n        Ensures a valid log level\n\n        @param level: the asked for level\n        '''\n        if level not in list(self.level_dict.keys()):\n            self.log_level = 'DEBUG'\n            self.logger.warn(\"Unknown log level '{lev}', defaulting to DEBUG\"\n                             .format(lev=level))", "response": "Ensures a valid log level"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the proper log formatter", "response": "def _get_formatter(self, json):\n        '''\n        Return the proper log formatter\n\n        @param json: Boolean value\n        '''\n        if json:\n            return jsonlogger.JsonFormatter()\n        else:\n            return logging.Formatter(self.format_string)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting an error message to the log", "response": "def debug(self, message, extra={}):\n        '''\n        Writes an error message to the log\n\n        @param message: The message to write\n        @param extra: The extras object to pass in\n        '''\n        if self.level_dict['DEBUG'] >= self.level_dict[self.log_level]:\n            extras = self.add_extras(extra, \"DEBUG\")\n            self._write_message(message, extras)\n            self.fire_callbacks('DEBUG', message, extra)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_message(self, message, extra):\n        '''\n        Writes the log output\n        @param message: The message to write\n        @param extra: The potential object to write\n        '''\n        if not self.json:\n            self._write_standard(message, extra)\n        else:\n            self._write_json(message, extra)", "response": "Writes the log output\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a standard log statement.", "response": "def _write_standard(self, message, extra):\n        '''\n        Writes a standard log statement\n\n        @param message: The message to write\n        @param extra: The object to pull defaults from\n        '''\n        level = extra['level']\n        if self.include_extra:\n            del extra['timestamp']\n            del extra['level']\n            del extra['logger']\n            if len(extra) > 0:\n                message += \" \" + str(extra)\n\n        if level == 'INFO':\n            self.logger.info(message)\n        elif level == 'DEBUG':\n            self.logger.debug(message)\n        elif level == 'WARNING':\n            self.logger.warning(message)\n        elif level == 'ERROR':\n            self.logger.error(message)\n        elif level == 'CRITICAL':\n            self.logger.critical(message)\n        else:\n            self.logger.debug(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a JSON message to the log.", "response": "def _write_json(self, message, extra):\n        '''\n        The JSON logger doesn't obey log levels\n\n        @param message: The message to write\n        @param extra: The object to write\n        '''\n        self.logger.info(message, extra=extra)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_extras(self, dict, level):\n        '''\n        Adds the log level to the dict object\n        '''\n        my_copy = copy.deepcopy(dict)\n        if 'level' not in my_copy:\n            my_copy['level'] = level\n        if 'timestamp' not in my_copy:\n            my_copy['timestamp'] = self._get_time()\n        if 'logger' not in my_copy:\n            my_copy['logger'] = self.name\n        return my_copy", "response": "Adds the log level to the dict object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _increment_504_stat(self, request):\n        '''\n        Increments the 504 stat counters\n\n        @param request: The scrapy request in the spider\n        '''\n        for key in self.stats_dict:\n            if key == 'lifetime':\n                unique = request.url + str(time.time())\n                self.stats_dict[key].increment(unique)\n            else:\n                self.stats_dict[key].increment()\n        self.logger.debug(\"Incremented status_code '504' stats\")", "response": "Increments the 504 stat counters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing all data associated with the throttled queue", "response": "def clear(self):\n        '''\n        Clears all data associated with the throttled queue\n        '''\n        self.redis_conn.delete(self.window_key)\n        self.redis_conn.delete(self.moderate_key)\n        self.queue.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef allowed(self):\n        '''\n        Check to see if the pop request is allowed\n\n        @return: True means the maximum was not been reached for the current\n            time window, thus allowing what ever operation follows\n        '''\n        # Expire old keys (hits)\n        expires = time.time() - self.window\n        self.redis_conn.zremrangebyscore(self.window_key, '-inf', expires)\n\n        # check if we are hitting too fast for moderation\n        if self.moderation:\n            with self.redis_conn.pipeline() as pipe:\n                try:\n                    pipe.watch(self.moderate_key)  # ---- LOCK\n                    # from this point onward if no errors are raised we\n                    # successfully incremented the counter\n\n                    curr_time = time.time()\n                    if self.is_moderated(curr_time, pipe) and not \\\n                            self.check_elastic():\n                        return False\n\n                    # passed the moderation limit, now check time window\n                    # If we have less keys than max, update out moderate key\n                    if self.test_hits():\n                        # this is a valid transaction, set the new time\n                        pipe.multi()\n                        pipe.set(name=self.moderate_key,\n                                 value=str(curr_time),\n                                 ex=int(self.window * 2))\n                        pipe.execute()\n                        return True\n\n                except WatchError:\n                    # watch was changed, another thread just incremented\n                    # the value\n                    return False\n\n        # If we currently have more keys than max,\n        # then limit the action\n        else:\n            return self.test_hits()\n\n        return False", "response": "Check to see if the pop request is allowed for the current key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_elastic(self):\n        '''\n        Checks if we need to break moderation in order to maintain our desired\n        throttle limit\n\n        @return: True if we need to break moderation\n        '''\n        if self.elastic and self.elastic_kick_in == self.limit:\n            value = self.redis_conn.zcard(self.window_key)\n            if self.limit - value > self.elastic_buffer:\n                return True\n        return False", "response": "Checks if we need to break moderation in order to maintain our desired\n        throttle limit\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest to see if the moderation limit is not exceeded", "response": "def is_moderated(self, curr_time, pipe):\n        '''\n        Tests to see if the moderation limit is not exceeded\n\n        @return: True if the moderation limit is exceeded\n        '''\n        # get key, otherwise default the moderate key expired and\n        # we dont care\n        value = pipe.get(self.moderate_key)\n        if value is None:\n            value = 0.0\n        else:\n            value = float(value)\n\n        # check moderation difference\n        if (curr_time - value) < self.moderation:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle(self, key, value):\n        '''\n        Processes a vaild action info request\n\n        @param key: The key that matched the request\n        @param value: The value associated with the key\n        '''\n        # the master dict to return\n        master = {}\n        master['uuid'] = value\n        master['total_pending'] = 0\n        master['server_time'] = int(self.get_current_time())\n\n        # break down key\n        elements = key.split(\":\")\n        dict = {}\n        dict['spiderid'] = elements[1]\n        dict['appid'] = elements[2]\n\n        # log we received the info message\n        extras = self.get_log_dict('info', dict['appid'],\n                                   dict['spiderid'], master['uuid'])\n\n        if len(elements) == 4:\n            dict['crawlid'] = elements[3]\n            extras = self.get_log_dict('info', dict['appid'],\n                                       dict['spiderid'], master['uuid'],\n                                       elements[3])\n        self.logger.info('Received info request', extra=extras)\n\n        # generate the information requested\n        if 'crawlid' in dict:\n            master = self._build_crawlid_info(master, dict)\n        else:\n            master = self._build_appid_info(master, dict)\n\n        if self._send_to_kafka(master):\n            extras['success'] = True\n            self.logger.info('Sent info to kafka', extra=extras)\n        else:\n            extras['success'] = False\n            self.logger.error('Failed to send info to kafka',\n                              extra=extras)", "response": "Processes a vaild action info request and returns the master dict that is used to store the result in the master dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_bin(self, key):\n        '''\n        Returns a binned dictionary based on redis zscore\n\n        @return: The sorted dict\n        '''\n        # keys based on score\n        sortedDict = {}\n        # this doesnt return them in order, need to bin first\n        for item in self.redis_conn.zscan_iter(key):\n            my_item = ujson.loads(item[0])\n            # score is negated in redis\n            my_score = -item[1]\n\n            if my_score not in sortedDict:\n                sortedDict[my_score] = []\n\n            sortedDict[my_score].append(my_item)\n\n        return sortedDict", "response": "Returns a binned dictionary based on redis zscore\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_appid_info(self, master, dict):\n        '''\n        Builds the appid info object\n\n        @param master: the master dict\n        @param dict: the dict object received\n        @return: the appid info object\n        '''\n        master['total_crawlids'] = 0\n        master['total_pending'] = 0\n        master['total_domains'] = 0\n        master['crawlids'] = {}\n        master['appid'] = dict['appid']\n        master['spiderid'] = dict['spiderid']\n\n        # used for finding total count of domains\n        domain_dict = {}\n\n        # get all domain queues\n        match_string = '{sid}:*:queue'.format(sid=dict['spiderid'])\n        for key in self.redis_conn.scan_iter(match=match_string):\n            domain = key.split(\":\")[1]\n            sortedDict = self._get_bin(key)\n\n            # now iterate through binned dict\n            for score in sortedDict:\n                for item in sortedDict[score]:\n                    if 'meta' in item:\n                        item = item['meta']\n                    if item['appid'] == dict['appid']:\n                        crawlid = item['crawlid']\n\n                        # add new crawlid to master dict\n                        if crawlid not in master['crawlids']:\n                            master['crawlids'][crawlid] = {\n                                'total': 0,\n                                'domains': {},\n                                'distinct_domains': 0\n                            }\n\n                            if 'expires' in item and item['expires'] != 0:\n                                master['crawlids'][crawlid]['expires'] = item['expires']\n\n                            master['total_crawlids'] += 1\n\n                        master['crawlids'][crawlid]['total'] = master['crawlids'][crawlid]['total'] + 1\n\n                        if domain not in master['crawlids'][crawlid]['domains']:\n                            master['crawlids'][crawlid]['domains'][domain] = {\n                                'total': 0,\n                                'high_priority': -9999,\n                                'low_priority': 9999,\n                            }\n                            master['crawlids'][crawlid]['distinct_domains'] += 1\n                            domain_dict[domain] = True\n\n\n                        master['crawlids'][crawlid]['domains'][domain]['total'] = master['crawlids'][crawlid]['domains'][domain]['total'] + 1\n\n                        if item['priority'] > master['crawlids'][crawlid]['domains'][domain]['high_priority']:\n                            master['crawlids'][crawlid]['domains'][domain]['high_priority'] = item['priority']\n\n                        if item['priority'] < master['crawlids'][crawlid]['domains'][domain]['low_priority']:\n                            master['crawlids'][crawlid]['domains'][domain]['low_priority'] = item['priority']\n\n                        master['total_pending'] += 1\n\n        master['total_domains'] = len(domain_dict)\n\n        return master", "response": "Builds the appid info object from the master dict and the dict object received by the master."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the crawlid info object from the master dict object received by the master object.", "response": "def _build_crawlid_info(self, master, dict):\n        '''\n        Builds the crawlid info object\n\n        @param master: the master dict\n        @param dict: the dict object received\n        @return: the crawlid info object\n        '''\n        master['total_pending'] = 0\n        master['total_domains'] = 0\n        master['appid'] = dict['appid']\n        master['crawlid'] = dict['crawlid']\n        master['spiderid'] = dict['spiderid']\n        master['domains'] = {}\n\n        timeout_key = 'timeout:{sid}:{aid}:{cid}'.format(sid=dict['spiderid'],\n                                                         aid=dict['appid'],\n                                                         cid=dict['crawlid'])\n        if self.redis_conn.exists(timeout_key):\n            master['expires'] = self.redis_conn.get(timeout_key)\n\n        # get all domain queues\n        match_string = '{sid}:*:queue'.format(sid=dict['spiderid'])\n        for key in self.redis_conn.scan_iter(match=match_string):\n            domain = key.split(\":\")[1]\n            sortedDict = self._get_bin(key)\n\n            # now iterate through binned dict\n            for score in sortedDict:\n                for item in sortedDict[score]:\n                    if 'meta' in item:\n                        item = item['meta']\n                    if item['appid'] == dict['appid'] and item['crawlid'] == dict['crawlid']:\n                        if domain not in master['domains']:\n                            master['domains'][domain] = {}\n                            master['domains'][domain]['total'] = 0\n                            master['domains'][domain]['high_priority'] = -9999\n                            master['domains'][domain]['low_priority'] = 9999\n                            master['total_domains'] = master['total_domains'] + 1\n\n                        master['domains'][domain]['total'] = master['domains'][domain]['total'] + 1\n\n                        if item['priority'] > master['domains'][domain]['high_priority']:\n                            master['domains'][domain]['high_priority'] = item['priority']\n\n                        if item['priority'] < master['domains'][domain]['low_priority']:\n                            master['domains'][domain]['low_priority'] = item['priority']\n\n                        master['total_pending'] = master['total_pending'] + 1\n\n        return master"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a stop request and send it to kafka", "response": "def handle(self, key, value):\n        '''\n        Processes a vaild action info request\n\n        @param key: The key that matched the request\n        @param value: The value associated with the key\n        '''\n        # break down key\n        elements = key.split(\":\")\n\n        if len(elements) != 4:\n            self.logger.warn(\"Stop requests need a crawlid and appid\")\n            return\n\n        spiderid = elements[1]\n        appid = elements[2]\n        crawlid = elements[3]\n        uuid = value\n\n        # log we received the stop message\n        extras = self.get_log_dict('stop', appid,\n                                   spiderid, uuid, crawlid)\n        self.logger.info('Received stop request', extra=extras)\n\n        redis_key = spiderid + \":blacklist\"\n        value = '{appid}||{crawlid}'.format(appid=appid,\n                                            crawlid=crawlid)\n\n        # add this to the blacklist set\n        self.redis_conn.sadd(redis_key, value)\n\n        # purge crawlid from current set\n        result = self._purge_crawl(spiderid, appid, crawlid)\n\n        # item to send to kafka\n        extras = {}\n        extras['action'] = \"stop\"\n        extras['spiderid'] = spiderid\n        extras['appid'] = appid\n        extras['crawlid'] = crawlid\n        extras['total_purged'] = result\n        extras['uuid'] = uuid\n        extras['server_time'] = int(self.get_current_time())\n\n        if self._send_to_kafka(extras):\n            # delete timeout for crawl (if needed) since stopped\n            timeout_key = 'timeout:{sid}:{aid}:{cid}'.format(\n                                    sid=spiderid,\n                                    aid=appid,\n                                    cid=crawlid)\n            self.redis_conn.delete(timeout_key)\n            extras['success'] = True\n            self.logger.info('Sent stop ack to kafka', extra=extras)\n        else:\n            extras['success'] = False\n            self.logger.error('Failed to send stop ack to kafka', extra=extras)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _purge_crawl(self, spiderid, appid, crawlid):\n        '''\n        Wrapper for purging the crawlid from the queues\n\n        @param spiderid: the spider id\n        @param appid: the app id\n        @param crawlid: the crawl id\n        @return: The number of requests purged\n        '''\n        # purge three times to try to make sure everything is cleaned\n        total = self._mini_purge(spiderid, appid, crawlid)\n        total = total + self._mini_purge(spiderid, appid, crawlid)\n        total = total + self._mini_purge(spiderid, appid, crawlid)\n\n        return total", "response": "Wrapper for purging the crawlid from the queues\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog the API call to the logger.", "response": "def log_call(call_name):\n    \"\"\"Log the API call to the logger.\"\"\"\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            instance = args[0]\n            instance.logger.info(call_name, {\"content\": request.get_json()})\n            return f(*args, **kw)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle unexpected errors within the rest function.", "response": "def error_catch(f):\n    \"\"\"Handle unexpected errors within the rest function.\"\"\"\n    @wraps(f)\n    def wrapper(*args, **kw):\n        instance = args[0]\n        try:\n            result = f(*args, **kw)\n            if isinstance(result, tuple):\n                return jsonify(result[0]), result[1]\n            else:\n                return jsonify(result), 200\n        except Exception as e:\n            ret_dict = instance._create_ret_object(instance.FAILURE, None,\n                                                   True,\n                                                   instance.UNKNOWN_ERROR)\n            log_dict = deepcopy(ret_dict)\n            log_dict['error']['cause'] = e.message\n            log_dict['error']['exception'] = str(e)\n            log_dict['error']['ex'] = traceback.format_exc()\n            instance.logger.error(\"Uncaught Exception Thrown\", log_dict)\n            return jsonify(ret_dict), 500\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_json(f):\n    @wraps(f)\n    def wrapper(*args, **kw):\n        instance = args[0]\n        try:\n            if request.get_json() is None:\n                ret_dict = instance._create_ret_object(instance.FAILURE,\n                                                       None, True,\n                                                       instance.MUST_JSON)\n                instance.logger.error(instance.MUST_JSON)\n                return jsonify(ret_dict), 400\n        except BadRequest:\n            ret_dict = instance._create_ret_object(instance.FAILURE, None,\n                                                   True,\n                                                   instance.MUST_JSON)\n            instance.logger.error(instance.MUST_JSON)\n            return jsonify(ret_dict), 400\n        instance.logger.debug(\"JSON is valid\")\n        return f(*args, **kw)\n    return wrapper", "response": "Validate that the call is JSON."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the JSON against a required schema_name.", "response": "def validate_schema(schema_name):\n    \"\"\"Validate the JSON against a required schema_name.\"\"\"\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kw):\n            instance = args[0]\n            try:\n                instance.validator(instance.schemas[schema_name]).validate(request.get_json())\n            except ValidationError, e:\n                ret_dict = instance._create_ret_object(instance.FAILURE,\n                                                       None, True,\n                                                       instance.BAD_SCHEMA,\n                                                       e.message)\n                instance.logger.error(\"Invalid Schema\", ret_dict)\n                return jsonify(ret_dict), 400\n            instance.logger.debug(\"Schema is valid\")\n            return f(*args, **kw)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads everything up. Note that any arg here will override both default and custom settings @param level: the log level @param log_file: boolean t/f whether to log to a file, else stdout @param json: boolean t/f whether to write the logs in json", "response": "def setup(self, level=None, log_file=None, json=None):\n        \"\"\"\n        Load everything up. Note that any arg here will override both\n        default and custom settings\n\n        @param level: the log level\n        @param log_file: boolean t/f whether to log to a file, else stdout\n        @param json: boolean t/f whether to write the logs in json\n        \"\"\"\n        self.settings = self.wrapper.load(self.settings_name)\n\n        my_level = level if level else self.settings['LOG_LEVEL']\n        # negate because logger wants True for std out\n        my_output = not log_file if log_file else self.settings['LOG_STDOUT']\n        my_json = json if json else self.settings['LOG_JSON']\n        self.logger = LogFactory.get_instance(json=my_json, stdout=my_output,\n                                              level=my_level,\n                                              name=self.settings['LOGGER_NAME'],\n                                              dir=self.settings['LOG_DIR'],\n                                              file=self.settings['LOG_FILE'],\n                                              bytes=self.settings['LOG_MAX_BYTES'],\n                                              backups=self.settings['LOG_BACKUPS'])\n\n        self._decorate_routes()\n        self._spawn_redis_connection_thread()\n        self._spawn_kafka_connection_thread()\n\n        # spawn heartbeat processing loop\n        self._heartbeat_thread = Thread(target=self._heartbeat_loop)\n        self._heartbeat_thread.setDaemon(True)\n        self._heartbeat_thread.start()\n\n        self.start_time = self.get_time()\n\n        # disable flask logger\n        if self.settings['FLASK_LOGGING_ENABLED'] == False:\n            log = logging.getLogger('werkzeug')\n            log.disabled = True\n\n        self._load_schemas()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_schemas(self):\n        for filename in os.listdir(self.settings['SCHEMA_DIR']):\n            if filename[-4:] == 'json':\n                name = filename[:-5]\n                with open(self.settings['SCHEMA_DIR'] + filename) as the_file:\n                    self.schemas[name] = json.load(the_file)\n                    self.logger.debug(\"Successfully loaded \" + filename + \" schema\")", "response": "Loads any schemas for JSON validation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nspawn a redis connection thread", "response": "def _spawn_redis_connection_thread(self):\n        \"\"\"Spawns a redis connection thread\"\"\"\n        self.logger.debug(\"Spawn redis connection thread\")\n        self.redis_connected = False\n        self._redis_thread = Thread(target=self._setup_redis)\n        self._redis_thread.setDaemon(True)\n        self._redis_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _spawn_kafka_connection_thread(self):\n        self.logger.debug(\"Spawn kafka connection thread\")\n        self.kafka_connected = False\n        self._kafka_thread = Thread(target=self._setup_kafka)\n        self._kafka_thread.setDaemon(True)\n        self._kafka_thread.start()", "response": "Spawns a kafka connection thread"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nspawning a kafka continuous consumer thread", "response": "def _spawn_kafka_consumer_thread(self):\n        \"\"\"Spawns a kafka continuous consumer thread\"\"\"\n        self.logger.debug(\"Spawn kafka consumer thread\"\"\")\n        self._consumer_thread = Thread(target=self._consumer_loop)\n        self._consumer_thread.setDaemon(True)\n        self._consumer_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_messages(self):\n        try:\n            for message in self.consumer:\n                try:\n                    if message is None:\n                        self.logger.debug(\"no message\")\n                        break\n                    loaded_dict = json.loads(message.value)\n                    self.logger.debug(\"got valid kafka message\")\n\n                    with self.uuids_lock:\n                        if 'uuid' in loaded_dict:\n                            if loaded_dict['uuid'] in self.uuids and \\\n                                    self.uuids[loaded_dict['uuid']] != 'poll':\n                                self.logger.debug(\"Found Kafka message from request\")\n                                self.uuids[loaded_dict['uuid']] = loaded_dict\n                            else:\n                                self.logger.debug(\"Got poll result\")\n                                self._send_result_to_redis(loaded_dict)\n                        else:\n                            self.logger.debug(\"Got message not intended for this process\")\n                except ValueError:\n                    extras = {}\n                    if message is not None:\n                            extras[\"data\"] = message.value\n                    self.logger.warning('Unparseable JSON Received from kafka',\n                                                extra=extras)\n\n            self._check_kafka_disconnect()\n\n        except OffsetOutOfRangeError:\n            # consumer has no idea where they are\n            self.consumer.seek_to_end()\n            self.logger.error(\"Kafka offset out of range error\")", "response": "Processes messages received from kafka"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _send_result_to_redis(self, result):\n        if self.redis_connected:\n            self.logger.debug(\"Sending result to redis\")\n            try:\n                key = \"rest:poll:{u}\".format(u=result['uuid'])\n                self.redis_conn.set(key, json.dumps(result))\n            except ConnectionError:\n                self.logger.error(\"Lost connection to Redis\")\n                self._spawn_redis_connection_thread()\n        else:\n            self.logger.warning(\"Unable to send result to redis, not connected\")", "response": "Sends the result of a poll to redis to be used potentially by\n        another process\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_kafka_disconnect(self):\n        for node_id in self.consumer._client._conns:\n            conn = self.consumer._client._conns[node_id]\n            if conn.state == ConnectionStates.DISCONNECTED or \\\n                    conn.state == ConnectionStates.DISCONNECTING:\n                self._spawn_kafka_connection_thread()\n                break", "response": "Checks the kafka connection is still valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _report_self(self):\n        if self.redis_connected:\n            self.logger.debug(\"Reporting self to redis\")\n            try:\n                key = \"stats:rest:self:{m}:{u}\".format(\n                    m=socket.gethostname(),\n                    u=self.my_uuid)\n                self.redis_conn.set(key, self.get_time())\n                self.redis_conn.expire(key, self.settings['HEARTBEAT_TIMEOUT'])\n            except ConnectionError:\n                self.logger.error(\"Lost connection to Redis\")\n                self._spawn_redis_connection_thread()\n        else:\n            self.logger.warn(\"Cannot report self to redis, not connected\")", "response": "Report the crawler uuid to redis"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a Redis Client", "response": "def _setup_redis(self):\n        \"\"\"Returns a Redis Client\"\"\"\n        if not self.closed:\n            try:\n                self.logger.debug(\"Creating redis connection to host \" +\n                                  str(self.settings['REDIS_HOST']))\n                self.redis_conn = redis.StrictRedis(host=self.settings['REDIS_HOST'],\n                                              port=self.settings['REDIS_PORT'],\n                                              db=self.settings['REDIS_DB'])\n                self.redis_conn.info()\n                self.redis_connected = True\n                self.logger.info(\"Successfully connected to redis\")\n            except KeyError as e:\n                self.logger.error('Missing setting named ' + str(e),\n                                   {'ex': traceback.format_exc()})\n            except:\n                self.logger.error(\"Couldn't initialize redis client.\",\n                                   {'ex': traceback.format_exc()})\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_kafka(self):\n        # close older connections\n        if self.consumer is not None:\n            self.logger.debug(\"Closing existing kafka consumer\")\n            self.consumer.close()\n            self.consumer = None\n        if self.producer is not None:\n            self.logger.debug(\"Closing existing kafka producer\")\n            self.producer.flush()\n            self.producer.close(timeout=10)\n            self.producer = None\n\n        # create new connections\n        self._consumer_thread = None\n        self.logger.debug(\"Creating kafka connections\")\n        self.consumer = self._create_consumer()\n        if not self.closed:\n            self.logger.debug(\"Kafka Conumer created\")\n        self.producer = self._create_producer()\n        if not self.closed:\n            self.logger.debug(\"Kafka Producer created\")\n\n        if not self.closed:\n            self.kafka_connected = True\n            self.logger.info(\"Connected successfully to Kafka\")\n            self._spawn_kafka_consumer_thread()", "response": "Sets up kafka connections"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_consumer(self):\n        if not self.closed:\n            try:\n                self.logger.debug(\"Creating new kafka consumer using brokers: \" +\n                                   str(self.settings['KAFKA_HOSTS']) + ' and topic ' +\n                                   self.settings['KAFKA_TOPIC_PREFIX'] +\n                                   \".outbound_firehose\")\n\n                return KafkaConsumer(\n                    self.settings['KAFKA_TOPIC_PREFIX'] + \".outbound_firehose\",\n                    group_id=None,\n                    bootstrap_servers=self.settings['KAFKA_HOSTS'],\n                    consumer_timeout_ms=self.settings['KAFKA_CONSUMER_TIMEOUT'],\n                    auto_offset_reset=self.settings['KAFKA_CONSUMER_AUTO_OFFSET_RESET'],\n                    auto_commit_interval_ms=self.settings['KAFKA_CONSUMER_COMMIT_INTERVAL_MS'],\n                    enable_auto_commit=self.settings['KAFKA_CONSUMER_AUTO_COMMIT_ENABLE'],\n                    max_partition_fetch_bytes=self.settings['KAFKA_CONSUMER_FETCH_MESSAGE_MAX_BYTES'])\n            except KeyError as e:\n                self.logger.error('Missing setting named ' + str(e),\n                                   {'ex': traceback.format_exc()})\n            except:\n                self.logger.error(\"Couldn't initialize kafka consumer for topic\",\n                                   {'ex': traceback.format_exc()})\n                raise", "response": "Creates a new KafkaConsumer instance based on the current settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to establish a Kafka consumer connection", "response": "def _create_producer(self):\n        \"\"\"Tries to establish a Kafka consumer connection\"\"\"\n        if not self.closed:\n            try:\n                self.logger.debug(\"Creating new kafka producer using brokers: \" +\n                                   str(self.settings['KAFKA_HOSTS']))\n\n                return KafkaProducer(bootstrap_servers=self.settings['KAFKA_HOSTS'],\n                                     value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n                                     retries=3,\n                                     linger_ms=self.settings['KAFKA_PRODUCER_BATCH_LINGER_MS'],\n                                     buffer_memory=self.settings['KAFKA_PRODUCER_BUFFER_BYTES'])\n            except KeyError as e:\n                self.logger.error('Missing setting named ' + str(e),\n                                   {'ex': traceback.format_exc()})\n            except:\n                self.logger.error(\"Couldn't initialize kafka producer.\",\n                                   {'ex': traceback.format_exc()})\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates generic reponse objects.", "response": "def _create_ret_object(self, status=SUCCESS, data=None, error=False,\n                           error_message=None, error_cause=None):\n        \"\"\"\n        Create generic reponse objects.\n\n        :param str status: The SUCCESS or FAILURE of the request\n        :param obj data: The data to return\n        :param bool error: Set to True to add Error response\n        :param str error_message: The generic error message\n        :param str error_cause: The cause of the error\n        :returns: A dictionary of values\n        \"\"\"\n        ret = {}\n        if status == self.FAILURE:\n            ret['status'] = self.FAILURE\n        else:\n            ret['status'] = self.SUCCESS\n        ret['data'] = data\n\n        if error:\n            ret['error'] = {}\n            if error_message is not None:\n                ret['error']['message'] = error_message\n            if error_cause is not None:\n                ret['error']['cause'] = error_cause\n        else:\n            ret['error'] = None\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses daemon threads @param thread: the thread to close @param thread_name: a human readable name of the thread", "response": "def _close_thread(self, thread, thread_name):\n        \"\"\"Closes daemon threads\n\n        @param thread: the thread to close\n        @param thread_name: a human readable name of the thread\n        \"\"\"\n        if thread is not None and thread.isAlive():\n            self.logger.debug(\"Waiting for {} thread to close\".format(thread_name))\n            thread.join(timeout=self.settings['DAEMON_THREAD_JOIN_TIMEOUT'])\n            if thread.isAlive():\n                self.logger.warn(\"{} daemon thread unable to be shutdown\"\n                                 \" within timeout\".format(thread_name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosing all resources associated with this instance.", "response": "def close(self):\n        \"\"\"\n        Cleans up anything from the process\n        \"\"\"\n        self.logger.info(\"Closing Rest Service\")\n        self.closed = True\n\n        # close threads\n        self._close_thread(self._redis_thread, \"Redis setup\")\n        self._close_thread(self._heartbeat_thread, \"Heartbeat\")\n        self._close_thread(self._kafka_thread, \"Kafka setup\")\n        self._close_thread(self._consumer_thread, \"Consumer\")\n\n        # close kafka\n        if self.consumer is not None:\n            self.logger.debug(\"Closing kafka consumer\")\n            self.consumer.close()\n        if self.producer is not None:\n            self.logger.debug(\"Closing kafka producer\")\n            self.producer.close(timeout=10)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string representation of the node health", "response": "def _calculate_health(self):\n        \"\"\"Returns a string representation of the node health\n\n        @returns: GREEN if fully connected, YELLOW if partially connected,\n                  RED if not connected\n        \"\"\"\n        if self.redis_connected and self.kafka_connected:\n            return \"GREEN\"\n        elif self.redis_connected or self.kafka_connected:\n            return \"YELLOW\"\n        else:\n            return \"RED\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _feed_to_kafka(self, json_item):\n        @MethodTimer.timeout(self.settings['KAFKA_FEED_TIMEOUT'], False)\n        def _feed(json_item):\n            try:\n                self.logger.debug(\"Sending json to kafka at \" +\n                                  str(self.settings['KAFKA_PRODUCER_TOPIC']))\n                future = self.producer.send(self.settings['KAFKA_PRODUCER_TOPIC'],\n                                   json_item)\n                future.add_callback(self._kafka_success)\n                future.add_errback(self._kafka_failure)\n\n                self.producer.flush()\n\n                return True\n\n            except Exception as e:\n                self.logger.error(\"Lost connection to Kafka\")\n                self._spawn_kafka_connection_thread()\n                return False\n\n        return _feed(json_item)", "response": "Sends a request to Kafka and returns True if the data was sent successfully False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecorating the routes to use within the flask app", "response": "def _decorate_routes(self):\n        \"\"\"\n        Decorates the routes to use within the flask app\n        \"\"\"\n        self.logger.debug(\"Decorating routes\")\n        # self.app.add_url_rule('/', 'catch', self.catch, methods=['GET'],\n        #                        defaults={'path': ''})\n        self.app.add_url_rule('/<path:path>', 'catch', self.catch,\n                              methods=['GET', 'POST'], defaults={'path': ''})\n        self.app.add_url_rule('/', 'index', self.index,\n                              methods=['POST', 'GET'])\n        self.app.add_url_rule('/feed', 'feed', self.feed,\n                              methods=['POST'])\n        self.app.add_url_rule('/poll', 'poll', self.poll,\n                              methods=['POST'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve older requests that may not make it back quick enough", "response": "def poll(self):\n        \"\"\"Retrieves older requests that may not make it back quick\n        enough\"\"\"\n        if self.redis_connected:\n            json_item = request.get_json()\n            result = None\n            try:\n                key = \"rest:poll:{u}\".format(u=json_item['poll_id'])\n                result = self.redis_conn.get(key)\n\n                if result is not None:\n                    result = json.loads(result)\n                    self.logger.debug(\"Found previous poll\")\n                    self.redis_conn.delete(key)\n                    return self._create_ret_object(self.SUCCESS, result)\n                else:\n                    self.logger.debug(\"poll key does not exist\")\n                    return self._create_ret_object(self.FAILURE, None, True,\n                                       \"Could not find matching poll_id\"), 404\n            except ConnectionError:\n                self.logger.error(\"Lost connection to Redis\")\n                self._spawn_redis_connection_thread()\n            except ValueError:\n                extras = {\n                    \"value\": result\n                }\n                self.logger.warning('Unparseable JSON Received from redis',\n                                                extra=extras)\n                self.redis_conn.delete(key)\n                return self._create_ret_object(self.FAILURE, None, True,\n                                               \"Unparseable JSON Received \"\n                                               \"from redis\"), 500\n        self.logger.warn(\"Unable to poll redis, not connected\")\n        return self._create_ret_object(self.FAILURE, None, True,\n                                       \"Unable to connect to Redis\"), 500"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_token(encoded_token, csrf_value=None, allow_expired=False):\n    jwt_manager = _get_jwt_manager()\n    unverified_claims = jwt.decode(\n        encoded_token, verify=False, algorithms=config.algorithm\n    )\n    unverified_headers = jwt.get_unverified_header(encoded_token)\n    # Attempt to call callback with both claims and headers, but fallback to just claims\n    # for backwards compatibility\n    try:\n        secret = jwt_manager._decode_key_callback(unverified_claims, unverified_headers)\n    except TypeError:\n        msg = (\n            \"The single-argument (unverified_claims) form of decode_key_callback \",\n            \"is deprecated. Update your code to use the two-argument form \",\n            \"(unverified_claims, unverified_headers).\"\n        )\n        warn(msg, DeprecationWarning)\n        secret = jwt_manager._decode_key_callback(unverified_claims)\n\n    try:\n        return decode_jwt(\n            encoded_token=encoded_token,\n            secret=secret,\n            algorithm=config.algorithm,\n            identity_claim_key=config.identity_claim_key,\n            user_claims_key=config.user_claims_key,\n            csrf_value=csrf_value,\n            audience=config.audience,\n            leeway=config.leeway,\n            allow_expired=allow_expired\n        )\n    except ExpiredSignatureError:\n        expired_token = decode_jwt(\n            encoded_token=encoded_token,\n            secret=secret,\n            algorithm=config.algorithm,\n            identity_claim_key=config.identity_claim_key,\n            user_claims_key=config.user_claims_key,\n            csrf_value=csrf_value,\n            audience=config.audience,\n            leeway=config.leeway,\n            allow_expired=True\n        )\n        ctx_stack.top.expired_jwt = expired_token\n        raise", "response": "Decodes an encoded JWT into a python dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_access_token(identity, fresh=False, expires_delta=None, user_claims=None):\n    jwt_manager = _get_jwt_manager()\n    return jwt_manager._create_access_token(identity, fresh, expires_delta, user_claims)", "response": "Create a new access token."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_refresh_token(identity, expires_delta=None, user_claims=None):\n    jwt_manager = _get_jwt_manager()\n    return jwt_manager._create_refresh_token(identity, expires_delta, user_claims)", "response": "Creates a new refresh token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_access_cookies(response, encoded_access_token, max_age=None):\n    if not config.jwt_in_cookies:\n        raise RuntimeWarning(\"set_access_cookies() called without \"\n                             \"'JWT_TOKEN_LOCATION' configured to use cookies\")\n\n    # Set the access JWT in the cookie\n    response.set_cookie(config.access_cookie_name,\n                        value=encoded_access_token,\n                        max_age=max_age or config.cookie_max_age,\n                        secure=config.cookie_secure,\n                        httponly=True,\n                        domain=config.cookie_domain,\n                        path=config.access_cookie_path,\n                        samesite=config.cookie_samesite)\n\n    # If enabled, set the csrf double submit access cookie\n    if config.csrf_protect and config.csrf_in_cookies:\n        response.set_cookie(config.access_csrf_cookie_name,\n                            value=get_csrf_token(encoded_access_token),\n                            max_age=max_age or config.cookie_max_age,\n                            secure=config.cookie_secure,\n                            httponly=False,\n                            domain=config.cookie_domain,\n                            path=config.access_csrf_cookie_path,\n                            samesite=config.cookie_samesite)", "response": "Sets the access token in the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_refresh_cookies(response, encoded_refresh_token, max_age=None):\n    if not config.jwt_in_cookies:\n        raise RuntimeWarning(\"set_refresh_cookies() called without \"\n                             \"'JWT_TOKEN_LOCATION' configured to use cookies\")\n\n    # Set the refresh JWT in the cookie\n    response.set_cookie(config.refresh_cookie_name,\n                        value=encoded_refresh_token,\n                        max_age=max_age or config.cookie_max_age,\n                        secure=config.cookie_secure,\n                        httponly=True,\n                        domain=config.cookie_domain,\n                        path=config.refresh_cookie_path,\n                        samesite=config.cookie_samesite)\n\n    # If enabled, set the csrf double submit refresh cookie\n    if config.csrf_protect and config.csrf_in_cookies:\n        response.set_cookie(config.refresh_csrf_cookie_name,\n                            value=get_csrf_token(encoded_refresh_token),\n                            max_age=max_age or config.cookie_max_age,\n                            secure=config.cookie_secure,\n                            httponly=False,\n                            domain=config.cookie_domain,\n                            path=config.refresh_csrf_cookie_path,\n                            samesite=config.cookie_samesite)", "response": "Sets the refresh token in the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the access token from the response cookies.", "response": "def unset_access_cookies(response):\n    \"\"\"\n    takes a flask response object, and configures it to unset (delete) the\n    access token from the response cookies. if `jwt_csrf_in_cookies`\n    (see :ref:`configuration options`) is `true`, this will also remove the\n    access csrf double submit value from the response cookies as well.\n\n    :param response: the flask response object to delete the jwt cookies in.\n    \"\"\"\n    if not config.jwt_in_cookies:\n        raise RuntimeWarning(\"unset_refresh_cookies() called without \"\n                             \"'JWT_TOKEN_LOCATION' configured to use cookies\")\n\n    response.set_cookie(config.access_cookie_name,\n                        value='',\n                        expires=0,\n                        secure=config.cookie_secure,\n                        httponly=True,\n                        domain=config.cookie_domain,\n                        path=config.access_cookie_path,\n                        samesite=config.cookie_samesite)\n\n    if config.csrf_protect and config.csrf_in_cookies:\n        response.set_cookie(config.access_csrf_cookie_name,\n                            value='',\n                            expires=0,\n                            secure=config.cookie_secure,\n                            httponly=False,\n                            domain=config.cookie_domain,\n                            path=config.access_csrf_cookie_path,\n                            samesite=config.cookie_samesite)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unset_refresh_cookies(response):\n    if not config.jwt_in_cookies:\n        raise RuntimeWarning(\"unset_refresh_cookies() called without \"\n                             \"'JWT_TOKEN_LOCATION' configured to use cookies\")\n\n    response.set_cookie(config.refresh_cookie_name,\n                        value='',\n                        expires=0,\n                        secure=config.cookie_secure,\n                        httponly=True,\n                        domain=config.cookie_domain,\n                        path=config.refresh_cookie_path,\n                        samesite=config.cookie_samesite)\n\n    if config.csrf_protect and config.csrf_in_cookies:\n        response.set_cookie(config.refresh_csrf_cookie_name,\n                            value='',\n                            expires=0,\n                            secure=config.cookie_secure,\n                            httponly=False,\n                            domain=config.cookie_domain,\n                            path=config.refresh_csrf_cookie_path,\n                            samesite=config.cookie_samesite)", "response": "Removes the refresh token from the response cookies."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_access_token(identity, secret, algorithm, expires_delta, fresh,\n                        user_claims, csrf, identity_claim_key, user_claims_key,\n                        json_encoder=None):\n    \"\"\"\n    Creates a new encoded (utf-8) access token.\n\n    :param identity: Identifier for who this token is for (ex, username). This\n                     data must be json serializable\n    :param secret: Secret key to encode the JWT with\n    :param algorithm: Which algorithm to encode this JWT with\n    :param expires_delta: How far in the future this token should expire\n                          (set to False to disable expiration)\n    :type expires_delta: datetime.timedelta or False\n    :param fresh: If this should be a 'fresh' token or not. If a\n                  datetime.timedelta is given this will indicate how long this\n                  token will remain fresh.\n    :param user_claims: Custom claims to include in this token. This data must\n                        be json serializable\n    :param csrf: Whether to include a csrf double submit claim in this token\n                 (boolean)\n    :param identity_claim_key: Which key should be used to store the identity\n    :param user_claims_key: Which key should be used to store the user claims\n    :return: Encoded access token\n    \"\"\"\n\n    if isinstance(fresh, datetime.timedelta):\n        now = datetime.datetime.utcnow()\n        fresh = timegm((now + fresh).utctimetuple())\n\n    token_data = {\n        identity_claim_key: identity,\n        'fresh': fresh,\n        'type': 'access',\n    }\n\n    # Don't add extra data to the token if user_claims is empty.\n    if user_claims:\n        token_data[user_claims_key] = user_claims\n\n    if csrf:\n        token_data['csrf'] = _create_csrf_token()\n    return _encode_jwt(token_data, expires_delta, secret, algorithm,\n                       json_encoder=json_encoder)", "response": "Encodes an access token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode_refresh_token(identity, secret, algorithm, expires_delta, user_claims,\n                         csrf, identity_claim_key, user_claims_key,\n                         json_encoder=None):\n    \"\"\"\n    Creates a new encoded (utf-8) refresh token.\n\n    :param identity: Some identifier used to identify the owner of this token\n    :param secret: Secret key to encode the JWT with\n    :param algorithm: Which algorithm to use for the toek\n    :param expires_delta: How far in the future this token should expire\n                          (set to False to disable expiration)\n    :type expires_delta: datetime.timedelta or False\n    :param user_claims: Custom claims to include in this token. This data must\n                        be json serializable\n    :param csrf: Whether to include a csrf double submit claim in this token\n                 (boolean)\n    :param identity_claim_key: Which key should be used to store the identity\n    :param user_claims_key: Which key should be used to store the user claims\n    :return: Encoded refresh token\n    \"\"\"\n    token_data = {\n        identity_claim_key: identity,\n        'type': 'refresh',\n    }\n\n    # Don't add extra data to the token if user_claims is empty.\n    if user_claims:\n        token_data[user_claims_key] = user_claims\n\n    if csrf:\n        token_data['csrf'] = _create_csrf_token()\n    return _encode_jwt(token_data, expires_delta, secret, algorithm,\n                       json_encoder=json_encoder)", "response": "Encodes a refresh token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_jwt(encoded_token, secret, algorithm, identity_claim_key,\n               user_claims_key, csrf_value=None, audience=None,\n               leeway=0, allow_expired=False):\n    \"\"\"\n    Decodes an encoded JWT\n\n    :param encoded_token: The encoded JWT string to decode\n    :param secret: Secret key used to encode the JWT\n    :param algorithm: Algorithm used to encode the JWT\n    :param identity_claim_key: expected key that contains the identity\n    :param user_claims_key: expected key that contains the user claims\n    :param csrf_value: Expected double submit csrf value\n    :param audience: expected audience in the JWT\n    :param leeway: optional leeway to add some margin around expiration times\n    :param allow_expired: Options to ignore exp claim validation in token\n    :return: Dictionary containing contents of the JWT\n    \"\"\"\n    options = {}\n    if allow_expired:\n        options['verify_exp'] = False\n\n    # This call verifies the ext, iat, nbf, and aud claims\n    data = jwt.decode(encoded_token, secret, algorithms=[algorithm], audience=audience,\n                      leeway=leeway, options=options)\n\n    # Make sure that any custom claims we expect in the token are present\n    if 'jti' not in data:\n        data['jti'] = None\n    if identity_claim_key not in data:\n        raise JWTDecodeError(\"Missing claim: {}\".format(identity_claim_key))\n    if 'type' not in data:\n        data['type'] = 'access'\n    if data['type'] not in ('refresh', 'access'):\n        raise JWTDecodeError(\"Missing or invalid claim: type\")\n    if data['type'] == 'access':\n        if 'fresh' not in data:\n            data['fresh'] = False\n    if user_claims_key not in data:\n        data[user_claims_key] = {}\n    if csrf_value:\n        if 'csrf' not in data:\n            raise JWTDecodeError(\"Missing claim: csrf\")\n        if not safe_str_cmp(data['csrf'], csrf_value):\n            raise CSRFError(\"CSRF double submit tokens do not match\")\n    return data", "response": "Decodes an encoded JWT into a dictionary containing the contents of the JWT\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_token_to_database(encoded_token, identity_claim):\n    decoded_token = decode_token(encoded_token)\n    jti = decoded_token['jti']\n    token_type = decoded_token['type']\n    user_identity = decoded_token[identity_claim]\n    expires = _epoch_utc_to_datetime(decoded_token['exp'])\n    revoked = False\n\n    db_token = TokenBlacklist(\n        jti=jti,\n        token_type=token_type,\n        user_identity=user_identity,\n        expires=expires,\n        revoked=revoked,\n    )\n    db.session.add(db_token)\n    db.session.commit()", "response": "Adds a new token to the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the given token is revoked or not.", "response": "def is_token_revoked(decoded_token):\n    \"\"\"\n    Checks if the given token is revoked or not. Because we are adding all the\n    tokens that we create into this database, if the token is not present\n    in the database we are going to consider it revoked, as we don't know where\n    it was created.\n    \"\"\"\n    jti = decoded_token['jti']\n    try:\n        token = TokenBlacklist.query.filter_by(jti=jti).one()\n        return token.revoked\n    except NoResultFound:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrevoking the given token. Raises a TokenNotFound error if the token does not exist in the database. Raises a TokenNotFound error if the token does not exist in the database. Raises a TokenNotFound error if the token does not exist in the database.", "response": "def revoke_token(token_id, user):\n    \"\"\"\n    Revokes the given token. Raises a TokenNotFound error if the token does\n    not exist in the database\n    \"\"\"\n    try:\n        token = TokenBlacklist.query.filter_by(id=token_id, user_identity=user).one()\n        token.revoked = True\n        db.session.commit()\n    except NoResultFound:\n        raise TokenNotFound(\"Could not find the token {}\".format(token_id))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete all expired tokens from the database.", "response": "def prune_database():\n    \"\"\"\n    Delete tokens that have expired from the database.\n\n    How (and if) you call this is entirely up you. You could expose it to an\n    endpoint that only administrators could call, you could run it as a cron,\n    set it up with flask cli, etc.\n    \"\"\"\n    now = datetime.now()\n    expired = TokenBlacklist.query.filter(TokenBlacklist.expires < now).all()\n    for token in expired:\n        db.session.delete(token)\n    db.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_app(self, app):\n        # Save this so we can use it later in the extension\n        if not hasattr(app, 'extensions'):   # pragma: no cover\n            app.extensions = {}\n        app.extensions['flask-jwt-extended'] = self\n\n        # Set all the default configurations for this extension\n        self._set_default_configuration_options(app)\n        self._set_error_handler_callbacks(app)", "response": "Register this extension with the flask app."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the error handler callbacks used by this extension.", "response": "def _set_error_handler_callbacks(self, app):\n        \"\"\"\n        Sets the error handler callbacks used by this extension\n        \"\"\"\n        @app.errorhandler(NoAuthorizationError)\n        def handle_auth_error(e):\n            return self._unauthorized_callback(str(e))\n\n        @app.errorhandler(CSRFError)\n        def handle_csrf_error(e):\n            return self._unauthorized_callback(str(e))\n\n        @app.errorhandler(ExpiredSignatureError)\n        def handle_expired_error(e):\n            try:\n                token = ctx_stack.top.expired_jwt\n                return self._expired_token_callback(token)\n            except TypeError:\n                msg = (\n                    \"jwt.expired_token_loader callback now takes the expired token \"\n                    \"as an additional paramter. Example: expired_callback(token)\"\n                )\n                warn(msg, DeprecationWarning)\n                return self._expired_token_callback()\n\n        @app.errorhandler(InvalidHeaderError)\n        def handle_invalid_header_error(e):\n            return self._invalid_token_callback(str(e))\n\n        @app.errorhandler(InvalidTokenError)\n        def handle_invalid_token_error(e):\n            return self._invalid_token_callback(str(e))\n\n        @app.errorhandler(JWTDecodeError)\n        def handle_jwt_decode_error(e):\n            return self._invalid_token_callback(str(e))\n\n        @app.errorhandler(WrongTokenError)\n        def handle_wrong_token_error(e):\n            return self._invalid_token_callback(str(e))\n\n        @app.errorhandler(InvalidAudienceError)\n        def handle_invalid_audience_error(e):\n            return self._invalid_token_callback(str(e))\n\n        @app.errorhandler(RevokedTokenError)\n        def handle_revoked_token_error(e):\n            return self._revoked_token_callback()\n\n        @app.errorhandler(FreshTokenRequired)\n        def handle_fresh_token_required(e):\n            return self._needs_fresh_token_callback()\n\n        @app.errorhandler(UserLoadError)\n        def handler_user_load_error(e):\n            # The identity is already saved before this exception was raised,\n            # otherwise a different exception would be raised, which is why we\n            # can safely call get_jwt_identity() here\n            identity = get_jwt_identity()\n            return self._user_loader_error_callback(identity)\n\n        @app.errorhandler(UserClaimsVerificationError)\n        def handle_failed_user_claims_verification(e):\n            return self._verify_claims_failed_callback()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the default configuration options for the extension.", "response": "def _set_default_configuration_options(app):\n        \"\"\"\n        Sets the default configuration options used by this extension\n        \"\"\"\n        # Where to look for the JWT. Available options are cookies or headers\n        app.config.setdefault('JWT_TOKEN_LOCATION', ('headers',))\n\n        # Options for JWTs when the TOKEN_LOCATION is headers\n        app.config.setdefault('JWT_HEADER_NAME', 'Authorization')\n        app.config.setdefault('JWT_HEADER_TYPE', 'Bearer')\n\n        # Options for JWTs then the TOKEN_LOCATION is query_string\n        app.config.setdefault('JWT_QUERY_STRING_NAME', 'jwt')\n\n        # Option for JWTs when the TOKEN_LOCATION is cookies\n        app.config.setdefault('JWT_ACCESS_COOKIE_NAME', 'access_token_cookie')\n        app.config.setdefault('JWT_REFRESH_COOKIE_NAME', 'refresh_token_cookie')\n        app.config.setdefault('JWT_ACCESS_COOKIE_PATH', '/')\n        app.config.setdefault('JWT_REFRESH_COOKIE_PATH', '/')\n        app.config.setdefault('JWT_COOKIE_SECURE', False)\n        app.config.setdefault('JWT_COOKIE_DOMAIN', None)\n        app.config.setdefault('JWT_SESSION_COOKIE', True)\n        app.config.setdefault('JWT_COOKIE_SAMESITE', None)\n\n        # Option for JWTs when the TOKEN_LOCATION is json\n        app.config.setdefault('JWT_JSON_KEY', 'access_token')\n        app.config.setdefault('JWT_REFRESH_JSON_KEY', 'refresh_token')\n\n        # Options for using double submit csrf protection\n        app.config.setdefault('JWT_COOKIE_CSRF_PROTECT', True)\n        app.config.setdefault('JWT_CSRF_METHODS', ['POST', 'PUT', 'PATCH', 'DELETE'])\n        app.config.setdefault('JWT_ACCESS_CSRF_HEADER_NAME', 'X-CSRF-TOKEN')\n        app.config.setdefault('JWT_REFRESH_CSRF_HEADER_NAME', 'X-CSRF-TOKEN')\n        app.config.setdefault('JWT_CSRF_IN_COOKIES', True)\n        app.config.setdefault('JWT_ACCESS_CSRF_COOKIE_NAME', 'csrf_access_token')\n        app.config.setdefault('JWT_REFRESH_CSRF_COOKIE_NAME', 'csrf_refresh_token')\n        app.config.setdefault('JWT_ACCESS_CSRF_COOKIE_PATH', '/')\n        app.config.setdefault('JWT_REFRESH_CSRF_COOKIE_PATH', '/')\n\n        # How long an a token will live before they expire.\n        app.config.setdefault('JWT_ACCESS_TOKEN_EXPIRES', datetime.timedelta(minutes=15))\n        app.config.setdefault('JWT_REFRESH_TOKEN_EXPIRES', datetime.timedelta(days=30))\n\n        # What algorithm to use to sign the token. See here for a list of options:\n        # https://github.com/jpadilla/pyjwt/blob/master/jwt/api_jwt.py\n        app.config.setdefault('JWT_ALGORITHM', 'HS256')\n\n        # Secret key to sign JWTs with. Only used if a symmetric algorithm is\n        # used (such as the HS* algorithms). We will use the app secret key\n        # if this is not set.\n        app.config.setdefault('JWT_SECRET_KEY', None)\n\n        # Keys to sign JWTs with when use when using an asymmetric\n        # (public/private key) algorithm, such as RS* or EC*\n        app.config.setdefault('JWT_PRIVATE_KEY', None)\n        app.config.setdefault('JWT_PUBLIC_KEY', None)\n\n        # Options for blacklisting/revoking tokens\n        app.config.setdefault('JWT_BLACKLIST_ENABLED', False)\n        app.config.setdefault('JWT_BLACKLIST_TOKEN_CHECKS', ('access', 'refresh'))\n\n        app.config.setdefault('JWT_IDENTITY_CLAIM', 'identity')\n        app.config.setdefault('JWT_USER_CLAIMS', 'user_claims')\n        app.config.setdefault('JWT_DECODE_AUDIENCE', None)\n        app.config.setdefault('JWT_DECODE_LEEWAY', 0)\n\n        app.config.setdefault('JWT_CLAIMS_IN_REFRESH_TOKEN', False)\n\n        app.config.setdefault('JWT_ERROR_MESSAGE_KEY', 'msg')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_jwt_in_request():\n    if request.method not in config.exempt_methods:\n        jwt_data = _decode_jwt_from_request(request_type='access')\n        ctx_stack.top.jwt = jwt_data\n        verify_token_claims(jwt_data)\n        _load_user(jwt_data[config.identity_claim_key])", "response": "Verify that the requester has a valid access token."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verify_jwt_in_request_optional():\n    try:\n        if request.method not in config.exempt_methods:\n            jwt_data = _decode_jwt_from_request(request_type='access')\n            ctx_stack.top.jwt = jwt_data\n            verify_token_claims(jwt_data)\n            _load_user(jwt_data[config.identity_claim_key])\n    except (NoAuthorizationError, InvalidHeaderError):\n        pass", "response": "Checks if the request has a valid access token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that the requester has a valid and fresh access token. Raises an exception if there is no valid token or the token is not fresh.", "response": "def verify_fresh_jwt_in_request():\n    \"\"\"\n    Ensure that the requester has a valid and fresh access token. Raises an\n    appropiate exception if there is no token, the token is invalid, or the\n    token is not marked as fresh.\n    \"\"\"\n    if request.method not in config.exempt_methods:\n        jwt_data = _decode_jwt_from_request(request_type='access')\n        ctx_stack.top.jwt = jwt_data\n        fresh = jwt_data['fresh']\n        if isinstance(fresh, bool):\n            if not fresh:\n                raise FreshTokenRequired('Fresh token required')\n        else:\n            now = timegm(datetime.utcnow().utctimetuple())\n            if fresh < now:\n                raise FreshTokenRequired('Fresh token required')\n        verify_token_claims(jwt_data)\n        _load_user(jwt_data[config.identity_claim_key])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef jwt_required(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        verify_jwt_in_request()\n        return fn(*args, **kwargs)\n    return wrapper", "response": "Decorator to protect an endpoint with this decorator that ensures that the requester\n    has a valid access token and that the requester\n    has a valid access token."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a Keras model from file handling custom loss function", "response": "def load_precise_model(model_name: str) -> Any:\n    \"\"\"Loads a Keras model from file, handling custom loss function\"\"\"\n    if not model_name.endswith('.net'):\n        print('Warning: Unknown model type, ', model_name)\n\n    inject_params(model_name)\n    return load_keras().models.load_model(model_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_model(model_name: Optional[str], params: ModelParams) -> 'Sequential':\n    if model_name and isfile(model_name):\n        print('Loading from ' + model_name + '...')\n        model = load_precise_model(model_name)\n    else:\n        from keras.layers.core import Dense\n        from keras.layers.recurrent import GRU\n        from keras.models import Sequential\n\n        model = Sequential()\n        model.add(GRU(\n            params.recurrent_units, activation='linear',\n            input_shape=(pr.n_features, pr.feature_size), dropout=params.dropout, name='net'\n        ))\n        model.add(Dense(1, activation='sigmoid'))\n\n    load_keras()\n    metrics = ['accuracy'] + params.extra_metrics * [false_pos, false_neg]\n    set_loss_bias(params.loss_bias)\n    for i in model.layers[:params.freeze_till]:\n        i.trainable = False\n    model.compile('rmsprop', weighted_log_loss, metrics=(not params.skip_acc) * metrics)\n    return model", "response": "Load or create a precise Keras model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an identical 2d array where the first row is filled with value", "response": "def layer_with(self, sample: np.ndarray, value: int) -> np.ndarray:\n        \"\"\"Create an identical 2d array where the second row is filled with value\"\"\"\n        b = np.full((2, len(sample)), value, dtype=float)\n        b[0] = sample\n        return b"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_wakeword_pieces(self, volume):\n        while True:\n            target = 1 if random() > 0.5 else 0\n            it = self.pos_files_it if target else self.neg_files_it\n            sample_file = next(it)\n            yield self.layer_with(self.normalize_volume_to(load_audio(sample_file), volume), target)\n            yield self.layer_with(np.zeros(int(pr.sample_rate * (0.5 + 2.0 * random()))), 0)", "response": "Generates chunks of audio that represent the wakeword stream"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chunk_audio_pieces(self, pieces, chunk_size):\n        left_over = np.array([])\n        for piece in pieces:\n            if left_over.size == 0:\n                combined = piece\n            else:\n                combined = np.concatenate([left_over, piece], axis=-1)\n            for chunk in chunk_audio(combined.T, chunk_size):\n                yield chunk.T\n            left_over = piece[-(len(piece) % chunk_size):]", "response": "Convert a series of audio pieces into a series of equally sized pieces"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the RMS of the audio", "response": "def calc_volume(self, sample: np.ndarray):\n        \"\"\"Find the RMS of the audio\"\"\"\n        return sqrt(np.mean(np.square(sample)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the maximum continuous length of the given value in the sequence", "response": "def max_run_length(x: np.ndarray, val: int):\n        \"\"\"Finds the maximum continuous length of the given value in the sequence\"\"\"\n        if x.size == 0:\n            return 0\n        else:\n            y = np.array(x[1:] != x[:-1])\n            i = np.append(np.where(y), len(x) - 1)\n            run_lengths = np.diff(np.append(-1, i))\n            run_length_values = x[i]\n            return max([rl for rl, v in zip(run_lengths, run_length_values) if v == val], default=0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vectors_from_fn(self, fn: str):\n        audio = load_audio(fn)\n        audio_volume = self.calc_volume(audio)\n        audio_volume *= 0.4 + 0.5 * random()\n        audio = self.normalize_volume_to(audio, audio_volume)\n\n        self.listener.clear()\n        chunked_bg = chunk_audio(audio, self.args.chunk_size)\n        chunked_ww = self.chunk_audio_pieces(self.generate_wakeword_pieces(audio_volume), self.args.chunk_size)\n\n        for i, (chunk_bg, (chunk_ww, targets)) in enumerate(zip(chunked_bg, chunked_ww)):\n            chunk = self.merge(chunk_bg, chunk_ww, 0.6)\n            self.vals_buffer = np.concatenate((self.vals_buffer[len(targets):], targets))\n            self.audio_buffer = np.concatenate((self.audio_buffer[len(chunk):], chunk))\n            mfccs = self.listener.update_vectors(chunk)\n            percent_overlapping = self.max_run_length(self.vals_buffer, 1) / len(self.vals_buffer)\n\n            if self.vals_buffer[-1] == 0 and percent_overlapping > 0.8:\n                target = 1\n            elif percent_overlapping < 0.5:\n                target = 0\n            else:\n                continue\n\n            if random() > 1.0 - self.args.save_prob:\n                name = splitext(basename(fn))[0]\n                wav_file = join('debug', 'ww' if target == 1 else 'nww', '{} - {}.wav'.format(name, i))\n                save_audio(wav_file, self.audio_buffer)\n            yield mfccs, target", "response": "Generate vectors from a single background audio file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef samples_to_batches(samples: Iterable, batch_size: int):\n        it = iter(samples)\n        while True:\n            with suppress(StopIteration):\n                batch_in, batch_out = [], []\n                for i in range(batch_size):\n                    sample_in, sample_out = next(it)\n                    batch_in.append(sample_in)\n                    batch_out.append(sample_out)\n            if not batch_in:\n                raise StopIteration\n            yield np.array(batch_in), np.array(batch_out)", "response": "Chunk a series of network inputs and outputs into larger batches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate training samples (network inputs and outputs)", "response": "def generate_samples(self):\n        \"\"\"Generate training samples (network inputs and outputs)\"\"\"\n        filenames = glob_all(self.args.random_data_folder, '*.wav')\n        shuffle(filenames)\n        while True:\n            for fn in filenames:\n                for x, y in self.vectors_from_fn(fn):\n                    yield x, y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraining the model on randomly generated batches", "response": "def run(self):\n        \"\"\"Train the model on randomly generated batches\"\"\"\n        _, test_data = self.data.load(train=False, test=True)\n        try:\n            self.model.fit_generator(\n                self.samples_to_batches(self.generate_samples(), self.args.batch_size), steps_per_epoch=self.args.steps_per_epoch,\n                epochs=self.epoch + self.args.epochs, validation_data=test_data,\n                callbacks=self.callbacks, initial_epoch=self.epoch\n            )\n        finally:\n            self.model.save(self.args.model)\n            save_params(self.args.model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_folder(cls, folder: str) -> 'TrainData':\n        return cls(find_wavs(folder), find_wavs(join(folder, 'test')))", "response": "Load a set of data from a structured folder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_tags(cls, tags_file: str, tags_folder: str) -> 'TrainData':\n        if not tags_file:\n            num_ignored_wavs = len(glob(join(tags_folder, '*.wav')))\n            if num_ignored_wavs > 10:\n                print('WARNING: Found {} wavs but no tags file specified!'.format(num_ignored_wavs))\n            return cls(([], []), ([], []))\n        if not isfile(tags_file):\n            raise RuntimeError('Database file does not exist: ' + tags_file)\n\n        train_groups = {}\n        train_group_file = join(tags_file.replace('.txt', '') + '.groups.json')\n        if isfile(train_group_file):\n            try:\n                with open(train_group_file) as f:\n                    train_groups = json.load(f)\n            except ValueError:\n                pass\n\n        tags_files = {\n            'wake-word': [],\n            'not-wake-word': []\n        }\n        with open(tags_file) as f:\n            for line in f.read().split('\\n'):\n                if not line:\n                    continue\n                file, tag = line.split('\\t')\n                tags_files[tag.strip()].append(join(tags_folder, file.strip() + '.wav'))\n\n        train_files, test_files = ([], []), ([], [])\n        for label, rows in enumerate([tags_files['wake-word'], tags_files['not-wake-word']]):\n            for fn in rows:\n                if not isfile(fn):\n                    print('Missing file:', fn)\n                    continue\n                if fn not in train_groups:\n                    train_groups[fn] = (\n                        'test' if md5(fn.encode('utf8')).hexdigest() > 'c' * 32\n                        else 'train'\n                    )\n                {\n                    'train': train_files,\n                    'test': test_files\n                }[train_groups[fn]][label].append(fn)\n\n        with open(train_group_file, 'w') as f:\n            json.dump(train_groups, f)\n\n        return cls(train_files, test_files)", "response": "Load a set of train data from a text file with tags in the tags folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads data from both a database and a structured folder", "response": "def from_both(cls, tags_file: str, tags_folder: str, folder: str) -> 'TrainData':\n        \"\"\"Load data from both a database and a structured folder\"\"\"\n        return cls.from_tags(tags_file, tags_folder) + cls.from_folder(folder)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the vectorized representations of the stored data files.", "response": "def load(self, train=True, test=True, shuffle=True) -> tuple:\n        \"\"\"\n        Load the vectorized representations of the stored data files\n        Args:\n            train: Whether to load train data\n            test: Whether to load test data\n        \"\"\"\n        return self.__load(self.__load_files, train, test, shuffle=shuffle)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_inhibit(self, train=True, test=True) -> tuple:\n\n        def loader(kws: list, nkws: list):\n            from precise.params import pr\n            inputs = np.empty((0, pr.n_features, pr.feature_size))\n            outputs = np.zeros((len(kws), 1))\n            for f in kws:\n                if not isfile(f):\n                    continue\n                new_vec = load_vector(f, vectorize_inhibit)\n                inputs = np.concatenate([inputs, new_vec])\n\n            return self.merge((inputs, outputs), self.__load_files(kws, nkws))\n\n        return self.__load(loader, train, test)", "response": "Generate data with inhibitory inputs created from wake word samples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_args(parser: ArgumentParser) -> Any:\n        extra_usage = '''\n            :folder str\n                Folder to load wav files from\n            \n            :-tf --tags-folder str {folder}\n                Specify a different folder to load file ids\n                in tags file from\n            \n            :-tg --tags-file str -\n                Text file to load tags from where each line is\n                <file_id> TAB (wake-word|not-wake-word) and\n                {folder}/<file_id>.wav exists\n            \n        '''\n        add_to_parser(parser, extra_usage)\n        args = parser.parse_args()\n        args.tags_folder = args.tags_folder.format(folder=args.folder)\n        return args", "response": "Return parsed args from parser adding options for train data inputs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sampled_data(self):\n        if self.args.samples_file:\n            if self.args.invert_samples:\n                chosen_samples = set(self.hash_to_ind) - self.samples\n            else:\n                chosen_samples = self.samples\n            selected_indices = [self.hash_to_ind[h] for h in chosen_samples]\n            return self.train[0][selected_indices], self.train[1][selected_indices]\n        else:\n            return self.train[0], self.train[1]", "response": "Returns ( train_inputs train_outputs )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn audio into feature vectors without clipping for length", "response": "def vectorize_raw(audio: np.ndarray) -> np.ndarray:\n    \"\"\"Turns audio into feature vectors, without clipping for length\"\"\"\n    if len(audio) == 0:\n        raise InvalidAudio('Cannot vectorize empty audio!')\n    return vectorizers[pr.vectorizer](audio)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vectorize(audio: np.ndarray) -> np.ndarray:\n    if len(audio) > pr.max_samples:\n        audio = audio[-pr.max_samples:]\n    features = vectorize_raw(audio)\n    if len(features) < pr.n_features:\n        features = np.concatenate([\n            np.zeros((pr.n_features - len(features), features.shape[1])),\n            features\n        ])\n    if len(features) > pr.n_features:\n        features = features[-pr.n_features:]\n\n    return features", "response": "Vectorizes audio into n_features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vectorize_inhibit(audio: np.ndarray) -> np.ndarray:\n\n    def samp(x):\n        return int(pr.sample_rate * x)\n\n    inputs = []\n    for offset in range(samp(inhibit_t), samp(inhibit_dist_t), samp(inhibit_hop_t)):\n        if len(audio) - offset < samp(pr.buffer_t / 2.):\n            break\n        inputs.append(vectorize(audio[:-offset]))\n    return np.array(inputs) if inputs else np.empty((0, pr.n_features, pr.feature_size))", "response": "Vectorizes the audio that shouldn t cause an activation\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether the new prediction caused an activation", "response": "def update(self, prob):\n        # type: (float) -> bool\n        \"\"\"Returns whether the new prediction caused an activation\"\"\"\n        chunk_activated = prob > 1.0 - self.sensitivity\n\n        if chunk_activated or self.activation < 0:\n            self.activation += 1\n            has_activated = self.activation > self.trigger_level\n            if has_activated or chunk_activated and self.activation < 0:\n                self.activation = -(8 * 2048) // self.chunk_size\n\n            if has_activated:\n                return True\n        elif self.activation > 0:\n            self.activation -= 1\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps the read method in a stream so that it can be read by the user.", "response": "def _wrap_stream_read(self, stream):\n        \"\"\"\n        pyaudio.Stream.read takes samples as n, not bytes\n        so read(n) should be read(n // sample_depth)\n        \"\"\"\n        import pyaudio\n        if getattr(stream.read, '__func__', None) is pyaudio.Stream.read:\n            stream.read = lambda x: pyaudio.Stream.read(stream, x // 2, False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self):\n        if self.stream is None:\n            from pyaudio import PyAudio, paInt16\n            self.pa = PyAudio()\n            self.stream = self.pa.open(\n                16000, 1, paInt16, True, frames_per_buffer=self.chunk_size\n            )\n\n        self._wrap_stream_read(self.stream)\n\n        self.engine.start()\n        self.running = True\n        self.is_paused = False\n        self.thread = Thread(target=self._handle_predictions)\n        self.thread.daemon = True\n        self.thread.start()", "response": "Start listening from stream"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop(self):\n        if self.thread:\n            self.running = False\n            if isinstance(self.stream, ReadWriteStream):\n                self.stream.write(b'\\0' * self.chunk_size)\n            self.thread.join()\n            self.thread = None\n\n        self.engine.stop()\n\n        if self.pa:\n            self.pa.terminate()\n            self.stream.stop_stream()\n            self.stream = self.pa = None", "response": "Stop listening and close stream"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_predictions(self):\n        while self.running:\n            chunk = self.stream.read(self.chunk_size)\n\n            if self.is_paused:\n                continue\n\n            prob = self.engine.get_prediction(chunk)\n            self.on_prediction(prob)\n            if self.detector.update(prob):\n                self.on_activation()", "response": "Continuously check Precise process output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_filenames(self, is_correct: bool, actual_output: bool, threshold=0.5) -> list:\n        return [\n            filename\n            for output, target, filename in zip(self.outputs, self.targets, self.filenames)\n            if ((output > threshold) == bool(target)) == is_correct and actual_output == bool(output > threshold)\n        ]", "response": "Find a list of files with the given classification"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_metric(self, is_correct: bool, actual_output: bool, threshold=0.5) -> int:\n        return int(\n            ((((self.outputs > threshold) == self.targets.astype(bool)) == is_correct) &\n             ((self.outputs > threshold) == actual_output)).sum()\n        )", "response": "Calculates the metric of the current state of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a sample with the given network output target output threshold is the classification of the sample.", "response": "def matches_sample(output, target, threshold, is_correct, actual_output):\n        \"\"\"\n        Check if a sample with the given network output, target output, and threshold\n        is the classification (is_correct, actual_output) like true positive or false negative\n        \"\"\"\n        return (bool(output > threshold) == bool(target)) == is_correct and actual_output == bool(output > threshold)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_thresholds(points=100, power=3) -> list:\n    return [(i / (points + 1)) ** power for i in range(1, points + 1)]", "response": "Run a function with a series of thresholds between 0 and 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_for(self, model: str) -> Tuple[list, list]:\n        inject_params(model)\n        if self.prev_cache != pr.vectorization_md5_hash():\n            self.prev_cache = pr.vectorization_md5_hash()\n            self.data = self.loader()\n        return self.data", "response": "Loads the data for the given model and returns the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef buffer_to_audio(buffer: bytes) -> np.ndarray:\n    return np.fromstring(buffer, dtype='<i2').astype(np.float32, order='C') / 32768.0", "response": "Convert a raw mono audio byte string to numpy array of floats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading an audio file into a new object.", "response": "def load_audio(file: Any) -> np.ndarray:\n    \"\"\"\n    Args:\n        file: Audio filename or file object\n    Returns:\n        samples: Sample rate and audio samples from 0..1\n    \"\"\"\n    import wavio\n    try:\n        wav = wavio.read(file)\n    except EOFError:\n        wav = wavio.Wav(np.array([[]], dtype=np.int16), 16000, 2)\n    if wav.data.dtype != np.int16:\n        raise InvalidAudio('Unsupported data type: ' + str(wav.data.dtype))\n    if wav.rate != pr.sample_rate:\n        raise InvalidAudio('Unsupported sample rate: ' + str(wav.rate))\n\n    data = np.squeeze(wav.data)\n    return data.astype(np.float32) / float(np.iinfo(data.dtype).max)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef play_audio(filename: str):\n    import platform\n    from subprocess import Popen\n\n    player = 'play' if platform.system() == 'Darwin' else 'aplay'\n    Popen([player, '-q', filename])", "response": "Play audio file in\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind wake - word and not - wake - word wavs in folder", "response": "def find_wavs(folder: str) -> Tuple[List[str], List[str]]:\n    \"\"\"Finds wake-word and not-wake-word wavs in folder\"\"\"\n    return (glob_all(join(folder, 'wake-word'), '*.wav'),\n            glob_all(join(folder, 'not-wake-word'), '*.wav'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert(model_path: str, out_file: str):\n    print('Converting', model_path, 'to', out_file, '...')\n\n    import tensorflow as tf\n    from precise.model import load_precise_model\n    from keras import backend as K\n\n    out_dir, filename = split(out_file)\n    out_dir = out_dir or '.'\n    os.makedirs(out_dir, exist_ok=True)\n\n    K.set_learning_phase(0)\n    model = load_precise_model(model_path)\n\n    out_name = 'net_output'\n    tf.identity(model.output, name=out_name)\n    print('Output node name:', out_name)\n    print('Output folder:', out_dir)\n\n    sess = K.get_session()\n\n    # Write the graph in human readable\n    tf.train.write_graph(sess.graph.as_graph_def(), out_dir, filename + 'txt', as_text=True)\n    print('Saved readable graph to:', filename + 'txt')\n\n    # Write the graph in binary .pb file\n    from tensorflow.python.framework import graph_util\n    from tensorflow.python.framework import graph_io\n\n    cgraph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [out_name])\n    graph_io.write_graph(cgraph, out_dir, filename, as_text=False)\n\n    if isfile(model_path + '.params'):\n        copyfile(model_path + '.params', out_file + '.params')\n\n    print('Saved graph to:', filename)\n\n    del sess", "response": "Converts an HD5F file from Keras to a protobuf file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef weighted_log_loss(yt, yp) -> Any:\n    from keras import backend as K\n\n    pos_loss = -(0 + yt) * K.log(0 + yp + K.epsilon())\n    neg_loss = -(1 - yt) * K.log(1 - yp + K.epsilon())\n\n    return LOSS_BIAS * K.mean(neg_loss) + (1. - LOSS_BIAS) * K.mean(pos_loss)", "response": "Binary crossentropy with a bias towards false negatives\n    yt yp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pdf(x, mu, std):\n    return (1.0 / (std * sqrt(2 * pi))) * np.exp(-(x - mu) ** 2 / (2 * std ** 2))", "response": "Probability density function (normal distribution)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the global listener params to a saved model", "response": "def inject_params(model_name: str) -> ListenerParams:\n    \"\"\"Set the global listener params to a saved model\"\"\"\n    params_file = model_name + '.params'\n    try:\n        with open(params_file) as f:\n            pr.__dict__.update(compatibility_params, **json.load(f))\n    except (OSError, ValueError, TypeError):\n        if isfile(model_name):\n            print('Warning: Failed to load parameters from ' + params_file)\n    return pr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving current global listener params to a file", "response": "def save_params(model_name: str):\n    \"\"\"Save current global listener params to a file\"\"\"\n    with open(model_name + '.params', 'w') as f:\n        json.dump(pr.__dict__, f)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retrain(self):\n        folder = TrainData.from_folder(self.args.folder)\n        train_data, test_data = folder.load(True, not self.args.no_validation)\n\n        train_data = TrainData.merge(train_data, self.sampled_data)\n        test_data = TrainData.merge(test_data, self.test)\n        train_inputs, train_outputs = train_data\n        print()\n        try:\n            self.listener.runner.model.fit(\n                train_inputs, train_outputs, self.args.batch_size, self.epoch + self.args.epochs,\n                validation_data=test_data, callbacks=self.callbacks, initial_epoch=self.epoch\n            )\n        finally:\n            self.listener.runner.model.save(self.args.model)", "response": "Train for a session pulling in any new data from the filesystem and saving the model to disk"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns through a single audio file and train on it", "response": "def train_on_audio(self, fn: str):\n        \"\"\"Run through a single audio file\"\"\"\n        save_test = random() > 0.8\n        audio = load_audio(fn)\n        num_chunks = len(audio) // self.args.chunk_size\n\n        self.listener.clear()\n\n        for i, chunk in enumerate(chunk_audio(audio, self.args.chunk_size)):\n            print('\\r' + str(i * 100. / num_chunks) + '%', end='', flush=True)\n            self.audio_buffer = np.concatenate((self.audio_buffer[len(chunk):], chunk))\n            conf = self.listener.update(chunk)\n            if conf > self.args.threshold:\n                self.samples_since_train += 1\n                name = splitext(basename(fn))[0] + '-' + str(i) + '.wav'\n                name = join(self.args.folder, 'test' if save_test else '', 'not-wake-word',\n                            'generated', name)\n                save_audio(name, self.audio_buffer)\n                print()\n                print('Saved to:', name)\n\n            if not save_test and self.samples_since_train >= self.args.delay_samples and \\\n                    self.args.epochs > 0:\n                self.samples_since_train = 0\n                self.retrain()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        for fn in glob_all(self.args.random_data_folder, '*.wav'):\n            if fn in self.trained_fns:\n                print('Skipping ' + fn + '...')\n                continue\n\n            print('Starting file ' + fn + '...')\n            self.train_on_audio(fn)\n            print('\\r100%                 ')\n\n            self.trained_fns.append(fn)\n            save_trained_fns(self.trained_fns, self.args.model)", "response": "Runs training on audio files and saves activations and retraining when necessary"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict(self, inputs: np.ndarray) -> np.ndarray:\n        return self.sess.run(self.out_var, {self.inp_var: inputs})", "response": "Run on multiple inputs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef currency_to_satoshi(amount, currency):\n    satoshis = EXCHANGE_RATES[currency]()\n    return int(satoshis * Decimal(amount))", "response": "Converts a given amount of currency to the equivalent number of\n    satoshi."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a given number of satoshi to another currency as a formatted string rounded down to the proper number of decimal places.", "response": "def satoshi_to_currency(num, currency):\n    \"\"\"Converts a given number of satoshi to another currency as a formatted\n    string rounded down to the proper number of decimal places.\n\n    :param num: The number of satoshi.\n    :type num: ``int``\n    :param currency: One of the :ref:`supported currencies`.\n    :type currency: ``str``\n    :rtype: ``str``\n    \"\"\"\n    return '{:f}'.format(\n        Decimal(\n            num / Decimal(EXCHANGE_RATES[currency]())\n        ).quantize(\n            Decimal('0.' + '0' * CURRENCY_PRECISION[currency]),\n            rounding=ROUND_DOWN\n        ).normalize()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef satoshi_to_currency_cached(num, currency):\n    return '{:f}'.format(\n        Decimal(\n            num / Decimal(currency_to_satoshi_cached(1, currency))\n        ).quantize(\n            Decimal('0.' + '0' * CURRENCY_PRECISION[currency]),\n            rounding=ROUND_DOWN\n        ).normalize()\n    )", "response": "Converts a given number of satoshi to another currency as a formatted string rounded down to the proper number of decimal places."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the balance of an address in satoshi.", "response": "def get_balance(cls, address):\n        \"\"\"Gets the balance of an address in satoshi.\n\n        :param address: The address in question.\n        :type address: ``str``\n        :raises ConnectionError: If all API services fail.\n        :rtype: ``int``\n        \"\"\"\n\n        for api_call in cls.GET_BALANCE_MAIN:\n            try:\n                return api_call(address)\n            except cls.IGNORED_ERRORS:\n                pass\n\n        raise ConnectionError('All APIs are unreachable.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_transactions(cls, address):\n\n        for api_call in cls.GET_TRANSACTIONS_MAIN:\n            try:\n                return api_call(address)\n            except cls.IGNORED_ERRORS:\n                pass\n\n        raise ConnectionError('All APIs are unreachable.')", "response": "Get the ID of all transactions related to an address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_transaction_by_id(cls, txid):\n\n        for api_call in cls.GET_TRANSACTION_BY_ID_MAIN:\n            try:\n                return api_call(txid)\n            except cls.IGNORED_ERRORS:\n                pass\n\n        raise ConnectionError('All APIs are unreachable.')", "response": "Get a raw transaction hex by its transaction id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_unspent(cls, address):\n\n        for api_call in cls.GET_UNSPENT_MAIN:\n            try:\n                return api_call(address)\n            except cls.IGNORED_ERRORS:\n                pass\n\n        raise ConnectionError('All APIs are unreachable.')", "response": "Returns all unspent transaction outputs belonging to an address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbroadcasting a signed transaction to the blockchain.", "response": "def broadcast_tx(cls, tx_hex):  # pragma: no cover\n        \"\"\"Broadcasts a transaction to the blockchain.\n\n        :param tx_hex: A signed transaction in hex form.\n        :type tx_hex: ``str``\n        :raises ConnectionError: If all API services fail.\n        \"\"\"\n        success = None\n\n        for api_call in cls.BROADCAST_TX_MAIN:\n            try:\n                success = api_call(tx_hex)\n                if not success:\n                    continue\n                return\n            except cls.IGNORED_ERRORS:\n                pass\n\n        if success is False:\n            raise ConnectionError('Transaction broadcast failed, or '\n                                  'Unspents were already used.')\n\n        raise ConnectionError('All APIs are unreachable.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sanitize_tx_data(unspents, outputs, fee, leftover, combine=True,\n                     message=None, compressed=True, absolute_fee=False,\n                     min_change=0, version='main'):\n    \"\"\"\n    sanitize_tx_data()\n\n    fee is in satoshis per byte.\n    \"\"\"\n\n    outputs = outputs.copy()\n\n    for i, output in enumerate(outputs):\n        dest, amount, currency = output\n        outputs[i] = (dest, currency_to_satoshi_cached(amount, currency))\n\n    if not unspents:\n        raise ValueError('Transactions must have at least one unspent.')\n\n    # Temporary storage so all outputs precede messages.\n    messages = []\n\n    if message:\n        message_chunks = chunk_data(message.encode('utf-8'), MESSAGE_LIMIT)\n\n        for message in message_chunks:\n            messages.append((message, 0))\n\n    # Include return address in output count.\n    # Calculate output size as a list (including return address).\n    output_size = [len(address_to_scriptpubkey(o[0])) + 9 for o in outputs]\n    output_size.append(len(messages) * (MESSAGE_LIMIT + 9))\n    output_size.append(len(address_to_scriptpubkey(leftover)) + 9)\n    sum_outputs = sum(out[1] for out in outputs)\n\n    # Use Branch-and-Bound for coin selection:\n    unspents[:], remaining = select_coins(\n        sum_outputs, fee, output_size, min_change=min_change,\n        absolute_fee=absolute_fee, consolidate=combine, unspents=unspents\n    )\n\n    if remaining > 0:\n        outputs.append((leftover, remaining))\n\n    # Sanity check: If spending from main-/testnet, then all output addresses must also be for main-/testnet.\n    for output in outputs:\n        dest, amount = output\n        vs = get_version(dest)\n        if vs and vs != version:\n            raise ValueError('Cannot send to ' + vs + 'net address when '\n                             'spending from a ' + version + 'net address.')\n\n    outputs.extend(messages)\n\n    return unspents, outputs", "response": "Sanitize the data for a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate preimages for the input sequence and input sequence of the input sequence of the input sequence of the input sequence of the input sequence of the input sequence.", "response": "def calculate_preimages(tx_obj, inputs_parameters):\n    \"\"\"Calculates preimages for provided transaction structure and input\n    values.\n\n    :param tx_obj: The transaction object used to calculate preimage from using\n                   a transaction digest algorithm, such as BIP-143 for Segwit\n                   inputs. This transaction object must hence have scriptCodes\n                   filled into the corresponding scriptSigs in the inputs.\n    :type tx_obj: :object:`~bit.transaction.TxObj`\n    :param inputs_parameters: A list of tuples with input index as integer,\n                              hash type as integer and a boolean flag to denote\n                              if the input is spending from a Segwit output.\n                              For example: [(0, 1, True), (2, 1, False), (...)]\n    :type inputs_parameters: A `list` of `tuple`\n    \"\"\"\n\n    # Tx object data:\n    input_count = int_to_varint(len(tx_obj.TxIn))\n    output_count = int_to_varint(len(tx_obj.TxOut))\n    output_block = b''.join([bytes(o) for o in tx_obj.TxOut])\n\n    hashPrevouts = double_sha256(b''.join([i.txid+i.txindex for i in tx_obj.TxIn]))\n    hashSequence = double_sha256(b''.join([i.sequence for i in tx_obj.TxIn]))\n    hashOutputs = double_sha256(output_block)\n\n    preimages = []\n    for input_index, hash_type, segwit_input in inputs_parameters:\n        # We can only handle hashType == 1:\n        if hash_type != HASH_TYPE:\n            raise ValueError('Bit only support hashType of value 1.')\n        # Calculate prehashes:\n        if segwit_input:\n            # BIP-143 preimage:\n            hashed = sha256(\n                tx_obj.version +\n                hashPrevouts +\n                hashSequence +\n                tx_obj.TxIn[input_index].txid +\n                tx_obj.TxIn[input_index].txindex +\n                tx_obj.TxIn[input_index].script_sig_len +  # scriptCode length\n                tx_obj.TxIn[input_index].script_sig +  # scriptCode (includes amount)\n                tx_obj.TxIn[input_index].sequence +\n                hashOutputs +\n                tx_obj.locktime +\n                hash_type\n            )\n        else:\n            hashed = sha256(\n                tx_obj.version +\n                input_count +\n                b''.join(ti.txid + ti.txindex + OP_0 + ti.sequence\n                         for ti in islice(tx_obj.TxIn, input_index)) +\n                tx_obj.TxIn[input_index].txid +\n                tx_obj.TxIn[input_index].txindex +\n                tx_obj.TxIn[input_index].script_sig_len +  # scriptCode length\n                tx_obj.TxIn[input_index].script_sig +  # scriptCode\n                tx_obj.TxIn[input_index].sequence +\n                b''.join(ti.txid + ti.txindex + OP_0 + ti.sequence\n                         for ti in islice(tx_obj.TxIn, input_index + 1, None)) +\n                output_count +\n                output_block +\n                tx_obj.locktime +\n                hash_type\n            )\n        preimages.append(hashed)\n    return preimages"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sign_tx(private_key, tx, *, unspents):\n\n    # input_dict contains those unspents that can be signed by private_key,\n    # providing additional information for segwit-inputs (the amount to spend)\n    input_dict = {}\n    try:\n        for unspent in unspents:\n            if not private_key.can_sign_unspent(unspent):\n                continue\n            tx_input = hex_to_bytes(unspent.txid)[::-1] + \\\n                unspent.txindex.to_bytes(4, byteorder='little')\n            input_dict[tx_input] = unspent.to_dict()\n    except TypeError:\n        raise TypeError('Please provide as unspents at least all inputs to '\n                        'be signed with the function call in a list.')\n\n    # Determine input indices to sign from input_dict (allows for transaction batching)\n    sign_inputs = [j for j, i in enumerate(tx.TxIn) if i.txid+i.txindex in input_dict]\n\n    segwit_tx = TxObj.is_segwit(tx)\n    public_key = private_key.public_key\n    public_key_push = script_push(len(public_key))\n    hash_type = HASH_TYPE\n\n    # Make input parameters for preimage calculation\n    inputs_parameters = []\n    for i in sign_inputs:\n        # Create transaction object for preimage calculation\n        tx_input = tx.TxIn[i].txid + tx.TxIn[i].txindex\n        segwit_input = input_dict[tx_input]['segwit']\n        tx.TxIn[i].segwit_input = segwit_input\n        # For partially signed transaction we must extract the signatures:\n        input_script_field = tx.TxIn[i].script_sig\n\n        script_code = private_key.scriptcode\n        script_code_len = int_to_varint(len(script_code))\n\n        # Use scriptCode for preimage calculation of transaction object:\n        tx.TxIn[i].script_sig = script_code\n        tx.TxIn[i].script_sig_len = script_code_len\n\n        if segwit_input:\n            try:\n                tx.TxIn[i].script_sig += input_dict[tx_input]['amount']\\\n                                         .to_bytes(8, byteorder='little')\n                # For partially signed transaction we must extract the\n                # signatures:\n                input_script_field = tx.TxIn[i].witness\n            except AttributeError:\n                raise ValueError(\n                    'Cannot sign a segwit input when the input\\'s amount is '\n                    'unknown. Maybe no network connection or the input is '\n                    'already spent? Then please provide all inputs to sign as '\n                    '`Unspent` objects to the function call.')\n\n        inputs_parameters.append([i, hash_type, segwit_input])\n    preimages = calculate_preimages(tx, inputs_parameters)\n\n    # Calculate signature scripts:\n    for hash, (i, _, segwit_input) in zip(preimages, inputs_parameters):\n        signature = private_key.sign(hash) + b'\\x01'\n\n        # ------------------------------------------------------------------\n        if (private_key.instance == 'MultiSig' or\n                private_key.instance == 'MultiSigTestnet'):\n            # P2(W)SH input\n\n            script_blob = b''\n            sigs = {}\n            # Initial number of witness items (OP_0 + one signature + redeemscript).\n            witness_count = 3\n            if input_script_field:\n                sig_list = get_signatures_from_script(input_script_field)\n                # Bitcoin Core convention: Every missing signature is denoted\n                # by 0x00. Only used for already partially-signed scriptSigs:\n                script_blob += b'\\x00' * (private_key.m - len(sig_list)-1)\n                # Total number of witness items when partially or fully signed:\n                witness_count = private_key.m + 2\n                # For a partially signed input make a dictionary containing\n                # all the provided signatures with public-keys as keys:\n                for sig in sig_list:\n                    for pub in private_key.public_keys:\n                        if verify_sig(sig[:-1], hash, pub):\n                            # If we already found a valid signature for pubkey\n                            # we just overwrite it and don't care.\n                            sigs[pub] = sig\n                if len(sigs) >= private_key.m:\n                    raise ValueError('Transaction is already signed with '\n                                     'sufficiently needed signatures.')\n\n            sigs[public_key] = signature\n\n            witness = b''\n            # Sort ingthe signatures according to the public-key list:\n            for pub in private_key.public_keys:\n                if pub in sigs:\n                    sig = sigs[pub]\n                    length = int_to_varint(len(sig)) if segwit_input else \\\n                        script_push(len(sig))\n                    witness += length + sig\n\n            script_sig = b'\\x22' + private_key.segwit_scriptcode\n\n            witness = (int_to_varint(witness_count) if segwit_input else b'') \\\n                + b'\\x00' + witness + script_blob\n            witness += (int_to_varint(len(private_key.redeemscript)) if\n                segwit_input else script_push(len(private_key.redeemscript))) \\\n                + private_key.redeemscript\n\n            script_sig = script_sig if segwit_input else witness\n            witness = witness if segwit_input else b'\\x00' if segwit_tx else b''\n\n        # ------------------------------------------------------------------\n        else:\n            # P2(W)PKH input\n\n            script_sig = b'\\x16' + private_key.segwit_scriptcode\n\n            witness = (\n                      (b'\\x02' if segwit_input else b'') +  # witness counter\n                      len(signature).to_bytes(1, byteorder='little') +\n                      signature +\n                      public_key_push +\n                      public_key\n                     )\n\n            script_sig = script_sig if segwit_input else witness\n            witness = witness if segwit_input else b'\\x00' if segwit_tx else b''\n\n        # Providing the signature(s) to the input\n        tx.TxIn[i].script_sig = script_sig\n        tx.TxIn[i].script_sig_len = int_to_varint(len(script_sig))\n        tx.TxIn[i].witness = witness\n\n    return tx.to_hex()", "response": "Signs inputs in provided transaction object for which unspents are provided and can be signed by the private key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify(self, signature, data):\n        return self._pk.public_key.verify(signature, data)", "response": "Verifies some data was signed by this private key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef segwit_address(self):\n        # Only make segwit address if public key is compressed\n        if self._segwit_address is None and self.is_compressed():\n            self._segwit_address = public_key_to_segwit_address(\n                self._public_key, version=self.version)\n        return self._segwit_address", "response": "The public segwit address you share with others to\n        receive funds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch all available unspent transaction outputs.", "response": "def get_unspents(self):\n        \"\"\"Fetches all available unspent transaction outputs.\n\n        :rtype: ``list`` of :class:`~bit.network.meta.Unspent`\n        \"\"\"\n        self.unspents[:] = list(map(\n            lambda u: u.set_type('p2pkh' if self.is_compressed() else\n                                 'p2pkh-uncompressed'),\n            NetworkAPI.get_unspent(self.address)\n        ))\n        if self.segwit_address:\n            self.unspents += list(map(\n                lambda u: u.set_type('np2wkh'),\n                NetworkAPI.get_unspent(self.segwit_address)\n            ))\n        self.balance = sum(unspent.amount for unspent in self.unspents)\n        return self.unspents"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching transaction history. :rtype: ``list`` of ``str`` transaction IDs", "response": "def get_transactions(self):\n        \"\"\"Fetches transaction history.\n\n        :rtype: ``list`` of ``str`` transaction IDs\n        \"\"\"\n        self.transactions[:] = NetworkAPI.get_transactions(self.address)\n        if self.segwit_address:\n            self.transactions += NetworkAPI.get_transactions(self.segwit_address)\n        return self.transactions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_transaction(self, outputs, fee=None, absolute_fee=False,\n                           leftover=None, combine=True, message=None,\n                           unspents=None):  # pragma: no cover\n        \"\"\"Creates a signed P2PKH transaction.\n\n        :param outputs: A sequence of outputs you wish to send in the form\n                        ``(destination, amount, currency)``. The amount can\n                        be either an int, float, or string as long as it is\n                        a valid input to ``decimal.Decimal``. The currency\n                        must be :ref:`supported <supported currencies>`.\n        :type outputs: ``list`` of ``tuple``\n        :param fee: The number of satoshi per byte to pay to miners. By default\n                    Bit will poll `<https://bitcoinfees.earn.com>`_ and use a fee\n                    that will allow your transaction to be confirmed as soon as\n                    possible.\n        :type fee: ``int``\n        :param leftover: The destination that will receive any change from the\n                         transaction. By default Bit will send any change to\n                         the same address you sent from.\n        :type leftover: ``str``\n        :param combine: Whether or not Bit should use all available UTXOs to\n                        make future transactions smaller and therefore reduce\n                        fees. By default Bit will consolidate UTXOs.\n        :type combine: ``bool``\n        :param message: A message to include in the transaction. This will be\n                        stored in the blockchain forever. Due to size limits,\n                        each message will be stored in chunks of 40 bytes.\n        :type message: ``str``\n        :param unspents: The UTXOs to use as the inputs. By default Bit will\n                         communicate with the blockchain itself.\n        :type unspents: ``list`` of :class:`~bit.network.meta.Unspent`\n        :returns: The signed transaction as hex.\n        :rtype: ``str``\n        \"\"\"\n        try:\n            unspents = unspents or self.get_unspents()\n        except ConnectionError:\n            raise ConnectionError('All APIs are unreachable. Please provide '\n                                  'the unspents to spend from directly.')\n\n        # If at least one input is from segwit the return address is for segwit\n        return_address = self.segwit_address if any(\n            [u.segwit for u in unspents]) else self.address\n\n        unspents, outputs = sanitize_tx_data(\n            unspents,\n            outputs,\n            fee or get_fee_cached(),\n            leftover or return_address,\n            combine=combine,\n            message=message,\n            absolute_fee=absolute_fee,\n            version=self.version\n        )\n\n        return create_new_transaction(self, unspents, outputs)", "response": "Creates a signed P2PKH transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sign_transaction(self, tx_data, unspents=None):  # pragma: no cover\n        try:  # Json-tx-data from :func:`~bit.Key.prepare_transaction`\n            data = json.loads(tx_data)\n            assert(unspents is None)\n\n            unspents = [Unspent.from_dict(unspent) for unspent in data['unspents']]\n            outputs = data['outputs']\n\n            return create_new_transaction(self, unspents, outputs)\n        except:  # May be hex-encoded transaction using batching:\n            try:\n                unspents = unspents or self.get_unspents()\n            except ConnectionError:\n                raise ConnectionError(\n                    'All APIs are unreachable. Please provide the unspent '\n                    'inputs as unspents directly to sign this transaction.')\n\n            tx_data = deserialize(tx_data)\n            return sign_tx(self, tx_data, unspents=unspents)", "response": "Signs a transaction using previously prepared version of the key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_signatures_from_script(script):\n\n    script = script[1:]  # remove the first OP_0\n    sigs = []\n    while len(script) > 0:\n        # Consume script while extracting possible signatures:\n        val, script = read_var_int(script)\n        potential_sig, script = read_bytes(script, val)\n        try:\n            # Raises ValueError if argument to `der_to_cdata` is not a\n            # DER-encoding of a signature (without the appended SIGHASH).\n            der_to_cdata(potential_sig[:-1])\n            # We only add if DER-encoded signature:\n            sigs.append(potential_sig)\n        except ValueError:\n            pass\n    return sigs", "response": "Returns a list of signatures retrieved from the provided scriptSig."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bech32_polymod(values):\n    generator = [0x3b6a57b2, 0x26508e6d, 0x1ea119fa, 0x3d4233dd, 0x2a1462b3]\n    chk = 1\n    for value in values:\n        top = chk >> 25\n        chk = (chk & 0x1ffffff) << 5 ^ value\n        for i in range(5):\n            chk ^= generator[i] if ((top >> i) & 1) else 0\n    return chk", "response": "Internal function that computes the Bech32 checksum."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands the HRP into values for checksum computation.", "response": "def bech32_hrp_expand(hrp):\n    \"\"\"Expand the HRP into values for checksum computation.\"\"\"\n    return [ord(x) >> 5 for x in hrp] + [0] + [ord(x) & 31 for x in hrp]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bech32_create_checksum(hrp, data):\n    values = bech32_hrp_expand(hrp) + data\n    polymod = bech32_polymod(values + [0, 0, 0, 0, 0, 0]) ^ 1\n    return [(polymod >> 5 * (5 - i)) & 31 for i in range(6)]", "response": "Compute the checksum values given HRP and data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bech32_encode(hrp, data):\n    combined = data + bech32_create_checksum(hrp, data)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])", "response": "Compute a Bech32 string given HRP and data values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convertbits(data, frombits, tobits, pad=True):\n    acc = 0\n    bits = 0\n    ret = []\n    maxv = (1 << tobits) - 1\n    max_acc = (1 << (frombits + tobits - 1)) - 1\n    for value in data:\n        if value < 0 or (value >> frombits):\n            return None\n        acc = ((acc << frombits) | value) & max_acc\n        bits += frombits\n        while bits >= tobits:\n            bits -= tobits\n            ret.append((acc >> bits) & maxv)\n    if pad:\n        if bits:\n            ret.append((acc << (tobits - bits)) & maxv)\n    elif bits >= frombits or ((acc << (tobits - bits)) & maxv):\n        return None\n    return ret", "response": "General power - of - 2 base conversion."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(addr):\n    hrpgot, data = bech32_decode(addr)\n    # if hrpgot != hrp:\n    if hrpgot not in BECH32_VERSION_SET:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    return (data[0], decoded)", "response": "Decode a segwit address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode(hrp, witver, witprog):\n    ret = bech32_encode(hrp, [witver] + convertbits(witprog, 8, 5))\n    if decode(ret) == (None, None):\n        return None\n    return ret", "response": "Encode a segwit address."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.n_features = len(est.sigma_[0])\n        self.n_classes = len(est.classes_)\n\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Create class prior probabilities:\n        priors = [temp_type.format(self.repr(c)) for c in est.class_prior_]\n        priors = ', '.join(priors)\n        self.priors = temp_arr_.format(type='double', name='priors',\n                                       values=priors)\n\n        # Create sigmas:\n        sigmas = []\n        for sigma in est.sigma_:\n            tmp = [temp_type.format(self.repr(s)) for s in sigma]\n            tmp = temp_arr.format(', '.join(tmp))\n            sigmas.append(tmp)\n        sigmas = ', '.join(sigmas)\n        self.sigmas = temp_arr__.format(type='double', name='sigmas',\n                                        values=sigmas)\n\n        # Create thetas:\n        thetas = []\n        for theta in est.theta_:\n            tmp = [temp_type.format(self.repr(t)) for t in theta]\n            tmp = temp_arr.format(', '.join(tmp))\n            thetas.append(tmp)\n        thetas = ', '.join(thetas)\n        self.thetas = temp_arr__.format(type='double', name='thetas',\n                                        values=thetas)\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')", "response": "Port a trained estimator to the syntax of a chosen programming language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport the model data in a JSON file.", "response": "def export_data(self, directory, filename, with_md5_hash=False):\n        \"\"\"\n        Save model data in a JSON file.\n\n        Parameters\n        ----------\n        :param directory : string\n            The directory.\n        :param filename : string\n            The filename.\n        :param with_md5_hash : bool\n            Whether to append the checksum to the filename or not.\n        \"\"\"\n        model_data = {\n            'priors': self.estimator.class_prior_.tolist(),\n            'sigmas': self.estimator.sigma_.tolist(),\n            'thetas': self.estimator.theta_.tolist()\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the estimator class.", "response": "def create_class(self, method):\n        \"\"\"\n        Build the estimator class.\n\n        Returns\n        -------\n        :return : string\n            The built class as string.\n        \"\"\"\n        self.__dict__.update(dict(method=method))\n        temp_class = self.temp('separated.class')\n        return temp_class.format(**self.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nports a trained estimator to the syntax of a chosen programming language.", "response": "def export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        # Basic parameters:\n        self.metric = est.metric\n        self.n_classes = len(est.classes_)\n        self.n_templates = len(est._fit_X)  # pylint: disable=W0212\n        self.n_features = len(est._fit_X[0])  # pylint: disable=W0212\n        self.n_neighbors = est.n_neighbors\n        self.algorithm = est.algorithm\n        self.power_param = est.p\n\n        if self.algorithm != 'brute':\n            from sklearn.neighbors.kd_tree import KDTree  # pylint: disable-msg=E0611\n            from sklearn.neighbors.ball_tree import BallTree  # pylint: disable-msg=E0611\n            tree = est._tree  # pylint: disable=W0212\n            if isinstance(tree, (KDTree, BallTree)):\n                self.tree = tree\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_data(self, directory, filename, with_md5_hash=False):\n        model_data = {\n            'X': self.estimator._fit_X.tolist(),  # pylint: disable=W0212\n            'y': self.estimator._y.tolist(),  # pylint: disable=W0212\n            'kNeighbors': self.n_neighbors,\n            'nClasses': self.n_classes,\n            'power': self.power_param\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)", "response": "Exports the model data in a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(self, temp_type):\n        # Exported:\n        if temp_type == 'exported':\n            temp = self.temp('exported.class')\n            return temp.format(class_name=self.class_name,\n                               method_name=self.method_name,\n                               n_features=self.n_features)\n        # Separated:\n        if temp_type == 'separated':\n            meth = self.create_method()\n            return self.create_class(meth)", "response": "Transpile the predict method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the estimator method or function.", "response": "def create_method(self):\n        \"\"\"\n        Build the estimator method or function.\n\n        Returns\n        -------\n        :return : string\n            The built method as string.\n        \"\"\"\n        # Distance computation\n        metric_name = '.'.join(['separated', 'metric', self.metric])\n        distance_comp = self.temp(metric_name, n_indents=1, skipping=True)\n\n        temp_method = self.temp('separated.method.predict', n_indents=1,\n                                skipping=True)\n        return temp_method.format(class_name=self.class_name,\n                                  method_name=self.method_name,\n                                  distance_computation=distance_comp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_class(self, method):\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Samples:\n        temps = []\n        for atts in enumerate(self.estimator._fit_X):  # pylint: disable=W0212\n            tmp = [temp_type.format(self.repr(a)) for a in atts[1]]\n            tmp = temp_arr.format(', '.join(tmp))\n            temps.append(tmp)\n        temps = ', '.join(temps)\n        temps = temp_arr__.format(type='double', name='X', values=temps,\n                                  n=self.n_templates, m=self.n_features)\n\n        # Classes:\n        classes = self.estimator._y  # pylint: disable=W0212\n        classes = [temp_type.format(int(c)) for c in classes]\n        classes = ', '.join(classes)\n        classes = temp_arr_.format(type='int', name='y', values=classes,\n                                   n=self.n_templates)\n\n        temp_class = self.temp('separated.class')\n        return temp_class.format(class_name=self.class_name,\n                                 method_name=self.method_name, method=method,\n                                 n_features=self.n_features, X=temps, y=classes,\n                                 n_neighbors=self.n_neighbors,\n                                 n_templates=self.n_templates,\n                                 n_classes=self.n_classes,\n                                 power=self.power_param)", "response": "Builds the estimator class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nporting a trained estimator to the syntax of a chosen programming language.", "response": "def export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Templates of primitive data types:\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Estimator:\n        est = self.estimator\n        self.params = est.get_params()\n\n        # Check kernel type:\n        supported_kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n        if self.params['kernel'] not in supported_kernels:\n            msg = 'The kernel type is not supported.'\n            raise ValueError(msg)\n\n        self.n_features = len(est.support_vectors_[0])\n        self.svs_rows = est.n_support_\n        self.n_svs_rows = len(est.n_support_)\n\n        self.weights = self.temp('arr[]', skipping=True).format(\n            type='int', name='weights', values=', '.join([str(e) for e in\n                                                          self.svs_rows]),\n            n=len(self.svs_rows))\n        self.n_weights = len(self.svs_rows)\n\n        self.n_classes = len(est.classes_)\n        self.is_binary = self.n_classes == 2\n        self.prefix = 'binary' if self.is_binary else 'multi'\n\n        # Support vectors:\n        vectors = []\n        for vector in est.support_vectors_:\n            _vectors = [temp_type.format(self.repr(v)) for v in vector]\n            _vectors = temp_arr.format(', '.join(_vectors))\n            vectors.append(_vectors)\n        vectors = ', '.join(vectors)\n        vectors = self.temp('arr[][]', skipping=True).format(\n            type='double', name='vectors', values=vectors,\n            n=len(est.support_vectors_), m=len(est.support_vectors_[0]))\n        self.vectors = vectors\n        self.n_vectors = len(est.support_vectors_)\n\n        # Coefficients:\n        coeffs = []\n        for coeff in est.dual_coef_:\n            _coeffs = [temp_type.format(self.repr(c)) for c in coeff]\n            _coeffs = temp_arr.format(', '.join(_coeffs))\n            coeffs.append(_coeffs)\n        coeffs = ', '.join(coeffs)\n        coeffs = temp_arr__.format(type='double', name='coefficients',\n                                   values=coeffs, n=len(est.dual_coef_),\n                                   m=len(est.dual_coef_[0]))\n        self.coefficients = coeffs\n        self.n_coefficients = len(est.dual_coef_)\n\n        # Interceptions:\n        inters = [temp_type.format(self.repr(i)) for i in est._intercept_]\n        inters = ', '.join(inters)\n        inters = temp_arr_.format(type='double', name='intercepts',\n                                  values=inters, n=len(est._intercept_))\n        self.intercepts = inters\n        self.n_intercepts = len(est._intercept_)\n\n        # Kernel:\n        self.kernel = str(self.params['kernel'])\n        if self.target_language == 'c':\n            self.kernel = self.kernel[0]\n\n        # Gamma:\n        self.gamma = self.params['gamma']\n        if self.gamma == 'auto':\n            self.gamma = 1. / self.n_features\n        self.gamma = self.repr(self.gamma)\n\n        # Coefficient and degree:\n        self.coef0 = self.repr(self.params['coef0'])\n        self.degree = self.repr(self.params['degree'])\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, temp_type):\n        # Exported:\n        if temp_type == 'exported':\n            temp = self.temp('exported.class')\n            return temp.format(class_name=self.class_name,\n                               method_name=self.method_name)\n        # Separated\n        self.method = self.create_method()\n        return self.create_class()", "response": "Transpile the predict method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_data(self, directory, filename, with_md5_hash=False):\n        model_data = {\n            'vectors': self.estimator.support_vectors_.tolist(),\n            'coefficients': self.estimator.dual_coef_.tolist(),\n            'intercepts': self.estimator._intercept_.tolist(),\n            'weights': self.estimator.n_support_.tolist(),\n            'kernel': self.kernel,\n            'gamma': float(self.gamma),\n            'coef0': float(self.coef0),\n            'degree': float(self.degree),\n            'nClasses': int(self.n_classes),\n            'nRows': int(self.n_svs_rows)\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)", "response": "Exports the model data in a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_method(self):\n        n_indents = 1 if self.target_language in ['java', 'js',\n                                                  'php', 'ruby'] else 0\n        return self.temp('separated.method', n_indents=n_indents,\n                         skipping=True).format(**self.__dict__)", "response": "Build the estimator method or function."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a specific command.", "response": "def call(cmd, shell=True, cwd=None, universal_newlines=True, stderr=STDOUT):\n        \"\"\"Just execute a specific command.\"\"\"\n        return Shell._run(call, cmd, shell=shell, cwd=cwd, stderr=stderr,\n                          universal_newlines=universal_newlines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_meta(path):\n    with open(path) as f:\n        meta = load(f, encoding='utf-8')\n        meta = {k: v.decode('utf-8') if isinstance(v, bytes) else v\n                for k, v in meta.items()}\n\n        src_dir = abspath(dirname(path))\n\n        if 'requirements' in meta and \\\n                str(meta['requirements']).startswith('file://'):\n            req_path = str(meta['requirements'])[7:]\n            req_path = join(src_dir, req_path)\n            if exists(req_path):\n                reqs = open(req_path, 'r').read().strip().split('\\n')\n                reqs = [req.strip() for req in reqs if 'git+' not in req]\n                meta['requirements'] = reqs\n            else:\n                meta['requirements'] = ''\n\n        if 'long_description' in meta and \\\n                str(meta['long_description']).startswith('file://'):\n            readme_path = str(meta['long_description'])[7:]\n            readme_path = join(src_dir, readme_path)\n            if exists(readme_path):\n                readme = open(readme_path, 'r').read().strip()\n                meta['long_description'] = readme\n            else:\n                meta['long_description'] = ''\n\n    return meta", "response": "Load meta data about this package from file pypi. json."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming\n        language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.n_classes = len(est.classes_)\n        self.n_features = len(est.feature_log_prob_[0])\n\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Create class prior probabilities:\n        priors = [self.temp('type').format(self.repr(p)) for p in\n                  est.class_log_prior_]\n        priors = ', '.join(priors)\n        self.priors = temp_arr_.format(type='double', name='priors',\n                                       values=priors)\n\n        # Create negative probabilities:\n        neg_prob = log(1 - exp(est.feature_log_prob_))\n        probs = []\n        for prob in neg_prob:\n            tmp = [temp_type.format(self.repr(p)) for p in prob]\n            tmp = temp_arr.format(', '.join(tmp))\n            probs.append(tmp)\n        probs = ', '.join(probs)\n        self.neg_probs = temp_arr__.format(type='double', name='negProbs',\n                                           values=probs)\n\n        delta_probs = (est.feature_log_prob_ - neg_prob).T\n        probs = []\n        for prob in delta_probs:\n            tmp = [temp_type.format(self.repr(p)) for p in prob]\n            tmp = temp_arr.format(', '.join(tmp))\n            probs.append(tmp)\n        probs = ', '.join(probs)\n        self.del_probs = temp_arr__.format(type='double', name='delProbs',\n                                           values=probs)\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')", "response": "Port a trained estimator to the syntax of a chosen programming language and export the model data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports the model data in a JSON file.", "response": "def export_data(self, directory, filename, with_md5_hash=False):\n        \"\"\"\n        Save model data in a JSON file.\n\n        Parameters\n        ----------\n        :param directory : string\n            The directory.\n        :param filename : string\n            The filename.\n        :param with_md5_hash : bool, default: False\n            Whether to append the checksum to the filename or not.\n        \"\"\"\n        neg_prob = log(1 - exp(self.estimator.feature_log_prob_))\n        delta_probs = (self.estimator.feature_log_prob_ - neg_prob).T\n        model_data = {\n            'priors': self.estimator.class_log_prior_.tolist(),\n            'negProbs': neg_prob.tolist(),\n            'delProbs': delta_probs.tolist()\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nindenting text with single spaces.", "response": "def indent(self, text, n_indents=1, skipping=False):\n        \"\"\"\n        Indent text with single spaces.\n\n        Parameters\n        ----------\n        :param text : string\n            The text which get a specific indentation.\n        :param n_indents : int, default: 1\n            The number of indentations.\n        :param skipping : boolean, default: False\n            Whether to skip the initial indentation.\n\n        Returns\n        -------\n        return : string\n            The indented text.\n        \"\"\"\n        lines = text.splitlines()\n        space = self.TEMPLATES.get(self.target_language).get('indent', ' ')\n\n        # Single line:\n        if len(lines) == 1:\n            if skipping:\n                return text.strip()\n            return n_indents * space + text.strip()\n\n        # Multiple lines:\n        indented_lines = []\n        for idx, line in enumerate(lines):\n            if skipping and idx is 0:\n                indented_lines.append(line)\n            else:\n                line = n_indents * space + line\n                indented_lines.append(line)\n        indented_text = '\\n'.join(indented_lines)\n        return indented_text"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string that can be used to create a new template for a specific key name.", "response": "def temp(self, name, templates=None, n_indents=None, skipping=False):\n        \"\"\"\n        Get specific template of chosen\n        programming language.\n\n        Parameters\n        ----------\n        :param param name : string\n            The key name of the template.\n        :param param templates : string, default: None\n            The template with placeholders.\n        :param param n_indents : int, default: None\n            The number of indentations.\n        :param param skipping : bool, default: False\n            Whether to skip the initial indentation.\n\n        Returns\n        -------\n        return : string\n            The wanted template string.\n        \"\"\"\n        if templates is None:\n            templates = self.TEMPLATES.get(self.target_language)\n        keys = name.split('.')\n        key = keys.pop(0).lower()\n        template = templates.get(key, None)\n        if template is not None:\n            if isinstance(template, str):\n                if n_indents is not None:\n                    template = self.indent(template, n_indents, skipping)\n                return template\n            else:\n                keys = '.'.join(keys)\n                return self.temp(keys, template, skipping=False)\n        else:\n            class_name = self.__class__.__name__\n            estimator_type = getattr(self, 'estimator_type') if \\\n                hasattr(self, 'estimator_type') else 'classifier'\n            path = join(dirname(__file__), 'estimator',\n                        estimator_type, class_name, 'templates',\n                        self.target_language, name + '.txt')\n            if isfile(path):\n                with open(path, 'r') as file_:\n                    template = file_.read()\n                if n_indents is not None:\n                    template = self.indent(template, n_indents, skipping)\n                return template\n            else:\n                err = \"Template '{}' wasn't found.\".format(name)\n                raise AttributeError(err)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the model data in a JSON file.", "response": "def export_data(self, directory, filename, with_md5_hash=False):\n        \"\"\"\n        Save model data in a JSON file.\n\n        Parameters\n        ----------\n        :param directory : string\n            The directory.\n        :param filename : string\n            The filename.\n        :param with_md5_hash : bool\n            Whether to append the checksum to the filename or not.\n        \"\"\"\n        model_data = []\n        for est in self.estimators:\n            model_data.append({\n                'childrenLeft': est.tree_.children_left.tolist(),\n                'childrenRight': est.tree_.children_right.tolist(),\n                'thresholds': est.tree_.threshold.tolist(),\n                'classes': [e[0] for e in est.tree_.value.tolist()],\n                'indices': est.tree_.feature.tolist()\n            })\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and port a single tree estimator.", "response": "def create_branches(self, left_nodes, right_nodes, threshold,\n                        value, features, node, depth):\n        \"\"\"\n        Parse and port a single tree estimator.\n\n        Parameters\n        ----------\n        :param left_nodes : object\n            The left children node.\n        :param right_nodes : object\n            The left children node.\n        :param threshold : object\n            The decision threshold.\n        :param value : object\n            The label or class.\n        :param features : object\n            The feature values.\n        :param node : int\n            The current node.\n        :param depth : int\n            The tree depth.\n\n        Returns\n        -------\n        :return out : string\n            The ported single tree as function or method.\n        \"\"\"\n        out = ''  # returned output\n        if threshold[node] != -2.:\n            out += '\\n'\n            temp = self.temp('if', n_indents=depth)\n            out += temp.format(features[node], '<=', self.repr(threshold[node]))\n            if left_nodes[node] != -1.:\n                out += self.create_branches(\n                    left_nodes, right_nodes, threshold, value,\n                    features, left_nodes[node], depth + 1)\n            out += '\\n'\n            out += self.temp('else', n_indents=depth)\n            if right_nodes[node] != -1.:\n                out += self.create_branches(\n                    left_nodes, right_nodes, threshold, value,\n                    features, right_nodes[node], depth + 1)\n            out += '\\n'\n            out += self.temp('endif', n_indents=depth)\n        else:\n            clazzes = []\n            temp_arr = self.temp('arr', n_indents=depth)\n            for i, rate in enumerate(value[node][0]):\n                clazz = temp_arr.format(i, int(rate))\n                clazz = '\\n' + clazz\n                clazzes.append(clazz)\n            out += self.temp('join').join(clazzes) + self.temp('join')\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_single_method(self, estimator_index, estimator):\n        indices = [str(e) for e in estimator.tree_.feature]\n\n        tree_branches = self.create_branches(\n            estimator.tree_.children_left, estimator.tree_.children_right,\n            estimator.tree_.threshold, estimator.tree_.value, indices, 0, 1)\n\n        temp_single_method = self.temp('embedded.single_method')\n        return temp_single_method.format(method_name=self.method_name,\n                                         method_id=str(estimator_index),\n                                         n_classes=self.n_classes,\n                                         tree_branches=tree_branches)", "response": "Creates a method for a single tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_sklearn_version():\n        from sklearn import __version__ as sklearn_ver\n        sklearn_ver = str(sklearn_ver).split('.')\n        sklearn_ver = [int(v) for v in sklearn_ver]\n        major, minor = sklearn_ver[0], sklearn_ver[1]\n        patch = sklearn_ver[2] if len(sklearn_ver) >= 3 else 0\n        return major, minor, patch", "response": "Determine the installed version of sklearn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _platform_is_windows(platform=sys.platform):\n        matched = platform in ('cygwin', 'win32', 'win64')\n        if matched:\n            error_msg = \"Windows isn't supported yet\"\n            raise OSError(error_msg)\n        return matched", "response": "Is the current OS a Windows?"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_app(name, check_platform=True):\n        if check_platform:\n            Environment._platform_is_windows()\n        return which(str(name)) is not None", "response": "Check whether the application with the given name is installed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_apps(names, check_platform=True):\n        if check_platform:\n            Environment._platform_is_windows()\n        for name in names:\n            yield Environment.has_app(str(name), check_platform=False)", "response": "Check whether the applications [ <name >... ] are installed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck whether specific requirements are available.", "response": "def check_deps(deps):\n        \"\"\"check whether specific requirements are available.\"\"\"\n        if not isinstance(deps, list):\n            deps = [deps]\n        checks = list(Environment.has_apps(deps))\n        if not all(checks):\n            for name, available in list(dict(zip(deps, checks)).items()):\n                if not available:\n                    error_msg = \"The required application/dependency '{0}'\" \\\n                                \" isn't available.\".format(name)\n                    raise SystemError(error_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming\n        language.\n\n        Parameters\n        ----------\n        :param class_name : string, default: 'Brain'\n            The name of the class in the returned result.\n        :param method_name : string, default: 'predict'\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.n_features = len(est.coef_[0])\n        self.n_classes = len(est.classes_)\n        self.is_binary = self.n_classes == 2\n        self.prefix = 'binary' if self.is_binary else 'multi'\n\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Coefficients:\n        if self.is_binary:\n            coefs = est.coef_[0]\n            coefs = [temp_type.format(self.repr(c)) for c in coefs]\n            coefs = ', '.join(coefs)\n            coefs = temp_arr_.format(type='double', name='coefficients',\n                                     values=coefs, n=self.n_features)\n        else:\n            coefs = []\n            for coef in est.coef_:\n                tmp = [temp_type.format(self.repr(c)) for c in coef]\n                tmp = temp_arr.format(', '.join(tmp))\n                coefs.append(tmp)\n            coefs = ', '.join(coefs)\n            coefs = temp_arr__.format(type='double', name='coefficients',\n                                      values=coefs, n=self.n_classes,\n                                      m=self.n_features)\n        self.coefficients = coefs\n\n        # Intercepts:\n        if self.is_binary:\n            inters = est.intercept_[0]\n            temp_init = self.temp('init')\n            inters = temp_init.format(type='double', name='intercepts',\n                                      value=self.repr(inters))\n        else:\n            inters = est.intercept_\n            inters = [temp_type.format(self.repr(i)) for i in inters]\n            inters = ', '.join(inters)\n            inters = temp_arr_.format(type='double', name='intercepts',\n                                      values=inters, n=self.n_classes)\n        self.intercepts = inters\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')", "response": "Port a trained estimator to the syntax of a chosen programming language."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_data(self, directory, filename, with_md5_hash=False):\n        est = self.estimator\n        coefs = est.coef_[0] if self.is_binary else est.coef_\n        inters = est.intercept_[0] if self.is_binary else est.intercept_\n        model_data = {\n            'coefficients': coefs.tolist(),\n            'intercepts': inters.tolist(),\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)", "response": "Exports the model data in a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the method or function for the estimator.", "response": "def create_method(self):\n        \"\"\"\n        Build the estimator method or function.\n\n        Returns\n        -------\n        :return : string\n            The built method as string.\n        \"\"\"\n        n_indents = 0 if self.target_language in ['c', 'go'] else 1\n        method_type = 'separated.{}.method'.format(self.prefix)\n        method_temp = self.temp(method_type, n_indents=n_indents,\n                                skipping=True)\n        return method_temp.format(**self.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the estimator class.", "response": "def create_class(self):\n        \"\"\"\n        Build the estimator class.\n\n        Returns\n        -------\n        :return : string\n            The built class as string.\n        \"\"\"\n        if self.target_language in ['java', 'go']:\n            n_indents = 1 if self.target_language == 'java' else 0\n            class_head_temp = self.temp('separated.{}.class'.format(\n                self.prefix), n_indents=n_indents, skipping=True)\n            self.class_head = class_head_temp.format(**self.__dict__)\n\n        return self.temp('separated.class').format(**self.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, embed_data=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming\n        language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n        :param embed_data : bool, default: False\n            Whether the model data should be embedded in the template or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.n_features = est.n_features_\n        self.n_classes = len(self.estimator.tree_.value.tolist()[0][0])\n\n        temp_arr_scope = self.temp('arr_scope')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        left_childs = [str(e) for e in self.estimator.tree_.children_left]\n        left_childs = temp_arr_.format(type='int', name='lChilds',\n                                       values=', '.join(left_childs),\n                                       n=len(left_childs))\n        self.left_childs = left_childs\n\n        right_childs = [str(e) for e in self.estimator.tree_.children_right]\n        right_childs = temp_arr_.format(type='int', name='rChilds',\n                                        values=', '.join(right_childs),\n                                        n=len(right_childs))\n        self.right_childs = right_childs\n\n        thresholds = [self.repr(e) for e in\n                      self.estimator.tree_.threshold.tolist()]\n        type_ = 'float64' if self.target_language == 'go' else 'double'\n        thresholds = temp_arr_.format(type=type_, name='thresholds',\n                                      values=', '.join(thresholds),\n                                      n=len(thresholds))\n        self.thresholds = thresholds\n\n        indices = [str(e) for e in self.estimator.tree_.feature]\n        indices = temp_arr_.format(type='int', name='indices',\n                                   values=', '.join(indices), n=len(indices))\n        self.indices = indices\n\n        classes = self.estimator.tree_.value.tolist()\n        n = len(classes)\n        m = self.n_classes\n        classes = [', '.join([str(int(x)) for x in e[0]]) for e in classes]\n        classes = ', '.join([temp_arr_scope.format(v) for v in classes])\n        classes = temp_arr__.format(type='int', name='classes', values=classes,\n                                    n=n, m=m)\n        self.classes = classes\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Embedded:\n            if embed_data:\n                return self.predict('embedded')\n            # Separated:\n            return self.predict('separated')", "response": "Port a trained estimator to the syntax of a chosen programming language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(self, temp_type='separated'):\n        if temp_type == 'exported':\n            exported_temp = self.temp('exported.class')\n            return exported_temp.format(class_name=self.class_name,\n                                        method_name=self.method_name,\n                                        n_features=self.n_features)\n\n        if temp_type == 'separated':\n            separated_temp = self.temp('separated.class')\n            return separated_temp.format(**self.__dict__)\n\n        if temp_type == 'embedded':\n            n_indents = 1 if self.target_language in ['java', 'js',\n                                                      'php', 'ruby'] else 0\n            branches = self.indent(self.create_tree(), n_indents=1)\n            embedded_temp = self.temp('embedded.method', n_indents=n_indents,\n                                      skipping=True)\n            method = embedded_temp.format(class_name=self.class_name,\n                                          method_name=self.method_name,\n                                          n_classes=self.n_classes,\n                                          n_features=self.n_features,\n                                          branches=branches)\n            embedded_temp = self.temp('embedded.class')\n            return embedded_temp.format(class_name=self.class_name,\n                                        method_name=self.method_name,\n                                        n_classes=self.n_classes,\n                                        n_features=self.n_features,\n                                        method=method)", "response": "Transpile the predict method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_tree(self):\n        feature_indices = []\n        for i in self.estimator.tree_.feature:\n            n_features = self.n_features\n            if self.n_features > 1 or (self.n_features == 1 and i >= 0):\n                feature_indices.append([str(j) for j in range(n_features)][i])\n        indentation = 1 if self.target_language in ['java', 'js',\n                                                    'php', 'ruby'] else 0\n        return self.create_branches(\n            self.estimator.tree_.children_left,\n            self.estimator.tree_.children_right,\n            self.estimator.tree_.threshold,\n            self.estimator.tree_.value,\n            feature_indices, 0, indentation)", "response": "Parse and build the tree branches."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.output_activation = est.out_activation_\n        self.hidden_activation = est.activation\n\n        self.n_layers = est.n_layers_\n        self.n_hidden_layers = est.n_layers_ - 2\n\n        self.n_inputs = len(est.coefs_[0])\n        self.n_outputs = est.n_outputs_\n\n        self.hidden_layer_sizes = est.hidden_layer_sizes\n        if isinstance(self.hidden_layer_sizes, int):\n            self.hidden_layer_sizes = [self.hidden_layer_sizes]\n        self.hidden_layer_sizes = list(self.hidden_layer_sizes)\n\n        self.layer_units = \\\n            [self.n_inputs] + self.hidden_layer_sizes + [est.n_outputs_]\n\n        # Weights:\n        self.coefficients = est.coefs_\n\n        # Bias:\n        self.intercepts = est.intercepts_\n\n        # Binary or multiclass classifier?\n        self.is_binary = self.n_outputs == 1\n        self.prefix = 'binary' if self.is_binary else 'multi'\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')", "response": "Port a trained estimator to the syntax of a chosen programming language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict(self, temp_type):\n        # Exported:\n        if temp_type == 'exported':\n            temp = self.temp('exported.class')\n            return temp.format(class_name=self.class_name,\n                               method_name=self.method_name)\n        # Separated:\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n        temp_arr___ = self.temp('arr[][][]')\n\n        # Activations:\n        layers = list(self._get_activations())\n        layers = ', '.join(layers)\n        layers = temp_arr_.format(type='int', name='layers', values=layers)\n\n        # Coefficients (weights):\n        coefficients = []\n        for layer in self.coefficients:\n            layer_weights = []\n            for weights in layer:\n                weights = ', '.join([self.repr(w) for w in weights])\n                layer_weights.append(temp_arr.format(weights))\n            layer_weights = ', '.join(layer_weights)\n            coefficients.append(temp_arr.format(layer_weights))\n        coefficients = ', '.join(coefficients)\n        coefficients = temp_arr___.format(type='double',\n                                          name='weights',\n                                          values=coefficients)\n\n        # Intercepts (biases):\n        intercepts = list(self._get_intercepts())\n        intercepts = ', '.join(intercepts)\n        intercepts = temp_arr__.format(type='double',\n                                       name='bias',\n                                       values=intercepts)\n\n        temp_class = self.temp('separated.class')\n        file_name = '{}.js'.format(self.class_name.lower())\n        return temp_class.format(class_name=self.class_name,\n                                 method_name=self.method_name,\n                                 hidden_activation=self.hidden_activation,\n                                 output_activation=self.output_activation,\n                                 n_features=self.n_inputs,\n                                 weights=coefficients,\n                                 bias=intercepts,\n                                 layers=layers,\n                                 file_name=file_name)", "response": "Transpile the predict method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_data(self, directory, filename, with_md5_hash=False):\n        model_data = {\n            'layers': [int(l) for l in list(self._get_activations())],\n            'weights': [c.tolist() for c in self.coefficients],\n            'bias': [i.tolist() for i in self.intercepts],\n            'hidden_activation': self.hidden_activation,\n            'output_activation': self.output_activation\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)", "response": "Save the model data in a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_intercepts(self):\n        temp_arr = self.temp('arr')\n        for layer in self.intercepts:\n            inter = ', '.join([self.repr(b) for b in layer])\n            yield temp_arr.format(inter)", "response": "Yields all intercepts of the classifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nport a trained estimator to the syntax of a chosen programming language and export the model data.", "response": "def export(self, class_name, method_name,\n               export_data=False, export_dir='.', export_filename='data.json',\n               export_append_checksum=False, embed_data=True, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming\n        language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name: string\n            The name of the method in the returned result.\n        :param export_data : bool\n            Whether the model data should be saved or not.\n        :param export_dir : string\n            The directory where the model data should be saved.\n        :param export_filename : string\n            The filename of the exported model data.\n        :param export_append_checksum : bool\n            Whether to append the checksum to the filename or not.\n        :param embed_data : bool, default: True\n            Whether the model data should be embedded in the template or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        # Basic parameters:\n        self.estimators = []\n        for idx in range(est.n_estimators):\n            if est.estimator_weights_[idx] > 0:\n                self.estimators.append(est.estimators_[idx])\n        self.n_classes = est.n_classes_\n        self.n_features = est.estimators_[0].n_features_\n        self.n_estimators = len(self.estimator)\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Embedded:\n            return self.predict('embedded')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_branches(self, left_nodes, right_nodes, threshold,\n                        value, features, node, depth, init=False):\n        \"\"\"\n        Parse and port a single tree estimator.\n\n        Parameters\n        ----------\n        :param left_nodes : object\n            The left children node.\n        :param right_nodes : object\n            The left children node.\n        :param threshold : object\n            The decision threshold.\n        :param value : object\n            The label or class.\n        :param features : object\n            The feature values.\n        :param node : int\n            The current node.\n        :param depth : int\n            The tree depth.\n        :param init : bool, default: True\n            Whether it's the initial tree or not.\n\n        Returns\n        -------\n        :return out : string\n            The ported single tree as function or method.\n        \"\"\"\n        out = ''\n        if threshold[node] != -2.:\n            if not init:\n                out += '\\n'\n            temp = self.temp('if', n_indents=depth, skipping=init)\n            out += temp.format(features[node], '<=', self.repr(threshold[node]))\n            if left_nodes[node] != -1.:\n                out += self.create_branches(\n                    left_nodes, right_nodes, threshold, value,\n                    features, left_nodes[node], depth + 1)\n            out += '\\n'\n            out += self.temp('else', n_indents=depth)\n            if right_nodes[node] != -1.:\n                out += self.create_branches(\n                    left_nodes, right_nodes, threshold, value,\n                    features, right_nodes[node], depth + 1)\n            out += '\\n'\n            out += self.temp('endif', n_indents=depth)\n        else:\n            clazzes = []\n            temp = self.temp('arr', n_indents=depth)\n            for i, val in enumerate(value[node][0]):\n                clazz = temp.format(i, self.repr(val))\n                clazz = '\\n' + clazz\n                clazzes.append(clazz)\n            out += self.temp('join').join(clazzes) + self.temp('join')\n        return out", "response": "Parse and port a single tree estimator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a method for a single tree.", "response": "def create_single_method(self, estimator_index, estimator):\n        \"\"\"\n        Port a method for a single tree.\n\n        Parameters\n        ----------\n        :param estimator_index : int\n            The estimator index.\n        :param estimator : AdaBoostClassifier\n            The estimator itself.\n\n        Returns\n        -------\n        :return : string\n            The created method as string.\n        \"\"\"\n        feature_indices = []\n        for i in estimator.tree_.feature:\n            n_features = estimator.n_features_\n            if self.n_features > 1 or (self.n_features == 1 and i >= 0):\n                feature_indices.append([str(j) for j in range(n_features)][i])\n\n        tree_branches = self.create_branches(\n            estimator.tree_.children_left, estimator.tree_.children_right,\n            estimator.tree_.threshold, estimator.tree_.value,\n            feature_indices, 0, 1, init=True)\n\n        temp_tree = self.temp('embedded.tree')\n        return temp_tree.format(method_name=self.method_name,\n                                method_index=str(estimator_index),\n                                methods=tree_branches,\n                                n_classes=self.n_classes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the method or function names for the class and returns the string that is merged with the built methods or functions.", "response": "def create_embedded_meth(self):\n        \"\"\"\n        Build the estimator methods or functions.\n\n        Returns\n        -------\n        :return : string\n            The built methods as merged string.\n        \"\"\"\n        # Generate related trees:\n        fns = []\n        for idx, estimator in enumerate(self.estimators):\n            tree = self.create_single_method(idx, estimator)\n            fns.append(tree)\n        fns = '\\n'.join(fns)\n\n        # Generate method or function names:\n        fn_names = ''\n        if self.target_language in ['c', 'java']:\n            fn_names = []\n            temp_method_calls = self.temp('embedded.method_calls', n_indents=2,\n                                          skipping=True)\n            for idx, estimator in enumerate(self.estimators):\n                cl_name = self.class_name\n                fn_name = self.method_name + '_' + str(idx)\n                fn_name = temp_method_calls.format(class_name=cl_name,\n                                                   method_name=fn_name,\n                                                   method_index=idx)\n                fn_names.append(fn_name)\n            fn_names = '\\n'.join(fn_names)\n            fn_names = self.indent(fn_names, n_indents=1, skipping=True)\n\n        # Merge generated content:\n        n_indents = 1 if self.target_language in ['java', 'js'] else 0\n        temp_method = self.temp('embedded.method')\n        method = temp_method.format(method_name=self.method_name,\n                                    class_name=self.class_name,\n                                    method_calls=fn_names, methods=fns,\n                                    n_estimators=self.n_estimators,\n                                    n_classes=self.n_classes)\n        return self.indent(method, n_indents=n_indents, skipping=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_embedded_class(self, method):\n        temp_class = self.temp('embedded.class')\n        return temp_class.format(class_name=self.class_name,\n                                 method_name=self.method_name, method=method,\n                                 n_features=self.n_features)", "response": "Build the estimator class."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nports a trained estimator to the syntax of a chosen programming language.", "response": "def export(self, class_name, method_name, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.hidden_activation = est.activation\n        self.n_layers = est.n_layers_\n        self.n_hidden_layers = est.n_layers_ - 2\n        self.n_inputs = len(est.coefs_[0])\n        self.n_outputs = est.n_outputs_\n\n        self.hidden_layer_sizes = est.hidden_layer_sizes\n        if isinstance(self.hidden_layer_sizes, int):\n            self.hidden_layer_sizes = [self.hidden_layer_sizes]\n        self.hidden_layer_sizes = list(self.hidden_layer_sizes)\n\n        self.layer_units = \\\n            [self.n_inputs] + self.hidden_layer_sizes + [est.n_outputs_]\n\n        # Weights:\n        self.coefficients = est.coefs_\n\n        # Bias:\n        self.intercepts = est.intercepts_\n\n        if self.target_method == 'predict':\n            return self.predict()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_method(self):\n        temp_method = self.temp('method', skipping=True, n_indents=1)\n        return temp_method.format(class_name=self.class_name,\n                                  method_name=self.method_name,\n                                  n_features=self.n_inputs,\n                                  n_classes=self.n_outputs)", "response": "Build the estimator method or function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, class_name=None, method_name=None,\n               num_format=lambda x: str(x), details=False, **kwargs):\n        # pylint: disable=unused-argument\n        \"\"\"\n        Transpile a trained model to the syntax of a\n        chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string, default: None\n            The name for the ported class.\n\n        :param method_name : string, default: None\n            The name for the ported method.\n\n        :param num_format : lambda x, default: lambda x: str(x)\n            The representation of the floating-point values.\n\n        :param details : bool, default False\n            Return additional data for the compilation and execution.\n\n        Returns\n        -------\n        model : {mix}\n            The ported model as string or a dictionary\n            with further information.\n        \"\"\"\n\n        if class_name is None or class_name == '':\n            class_name = self.estimator_name\n\n        if method_name is None or method_name == '':\n            method_name = self.target_method\n\n        if isinstance(num_format, types.LambdaType):\n            self.template._num_format = num_format\n\n        output = self.template.export(class_name=class_name,\n                                      method_name=method_name, **kwargs)\n        if not details:\n            return output\n\n        language = self.target_language\n        filename = Porter._get_filename(class_name, language)\n        comp_cmd, exec_cmd = Porter._get_commands(filename, class_name,\n                                                  language)\n        output = {\n            'estimator': str(output),\n            'filename': filename,\n            'class_name': class_name,\n            'method_name': method_name,\n            'cmd': {\n                'compilation': comp_cmd,\n                'execution': exec_cmd\n            },\n            'algorithm': {\n                'type': self.estimator_type,\n                'name': self.estimator_name\n            }\n        }\n        return output", "response": "Exports a trained model to the syntax of the chosen programming language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef port(self, class_name=None, method_name=None,\n             num_format=lambda x: str(x), details=False, **kwargs):\n        # pylint: disable=unused-argument\n        \"\"\"\n        Transpile a trained model to the syntax of a\n        chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string, default: None\n            The name for the ported class.\n\n        :param method_name : string, default: None\n            The name for the ported method.\n\n        :param num_format : lambda x, default: lambda x: str(x)\n            The representation of the floating-point values.\n\n        :param details : bool, default: False\n            Return additional data for the compilation\n            and execution.\n\n        Returns\n        -------\n        model : {mix}\n            The ported model as string or a dictionary\n            with further information.\n        \"\"\"\n        loc = locals()\n        loc.pop(str('self'))\n        return self.export(**loc)", "response": "Returns a string representation of the ported model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _classifiers(self):\n\n        # sklearn version < 0.18.0\n        classifiers = (\n            AdaBoostClassifier,\n            BernoulliNB,\n            DecisionTreeClassifier,\n            ExtraTreesClassifier,\n            GaussianNB,\n            KNeighborsClassifier,\n            LinearSVC,\n            NuSVC,\n            RandomForestClassifier,\n            SVC,\n        )\n\n        # sklearn version >= 0.18.0\n        if self.sklearn_ver[:2] >= (0, 18):\n            from sklearn.neural_network.multilayer_perceptron \\\n                import MLPClassifier\n            classifiers += (MLPClassifier, )\n\n        return classifiers", "response": "Returns a set of supported classifiers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a set of supported regressors.", "response": "def _regressors(self):\n        \"\"\"\n        Get a set of supported regressors.\n\n        Returns\n        -------\n        regressors : {set}\n            The set of supported regressors.\n        \"\"\"\n\n        # sklearn version < 0.18.0\n        regressors = ()\n\n        # sklearn version >= 0.18.0\n        if self.sklearn_ver[:2] >= (0, 18):\n            from sklearn.neural_network.multilayer_perceptron \\\n                import MLPRegressor\n            regressors += (MLPRegressor, )\n\n        return regressors"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, X, class_name=None, method_name=None, tnp_dir='tmp',\n                keep_tmp_dir=False, num_format=lambda x: str(x)):\n        \"\"\"\n        Predict using the transpiled model.\n\n        Parameters\n        ----------\n        :param X : {array-like}, shape (n_features) or (n_samples, n_features)\n            The input data.\n\n        :param class_name : string, default: None\n            The name for the ported class.\n\n        :param method_name : string, default: None\n            The name for the ported method.\n\n        :param tnp_dir : string, default: 'tmp'\n            The path to the temporary directory for\n            storing the transpiled (and compiled) model.\n\n        :param keep_tmp_dir : bool, default: False\n            Whether to delete the temporary directory\n            or not.\n\n        :param num_format : lambda x, default: lambda x: str(x)\n            The representation of the floating-point values.\n\n        Returns\n        -------\n            y : int or array-like, shape (n_samples,)\n            The predicted class or classes.\n        \"\"\"\n\n        if class_name is None:\n            class_name = self.estimator_name\n\n        if method_name is None:\n            method_name = self.target_method\n\n        # Dependencies:\n        if not self._tested_dependencies:\n            self._test_dependencies()\n            self._tested_dependencies = True\n\n        # Support:\n        if 'predict' not in set(self.template.SUPPORTED_METHODS):\n            error = \"Currently the given model method\" \\\n                    \" '{}' isn't supported.\".format('predict')\n            raise AttributeError(error)\n\n        # Cleanup:\n        Shell.call('rm -rf {}'.format(tnp_dir))\n        Shell.call('mkdir {}'.format(tnp_dir))\n\n        # Transpiled model:\n        details = self.export(class_name=class_name,\n                              method_name=method_name,\n                              num_format=num_format,\n                              details=True)\n        filename = Porter._get_filename(class_name, self.target_language)\n        target_file = os.path.join(tnp_dir, filename)\n        with open(target_file, str('w')) as file_:\n            file_.write(details.get('estimator'))\n\n        # Compilation command:\n        comp_cmd = details.get('cmd').get('compilation')\n        if comp_cmd is not None:\n            Shell.call(comp_cmd, cwd=tnp_dir)\n\n        # Execution command:\n        exec_cmd = details.get('cmd').get('execution')\n        exec_cmd = str(exec_cmd).split()\n\n        pred_y = None\n\n        # Single feature set:\n        if exec_cmd is not None and len(X.shape) == 1:\n            full_exec_cmd = exec_cmd + [str(sample).strip() for sample in X]\n            pred_y = Shell.check_output(full_exec_cmd, cwd=tnp_dir)\n            pred_y = int(pred_y)\n\n        # Multiple feature sets:\n        if exec_cmd is not None and len(X.shape) > 1:\n            pred_y = np.empty(X.shape[0], dtype=int)\n            for idx, features in enumerate(X):\n                full_exec_cmd = exec_cmd + [str(f).strip() for f in features]\n                pred = Shell.check_output(full_exec_cmd, cwd=tnp_dir)\n                pred_y[idx] = int(pred)\n\n        # Cleanup:\n        if not keep_tmp_dir:\n            Shell.call('rm -rf {}'.format(tnp_dir))\n\n        return pred_y", "response": "Predict using the transpiled model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef integrity_score(self, X, method='predict', normalize=True,\n                        num_format=lambda x: str(x)):\n        \"\"\"\n        Compute the accuracy of the ported classifier.\n\n        Parameters\n        ----------\n        :param X : ndarray, shape (n_samples, n_features)\n            Input data.\n\n        :param method : string, default: 'predict'\n            The method which should be tested.\n\n        :param normalize : bool, default: True\n            If ``False``, return the number of correctly classified samples.\n            Otherwise, return the fraction of correctly classified samples.\n\n        :param num_format : lambda x, default: lambda x: str(x)\n            The representation of the floating-point values.\n\n        Returns\n        -------\n        score : float\n            If ``normalize == True``, return the correctly classified samples\n            (float), else it returns the number of correctly classified samples\n            (int).\n            The best performance is 1 with ``normalize == True`` and the number\n            of samples with ``normalize == False``.\n        \"\"\"\n        X = np.array(X)\n        if not X.ndim > 1:\n            X = np.array([X])\n\n        method = str(method).strip().lower()\n        if method not in ['predict', 'predict_proba']:\n            error = \"The given method '{}' isn't supported.\".format(method)\n            raise AttributeError(error)\n\n        if method == 'predict':\n            y_true = self.estimator.predict(X)\n            y_pred = self.predict(X, tnp_dir='tmp_integrity_score',\n                                  keep_tmp_dir=True, num_format=num_format)\n            return accuracy_score(y_true, y_pred, normalize=normalize)\n\n        return False", "response": "Compute the accuracy of the ported classifier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_filename(class_name, language):\n        name = str(class_name).strip()\n        lang = str(language)\n\n        # Name:\n        if language in ['java', 'php']:\n            name = \"\".join([name[0].upper() + name[1:]])\n\n        # Suffix:\n        suffix = {\n            'c': 'c', 'java': 'java', 'js': 'js',\n            'go': 'go', 'php': 'php', 'ruby': 'rb'\n        }\n        suffix = suffix.get(lang, lang)\n\n        # Filename:\n        return '{}.{}'.format(name, suffix)", "response": "Generate the specific filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the related compilation and execution commands.", "response": "def _get_commands(filename, class_name, language):\n        \"\"\"\n        Generate the related compilation and\n        execution commands.\n\n        Parameters\n        ----------\n        :param filename : str\n            The used filename.\n\n        :param class_name : str\n            The used class name.\n\n        :param language : {'c', 'go', 'java', 'js', 'php', 'ruby'}\n            The target programming language.\n\n        Returns\n        -------\n            comp_cmd, exec_cmd : (str, str)\n            The compilation and execution command.\n        \"\"\"\n        cname = str(class_name)\n        fname = str(filename)\n        lang = str(language)\n\n        # Compilation variants:\n        comp_vars = {\n            # gcc brain.c -o brain\n            'c': 'gcc {} -lm -o {}'.format(fname, cname),\n            # javac Brain.java\n            'java': 'javac {}'.format(fname),\n            # go build -o brain brain.go\n            'go': 'go build -o {} {}.go'.format(cname, cname)\n        }\n        comp_cmd = comp_vars.get(lang, None)\n\n        # Execution variants:\n        exec_vars = {\n            # ./brain\n            'c': os.path.join('.', cname),\n            # java -classpath . Brain\n            'java': 'java -classpath . {}'.format(cname),\n            # node brain.js\n            'js': 'node {}'.format(fname),\n            # php -f Brain.php\n            'php': 'php -f {}'.format(fname),\n            # ruby brain.rb\n            'ruby': 'ruby {}'.format(fname),\n            # ./brain\n            'go': os.path.join('.', cname),\n        }\n        exec_cmd = exec_vars.get(lang, None)\n\n        return comp_cmd, exec_cmd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the metadata for the main region for the given region.", "response": "def _get_metadata_for_region(region_code):\n    \"\"\"The metadata needed by this class is the same for all regions\n    sharing the same country calling code. Therefore, we return the\n    metadata for \"main\" region for this country calling code.\"\"\"\n    country_calling_code = country_code_for_region(region_code)\n    main_country = region_code_for_country_code(country_calling_code)\n    # Set to a default instance of the metadata. This allows us to\n    # function with an incorrect region code, even if formatting only\n    # works for numbers specified with \"+\".\n    return PhoneMetadata.metadata_for_region(main_country, _EMPTY_METADATA)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _maybe_create_new_template(self):\n        ii = 0\n        while ii < len(self._possible_formats):\n            number_format = self._possible_formats[ii]\n            pattern = number_format.pattern\n            if self._current_formatting_pattern == pattern:\n                return False\n            if self._create_formatting_template(number_format):\n                self._current_formatting_pattern = pattern\n                if number_format.national_prefix_formatting_rule is None:\n                    self._should_add_space_after_national_prefix = False\n                else:\n                    self._should_add_space_after_national_prefix = bool(_NATIONAL_PREFIX_SEPARATORS_PATTERN.search(number_format.national_prefix_formatting_rule))\n                # With a new formatting template, the matched position using\n                # the old template needs to be reset.\n                self._last_match_position = 0\n                return True\n            else:\n                # Remove the current number format from _possible_formats\n                del self._possible_formats[ii]\n                ii -= 1\n            ii += 1\n        self._able_to_format = False\n        return False", "response": "Returns True if a new template is created as opposed to reusing the existing template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a formatting template which can be used to efficiently format a partial number where digits are added one by one.", "response": "def _get_formatting_template(self, number_pattern, number_format):\n        \"\"\"Gets a formatting template which can be used to efficiently\n        format a partial number where digits are added one by one.\"\"\"\n        # Create a phone number consisting only of the digit 9 that matches the\n        # number_pattern by applying the pattern to the longest_phone_number string.\n        longest_phone_number = unicod(\"999999999999999\")\n        number_re = re.compile(number_pattern)\n        m = number_re.search(longest_phone_number)  # this will always succeed\n        a_phone_number = m.group(0)\n        # No formatting template can be created if the number of digits\n        # entered so far is longer than the maximum the current formatting\n        # rule can accommodate.\n        if len(a_phone_number) < len(self._national_number):\n            return U_EMPTY_STRING\n        # Formats the number according to number_format\n        template = re.sub(number_pattern, number_format, a_phone_number)\n        # Replaces each digit with character _DIGIT_PLACEHOLDER\n        template = re.sub(\"9\", _DIGIT_PLACEHOLDER, template)\n        return template"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clear(self):\n        self._current_output = U_EMPTY_STRING\n        self._accrued_input = U_EMPTY_STRING\n        self._accrued_input_without_formatting = U_EMPTY_STRING\n        self._formatting_template = U_EMPTY_STRING\n        self._last_match_position = 0\n\n        # The pattern from number_format that is currently used to create\n        # formatting_template.\n        self._current_formatting_pattern = U_EMPTY_STRING\n        # This contains anything that has been entered so far preceding the\n        # national significant number, and it is formatted (e.g. with space\n        # inserted). For example, this can contain IDD, country code, and/or\n        # NDD, etc.\n        self._prefix_before_national_number = U_EMPTY_STRING\n        self._should_add_space_after_national_prefix = False\n        # This contains the national prefix that has been extracted. It\n        # contains only digits without formatting.\n        self._extracted_national_prefix = U_EMPTY_STRING\n        self._national_number = U_EMPTY_STRING\n        # This indicates whether AsYouTypeFormatter is currently doing the\n        # formatting.\n        self._able_to_format = True\n        # Set to true when users enter their own\n        # formatting. AsYouTypeFormatter will do no formatting at all when\n        # this is set to True.\n        self._input_has_formatting = False\n        # The position of a digit upon which input_digit(remember_position=True) is\n        # most recently invoked, as found in accrued_input_without_formatting.\n        self._position_to_remember = 0\n        # The position of a digit upon which input_digit(remember_position=True) is\n        # most recently invoked, as found in the original sequence of\n        # characters the user entered.\n        self._original_position = 0\n        # This is set to true when we know the user is entering a full\n        # national significant number, since we have either detected a\n        # national prefix or an international dialing prefix. When this is\n        # true, we will no longer use local number formatting patterns.\n        self._is_complete_number = False\n        self._is_expecting_country_calling_code = False\n        self._possible_formats = []", "response": "Clears the internal state of the formatter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self):\n        self._clear()\n        if self._current_metadata != self._default_metadata:\n            self._current_metadata = _get_metadata_for_region(self._default_country)", "response": "Clears the internal state of the formatter so it can be reused."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats a phone number as a digit.", "response": "def input_digit(self, next_char, remember_position=False):\n        \"\"\"Formats a phone number on-the-fly as each digit is entered.\n\n        If remember_position is set, remembers the position where next_char is\n        inserted, so that it can be retrieved later by using\n        get_remembered_position. The remembered position will be automatically\n        adjusted if additional formatting characters are later\n        inserted/removed in front of next_char.\n\n        Arguments:\n\n        next_char -- The most recently entered digit of a phone\n              number. Formatting characters are allowed, but as soon as they\n              are encountered this method formats the number as entered and\n              not \"as you type\" anymore. Full width digits and Arabic-indic\n              digits are allowed, and will be shown as they are.\n        remember_position -- Whether to track the position where next_char is\n              inserted.\n\n        Returns the partially formatted phone number.\n        \"\"\"\n        self._accrued_input += next_char\n        if remember_position:\n            self._original_position = len(self._accrued_input)\n        # We do formatting on-the-fly only when each character entered is\n        # either a digit, or a plus sign (accepted at the start of the number\n        # only).\n        if not self._is_digit_or_leading_plus_sign(next_char):\n            self._able_to_format = False\n            self._input_has_formatting = True\n        else:\n            next_char = self._normalize_and_accrue_digits_and_plus_sign(next_char, remember_position)\n        if not self._able_to_format:\n            # When we are unable to format because of reasons other than that\n            # formatting chars have been entered, it can be due to really long\n            # IDDs or NDDs. If that is the case, we might be able to do\n            # formatting again after extracting them.\n            if self._input_has_formatting:\n                self._current_output = self._accrued_input\n                return self._current_output\n            elif self._attempt_to_extract_idd():\n                if self._attempt_to_extract_ccc():\n                    self._current_output = self._attempt_to_choose_pattern_with_prefix_extracted()\n                    return self._current_output\n            elif self._able_to_extract_longer_ndd():\n                # Add an additional space to separate long NDD and national\n                # significant number for readability. We don't set\n                # should_add_space_after_national_prefix to True, since we don't\n                # want this to change later when we choose formatting\n                # templates.\n                self._prefix_before_national_number += _SEPARATOR_BEFORE_NATIONAL_NUMBER\n                self._current_output = self._attempt_to_choose_pattern_with_prefix_extracted()\n                return self._current_output\n\n            self._current_output = self._accrued_input\n            return self._current_output\n\n        # We start to attempt to format only when at least\n        # MIN_LEADING_DIGITS_LENGTH digits (the plus sign is counted as a\n        # digit as well for this purpose) have been entered.\n        len_input = len(self._accrued_input_without_formatting)\n        if len_input >= 0 and len_input <= 2:\n            self._current_output = self._accrued_input\n            return self._current_output\n        elif len_input == 3:\n            if self._attempt_to_extract_idd():\n                self._is_expecting_country_calling_code = True\n            else:\n                # No IDD or plus sign is found, might be entering in national format.\n                self._extracted_national_prefix = self._remove_national_prefix_from_national_number()\n                self._current_output = self._attempt_to_choose_formatting_pattern()\n                return self._current_output\n        if self._is_expecting_country_calling_code:\n            if self._attempt_to_extract_ccc():\n                self._is_expecting_country_calling_code = False\n            self._current_output = self._prefix_before_national_number + self._national_number\n            return self._current_output\n\n        if len(self._possible_formats) > 0:  # The formatting patterns are already chosen.\n            temp_national_number = self._input_digit_helper(next_char)\n            # See if the accrued digits can be formatted properly already. If\n            # not, use the results from input_digit_helper, which does\n            # formatting based on the formatting pattern chosen.\n            formatted_number = self._attempt_to_format_accrued_digits()\n            if len(formatted_number) > 0:\n                self._current_output = formatted_number\n                return self._current_output\n            self._narrow_down_possible_formats(self._national_number)\n            if self._maybe_create_new_template():\n                self._current_output = self._input_accrued_national_number()\n                return self._current_output\n            if self._able_to_format:\n                self._current_output = self._append_national_number(temp_national_number)\n                return self._current_output\n            else:\n                self._current_output = self._accrued_input\n                return self._current_output\n        else:\n            self._current_output = self._attempt_to_choose_formatting_pattern()\n            return self._current_output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck to see if there is an exact pattern match for these digits and if so attempts to format them. If not attempts to append the national number to the current locale.", "response": "def _attempt_to_format_accrued_digits(self):\n        \"\"\"Checks to see if there is an exact pattern match for these digits. If so, we should use this\n        instead of any other formatting template whose leadingDigitsPattern also matches the input.\n        \"\"\"\n        for number_format in self._possible_formats:\n            num_re = re.compile(number_format.pattern)\n            if fullmatch(num_re, self._national_number):\n                if number_format.national_prefix_formatting_rule is None:\n                    self._should_add_space_after_national_prefix = False\n                else:\n                    self._should_add_space_after_national_prefix = bool(_NATIONAL_PREFIX_SEPARATORS_PATTERN.search(number_format.national_prefix_formatting_rule))\n                formatted_number = re.sub(num_re, number_format.format, self._national_number)\n                return self._append_national_number(formatted_number)\n        return U_EMPTY_STRING"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_remembered_position(self):\n        if not self._able_to_format:\n            return self._original_position\n        accrued_input_index = 0\n        current_output_index = 0\n        while (accrued_input_index < self._position_to_remember and\n               current_output_index < len(self._current_output)):\n            if (self._accrued_input_without_formatting[accrued_input_index] ==\n                self._current_output[current_output_index]):\n                accrued_input_index += 1\n            current_output_index += 1\n        return current_output_index", "response": "Returns the current position in the partially formatted phone number of the character which was previously passed in as the\n        parameter of input_digit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _append_national_number(self, national_number):\n        prefix_before_nn_len = len(self._prefix_before_national_number)\n        if (self._should_add_space_after_national_prefix and prefix_before_nn_len > 0 and\n            self._prefix_before_national_number[-1] != _SEPARATOR_BEFORE_NATIONAL_NUMBER):\n            # We want to add a space after the national prefix if the national\n            # prefix formatting rule indicates that this would normally be\n            # done, with the exception of the case where we already appended a\n            # space because the NDD was surprisingly long.\n            return self._prefix_before_national_number + _SEPARATOR_BEFORE_NATIONAL_NUMBER + national_number\n        else:\n            return self._prefix_before_national_number + national_number", "response": "Appends the national number to the national number list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattempts to set the formatting template and returns a string which contains the formatted version of the digits entered so far.", "response": "def _attempt_to_choose_formatting_pattern(self):\n        \"\"\"Attempts to set the formatting template and returns a string which\n        contains the formatted version of the digits entered so far.\"\"\"\n        # We start to attempt to format only when at least MIN_LEADING_DIGITS_LENGTH digits of national\n        # number (excluding national prefix) have been entered.\n        if len(self._national_number) >= _MIN_LEADING_DIGITS_LENGTH:\n            self._get_available_formats(self._national_number)\n            # See if the accrued digits can be formatted properly already.\n            formatted_number = self._attempt_to_format_accrued_digits()\n            if len(formatted_number) > 0:\n                return formatted_number\n            if self._maybe_create_new_template():\n                return self._input_accrued_national_number()\n            else:\n                return self._accrued_input\n        else:\n            return self._append_national_number(self._national_number)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _input_accrued_national_number(self):\n        length_of_national_number = len(self._national_number)\n        if length_of_national_number > 0:\n            temp_national_number = U_EMPTY_STRING\n            for ii in range(length_of_national_number):\n                temp_national_number = self._input_digit_helper(self._national_number[ii])\n            if self._able_to_format:\n                return self._append_national_number(temp_national_number)\n            else:\n                return self._accrued_input\n        else:\n            return self._prefix_before_national_number", "response": "Invokes input_digit_helper on each digit of the national number\n            and returns a formatted string in the end."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns true if the current country is a NANPA country and the national number begins with the national prefix.", "response": "def _is_nanpa_number_with_national_prefix(self):\n        \"\"\"Returns true if the current country is a NANPA country and the\n        national number begins with the national prefix.\n        \"\"\"\n        # For NANPA numbers beginning with 1[2-9], treat the 1 as the national\n        # prefix. The reason is that national significant numbers in NANPA\n        # always start with [2-9] after the national prefix.  Numbers\n        # beginning with 1[01] can only be short/emergency numbers, which\n        # don't need the national prefix.\n        return (self._current_metadata.country_code == 1 and self._national_number[0] == '1' and\n                self._national_number[1] != '0' and self._national_number[1] != '1')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _attempt_to_extract_idd(self):\n        international_prefix = re.compile(unicod(\"\\\\\") + _PLUS_SIGN + unicod(\"|\") +\n                                          (self._current_metadata.international_prefix or U_EMPTY_STRING))\n        idd_match = international_prefix.match(self._accrued_input_without_formatting)\n        if idd_match:\n            self._is_complete_number = True\n            start_of_country_calling_code = idd_match.end()\n            self._national_number = self._accrued_input_without_formatting[start_of_country_calling_code:]\n            self._prefix_before_national_number = self._accrued_input_without_formatting[:start_of_country_calling_code]\n            if self._accrued_input_without_formatting[0] != _PLUS_SIGN:\n                self._prefix_before_national_number += _SEPARATOR_BEFORE_NATIONAL_NUMBER\n            return True\n        return False", "response": "Extracts IDD and plus sign from self. _national_number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the country calling code from the beginning of the _national_number to _prefix_before_national_number when they are available and places the remaining input into _national_number. Returns True when the country calling code can be found.", "response": "def _attempt_to_extract_ccc(self):\n        \"\"\"Extracts the country calling code from the beginning of\n        _national_number to _prefix_before_national_number when they are\n        available, and places the remaining input into _national_number.\n\n        Returns True when a valid country calling code can be found.\n        \"\"\"\n        if len(self._national_number) == 0:\n            return False\n\n        country_code, number_without_ccc = _extract_country_code(self._national_number)\n        if country_code == 0:\n            return False\n\n        self._national_number = number_without_ccc\n        new_region_code = region_code_for_country_code(country_code)\n        if new_region_code == REGION_CODE_FOR_NON_GEO_ENTITY:\n            self._current_metadata = PhoneMetadata.metadata_for_nongeo_region(country_code)\n        elif new_region_code != self._default_country:\n            self._current_metadata = _get_metadata_for_region(new_region_code)\n\n        self._prefix_before_national_number += str(country_code)\n        self._prefix_before_national_number += _SEPARATOR_BEFORE_NATIONAL_NUMBER\n        # When we have successfully extracted the IDD, the previously\n        # extracted NDD should be cleared because it is no longer valid.\n        self._extracted_national_prefix = U_EMPTY_STRING\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalize_and_accrue_digits_and_plus_sign(self, next_char, remember_position):\n        if next_char == _PLUS_SIGN:\n            normalized_char = next_char\n            self._accrued_input_without_formatting += next_char\n        else:\n            next_digit = unicode_digit(next_char, -1)\n            if next_digit != -1:\n                normalized_char = unicod(next_digit)\n            else:  # pragma no cover\n                normalized_char = next_char\n            self._accrued_input_without_formatting += normalized_char\n            self._national_number += normalized_char\n        if remember_position:\n            self._position_to_remember = len(self._accrued_input_without_formatting)\n        return normalized_char", "response": "Accrues digits and the plus sign to\n        _accrued_input_without_formatting for later use."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the customary display name for the given phone number object.", "response": "def country_name_for_number(numobj, lang, script=None, region=None):\n    \"\"\"Returns the customary display name in the given langauge for the given\n    territory the given PhoneNumber object is from.  If it could be from many\n    territories, nothing is returned.\n\n    Arguments:\n    numobj -- The PhoneNumber object for which we want to get a text description.\n    lang -- A 2-letter lowercase ISO 639-1 language code for the language in\n                  which the description should be returned (e.g. \"en\")\n    script -- A 4-letter titlecase (first letter uppercase, rest lowercase)\n                  ISO script code as defined in ISO 15924, separated by an\n                  underscore (e.g. \"Hant\")\n    region --  A 2-letter uppercase ISO 3166-1 country code (e.g. \"GB\")\n\n    The script and region parameters are currently ignored.\n\n    Returns a text description in the given language code, for the given phone\n    number's region, or an empty string if no description is available.\"\"\"\n    region_codes = region_codes_for_country_code(numobj.country_code)\n    if len(region_codes) == 1:\n        return _region_display_name(region_codes[0], lang, script, region)\n    else:\n        region_where_number_is_valid = u(\"ZZ\")\n        for region_code in region_codes:\n            if is_valid_number_for_region(numobj, region_code):\n                # If the number has already been found valid for one region,\n                # then we don't know which region it belongs to so we return\n                # nothing.\n                if region_where_number_is_valid != u(\"ZZ\"):\n                    return U_EMPTY_STRING\n                region_where_number_is_valid = region_code\n        return _region_display_name(region_where_number_is_valid, lang, script, region)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a text description of a valid phone number in the given language.", "response": "def description_for_valid_number(numobj, lang, script=None, region=None):\n    \"\"\"Return a text description of a PhoneNumber object, in the language\n    provided.\n\n    The description might consist of the name of the country where the phone\n    number is from and/or the name of the geographical area the phone number\n    is from if more detailed information is available.\n\n    If the phone number is from the same region as the user, only a\n    lower-level description will be returned, if one exists. Otherwise, the\n    phone number's region will be returned, with optionally some more detailed\n    information.\n\n    For example, for a user from the region \"US\" (United States), we would\n    show \"Mountain View, CA\" for a particular number, omitting the United\n    States from the description. For a user from the United Kingdom (region\n    \"GB\"), for the same number we may show \"Mountain View, CA, United States\"\n    or even just \"United States\".\n\n    This function assumes the validity of the number passed in has already\n    been checked, and that the number is suitable for geocoding.  We consider\n    fixed-line and mobile numbers possible candidates for geocoding.\n\n    Arguments:\n    numobj -- A valid PhoneNumber object for which we want to get a text\n                  description.\n    lang -- A 2-letter lowercase ISO 639-1 language code for the language in\n                  which the description should be returned (e.g. \"en\")\n    script -- A 4-letter titlecase (first letter uppercase, rest lowercase)\n                  ISO script code as defined in ISO 15924, separated by an\n                  underscore (e.g. \"Hant\")\n    region -- The region code for a given user. This region will be omitted\n                  from the description if the phone number comes from this\n                  region. It should be a two-letter upper-case CLDR region\n                  code.\n\n    Returns a text description in the given language code, for the given phone\n    number, or an empty string if the number could come from multiple countries,\n    or the country code is in fact invalid.\"\"\"\n    number_region = region_code_for_number(numobj)\n    if region is None or region == number_region:\n        mobile_token = country_mobile_token(numobj.country_code)\n        national_number = national_significant_number(numobj)\n        if mobile_token != U_EMPTY_STRING and national_number.startswith(mobile_token):\n            # In some countries, eg. Argentina, mobile numbers have a mobile token\n            # before the national destination code, this should be removed before\n            # geocoding.\n            national_number = national_number[len(mobile_token):]\n\n            region = region_code_for_country_code(numobj.country_code)\n            try:\n                copied_numobj = parse(national_number, region)\n            except NumberParseException:\n                # If this happens, just re-use what we had.\n                copied_numobj = numobj\n            area_description = _prefix_description_for_number(GEOCODE_DATA, GEOCODE_LONGEST_PREFIX,\n                                                              copied_numobj, lang, script, region)\n        else:\n            area_description = _prefix_description_for_number(GEOCODE_DATA, GEOCODE_LONGEST_PREFIX,\n                                                              numobj, lang, script, region)\n        if area_description != \"\":\n            return area_description\n        else:\n            # Fall back to the description of the number's region\n            return country_name_for_number(numobj, lang, script, region)\n    else:\n        # Otherwise, we just show the region(country) name for now.\n        return _region_display_name(number_region, lang, script, region)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef description_for_number(numobj, lang, script=None, region=None):\n    ntype = number_type(numobj)\n    if ntype == PhoneNumberType.UNKNOWN:\n        return \"\"\n    elif not is_number_type_geographical(ntype, numobj.country_code):\n        return country_name_for_number(numobj, lang, script, region)\n    return description_for_valid_number(numobj, lang, script, region)", "response": "Returns a text description of a PhoneNumber object for the given language."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_lang(langdict, lang, script, region):\n    # Check if we should map this to a different locale.\n    full_locale = _full_locale(lang, script, region)\n    if (full_locale in _LOCALE_NORMALIZATION_MAP and\n        _LOCALE_NORMALIZATION_MAP[full_locale] in langdict):\n        return langdict[_LOCALE_NORMALIZATION_MAP[full_locale]]\n    # First look for the full locale\n    if full_locale in langdict:\n        return langdict[full_locale]\n    # Then look for lang, script as a combination\n    if script is not None:\n        lang_script = \"%s_%s\" % (lang, script)\n        if lang_script in langdict:\n            return langdict[lang_script]\n    # Next look for lang, region as a combination\n    if region is not None:\n        lang_region = \"%s_%s\" % (lang, region)\n        if lang_region in langdict:\n            return langdict[lang_region]\n    # Fall back to bare language code lookup\n    if lang in langdict:\n        return langdict[lang]\n    # Possibly fall back to english\n    if _may_fall_back_to_english(lang):\n        return langdict.get(\"en\", None)\n    else:\n        return None", "response": "Return the entry in the dictionary for the given language information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a text description of a phone number in a given language.", "response": "def _prefix_description_for_number(data, longest_prefix, numobj, lang, script=None, region=None):\n    \"\"\"Return a text description of a PhoneNumber for the given language.\n\n    Arguments:\n    data -- Prefix dictionary to lookup up number in.\n    longest_prefix -- Length of the longest key in data.\n    numobj -- The PhoneNumber object for which we want to get a text description.\n    lang -- A 2-letter lowercase ISO 639-1 language code for the language in\n                  which the description should be returned (e.g. \"en\")\n    script -- A 4-letter titlecase (first letter uppercase, rest lowercase)\n                  ISO script code as defined in ISO 15924, separated by an\n                  underscore (e.g. \"Hant\")\n    region --  A 2-letter uppercase ISO 3166-1 country code (e.g. \"GB\")\n\n    Returns a text description in the given language code, for the given phone\n    number's area, or an empty string if no description is available.\"\"\"\n    e164_num = format_number(numobj, PhoneNumberFormat.E164)\n    if not e164_num.startswith(U_PLUS):  # pragma no cover\n        # Can only hit this arm if there's an internal error in the rest of\n        # the library\n        raise Exception(\"Expect E164 number to start with +\")\n    for prefix_len in range(longest_prefix, 0, -1):\n        prefix = e164_num[1:(1 + prefix_len)]\n        if prefix in data:\n            # This prefix is present in the geocoding data, as a dictionary\n            # mapping language info to location name.\n            name = _find_lang(data[prefix], lang, script, region)\n            if name is not None:\n                return name\n            else:\n                return U_EMPTY_STRING\n    return U_EMPTY_STRING"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging information from another NumberFormat object into this one.", "response": "def merge_from(self, other):\n        \"\"\"Merge information from another NumberFormat object into this one.\"\"\"\n        if other.pattern is not None:\n            self.pattern = other.pattern\n        if other.format is not None:\n            self.format = other.format\n        self.leading_digits_pattern.extend(other.leading_digits_pattern)\n        if other.national_prefix_formatting_rule is not None:\n            self.national_prefix_formatting_rule = other.national_prefix_formatting_rule\n        if other.national_prefix_optional_when_formatting is not None:\n            self.national_prefix_optional_when_formatting = other.national_prefix_optional_when_formatting\n        if other.domestic_carrier_code_formatting_rule is not None:\n            self.domestic_carrier_code_formatting_rule = other.domestic_carrier_code_formatting_rule"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_from(self, other):\n        if other.national_number_pattern is not None:\n            self.national_number_pattern = other.national_number_pattern\n        if other.example_number is not None:\n            self.example_number = other.example_number", "response": "Merge information from another PhoneNumberDesc object into this one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforcing immediate load of all metadata", "response": "def load_all(kls):\n        \"\"\"Force immediate load of all metadata\"\"\"\n        # Force expansion of contents to lists because we invalidate the iterator\n        for region_code, loader in list(kls._region_available.items()):\n            if loader is not None:  # pragma no cover\n                loader(region_code)\n                kls._region_available[region_code] = None\n        for country_code, loader in list(kls._country_code_available.items()):\n            if loader is not None:\n                loader(country_code)\n                kls._country_code_available[region_code] = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef PBToPy(numpb):\n    return PhoneNumber(country_code=numpb.country_code if numpb.HasField(\"country_code\") else None,\n                       national_number=numpb.national_number if numpb.HasField(\"national_number\") else None,\n                       extension=numpb.extension if numpb.HasField(\"extension\") else None,\n                       italian_leading_zero=numpb.italian_leading_zero if numpb.HasField(\"italian_leading_zero\") else None,\n                       number_of_leading_zeros=numpb.number_of_leading_zeros if numpb.HasField(\"number_of_leading_zeros\") else None,\n                       raw_input=numpb.raw_input if numpb.HasField(\"raw_input\") else None,\n                       country_code_source=numpb.country_code_source if numpb.HasField(\"country_code_source\") else CountryCodeSource.UNSPECIFIED,\n                       preferred_domestic_carrier_code=numpb.preferred_domestic_carrier_code if numpb.HasField(\"preferred_domestic_carrier_code\") else None)", "response": "Convert phonenumber_pb2. PhoneNumber to phonenumber. PhoneNumber"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts phonenumber. PhoneNumber to phonenumber_pb2. PhoneNumber", "response": "def PyToPB(numobj):\n    \"\"\"Convert phonenumber.PhoneNumber to phonenumber_pb2.PhoneNumber\"\"\"\n    numpb = PhoneNumberPB()\n    if numobj.country_code is not None:\n        numpb.country_code = numobj.country_code\n    if numobj.national_number is not None:\n        numpb.national_number = numobj.national_number\n    if numobj.extension is not None:\n        numpb.extension = numobj.extension\n    if numobj.italian_leading_zero is not None:\n        numpb.italian_leading_zero = numobj.italian_leading_zero\n    if numobj.number_of_leading_zeros is not None:\n        numpb.number_of_leading_zeros = numobj.number_of_leading_zeros\n    if numobj.raw_input is not None:\n        numpb.raw_input = numobj.raw_input\n    numpb.country_code_source = numobj.country_code_source\n    if numobj.preferred_domestic_carrier_code is not None:\n        numpb.preferred_domestic_carrier_code = numobj.preferred_domestic_carrier_code\n    return numpb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_unique_child(xtag, eltname):\n    try:\n        results = xtag.findall(eltname)\n        if len(results) > 1:\n            raise Exception(\"Multiple elements found where 0/1 expected\")\n        elif len(results) == 1:\n            return results[0]\n        else:\n            return None\n    except Exception:\n        return None", "response": "Get the unique child element under xtag with name eltname"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_unique_child_value(xtag, eltname):\n    xelt = _get_unique_child(xtag, eltname)\n    if xelt is None:\n        return None\n    else:\n        return xelt.text", "response": "Get the text content of the unique child element under xtag with name eltname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract list of possible lengths from string", "response": "def _extract_lengths(ll):\n    \"\"\"Extract list of possible lengths from string\"\"\"\n    results = set()\n    if ll is None:\n        return []\n    for val in ll.split(','):\n        m = _NUM_RE.match(val)\n        if m:\n            results.add(int(val))\n        else:\n            m = _RANGE_RE.match(val)\n            if m is None:\n                raise Exception(\"Unrecognized length specification %s\" % ll)\n            min = int(m.group('min'))\n            max = int(m.group('max'))\n            for ii in range(min, max + 1):\n                results.add(ii)\n    return sorted(list(results))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _expand_formatting_rule(rule, national_prefix):\n    if rule is None:\n        return None\n    if national_prefix is None:\n        national_prefix = u(\"\")\n    rule = re.sub(u(r\"\\$NP\"), national_prefix, rule)\n    rule = re.sub(u(r\"\\$FG\"), u(\"$1\"), rule)\n    return rule", "response": "Expand formatting rules to include terms NP and FG."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the given XML file and emit generated code.", "response": "def _standalone(argv):\n    \"\"\"Parse the given XML file and emit generated code.\"\"\"\n    alternate = None\n    short_data = False\n    try:\n        opts, args = getopt.getopt(argv, \"hlsa:\", (\"help\", \"lax\", \"short\", \"alt=\"))\n    except getopt.GetoptError:\n        prnt(__doc__, file=sys.stderr)\n        sys.exit(1)\n    for opt, arg in opts:\n        if opt in (\"-h\", \"--help\"):\n            prnt(__doc__, file=sys.stderr)\n            sys.exit(1)\n        elif opt in (\"-s\", \"--short\"):\n            short_data = True\n        elif opt in (\"-l\", \"--lax\"):\n            global lax\n            lax = True\n        elif opt in (\"-a\", \"--alt\"):\n            alternate = arg\n        else:\n            prnt(\"Unknown option %s\" % opt, file=sys.stderr)\n            prnt(__doc__, file=sys.stderr)\n            sys.exit(1)\n\n    if len(args) != 3:\n        prnt(__doc__, file=sys.stderr)\n        sys.exit(1)\n    pmd = XPhoneNumberMetadata(args[0], short_data)\n    if alternate is not None:\n        pmd.add_alternate_formats(alternate)\n    pmd.emit_metadata_py(args[1], args[2])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding phone number alternate format metadata retrieved from XML file.", "response": "def add_alternate_formats(self, filename):\n        \"\"\"Add phone number alternate format metadata retrieved from XML\"\"\"\n        with open(filename, \"r\") as infile:\n            xtree = etree.parse(infile)\n        self.alt_territory = {}  # country_code to XAlternateTerritory\n        xterritories = xtree.find(TOP_XPATH)\n        for xterritory in xterritories:\n            if xterritory.tag == TERRITORY_TAG:\n                terrobj = XAlternateTerritory(xterritory)\n                id = str(terrobj.country_code)\n                if id in self.alt_territory:\n                    raise Exception(\"Duplicate entry for %s\" % id)\n                self.alt_territory[id] = terrobj\n            else:\n                raise Exception(\"Unexpected element %s found\" % xterritory.tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef emit_metadata_for_region_py(self, region, region_filename, module_prefix):\n        terrobj = self.territory[region]\n        with open(region_filename, \"w\") as outfile:\n            prnt(_REGION_METADATA_PROLOG % {'region': terrobj.identifier(), 'module': module_prefix}, file=outfile)\n            prnt(\"PHONE_METADATA_%s = %s\" % (terrobj.identifier(), terrobj), file=outfile)", "response": "Emit the metadata for the given region in the given Python file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nemit the alternate format metadata for the given country code.", "response": "def emit_alt_formats_for_cc_py(self, cc, cc_filename, module_prefix):\n        \"\"\"Emit Python code generating the alternate format metadata for the given country code\"\"\"\n        terrobj = self.alt_territory[cc]\n        with open(cc_filename, \"w\") as outfile:\n            prnt(_ALT_FORMAT_METADATA_PROLOG % (cc, module_prefix), file=outfile)\n            prnt(\"PHONE_ALT_FORMAT_%s = %s\" % (cc, terrobj), file=outfile)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nemits the metadata for the phone number and all of its territory.", "response": "def emit_metadata_py(self, datadir, module_prefix):\n        \"\"\"Emit Python code for the phone number metadata to the given file, and\n        to a data/ subdirectory in the same directory as that file.\"\"\"\n\n        if not os.path.isdir(datadir):\n            os.mkdir(datadir)\n        modulefilename = os.path.join(datadir, '__init__.py')\n\n        # First, generate all of the individual per-region files in that directory\n        for country_id in sorted(self.territory.keys()):\n            filename = os.path.join(datadir, \"region_%s.py\" % country_id)\n            self.emit_metadata_for_region_py(country_id, filename, module_prefix)\n\n        # Same for any per-country-code alternate format files\n        if self.alt_territory is not None:\n            for country_code in sorted(self.alt_territory.keys()):\n                filename = os.path.join(datadir, \"alt_format_%s.py\" % country_code)\n                self.emit_alt_formats_for_cc_py(country_code, filename, module_prefix)\n\n        # Now build a module file that includes them all\n        with open(modulefilename, \"w\") as outfile:\n            prnt(METADATA_FILE_PROLOG, file=outfile)\n            prnt(COPYRIGHT_NOTICE, file=outfile)\n            prnt(METADATA_FILE_IMPORT % {'module': module_prefix}, file=outfile)\n            nongeo_codes = []\n            country_codes = []\n            for country_id in sorted(self.territory.keys()):\n                terrobj = self.territory[country_id]\n                if terrobj.o.id == REGION_CODE_FOR_NON_GEO_ENTITY:\n                    nongeo_codes.append(country_id)  # int\n                else:\n                    country_codes.append(\"'%s'\" % country_id)  # quoted string\n            prnt(\"_AVAILABLE_REGION_CODES = [%s]\" % \",\".join(country_codes), file=outfile)\n            if len(nongeo_codes) > 0:\n                prnt(\"_AVAILABLE_NONGEO_COUNTRY_CODES = [%s]\" % \", \".join(nongeo_codes), file=outfile)\n            register_prefix = \"short_\" if self.short_data else \"\"\n            prnt(METADATA_FILE_LOOP % {'prefix': register_prefix}, file=outfile)\n            if len(nongeo_codes) > 0:\n                prnt(METADATA_NONGEO_FILE_LOOP, file=outfile)\n\n            if self.alt_territory is not None:\n                for country_code in sorted(self.alt_territory.keys()):\n                    prnt(\"from .alt_format_%s import PHONE_ALT_FORMAT_%s\" % (country_code, country_code), file=outfile)\n                prnt(\"_ALT_NUMBER_FORMATS = {%s}\" %\n                     \", \".join([\"%s: PHONE_ALT_FORMAT_%s\" % (cc, cc) for cc in sorted(self.alt_territory.keys())]),\n                     file=outfile)\n\n            # Build up a map from country code (int) to list of region codes (ISO 3166-1 alpha 2)\n            country_code_to_region_code = {}\n            for country_id in sorted(self.territory.keys()):\n                terrobj = self.territory[country_id]\n                if terrobj.o.country_code is not None:\n                    country_code = int(terrobj.o.country_code)\n                    if country_code not in country_code_to_region_code:\n                        country_code_to_region_code[country_code] = []\n                    if terrobj.o.main_country_for_code:\n                        country_code_to_region_code[country_code].insert(0, terrobj.o.id)\n                    else:\n                        country_code_to_region_code[country_code].append(terrobj.o.id)\n\n            # Emit the mapping from country code to region code if nonempty.\n            if len(country_code_to_region_code.keys()) > 0:\n                prnt(_COUNTRY_CODE_TO_REGION_CODE_PROLOG, file=outfile)\n                prnt(\"_COUNTRY_CODE_TO_REGION_CODE = {\", file=outfile)\n                for country_code in sorted(country_code_to_region_code.keys()):\n                    country_ids = country_code_to_region_code[country_code]\n                    prnt('    %d: (\"%s\",),' % (country_code, '\", \"'.join(country_ids)), file=outfile)\n                prnt(\"}\", file=outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a mutable copy of the given NumberFormat object", "response": "def _copy_number_format(other):\n    \"\"\"Return a mutable copy of the given NumberFormat object\"\"\"\n    copy = NumberFormat(pattern=other.pattern,\n                        format=other.format,\n                        leading_digits_pattern=list(other.leading_digits_pattern),\n                        national_prefix_formatting_rule=other.national_prefix_formatting_rule,\n                        national_prefix_optional_when_formatting=other.national_prefix_optional_when_formatting,\n                        domestic_carrier_code_formatting_rule=other.domestic_carrier_code_formatting_rule)\n    copy._mutable = True\n    return copy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to extract a possible number from the string passed in.", "response": "def _extract_possible_number(number):\n    \"\"\"Attempt to extract a possible number from the string passed in.\n\n    This currently strips all leading characters that cannot be used to\n    start a phone number. Characters that can be used to start a phone number\n    are defined in the VALID_START_CHAR_PATTERN. If none of these characters\n    are found in the number passed in, an empty string is returned. This\n    function also attempts to strip off any alternative extensions or endings\n    if two or more are present, such as in the case of: (530) 583-6985\n    x302/x2303. The second extension here makes this actually two phone\n    numbers, (530) 583-6985 x302 and (530) 583-6985 x2303. We remove the\n    second extension so that the first number is parsed correctly.\n\n    Arguments:\n    number -- The string that might contain a phone number.\n\n    Returns the number, stripped of any non-phone-number prefix (such\n    as \"Tel:\") or an empty string if no character used to start phone\n    numbers (such as + or any digit) is found in the number\n    \"\"\"\n    match = _VALID_START_CHAR_PATTERN.search(number)\n    if match:\n        number = number[match.start():]\n        # Remove trailing non-alpha non-numberical characters.\n        trailing_chars_match = _UNWANTED_END_CHAR_PATTERN.search(number)\n        if trailing_chars_match:\n            number = number[:trailing_chars_match.start()]\n        # Check for extra numbers at the end.\n        second_number_match = _SECOND_NUMBER_START_PATTERN.search(number)\n        if second_number_match:\n            number = number[:second_number_match.start()]\n        return number\n    else:\n        return U_EMPTY_STRING"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking to see if a string could possibly be a phone number.", "response": "def _is_viable_phone_number(number):\n    \"\"\"Checks to see if a string could possibly be a phone number.\n\n    At the moment, checks to see that the string begins with at least 2\n    digits, ignoring any punctuation commonly found in phone numbers.  This\n    method does not require the number to be normalized in advance - but does\n    assume that leading non-number symbols have been removed, such as by the\n    method _extract_possible_number.\n\n    Arguments:\n    number -- string to be checked for viability as a phone number\n\n    Returns True if the number could be a phone number of some sort, otherwise\n    False\n    \"\"\"\n    if len(number) < _MIN_LENGTH_FOR_NSN:\n        return False\n    match = fullmatch(_VALID_PHONE_NUMBER_PATTERN, number)\n    return bool(match)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _normalize(number):\n    m = fullmatch(_VALID_ALPHA_PHONE_PATTERN, number)\n    if m:\n        return _normalize_helper(number, _ALPHA_PHONE_MAPPINGS, True)\n    else:\n        return normalize_digits_only(number)", "response": "Normalizes a string of characters representing a phone number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes a string of digits only.", "response": "def normalize_digits_only(number, keep_non_digits=False):\n    \"\"\"Normalizes a string of characters representing a phone number.\n\n    This converts wide-ascii and arabic-indic numerals to European numerals,\n    and strips punctuation and alpha characters (optional).\n\n    Arguments:\n    number -- a string representing a phone number\n    keep_non_digits -- whether to keep non-digits\n\n    Returns the normalized string version of the phone number.\n    \"\"\"\n    number = unicod(number)\n    number_length = len(number)\n    normalized_digits = U_EMPTY_STRING\n    for ii in range(number_length):\n        d = unicode_digit(number[ii], -1)\n        if d != -1:\n            normalized_digits += unicod(d)\n        elif keep_non_digits:\n            normalized_digits += number[ii]\n    return normalized_digits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef length_of_geographical_area_code(numobj):\n    metadata = PhoneMetadata.metadata_for_region(region_code_for_number(numobj), None)\n    if metadata is None:\n        return 0\n\n    # If a country doesn't use a national prefix, and this number doesn't have\n    # an Italian leading zero, we assume it is a closed dialling plan with no\n    # area codes.\n    if metadata.national_prefix is None and not numobj.italian_leading_zero:\n        return 0\n\n    ntype = number_type(numobj)\n    country_code = numobj.country_code\n    if (ntype == PhoneNumberType.MOBILE and\n        (country_code in _GEO_MOBILE_COUNTRIES_WITHOUT_MOBILE_AREA_CODES)):\n        # Note this is a rough heuristic; it doesn't cover Indonesia well, for\n        # example, where area codes are present for some mobile phones but not\n        # for others. We have no better way of representing this in the\n        # metadata at this point.\n        return 0\n\n    if not is_number_type_geographical(ntype, country_code):\n        return 0\n\n    return length_of_national_destination_code(numobj)", "response": "Returns the length of the geographical area code for a number."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the length of the national destination code for a number.", "response": "def length_of_national_destination_code(numobj):\n    \"\"\"Return length of the national destination code code for a number.\n\n    Gets the length of the national destination code (NDC) from the\n    PhoneNumber object passed in, so that clients could use it to split a\n    national significant number into NDC and subscriber number. The NDC of a\n    phone number is normally the first group of digit(s) right after the\n    country calling code when the number is formatted in the international\n    format, if there is a subscriber number part that follows.\n\n    N.B.: similar to an area code, not all numbers have an NDC!\n\n    An example of how this could be used:\n\n    >>> import phonenumbers\n    >>> numobj = phonenumbers.parse(\"18002530000\", \"US\")\n    >>> nsn = phonenumbers.national_significant_number(numobj)\n    >>> ndc_len = phonenumbers.length_of_national_destination_code(numobj)\n    >>> if ndc_len > 0:\n    ...     national_destination_code = nsn[:ndc_len]\n    ...     subscriber_number = nsn[ndc_len:]\n    ... else:\n    ...     national_destination_code = \"\"\n    ...     subscriber_number = nsn\n\n    Refer to the unittests to see the difference between this function and\n    length_of_geographical_area_code.\n\n    Arguments:\n    numobj -- The PhoneNumber object to find the length of the NDC from.\n\n    Returns the length of NDC of the PhoneNumber object passed in, which\n    could be zero.\n    \"\"\"\n    if numobj.extension is not None:\n        # We don't want to alter the object given to us, but we don't want to\n        # include the extension when we format it, so we copy it and clear the\n        # extension here.\n        copied_numobj = PhoneNumber()\n        copied_numobj.merge_from(numobj)\n        copied_numobj.extension = None\n    else:\n        copied_numobj = numobj\n\n    nsn = format_number(copied_numobj, PhoneNumberFormat.INTERNATIONAL)\n    number_groups = re.split(NON_DIGITS_PATTERN, nsn)\n\n    # The pattern will start with \"+COUNTRY_CODE \" so the first group will\n    # always be the empty string (before the + symbol) and the second group\n    # will be the country calling code. The third group will be area code if\n    # it is not the last group.\n    if len(number_groups) <= 3:\n        return 0\n\n    if number_type(numobj) == PhoneNumberType.MOBILE:\n        # For example Argentinian mobile numbers, when formatted in the\n        # international format, are in the form of +54 9 NDC XXXX... As a\n        # result, we take the length of the third group (NDC) and add the\n        # length of the second group (which is the mobile token), which also\n        # forms part of the national significant number.  This assumes that\n        # the mobile token is always formatted separately from the rest of the\n        # phone number.\n        mobile_token = country_mobile_token(numobj.country_code)\n        if mobile_token != U_EMPTY_STRING:\n            return len(number_groups[2]) + len(number_groups[3])\n    return len(number_groups[2])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalize_helper(number, replacements, remove_non_matches):\n    normalized_number = []\n    for char in number:\n        new_digit = replacements.get(char.upper(), None)\n        if new_digit is not None:\n            normalized_number.append(new_digit)\n        elif not remove_non_matches:\n            normalized_number.append(char)\n        # If neither of the above are true, we remove this character\n    return U_EMPTY_STRING.join(normalized_number)", "response": "Normalizes a string of characters representing a phone number by replacing all found characters with the values\nIDIetable therein and stripping all other characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if there is any data set for a particular PhoneNumberDesc.", "response": "def _desc_has_data(desc):\n    \"\"\"Returns true if there is any data set for a particular PhoneNumberDesc.\"\"\"\n    if desc is None:\n        return False\n    # Checking most properties since we don't know what's present, since a custom build may have\n    # stripped just one of them (e.g. liteBuild strips exampleNumber). We don't bother checking the\n    # possibleLengthsLocalOnly, since if this is the only thing that's present we don't really\n    # support the type at all: no type-specific methods will work with only this data.\n    return ((desc.example_number is not None) or\n            _desc_has_possible_number_data(desc) or\n            (desc.national_number_pattern is not None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _supported_types_for_metadata(metadata):\n    numtypes = set()\n    for numtype in PhoneNumberType.values():\n        if numtype in (PhoneNumberType.FIXED_LINE_OR_MOBILE, PhoneNumberType.UNKNOWN):\n            # Never return FIXED_LINE_OR_MOBILE (it is a convenience type, and represents that a\n            # particular number type can't be determined) or UNKNOWN (the non-type).\n            continue\n        if _desc_has_data(_number_desc_by_type(metadata, numtype)):\n            numtypes.add(numtype)\n    return numtypes", "response": "Returns the types we have metadata for based on the object passed in which must be non - None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef supported_types_for_region(region_code):\n    if not _is_valid_region_code(region_code):\n        return set()\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper())\n    return _supported_types_for_metadata(metadata)", "response": "Returns the types of the phone number that the library has metadata for the given region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the types of a country - code belonging to a non - geographical entity.", "response": "def supported_types_for_non_geo_entity(country_code):\n    \"\"\"Returns the types for a country-code belonging to a non-geographical entity\n    which the library has metadata for. Will not include FIXED_LINE_OR_MOBILE\n    (if numbers for this non-geographical entity could be classified as\n    FIXED_LINE_OR_MOBILE, both FIXED_LINE and MOBILE would be present) and\n    UNKNOWN.\n\n    No types will be returned for country calling codes that do not map to a\n    known non-geographical entity.\n    \"\"\"\n    metadata = PhoneMetadata.metadata_for_nongeo_region(country_code, None)\n    if metadata is None:\n        return set()\n    return _supported_types_for_metadata(metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_number_type_geographical(num_type, country_code):\n    return (num_type == PhoneNumberType.FIXED_LINE or\n            num_type == PhoneNumberType.FIXED_LINE_OR_MOBILE or\n            ((country_code in _GEO_MOBILE_COUNTRIES) and\n             num_type == PhoneNumberType.MOBILE))", "response": "Tests whether a phone number has a geographical association."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats a phone number into the specified format using the default rules.", "response": "def format_number(numobj, num_format):\n    \"\"\"Formats a phone number in the specified format using default rules.\n\n    Note that this does not promise to produce a phone number that the user\n    can dial from where they are - although we do format in either 'national'\n    or 'international' format depending on what the client asks for, we do not\n    currently support a more abbreviated format, such as for users in the same\n    \"area\" who could potentially dial the number without area code. Note that\n    if the phone number has a country calling code of 0 or an otherwise\n    invalid country calling code, we cannot work out which formatting rules to\n    apply so we return the national significant number with no formatting\n    applied.\n\n    Arguments:\n    numobj -- The phone number to be formatted.\n    num_format -- The format the phone number should be formatted into\n\n    Returns the formatted phone number.\n    \"\"\"\n    if numobj.national_number == 0 and numobj.raw_input is not None:\n        # Unparseable numbers that kept their raw input just use that.  This\n        # is the only case where a number can be formatted as E164 without a\n        # leading '+' symbol (but the original number wasn't parseable\n        # anyway).\n        # TODO: Consider removing the 'if' above so that unparseable strings\n        # without raw input format to the empty string instead of \"+00\".\n        if len(numobj.raw_input) > 0:\n            return numobj.raw_input\n    country_calling_code = numobj.country_code\n    nsn = national_significant_number(numobj)\n    if num_format == PhoneNumberFormat.E164:\n        # Early exit for E164 case (even if the country calling code is\n        # invalid) since no formatting of the national number needs to be\n        # applied.  Extensions are not formatted.\n        return _prefix_number_with_country_calling_code(country_calling_code, num_format, nsn)\n    if not _has_valid_country_calling_code(country_calling_code):\n        return nsn\n    # Note region_code_for_country_code() is used because formatting\n    # information for regions which share a country calling code is contained\n    # by only one region for performance reasons. For example, for NANPA\n    # regions it will be contained in the metadata for US.\n    region_code = region_code_for_country_code(country_calling_code)\n    # Metadata cannot be None because the country calling code is valid (which\n    # means that the region code cannot be ZZ and must be one of our supported\n    # region codes).\n    metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_calling_code, region_code.upper())\n    formatted_number = _format_nsn(nsn, metadata, num_format)\n    formatted_number = _maybe_append_formatted_extension(numobj,\n                                                         metadata,\n                                                         num_format,\n                                                         formatted_number)\n    return _prefix_number_with_country_calling_code(country_calling_code,\n                                                    num_format,\n                                                    formatted_number)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats a phone number using client - defined formatting rules.", "response": "def format_by_pattern(numobj, number_format, user_defined_formats):\n    \"\"\"Formats a phone number using client-defined formatting rules.\"\n\n    Note that if the phone number has a country calling code of zero or an\n    otherwise invalid country calling code, we cannot work out things like\n    whether there should be a national prefix applied, or how to format\n    extensions, so we return the national significant number with no\n    formatting applied.\n\n    Arguments:\n    numobj -- The phone number to be formatted\n    num_format -- The format the phone number should be formatted into\n    user_defined_formats -- formatting rules specified by clients\n\n    Returns the formatted phone number.\n    \"\"\"\n    country_code = numobj.country_code\n    nsn = national_significant_number(numobj)\n    if not _has_valid_country_calling_code(country_code):\n        return nsn\n    # Note region_code_for_country_code() is used because formatting\n    # information for regions which share a country calling code is contained\n    # by only one region for performance reasons. For example, for NANPA\n    # regions it will be contained in the metadata for US.\n    region_code = region_code_for_country_code(country_code)\n    # Metadata cannot be None because the country calling code is valid.\n    metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code)\n\n    formatted_number = U_EMPTY_STRING\n    formatting_pattern = _choose_formatting_pattern_for_number(user_defined_formats, nsn)\n    if formatting_pattern is None:\n        # If no pattern above is matched, we format the number as a whole.\n        formatted_number = nsn\n    else:\n        num_format_copy = _copy_number_format(formatting_pattern)\n        # Before we do a replacement of the national prefix pattern $NP with\n        # the national prefix, we need to copy the rule so that subsequent\n        # replacements for different numbers have the appropriate national\n        # prefix.\n        np_formatting_rule = formatting_pattern.national_prefix_formatting_rule\n        if np_formatting_rule:\n            national_prefix = metadata.national_prefix\n            if national_prefix:\n                # Replace $NP with national prefix and $FG with the first\n                # group (\\1) matcher.\n                np_formatting_rule = np_formatting_rule.replace(_NP_STRING, national_prefix)\n                np_formatting_rule = np_formatting_rule.replace(_FG_STRING, unicod(\"\\\\1\"))\n                num_format_copy.national_prefix_formatting_rule = np_formatting_rule\n            else:\n                # We don't want to have a rule for how to format the national\n                # prefix if there isn't one.\n                num_format_copy.national_prefix_formatting_rule = None\n        formatted_number = _format_nsn_using_pattern(nsn, num_format_copy, number_format)\n    formatted_number = _maybe_append_formatted_extension(numobj,\n                                                         metadata,\n                                                         number_format,\n                                                         formatted_number)\n    formatted_number = _prefix_number_with_country_calling_code(country_code,\n                                                                number_format,\n                                                                formatted_number)\n    return formatted_number"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a number in national format for dialing using the specified carrier - code.", "response": "def format_national_number_with_carrier_code(numobj, carrier_code):\n    \"\"\"Format a number in national format for dialing using the specified carrier.\n\n    The carrier-code will always be used regardless of whether the phone\n    number already has a preferred domestic carrier code stored. If\n    carrier_code contains an empty string, returns the number in national\n    format without any carrier code.\n\n    Arguments:\n    numobj -- The phone number to be formatted\n    carrier_code -- The carrier selection code to be used\n\n    Returns the formatted phone number in national format for dialing using\n    the carrier as specified in the carrier_code.\n    \"\"\"\n    country_code = numobj.country_code\n    nsn = national_significant_number(numobj)\n    if not _has_valid_country_calling_code(country_code):\n        return nsn\n    # Note region_code_for_country_code() is used because formatting\n    # information for regions which share a country calling code is contained\n    # by only one region for performance reasons. For example, for NANPA\n    # regions it will be contained in the metadata for US.\n    region_code = region_code_for_country_code(country_code)\n    # Metadata cannot be None because the country calling code is valid\n    metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code)\n    formatted_number = _format_nsn(nsn,\n                                   metadata,\n                                   PhoneNumberFormat.NATIONAL,\n                                   carrier_code)\n    formatted_number = _maybe_append_formatted_extension(numobj,\n                                                         metadata,\n                                                         PhoneNumberFormat.NATIONAL,\n                                                         formatted_number)\n    formatted_number = _prefix_number_with_country_calling_code(country_code,\n                                                                PhoneNumberFormat.NATIONAL,\n                                                                formatted_number)\n    return formatted_number"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_national_number_with_preferred_carrier_code(numobj, fallback_carrier_code):\n    # Historically, we set this to an empty string when parsing with raw input\n    # if none was found in the input string. However, this doesn't result in a\n    # number we can dial. For this reason, we treat the empty string the same\n    # as if it isn't set at all.\n    if (numobj.preferred_domestic_carrier_code is not None and\n        len(numobj.preferred_domestic_carrier_code) > 0):\n        carrier_code = numobj.preferred_domestic_carrier_code\n    else:\n        carrier_code = fallback_carrier_code\n    return format_national_number_with_carrier_code(numobj, carrier_code)", "response": "Formats a phone number in national format using the carrier code specified in the preferred_domestic_carrier_code field of the PhoneNumber object passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_number_for_mobile_dialing(numobj, region_calling_from, with_formatting):\n    country_calling_code = numobj.country_code\n    if not _has_valid_country_calling_code(country_calling_code):\n        if numobj.raw_input is None:\n            return U_EMPTY_STRING\n        else:\n            return numobj.raw_input\n    formatted_number = U_EMPTY_STRING\n    # Clear the extension, as that part cannot normally be dialed together with the main number.\n    numobj_no_ext = PhoneNumber()\n    numobj_no_ext.merge_from(numobj)\n    numobj_no_ext.extension = None\n    region_code = region_code_for_country_code(country_calling_code)\n    numobj_type = number_type(numobj_no_ext)\n    is_valid_number = (numobj_type != PhoneNumberType.UNKNOWN)\n    if region_calling_from == region_code:\n        is_fixed_line_or_mobile = ((numobj_type == PhoneNumberType.FIXED_LINE) or\n                                   (numobj_type == PhoneNumberType.MOBILE) or\n                                   (numobj_type == PhoneNumberType.FIXED_LINE_OR_MOBILE))\n        # Carrier codes may be needed in some countries. We handle this here.\n        if region_code == \"CO\" and numobj_type == PhoneNumberType.FIXED_LINE:\n            formatted_number = format_national_number_with_carrier_code(numobj_no_ext,\n                                                                        _COLOMBIA_MOBILE_TO_FIXED_LINE_PREFIX)\n        elif region_code == \"BR\" and is_fixed_line_or_mobile:\n            # Historically, we set this to an empty string when parsing with\n            # raw input if none was found in the input string. However, this\n            # doesn't result in a number we can dial. For this reason, we\n            # treat the empty string the same as if it isn't set at all.\n            if (numobj_no_ext.preferred_domestic_carrier_code is not None and\n                len(numobj_no_ext.preferred_domestic_carrier_code) > 0):\n                formatted_number = format_national_number_with_preferred_carrier_code(numobj_no_ext, \"\")\n            else:\n                # Brazilian fixed line and mobile numbers need to be dialed with a\n                # carrier code when called within Brazil. Without that, most of\n                # the carriers won't connect the call.  Because of that, we return\n                # an empty string here.\n                formatted_number = U_EMPTY_STRING\n        elif is_valid_number and region_code == \"HU\":\n            # The national format for HU numbers doesn't contain the national\n            # prefix, because that is how numbers are normally written\n            # down. However, the national prefix is obligatory when dialing\n            # from a mobile phone, except for short numbers. As a result, we\n            # add it back here if it is a valid regular length phone number.\n            formatted_number = (ndd_prefix_for_region(region_code, True) +  # strip non-digits\n                                U_SPACE + format_number(numobj_no_ext, PhoneNumberFormat.NATIONAL))\n        elif country_calling_code == _NANPA_COUNTRY_CODE:\n            # For NANPA countries, we output international format for numbers\n            # that can be dialed internationally, since that always works,\n            # except for numbers which might potentially be short numbers,\n            # which are always dialled in national format.\n            metadata = PhoneMetadata.metadata_for_region(region_calling_from)\n            if (can_be_internationally_dialled(numobj_no_ext) and\n                _test_number_length(national_significant_number(numobj_no_ext),\n                                    metadata) != ValidationResult.TOO_SHORT):\n                formatted_number = format_number(numobj_no_ext, PhoneNumberFormat.INTERNATIONAL)\n            else:\n                formatted_number = format_number(numobj_no_ext, PhoneNumberFormat.NATIONAL)\n        else:\n            # For non-geographical countries, and Mexican, Chilean, and Uzbek\n            # fixed line and mobile numbers, we output international format for\n            # numbers that can be dialed internationally as that always works.\n            if ((region_code == REGION_CODE_FOR_NON_GEO_ENTITY or\n                 ((region_code == unicod(\"MX\") or region_code == unicod(\"CL\") or\n                   region_code == unicod(\"UZ\")) and\n                  is_fixed_line_or_mobile)) and\n                can_be_internationally_dialled(numobj_no_ext)):\n                # MX fixed line and mobile numbers should always be formatted\n                # in international format, even when dialed within MX. For\n                # national format to work, a carrier code needs to be used,\n                # and the correct carrier code depends on if the caller and\n                # callee are from the same local area. It is trickier to get\n                # that to work correctly than using international format,\n                # which is tested to work fine on all carriers.\n                # CL fixed line numbers need the national prefix when dialing\n                # in the national format, but don't have it when used for\n                # display. The reverse is true for mobile numbers.  As a\n                # result, we output them in the international format to make\n                # it work.\n                # UZ mobile and fixed-line numbers have to be formatted in\n                # international format or prefixed with special codes like 03,\n                # 04 (for fixed-line) and 05 (for mobile) for dialling\n                # successfully from mobile devices. As we do not have complete\n                # information on special codes and to be consistent with\n                # formatting across all phone types we return the number in\n                # international format here.\n                formatted_number = format_number(numobj_no_ext, PhoneNumberFormat.INTERNATIONAL)\n            else:\n                formatted_number = format_number(numobj_no_ext, PhoneNumberFormat.NATIONAL)\n    elif is_valid_number and can_be_internationally_dialled(numobj_no_ext):\n        # We assume that short numbers are not diallable from outside their\n        # region, so if a number is not a valid regular length phone number,\n        # we treat it as if it cannot be internationally dialled.\n        if with_formatting:\n            return format_number(numobj_no_ext, PhoneNumberFormat.INTERNATIONAL)\n        else:\n            return format_number(numobj_no_ext, PhoneNumberFormat.E164)\n\n    if with_formatting:\n        return formatted_number\n    else:\n        return normalize_diallable_chars_only(formatted_number)", "response": "Returns a number formatted in such a way that it can be dialed from a specific mobile phone in a specific region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_out_of_country_calling_number(numobj, region_calling_from):\n    if not _is_valid_region_code(region_calling_from):\n        return format_number(numobj, PhoneNumberFormat.INTERNATIONAL)\n    country_code = numobj.country_code\n    nsn = national_significant_number(numobj)\n    if not _has_valid_country_calling_code(country_code):\n        return nsn\n    if country_code == _NANPA_COUNTRY_CODE:\n        if is_nanpa_country(region_calling_from):\n            # For NANPA regions, return the national format for these regions\n            # but prefix it with the country calling code.\n            return (unicod(country_code) + U_SPACE +\n                    format_number(numobj, PhoneNumberFormat.NATIONAL))\n    elif country_code == country_code_for_valid_region(region_calling_from):\n        # If regions share a country calling code, the country calling code\n        # need not be dialled.  This also applies when dialling within a\n        # region, so this if clause covers both these cases.  Technically this\n        # is the case for dialling from La Reunion to other overseas\n        # departments of France (French Guiana, Martinique, Guadeloupe), but\n        # not vice versa - so we don't cover this edge case for now and for\n        # those cases return the version including country calling code.\n        # Details here:\n        # http://www.petitfute.com/voyage/225-info-pratiques-reunion\n        return format_number(numobj, PhoneNumberFormat.NATIONAL)\n\n    # Metadata cannot be None because we checked '_is_valid_region_code()' above.\n    metadata_for_region_calling_from = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_calling_from.upper())\n    international_prefix = metadata_for_region_calling_from.international_prefix\n\n    # For regions that have multiple international prefixes, the international\n    # format of the number is returned, unless there is a preferred\n    # international prefix.\n    i18n_prefix_for_formatting = U_EMPTY_STRING\n    i18n_match = fullmatch(_SINGLE_INTERNATIONAL_PREFIX, international_prefix)\n    if i18n_match:\n        i18n_prefix_for_formatting = international_prefix\n    elif metadata_for_region_calling_from.preferred_international_prefix is not None:\n        i18n_prefix_for_formatting = metadata_for_region_calling_from.preferred_international_prefix\n\n    region_code = region_code_for_country_code(country_code)\n    # Metadata cannot be None because the country calling code is valid.\n    metadata_for_region = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code.upper())\n    formatted_national_number = _format_nsn(nsn,\n                                            metadata_for_region,\n                                            PhoneNumberFormat.INTERNATIONAL)\n    formatted_number = _maybe_append_formatted_extension(numobj,\n                                                         metadata_for_region,\n                                                         PhoneNumberFormat.INTERNATIONAL,\n                                                         formatted_national_number)\n    if len(i18n_prefix_for_formatting) > 0:\n        formatted_number = (i18n_prefix_for_formatting + U_SPACE +\n                            unicod(country_code) + U_SPACE + formatted_number)\n    else:\n        formatted_number = _prefix_number_with_country_calling_code(country_code,\n                                                                    PhoneNumberFormat.INTERNATIONAL,\n                                                                    formatted_number)\n    return formatted_number", "response": "Formats a phone number for out - of - country dialing purposes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_in_original_format(numobj, region_calling_from):\n    if (numobj.raw_input is not None and not _has_formatting_pattern_for_number(numobj)):\n        # We check if we have the formatting pattern because without that, we\n        # might format the number as a group without national prefix.\n        return numobj.raw_input\n    if numobj.country_code_source is CountryCodeSource.UNSPECIFIED:\n        return format_number(numobj, PhoneNumberFormat.NATIONAL)\n\n    formatted_number = _format_original_allow_mods(numobj, region_calling_from)\n    num_raw_input = numobj.raw_input\n    # If no digit is inserted/removed/modified as a result of our formatting,\n    # we return the formatted phone number; otherwise we return the raw input\n    # the user entered.\n    if (formatted_number is not None and num_raw_input):\n        normalized_formatted_number = normalize_diallable_chars_only(formatted_number)\n        normalized_raw_input = normalize_diallable_chars_only(num_raw_input)\n        if normalized_formatted_number != normalized_raw_input:\n            formatted_number = num_raw_input\n    return formatted_number", "response": "Formats a number in the original format using the original format that the original number was parsed from."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if raw_input contains national prefix.", "response": "def _raw_input_contains_national_prefix(raw_input, national_prefix, region_code):\n    \"\"\"Check if raw_input, which is assumed to be in the national format, has a\n    national prefix. The national prefix is assumed to be in digits-only\n    form.\"\"\"\n    nnn = normalize_digits_only(raw_input)\n    if nnn.startswith(national_prefix):\n        try:\n            # Some Japanese numbers (e.g. 00777123) might be mistaken to\n            # contain the national prefix when written without it\n            # (e.g. 0777123) if we just do prefix matching. To tackle that, we\n            # check the validity of the number if the assumed national prefix\n            # is removed (777123 won't be valid in Japan).\n            return is_valid_number(parse(nnn[len(national_prefix):], region_code))\n        except NumberParseException:\n            return False\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat a number for out - of - country dialing purposes.", "response": "def format_out_of_country_keeping_alpha_chars(numobj, region_calling_from):\n    \"\"\"Formats a phone number for out-of-country dialing purposes.\n\n    Note that in this version, if the number was entered originally using\n    alpha characters and this version of the number is stored in raw_input,\n    this representation of the number will be used rather than the digit\n    representation. Grouping information, as specified by characters such as\n    \"-\" and \" \", will be retained.\n\n    Caveats:\n\n     - This will not produce good results if the country calling code is both\n       present in the raw input _and_ is the start of the national\n       number. This is not a problem in the regions which typically use alpha\n       numbers.\n\n     - This will also not produce good results if the raw input has any\n       grouping information within the first three digits of the national\n       number, and if the function needs to strip preceding digits/words in\n       the raw input before these digits. Normally people group the first\n       three digits together so this is not a huge problem - and will be fixed\n       if it proves to be so.\n\n    Arguments:\n    numobj -- The phone number that needs to be formatted.\n    region_calling_from -- The region where the call is being placed.\n\n    Returns the formatted phone number\n    \"\"\"\n    num_raw_input = numobj.raw_input\n    # If there is no raw input, then we can't keep alpha characters because there aren't any.\n    # In this case, we return format_out_of_country_calling_number.\n    if num_raw_input is None or len(num_raw_input) == 0:\n        return format_out_of_country_calling_number(numobj, region_calling_from)\n    country_code = numobj.country_code\n    if not _has_valid_country_calling_code(country_code):\n        return num_raw_input\n    # Strip any prefix such as country calling code, IDD, that was present. We\n    # do this by comparing the number in raw_input with the parsed number.  To\n    # do this, first we normalize punctuation. We retain number grouping\n    # symbols such as \" \" only.\n    num_raw_input = _normalize_helper(num_raw_input,\n                                      _ALL_PLUS_NUMBER_GROUPING_SYMBOLS,\n                                      True)\n    # Now we trim everything before the first three digits in the parsed\n    # number. We choose three because all valid alpha numbers have 3 digits at\n    # the start - if it does not, then we don't trim anything at\n    # all. Similarly, if the national number was less than three digits, we\n    # don't trim anything at all.\n    national_number = national_significant_number(numobj)\n    if len(national_number) > 3:\n        first_national_number_digit = num_raw_input.find(national_number[:3])\n        if first_national_number_digit != -1:\n            num_raw_input = num_raw_input[first_national_number_digit:]\n\n    metadata_for_region_calling_from = PhoneMetadata.metadata_for_region(region_calling_from.upper(), None)\n    if country_code == _NANPA_COUNTRY_CODE:\n        if is_nanpa_country(region_calling_from):\n            return unicod(country_code) + U_SPACE + num_raw_input\n    elif (metadata_for_region_calling_from is not None and\n          country_code == country_code_for_region(region_calling_from)):\n        formatting_pattern = _choose_formatting_pattern_for_number(metadata_for_region_calling_from.number_format,\n                                                                   national_number)\n        if formatting_pattern is None:\n            # If no pattern above is matched, we format the original input\n            return num_raw_input\n        new_format = _copy_number_format(formatting_pattern)\n        # The first group is the first group of digits that the user\n        # wrote together.\n        new_format.pattern = u(\"(\\\\d+)(.*)\")\n        # Here we just concatenate them back together after the national\n        # prefix has been fixed.\n        new_format.format = u(r\"\\1\\2\")\n        # Now we format using this pattern instead of the default pattern,\n        # but with the national prefix prefixed if necessary.\n        # This will not work in the cases where the pattern (and not the\n        # leading digits) decide whether a national prefix needs to be used,\n        # since we have overridden the pattern to match anything, but that is\n        # not the case in the metadata to date.\n        return _format_nsn_using_pattern(num_raw_input,\n                                         new_format,\n                                         PhoneNumberFormat.NATIONAL)\n    i18n_prefix_for_formatting = U_EMPTY_STRING\n    # If an unsupported region-calling-from is entered, or a country with\n    # multiple international prefixes, the international format of the number\n    # is returned, unless there is a preferred international prefix.\n    if metadata_for_region_calling_from is not None:\n        international_prefix = metadata_for_region_calling_from.international_prefix\n        i18n_match = fullmatch(_SINGLE_INTERNATIONAL_PREFIX, international_prefix)\n        if i18n_match:\n            i18n_prefix_for_formatting = international_prefix\n        else:\n            i18n_prefix_for_formatting = metadata_for_region_calling_from.preferred_international_prefix\n\n    region_code = region_code_for_country_code(country_code)\n    # Metadata cannot be None because the country calling code is valid.\n    metadata_for_region = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code)\n    formatted_number = _maybe_append_formatted_extension(numobj,\n                                                         metadata_for_region,\n                                                         PhoneNumberFormat.INTERNATIONAL,\n                                                         num_raw_input)\n    if i18n_prefix_for_formatting:\n        formatted_number = (i18n_prefix_for_formatting + U_SPACE +\n                            unicod(country_code) + U_SPACE + formatted_number)\n    else:\n        # Invalid region entered as country-calling-from (so no metadata was\n        # found for it) or the region chosen has multiple international\n        # dialling prefixes.\n        formatted_number = _prefix_number_with_country_calling_code(country_code,\n                                                                    PhoneNumberFormat.INTERNATIONAL,\n                                                                    formatted_number)\n    return formatted_number"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the national significant number of a given phone number.", "response": "def national_significant_number(numobj):\n    \"\"\"Gets the national significant number of a phone number.\n\n    Note that a national significant number doesn't contain a national prefix\n    or any formatting.\n\n    Arguments:\n    numobj -- The PhoneNumber object for which the national significant number\n              is needed.\n\n    Returns the national significant number of the PhoneNumber object passed\n    in.\n    \"\"\"\n    # If leading zero(s) have been set, we prefix this now. Note this is not a\n    # national prefix.\n    national_number = U_EMPTY_STRING\n    if numobj.italian_leading_zero:\n        num_zeros = numobj.number_of_leading_zeros\n        if num_zeros is None:\n            num_zeros = 1\n        if num_zeros > 0:\n            national_number = U_ZERO * num_zeros\n    national_number += str(numobj.national_number)\n    return national_number"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prefix_number_with_country_calling_code(country_code, num_format, formatted_number):\n    if num_format == PhoneNumberFormat.E164:\n        return _PLUS_SIGN + unicod(country_code) + formatted_number\n    elif num_format == PhoneNumberFormat.INTERNATIONAL:\n        return _PLUS_SIGN + unicod(country_code) + U_SPACE + formatted_number\n    elif num_format == PhoneNumberFormat.RFC3966:\n        return _RFC3966_PREFIX + _PLUS_SIGN + unicod(country_code) + U_DASH + formatted_number\n    else:\n        return formatted_number", "response": "A helper function that is used by format_number and format_by_pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats a national number in two national number sets.", "response": "def _format_nsn(number, metadata, num_format, carrier_code=None):\n    \"\"\"Format a national number.\"\"\"\n    # Note in some regions, the national number can be written in two\n    # completely different ways depending on whether it forms part of the\n    # NATIONAL format or INTERNATIONAL format. The num_format parameter here\n    # is used to specify which format to use for those cases. If a carrier_code\n    # is specified, this will be inserted into the formatted string to replace\n    # $CC.\n    intl_number_formats = metadata.intl_number_format\n\n    # When the intl_number_formats exists, we use that to format national\n    # number for the INTERNATIONAL format instead of using the\n    # number_desc.number_formats.\n    if (len(intl_number_formats) == 0 or\n        num_format == PhoneNumberFormat.NATIONAL):\n        available_formats = metadata.number_format\n    else:\n        available_formats = metadata.intl_number_format\n    formatting_pattern = _choose_formatting_pattern_for_number(available_formats, number)\n    if formatting_pattern is None:\n        return number\n    else:\n        return _format_nsn_using_pattern(number, formatting_pattern, num_format, carrier_code)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an invalid number for the specified region.", "response": "def invalid_example_number(region_code):\n    \"\"\"Gets an invalid number for the specified region.\n\n    This is useful for unit-testing purposes, where you want to test what\n    will happen with an invalid number. Note that the number that is\n    returned will always be able to be parsed and will have the correct\n    country code. It may also be a valid *short* number/code for this\n    region. Validity checking such numbers is handled with shortnumberinfo.\n\n    Arguments:\n    region_code -- The region for which an example number is needed.\n\n\n    Returns an invalid number for the specified region. Returns None when an\n    unsupported region or the region 001 (Earth) is passed in.\n    \"\"\"\n    if not _is_valid_region_code(region_code):\n        return None\n    # We start off with a valid fixed-line number since every country\n    # supports this. Alternatively we could start with a different number\n    # type, since fixed-line numbers typically have a wide breadth of valid\n    # number lengths and we may have to make it very short before we get an\n    # invalid number.\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper())\n    desc = _number_desc_by_type(metadata, PhoneNumberType.FIXED_LINE)\n    if desc is None or desc.example_number is None:\n        # This shouldn't happen; we have a test for this.\n        return None  # pragma no cover\n    example_number = desc.example_number\n    # Try and make the number invalid. We do this by changing the length. We\n    # try reducing the length of the number, since currently no region has a\n    # number that is the same length as MIN_LENGTH_FOR_NSN. This is probably\n    # quicker than making the number longer, which is another\n    # alternative. We could also use the possible number pattern to extract\n    # the possible lengths of the number to make this faster, but this\n    # method is only for unit-testing so simplicity is preferred to\n    # performance.  We don't want to return a number that can't be parsed,\n    # so we check the number is long enough. We try all possible lengths\n    # because phone number plans often have overlapping prefixes so the\n    # number 123456 might be valid as a fixed-line number, and 12345 as a\n    # mobile number. It would be faster to loop in a different order, but we\n    # prefer numbers that look closer to real numbers (and it gives us a\n    # variety of different lengths for the resulting phone numbers -\n    # otherwise they would all be MIN_LENGTH_FOR_NSN digits long.)\n    phone_number_length = len(example_number) - 1\n    while phone_number_length >= _MIN_LENGTH_FOR_NSN:\n        number_to_try = example_number[:phone_number_length]\n        try:\n            possibly_valid_number = parse(number_to_try, region_code)\n            if not is_valid_number(possibly_valid_number):\n                return possibly_valid_number\n        except NumberParseException:  # pragma no cover\n            # Shouldn't happen: we have already checked the length, we know\n            # example numbers have only valid digits, and we know the region\n            # code is fine.\n            pass\n        phone_number_length -= 1\n\n    # We have a test to check that this doesn't happen for any of our\n    # supported regions.\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef example_number_for_type(region_code, num_type):\n    if region_code is None:\n        return _example_number_anywhere_for_type(num_type)\n    # Check the region code is valid.\n    if not _is_valid_region_code(region_code):\n        return None\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper())\n    desc = _number_desc_by_type(metadata, num_type)\n    if desc is not None and desc.example_number is not None:\n        try:\n            return parse(desc.example_number, region_code)\n        except NumberParseException:  # pragma no cover\n            pass\n    return None", "response": "Returns a valid number for the specified region and number type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _example_number_anywhere_for_type(num_type):\n    for region_code in SUPPORTED_REGIONS:\n        example_numobj = example_number_for_type(region_code, num_type)\n        if example_numobj is not None:\n            return example_numobj\n    # If there wasn't an example number for a region, try the non-geographical entities.\n    for country_calling_code in COUNTRY_CODES_FOR_NON_GEO_REGIONS:\n        metadata = PhoneMetadata.metadata_for_nongeo_region(country_calling_code, None)\n        desc = _number_desc_by_type(metadata, num_type)\n        if desc is not None and desc.example_number is not None:\n            try:\n                return parse(_PLUS_SIGN + unicod(country_calling_code) + desc.example_number, UNKNOWN_REGION)\n            except NumberParseException:  # pragma no cover\n                pass\n\n    # There are no example numbers of this type for any country in the library.\n    return None", "response": "Gets a valid number for the specified number type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a valid number for the specified country calling code for a non - geographical entity.", "response": "def example_number_for_non_geo_entity(country_calling_code):\n    \"\"\"Gets a valid number for the specified country calling code for a non-geographical entity.\n\n    Arguments:\n    country_calling_code -- The country calling code for a non-geographical entity.\n\n    Returns a valid number for the non-geographical entity. Returns None when\n    the metadata does not contain such information, or the country calling\n    code passed in does not belong to a non-geographical entity.\n    \"\"\"\n    metadata = PhoneMetadata.metadata_for_nongeo_region(country_calling_code, None)\n    if metadata is not None:\n        # For geographical entities, fixed-line data is always present. However, for non-geographical\n        # entities, this is not the case, so we have to go through different types to find the\n        # example number. We don't check fixed-line or personal number since they aren't used by\n        # non-geographical entities (if this changes, a unit-test will catch this.)\n        for desc in (metadata.mobile, metadata.toll_free, metadata.shared_cost, metadata.voip,\n                     metadata.voicemail, metadata.uan, metadata.premium_rate):\n            try:\n                if (desc is not None and desc.example_number is not None):\n                    return parse(_PLUS_SIGN + unicod(country_calling_code) + desc.example_number, UNKNOWN_REGION)\n            except NumberParseException:\n                pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _maybe_append_formatted_extension(numobj, metadata, num_format, number):\n    if numobj.extension:\n        if num_format == PhoneNumberFormat.RFC3966:\n            return number + _RFC3966_EXTN_PREFIX + numobj.extension\n        else:\n            if metadata.preferred_extn_prefix is not None:\n                return number + metadata.preferred_extn_prefix + numobj.extension\n            else:\n                return number + _DEFAULT_EXTN_PREFIX + numobj.extension\n    return number", "response": "Appends the formatted extension of a phone number to the formatted number if it has one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _number_desc_by_type(metadata, num_type):\n    if num_type == PhoneNumberType.PREMIUM_RATE:\n        return metadata.premium_rate\n    elif num_type == PhoneNumberType.TOLL_FREE:\n        return metadata.toll_free\n    elif num_type == PhoneNumberType.MOBILE:\n        return metadata.mobile\n    elif (num_type == PhoneNumberType.FIXED_LINE or\n          num_type == PhoneNumberType.FIXED_LINE_OR_MOBILE):\n        return metadata.fixed_line\n    elif num_type == PhoneNumberType.SHARED_COST:\n        return metadata.shared_cost\n    elif num_type == PhoneNumberType.VOIP:\n        return metadata.voip\n    elif num_type == PhoneNumberType.PERSONAL_NUMBER:\n        return metadata.personal_number\n    elif num_type == PhoneNumberType.PAGER:\n        return metadata.pager\n    elif num_type == PhoneNumberType.UAN:\n        return metadata.uan\n    elif num_type == PhoneNumberType.VOICEMAIL:\n        return metadata.voicemail\n    else:\n        return metadata.general_desc", "response": "Return the PhoneNumberDesc of the given type in the metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef number_type(numobj):\n    region_code = region_code_for_number(numobj)\n    metadata = PhoneMetadata.metadata_for_region_or_calling_code(numobj.country_code, region_code)\n    if metadata is None:\n        return PhoneNumberType.UNKNOWN\n    national_number = national_significant_number(numobj)\n    return _number_type_helper(national_number, metadata)", "response": "Returns the type of a valid phone number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the type of the given number against the metadata", "response": "def _number_type_helper(national_number, metadata):\n    \"\"\"Return the type of the given number against the metadata\"\"\"\n    if not _is_number_matching_desc(national_number, metadata.general_desc):\n        return PhoneNumberType.UNKNOWN\n    if _is_number_matching_desc(national_number, metadata.premium_rate):\n        return PhoneNumberType.PREMIUM_RATE\n    if _is_number_matching_desc(national_number, metadata.toll_free):\n        return PhoneNumberType.TOLL_FREE\n    if _is_number_matching_desc(national_number, metadata.shared_cost):\n        return PhoneNumberType.SHARED_COST\n    if _is_number_matching_desc(national_number, metadata.voip):\n        return PhoneNumberType.VOIP\n    if _is_number_matching_desc(national_number, metadata.personal_number):\n        return PhoneNumberType.PERSONAL_NUMBER\n    if _is_number_matching_desc(national_number, metadata.pager):\n        return PhoneNumberType.PAGER\n    if _is_number_matching_desc(national_number, metadata.uan):\n        return PhoneNumberType.UAN\n    if _is_number_matching_desc(national_number, metadata.voicemail):\n        return PhoneNumberType.VOICEMAIL\n\n    if _is_number_matching_desc(national_number, metadata.fixed_line):\n        if metadata.same_mobile_and_fixed_line_pattern:\n            return PhoneNumberType.FIXED_LINE_OR_MOBILE\n        elif _is_number_matching_desc(national_number, metadata.mobile):\n            return PhoneNumberType.FIXED_LINE_OR_MOBILE\n        return PhoneNumberType.FIXED_LINE\n\n    # Otherwise, test to see if the number is mobile. Only do this if certain\n    # that the patterns for mobile and fixed line aren't the same.\n    if (not metadata.same_mobile_and_fixed_line_pattern and\n        _is_number_matching_desc(national_number, metadata.mobile)):\n        return PhoneNumberType.MOBILE\n    return PhoneNumberType.UNKNOWN"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine if the number matches the given PhoneNumberDesc", "response": "def _is_number_matching_desc(national_number, number_desc):\n    \"\"\"Determine if the number matches the given PhoneNumberDesc\"\"\"\n    # Check if any possible number lengths are present; if so, we use them to avoid checking the\n    # validation pattern if they don't match. If they are absent, this means they match the general\n    # description, which we have already checked before checking a specific number type.\n    if number_desc is None:\n        return False\n    actual_length = len(national_number)\n    possible_lengths = number_desc.possible_length\n    if len(possible_lengths) > 0 and actual_length not in possible_lengths:\n        return False\n    return _match_national_number(national_number, number_desc, False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_valid_number_for_region(numobj, region_code):\n    country_code = numobj.country_code\n    if region_code is None:\n        return False\n    metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code.upper())\n    if (metadata is None or\n        (region_code != REGION_CODE_FOR_NON_GEO_ENTITY and\n         country_code != country_code_for_valid_region(region_code))):\n        # Either the region code was invalid, or the country calling code for\n        # this number does not match that of the region code.\n        return False\n    nsn = national_significant_number(numobj)\n    return (_number_type_helper(nsn, metadata) != PhoneNumberType.UNKNOWN)", "response": "Tests whether a number is valid for a certain region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef region_code_for_number(numobj):\n    country_code = numobj.country_code\n    regions = COUNTRY_CODE_TO_REGION_CODE.get(country_code, None)\n    if regions is None:\n        return None\n\n    if len(regions) == 1:\n        return regions[0]\n    else:\n        return _region_code_for_number_from_list(numobj, regions)", "response": "Returns the region code for a given phone number object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the region code for a number in a list that matches a number", "response": "def _region_code_for_number_from_list(numobj, regions):\n    \"\"\"Find the region in a list that matches a number\"\"\"\n    national_number = national_significant_number(numobj)\n    for region_code in regions:\n        # If leading_digits is present, use this. Otherwise, do full\n        # validation.\n        # Metadata cannot be None because the region codes come from\n        # the country calling code map.\n        metadata = PhoneMetadata.metadata_for_region(region_code.upper(), None)\n        if metadata is None:\n            continue\n        if metadata.leading_digits is not None:\n            leading_digit_re = re.compile(metadata.leading_digits)\n            match = leading_digit_re.match(national_number)\n            if match:\n                return region_code\n        elif _number_type_helper(national_number, metadata) != PhoneNumberType.UNKNOWN:\n            return region_code\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef country_code_for_valid_region(region_code):\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper(), None)\n    if metadata is None:\n        raise Exception(\"Invalid region code %s\" % region_code)\n    return metadata.country_code", "response": "Returns the country calling code for a specific region."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the national dialling prefix for a specific region.", "response": "def ndd_prefix_for_region(region_code, strip_non_digits):\n    \"\"\"Returns the national dialling prefix for a specific region.\n\n    For example, this would be 1 for the United States, and 0 for New\n    Zealand. Set strip_non_digits to True to strip symbols like \"~\" (which\n    indicates a wait for a dialling tone) from the prefix returned. If no\n    national prefix is present, we return None.\n\n    Warning: Do not use this method for do-your-own formatting - for some\n    regions, the national dialling prefix is used only for certain types of\n    numbers. Use the library's formatting functions to prefix the national\n    prefix when required.\n\n    Arguments:\n    region_code -- The region that we want to get the dialling prefix for.\n    strip_non_digits -- whether to strip non-digits from the national\n               dialling prefix.\n\n    Returns the dialling prefix for the region denoted by region_code.\n    \"\"\"\n    if region_code is None:\n        return None\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper(), None)\n    if metadata is None:\n        return None\n    national_prefix = metadata.national_prefix\n    if national_prefix is None or len(national_prefix) == 0:\n        return None\n    if strip_non_digits:\n        # Note: if any other non-numeric symbols are ever used in national\n        # prefixes, these would have to be removed here as well.\n        national_prefix = re.sub(U_TILDE, U_EMPTY_STRING, national_prefix)\n    return national_prefix"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_alpha_number(number):\n    if not _is_viable_phone_number(number):\n        # Number is too short, or doesn't match the basic phone number pattern.\n        return False\n    extension, stripped_number = _maybe_strip_extension(number)\n    return bool(fullmatch(_VALID_ALPHA_PHONE_PATTERN, stripped_number))", "response": "Checks if the number is a valid alpha vanity number such as 8008."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_possible_number(numobj):\n    result = is_possible_number_with_reason(numobj)\n    return (result == ValidationResult.IS_POSSIBLE or\n            result == ValidationResult.IS_POSSIBLE_LOCAL_ONLY)", "response": "Returns True if the number is possible in the\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_possible_number_for_type(numobj, numtype):\n    result = is_possible_number_for_type_with_reason(numobj, numtype)\n    return (result == ValidationResult.IS_POSSIBLE or\n            result == ValidationResult.IS_POSSIBLE_LOCAL_ONLY)", "response": "Returns True if the number is possible for the given type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_possible_number_for_type_with_reason(numobj, numtype):\n    national_number = national_significant_number(numobj)\n    country_code = numobj.country_code\n    # Note: For regions that share a country calling code, like NANPA numbers,\n    # we just use the rules from the default region (US in this case) since the\n    # region_code_for_number will not work if the number is possible but not\n    # valid. There is in fact one country calling code (290) where the possible\n    # number pattern differs between various regions (Saint Helena and Tristan\n    # da Cu\u00f1ha), but this is handled by putting all possible lengths for any\n    # country with this country calling code in the metadata for the default\n    # region in this case.\n    if not _has_valid_country_calling_code(country_code):\n        return ValidationResult.INVALID_COUNTRY_CODE\n    region_code = region_code_for_country_code(country_code)\n    # Metadata cannot be None because the country calling code is valid.\n    metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code)\n    return _test_number_length(national_number, metadata, numtype)", "response": "Checks whether a number is a possible number of a particular type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef truncate_too_long_number(numobj):\n    if is_valid_number(numobj):\n        return True\n    numobj_copy = PhoneNumber()\n    numobj_copy.merge_from(numobj)\n    national_number = numobj.national_number\n\n    while not is_valid_number(numobj_copy):\n        # Strip a digit off the RHS\n        national_number = national_number // 10\n        numobj_copy.national_number = national_number\n        validation_result = is_possible_number_with_reason(numobj_copy)\n        if (validation_result == ValidationResult.TOO_SHORT or\n            national_number == 0):\n            return False\n    # To reach here, numobj_copy is a valid number.  Modify the original object\n    numobj.national_number = national_number\n    return True", "response": "Truncate a number object that is too long to a valid number. Returns True if a valid number is extracted False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the country calling code from a number.", "response": "def _extract_country_code(number):\n    \"\"\"Extracts country calling code from number.\n\n    Returns a 2-tuple of (country_calling_code, rest_of_number).  It assumes\n    that the leading plus sign or IDD has already been removed.  Returns (0,\n    number) if number doesn't start with a valid country calling code.\n    \"\"\"\n\n    if len(number) == 0 or number[0] == U_ZERO:\n        # Country codes do not begin with a '0'.\n        return (0, number)\n    for ii in range(1, min(len(number), _MAX_LENGTH_COUNTRY_CODE) + 1):\n        try:\n            country_code = int(number[:ii])\n            if country_code in COUNTRY_CODE_TO_REGION_CODE:\n                return (country_code, number[ii:])\n        except Exception:\n            pass\n    return (0, number)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to extract a country calling code from a number.", "response": "def _maybe_extract_country_code(number, metadata, keep_raw_input, numobj):\n    \"\"\"Tries to extract a country calling code from a number.\n\n    This method will return zero if no country calling code is considered to\n    be present. Country calling codes are extracted in the following ways:\n\n     - by stripping the international dialing prefix of the region the person\n       is dialing from, if this is present in the number, and looking at the\n       next digits\n\n     - by stripping the '+' sign if present and then looking at the next\n       digits\n\n     - by comparing the start of the number and the country calling code of\n       the default region.  If the number is not considered possible for the\n       numbering plan of the default region initially, but starts with the\n       country calling code of this region, validation will be reattempted\n       after stripping this country calling code. If this number is considered\n       a possible number, then the first digits will be considered the country\n       calling code and removed as such.\n\n    It will raise a NumberParseException if the number starts with a '+' but\n    the country calling code supplied after this does not match that of any\n    known region.\n\n    Arguments:\n    number -- non-normalized telephone number that we wish to extract a\n              country calling code from; may begin with '+'\n    metadata -- metadata about the region this number may be from, or None\n    keep_raw_input -- True if the country_code_source and\n              preferred_carrier_code fields of numobj should be populated.\n    numobj -- The PhoneNumber object where the country_code and\n              country_code_source need to be populated. Note the country_code\n              is always populated, whereas country_code_source is only\n              populated when keep_raw_input is True.\n\n    Returns a 2-tuple containing:\n      - the country calling code extracted or 0 if none could be extracted\n      - a string holding the national significant number, in the case\n        that a country calling code was extracted. If no country calling code\n        was extracted, this will be empty.\n    \"\"\"\n    if len(number) == 0:\n        return (0, U_EMPTY_STRING)\n    full_number = number\n    # Set the default prefix to be something that will never match.\n    possible_country_idd_prefix = unicod(\"NonMatch\")\n    if metadata is not None and metadata.international_prefix is not None:\n        possible_country_idd_prefix = metadata.international_prefix\n\n    country_code_source, full_number = _maybe_strip_i18n_prefix_and_normalize(full_number,\n                                                                              possible_country_idd_prefix)\n    if keep_raw_input:\n        numobj.country_code_source = country_code_source\n\n    if country_code_source != CountryCodeSource.FROM_DEFAULT_COUNTRY:\n        if len(full_number) <= _MIN_LENGTH_FOR_NSN:\n            raise NumberParseException(NumberParseException.TOO_SHORT_AFTER_IDD,\n                                       \"Phone number had an IDD, but after this was not \" +\n                                       \"long enough to be a viable phone number.\")\n        potential_country_code, rest_of_number = _extract_country_code(full_number)\n        if potential_country_code != 0:\n            numobj.country_code = potential_country_code\n            return (potential_country_code, rest_of_number)\n\n        # If this fails, they must be using a strange country calling code\n        # that we don't recognize, or that doesn't exist.\n        raise NumberParseException(NumberParseException.INVALID_COUNTRY_CODE,\n                                   \"Country calling code supplied was not recognised.\")\n    elif metadata is not None:\n        # Check to see if the number starts with the country calling code for\n        # the default region. If so, we remove the country calling code, and\n        # do some checks on the validity of the number before and after.\n        default_country_code = metadata.country_code\n        default_country_code_str = str(metadata.country_code)\n        normalized_number = full_number\n        if normalized_number.startswith(default_country_code_str):\n            potential_national_number = full_number[len(default_country_code_str):]\n            general_desc = metadata.general_desc\n            _, potential_national_number, _ = _maybe_strip_national_prefix_carrier_code(potential_national_number,\n                                                                                        metadata)\n\n            # If the number was not valid before but is valid now, or if it\n            # was too long before, we consider the number with the country\n            # calling code stripped to be a better result and keep that\n            # instead.\n            if ((not _match_national_number(full_number, general_desc, False) and\n                 _match_national_number(potential_national_number, general_desc, False)) or\n                (_test_number_length(full_number, metadata) == ValidationResult.TOO_LONG)):\n                if keep_raw_input:\n                    numobj.country_code_source = CountryCodeSource.FROM_NUMBER_WITHOUT_PLUS_SIGN\n                numobj.country_code = default_country_code\n                return (default_country_code, potential_national_number)\n\n    # No country calling code present.\n    numobj.country_code = 0\n    return (0, U_EMPTY_STRING)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the prefix of a number into a 2 - tuple containing a boolean indicating if the IDD was stripped.", "response": "def _parse_prefix_as_idd(idd_pattern, number):\n    \"\"\"Strips the IDD from the start of the number if present.\n\n    Helper function used by _maybe_strip_i18n_prefix_and_normalize().\n\n    Returns a 2-tuple:\n      - Boolean indicating if IDD was stripped\n      - Number with IDD stripped\n    \"\"\"\n    match = idd_pattern.match(number)\n    if match:\n        match_end = match.end()\n        # Only strip this if the first digit after the match is not a 0, since\n        # country calling codes cannot begin with 0.\n        digit_match = _CAPTURING_DIGIT_PATTERN.search(number[match_end:])\n        if digit_match:\n            normalized_group = normalize_digits_only(digit_match.group(1))\n            if normalized_group == U_ZERO:\n                return (False, number)\n        return (True, number[match_end:])\n    return (False, number)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstrips any international prefix (such as +, 00, 011) present in the number provided, normalizes the resulting number, and indicates if an international prefix was present. Arguments: number -- The non-normalized telephone number that we wish to strip any international dialing prefix from. possible_idd_prefix -- The international direct dialing prefix from the region we think this number may be dialed in. Returns a 2-tuple containing: - The corresponding CountryCodeSource if an international dialing prefix could be removed from the number, otherwise CountryCodeSource.FROM_DEFAULT_COUNTRY if the number did not seem to be in international format. - The number with the prefix stripped.", "response": "def _maybe_strip_i18n_prefix_and_normalize(number, possible_idd_prefix):\n    \"\"\"Strips any international prefix (such as +, 00, 011) present in the\n    number provided, normalizes the resulting number, and indicates if an\n    international prefix was present.\n\n    Arguments:\n    number -- The non-normalized telephone number that we wish to strip any international\n              dialing prefix from.\n    possible_idd_prefix -- The international direct dialing prefix from the region we\n              think this number may be dialed in.\n\n    Returns a 2-tuple containing:\n      - The corresponding CountryCodeSource if an international dialing prefix\n        could be removed from the number, otherwise\n        CountryCodeSource.FROM_DEFAULT_COUNTRY if the number did not seem to\n        be in international format.\n      - The number with the prefix stripped.\n    \"\"\"\n    if len(number) == 0:\n        return (CountryCodeSource.FROM_DEFAULT_COUNTRY, number)\n    # Check to see if the number begins with one or more plus signs.\n    m = _PLUS_CHARS_PATTERN.match(number)\n    if m:\n        number = number[m.end():]\n        # Can now normalize the rest of the number since we've consumed the\n        # \"+\" sign at the start.\n        return (CountryCodeSource.FROM_NUMBER_WITH_PLUS_SIGN,\n                _normalize(number))\n\n    # Attempt to parse the first digits as an international prefix.\n    idd_pattern = re.compile(possible_idd_prefix)\n    number = _normalize(number)\n    stripped, number = _parse_prefix_as_idd(idd_pattern, number)\n    if stripped:\n        return (CountryCodeSource.FROM_NUMBER_WITH_IDD, number)\n    else:\n        return (CountryCodeSource.FROM_DEFAULT_COUNTRY, number)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstripping any national prefix (such as 0, 1) present in a number. Arguments: number -- The normalized telephone number that we wish to strip any national dialing prefix from metadata -- The metadata for the region that we think this number is from. Returns a 3-tuple of - The carrier code extracted if it is present, otherwise an empty string. - The number with the prefix stripped. - Boolean indicating if a national prefix or carrier code (or both) could be extracted.", "response": "def _maybe_strip_national_prefix_carrier_code(number, metadata):\n    \"\"\"Strips any national prefix (such as 0, 1) present in a number.\n\n    Arguments:\n    number -- The normalized telephone number that we wish to strip any\n              national dialing prefix from\n    metadata -- The metadata for the region that we think this number\n              is from.\n\n    Returns a 3-tuple of\n     - The carrier code extracted if it is present, otherwise an empty string.\n     - The number with the prefix stripped.\n     - Boolean indicating if a national prefix or carrier code (or both) could be extracted.\n     \"\"\"\n    carrier_code = U_EMPTY_STRING\n    possible_national_prefix = metadata.national_prefix_for_parsing\n    if (len(number) == 0 or\n        possible_national_prefix is None or\n        len(possible_national_prefix) == 0):\n        # Early return for numbers of zero length.\n        return (U_EMPTY_STRING, number, False)\n\n    # Attempt to parse the first digits as a national prefix.\n    prefix_pattern = re.compile(possible_national_prefix)\n    prefix_match = prefix_pattern.match(number)\n    if prefix_match:\n        general_desc = metadata.general_desc\n        # Check if the original number is viable.\n        is_viable_original_number = _match_national_number(number, general_desc, False)\n        # prefix_match.groups() == () implies nothing was captured by the\n        # capturing groups in possible_national_prefix; therefore, no\n        # transformation is necessary, and we just remove the national prefix.\n        num_groups = len(prefix_match.groups())\n        transform_rule = metadata.national_prefix_transform_rule\n        if (transform_rule is None or\n            len(transform_rule) == 0 or\n            prefix_match.groups()[num_groups - 1] is None):\n            # If the original number was viable, and the resultant number is not, we return.\n            # Check that the resultant number is viable. If not, return.\n            national_number_match = _match_national_number(number[prefix_match.end():], general_desc, False)\n            if (is_viable_original_number and not national_number_match):\n                return (U_EMPTY_STRING, number, False)\n\n            if (num_groups > 0 and\n                prefix_match.groups(num_groups) is not None):\n                carrier_code = prefix_match.group(1)\n            return (carrier_code, number[prefix_match.end():], True)\n        else:\n            # Check that the resultant number is still viable. If not,\n            # return. Check this by copying the number and making the\n            # transformation on the copy first.\n            transformed_number = re.sub(prefix_pattern, transform_rule, number, count=1)\n            national_number_match = _match_national_number(transformed_number, general_desc, False)\n            if (is_viable_original_number and not national_number_match):\n                return (\"\", number, False)\n            if num_groups > 1:\n                carrier_code = prefix_match.group(1)\n            return (carrier_code, transformed_number, True)\n    else:\n        return (carrier_code, number, False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _maybe_strip_extension(number):\n    match = _EXTN_PATTERN.search(number)\n    # If we find a potential extension, and the number preceding this is a\n    # viable number, we assume it is an extension.\n    if match and _is_viable_phone_number(number[:match.start()]):\n        # The numbers are captured into groups in the regular expression.\n        for group in match.groups():\n            # We go through the capturing groups until we find one that\n            # captured some digits. If none did, then we will return the empty\n            # string.\n            if group is not None:\n                return (group, number[:match.start()])\n    return (\"\", number)", "response": "Strip extension from the end of a number string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks to see if the number is valid and that it is not valid and that the region code used is not valid. Returns True if it is valid False otherwise.", "response": "def _check_region_for_parsing(number, default_region):\n    \"\"\"Checks to see that the region code used is valid, or if it is not\n    valid, that the number to parse starts with a + symbol so that we can\n    attempt to infer the region from the number.  Returns False if it cannot\n    use the region provided and the region cannot be inferred.\n    \"\"\"\n    if not _is_valid_region_code(default_region):\n        # If the number is None or empty, we can't infer the region.\n        if number is None or len(number) == 0:\n            return False\n        match = _PLUS_CHARS_PATTERN.match(number)\n        if match is None:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_italian_leading_zeros_for_phone_number(national_number, numobj):\n    if len(national_number) > 1 and national_number[0] == U_ZERO:\n        numobj.italian_leading_zero = True\n        number_of_leading_zeros = 1\n        # Note that if the number is all \"0\"s, the last \"0\" is not counted as\n        # a leading zero.\n        while (number_of_leading_zeros < len(national_number) - 1 and\n               national_number[number_of_leading_zeros] == U_ZERO):\n            number_of_leading_zeros += 1\n        if number_of_leading_zeros != 1:\n            numobj.number_of_leading_zeros = number_of_leading_zeros", "response": "A helper function to set the values related to leading zeros in a ArcGIS VAT."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a string and returns a corresponding PhoneNumber object.", "response": "def parse(number, region=None, keep_raw_input=False,\n          numobj=None, _check_region=True):\n    \"\"\"Parse a string and return a corresponding PhoneNumber object.\n\n    The method is quite lenient and looks for a number in the input text\n    (raw input) and does not check whether the string is definitely only a\n    phone number. To do this, it ignores punctuation and white-space, as\n    well as any text before the number (e.g. a leading \"Tel: \") and trims\n    the non-number bits.  It will accept a number in any format (E164,\n    national, international etc), assuming it can be interpreted with the\n    defaultRegion supplied. It also attempts to convert any alpha characters\n    into digits if it thinks this is a vanity number of the type \"1800\n    MICROSOFT\".\n\n    This method will throw a NumberParseException if the number is not\n    considered to be a possible number. Note that validation of whether the\n    number is actually a valid number for a particular region is not\n    performed. This can be done separately with is_valid_number.\n\n    Note this method canonicalizes the phone number such that different\n    representations can be easily compared, no matter what form it was\n    originally entered in (e.g. national, international). If you want to\n    record context about the number being parsed, such as the raw input that\n    was entered, how the country code was derived etc. then ensure\n    keep_raw_input is set.\n\n    Note if any new field is added to this method that should always be filled\n    in, even when keep_raw_input is False, it should also be handled in the\n    _copy_core_fields_only() function.\n\n    Arguments:\n    number -- The number that we are attempting to parse. This can\n              contain formatting such as +, ( and -, as well as a phone\n              number extension. It can also be provided in RFC3966 format.\n    region -- The region that we are expecting the number to be from. This\n              is only used if the number being parsed is not written in\n              international format. The country_code for the number in\n              this case would be stored as that of the default region\n              supplied. If the number is guaranteed to start with a '+'\n              followed by the country calling code, then None or\n              UNKNOWN_REGION can be supplied.\n    keep_raw_input -- Whether to populate the raw_input field of the\n              PhoneNumber object with number (as well as the\n              country_code_source field).\n    numobj -- An optional existing PhoneNumber object to receive the\n              parsing results\n    _check_region -- Whether to check the supplied region parameter;\n              should always be True for external callers.\n\n    Returns a PhoneNumber object filled with the parse number.\n\n    Raises:\n    NumberParseException if the string is not considered to be a viable\n    phone number (e.g.  too few or too many digits) or if no default\n    region was supplied and the number is not in international format\n    (does not start with +).\n\n    \"\"\"\n    if numobj is None:\n        numobj = PhoneNumber()\n    if number is None:\n        raise NumberParseException(NumberParseException.NOT_A_NUMBER,\n                                   \"The phone number supplied was None.\")\n    elif len(number) > _MAX_INPUT_STRING_LENGTH:\n        raise NumberParseException(NumberParseException.TOO_LONG,\n                                   \"The string supplied was too long to parse.\")\n\n    national_number = _build_national_number_for_parsing(number)\n\n    if not _is_viable_phone_number(national_number):\n        raise NumberParseException(NumberParseException.NOT_A_NUMBER,\n                                   \"The string supplied did not seem to be a phone number.\")\n\n    # Check the region supplied is valid, or that the extracted number starts\n    # with some sort of + sign so the number's region can be determined.\n    if _check_region and not _check_region_for_parsing(national_number, region):\n        raise NumberParseException(NumberParseException.INVALID_COUNTRY_CODE,\n                                   \"Missing or invalid default region.\")\n    if keep_raw_input:\n        numobj.raw_input = number\n\n    # Attempt to parse extension first, since it doesn't require\n    # region-specific data and we want to have the non-normalised number here.\n    extension, national_number = _maybe_strip_extension(national_number)\n    if len(extension) > 0:\n        numobj.extension = extension\n    if region is None:\n        metadata = None\n    else:\n        metadata = PhoneMetadata.metadata_for_region(region.upper(), None)\n\n    country_code = 0\n    try:\n        country_code, normalized_national_number = _maybe_extract_country_code(national_number,\n                                                                               metadata,\n                                                                               keep_raw_input,\n                                                                               numobj)\n    except NumberParseException:\n        _, e, _ = sys.exc_info()\n        matchobj = _PLUS_CHARS_PATTERN.match(national_number)\n        if (e.error_type == NumberParseException.INVALID_COUNTRY_CODE and\n            matchobj is not None):\n            # Strip the plus-char, and try again.\n            country_code, normalized_national_number = _maybe_extract_country_code(national_number[matchobj.end():],\n                                                                                   metadata,\n                                                                                   keep_raw_input,\n                                                                                   numobj)\n            if country_code == 0:\n                raise NumberParseException(NumberParseException.INVALID_COUNTRY_CODE,\n                                           \"Could not interpret numbers after plus-sign.\")\n        else:\n            raise\n\n    if country_code != 0:\n        number_region = region_code_for_country_code(country_code)\n        if number_region != region:\n            # Metadata cannot be null because the country calling code is valid.\n            metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, number_region)\n    else:\n        # If no extracted country calling code, use the region supplied\n        # instead. The national number is just the normalized version of the\n        # number we were given to parse.\n        normalized_national_number += _normalize(national_number)\n        if region is not None:\n            country_code = metadata.country_code\n            numobj.country_code = country_code\n        elif keep_raw_input:\n            numobj.country_code_source = CountryCodeSource.UNSPECIFIED\n\n    if len(normalized_national_number) < _MIN_LENGTH_FOR_NSN:\n        raise NumberParseException(NumberParseException.TOO_SHORT_NSN,\n                                   \"The string supplied is too short to be a phone number.\")\n    if metadata is not None:\n        potential_national_number = normalized_national_number\n        carrier_code, potential_national_number, _ = _maybe_strip_national_prefix_carrier_code(potential_national_number,\n                                                                                               metadata)\n        # We require that the NSN remaining after stripping the national\n        # prefix and carrier code be long enough to be a possible length for\n        # the region. Otherwise, we don't do the stripping, since the original\n        # number could be a valid short number.\n        validation_result = _test_number_length(potential_national_number, metadata)\n        if validation_result not in (ValidationResult.TOO_SHORT,\n                                     ValidationResult.IS_POSSIBLE_LOCAL_ONLY,\n                                     ValidationResult.INVALID_LENGTH):\n            normalized_national_number = potential_national_number\n            if keep_raw_input and carrier_code is not None and len(carrier_code) > 0:\n                numobj.preferred_domestic_carrier_code = carrier_code\n    len_national_number = len(normalized_national_number)\n    if len_national_number < _MIN_LENGTH_FOR_NSN:  # pragma no cover\n        # Check of _is_viable_phone_number() at the top of this function makes\n        # this effectively unhittable.\n        raise NumberParseException(NumberParseException.TOO_SHORT_NSN,\n                                   \"The string supplied is too short to be a phone number.\")\n    if len_national_number > _MAX_LENGTH_FOR_NSN:\n        raise NumberParseException(NumberParseException.TOO_LONG,\n                                   \"The string supplied is too long to be a phone number.\")\n    _set_italian_leading_zeros_for_phone_number(normalized_national_number, numobj)\n    numobj.national_number = to_long(normalized_national_number)\n    return numobj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string to a form that we can parse and return it if it is not already in RFC3966 ; otherwise return it.", "response": "def _build_national_number_for_parsing(number):\n    \"\"\"Converts number to a form that we can parse and return it if it is\n    written in RFC3966; otherwise extract a possible number out of it and return it.\"\"\"\n    index_of_phone_context = number.find(_RFC3966_PHONE_CONTEXT)\n    if index_of_phone_context >= 0:\n        phone_context_start = index_of_phone_context + len(_RFC3966_PHONE_CONTEXT)\n        # If the phone context contains a phone number prefix, we need to\n        # capture it, whereas domains will be ignored.\n        if (phone_context_start < (len(number) - 1) and\n            number[phone_context_start] == _PLUS_SIGN):\n            # Additional parameters might follow the phone context. If so, we\n            # will remove them here because the parameters after phone context\n            # are not important for parsing the phone number.\n            phone_context_end = number.find(U_SEMICOLON, phone_context_start)\n            if phone_context_end > 0:\n                national_number = number[phone_context_start:phone_context_end]\n            else:\n                national_number = number[phone_context_start:]\n        else:\n            national_number = U_EMPTY_STRING\n        # Now append everything between the \"tel:\" prefix and the\n        # phone-context. This should include the national number, an optional\n        # extension or isdn-subaddress component. Note we also handle the case\n        # when \"tel:\" is missing, as we have seen in some of the phone number\n        # inputs.  In that case we append everything from the beginning.\n        index_of_rfc3996_prefix = number.find(_RFC3966_PREFIX)\n        index_of_national_number = ((index_of_rfc3996_prefix + len(_RFC3966_PREFIX))\n                                    if (index_of_rfc3996_prefix >= 0) else 0)\n        national_number += number[index_of_national_number:index_of_phone_context]\n    else:\n        # Extract a possible number from the string passed in (this strips leading characters that\n        # could not be the start of a phone number.)\n        national_number = _extract_possible_number(number)\n\n    # Delete the isdn-subaddress and everything after it if it is\n    # present. Note extension won't appear at the same time with\n    # isdn-subaddress according to paragraph 5.3 of the RFC3966 spec,\n    index_of_isdn = national_number.find(_RFC3966_ISDN_SUBADDRESS)\n    if index_of_isdn > 0:\n        national_number = national_number[:index_of_isdn]\n    # If both phone context and isdn-subaddress are absent but other\n    # parameters are present, the parameters are left in national_number. This\n    # is because we are concerned about deleting content from a potential\n    # number string when there is no strong evidence that the number is\n    # actually written in RFC3966.\n    return national_number"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy_core_fields_only(inobj):\n    numobj = PhoneNumber()\n    numobj.country_code = inobj.country_code\n    numobj.national_number = inobj.national_number\n    if inobj.extension is not None and len(inobj.extension) > 0:\n        numobj.extension = inobj.extension\n    if inobj.italian_leading_zero:\n        numobj.italian_leading_zero = True\n        # This field is only relevant if there are leading zeros at all.\n        numobj.number_of_leading_zeros = inobj.number_of_leading_zeros\n        if numobj.number_of_leading_zeros is None:\n            # No number set is implicitly a count of 1; make it explicit.\n            numobj.number_of_leading_zeros = 1\n    return numobj", "response": "Returns a new phone number containing only the fields needed to uniquely\n    identify a phone number rather than any fields that capture the context in\n    which the phone number was created."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking two phone number objects and compares them for equality. Returns a MatchType value.", "response": "def _is_number_match_OO(numobj1_in, numobj2_in):\n    \"\"\"Takes two phone number objects and compares them for equality.\"\"\"\n    # We only care about the fields that uniquely define a number, so we copy these across explicitly.\n    numobj1 = _copy_core_fields_only(numobj1_in)\n    numobj2 = _copy_core_fields_only(numobj2_in)\n\n    # Early exit if both had extensions and these are different.\n    if (numobj1.extension is not None and\n        numobj2.extension is not None and\n        numobj1.extension != numobj2.extension):\n        return MatchType.NO_MATCH\n\n    country_code1 = numobj1.country_code\n    country_code2 = numobj2.country_code\n    # Both had country_code specified.\n    if country_code1 != 0 and country_code2 != 0:\n        if numobj1 == numobj2:\n            return MatchType.EXACT_MATCH\n        elif (country_code1 == country_code2 and\n              _is_national_number_suffix_of_other(numobj1, numobj2)):\n            # A SHORT_NSN_MATCH occurs if there is a difference because of the\n            # presence or absence of an 'Italian leading zero', the presence\n            # or absence of an extension, or one NSN being a shorter variant\n            # of the other.\n            return MatchType.SHORT_NSN_MATCH\n        # This is not a match.\n        return MatchType.NO_MATCH\n\n    # Checks cases where one or both country_code fields were not\n    # specified. To make equality checks easier, we first set the country_code\n    # fields to be equal.\n    numobj1.country_code = country_code2\n    # If all else was the same, then this is an NSN_MATCH.\n    if numobj1 == numobj2:\n        return MatchType.NSN_MATCH\n    if _is_national_number_suffix_of_other(numobj1, numobj2):\n        return MatchType.SHORT_NSN_MATCH\n    return MatchType.NO_MATCH"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_national_number_suffix_of_other(numobj1, numobj2):\n    nn1 = str(numobj1.national_number)\n    nn2 = str(numobj2.national_number)\n    # Note that endswith returns True if the numbers are equal.\n    return nn1.endswith(nn2) or nn2.endswith(nn1)", "response": "Returns true when one national number is the suffix of the other."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_number_match_SS(number1, number2):\n    try:\n        numobj1 = parse(number1, UNKNOWN_REGION)\n        return _is_number_match_OS(numobj1, number2)\n    except NumberParseException:\n        _, exc, _ = sys.exc_info()\n        if exc.error_type == NumberParseException.INVALID_COUNTRY_CODE:\n            try:\n                numobj2 = parse(number2, UNKNOWN_REGION)\n                return _is_number_match_OS(numobj2, number1)\n            except NumberParseException:\n                _, exc2, _ = sys.exc_info()\n                if exc2.error_type == NumberParseException.INVALID_COUNTRY_CODE:\n                    try:\n                        numobj1 = parse(number1, None, keep_raw_input=False,\n                                        _check_region=False, numobj=None)\n                        numobj2 = parse(number2, None, keep_raw_input=False,\n                                        _check_region=False, numobj=None)\n                        return _is_number_match_OO(numobj1, numobj2)\n                    except NumberParseException:\n                        return MatchType.NOT_A_NUMBER\n\n    # One or more of the phone numbers we are trying to match is not a viable\n    # phone number.\n    return MatchType.NOT_A_NUMBER", "response": "Takes two phone numbers as strings and compares them for equality."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_number_match_OS(numobj1, number2):\n    # First see if the second number has an implicit country calling code, by\n    # attempting to parse it.\n    try:\n        numobj2 = parse(number2, UNKNOWN_REGION)\n        return _is_number_match_OO(numobj1, numobj2)\n    except NumberParseException:\n        _, exc, _ = sys.exc_info()\n        if exc.error_type == NumberParseException.INVALID_COUNTRY_CODE:\n            # The second number has no country calling code. EXACT_MATCH is no\n            # longer possible.  We parse it as if the region was the same as\n            # that for the first number, and if EXACT_MATCH is returned, we\n            # replace this with NSN_MATCH.\n            region1 = region_code_for_country_code(numobj1.country_code)\n            try:\n                if region1 != UNKNOWN_REGION:\n                    numobj2 = parse(number2, region1)\n                    match = _is_number_match_OO(numobj1, numobj2)\n                    if match == MatchType.EXACT_MATCH:\n                        return MatchType.NSN_MATCH\n                    else:\n                        return match\n                else:\n                    # If the first number didn't have a valid country calling\n                    # code, then we parse the second number without one as\n                    # well.\n                    numobj2 = parse(number2, None, keep_raw_input=False,\n                                    _check_region=False, numobj=None)\n                    return _is_number_match_OO(numobj1, numobj2)\n            except NumberParseException:\n                return MatchType.NOT_A_NUMBER\n    # One or more of the phone numbers we are trying to match is not a viable\n    # phone number.\n    return MatchType.NOT_A_NUMBER", "response": "Wrapper variant of _is_number_match_OO that copes with onePhoneNumber object and one string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the phone numbers num1 and num2 are equal.", "response": "def is_number_match(num1, num2):\n    \"\"\"Takes two phone numbers and compares them for equality.\n\n    For example, the numbers +1 345 657 1234 and 657 1234 are a SHORT_NSN_MATCH.\n    The numbers +1 345 657 1234 and 345 657 are a NO_MATCH.\n\n    Arguments\n    num1 -- First number object or string to compare. Can contain formatting,\n              and can have country calling code specified with + at the start.\n    num2 -- Second number object or string to compare. Can contain formatting,\n              and can have country calling code specified with + at the start.\n\n    Returns:\n     - EXACT_MATCH if the country_code, NSN, presence of a leading zero for\n       Italian numbers and any extension present are the same.\n     - NSN_MATCH if either or both has no region specified, and the NSNs and\n       extensions are the same.\n     - SHORT_NSN_MATCH if either or both has no region specified, or the\n       region specified is the same, and one NSN could be a shorter version of\n       the other number. This includes the case where one has an extension\n       specified, and the other does not.\n     - NO_MATCH otherwise.\n     \"\"\"\n    if isinstance(num1, PhoneNumber) and isinstance(num2, PhoneNumber):\n        return _is_number_match_OO(num1, num2)\n    elif isinstance(num1, PhoneNumber):\n        return _is_number_match_OS(num1, num2)\n    elif isinstance(num2, PhoneNumber):\n        return _is_number_match_OS(num2, num1)\n    else:\n        return _is_number_match_SS(num1, num2)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef can_be_internationally_dialled(numobj):\n    metadata = PhoneMetadata.metadata_for_region(region_code_for_number(numobj), None)\n    if metadata is None:\n        # Note numbers belonging to non-geographical entities (e.g. +800\n        # numbers) are always internationally diallable, and will be caught\n        # here.\n        return True\n    nsn = national_significant_number(numobj)\n    return not _is_number_matching_desc(nsn, metadata.no_international_dialling)", "response": "Returns True if the number can only be dialled from within the region and that the number is not in the internationally diallable region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_mobile_number_portable_region(region_code):\n    metadata = PhoneMetadata.metadata_for_region(region_code, None)\n    if metadata is None:\n        return False\n    return metadata.mobile_number_portable_region", "response": "Returns true if the supplied region supports mobile number portability."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _match_national_number(number, number_desc, allow_prefix_match):\n    # We don't want to consider it a prefix match when matching non-empty input against an empty\n    # pattern.\n    if number_desc is None or number_desc.national_number_pattern is None or len(number_desc.national_number_pattern) == 0:\n        return False\n    return _match(number, re.compile(number_desc.national_number_pattern), allow_prefix_match)", "response": "Returns whether the given number matches the given PhoneNumberDesc object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef time_zones_for_geographical_number(numobj):\n    e164_num = format_number(numobj, PhoneNumberFormat.E164)\n    if not e164_num.startswith(U_PLUS):  # pragma no cover\n        # Can only hit this arm if there's an internal error in the rest of\n        # the library\n        raise Exception(\"Expect E164 number to start with +\")\n    for prefix_len in range(TIMEZONE_LONGEST_PREFIX, 0, -1):\n        prefix = e164_num[1:(1 + prefix_len)]\n        if prefix in TIMEZONE_DATA:\n            return TIMEZONE_DATA[prefix]\n    return _UNKNOWN_TIME_ZONE_LIST", "response": "Returns a list of time zones to which a phone number belongs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of time zones corresponding to the country calling code of a number.", "response": "def _country_level_time_zones_for_number(numobj):\n    \"\"\"Returns the list of time zones corresponding to the country calling code of a number.\n    Arguments:\n    numobj -- the phone number to look up\n    Returns a list of the corresponding time zones or a single element list with the default\n    unknown time zone if no other time zone was found or if the number was invalid\"\"\"\n    cc = str(numobj.country_code)\n    for prefix_len in range(TIMEZONE_LONGEST_PREFIX, 0, -1):\n        prefix = cc[:(1 + prefix_len)]\n        if prefix in TIMEZONE_DATA:\n            return TIMEZONE_DATA[prefix]\n    return _UNKNOWN_TIME_ZONE_LIST"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines whether the given Unicode character is a Unicode letter.", "response": "def is_letter(uni_char):\n    \"\"\"Determine whether the given Unicode character is a Unicode letter\"\"\"\n    category = Category.get(uni_char)\n    return (category == Category.UPPERCASE_LETTER or\n            category == Category.LOWERCASE_LETTER or\n            category == Category.TITLECASE_LETTER or\n            category == Category.MODIFIER_LETTER or\n            category == Category.OTHER_LETTER)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef digit(uni_char, default_value=None):\n    uni_char = unicod(uni_char)  # Force to Unicode.\n    if default_value is not None:\n        return unicodedata.digit(uni_char, default_value)\n    else:\n        return unicodedata.digit(uni_char)", "response": "Returns the digit value assigned to the Unicode character uni_char as\n    integer. If default_value is not given the digit value is returned."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(cls, uni_char):\n        uni_char = unicod(uni_char)  # Force to Unicode\n        return unicod(unicodedata.category(uni_char))", "response": "Return the general category code for the given Unicode character"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cls, uni_char):\n        uni_char = unicod(uni_char)  # Force to Unicode\n        code_point = ord(uni_char)\n        if Block._RANGE_KEYS is None:\n            Block._RANGE_KEYS = sorted(Block._RANGES.keys())\n        idx = bisect.bisect_left(Block._RANGE_KEYS, code_point)\n        if (idx > 0 and\n            code_point >= Block._RANGES[Block._RANGE_KEYS[idx - 1]].start and\n            code_point <= Block._RANGES[Block._RANGE_KEYS[idx - 1]].end):\n            return Block._RANGES[Block._RANGE_KEYS[idx - 1]]\n        elif (idx < len(Block._RANGES) and\n              code_point >= Block._RANGES[Block._RANGE_KEYS[idx]].start and\n              code_point <= Block._RANGES[Block._RANGE_KEYS[idx]].end):\n            return Block._RANGES[Block._RANGE_KEYS[idx]]\n        else:\n            return Block.UNKNOWN", "response": "Return the Unicode block of the given Unicode character"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a carrier name for the given phone number object in the given language code script and region.", "response": "def name_for_valid_number(numobj, lang, script=None, region=None):\n    \"\"\"Returns a carrier name for the given PhoneNumber object, in the\n    language provided.\n\n    The carrier name is the one the number was originally allocated to,\n    however if the country supports mobile number portability the number might\n    not belong to the returned carrier anymore. If no mapping is found an\n    empty string is returned.\n\n    This method assumes the validity of the number passed in has already been\n    checked, and that the number is suitable for carrier lookup. We consider\n    mobile and pager numbers possible candidates for carrier lookup.\n\n    Arguments:\n    numobj -- The PhoneNumber object for which we want to get a carrier name.\n    lang -- A 2-letter lowercase ISO 639-1 language code for the language in\n                  which the description should be returned (e.g. \"en\")\n    script -- A 4-letter titlecase (first letter uppercase, rest lowercase)\n                  ISO script code as defined in ISO 15924, separated by an\n                  underscore (e.g. \"Hant\")\n    region --  A 2-letter uppercase ISO 3166-1 country code (e.g. \"GB\")\n\n    Returns a carrier name in the given language code, for the given phone\n    number, or an empty string if no description is available.\n    \"\"\"\n    return _prefix_description_for_number(CARRIER_DATA, CARRIER_LONGEST_PREFIX,\n                                          numobj, lang, script, region)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a carrier name for the given phone number object in the given language code script and region.", "response": "def name_for_number(numobj, lang, script=None, region=None):\n    \"\"\"Returns a carrier name for the given PhoneNumber object, in the\n    language provided.\n\n    The carrier name is the one the number was originally allocated to,\n    however if the country supports mobile number portability the number might\n    not belong to the returned carrier anymore. If no mapping is found an\n    empty string is returned.\n\n    This function explicitly checks the validity of the number passed in\n\n    Arguments:\n    numobj -- The PhoneNumber object for which we want to get a carrier name.\n    lang -- A 2-letter lowercase ISO 639-1 language code for the language in\n                  which the description should be returned (e.g. \"en\")\n    script -- A 4-letter titlecase (first letter uppercase, rest lowercase)\n                  ISO script code as defined in ISO 15924, separated by an\n                  underscore (e.g. \"Hant\")\n    region --  A 2-letter uppercase ISO 3166-1 country code (e.g. \"GB\")\n\n    Returns a carrier name in the given language code, for the given phone\n    number, or an empty string if no description is available.\n    \"\"\"\n    ntype = number_type(numobj)\n    if _is_mobile(ntype):\n        return name_for_valid_number(numobj, lang, script, region)\n    return U_EMPTY_STRING"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the name of the carrier for the given PhoneNumber object only when the number is safe to display to users.", "response": "def safe_display_name(numobj, lang, script=None, region=None):\n    \"\"\"Gets the name of the carrier for the given PhoneNumber object only when\n    it is 'safe' to display to users.  A carrier name is onsidered safe if the\n    number is valid and for a region that doesn't support mobile number\n    portability (http://en.wikipedia.org/wiki/Mobile_number_portability).\n\n\n    This function explicitly checks the validity of the number passed in\n\n    Arguments:\n    numobj -- The PhoneNumber object for which we want to get a carrier name.\n    lang -- A 2-letter lowercase ISO 639-1 language code for the language in\n                  which the description should be returned (e.g. \"en\")\n    script -- A 4-letter titlecase (first letter uppercase, rest lowercase)\n                  ISO script code as defined in ISO 15924, separated by an\n                  underscore (e.g. \"Hant\")\n    region --  A 2-letter uppercase ISO 3166-1 country code (e.g. \"GB\")\n\n    Returns a carrier name that is safe to display to users, or the empty string.\n    \"\"\"\n    if is_mobile_number_portable_region(region_code_for_number(numobj)):\n        return U_EMPTY_STRING\n    return name_for_number(numobj, lang, script, region)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_mobile(ntype):\n    return (ntype == PhoneNumberType.MOBILE or\n            ntype == PhoneNumberType.FIXED_LINE_OR_MOBILE or\n            ntype == PhoneNumberType.PAGER)", "response": "Checks if the supplied number type supports carrier lookup"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear(self):\n        self.country_code = None\n        self.national_number = None\n        self.extension = None\n        self.italian_leading_zero = None\n        self.number_of_leading_zeros = None\n        self.raw_input = None\n        self.country_code_source = CountryCodeSource.UNSPECIFIED\n        self.preferred_domestic_carrier_code = None", "response": "Erase the contents of the object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmerging information from another PhoneNumber object into this one.", "response": "def merge_from(self, other):\n        \"\"\"Merge information from another PhoneNumber object into this one.\"\"\"\n        if other.country_code is not None:\n            self.country_code = other.country_code\n        if other.national_number is not None:\n            self.national_number = other.national_number\n        if other.extension is not None:\n            self.extension = other.extension\n        if other.italian_leading_zero is not None:\n            self.italian_leading_zero = other.italian_leading_zero\n        if other.number_of_leading_zeros is not None:\n            self.number_of_leading_zeros = other.number_of_leading_zeros\n        if other.raw_input is not None:\n            self.raw_input = other.raw_input\n        if other.country_code_source is not CountryCodeSource.UNSPECIFIED:\n            self.country_code_source = other.country_code_source\n        if other.preferred_domestic_carrier_code is not None:\n            self.preferred_domestic_carrier_code = other.preferred_domestic_carrier_code"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a representation of a Unicode string that can be used in both Python 2 and Python 3k allowing for use of the u function", "response": "def rpr(s):\n    \"\"\"Create a representation of a Unicode string that can be used in both\n    Python 2 and Python 3k, allowing for use of the u() function\"\"\"\n    if s is None:\n        return 'None'\n    seen_unicode = False\n    results = []\n    for cc in s:\n        ccn = ord(cc)\n        if ccn >= 32 and ccn < 127:\n            if cc == \"'\":  # escape single quote\n                results.append('\\\\')\n                results.append(cc)\n            elif cc == \"\\\\\":  # escape backslash\n                results.append('\\\\')\n                results.append(cc)\n            else:\n                results.append(cc)\n        else:\n            seen_unicode = True\n            if ccn <= 0xFFFF:\n                results.append('\\\\u')\n                results.append(\"%04x\" % ccn)\n            else:  # pragma no cover\n                results.append('\\\\U')\n                results.append(\"%08x\" % ccn)\n    result = \"'\" + \"\".join(results) + \"'\"\n    if seen_unicode:\n        return \"u(\" + result + \")\"\n    else:\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fullmatch(pattern, string, flags=0):\n    # Build a version of the pattern with a non-capturing group around it.\n    # This is needed to get m.end() to correctly report the size of the\n    # matched expression (as per the final doctest above).\n    grouped_pattern = re.compile(\"^(?:%s)$\" % pattern.pattern, pattern.flags)\n    m = grouped_pattern.match(string)\n    if m and m.end() < len(string):\n        # Incomplete match (which should never happen because of the $ at the\n        # end of the regexp), treat as failure.\n        m = None  # pragma no cover\n    return m", "response": "Try to apply the pattern at the start of the string returning a match\n    object if the whole string matches or None if no match was found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading per - prefix data from the given file for the given locale and prefix.", "response": "def load_locale_prefixdata_file(prefixdata, filename, locale=None, overall_prefix=None, separator=None):\n    \"\"\"Load per-prefix data from the given file, for the given locale and prefix.\n\n    We assume that this file:\n     - is encoded in UTF-8\n     - may have comment lines (starting with #) and blank lines\n     - has data lines of the form '<prefix>|<stringdata>'\n     - contains only data for prefixes that are extensions of the filename.\n\n    If overall_prefix is specified, lines are checked to ensure their prefix falls within this value.\n\n    If locale is specified, prefixdata[prefix][locale] is filled in; otherwise, just prefixdata[prefix].\n\n    If separator is specified, the string data will be split on this separator, and the output values\n    in the dict will be tuples of strings rather than strings.\n    \"\"\"\n    with open(filename, \"rb\") as infile:\n        lineno = 0\n        for line in infile:\n            uline = line.decode('utf-8')\n            lineno += 1\n            dm = DATA_LINE_RE.match(uline)\n            if dm:\n                prefix = dm.group('prefix')\n                stringdata = dm.group('stringdata')\n                if stringdata != stringdata.rstrip():\n                    print (\"%s:%d: Warning: stripping trailing whitespace\" % (filename, lineno))\n                    stringdata = stringdata.rstrip()\n                if overall_prefix is not None and not prefix.startswith(overall_prefix):\n                    raise Exception(\"%s:%d: Prefix %s is not within %s\" %\n                                    (filename, lineno, prefix, overall_prefix))\n                if separator is not None:\n                    stringdata = tuple(stringdata.split(separator))\n                if prefix not in prefixdata:\n                    prefixdata[prefix] = {}\n                if locale is not None:\n                    prefixdata[prefix][locale] = stringdata\n                else:\n                    prefixdata[prefix] = stringdata\n            elif BLANK_LINE_RE.match(uline):\n                pass\n            elif COMMENT_LINE_RE.match(uline):\n                pass\n            else:\n                raise Exception(\"%s:%d: Unexpected line format: %s\" %\n                                (filename, lineno, line))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading per - prefix data from the given top - level directory.", "response": "def load_locale_prefixdata(indir, separator=None):\n    \"\"\"Load per-prefix data from the given top-level directory.\n\n    Prefix data is assumed to be held in files <indir>/<locale>/<prefix>.txt.\n    The same prefix may occur in multiple files, giving the prefix's description\n    in different locales.\n    \"\"\"\n    prefixdata = {}  # prefix => dict mapping locale to description\n    for locale in os.listdir(indir):\n        if not os.path.isdir(os.path.join(indir, locale)):\n            continue\n        for filename in glob.glob(os.path.join(indir, locale, \"*%s\" % PREFIXDATA_SUFFIX)):\n            overall_prefix, ext = os.path.splitext(os.path.basename(filename))\n            load_locale_prefixdata_file(prefixdata, filename, locale, overall_prefix, separator)\n    return prefixdata"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string representation of a dict keyed by a string in sorted key order", "response": "def _stable_dict_repr(strdict):\n    \"\"\"Return a repr() for a dict keyed by a string, in sorted key order\"\"\"\n    lines = []\n    for key in sorted(strdict.keys()):\n        lines.append(\"'%s': %s\" % (key, rpr(strdict[key])))\n    return \"{%s}\" % \", \".join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a repr for a list or tuple", "response": "def _tuple_repr(data):\n    \"\"\"Return a repr() for a list/tuple\"\"\"\n    if len(data) == 1:\n        return \"(%s,)\" % rpr(data[0])\n    else:\n        return \"(%s)\" % \", \".join([rpr(x) for x in data])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noutput the per - prefix data in Python form to the given file.", "response": "def output_prefixdata_code(prefixdata, outfilename, module_prefix, varprefix, per_locale, chunks):\n    \"\"\"Output the per-prefix data in Python form to the given file \"\"\"\n    sorted_keys = sorted(prefixdata.keys())\n    total_keys = len(sorted_keys)\n    if chunks == -1:\n        chunk_size = PREFIXDATA_CHUNK_SIZE\n        total_chunks = int(math.ceil(total_keys / float(chunk_size)))\n    else:\n        chunk_size = int(math.ceil(total_keys / float(chunks)))\n        total_chunks = chunks\n\n    outdirname = os.path.dirname(outfilename)\n    longest_prefix = 0\n    for chunk_num in range(total_chunks):\n        chunk_index = chunk_size * chunk_num\n        chunk_keys = sorted_keys[chunk_index:chunk_index + chunk_size]\n        chunk_data = {}\n        for key in chunk_keys:\n            chunk_data[key] = prefixdata[key]\n        chunk_file = os.path.join(outdirname, 'data%d.py' % chunk_num)\n        chunk_longest = output_prefixdata_chunk(\n            chunk_data, chunk_file, module_prefix, per_locale)\n        if chunk_longest > longest_prefix:\n            longest_prefix = chunk_longest\n\n    with open(outfilename, \"w\") as outfile:\n        if per_locale:\n            prnt(PREFIXDATA_LOCALE_FILE_PROLOG % {'module': module_prefix}, file=outfile)\n        else:\n            prnt(PREFIXDATA_FILE_PROLOG % {'module': module_prefix}, file=outfile)\n        prnt(COPYRIGHT_NOTICE, file=outfile)\n        prnt(\"%s_DATA = {}\" % varprefix, file=outfile)\n        for chunk_num in range(total_chunks):\n            prnt(\"from .data%d import data\" % chunk_num, file=outfile)\n            prnt(\"%s_DATA.update(data)\" % varprefix, file=outfile)\n        prnt(\"del data\", file=outfile)\n        prnt(\"%s_LONGEST_PREFIX = %d\" % (varprefix, longest_prefix), file=outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the given input directory and emit generated code.", "response": "def _standalone(argv):\n    \"\"\"Parse the given input directory and emit generated code.\"\"\"\n    varprefix = \"GEOCODE\"\n    per_locale = True\n    separator = None\n    chunks = -1\n    try:\n        opts, args = getopt.getopt(argv, \"hv:fs:c:\", (\"help\", \"var=\", \"flat\", \"sep=\", \"chunks=\"))\n    except getopt.GetoptError:\n        prnt(__doc__, file=sys.stderr)\n        sys.exit(1)\n    for opt, arg in opts:\n        if opt in (\"-h\", \"--help\"):\n            prnt(__doc__, file=sys.stderr)\n            sys.exit(1)\n        elif opt in (\"-v\", \"--var\"):\n            varprefix = arg\n        elif opt in (\"-f\", \"--flat\"):\n            per_locale = False\n        elif opt in (\"-s\", \"--sep\"):\n            separator = arg\n        elif opt in (\"-c\", \"--chunks\"):\n            chunks = int(arg)\n        else:\n            prnt(\"Unknown option %s\" % opt, file=sys.stderr)\n            prnt(__doc__, file=sys.stderr)\n            sys.exit(1)\n    if len(args) != 3:\n        prnt(__doc__, file=sys.stderr)\n        sys.exit(1)\n    if per_locale:\n        prefixdata = load_locale_prefixdata(args[0], separator=separator)\n    else:\n        prefixdata = {}\n        load_locale_prefixdata_file(prefixdata, args[0], separator=separator)\n    output_prefixdata_code(prefixdata, args[1], args[2], varprefix, per_locale, chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a regular expression quantifier with an upper and lower limit.", "response": "def _limit(lower, upper):\n    \"\"\"Returns a regular expression quantifier with an upper and lower limit.\"\"\"\n    if ((lower < 0) or (upper <= 0) or (upper < lower)):\n        raise Exception(\"Illegal argument to _limit\")\n    return unicod(\"{%d,%d}\") % (lower, upper)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if number is a verified number according to the leniency.", "response": "def _verify(leniency, numobj, candidate, matcher):\n    \"\"\"Returns True if number is a verified number according to the\n    leniency.\"\"\"\n    if leniency == Leniency.POSSIBLE:\n        return is_possible_number(numobj)\n    elif leniency == Leniency.VALID:\n        if (not is_valid_number(numobj) or\n            not _contains_only_valid_x_chars(numobj, candidate)):\n            return False\n        return _is_national_prefix_present_if_required(numobj)\n    elif leniency == Leniency.STRICT_GROUPING:\n        return _verify_strict_grouping(numobj, candidate, matcher)\n    elif leniency == Leniency.EXACT_GROUPING:\n        return _verify_exact_grouping(numobj, candidate, matcher)\n    else:\n        raise Exception(\"Error: unsupported Leniency value %s\" % leniency)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if all the number groups of digits in the candidate match our expected number groups.", "response": "def _all_number_groups_remain_grouped(numobj, normalized_candidate, formatted_number_groups):\n    \"\"\"Returns True if the groups of digits found in our candidate phone number match our\n    expectations.\n\n    Arguments:\n    numobj -- the original number we found when parsing\n    normalized_candidate -- the candidate number, normalized to only contain ASCII digits,\n          but with non-digits (spaces etc) retained\n    expected_number_groups -- the groups of digits that we would expect to see if we\n          formatted this number\n    Returns True if expectations matched.\n    \"\"\"\n    from_index = 0\n    if numobj.country_code_source != CountryCodeSource.FROM_DEFAULT_COUNTRY:\n        # First skip the country code if the normalized candidate contained it.\n        country_code = str(numobj.country_code)\n        from_index = normalized_candidate.find(country_code) + len(country_code)\n    # Check each group of consecutive digits are not broken into separate\n    # groupings in the candidate string.\n    for ii, formatted_number_group in enumerate(formatted_number_groups):\n        # Fails if the substring of normalized_candidate starting from\n        # from_index doesn't contain the consecutive digits in\n        # formatted_number_group.\n        from_index = normalized_candidate.find(formatted_number_group, from_index)\n        if from_index < 0:\n            return False\n        # Moves from_index forward.\n        from_index += len(formatted_number_group)\n        if (ii == 0 and from_index < len(normalized_candidate)):\n            # We are at the position right after the NDC.  We get the region\n            # used for formatting information based on the country code in the\n            # phone number, rather than the number itself, as we do not need\n            # to distinguish between different countries with the same country\n            # calling code and this is faster.\n            region = region_code_for_country_code(numobj.country_code)\n            if (ndd_prefix_for_region(region, True) is not None and\n                normalized_candidate[from_index].isdigit()):\n                # This means there is no formatting symbol after the NDC. In\n                # this case, we only accept the number if there is no\n                # formatting symbol at all in the number, except for\n                # extensions.  This is only important for countries with\n                # national prefixes.\n                nsn = national_significant_number(numobj)\n                return normalized_candidate[(from_index - len(formatted_number_group)):].startswith(nsn)\n    # The check here makes sure that we haven't mistakenly already used the extension to\n    # match the last group of the subscriber number. Note the extension cannot have\n    # formatting in-between digits.\n    return (normalized_candidate[from_index:].find(numobj.extension or U_EMPTY_STRING) != -1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _all_number_groups_are_exactly_present(numobj, normalized_candidate, formatted_number_groups):\n    candidate_groups = re.split(NON_DIGITS_PATTERN, normalized_candidate)\n    # Set this to the last group, skipping it if the number has an extension.\n    if numobj.extension is not None:\n        candidate_number_group_index = len(candidate_groups) - 2\n    else:\n        candidate_number_group_index = len(candidate_groups) - 1\n    # First we check if the national significant number is formatted as a\n    # block.  We use contains and not equals, since the national significant\n    # number may be present with a prefix such as a national number prefix, or\n    # the country code itself.\n    if (len(candidate_groups) == 1 or\n        candidate_groups[candidate_number_group_index].find(national_significant_number(numobj)) != -1):\n        return True\n    # Starting from the end, go through in reverse, excluding the first group,\n    # and check the candidate and number groups are the same.\n    formatted_number_group_index = len(formatted_number_groups) - 1\n    while (formatted_number_group_index > 0 and candidate_number_group_index >= 0):\n        if (candidate_groups[candidate_number_group_index] !=\n            formatted_number_groups[formatted_number_group_index]):\n            return False\n        formatted_number_group_index -= 1\n        candidate_number_group_index -= 1\n    # Now check the first group. There may be a national prefix at the start, so we only check\n    # that the candidate group ends with the formatted number group.\n    return (candidate_number_group_index >= 0 and\n            candidate_groups[candidate_number_group_index].endswith(formatted_number_groups[0]))", "response": "Returns True if all the number groups of digits found in our candidate phone number are exactly present in our current language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_national_number_groups_without_pattern(numobj):\n    # This will be in the format +CC-DG1-DG2-DGX;ext=EXT where DG1..DGX represents groups of\n    # digits.\n    rfc3966_format = format_number(numobj, PhoneNumberFormat.RFC3966)\n    # We remove the extension part from the formatted string before splitting\n    # it into different groups.\n    end_index = rfc3966_format.find(U_SEMICOLON)\n    if end_index < 0:\n        end_index = len(rfc3966_format)\n\n    # The country-code will have a '-' following it.\n    start_index = rfc3966_format.find(U_DASH) + 1\n    return rfc3966_format[start_index:end_index].split(U_DASH)", "response": "Helper method to get the national - number part of a number formatted without any national national\n    prefix and return it as a set of digit blocks that would be formatted together following the standard formatting rules."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_national_number_groups(numobj, formatting_pattern):\n    # If a format is provided, we format the NSN only, and split that according to the separator.\n    nsn = national_significant_number(numobj)\n    return _format_nsn_using_pattern(nsn, formatting_pattern,\n                                     PhoneNumberFormat.RFC3966).split(U_DASH)", "response": "Helper method to get the national - number part of a number formatted without any national\n    prefix and return it as a set of digit blocks that should be formatted together according to the formatting pattern passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find(self, index):\n        match = _PATTERN.search(self.text, index)\n        while self._max_tries > 0 and match is not None:\n            start = match.start()\n            candidate = self.text[start:match.end()]\n\n            # Check for extra numbers at the end.\n            # TODO: This is the place to start when trying to support\n            # extraction of multiple phone number from split notations (+41 79\n            # 123 45 67 / 68).\n            candidate = self._trim_after_first_match(_SECOND_NUMBER_START_PATTERN,\n                                                     candidate)\n\n            match = self._extract_match(candidate, start)\n            if match is not None:\n                return match\n            # Move along\n            index = start + len(candidate)\n            self._max_tries -= 1\n            match = _PATTERN.search(self.text, index)\n        return None", "response": "Attempts to find the next subsequence in the searched sequence on or after index. Returns None if no match can be found."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _trim_after_first_match(self, pattern, candidate):\n        trailing_chars_match = pattern.search(candidate)\n        if trailing_chars_match:\n            candidate = candidate[:trailing_chars_match.start()]\n        return candidate", "response": "Trims away any characters after the first match of pattern in\n        candidate returning the trimmed version."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_latin_letter(cls, letter):\n        # Combining marks are a subset of non-spacing-mark\n        if (not is_letter(letter) and\n            Category.get(letter) != Category.NON_SPACING_MARK):\n            return False\n        block = Block.get(letter)\n        return (block == Block.BASIC_LATIN or\n                block == Block.LATIN_1_SUPPLEMENT or\n                block == Block.LATIN_EXTENDED_A or\n                block == Block.LATIN_EXTENDED_ADDITIONAL or\n                block == Block.LATIN_EXTENDED_B or\n                block == Block.COMBINING_DIACRITICAL_MARKS)", "response": "Helper method to determine if a character is a Latin - script letter. For our purposes combining marks should also return True\n        since we assume they have been added to a preceding Latin character."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to extract a match from a candidate string. Returns None if no match can be found.", "response": "def _extract_match(self, candidate, offset):\n        \"\"\"Attempts to extract a match from a candidate string.\n\n        Arguments:\n        candidate -- The candidate text that might contain a phone number.\n        offset -- The offset of candidate within self.text\n        Returns the match found, None if none can be found\n        \"\"\"\n        # Skip a match that is more likely a publication page reference or a\n        # date.\n        if (_SLASH_SEPARATED_DATES.search(candidate)):\n            return None\n\n        # Skip potential time-stamps.\n        if _TIME_STAMPS.search(candidate):\n            following_text = self.text[offset + len(candidate):]\n            if _TIME_STAMPS_SUFFIX.match(following_text):\n                return None\n\n        # Try to come up with a valid match given the entire candidate.\n        match = self._parse_and_verify(candidate, offset)\n        if match is not None:\n            return match\n\n        # If that failed, try to find an \"inner match\" -- there might be a\n        # phone number within this candidate.\n        return self._extract_inner_match(candidate, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to extract a match from candidate if the whole candidate does not qualify as a match. Returns None if none can be found.", "response": "def _extract_inner_match(self, candidate, offset):\n        \"\"\"Attempts to extract a match from candidate if the whole candidate\n        does not qualify as a match.\n\n        Arguments:\n        candidate -- The candidate text that might contain a phone number\n        offset -- The current offset of candidate within text\n        Returns the match found, None if none can be found\n        \"\"\"\n        for possible_inner_match in _INNER_MATCHES:\n            group_match = possible_inner_match.search(candidate)\n            is_first_match = True\n            while group_match and self._max_tries > 0:\n                if is_first_match:\n                    # We should handle any group before this one too.\n                    group = self._trim_after_first_match(_UNWANTED_END_CHAR_PATTERN,\n                                                         candidate[:group_match.start()])\n                    match = self._parse_and_verify(group, offset)\n                    if match is not None:\n                        return match\n                    self._max_tries -= 1\n                    is_first_match = False\n                group = self._trim_after_first_match(_UNWANTED_END_CHAR_PATTERN,\n                                                     group_match.group(1))\n                match = self._parse_and_verify(group, offset + group_match.start(1))\n                if match is not None:\n                    return match\n                self._max_tries -= 1\n                group_match = possible_inner_match.search(candidate, group_match.start() + 1)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_and_verify(self, candidate, offset):\n        try:\n            # Check the candidate doesn't contain any formatting which would\n            # indicate that it really isn't a phone number.\n            if (not fullmatch(_MATCHING_BRACKETS, candidate) or _PUB_PAGES.search(candidate)):\n                return None\n\n            # If leniency is set to VALID or stricter, we also want to skip\n            # numbers that are surrounded by Latin alphabetic characters, to\n            # skip cases like abc8005001234 or 8005001234def.\n            if self.leniency >= Leniency.VALID:\n                # If the candidate is not at the start of the text, and does\n                # not start with phone-number punctuation, check the previous\n                # character\n                if (offset > 0 and\n                    not _LEAD_PATTERN.match(candidate)):\n                    previous_char = self.text[offset - 1]\n                    # We return None if it is a latin letter or an invalid\n                    # punctuation symbol\n                    if (self._is_invalid_punctuation_symbol(previous_char) or\n                        self._is_latin_letter(previous_char)):\n                        return None\n                last_char_index = offset + len(candidate)\n                if last_char_index < len(self.text):\n                    next_char = self.text[last_char_index]\n                    if (self._is_invalid_punctuation_symbol(next_char) or\n                        self._is_latin_letter(next_char)):\n                        return None\n\n            numobj = parse(candidate, self.preferred_region, keep_raw_input=True)\n            if _verify(self.leniency, numobj, candidate, self):\n                # We used parse(keep_raw_input=True) to create this number,\n                # but for now we don't return the extra values parsed.\n                # TODO: stop clearing all values here and switch all users\n                # over to using raw_input rather than the raw_string of\n                # PhoneNumberMatch.\n                numobj.country_code_source = CountryCodeSource.UNSPECIFIED\n                numobj.raw_input = None\n                numobj.preferred_domestic_carrier_code = None\n                return PhoneNumberMatch(offset, candidate, numobj)\n        except NumberParseException:\n            # ignore and continue\n            pass\n        return None", "response": "Parses a phone number from the candidate using phonenumberutil. parse and validates it matches the leniency. Returns a PhoneNumberMatch object if parsing and verification succeed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_next(self):\n        if self._state == PhoneNumberMatcher._NOT_READY:\n            self._last_match = self._find(self._search_index)\n            if self._last_match is None:\n                self._state = PhoneNumberMatcher._DONE\n            else:\n                self._search_index = self._last_match.end\n                self._state = PhoneNumberMatcher._READY\n        return (self._state == PhoneNumberMatcher._READY)", "response": "Indicates whether there is another match available."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the next match ; raises Exception if no next match available", "response": "def next(self):\n        \"\"\"Return the next match; raises Exception if no next match available\"\"\"\n        # Check the state and find the next match as a side-effect if necessary.\n        if not self.has_next():\n            raise StopIteration(\"No next match\")\n        # Don't retain that memory any longer than necessary.\n        result = self._last_match\n        self._last_match = None\n        self._state = PhoneNumberMatcher._NOT_READY\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_possible_short_number_for_region(short_numobj, region_dialing_from):\n    if not _region_dialing_from_matches_number(short_numobj, region_dialing_from):\n        return False\n    metadata = PhoneMetadata.short_metadata_for_region(region_dialing_from)\n    if metadata is None:  # pragma no cover\n        return False\n    short_numlen = len(national_significant_number(short_numobj))\n    return (short_numlen in metadata.general_desc.possible_length)", "response": "Checks whether a short number is a possible number when dialled from a specific region."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_possible_short_number(numobj):\n    region_codes = region_codes_for_country_code(numobj.country_code)\n    short_number_len = len(national_significant_number(numobj))\n\n    for region in region_codes:\n        metadata = PhoneMetadata.short_metadata_for_region(region)\n        if metadata is None:\n            continue\n        if short_number_len in metadata.general_desc.possible_length:\n            return True\n    return False", "response": "Checks whether a short number is a possible number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests whether a short number matches a valid pattern in a region.", "response": "def is_valid_short_number_for_region(short_numobj, region_dialing_from):\n    \"\"\"Tests whether a short number matches a valid pattern in a region.\n\n    Note that this doesn't verify the number is actually in use, which is\n    impossible to tell by just looking at the number itself.\n\n    Arguments:\n    short_numobj -- the short number to check as a PhoneNumber object.\n    region_dialing_from -- the region from which the number is dialed\n\n    Return whether the short number matches a valid pattern\n    \"\"\"\n    if not _region_dialing_from_matches_number(short_numobj, region_dialing_from):\n        return False\n    metadata = PhoneMetadata.short_metadata_for_region(region_dialing_from)\n    if metadata is None:  # pragma no cover\n        return False\n    short_number = national_significant_number(short_numobj)\n    general_desc = metadata.general_desc\n    if not _matches_possible_number_and_national_number(short_number, general_desc):\n        return False\n    short_number_desc = metadata.short_code\n    if short_number_desc.national_number_pattern is None:  # pragma no cover\n        return False\n    return _matches_possible_number_and_national_number(short_number, short_number_desc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest whether a short number matches a valid pattern.", "response": "def is_valid_short_number(numobj):\n    \"\"\"Tests whether a short number matches a valid pattern.\n\n    If a country calling code is shared by multiple regions, this returns True\n    if it's valid in any of them. Note that this doesn't verify the number is\n    actually in use, which is impossible to tell by just looking at the number\n    itself. See is_valid_short_number_for_region for details.\n\n    Arguments:\n    numobj - the short number for which we want to test the validity\n\n    Return whether the short number matches a valid pattern\n    \"\"\"\n    region_codes = region_codes_for_country_code(numobj.country_code)\n    region_code = _region_code_for_short_number_from_region_list(numobj, region_codes)\n    if len(region_codes) > 1 and region_code is not None:\n        # If a matching region had been found for the phone number from among two or more regions,\n        # then we have already implicitly verified its validity for that region.\n        return True\n    return is_valid_short_number_for_region(numobj, region_code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the expected cost category for a given short number when dialled from a given region.", "response": "def expected_cost_for_region(short_numobj, region_dialing_from):\n    \"\"\"Gets the expected cost category of a short number when dialled from a\n    region (however, nothing is implied about its validity). If it is\n    important that the number is valid, then its validity must first be\n    checked using is_valid_short_number_for_region. Note that emergency\n    numbers are always considered toll-free.\n\n    Example usage:\n    short_number = \"110\"\n    region_code = \"FR\"\n    if phonenumbers.is_valid_short_number_for_region(short_number, region_code):\n        cost = phonenumbers.expected_cost(short_number, region_code)  # ShortNumberCost\n        # Do something with the cost information here.\n\n    Arguments:\n    short_numobj -- the short number for which we want to know the expected cost category\n              as a PhoneNumber object.\n    region_dialing_from -- the region from which the number is dialed\n\n    Return the expected cost category for that region of the short\n    number. Returns UNKNOWN_COST if the number does not match a cost\n    category. Note that an invalid number may match any cost category.\n    \"\"\"\n    if not _region_dialing_from_matches_number(short_numobj, region_dialing_from):\n        return ShortNumberCost.UNKNOWN_COST\n    # Note that region_dialing_from may be None, in which case metadata will also be None.\n    metadata = PhoneMetadata.short_metadata_for_region(region_dialing_from)\n    if metadata is None:  # pragma no cover\n        return ShortNumberCost.UNKNOWN_COST\n    short_number = national_significant_number(short_numobj)\n\n    # The possible lengths are not present for a particular sub-type if they match the general\n    # description; for this reason, we check the possible lengths against the general description\n    # first to allow an early exit if possible.\n    if not(len(short_number) in metadata.general_desc.possible_length):\n        return ShortNumberCost.UNKNOWN_COST\n\n    # The cost categories are tested in order of decreasing expense, since if\n    # for some reason the patterns overlap the most expensive matching cost\n    # category should be returned.\n    if _matches_possible_number_and_national_number(short_number, metadata.premium_rate):\n        return ShortNumberCost.PREMIUM_RATE\n    if _matches_possible_number_and_national_number(short_number, metadata.standard_rate):\n        return ShortNumberCost.STANDARD_RATE\n    if _matches_possible_number_and_national_number(short_number, metadata.toll_free):\n        return ShortNumberCost.TOLL_FREE\n    if is_emergency_number(short_number, region_dialing_from):  # pragma no cover\n        # Emergency numbers are implicitly toll-free.\n        return ShortNumberCost.TOLL_FREE\n    return ShortNumberCost.UNKNOWN_COST"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the expected cost category of a given short number in a given region.", "response": "def expected_cost(numobj):\n    \"\"\"Gets the expected cost category of a short number (however, nothing is\n    implied about its validity). If the country calling code is unique to a\n    region, this method behaves exactly the same as\n    expected_cost_for_region. However, if the country calling code is\n    shared by multiple regions, then it returns the highest cost in the\n    sequence PREMIUM_RATE, UNKNOWN_COST, STANDARD_RATE, TOLL_FREE. The reason\n    for the position of UNKNOWN_COST in this order is that if a number is\n    UNKNOWN_COST in one region but STANDARD_RATE or TOLL_FREE in another, its\n    expected cost cannot be estimated as one of the latter since it might be a\n    PREMIUM_RATE number.\n\n    For example, if a number is STANDARD_RATE in the US, but TOLL_FREE in\n    Canada, the expected cost returned by this method will be STANDARD_RATE,\n    since the NANPA countries share the same country calling code.\n\n    Note: If the region from which the number is dialed is known, it is highly preferable to call\n    expected_cost_for_region instead.\n\n    Arguments:\n    numobj -- the short number for which we want to know the expected cost category\n\n    Return the highest expected cost category of the short number in the\n    region(s) with the given country calling code\n    \"\"\"\n    region_codes = region_codes_for_country_code(numobj.country_code)\n    if len(region_codes) == 0:\n        return ShortNumberCost.UNKNOWN_COST\n    if len(region_codes) == 1:\n        return expected_cost_for_region(numobj, region_codes[0])\n    cost = ShortNumberCost.TOLL_FREE\n    for region_code in region_codes:\n        cost_for_region = expected_cost_for_region(numobj, region_code)\n        if cost_for_region == ShortNumberCost.PREMIUM_RATE:\n            return ShortNumberCost.PREMIUM_RATE\n        elif cost_for_region == ShortNumberCost.UNKNOWN_COST:\n            return ShortNumberCost.UNKNOWN_COST\n        elif cost_for_region == ShortNumberCost.STANDARD_RATE:\n            if cost != ShortNumberCost.UNKNOWN_COST:\n                cost = ShortNumberCost.STANDARD_RATE\n        elif cost_for_region == ShortNumberCost.TOLL_FREE:\n            # Do nothing\n            pass\n        else:  # pragma no cover\n            raise Exception(\"Unrecognized cost for region: %s\", cost_for_region)\n    return cost"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a valid short number for the specified region.", "response": "def _example_short_number(region_code):\n    \"\"\"Gets a valid short number for the specified region.\n\n    Arguments:\n    region_code -- the region for which an example short number is needed.\n\n    Returns a valid short number for the specified region. Returns an empty\n    string when the metadata does not contain such information.\n    \"\"\"\n    metadata = PhoneMetadata.short_metadata_for_region(region_code)\n    if metadata is None:\n        return U_EMPTY_STRING\n    desc = metadata.short_code\n    if desc.example_number is not None:\n        return desc.example_number\n    return U_EMPTY_STRING"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an example short number for the specified cost category.", "response": "def _example_short_number_for_cost(region_code, cost):\n    \"\"\"Gets a valid short number for the specified cost category.\n\n    Arguments:\n    region_code -- the region for which an example short number is needed.\n    cost -- the cost category of number that is needed.\n\n    Returns a valid short number for the specified region and cost\n    category. Returns an empty string when the metadata does not contain such\n    information, or the cost is UNKNOWN_COST.\n    \"\"\"\n    metadata = PhoneMetadata.short_metadata_for_region(region_code)\n    if metadata is None:\n        return U_EMPTY_STRING\n    desc = None\n    if cost == ShortNumberCost.TOLL_FREE:\n        desc = metadata.toll_free\n    elif cost == ShortNumberCost.STANDARD_RATE:\n        desc = metadata.standard_rate\n    elif cost == ShortNumberCost.PREMIUM_RATE:\n        desc = metadata.premium_rate\n    else:\n        # ShortNumberCost.UNKNOWN_COST numbers are computed by the process of\n        # elimination from the other cost categoried.\n        pass\n    if desc is not None and desc.example_number is not None:\n        return desc.example_number\n    return U_EMPTY_STRING"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_carrier_specific(numobj):\n    region_codes = region_codes_for_country_code(numobj.country_code)\n    region_code = _region_code_for_short_number_from_region_list(numobj, region_codes)\n    national_number = national_significant_number(numobj)\n    metadata = PhoneMetadata.short_metadata_for_region(region_code)\n    return (metadata is not None and\n            _matches_possible_number_and_national_number(national_number, metadata.carrier_specific))", "response": "Determines whether a valid short number is a carrier - specific number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a valid short number determines whether it is carrier - specific when the given number is dialed from the given region.", "response": "def is_carrier_specific_for_region(numobj, region_dialing_from):\n    \"\"\"Given a valid short number, determines whether it is carrier-specific when\n    dialed from the given region (however, nothing is implied about its\n    validity). Carrier-specific numbers may connect to a different end-point,\n    or not connect at all, depending on the user's carrier. If it is important\n    that the number is valid, then its validity must first be checked using\n    isValidShortNumber or isValidShortNumberForRegion. Returns false if the\n    number doesn't match the region provided.\n\n    Arguments:\n    numobj -- the valid short number to check\n    region_dialing_from -- the region from which the number is dialed\n\n    Returns whether the short number is carrier-specific, assuming the input\n    was a valid short number.\n    \"\"\"\n    if not _region_dialing_from_matches_number(numobj, region_dialing_from):\n        return False\n    national_number = national_significant_number(numobj)\n    metadata = PhoneMetadata.short_metadata_for_region(region_dialing_from)\n    return (metadata is not None and\n            _matches_possible_number_and_national_number(national_number, metadata.carrier_specific))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_sms_service_for_region(numobj, region_dialing_from):\n    if not _region_dialing_from_matches_number(numobj, region_dialing_from):\n        return False\n    metadata = PhoneMetadata.short_metadata_for_region(region_dialing_from)\n    return (metadata is not None and\n            _matches_possible_number_and_national_number(national_significant_number(numobj), metadata.sms_services))", "response": "Returns True if the number is an SMS service in the provided region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind an asset by key", "response": "def find(cls, key=None, **kwargs):\n        \"\"\"\n        Find an asset by key\n        E.g.\n            shopify.Asset.find('layout/theme.liquid', theme_id=99)\n        \"\"\"\n        if not key:\n            return super(Asset, cls).find(**kwargs)\n\n        params = {\"asset[key]\": key}\n        params.update(kwargs)\n        theme_id = params.get(\"theme_id\")\n        path_prefix = \"%s/themes/%s\" % (cls.site, theme_id) if theme_id else cls.site\n\n        resource = cls.find_one(\"%s/assets.%s\" % (path_prefix, cls.format.extension), **params)\n\n        if theme_id and resource:\n            resource._prefix_options[\"theme_id\"] = theme_id\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connection(cls):\n        local = cls._threadlocal\n        if not getattr(local, 'connection', None):\n            # Make sure these variables are no longer affected by other threads.\n            local.user = cls.user\n            local.password = cls.password\n            local.site = cls.site\n            local.timeout = cls.timeout\n            local.headers = cls.headers\n            local.format = cls.format\n            local.version = cls.version\n            local.url = cls.url\n            if cls.site is None:\n                raise ValueError(\"No shopify session is active\")\n            local.connection = ShopifyConnection(\n                cls.site, cls.user, cls.password, cls.timeout, cls.format)\n        return local.connection", "response": "HTTP connection for the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_prefix_source(cls):\n        try:\n            return cls.override_prefix()\n        except AttributeError:\n            if hasattr(cls, '_prefix_source'):\n                return cls.site + cls._prefix_source\n            else:\n                return cls.site", "response": "Return the prefix source by default derived from site."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_hmac(cls, params):\n        encoded_params = cls.__encoded_params_for_signature(params)\n        # Generate the hex digest for the sorted parameters using the secret.\n        return hmac.new(cls.secret.encode(), encoded_params.encode(), sha256).hexdigest()", "response": "Calculate the HMAC of the given parameters in line with Shopify s rules for OAuth authentication."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __encoded_params_for_signature(cls, params):\n        def encoded_pairs(params):\n            for k, v in six.iteritems(params):\n                if k == 'hmac':\n                    continue\n\n                if k.endswith('[]'):\n                    #foo[]=1&foo[]=2 has to be transformed as foo=[\"1\", \"2\"] note the whitespace after comma\n                    k = k.rstrip('[]')\n                    v = json.dumps(list(map(str, v)))\n\n                # escape delimiters to avoid tampering\n                k = str(k).replace(\"%\", \"%25\").replace(\"=\", \"%3D\")\n                v = str(v).replace(\"%\", \"%25\")\n                yield '{0}={1}'.format(k, v).replace(\"&\", \"%26\")\n\n        return \"&\".join(sorted(encoded_pairs(params)))", "response": "Sort and combine query parameters into a single string excluding those that should be removed and joining with '&'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the refund record for a given order.", "response": "def calculate(cls, order_id, shipping=None, refund_line_items=None):\n        \"\"\"\n        Calculates refund transactions based on line items and shipping.\n        When you want to create a refund, you should first use the calculate\n        endpoint to generate accurate refund transactions.\n\n        Args:\n           order_id: Order ID for which the Refund has to created.\n           shipping: Specify how much shipping to refund.\n           refund_line_items: A list of line item IDs and quantities to refund.\n        Returns:\n           Unsaved refund record\n        \"\"\"\n        data = {}\n        if shipping:\n            data['shipping'] = shipping\n        data['refund_line_items'] = refund_line_items or []\n        body = {'refund': data}\n        resource = cls.post(\n            \"calculate\", order_id=order_id, body=json.dumps(body).encode()\n        )\n        return cls(\n            cls.format.decode(resource.body),\n            prefix_options={'order_id': order_id}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help(cls, task=None):\n        if task is None:\n            usage_list = []\n            for task in iter(cls._tasks):\n                task_func = getattr(cls, task)\n                usage_string = \"  %s %s\" % (cls._prog, task_func.usage)\n                desc = task_func.__doc__.splitlines()[0]\n                usage_list.append((usage_string, desc))\n            max_len = functools.reduce(lambda m, item: max(m, len(item[0])), usage_list, 0)\n            print(\"Tasks:\")\n            cols = int(os.environ.get(\"COLUMNS\", 80))\n            for line, desc in usage_list:\n                task_func = getattr(cls, task)\n                if desc:\n                    line = \"%s%s  # %s\" % (line, \" \" * (max_len - len(line)), desc)\n                if len(line) > cols:\n                    line = line[:cols - 3] + \"...\"\n                print(line)\n        else:\n            task_func = getattr(cls, task)\n            print(\"Usage:\")\n            print(\"  %s %s\" % (cls._prog, task_func.usage))\n            print(\"\")\n            print(task_func.__doc__)", "response": "Describe available tasks or one specific task"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new Gift Card Adjustment to the current Gift Card", "response": "def add_adjustment(self, adjustment):\n        \"\"\"\n        Create a new Gift Card Adjustment\n        \"\"\"\n        resource = self.post(\"adjustments\", adjustment.encode())\n        return GiftCardAdjustment(GiftCard.format.decode(resource.body))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef atomic_create_file(file_path):\n    fd, tmp_file_path = tempfile.mkstemp(\n        prefix=os.path.basename(file_path), dir=os.path.dirname(file_path), suffix=\".metadata_tmp\"\n    )\n    try:\n        with os.fdopen(fd, \"w\") as out_file:\n            yield out_file\n\n        os.rename(tmp_file_path, file_path)\n    except Exception:  # pytest: disable=broad-except\n        with contextlib.suppress(Exception):\n            os.unlink(tmp_file_path)\n        raise", "response": "Open a temporary file for writing rename to final name when done"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstreaming contents of given key to given fileobj.", "response": "def _stream_blob(self, key, fileobj, progress_callback):\n        \"\"\"Streams contents of given key to given fileobj. Data is read sequentially in chunks\n        without any seeks. This requires duplicating some functionality of the Azure SDK, which only\n        allows reading entire blob into memory at once or returning data from random offsets\"\"\"\n        file_size = None\n        start_range = 0\n        chunk_size = self.conn.MAX_CHUNK_GET_SIZE\n        end_range = chunk_size - 1\n        while True:\n            try:\n                # pylint: disable=protected-access\n                blob = self.conn._get_blob(self.container_name, key, start_range=start_range, end_range=end_range)\n                if file_size is None:\n                    file_size = self._parse_length_from_content_range(blob.properties.content_range)\n                fileobj.write(blob.content)\n                start_range += blob.properties.content_length\n                if start_range == file_size:\n                    break\n                if blob.properties.content_length == 0:\n                    raise StorageError(\n                        \"Empty response received for {}, range {}-{}\".format(key, start_range, end_range)\n                    )\n                end_range += blob.properties.content_length\n                if end_range >= file_size:\n                    end_range = file_size - 1\n                if progress_callback:\n                    progress_callback(start_range, file_size)\n            except azure.common.AzureHttpError as ex:  # pylint: disable=no-member\n                if ex.status_code == 416:  # Empty file\n                    return\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_total_memory():\n    if platform.system() != \"Linux\":\n        return None\n\n    with open(\"/proc/meminfo\", \"r\") as in_file:\n        for line in in_file:\n            info = line.split()\n            if info[0] == \"MemTotal:\" and info[-1] == \"kB\":\n                memory_mb = int(int(info[1]) / 1024)\n                return memory_mb\n\n    return None", "response": "return total system memory in mebibytes or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_key_for_backend(self, key, remove_slash_prefix=False, trailing_slash=False):\n        path = self.prefix + key\n        if trailing_slash:\n            if not path or path[-1] != \"/\":\n                path += \"/\"\n        else:\n            path = path.rstrip(\"/\")\n        if remove_slash_prefix:  # Azure defines slashes in the beginning as \"dirs\" for listing purposes\n            path = path.lstrip(\"/\")\n        return path", "response": "Format the key for the backend"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_key_from_backend(self, key):\n        if not self.prefix:\n            return key\n        if not key.startswith(self.prefix):\n            raise StorageError(\"Key {!r} does not start with expected prefix {!r}\".format(key, self.prefix))\n        return key[len(self.prefix):]", "response": "Strip the configured prefix from a key retrieved from the backend and return it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_tree(self, key):\n        self.log.debug(\"Deleting tree: %r\", key)\n        names = [item[\"name\"] for item in self.list_path(key, with_metadata=False, deep=True)]\n        for name in names:\n            self.delete_key(name)", "response": "Delete all keys under given root key. Basic implementation works by just listing all available\n        keys and deleting them individually."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert non - string metadata values to strings and drop null values", "response": "def sanitize_metadata(self, metadata, replace_hyphen_with=\"-\"):\n        \"\"\"Convert non-string metadata values to strings and drop null values\"\"\"\n        return {str(k).replace(\"-\", replace_hyphen_with): str(v)\n                for k, v in (metadata or {}).items() if v is not None}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_timestamp(ts, *, with_tz=True, assume_local=False):\n    parse_result = dateutil.parser.parse(ts)\n\n    # pylint thinks dateutil.parser.parse always returns a tuple even though we didn't request it.\n    # So this check is pointless but convinces pylint that we really have a datetime object now.\n    dt = parse_result[0] if isinstance(parse_result, tuple) else parse_result  # pylint: disable=unsubscriptable-object\n\n    if with_tz is False:\n        if not dt.tzinfo:\n            return dt\n\n        tz = dateutil.tz.tzlocal() if assume_local else datetime.timezone.utc\n        return dt.astimezone(tz).replace(tzinfo=None)\n\n    if dt.tzinfo:\n        return dt\n\n    tz = dateutil.tz.tzlocal() if assume_local else datetime.timezone.utc\n    return dt.replace(tzinfo=tz)", "response": "Parse a given timestamp and return a datetime object with or without tzinfo."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a pgpass file for the given connection string or dict.", "response": "def create_pgpass_file(connection_string_or_info):\n    \"\"\"Look up password from the given object which can be a dict or a\n    string and write a possible password in a pgpass file;\n    returns a connection_string without a password in it\"\"\"\n    info = pgutil.get_connection_info(connection_string_or_info)\n    if \"password\" not in info:\n        return pgutil.create_connection_string(info)\n    linekey = \"{host}:{port}:{dbname}:{user}:\".format(\n        host=info.get(\"host\", \"localhost\"),\n        port=info.get(\"port\", 5432),\n        user=info.get(\"user\", \"\"),\n        dbname=info.get(\"dbname\", \"*\"))\n    pwline = \"{linekey}{password}\".format(linekey=linekey, password=info.pop(\"password\"))\n    pgpass_path = os.path.join(os.environ.get(\"HOME\"), \".pgpass\")\n    if os.path.exists(pgpass_path):\n        with open(pgpass_path, \"r\") as fp:\n            pgpass_lines = fp.read().splitlines()\n    else:\n        pgpass_lines = []\n    if pwline in pgpass_lines:\n        LOG.debug(\"Not adding authentication data to: %s since it's already there\", pgpass_path)\n    else:\n        # filter out any existing lines with our linekey and add the new line\n        pgpass_lines = [line for line in pgpass_lines if not line.startswith(linekey)] + [pwline]\n        content = \"\\n\".join(pgpass_lines) + \"\\n\"\n        with open(pgpass_path, \"w\") as fp:\n            os.fchmod(fp.fileno(), 0o600)\n            fp.write(content)\n        LOG.debug(\"Wrote %r to %r\", pwline, pgpass_path)\n    return pgutil.create_connection_string(info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses the input target_node_info entry which may be a libpq connection string or uri or a dict containing key value pairs of Taxonomy connection info entries or just the connection string with a replication slot name.", "response": "def connection_info_and_slot(target_node_info):\n    \"\"\"Process the input `target_node_info` entry which may be a libpq\n    connection string or uri, or a dict containing key:value pairs of\n    connection info entries or just the connection string with a replication\n    slot name.  Return the connection info dict and a possible slot.\"\"\"\n    slot = None\n    if isinstance(target_node_info, dict):\n        target_node_info = target_node_info.copy()\n        slot = target_node_info.pop(\"slot\", None)\n        if list(target_node_info) == [\"connection_string\"]:\n            # if the dict only contains the `connection_string` key use it as-is\n            target_node_info = target_node_info[\"connection_string\"]\n    connection_info = pgutil.get_connection_info(target_node_info)\n    return connection_info, slot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replication_connection_string_and_slot_using_pgpass(target_node_info):\n    connection_info, slot = connection_info_and_slot(target_node_info)\n    connection_info[\"dbname\"] = \"replication\"\n    connection_info[\"replication\"] = \"true\"\n    connection_string = create_pgpass_file(connection_info)\n    return connection_string, slot", "response": "Like connection_string_and_slot_using_pgpass but returns a\n    connection string and slot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_pg_command_version_to_number(command_version_string):\n    match = re.search(r\" \\(PostgreSQL\\) ([0-9]+(?:\\.[0-9]+)+)\", command_version_string)\n    if not match:\n        match = re.search(r\" \\(PostgreSQL\\) ([0-9]+)beta([0-9])\", command_version_string)\n        if not match:\n            raise Error(\"Unrecognized PostgreSQL version string {!r}\".format(command_version_string))\n\n    parts = match.group(1).split(\".\")  # ['9', '6', '5'] or ['10', '1']\n\n    if int(parts[0]) >= 10:  # PG 10+: just major and minor\n        if len(parts) == 1:  # development version above 10\n            return int(parts[0]) * 10000\n        return int(parts[0]) * 10000 + int(parts[1])\n    elif len(parts) == 2:\n        parts.append(\"0\")  # padding for development version numbers\n\n    return int(parts[0]) * 10000 + int(parts[1]) * 100 + int(parts[2])", "response": "Convert a PostgreSQL version string like PostgreSQL 9. 4. 4 to 90404."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef construct_wal_name(sysinfo):\n    log_hex, seg_hex = sysinfo[\"xlogpos\"].split(\"/\", 1)\n    # seg_hex's topmost 8 bits are filename, low 24 bits are position in\n    # file which we are not interested in\n    return name_for_tli_log_seg(\n        tli=int(sysinfo[\"timeline\"]),\n        log=int(log_hex, 16),\n        seg=int(seg_hex, 16) >> 24)", "response": "Construct wal file name from sysinfo dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        if self.state == \"CLOSED\":\n            return\n        self.flush()\n        # We close the stack of rohmu file wrappers, but leave the underlying real io object open to allow the\n        # caller to do something useful with it if they like, for example reading the output out of a BytesIO\n        # object or linking a temporary file to another name, etc.\n        if isinstance(self.next_fp, FileWrap):\n            self.next_fp.close()\n        self.next_fp = None\n        self.state = \"CLOSED\"", "response": "Close the rohmu file and flush the underlying file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nkeep metrics updated about how long time ago each filetype was successfully uploaded.", "response": "def transmit_metrics(self):\n        \"\"\"\n        Keep metrics updated about how long time ago each filetype was successfully uploaded.\n        Transmits max once per ten seconds, regardless of how many threads are running.\n        \"\"\"\n        global _last_stats_transmit_time  # pylint: disable=global-statement\n        with _STATS_LOCK:  # pylint: disable=not-context-manager\n            if time.monotonic() - _last_stats_transmit_time < 10.0:\n                return\n\n            for site in self.state:\n                for filetype, prop in self.state[site][\"upload\"].items():\n                    if prop[\"last_success\"]:\n                        self.metrics.gauge(\n                            \"pghoard.last_upload_age\",\n                            time.monotonic() - prop[\"last_success\"],\n                            tags={\n                                \"site\": site,\n                                \"type\": filetype,\n                            }\n                        )\n            _last_stats_transmit_time = time.monotonic()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_backup_end_segment_and_time(self, db_conn, backup_mode):\n        cursor = db_conn.cursor()\n\n        # Get backup end time and end segment and forcibly register a transaction in the current segment\n        # Note that we can't call pg_walfile_name() or pg_current_wal_lsn() in recovery\n        cursor.execute(\"SELECT now(), pg_is_in_recovery()\")\n        backup_end_time, in_recovery = cursor.fetchone()\n        if in_recovery:\n            db_conn.commit()\n            return None, backup_end_time\n\n        if self.pg_version_server >= 100000:\n            cursor.execute(\"SELECT pg_walfile_name(pg_current_wal_lsn()), txid_current()\")\n        else:\n            cursor.execute(\"SELECT pg_xlogfile_name(pg_current_xlog_location()), txid_current()\")\n        backup_end_wal_segment, _ = cursor.fetchone()\n        db_conn.commit()\n\n        # Now force switch of the WAL segment to make sure we have archived a segment with a known\n        # timestamp after pg_stop_backup() was called.\n        backup_end_name = \"pghoard_end_of_backup\"\n        if backup_mode == \"non-exclusive\":\n            cursor.execute(\"SELECT pg_start_backup(%s, true, false)\", [backup_end_name])\n            cursor.execute(\"SELECT pg_stop_backup(false)\")\n        elif backup_mode == \"pgespresso\":\n            cursor.execute(\"SELECT pgespresso_start_backup(%s, false)\", [backup_end_name])\n            backup_label = cursor.fetchone()[0]\n            cursor.execute(\"SELECT pgespresso_stop_backup(%s)\", [backup_label])\n        else:\n            cursor.execute(\"SELECT pg_start_backup(%s)\", [backup_end_name])\n            cursor.execute(\"SELECT pg_stop_backup()\")\n        db_conn.commit()\n\n        return backup_end_wal_segment, backup_end_time", "response": "Get the end time and WAL segment name after the end of the backup."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, data):\n        self._check_not_closed()\n        if not data:\n            return 0\n        enc_data = self.encryptor.update(data)\n        self.next_fp.write(enc_data)\n        self.offset += len(data)\n        return len(data)", "response": "Encrypt and write the given bytes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread up to size decrypted bytes.", "response": "def read(self, size=-1):\n        \"\"\"Read up to size decrypted bytes\"\"\"\n        self._check_not_closed()\n        if self.state == \"EOF\" or size == 0:\n            return b\"\"\n        elif size < 0:\n            return self._read_all()\n        else:\n            return self._read_block(size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CompressionFile(dst_fp, algorithm, level=0):\n    if algorithm == \"lzma\":\n        return lzma.open(dst_fp, \"w\", preset=level)\n\n    if algorithm == \"snappy\":\n        return SnappyFile(dst_fp, \"wb\")\n\n    if algorithm:\n        raise InvalidConfigurationError(\"invalid compression algorithm: {!r}\".format(algorithm))\n\n    return dst_fp", "response": "This function creates a class that instantiates a class based on algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tokenize_string(cls, string, separator):\n        results = []\n        token = \"\"\n        found_escape = False\n        for c in string:\n            if found_escape:\n                if c == separator:\n                    token += separator\n                else:\n                    token += \"\\\\\" + c\n                found_escape = False\n                continue\n            if c == \"\\\\\":\n                found_escape = True\n            elif c == separator:\n                results.append(token)\n                token = \"\"\n            else:\n                token += c\n\n        results.append(token)\n        return results", "response": "Split string with given separator unless the separator is escaped with backslash"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _object_client(self, *, not_found=None):\n        if self.gs_object_client is None:\n            if self.gs is None:\n                self.gs = self._init_google_client()\n            self.gs_object_client = self.gs.objects()  # pylint: disable=no-member\n\n        try:\n            yield self.gs_object_client\n        except HttpError as ex:\n            if ex.resp[\"status\"] == \"404\" and not_found is not None:\n                raise FileNotFoundFromStorageError(not_found)\n            if ex.resp[\"status\"] >= \"500\" and ex.resp[\"status\"] <= \"599\":\n                self.log.error(\"Received server error %r, resetting Google API client\", ex.resp[\"status\"])\n                self.gs = None\n                self.gs_object_client = None\n            raise", "response": "A context manager that re - initializes the object client if required."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating thru the request pages until all items have been processed", "response": "def _unpaginate(self, domain, initial_op, *, on_properties):\n        \"\"\"Iterate thru the request pages until all items have been processed\"\"\"\n        request = initial_op(domain)\n        while request is not None:\n            result = self._retry_on_reset(request, request.execute)\n            for on_property in on_properties:\n                items = result.get(on_property)\n                if items is not None:\n                    yield on_property, items\n            request = domain.list_next(request, result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlooks up the bucket if it already exists and try to create it if it doesn t.", "response": "def get_or_create_bucket(self, bucket_name):\n        \"\"\"Look up the bucket if it already exists and try to create the\n        bucket in case it doesn't.  Note that we can't just always try to\n        unconditionally create the bucket as Google imposes a strict rate\n        limit on bucket creation operations, even if it doesn't result in a\n        new bucket.\n\n        Quietly handle the case where the bucket already exists to avoid\n        race conditions.  Note that we'll get a 400 Bad Request response for\n        invalid bucket names (\"Invalid bucket name\") as well as for invalid\n        project (\"Invalid argument\"), try to handle both gracefully.\"\"\"\n        start_time = time.time()\n        gs_buckets = self.gs.buckets()  # pylint: disable=no-member\n        try:\n            request = gs_buckets.get(bucket=bucket_name)\n            self._retry_on_reset(request, request.execute)\n            self.log.debug(\"Bucket: %r already exists, took: %.3fs\", bucket_name, time.time() - start_time)\n        except HttpError as ex:\n            if ex.resp[\"status\"] == \"404\":\n                pass  # we need to create it\n            elif ex.resp[\"status\"] == \"403\":\n                raise InvalidConfigurationError(\"Bucket {0!r} exists but isn't accessible\".format(bucket_name))\n            else:\n                raise\n        else:\n            return bucket_name\n\n        try:\n            req = gs_buckets.insert(project=self.project_id, body={\"name\": bucket_name})\n            self._retry_on_reset(req, req.execute)\n            self.log.debug(\"Created bucket: %r successfully, took: %.3fs\", bucket_name, time.time() - start_time)\n        except HttpError as ex:\n            error = json.loads(ex.content.decode(\"utf-8\"))[\"error\"]\n            if error[\"message\"].startswith(\"You already own this bucket\"):\n                self.log.debug(\"Bucket: %r already exists, took: %.3fs\", bucket_name, time.time() - start_time)\n            elif error[\"message\"] == \"Invalid argument.\":\n                raise InvalidConfigurationError(\"Invalid project id {0!r}\".format(self.project_id))\n            elif error[\"message\"].startswith(\"Invalid bucket name\"):\n                raise InvalidConfigurationError(\"Invalid bucket name {0!r}\".format(bucket_name))\n            else:\n                raise\n\n        return bucket_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying all timeline history files on the server without checking if we have them.", "response": "def fetch_timeline_history_files(self, max_timeline):\n        \"\"\"Copy all timeline history files found on the server without\n           checking if we have them or not. The history files are very small\n           so reuploading them should not matter.\"\"\"\n        while max_timeline > 1:\n            self.c.execute(\"TIMELINE_HISTORY {}\".format(max_timeline))\n            timeline_history = self.c.fetchone()\n            history_filename = timeline_history[0]\n            history_data = timeline_history[1].tobytes()\n            self.log.debug(\"Received timeline history: %s for timeline %r\",\n                           history_filename, max_timeline)\n\n            compression_event = {\n                \"type\": \"CLOSE_WRITE\",\n                \"compress_to_memory\": True,\n                \"delete_file_after_compression\": False,\n                \"input_data\": BytesIO(history_data),\n                \"full_path\": history_filename,\n                \"site\": self.site,\n            }\n            self.compression_queue.put(compression_event)\n            max_timeline -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_backup_count_and_state(self, site):\n        basebackups = self.get_remote_basebackups_info(site)\n        self.log.debug(\"Found %r basebackups\", basebackups)\n\n        if basebackups:\n            last_backup_time = basebackups[-1][\"metadata\"][\"start-time\"]\n        else:\n            last_backup_time = None\n\n        allowed_basebackup_count = self.config[\"backup_sites\"][site][\"basebackup_count\"]\n        if allowed_basebackup_count is None:\n            allowed_basebackup_count = len(basebackups)\n\n        while len(basebackups) > allowed_basebackup_count:\n            self.log.warning(\"Too many basebackups: %d > %d, %r, starting to get rid of %r\",\n                             len(basebackups), allowed_basebackup_count, basebackups, basebackups[0][\"name\"])\n            basebackup_to_be_deleted = basebackups.pop(0)\n            pg_version = basebackup_to_be_deleted[\"metadata\"].get(\"pg-version\")\n            last_wal_segment_still_needed = 0\n            if basebackups:\n                last_wal_segment_still_needed = basebackups[0][\"metadata\"][\"start-wal-segment\"]\n\n            if last_wal_segment_still_needed:\n                self.delete_remote_wal_before(last_wal_segment_still_needed, site, pg_version)\n            self.delete_remote_basebackup(site, basebackup_to_be_deleted[\"name\"], basebackup_to_be_deleted[\"metadata\"])\n        self.state[\"backup_sites\"][site][\"basebackups\"] = basebackups\n\n        return last_backup_time", "response": "Check if there are any extra basebackups and delete them and return the datetime of the latest backup."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef startup_walk_for_missed_files(self):\n        for site in self.config[\"backup_sites\"]:\n            compressed_xlog_path, _ = self.create_backup_site_paths(site)\n            uncompressed_xlog_path = compressed_xlog_path + \"_incoming\"\n\n            # Process uncompressed files (ie WAL pg_receivexlog received)\n            for filename in os.listdir(uncompressed_xlog_path):\n                full_path = os.path.join(uncompressed_xlog_path, filename)\n                if not wal.WAL_RE.match(filename) and not wal.TIMELINE_RE.match(filename):\n                    self.log.warning(\"Found invalid file %r from incoming xlog directory\", full_path)\n                    continue\n                compression_event = {\n                    \"delete_file_after_compression\": True,\n                    \"full_path\": full_path,\n                    \"site\": site,\n                    \"src_path\": \"{}.partial\",\n                    \"type\": \"MOVE\",\n                }\n                self.log.debug(\"Found: %r when starting up, adding to compression queue\", compression_event)\n                self.compression_queue.put(compression_event)\n\n            # Process compressed files (ie things we've processed but not yet uploaded)\n            for filename in os.listdir(compressed_xlog_path):\n                if filename.endswith(\".metadata\"):\n                    continue  # silently ignore .metadata files, they're expected and processed below\n                full_path = os.path.join(compressed_xlog_path, filename)\n                metadata_path = full_path + \".metadata\"\n                is_xlog = wal.WAL_RE.match(filename)\n                is_timeline = wal.TIMELINE_RE.match(filename)\n                if not ((is_xlog or is_timeline) and os.path.exists(metadata_path)):\n                    self.log.warning(\"Found invalid file %r from compressed xlog directory\", full_path)\n                    continue\n                with open(metadata_path, \"r\") as fp:\n                    metadata = json.load(fp)\n\n                transfer_event = {\n                    \"file_size\": os.path.getsize(full_path),\n                    \"filetype\": \"xlog\" if is_xlog else \"timeline\",\n                    \"local_path\": full_path,\n                    \"metadata\": metadata,\n                    \"site\": site,\n                    \"type\": \"UPLOAD\",\n                }\n                self.log.debug(\"Found: %r when starting up, adding to transfer queue\", transfer_event)\n                self.transfer_queue.put(transfer_event)", "response": "Check xlog and xlog_incoming directories for files that receivexlog has received but not yet uploaded and process them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connection_info(info):\n    if isinstance(info, dict):\n        return info.copy()\n    elif info.startswith(\"postgres://\") or info.startswith(\"postgresql://\"):\n        return parse_connection_string_url(info)\n    else:\n        return parse_connection_string_libpq(info)", "response": "turn a connection info object into a dict or return it if it was a\n    dict already."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_connection_string_libpq(connection_string):\n    fields = {}\n    while True:\n        connection_string = connection_string.strip()\n        if not connection_string:\n            break\n        if \"=\" not in connection_string:\n            raise ValueError(\"expecting key=value format in connection_string fragment {!r}\".format(connection_string))\n        key, rem = connection_string.split(\"=\", 1)\n        if rem.startswith(\"'\"):\n            asis, value = False, \"\"\n            for i in range(1, len(rem)):\n                if asis:\n                    value += rem[i]\n                    asis = False\n                elif rem[i] == \"'\":\n                    break  # end of entry\n                elif rem[i] == \"\\\\\":\n                    asis = True\n                else:\n                    value += rem[i]\n            else:\n                raise ValueError(\"invalid connection_string fragment {!r}\".format(rem))\n            connection_string = rem[i + 1:]  # pylint: disable=undefined-loop-variable\n        else:\n            res = rem.split(None, 1)\n            if len(res) > 1:\n                value, connection_string = res\n            else:\n                value, connection_string = rem, \"\"\n        fields[key] = value\n    return fields", "response": "parse a postgresql connection string as defined in\n    http://www. postgresql. org / docs / current / static / libpq - connect. html#LIBPQ - CONNSTRING"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists available basebackups from a HTTP source", "response": "def list_basebackups_http(self, arg):\n        \"\"\"List available basebackups from a HTTP source\"\"\"\n        self.storage = HTTPRestore(arg.host, arg.port, arg.site)\n        self.storage.show_basebackup_list(verbose=arg.verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting basebackups from an object store", "response": "def list_basebackups(self, arg):\n        \"\"\"List basebackups from an object store\"\"\"\n        self.config = config.read_json_config_file(arg.config, check_commands=False, check_pgdata=False)\n        site = config.get_site_from_config(self.config, arg.site)\n        self.storage = self._get_object_storage(site, pgdata=None)\n        self.storage.show_basebackup_list(verbose=arg.verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading a basebackup from an object store", "response": "def get_basebackup(self, arg):\n        \"\"\"Download a basebackup from an object store\"\"\"\n        if not arg.tablespace_dir:\n            tablespace_mapping = {}\n        else:\n            try:\n                tablespace_mapping = dict(v.split(\"=\", 1) for v in arg.tablespace_dir)\n            except ValueError:\n                raise RestoreError(\"Invalid tablespace mapping {!r}\".format(arg.tablespace_dir))\n\n        self.config = config.read_json_config_file(arg.config, check_commands=False, check_pgdata=False)\n        site = config.get_site_from_config(self.config, arg.site)\n        try:\n            self.storage = self._get_object_storage(site, arg.target_dir)\n            self._get_basebackup(\n                pgdata=arg.target_dir,\n                basebackup=arg.basebackup,\n                site=site,\n                debug=arg.debug,\n                status_output_file=arg.status_output_file,\n                primary_conninfo=arg.primary_conninfo,\n                recovery_end_command=arg.recovery_end_command,\n                recovery_target_action=arg.recovery_target_action,\n                recovery_target_name=arg.recovery_target_name,\n                recovery_target_time=arg.recovery_target_time,\n                recovery_target_xid=arg.recovery_target_xid,\n                restore_to_master=arg.restore_to_master,\n                overwrite=arg.overwrite,\n                tablespace_mapping=tablespace_mapping,\n                tablespace_base_dir=arg.tablespace_base_dir,\n            )\n        except RestoreError:  # pylint: disable=try-except-raise\n            # Pass RestoreErrors thru\n            raise\n        except Exception as ex:\n            if self.log_tracebacks:\n                self.log.exception(\"Unexpected _get_basebackup failure\")\n            raise RestoreError(\"{}: {}\".format(ex.__class__.__name__, ex))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy a file - like object src to dst.", "response": "def copyfileobj(src, dst, length=None, exception=OSError):\n    \"\"\"Copy length bytes from fileobj src to fileobj dst.\n       If length is None, copy the entire content.\n    \"\"\"\n    if length == 0:\n        return\n    if length is None:\n        shutil.copyfileobj(src, dst)\n        return\n\n    # BUFSIZE = 16 * 1024\n    blocks, remainder = divmod(length, BUFSIZE)\n    # for b in range(blocks):\n    for _ in range(blocks):\n        buf = src.read(BUFSIZE)\n        if len(buf) < BUFSIZE:\n            raise exception(\"unexpected end of data\")\n        dst.write(buf)\n\n    if remainder != 0:\n        buf = src.read(remainder)\n        if len(buf) < remainder:\n            raise exception(\"unexpected end of data\")\n        dst.write(buf)\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts the object this key points to.", "response": "def get(self, dismiss=True):\n        \"\"\"Extract the object this key points to.\n\n        Objects are not read or decompressed until this function is explicitly called.\n        \"\"\"\n\n        try:\n            return _classof(self._context, self._fClassName).read(self._source, self._cursor.copied(), self._context, self)\n        finally:\n            if dismiss:\n                self._source.dismiss()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a group with the given store and optional chunk store.", "response": "def group(store=None, overwrite=False, chunk_store=None,\n          cache_attrs=True, synchronizer=None, path=None):\n    \"\"\"Create a group.\n\n    Parameters\n    ----------\n    store : MutableMapping or string, optional\n        Store or path to directory in file system.\n    overwrite : bool, optional\n        If True, delete any pre-existing data in `store` at `path` before\n        creating the group.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    synchronizer : object, optional\n        Array synchronizer.\n    path : string, optional\n        Group path within store.\n\n    Returns\n    -------\n    g : zarr.hierarchy.Group\n\n    Examples\n    --------\n    Create a group in memory::\n\n        >>> import zarr\n        >>> g = zarr.group()\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    Create a group with a different store::\n\n        >>> store = zarr.DirectoryStore('data/example.zarr')\n        >>> g = zarr.group(store=store, overwrite=True)\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    \"\"\"\n\n    # handle polymorphic store arg\n    store = _normalize_store_arg(store)\n    path = normalize_storage_path(path)\n\n    # require group\n    if overwrite or not contains_group(store):\n        init_group(store, overwrite=overwrite, chunk_store=chunk_store,\n                   path=path)\n\n    return Group(store, read_only=False, chunk_store=chunk_store,\n                 cache_attrs=cache_attrs, synchronizer=synchronizer, path=path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens a group using file - mode - like semantics.", "response": "def open_group(store=None, mode='a', cache_attrs=True, synchronizer=None, path=None,\n               chunk_store=None):\n    \"\"\"Open a group using file-mode-like semantics.\n\n    Parameters\n    ----------\n    store : MutableMapping or string, optional\n        Store or path to directory in file system or name of zip file.\n    mode : {'r', 'r+', 'a', 'w', 'w-'}, optional\n        Persistence mode: 'r' means read only (must exist); 'r+' means\n        read/write (must exist); 'a' means read/write (create if doesn't\n        exist); 'w' means create (overwrite if exists); 'w-' means create\n        (fail if exists).\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    synchronizer : object, optional\n        Array synchronizer.\n    path : string, optional\n        Group path within store.\n    chunk_store : MutableMapping or string, optional\n        Store or path to directory in file system or name of zip file.\n\n    Returns\n    -------\n    g : zarr.hierarchy.Group\n\n    Examples\n    --------\n    >>> import zarr\n    >>> root = zarr.open_group('data/example.zarr', mode='w')\n    >>> foo = root.create_group('foo')\n    >>> bar = root.create_group('bar')\n    >>> root\n    <zarr.hierarchy.Group '/'>\n    >>> root2 = zarr.open_group('data/example.zarr', mode='a')\n    >>> root2\n    <zarr.hierarchy.Group '/'>\n    >>> root == root2\n    True\n\n    \"\"\"\n\n    # handle polymorphic store arg\n    store = _normalize_store_arg(store)\n    if chunk_store is not None:\n        chunk_store = _normalize_store_arg(chunk_store)\n    path = normalize_storage_path(path)\n\n    # ensure store is initialized\n\n    if mode in ['r', 'r+']:\n        if contains_array(store, path=path):\n            err_contains_array(path)\n        elif not contains_group(store, path=path):\n            err_group_not_found(path)\n\n    elif mode == 'w':\n        init_group(store, overwrite=True, path=path, chunk_store=chunk_store)\n\n    elif mode == 'a':\n        if contains_array(store, path=path):\n            err_contains_array(path)\n        if not contains_group(store, path=path):\n            init_group(store, path=path, chunk_store=chunk_store)\n\n    elif mode in ['w-', 'x']:\n        if contains_array(store, path=path):\n            err_contains_array(path)\n        elif contains_group(store, path=path):\n            err_contains_group(path)\n        else:\n            init_group(store, path=path, chunk_store=chunk_store)\n\n    # determine read only status\n    read_only = mode == 'r'\n\n    return Group(store, read_only=read_only, cache_attrs=cache_attrs,\n                 synchronizer=synchronizer, path=path, chunk_store=chunk_store)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngroup name following h5py convention.", "response": "def name(self):\n        \"\"\"Group name following h5py convention.\"\"\"\n        if self._path:\n            # follow h5py convention: add leading slash\n            name = self._path\n            if name[0] != '/':\n                name = '/' + name\n            return name\n        return '/'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator over member names for groups only.", "response": "def group_keys(self):\n        \"\"\"Return an iterator over member names for groups only.\n\n        Examples\n        --------\n        >>> import zarr\n        >>> g1 = zarr.group()\n        >>> g2 = g1.create_group('foo')\n        >>> g3 = g1.create_group('bar')\n        >>> d1 = g1.create_dataset('baz', shape=100, chunks=10)\n        >>> d2 = g1.create_dataset('quux', shape=200, chunks=20)\n        >>> sorted(g1.group_keys())\n        ['bar', 'foo']\n\n        \"\"\"\n        for key in sorted(listdir(self._store, self._path)):\n            path = self._key_prefix + key\n            if contains_group(self._store, path):\n                yield key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator over the names and values of all groups in the cache.", "response": "def groups(self):\n        \"\"\"Return an iterator over (name, value) pairs for groups only.\n\n        Examples\n        --------\n        >>> import zarr\n        >>> g1 = zarr.group()\n        >>> g2 = g1.create_group('foo')\n        >>> g3 = g1.create_group('bar')\n        >>> d1 = g1.create_dataset('baz', shape=100, chunks=10)\n        >>> d2 = g1.create_dataset('quux', shape=200, chunks=20)\n        >>> for n, v in g1.groups():\n        ...     print(n, type(v))\n        bar <class 'zarr.hierarchy.Group'>\n        foo <class 'zarr.hierarchy.Group'>\n\n        \"\"\"\n        for key in sorted(listdir(self._store, self._path)):\n            path = self._key_prefix + key\n            if contains_group(self._store, path):\n                yield key, Group(self._store, path=path, read_only=self._read_only,\n                                 chunk_store=self._chunk_store,\n                                 cache_attrs=self.attrs.cache,\n                                 synchronizer=self._synchronizer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef array_keys(self):\n        for key in sorted(listdir(self._store, self._path)):\n            path = self._key_prefix + key\n            if contains_array(self._store, path):\n                yield key", "response": "Return an iterator over member names for arrays only."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arrays(self):\n        for key in sorted(listdir(self._store, self._path)):\n            path = self._key_prefix + key\n            if contains_array(self._store, path):\n                yield key, Array(self._store, path=path, read_only=self._read_only,\n                                 chunk_store=self._chunk_store,\n                                 cache_attrs=self.attrs.cache,\n                                 synchronizer=self._synchronizer)", "response": "Return an iterator over the names and values of all arrays in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visitvalues(self, func):\n\n        def _visit(obj):\n            yield obj\n            keys = sorted(getattr(obj, \"keys\", lambda: [])())\n            for k in keys:\n                for v in _visit(obj[k]):\n                    yield v\n\n        for each_obj in islice(_visit(self), 1, None):\n            value = func(each_obj)\n            if value is not None:\n                return value", "response": "Visit the values of the object and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit(self, func):\n\n        base_len = len(self.name)\n        return self.visitvalues(lambda o: func(o.name[base_len:].lstrip(\"/\")))", "response": "Visit each object s path and call func on each object s path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide a print -able display of the hierarchy of the current object.", "response": "def tree(self, expand=False, level=None):\n        \"\"\"Provide a ``print``-able display of the hierarchy.\n\n        Parameters\n        ----------\n        expand : bool, optional\n            Only relevant for HTML representation. If True, tree will be fully expanded.\n        level : int, optional\n            Maximum depth to descend into hierarchy.\n\n        Examples\n        --------\n        >>> import zarr\n        >>> g1 = zarr.group()\n        >>> g2 = g1.create_group('foo')\n        >>> g3 = g1.create_group('bar')\n        >>> g4 = g3.create_group('baz')\n        >>> g5 = g3.create_group('quux')\n        >>> d1 = g5.create_dataset('baz', shape=100, chunks=10)\n        >>> g1.tree()\n        /\n         \u251c\u2500\u2500 bar\n         \u2502   \u251c\u2500\u2500 baz\n         \u2502   \u2514\u2500\u2500 quux\n         \u2502       \u2514\u2500\u2500 baz (100,) float64\n         \u2514\u2500\u2500 foo\n        >>> g1.tree(level=2)\n        /\n         \u251c\u2500\u2500 bar\n         \u2502   \u251c\u2500\u2500 baz\n         \u2502   \u2514\u2500\u2500 quux\n         \u2514\u2500\u2500 foo\n        >>> g3.tree()\n        bar\n         \u251c\u2500\u2500 baz\n         \u2514\u2500\u2500 quux\n             \u2514\u2500\u2500 baz (100,) float64\n\n        Notes\n        -----\n        Please note that this is an experimental feature. The behaviour of this\n        function is still evolving and the default output and/or parameters may change\n        in future versions.\n\n        \"\"\"\n\n        return TreeViewer(self, expand=expand, level=level)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a sub - group.", "response": "def create_group(self, name, overwrite=False):\n        \"\"\"Create a sub-group.\n\n        Parameters\n        ----------\n        name : string\n            Group name.\n        overwrite : bool, optional\n            If True, overwrite any existing array with the given name.\n\n        Returns\n        -------\n        g : zarr.hierarchy.Group\n\n        Examples\n        --------\n        >>> import zarr\n        >>> g1 = zarr.group()\n        >>> g2 = g1.create_group('foo')\n        >>> g3 = g1.create_group('bar')\n        >>> g4 = g1.create_group('baz/quux')\n\n        \"\"\"\n\n        return self._write_op(self._create_group_nosync, name, overwrite=overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef require_group(self, name, overwrite=False):\n\n        return self._write_op(self._require_group_nosync, name,\n                              overwrite=overwrite)", "response": "Obtain a sub - group of the given name creating one if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_dataset(self, name, **kwargs):\n\n        return self._write_op(self._create_dataset_nosync, name, **kwargs)", "response": "Create an array of uninitialized versions of the object with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain an array creating if it doesn t exist.", "response": "def require_dataset(self, name, shape, dtype=None, exact=False, **kwargs):\n        \"\"\"Obtain an array, creating if it doesn't exist. Other `kwargs` are\n        as per :func:`zarr.hierarchy.Group.create_dataset`.\n\n        Parameters\n        ----------\n        name : string\n            Array name.\n        shape : int or tuple of ints\n            Array shape.\n        dtype : string or dtype, optional\n            NumPy dtype.\n        exact : bool, optional\n            If True, require `dtype` to match exactly. If false, require\n            `dtype` can be cast from array dtype.\n\n        \"\"\"\n\n        return self._write_op(self._require_dataset_nosync, name, shape=shape,\n                              dtype=dtype, exact=exact, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, name, **kwargs):\n        return self._write_op(self._create_nosync, name, **kwargs)", "response": "Create an array. Keyword arguments as per\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef empty(self, name, **kwargs):\n        return self._write_op(self._empty_nosync, name, **kwargs)", "response": "Create an array. Keyword arguments as per\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zeros(self, name, **kwargs):\n        return self._write_op(self._zeros_nosync, name, **kwargs)", "response": "Create an array. Keyword arguments as per\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ones(self, name, **kwargs):\n        return self._write_op(self._ones_nosync, name, **kwargs)", "response": "Create an array. Keyword arguments as per\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an array. Keyword arguments as per .", "response": "def full(self, name, fill_value, **kwargs):\n        \"\"\"Create an array. Keyword arguments as per\n        :func:`zarr.creation.full`.\"\"\"\n        return self._write_op(self._full_nosync, name, fill_value, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef array(self, name, data, **kwargs):\n        return self._write_op(self._array_nosync, name, data, **kwargs)", "response": "Create an array. Keyword arguments as per\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an array. Keyword arguments as per", "response": "def empty_like(self, name, data, **kwargs):\n        \"\"\"Create an array. Keyword arguments as per\n        :func:`zarr.creation.empty_like`.\"\"\"\n        return self._write_op(self._empty_like_nosync, name, data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef zeros_like(self, name, data, **kwargs):\n        return self._write_op(self._zeros_like_nosync, name, data, **kwargs)", "response": "Create an array. Keyword arguments as per\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an array. Keyword arguments as per .", "response": "def ones_like(self, name, data, **kwargs):\n        \"\"\"Create an array. Keyword arguments as per\n        :func:`zarr.creation.ones_like`.\"\"\"\n        return self._write_op(self._ones_like_nosync, name, data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an array. Keyword arguments as per", "response": "def full_like(self, name, data, **kwargs):\n        \"\"\"Create an array. Keyword arguments as per\n        :func:`zarr.creation.full_like`.\"\"\"\n        return self._write_op(self._full_like_nosync, name, data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves contents from one path to another relative to the Group.", "response": "def move(self, source, dest):\n        \"\"\"Move contents from one path to another relative to the Group.\n\n        Parameters\n        ----------\n        source : string\n            Name or path to a Zarr object to move.\n        dest : string\n            New name or path of the Zarr object.\n        \"\"\"\n\n        source = self._item_path(source)\n        dest = self._item_path(dest)\n\n        # Check that source exists.\n        if not (contains_array(self._store, source) or\n                contains_group(self._store, source)):\n            raise ValueError('The source, \"%s\", does not exist.' % source)\n        if contains_array(self._store, dest) or contains_group(self._store, dest):\n            raise ValueError('The dest, \"%s\", already exists.' % dest)\n\n        # Ensure groups needed for `dest` exist.\n        if \"/\" in dest:\n            self.require_group(\"/\" + dest.rsplit(\"/\", 1)[0])\n\n        self._write_op(self._move_nosync, source, dest)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve all attributes as a dictionary.", "response": "def asdict(self):\n        \"\"\"Retrieve all attributes as a dictionary.\"\"\"\n        if self.cache and self._cached_asdict is not None:\n            return self._cached_asdict\n        d = self._get_nosync()\n        if self.cache:\n            self._cached_asdict = d\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, *args, **kwargs):\n        self._write_op(self._update_nosync, *args, **kwargs)", "response": "Update the values of several attributes in a single operation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_dumps(o):\n    return json.dumps(o, indent=4, sort_keys=True, ensure_ascii=True,\n                      separators=(',', ': ')).encode('ascii')", "response": "Write JSON in a consistent human - readable way."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_shape(shape):\n\n    if shape is None:\n        raise TypeError('shape is None')\n\n    # handle 1D convenience form\n    if isinstance(shape, numbers.Integral):\n        shape = (int(shape),)\n\n    # normalize\n    shape = tuple(int(s) for s in shape)\n    return shape", "response": "Convenience function to normalize the shape argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nguesses an appropriate chunk layout for a dataset given its shape and typesize.", "response": "def guess_chunks(shape, typesize):\n    \"\"\"\n    Guess an appropriate chunk layout for a dataset, given its shape and\n    the size of each element in bytes.  Will allocate chunks only as large\n    as MAX_SIZE.  Chunks are generally close to some power-of-2 fraction of\n    each axis, slightly favoring bigger values for the last index.\n    Undocumented and subject to change without warning.\n    \"\"\"\n\n    ndims = len(shape)\n    # require chunks to have non-zero length for all dimensions\n    chunks = np.maximum(np.array(shape, dtype='=f8'), 1)\n\n    # Determine the optimal chunk size in bytes using a PyTables expression.\n    # This is kept as a float.\n    dset_size = np.product(chunks)*typesize\n    target_size = CHUNK_BASE * (2**np.log10(dset_size/(1024.*1024)))\n\n    if target_size > CHUNK_MAX:\n        target_size = CHUNK_MAX\n    elif target_size < CHUNK_MIN:\n        target_size = CHUNK_MIN\n\n    idx = 0\n    while True:\n        # Repeatedly loop over the axes, dividing them by 2.  Stop when:\n        # 1a. We're smaller than the target chunk size, OR\n        # 1b. We're within 50% of the target chunk size, AND\n        # 2. The chunk is smaller than the maximum chunk size\n\n        chunk_bytes = np.product(chunks)*typesize\n\n        if (chunk_bytes < target_size or\n                abs(chunk_bytes-target_size)/target_size < 0.5) and \\\n                chunk_bytes < CHUNK_MAX:\n            break\n\n        if np.product(chunks) == 1:\n            break  # Element size larger than CHUNK_MAX\n\n        chunks[idx % ndims] = np.ceil(chunks[idx % ndims] / 2.0)\n        idx += 1\n\n    return tuple(int(x) for x in chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_chunks(chunks, shape, typesize):\n\n    # N.B., expect shape already normalized\n\n    # handle auto-chunking\n    if chunks is None or chunks is True:\n        return guess_chunks(shape, typesize)\n\n    # handle no chunking\n    if chunks is False:\n        return shape\n\n    # handle 1D convenience form\n    if isinstance(chunks, numbers.Integral):\n        chunks = (int(chunks),)\n\n    # handle bad dimensionality\n    if len(chunks) > len(shape):\n        raise ValueError('too many dimensions in chunks')\n\n    # handle underspecified chunks\n    if len(chunks) < len(shape):\n        # assume chunks across remaining dimensions\n        chunks += shape[len(chunks):]\n\n    # handle None in chunks\n    chunks = tuple(s if c is None else int(c)\n                   for s, c in zip(shape, chunks))\n\n    return chunks", "response": "Convenience function to normalize the chunks argument for an array\n    with the given shape."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining whether item specifies a complete slice of array with the given shape. Used to optimize __setitem__ operations on the Chunk class.", "response": "def is_total_slice(item, shape):\n    \"\"\"Determine whether `item` specifies a complete slice of array with the\n    given `shape`. Used to optimize __setitem__ operations on the Chunk\n    class.\"\"\"\n\n    # N.B., assume shape is normalized\n\n    if item == Ellipsis:\n        return True\n    if item == slice(None):\n        return True\n    if isinstance(item, slice):\n        item = item,\n    if isinstance(item, tuple):\n        return all(\n            (isinstance(s, slice) and\n                ((s == slice(None)) or\n                 ((s.stop - s.start == l) and (s.step in [1, None]))))\n            for s, l in zip(item, shape)\n        )\n    else:\n        raise TypeError('expected slice or tuple of slices, found %r' % item)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef instance_dir(obj):  # pragma: py3 no cover\n    d = dict()\n    d.update(obj.__dict__)\n    d.update(class_dir(obj.__class__))\n    result = sorted(d.keys())\n    return result", "response": "Returns a list of the instance directories of an object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains_array(store, path=None):\n    path = normalize_storage_path(path)\n    prefix = _path_to_prefix(path)\n    key = prefix + array_meta_key\n    return key in store", "response": "Return True if the store contains an array at the given logical path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the store contains a group at the given logical path.", "response": "def contains_group(store, path=None):\n    \"\"\"Return True if the store contains a group at the given logical path.\"\"\"\n    path = normalize_storage_path(path)\n    prefix = _path_to_prefix(path)\n    key = prefix + group_meta_key\n    return key in store"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving all items under the given path.", "response": "def rmdir(store, path=None):\n    \"\"\"Remove all items under the given path. If `store` provides a `rmdir` method,\n    this will be called, otherwise will fall back to implementation via the\n    `MutableMapping` interface.\"\"\"\n    path = normalize_storage_path(path)\n    if hasattr(store, 'rmdir'):\n        # pass through\n        store.rmdir(path)\n    else:\n        # slow version, delete one key at a time\n        _rmdir_from_keys(store, path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename(store, src_path, dst_path):\n    src_path = normalize_storage_path(src_path)\n    dst_path = normalize_storage_path(dst_path)\n    if hasattr(store, 'rename'):\n        # pass through\n        store.rename(src_path, dst_path)\n    else:\n        # slow version, delete one key at a time\n        _rename_from_keys(store, src_path, dst_path)", "response": "Rename all items under the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nobtains a directory listing for the given path.", "response": "def listdir(store, path=None):\n    \"\"\"Obtain a directory listing for the given path. If `store` provides a `listdir`\n    method, this will be called, otherwise will fall back to implementation via the\n    `MutableMapping` interface.\"\"\"\n    path = normalize_storage_path(path)\n    if hasattr(store, 'listdir'):\n        # pass through\n        return store.listdir(path)\n    else:\n        # slow version, iterate through all keys\n        return _listdir_from_keys(store, path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing size of stored items for a given path.", "response": "def getsize(store, path=None):\n    \"\"\"Compute size of stored items for a given path. If `store` provides a `getsize`\n    method, this will be called, otherwise will return -1.\"\"\"\n    path = normalize_storage_path(path)\n    if hasattr(store, 'getsize'):\n        # pass through\n        return store.getsize(path)\n    elif isinstance(store, dict):\n        # compute from size of values\n        if path in store:\n            v = store[path]\n            size = buffer_size(v)\n        else:\n            members = listdir(store, path)\n            prefix = _path_to_prefix(path)\n            size = 0\n            for k in members:\n                try:\n                    v = store[prefix + k]\n                except KeyError:\n                    pass\n                else:\n                    try:\n                        size += buffer_size(v)\n                    except TypeError:\n                        return -1\n        return size\n    else:\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_array(store, shape, chunks=True, dtype=None, compressor='default',\n               fill_value=None, order='C', overwrite=False, path=None,\n               chunk_store=None, filters=None, object_codec=None):\n    \"\"\"Initialize an array store with the given configuration. Note that this is a low-level\n    function and there should be no need to call this directly from user code.\n\n    Parameters\n    ----------\n    store : MutableMapping\n        A mapping that supports string keys and bytes-like values.\n    shape : int or tuple of ints\n        Array shape.\n    chunks : int or tuple of ints, optional\n        Chunk shape. If True, will be guessed from `shape` and `dtype`. If\n        False, will be set to `shape`, i.e., single chunk for the whole array.\n    dtype : string or dtype, optional\n        NumPy dtype.\n    compressor : Codec, optional\n        Primary compressor.\n    fill_value : object\n        Default value to use for uninitialized portions of the array.\n    order : {'C', 'F'}, optional\n        Memory layout to be used within each chunk.\n    overwrite : bool, optional\n        If True, erase all data in `store` prior to initialisation.\n    path : string, optional\n        Path under which array is stored.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n    filters : sequence, optional\n        Sequence of filters to use to encode chunk data prior to compression.\n    object_codec : Codec, optional\n        A codec to encode object arrays, only needed if dtype=object.\n\n    Examples\n    --------\n    Initialize an array store::\n\n        >>> from zarr.storage import init_array\n        >>> store = dict()\n        >>> init_array(store, shape=(10000, 10000), chunks=(1000, 1000))\n        >>> sorted(store.keys())\n        ['.zarray']\n\n    Array metadata is stored as JSON::\n\n        >>> print(store['.zarray'].decode())\n        {\n            \"chunks\": [\n                1000,\n                1000\n            ],\n            \"compressor\": {\n                \"blocksize\": 0,\n                \"clevel\": 5,\n                \"cname\": \"lz4\",\n                \"id\": \"blosc\",\n                \"shuffle\": 1\n            },\n            \"dtype\": \"<f8\",\n            \"fill_value\": null,\n            \"filters\": null,\n            \"order\": \"C\",\n            \"shape\": [\n                10000,\n                10000\n            ],\n            \"zarr_format\": 2\n        }\n\n    Initialize an array using a storage path::\n\n        >>> store = dict()\n        >>> init_array(store, shape=100000000, chunks=1000000, dtype='i1', path='foo')\n        >>> sorted(store.keys())\n        ['.zgroup', 'foo/.zarray']\n        >>> print(store['foo/.zarray'].decode())\n        {\n            \"chunks\": [\n                1000000\n            ],\n            \"compressor\": {\n                \"blocksize\": 0,\n                \"clevel\": 5,\n                \"cname\": \"lz4\",\n                \"id\": \"blosc\",\n                \"shuffle\": 1\n            },\n            \"dtype\": \"|i1\",\n            \"fill_value\": null,\n            \"filters\": null,\n            \"order\": \"C\",\n            \"shape\": [\n                100000000\n            ],\n            \"zarr_format\": 2\n        }\n\n    Notes\n    -----\n    The initialisation process involves normalising all array metadata, encoding\n    as JSON and storing under the '.zarray' key.\n\n    \"\"\"\n\n    # normalize path\n    path = normalize_storage_path(path)\n\n    # ensure parent group initialized\n    _require_parent_group(path, store=store, chunk_store=chunk_store, overwrite=overwrite)\n\n    _init_array_metadata(store, shape=shape, chunks=chunks, dtype=dtype,\n                         compressor=compressor, fill_value=fill_value,\n                         order=order, overwrite=overwrite, path=path,\n                         chunk_store=chunk_store, filters=filters,\n                         object_codec=object_codec)", "response": "Initialize an array store with the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a group store.", "response": "def init_group(store, overwrite=False, path=None, chunk_store=None):\n    \"\"\"Initialize a group store. Note that this is a low-level function and there should be no\n    need to call this directly from user code.\n\n    Parameters\n    ----------\n    store : MutableMapping\n        A mapping that supports string keys and byte sequence values.\n    overwrite : bool, optional\n        If True, erase all data in `store` prior to initialisation.\n    path : string, optional\n        Path under which array is stored.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n\n    \"\"\"\n\n    # normalize path\n    path = normalize_storage_path(path)\n\n    # ensure parent group initialized\n    _require_parent_group(path, store=store, chunk_store=chunk_store,\n                          overwrite=overwrite)\n\n    # initialise metadata\n    _init_group_metadata(store=store, overwrite=overwrite, path=path,\n                         chunk_store=chunk_store)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures directory removal at interpreter exit.", "response": "def atexit_rmtree(path,\n                  isdir=os.path.isdir,\n                  rmtree=shutil.rmtree):  # pragma: no cover\n    \"\"\"Ensure directory removal at interpreter exit.\"\"\"\n    if isdir(path):\n        rmtree(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures removal of multiple files at interpreter exit.", "response": "def atexit_rmglob(path,\n                  glob=glob.glob,\n                  isdir=os.path.isdir,\n                  isfile=os.path.isfile,\n                  remove=os.remove,\n                  rmtree=shutil.rmtree):  # pragma: no cover\n    \"\"\"Ensure removal of multiple files at interpreter exit.\"\"\"\n    for p in glob(path):\n        if isfile(p):\n            remove(p)\n        elif isdir(p):\n            rmtree(p)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmigrate array metadata in store from Zarr format version 1 to version 2.", "response": "def migrate_1to2(store):\n    \"\"\"Migrate array metadata in `store` from Zarr format version 1 to\n    version 2.\n\n    Parameters\n    ----------\n    store : MutableMapping\n        Store to be migrated.\n\n    Notes\n    -----\n    Version 1 did not support hierarchies, so this migration function will\n    look for a single array in `store` and migrate the array metadata to\n    version 2.\n\n    \"\"\"\n\n    # migrate metadata\n    from zarr import meta_v1\n    meta = meta_v1.decode_metadata(store['meta'])\n    del store['meta']\n\n    # add empty filters\n    meta['filters'] = None\n\n    # migration compression metadata\n    compression = meta['compression']\n    if compression is None or compression == 'none':\n        compressor_config = None\n    else:\n        compression_opts = meta['compression_opts']\n        codec_cls = codec_registry[compression]\n        if isinstance(compression_opts, dict):\n            compressor = codec_cls(**compression_opts)\n        else:\n            compressor = codec_cls(compression_opts)\n        compressor_config = compressor.get_config()\n    meta['compressor'] = compressor_config\n    del meta['compression']\n    del meta['compression_opts']\n\n    # store migrated metadata\n    store[array_meta_key] = encode_array_metadata(meta)\n\n    # migrate user attributes\n    store[attrs_key] = store['attrs']\n    del store['attrs']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(self):\n        if self.mode != 'r':\n            with self.mutex:\n                self.zf.close()\n                # N.B., re-open with mode 'a' regardless of initial mode so we don't wipe\n                # what's been written\n                self.zf = zipfile.ZipFile(self.path, mode='a',\n                                          compression=self.compression,\n                                          allowZip64=self.allowZip64)", "response": "Closes the underlying zip file ensuring all records are written and re - opens the file for further modifications."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the underlying database file.", "response": "def close(self):\n        \"\"\"Closes the underlying database file.\"\"\"\n        if hasattr(self.db, 'close'):\n            with self.write_mutex:\n                self.db.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(self):\n        if self.flag[0] != 'r':\n            with self.write_mutex:\n                if hasattr(self.db, 'sync'):\n                    self.db.sync()\n                else:\n                    # fall-back, close and re-open, needed for ndbm\n                    flag = self.flag\n                    if flag[0] == 'n':\n                        flag = 'c' + flag[1:]  # don't clobber an existing database\n                    self.db.close()\n                    # noinspection PyArgumentList\n                    self.db = self.open(self.path, flag, self.mode, **self.open_kwargs)", "response": "Synchronizes data to the underlying database file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an orthogonal selection to a numpy advanced selection like numpy. ix_ix_c.", "response": "def ix_(selection, shape):\n    \"\"\"Convert an orthogonal selection to a numpy advanced (fancy) selection, like numpy.ix_\n    but with support for slices and single ints.\"\"\"\n\n    # normalisation\n    selection = replace_ellipsis(selection, shape)\n\n    # replace slice and int as these are not supported by numpy.ix_\n    selection = [slice_to_range(dim_sel, dim_len) if isinstance(dim_sel, slice)\n                 else [dim_sel] if is_integer(dim_sel)\n                 else dim_sel\n                 for dim_sel, dim_len in zip(selection, shape)]\n\n    # now get numpy to convert to a coordinate selection\n    selection = np.ix_(*selection)\n\n    return selection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef oindex(a, selection):\n    selection = replace_ellipsis(selection, a.shape)\n    drop_axes = tuple([i for i, s in enumerate(selection) if is_integer(s)])\n    selection = ix_(selection, a.shape)\n    result = a[selection]\n    if drop_axes:\n        result = result.squeeze(axis=drop_axes)\n    return result", "response": "Implementation of orthogonal indexing with slices and ints."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef array_metadata_to_n5(array_metadata):\n    '''Convert array metadata from zarr to N5 format.'''\n\n    for f, t in zarr_to_n5_keys:\n        array_metadata[t] = array_metadata[f]\n        del array_metadata[f]\n    del array_metadata['zarr_format']\n\n    try:\n        dtype = np.dtype(array_metadata['dataType'])\n    except TypeError:  # pragma: no cover\n        raise TypeError(\n            \"data type %s not supported by N5\" % array_metadata['dataType'])\n\n    array_metadata['dataType'] = dtype.name\n    array_metadata['dimensions'] = array_metadata['dimensions'][::-1]\n    array_metadata['blockSize'] = array_metadata['blockSize'][::-1]\n\n    if 'fill_value' in array_metadata:\n        if array_metadata['fill_value'] != 0 and array_metadata['fill_value'] is not None:\n            raise ValueError(\"N5 only supports fill_value == 0 (for now)\")\n        del array_metadata['fill_value']\n\n    if 'order' in array_metadata:\n        if array_metadata['order'] != 'C':\n            raise ValueError(\"zarr N5 storage only stores arrays in C order (for now)\")\n        del array_metadata['order']\n\n    if 'filters' in array_metadata:\n        if array_metadata['filters'] != [] and array_metadata['filters'] is not None:\n            raise ValueError(\"N5 storage does not support zarr filters\")\n        del array_metadata['filters']\n\n    assert 'compression' in array_metadata\n    compressor_config = array_metadata['compression']\n    compressor_config = compressor_config_to_n5(compressor_config)\n    array_metadata['compression'] = compressor_config\n\n    return array_metadata", "response": "Convert array metadata from zarr to N5 format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef array_metadata_to_zarr(array_metadata):\n    '''Convert array metadata from N5 to zarr format.'''\n    for t, f in zarr_to_n5_keys:\n        array_metadata[t] = array_metadata[f]\n        del array_metadata[f]\n    array_metadata['zarr_format'] = ZARR_FORMAT\n\n    array_metadata['shape'] = array_metadata['shape'][::-1]\n    array_metadata['chunks'] = array_metadata['chunks'][::-1]\n    array_metadata['fill_value'] = 0  # also if None was requested\n    array_metadata['order'] = 'C'\n    array_metadata['filters'] = []\n\n    compressor_config = array_metadata['compressor']\n    compressor_config = compressor_config_to_zarr(compressor_config)\n    array_metadata['compressor'] = {\n        'id': N5ChunkWrapper.codec_id,\n        'compressor_config': compressor_config,\n        'dtype': array_metadata['dtype'],\n        'chunk_shape': array_metadata['chunks']\n    }\n\n    return array_metadata", "response": "Convert array metadata from N5 to zarr format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\narraying name following h5py convention.", "response": "def name(self):\n        \"\"\"Array name following h5py convention.\"\"\"\n        if self.path:\n            # follow h5py convention: add leading slash\n            name = self.path\n            if name[0] != '/':\n                name = '/' + name\n            return name\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the basic data for an item or region of the array.", "response": "def get_basic_selection(self, selection=Ellipsis, out=None, fields=None):\n        \"\"\"Retrieve data for an item or region of the array.\n\n        Parameters\n        ----------\n        selection : tuple\n            A tuple specifying the requested item or region for each dimension of the\n            array. May be any combination of int and/or slice for multidimensional arrays.\n        out : ndarray, optional\n            If given, load the selected data directly into this array.\n        fields : str or sequence of str, optional\n            For arrays with a structured dtype, one or more fields can be specified to\n            extract data for.\n\n        Returns\n        -------\n        out : ndarray\n            A NumPy array containing the data for the requested region.\n\n        Examples\n        --------\n        Setup a 1-dimensional array::\n\n            >>> import zarr\n            >>> import numpy as np\n            >>> z = zarr.array(np.arange(100))\n\n        Retrieve a single item::\n\n            >>> z.get_basic_selection(5)\n            5\n\n        Retrieve a region via slicing::\n\n            >>> z.get_basic_selection(slice(5))\n            array([0, 1, 2, 3, 4])\n            >>> z.get_basic_selection(slice(-5, None))\n            array([95, 96, 97, 98, 99])\n            >>> z.get_basic_selection(slice(5, 10))\n            array([5, 6, 7, 8, 9])\n            >>> z.get_basic_selection(slice(5, 10, 2))\n            array([5, 7, 9])\n            >>> z.get_basic_selection(slice(None, None, 2))\n            array([  0,  2,  4, ..., 94, 96, 98])\n\n        Setup a 2-dimensional array::\n\n            >>> z = zarr.array(np.arange(100).reshape(10, 10))\n\n        Retrieve an item::\n\n            >>> z.get_basic_selection((2, 2))\n            22\n\n        Retrieve a region via slicing::\n\n            >>> z.get_basic_selection((slice(1, 3), slice(1, 3)))\n            array([[11, 12],\n                   [21, 22]])\n            >>> z.get_basic_selection((slice(1, 3), slice(None)))\n            array([[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n                   [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]])\n            >>> z.get_basic_selection((slice(None), slice(1, 3)))\n            array([[ 1,  2],\n                   [11, 12],\n                   [21, 22],\n                   [31, 32],\n                   [41, 42],\n                   [51, 52],\n                   [61, 62],\n                   [71, 72],\n                   [81, 82],\n                   [91, 92]])\n            >>> z.get_basic_selection((slice(0, 5, 2), slice(0, 5, 2)))\n            array([[ 0,  2,  4],\n                   [20, 22, 24],\n                   [40, 42, 44]])\n            >>> z.get_basic_selection((slice(None, None, 2), slice(None, None, 2)))\n            array([[ 0,  2,  4,  6,  8],\n                   [20, 22, 24, 26, 28],\n                   [40, 42, 44, 46, 48],\n                   [60, 62, 64, 66, 68],\n                   [80, 82, 84, 86, 88]])\n\n        For arrays with a structured dtype, specific fields can be retrieved, e.g.::\n\n            >>> a = np.array([(b'aaa', 1, 4.2),\n            ...               (b'bbb', 2, 8.4),\n            ...               (b'ccc', 3, 12.6)],\n            ...              dtype=[('foo', 'S3'), ('bar', 'i4'), ('baz', 'f8')])\n            >>> z = zarr.array(a)\n            >>> z.get_basic_selection(slice(2), fields='foo')\n            array([b'aaa', b'bbb'],\n                  dtype='|S3')\n\n        Notes\n        -----\n        Slices with step > 1 are supported, but slices with negative step are not.\n\n        Currently this method provides the implementation for accessing data via the\n        square bracket notation (__getitem__). See :func:`__getitem__` for examples\n        using the alternative notation.\n\n        See Also\n        --------\n        set_basic_selection, get_mask_selection, set_mask_selection,\n        get_coordinate_selection, set_coordinate_selection, get_orthogonal_selection,\n        set_orthogonal_selection, vindex, oindex, __getitem__, __setitem__\n\n        \"\"\"\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata()\n\n        # check args\n        check_fields(fields, self._dtype)\n\n        # handle zero-dimensional arrays\n        if self._shape == ():\n            return self._get_basic_selection_zd(selection=selection, out=out,\n                                                fields=fields)\n        else:\n            return self._get_basic_selection_nd(selection=selection, out=out,\n                                                fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_orthogonal_selection(self, selection, out=None, fields=None):\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata()\n\n        # check args\n        check_fields(fields, self._dtype)\n\n        # setup indexer\n        indexer = OrthogonalIndexer(selection, self)\n\n        return self._get_selection(indexer=indexer, out=out, fields=fields)", "response": "Retrieve data by making a selection for each dimension of the array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_coordinate_selection(self, selection, out=None, fields=None):\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata()\n\n        # check args\n        check_fields(fields, self._dtype)\n\n        # setup indexer\n        indexer = CoordinateIndexer(selection, self)\n\n        # handle output - need to flatten\n        if out is not None:\n            out = out.reshape(-1)\n\n        out = self._get_selection(indexer=indexer, out=out, fields=fields)\n\n        # restore shape\n        out = out.reshape(indexer.sel_shape)\n\n        return out", "response": "Retrieve a selection of individual items by providing the indices of each selected item."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a selection of individual items by providing a Boolean array of the same shape as the array against which the True values indicate a selected item.", "response": "def get_mask_selection(self, selection, out=None, fields=None):\n        \"\"\"Retrieve a selection of individual items, by providing a Boolean array of the\n        same shape as the array against which the selection is being made, where True\n        values indicate a selected item.\n\n        Parameters\n        ----------\n        selection : ndarray, bool\n            A Boolean array of the same shape as the array against which the selection is\n            being made.\n        out : ndarray, optional\n            If given, load the selected data directly into this array.\n        fields : str or sequence of str, optional\n            For arrays with a structured dtype, one or more fields can be specified to\n            extract data for.\n\n        Returns\n        -------\n        out : ndarray\n            A NumPy array containing the data for the requested selection.\n\n        Examples\n        --------\n        Setup a 2-dimensional array::\n\n            >>> import zarr\n            >>> import numpy as np\n            >>> z = zarr.array(np.arange(100).reshape(10, 10))\n\n        Retrieve items by specifying a maks::\n\n            >>> sel = np.zeros_like(z, dtype=bool)\n            >>> sel[1, 1] = True\n            >>> sel[4, 4] = True\n            >>> z.get_mask_selection(sel)\n            array([11, 44])\n\n        For convenience, the mask selection functionality is also available via the\n        `vindex` property, e.g.::\n\n            >>> z.vindex[sel]\n            array([11, 44])\n\n        Notes\n        -----\n        Mask indexing is a form of vectorized or inner indexing, and is equivalent to\n        coordinate indexing. Internally the mask array is converted to coordinate\n        arrays by calling `np.nonzero`.\n\n        See Also\n        --------\n        get_basic_selection, set_basic_selection, set_mask_selection,\n        get_orthogonal_selection, set_orthogonal_selection, get_coordinate_selection,\n        set_coordinate_selection, vindex, oindex, __getitem__, __setitem__\n\n        \"\"\"\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata()\n\n        # check args\n        check_fields(fields, self._dtype)\n\n        # setup indexer\n        indexer = MaskIndexer(selection, self)\n\n        return self._get_selection(indexer=indexer, out=out, fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying the data for an item or region of the array.", "response": "def set_basic_selection(self, selection, value, fields=None):\n        \"\"\"Modify data for an item or region of the array.\n\n        Parameters\n        ----------\n        selection : tuple\n            An integer index or slice or tuple of int/slice specifying the requested\n            region for each dimension of the array.\n        value : scalar or array-like\n            Value to be stored into the array.\n        fields : str or sequence of str, optional\n            For arrays with a structured dtype, one or more fields can be specified to set\n            data for.\n\n        Examples\n        --------\n        Setup a 1-dimensional array::\n\n            >>> import zarr\n            >>> import numpy as np\n            >>> z = zarr.zeros(100, dtype=int)\n\n        Set all array elements to the same scalar value::\n\n            >>> z.set_basic_selection(..., 42)\n            >>> z[...]\n            array([42, 42, 42, ..., 42, 42, 42])\n\n        Set a portion of the array::\n\n            >>> z.set_basic_selection(slice(10), np.arange(10))\n            >>> z.set_basic_selection(slice(-10, None), np.arange(10)[::-1])\n            >>> z[...]\n            array([ 0, 1, 2, ..., 2, 1, 0])\n\n        Setup a 2-dimensional array::\n\n            >>> z = zarr.zeros((5, 5), dtype=int)\n\n        Set all array elements to the same scalar value::\n\n            >>> z.set_basic_selection(..., 42)\n\n        Set a portion of the array::\n\n            >>> z.set_basic_selection((0, slice(None)), np.arange(z.shape[1]))\n            >>> z.set_basic_selection((slice(None), 0), np.arange(z.shape[0]))\n            >>> z[...]\n            array([[ 0,  1,  2,  3,  4],\n                   [ 1, 42, 42, 42, 42],\n                   [ 2, 42, 42, 42, 42],\n                   [ 3, 42, 42, 42, 42],\n                   [ 4, 42, 42, 42, 42]])\n\n        For arrays with a structured dtype, the `fields` parameter can be used to set\n        data for a specific field, e.g.::\n\n            >>> a = np.array([(b'aaa', 1, 4.2),\n            ...               (b'bbb', 2, 8.4),\n            ...               (b'ccc', 3, 12.6)],\n            ...              dtype=[('foo', 'S3'), ('bar', 'i4'), ('baz', 'f8')])\n            >>> z = zarr.array(a)\n            >>> z.set_basic_selection(slice(0, 2), b'zzz', fields='foo')\n            >>> z[:]\n            array([(b'zzz', 1,   4.2), (b'zzz', 2,   8.4), (b'ccc', 3,  12.6)],\n                  dtype=[('foo', 'S3'), ('bar', '<i4'), ('baz', '<f8')])\n\n        Notes\n        -----\n        This method provides the underlying implementation for modifying data via square\n        bracket notation, see :func:`__setitem__` for equivalent examples using the\n        alternative notation.\n\n        See Also\n        --------\n        get_basic_selection, get_mask_selection, set_mask_selection,\n        get_coordinate_selection, set_coordinate_selection, get_orthogonal_selection,\n        set_orthogonal_selection, vindex, oindex, __getitem__, __setitem__\n\n        \"\"\"\n\n        # guard conditions\n        if self._read_only:\n            err_read_only()\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata_nosync()\n\n        # handle zero-dimensional arrays\n        if self._shape == ():\n            return self._set_basic_selection_zd(selection, value, fields=fields)\n        else:\n            return self._set_basic_selection_nd(selection, value, fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_orthogonal_selection(self, selection, value, fields=None):\n\n        # guard conditions\n        if self._read_only:\n            err_read_only()\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata_nosync()\n\n        # setup indexer\n        indexer = OrthogonalIndexer(selection, self)\n\n        self._set_selection(indexer, value, fields=fields)", "response": "Modify data via a selection for each dimension of the array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying a selection of individual items by providing the indices of each item to be modified.", "response": "def set_coordinate_selection(self, selection, value, fields=None):\n        \"\"\"Modify a selection of individual items, by providing the indices (coordinates)\n        for each item to be modified.\n\n        Parameters\n        ----------\n        selection : tuple\n            An integer (coordinate) array for each dimension of the array.\n        value : scalar or array-like\n            Value to be stored into the array.\n        fields : str or sequence of str, optional\n            For arrays with a structured dtype, one or more fields can be specified to set\n            data for.\n\n        Examples\n        --------\n        Setup a 2-dimensional array::\n\n            >>> import zarr\n            >>> import numpy as np\n            >>> z = zarr.zeros((5, 5), dtype=int)\n\n        Set data for a selection of items::\n\n            >>> z.set_coordinate_selection(([1, 4], [1, 4]), 1)\n            >>> z[...]\n            array([[0, 0, 0, 0, 0],\n                   [0, 1, 0, 0, 0],\n                   [0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 1]])\n\n        For convenience, this functionality is also available via the `vindex` property.\n        E.g.::\n\n            >>> z.vindex[[1, 4], [1, 4]] = 2\n            >>> z[...]\n            array([[0, 0, 0, 0, 0],\n                   [0, 2, 0, 0, 0],\n                   [0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 2]])\n\n        Notes\n        -----\n        Coordinate indexing is also known as point selection, and is a form of vectorized\n        or inner indexing.\n\n        Slices are not supported. Coordinate arrays must be provided for all dimensions\n        of the array.\n\n        See Also\n        --------\n        get_basic_selection, set_basic_selection, get_mask_selection, set_mask_selection,\n        get_orthogonal_selection, set_orthogonal_selection, get_coordinate_selection,\n        vindex, oindex, __getitem__, __setitem__\n\n        \"\"\"\n\n        # guard conditions\n        if self._read_only:\n            err_read_only()\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata_nosync()\n\n        # setup indexer\n        indexer = CoordinateIndexer(selection, self)\n\n        # handle value - need to flatten\n        if not is_scalar(value, self._dtype):\n            value = np.asanyarray(value)\n        if hasattr(value, 'shape') and len(value.shape) > 1:\n            value = value.reshape(-1)\n\n        self._set_selection(indexer, value, fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_mask_selection(self, selection, value, fields=None):\n\n        # guard conditions\n        if self._read_only:\n            err_read_only()\n\n        # refresh metadata\n        if not self._cache_metadata:\n            self._load_metadata_nosync()\n\n        # setup indexer\n        indexer = MaskIndexer(selection, self)\n\n        self._set_selection(indexer, value, fields=fields)", "response": "Modify a selection of individual items by providing a Boolean array of the same shape as the array against which the selection is being made."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nobtains part or whole of a chunk. Parameters ---------- chunk_coords : tuple of ints Indices of the chunk. chunk_selection : selection Location of region within the chunk to extract. out : ndarray Array to store result in. out_selection : selection Location of region within output array to store results in. drop_axes : tuple of ints Axes to squeeze out of the chunk. fields TODO", "response": "def _chunk_getitem(self, chunk_coords, chunk_selection, out, out_selection,\n                       drop_axes=None, fields=None):\n        \"\"\"Obtain part or whole of a chunk.\n\n        Parameters\n        ----------\n        chunk_coords : tuple of ints\n            Indices of the chunk.\n        chunk_selection : selection\n            Location of region within the chunk to extract.\n        out : ndarray\n            Array to store result in.\n        out_selection : selection\n            Location of region within output array to store results in.\n        drop_axes : tuple of ints\n            Axes to squeeze out of the chunk.\n        fields\n            TODO\n\n        \"\"\"\n\n        assert len(chunk_coords) == len(self._cdata_shape)\n\n        # obtain key for chunk\n        ckey = self._chunk_key(chunk_coords)\n\n        try:\n            # obtain compressed data for chunk\n            cdata = self.chunk_store[ckey]\n\n        except KeyError:\n            # chunk not initialized\n            if self._fill_value is not None:\n                if fields:\n                    fill_value = self._fill_value[fields]\n                else:\n                    fill_value = self._fill_value\n                out[out_selection] = fill_value\n\n        else:\n\n            if (isinstance(out, np.ndarray) and\n                    not fields and\n                    is_contiguous_selection(out_selection) and\n                    is_total_slice(chunk_selection, self._chunks) and\n                    not self._filters and\n                    self._dtype != object):\n\n                dest = out[out_selection]\n                write_direct = (\n                    dest.flags.writeable and (\n                        (self._order == 'C' and dest.flags.c_contiguous) or\n                        (self._order == 'F' and dest.flags.f_contiguous)\n                    )\n                )\n\n                if write_direct:\n\n                    # optimization: we want the whole chunk, and the destination is\n                    # contiguous, so we can decompress directly from the chunk\n                    # into the destination array\n\n                    if self._compressor:\n                        self._compressor.decode(cdata, dest)\n                    else:\n                        chunk = ensure_ndarray(cdata).view(self._dtype)\n                        chunk = chunk.reshape(self._chunks, order=self._order)\n                        np.copyto(dest, chunk)\n                    return\n\n            # decode chunk\n            chunk = self._decode_chunk(cdata)\n\n            # select data from chunk\n            if fields:\n                chunk = chunk[fields]\n            tmp = chunk[chunk_selection]\n            if drop_axes:\n                tmp = np.squeeze(tmp, axis=drop_axes)\n\n            # store selected data in output\n            out[out_selection] = tmp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace part or whole of a chunk.", "response": "def _chunk_setitem(self, chunk_coords, chunk_selection, value, fields=None):\n        \"\"\"Replace part or whole of a chunk.\n\n        Parameters\n        ----------\n        chunk_coords : tuple of ints\n            Indices of the chunk.\n        chunk_selection : tuple of slices\n            Location of region within the chunk.\n        value : scalar or ndarray\n            Value to set.\n\n        \"\"\"\n\n        if self._synchronizer is None:\n            # no synchronization\n            lock = nolock\n        else:\n            # synchronize on the chunk\n            ckey = self._chunk_key(chunk_coords)\n            lock = self._synchronizer[ckey]\n\n        with lock:\n            self._chunk_setitem_nosync(chunk_coords, chunk_selection, value,\n                                       fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute a checksum for the data.", "response": "def digest(self, hashname=\"sha1\"):\n        \"\"\"\n        Compute a checksum for the data. Default uses sha1 for speed.\n\n        Examples\n        --------\n        >>> import binascii\n        >>> import zarr\n        >>> z = zarr.empty(shape=(10000, 10000), chunks=(1000, 1000))\n        >>> binascii.hexlify(z.digest())\n        b'041f90bc7a571452af4f850a8ca2c6cddfa8a1ac'\n        >>> z = zarr.zeros(shape=(10000, 10000), chunks=(1000, 1000))\n        >>> binascii.hexlify(z.digest())\n        b'7162d416d26a68063b66ed1f30e0a866e4abed60'\n        >>> z = zarr.zeros(shape=(10000, 10000), dtype=\"u1\", chunks=(1000, 1000))\n        >>> binascii.hexlify(z.digest())\n        b'cb387af37410ae5a3222e893cf3373e4e4f22816'\n        \"\"\"\n\n        h = hashlib.new(hashname)\n\n        for i in itertools.product(*[range(s) for s in self.cdata_shape]):\n            h.update(self.chunk_store.get(self._chunk_key(i), b\"\"))\n\n        h.update(self.store.get(self._key_prefix + array_meta_key, b\"\"))\n\n        h.update(self.store.get(self.attrs.key, b\"\"))\n\n        checksum = h.digest()\n\n        return checksum"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute a checksum for the data.", "response": "def hexdigest(self, hashname=\"sha1\"):\n        \"\"\"\n        Compute a checksum for the data. Default uses sha1 for speed.\n\n        Examples\n        --------\n        >>> import zarr\n        >>> z = zarr.empty(shape=(10000, 10000), chunks=(1000, 1000))\n        >>> z.hexdigest()\n        '041f90bc7a571452af4f850a8ca2c6cddfa8a1ac'\n        >>> z = zarr.zeros(shape=(10000, 10000), chunks=(1000, 1000))\n        >>> z.hexdigest()\n        '7162d416d26a68063b66ed1f30e0a866e4abed60'\n        >>> z = zarr.zeros(shape=(10000, 10000), dtype=\"u1\", chunks=(1000, 1000))\n        >>> z.hexdigest()\n        'cb387af37410ae5a3222e893cf3373e4e4f22816'\n        \"\"\"\n\n        checksum = binascii.hexlify(self.digest(hashname=hashname))\n\n        # This is a bytes object on Python 3 and we want a str.\n        if type(checksum) is not str:  # pragma: py2 no cover\n            checksum = checksum.decode('utf8')\n\n        return checksum"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending data to an axis of the internal array.", "response": "def append(self, data, axis=0):\n        \"\"\"Append `data` to `axis`.\n\n        Parameters\n        ----------\n        data : array_like\n            Data to be appended.\n        axis : int\n            Axis along which to append.\n\n        Returns\n        -------\n        new_shape : tuple\n\n        Notes\n        -----\n        The size of all dimensions other than `axis` must match between this\n        array and `data`.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import zarr\n        >>> a = np.arange(10000000, dtype='i4').reshape(10000, 1000)\n        >>> z = zarr.array(a, chunks=(1000, 100))\n        >>> z.shape\n        (10000, 1000)\n        >>> z.append(a)\n        (20000, 1000)\n        >>> z.append(np.vstack([a, a]), axis=1)\n        (20000, 2000)\n        >>> z.shape\n        (20000, 2000)\n\n        \"\"\"\n        return self._write_op(self._append_nosync, data, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an array sharing the same data.", "response": "def view(self, shape=None, chunks=None, dtype=None,\n             fill_value=None, filters=None, read_only=None,\n             synchronizer=None):\n        \"\"\"Return an array sharing the same data.\n\n        Parameters\n        ----------\n        shape : int or tuple of ints\n            Array shape.\n        chunks : int or tuple of ints, optional\n            Chunk shape.\n        dtype : string or dtype, optional\n            NumPy dtype.\n        fill_value : object\n            Default value to use for uninitialized portions of the array.\n        filters : sequence, optional\n            Sequence of filters to use to encode chunk data prior to\n            compression.\n        read_only : bool, optional\n            True if array should be protected against modification.\n        synchronizer : object, optional\n            Array synchronizer.\n\n        Notes\n        -----\n        WARNING: This is an experimental feature and should be used with care.\n        There are plenty of ways to generate errors and/or cause data\n        corruption.\n\n        Examples\n        --------\n\n        Bypass filters:\n\n            >>> import zarr\n            >>> import numpy as np\n            >>> np.random.seed(42)\n            >>> labels = ['female', 'male']\n            >>> data = np.random.choice(labels, size=10000)\n            >>> filters = [zarr.Categorize(labels=labels,\n            ...                            dtype=data.dtype,\n            ...                            astype='u1')]\n            >>> a = zarr.array(data, chunks=1000, filters=filters)\n            >>> a[:]\n            array(['female', 'male', 'female', ..., 'male', 'male', 'female'],\n                  dtype='<U6')\n            >>> v = a.view(dtype='u1', filters=[])\n            >>> v.is_view\n            True\n            >>> v[:]\n            array([1, 2, 1, ..., 2, 2, 1], dtype=uint8)\n\n        Views can be used to modify data:\n\n            >>> x = v[:]\n            >>> x.sort()\n            >>> v[:] = x\n            >>> v[:]\n            array([1, 1, 1, ..., 2, 2, 2], dtype=uint8)\n            >>> a[:]\n            array(['female', 'female', 'female', ..., 'male', 'male', 'male'],\n                  dtype='<U6')\n\n        View as a different dtype with the same item size:\n\n            >>> data = np.random.randint(0, 2, size=10000, dtype='u1')\n            >>> a = zarr.array(data, chunks=1000)\n            >>> a[:]\n            array([0, 0, 1, ..., 1, 0, 0], dtype=uint8)\n            >>> v = a.view(dtype=bool)\n            >>> v[:]\n            array([False, False,  True, ...,  True, False, False])\n            >>> np.all(a[:].view(dtype=bool) == v[:])\n            True\n\n        An array can be viewed with a dtype with a different item size, however\n        some care is needed to adjust the shape and chunk shape so that chunk\n        data is interpreted correctly:\n\n            >>> data = np.arange(10000, dtype='u2')\n            >>> a = zarr.array(data, chunks=1000)\n            >>> a[:10]\n            array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint16)\n            >>> v = a.view(dtype='u1', shape=20000, chunks=2000)\n            >>> v[:10]\n            array([0, 0, 1, 0, 2, 0, 3, 0, 4, 0], dtype=uint8)\n            >>> np.all(a[:].view('u1') == v[:])\n            True\n\n        Change fill value for uninitialized chunks:\n\n            >>> a = zarr.full(10000, chunks=1000, fill_value=-1, dtype='i1')\n            >>> a[:]\n            array([-1, -1, -1, ..., -1, -1, -1], dtype=int8)\n            >>> v = a.view(fill_value=42)\n            >>> v[:]\n            array([42, 42, 42, ..., 42, 42, 42], dtype=int8)\n\n        Note that resizing or appending to views is not permitted:\n\n            >>> a = zarr.empty(10000)\n            >>> v = a.view()\n            >>> try:\n            ...     v.resize(20000)\n            ... except PermissionError as e:\n            ...     print(e)\n            operation not permitted for views\n\n        \"\"\"\n\n        store = self._store\n        chunk_store = self._chunk_store\n        path = self._path\n        if read_only is None:\n            read_only = self._read_only\n        if synchronizer is None:\n            synchronizer = self._synchronizer\n        a = Array(store=store, path=path, chunk_store=chunk_store, read_only=read_only,\n                  synchronizer=synchronizer, cache_metadata=True)\n        a._is_view = True\n\n        # allow override of some properties\n        if dtype is None:\n            dtype = self._dtype\n        else:\n            dtype = np.dtype(dtype)\n            a._dtype = dtype\n        if shape is None:\n            shape = self._shape\n        else:\n            shape = normalize_shape(shape)\n            a._shape = shape\n        if chunks is not None:\n            chunks = normalize_chunks(chunks, shape, dtype.itemsize)\n            a._chunks = chunks\n        if fill_value is not None:\n            a._fill_value = fill_value\n        if filters is not None:\n            a._filters = filters\n\n        return a"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef astype(self, dtype):\n\n        dtype = np.dtype(dtype)\n\n        filters = []\n        if self._filters:\n            filters.extend(self._filters)\n        filters.insert(0, AsType(encode_dtype=self._dtype, decode_dtype=dtype))\n\n        return self.view(filters=filters, dtype=dtype, read_only=True)", "response": "Returns a view that does on the fly type conversion of the underlying data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an array of the specified shape and dtype.", "response": "def create(shape, chunks=True, dtype=None, compressor='default',\n           fill_value=0, order='C', store=None, synchronizer=None,\n           overwrite=False, path=None, chunk_store=None, filters=None,\n           cache_metadata=True, cache_attrs=True, read_only=False,\n           object_codec=None, **kwargs):\n    \"\"\"Create an array.\n\n    Parameters\n    ----------\n    shape : int or tuple of ints\n        Array shape.\n    chunks : int or tuple of ints, optional\n        Chunk shape. If True, will be guessed from `shape` and `dtype`. If\n        False, will be set to `shape`, i.e., single chunk for the whole array.\n    dtype : string or dtype, optional\n        NumPy dtype.\n    compressor : Codec, optional\n        Primary compressor.\n    fill_value : object\n        Default value to use for uninitialized portions of the array.\n    order : {'C', 'F'}, optional\n        Memory layout to be used within each chunk.\n    store : MutableMapping or string\n        Store or path to directory in file system or name of zip file.\n    synchronizer : object, optional\n        Array synchronizer.\n    overwrite : bool, optional\n        If True, delete all pre-existing data in `store` at `path` before\n        creating the array.\n    path : string, optional\n        Path under which array is stored.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n    filters : sequence of Codecs, optional\n        Sequence of filters to use to encode chunk data prior to compression.\n    cache_metadata : bool, optional\n        If True, array configuration metadata will be cached for the\n        lifetime of the object. If False, array metadata will be reloaded\n        prior to all data access and modification operations (may incur\n        overhead depending on storage and data access pattern).\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    read_only : bool, optional\n        True if array should be protected against modification.\n    object_codec : Codec, optional\n        A codec to encode object arrays, only needed if dtype=object.\n\n    Returns\n    -------\n    z : zarr.core.Array\n\n    Examples\n    --------\n\n    Create an array with default settings::\n\n        >>> import zarr\n        >>> z = zarr.create((10000, 10000), chunks=(1000, 1000))\n        >>> z\n        <zarr.core.Array (10000, 10000) float64>\n\n    Create an array with different some different configuration options::\n\n        >>> from numcodecs import Blosc\n        >>> compressor = Blosc(cname='zstd', clevel=1, shuffle=Blosc.BITSHUFFLE)\n        >>> z = zarr.create((10000, 10000), chunks=(1000, 1000), dtype='i1', order='F',\n        ...                 compressor=compressor)\n        >>> z\n        <zarr.core.Array (10000, 10000) int8>\n\n    To create an array with object dtype requires a filter that can handle Python object\n    encoding, e.g., `MsgPack` or `Pickle` from `numcodecs`::\n\n        >>> from numcodecs import MsgPack\n        >>> z = zarr.create((10000, 10000), chunks=(1000, 1000), dtype=object,\n        ...                 object_codec=MsgPack())\n        >>> z\n        <zarr.core.Array (10000, 10000) object>\n\n    Example with some filters, and also storing chunks separately from metadata::\n\n        >>> from numcodecs import Quantize, Adler32\n        >>> store, chunk_store = dict(), dict()\n        >>> z = zarr.create((10000, 10000), chunks=(1000, 1000), dtype='f8',\n        ...                 filters=[Quantize(digits=2, dtype='f8'), Adler32()],\n        ...                 store=store, chunk_store=chunk_store)\n        >>> z\n        <zarr.core.Array (10000, 10000) float64>\n\n    \"\"\"\n\n    # handle polymorphic store arg\n    store = normalize_store_arg(store)\n\n    # API compatibility with h5py\n    compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n\n    # initialize array metadata\n    init_array(store, shape=shape, chunks=chunks, dtype=dtype, compressor=compressor,\n               fill_value=fill_value, order=order, overwrite=overwrite, path=path,\n               chunk_store=chunk_store, filters=filters, object_codec=object_codec)\n\n    # instantiate array\n    z = Array(store, path=path, chunk_store=chunk_store, synchronizer=synchronizer,\n              cache_metadata=cache_metadata, cache_attrs=cache_attrs, read_only=read_only)\n\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef full(shape, fill_value, **kwargs):\n\n    return create(shape=shape, fill_value=fill_value, **kwargs)", "response": "Create an array of the specified shape and fill_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef array(data, **kwargs):\n\n    # ensure data is array-like\n    if not hasattr(data, 'shape') or not hasattr(data, 'dtype'):\n        data = np.asanyarray(data)\n\n    # setup dtype\n    kw_dtype = kwargs.get('dtype', None)\n    if kw_dtype is None:\n        kwargs['dtype'] = data.dtype\n    else:\n        kwargs['dtype'] = kw_dtype\n\n    # setup shape and chunks\n    data_shape, data_chunks = _get_shape_chunks(data)\n    kwargs['shape'] = data_shape\n    kw_chunks = kwargs.get('chunks', None)\n    if kw_chunks is None:\n        kwargs['chunks'] = data_chunks\n    else:\n        kwargs['chunks'] = kw_chunks\n\n    # pop read-only to apply after storing the data\n    read_only = kwargs.pop('read_only', False)\n\n    # instantiate array\n    z = create(**kwargs)\n\n    # fill with data\n    z[...] = data\n\n    # set read_only property afterwards\n    z.read_only = read_only\n\n    return z", "response": "Create an array filled with data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens an array using file - mode - like semantics.", "response": "def open_array(store=None, mode='a', shape=None, chunks=True, dtype=None,\n               compressor='default', fill_value=0, order='C', synchronizer=None,\n               filters=None, cache_metadata=True, cache_attrs=True, path=None,\n               object_codec=None, chunk_store=None, **kwargs):\n    \"\"\"Open an array using file-mode-like semantics.\n\n    Parameters\n    ----------\n    store : MutableMapping or string, optional\n        Store or path to directory in file system or name of zip file.\n    mode : {'r', 'r+', 'a', 'w', 'w-'}, optional\n        Persistence mode: 'r' means read only (must exist); 'r+' means\n        read/write (must exist); 'a' means read/write (create if doesn't\n        exist); 'w' means create (overwrite if exists); 'w-' means create\n        (fail if exists).\n    shape : int or tuple of ints, optional\n        Array shape.\n    chunks : int or tuple of ints, optional\n        Chunk shape. If True, will be guessed from `shape` and `dtype`. If\n        False, will be set to `shape`, i.e., single chunk for the whole array.\n    dtype : string or dtype, optional\n        NumPy dtype.\n    compressor : Codec, optional\n        Primary compressor.\n    fill_value : object, optional\n        Default value to use for uninitialized portions of the array.\n    order : {'C', 'F'}, optional\n        Memory layout to be used within each chunk.\n    synchronizer : object, optional\n        Array synchronizer.\n    filters : sequence, optional\n        Sequence of filters to use to encode chunk data prior to compression.\n    cache_metadata : bool, optional\n        If True, array configuration metadata will be cached for the\n        lifetime of the object. If False, array metadata will be reloaded\n        prior to all data access and modification operations (may incur\n        overhead depending on storage and data access pattern).\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    path : string, optional\n        Array path within store.\n    object_codec : Codec, optional\n        A codec to encode object arrays, only needed if dtype=object.\n    chunk_store : MutableMapping or string, optional\n        Store or path to directory in file system or name of zip file.\n\n    Returns\n    -------\n    z : zarr.core.Array\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import zarr\n    >>> z1 = zarr.open_array('data/example.zarr', mode='w', shape=(10000, 10000),\n    ...                      chunks=(1000, 1000), fill_value=0)\n    >>> z1[:] = np.arange(100000000).reshape(10000, 10000)\n    >>> z1\n    <zarr.core.Array (10000, 10000) float64>\n    >>> z2 = zarr.open_array('data/example.zarr', mode='r')\n    >>> z2\n    <zarr.core.Array (10000, 10000) float64 read-only>\n    >>> np.all(z1[:] == z2[:])\n    True\n\n    Notes\n    -----\n    There is no need to close an array. Data are automatically flushed to the\n    file system.\n\n    \"\"\"\n\n    # use same mode semantics as h5py\n    # r : read only, must exist\n    # r+ : read/write, must exist\n    # w : create, delete if exists\n    # w- or x : create, fail if exists\n    # a : read/write if exists, create otherwise (default)\n\n    # handle polymorphic store arg\n    clobber = mode == 'w'\n    store = normalize_store_arg(store, clobber=clobber)\n    if chunk_store is not None:\n        chunk_store = normalize_store_arg(chunk_store, clobber=clobber)\n    path = normalize_storage_path(path)\n\n    # API compatibility with h5py\n    compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n\n    # ensure fill_value of correct type\n    if fill_value is not None:\n        fill_value = np.array(fill_value, dtype=dtype)[()]\n\n    # ensure store is initialized\n\n    if mode in ['r', 'r+']:\n        if contains_group(store, path=path):\n            err_contains_group(path)\n        elif not contains_array(store, path=path):\n            err_array_not_found(path)\n\n    elif mode == 'w':\n        init_array(store, shape=shape, chunks=chunks, dtype=dtype,\n                   compressor=compressor, fill_value=fill_value,\n                   order=order, filters=filters, overwrite=True, path=path,\n                   object_codec=object_codec, chunk_store=chunk_store)\n\n    elif mode == 'a':\n        if contains_group(store, path=path):\n            err_contains_group(path)\n        elif not contains_array(store, path=path):\n            init_array(store, shape=shape, chunks=chunks, dtype=dtype,\n                       compressor=compressor, fill_value=fill_value,\n                       order=order, filters=filters, path=path,\n                       object_codec=object_codec, chunk_store=chunk_store)\n\n    elif mode in ['w-', 'x']:\n        if contains_group(store, path=path):\n            err_contains_group(path)\n        elif contains_array(store, path=path):\n            err_contains_array(path)\n        else:\n            init_array(store, shape=shape, chunks=chunks, dtype=dtype,\n                       compressor=compressor, fill_value=fill_value,\n                       order=order, filters=filters, path=path,\n                       object_codec=object_codec, chunk_store=chunk_store)\n\n    # determine read only status\n    read_only = mode == 'r'\n\n    # instantiate array\n    z = Array(store, read_only=read_only, synchronizer=synchronizer,\n              cache_metadata=cache_metadata, cache_attrs=cache_attrs, path=path,\n              chunk_store=chunk_store)\n\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef full_like(a, **kwargs):\n    _like_args(a, kwargs)\n    if isinstance(a, Array):\n        kwargs.setdefault('fill_value', a.fill_value)\n    return full(**kwargs)", "response": "Create a filled array like a."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a persistent array like a.", "response": "def open_like(a, path, **kwargs):\n    \"\"\"Open a persistent array like `a`.\"\"\"\n    _like_args(a, kwargs)\n    if isinstance(a, Array):\n        kwargs.setdefault('fill_value', a.fill_value)\n    return open_array(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_array(store, arr, **kwargs):\n    may_need_closing = isinstance(store, str)\n    store = normalize_store_arg(store, clobber=True)\n    try:\n        _create_array(arr, store=store, overwrite=True, **kwargs)\n    finally:\n        if may_need_closing and hasattr(store, 'close'):\n            # needed to ensure zip file records are written\n            store.close()", "response": "Convenience function to save a NumPy array to the local file system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_group(store, *args, **kwargs):\n    if len(args) == 0 and len(kwargs) == 0:\n        raise ValueError('at least one array must be provided')\n    # handle polymorphic store arg\n    may_need_closing = isinstance(store, str)\n    store = normalize_store_arg(store, clobber=True)\n    try:\n        grp = _create_group(store, overwrite=True)\n        for i, arr in enumerate(args):\n            k = 'arr_{}'.format(i)\n            grp.create_dataset(k, data=arr, overwrite=True)\n        for k, arr in kwargs.items():\n            grp.create_dataset(k, data=arr, overwrite=True)\n    finally:\n        if may_need_closing and hasattr(store, 'close'):\n            # needed to ensure zip file records are written\n            store.close()", "response": "Convenience function to save several NumPy arrays to the local file system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(store):\n    # handle polymorphic store arg\n    store = normalize_store_arg(store)\n    if contains_array(store, path=None):\n        return Array(store=store, path=None)[...]\n    elif contains_group(store, path=None):\n        grp = Group(store=store, path=None)\n        return LazyLoader(grp)", "response": "Load data from an array or group into memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tree view of a group or Zarr group.", "response": "def tree(grp, expand=False, level=None):\n    \"\"\"Provide a ``print``-able display of the hierarchy. This function is provided\n    mainly as a convenience for obtaining a tree view of an h5py group - zarr groups\n    have a ``.tree()`` method.\n\n    Parameters\n    ----------\n    grp : Group\n        Zarr or h5py group.\n    expand : bool, optional\n        Only relevant for HTML representation. If True, tree will be fully expanded.\n    level : int, optional\n        Maximum depth to descend into hierarchy.\n\n    Examples\n    --------\n    >>> import zarr\n    >>> g1 = zarr.group()\n    >>> g2 = g1.create_group('foo')\n    >>> g3 = g1.create_group('bar')\n    >>> g4 = g3.create_group('baz')\n    >>> g5 = g3.create_group('qux')\n    >>> d1 = g5.create_dataset('baz', shape=100, chunks=10)\n    >>> g1.tree()\n    /\n     \u251c\u2500\u2500 bar\n     \u2502   \u251c\u2500\u2500 baz\n     \u2502   \u2514\u2500\u2500 qux\n     \u2502       \u2514\u2500\u2500 baz (100,) float64\n     \u2514\u2500\u2500 foo\n    >>> import h5py\n    >>> h5f = h5py.File('data/example.h5', mode='w')\n    >>> zarr.copy_all(g1, h5f)\n    (5, 0, 800)\n    >>> zarr.tree(h5f)\n    /\n     \u251c\u2500\u2500 bar\n     \u2502   \u251c\u2500\u2500 baz\n     \u2502   \u2514\u2500\u2500 qux\n     \u2502       \u2514\u2500\u2500 baz (100,) float64\n     \u2514\u2500\u2500 foo\n\n    See Also\n    --------\n    zarr.hierarchy.Group.tree\n\n    Notes\n    -----\n    Please note that this is an experimental feature. The behaviour of this\n    function is still evolving and the default output and/or parameters may change\n    in future versions.\n\n    \"\"\"\n\n    return TreeViewer(grp, expand=expand, level=level)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_store(source, dest, source_path='', dest_path='', excludes=None,\n               includes=None, flags=0, if_exists='raise', dry_run=False,\n               log=None):\n    \"\"\"Copy data directly from the `source` store to the `dest` store. Use this\n    function when you want to copy a group or array in the most efficient way,\n    preserving all configuration and attributes. This function is more efficient\n    than the copy() or copy_all() functions because it avoids de-compressing and\n    re-compressing data, rather the compressed chunk data for each array are\n    copied directly between stores.\n\n    Parameters\n    ----------\n    source : Mapping\n        Store to copy data from.\n    dest : MutableMapping\n        Store to copy data into.\n    source_path : str, optional\n        Only copy data from under this path in the source store.\n    dest_path : str, optional\n        Copy data into this path in the destination store.\n    excludes : sequence of str, optional\n        One or more regular expressions which will be matched against keys in\n        the source store. Any matching key will not be copied.\n    includes : sequence of str, optional\n        One or more regular expressions which will be matched against keys in\n        the source store and will override any excludes also matching.\n    flags : int, optional\n        Regular expression flags used for matching excludes and includes.\n    if_exists : {'raise', 'replace', 'skip'}, optional\n        How to handle keys that already exist in the destination store. If\n        'raise' then a CopyError is raised on the first key already present\n        in the destination store. If 'replace' then any data will be replaced in\n        the destination. If 'skip' then any existing keys will not be copied.\n    dry_run : bool, optional\n        If True, don't actually copy anything, just log what would have\n        happened.\n    log : callable, file path or file-like object, optional\n        If provided, will be used to log progress information.\n\n    Returns\n    -------\n    n_copied : int\n        Number of items copied.\n    n_skipped : int\n        Number of items skipped.\n    n_bytes_copied : int\n        Number of bytes of data that were actually copied.\n\n    Examples\n    --------\n\n    >>> import zarr\n    >>> store1 = zarr.DirectoryStore('data/example.zarr')\n    >>> root = zarr.group(store1, overwrite=True)\n    >>> foo = root.create_group('foo')\n    >>> bar = foo.create_group('bar')\n    >>> baz = bar.create_dataset('baz', shape=100, chunks=50, dtype='i8')\n    >>> import numpy as np\n    >>> baz[:] = np.arange(100)\n    >>> root.tree()\n    /\n     \u2514\u2500\u2500 foo\n         \u2514\u2500\u2500 bar\n             \u2514\u2500\u2500 baz (100,) int64\n    >>> from sys import stdout\n    >>> store2 = zarr.ZipStore('data/example.zip', mode='w')\n    >>> zarr.copy_store(store1, store2, log=stdout)\n    copy .zgroup\n    copy foo/.zgroup\n    copy foo/bar/.zgroup\n    copy foo/bar/baz/.zarray\n    copy foo/bar/baz/0\n    copy foo/bar/baz/1\n    all done: 6 copied, 0 skipped, 566 bytes copied\n    (6, 0, 566)\n    >>> new_root = zarr.group(store2)\n    >>> new_root.tree()\n    /\n     \u2514\u2500\u2500 foo\n         \u2514\u2500\u2500 bar\n             \u2514\u2500\u2500 baz (100,) int64\n    >>> new_root['foo/bar/baz'][:]\n    array([ 0,  1,  2,  ..., 97, 98, 99])\n    >>> store2.close()  # zip stores need to be closed\n\n    Notes\n    -----\n    Please note that this is an experimental feature. The behaviour of this\n    function is still evolving and the default behaviour and/or parameters may change\n    in future versions.\n\n    \"\"\"\n\n    # normalize paths\n    source_path = normalize_storage_path(source_path)\n    dest_path = normalize_storage_path(dest_path)\n    if source_path:\n        source_path = source_path + '/'\n    if dest_path:\n        dest_path = dest_path + '/'\n\n    # normalize excludes and includes\n    if excludes is None:\n        excludes = []\n    elif isinstance(excludes, str):\n        excludes = [excludes]\n    if includes is None:\n        includes = []\n    elif isinstance(includes, str):\n        includes = [includes]\n    excludes = [re.compile(e, flags) for e in excludes]\n    includes = [re.compile(i, flags) for i in includes]\n\n    # check if_exists parameter\n    valid_if_exists = ['raise', 'replace', 'skip']\n    if if_exists not in valid_if_exists:\n        raise ValueError('if_exists must be one of {!r}; found {!r}'\n                         .format(valid_if_exists, if_exists))\n\n    # setup counting variables\n    n_copied = n_skipped = n_bytes_copied = 0\n\n    # setup logging\n    with _LogWriter(log) as log:\n\n        # iterate over source keys\n        for source_key in sorted(source.keys()):\n\n            # filter to keys under source path\n            if source_key.startswith(source_path):\n\n                # process excludes and includes\n                exclude = False\n                for prog in excludes:\n                    if prog.search(source_key):\n                        exclude = True\n                        break\n                if exclude:\n                    for prog in includes:\n                        if prog.search(source_key):\n                            exclude = False\n                            break\n                if exclude:\n                    continue\n\n                # map key to destination path\n                key_suffix = source_key[len(source_path):]\n                dest_key = dest_path + key_suffix\n\n                # create a descriptive label for this operation\n                descr = source_key\n                if dest_key != source_key:\n                    descr = descr + ' -> ' + dest_key\n\n                # decide what to do\n                do_copy = True\n                if if_exists != 'replace':\n                    if dest_key in dest:\n                        if if_exists == 'raise':\n                            raise CopyError('key {!r} exists in destination'\n                                            .format(dest_key))\n                        elif if_exists == 'skip':\n                            do_copy = False\n\n                # take action\n                if do_copy:\n                    log('copy {}'.format(descr))\n                    if not dry_run:\n                        data = source[source_key]\n                        n_bytes_copied += buffer_size(data)\n                        dest[dest_key] = data\n                    n_copied += 1\n                else:\n                    log('skip {}'.format(descr))\n                    n_skipped += 1\n\n        # log a final message with a summary of what happened\n        _log_copy_summary(log, dry_run, n_copied, n_skipped, n_bytes_copied)\n\n    return n_copied, n_skipped, n_bytes_copied", "response": "Copy data from source store to dest store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy the source array or dataset into the dest array or dataset.", "response": "def copy(source, dest, name=None, shallow=False, without_attrs=False, log=None,\n         if_exists='raise', dry_run=False, **create_kws):\n    \"\"\"Copy the `source` array or group into the `dest` group.\n\n    Parameters\n    ----------\n    source : group or array/dataset\n        A zarr group or array, or an h5py group or dataset.\n    dest : group\n        A zarr or h5py group.\n    name : str, optional\n        Name to copy the object to.\n    shallow : bool, optional\n        If True, only copy immediate children of `source`.\n    without_attrs : bool, optional\n        Do not copy user attributes.\n    log : callable, file path or file-like object, optional\n        If provided, will be used to log progress information.\n    if_exists : {'raise', 'replace', 'skip', 'skip_initialized'}, optional\n        How to handle arrays that already exist in the destination group. If\n        'raise' then a CopyError is raised on the first array already present\n        in the destination group. If 'replace' then any array will be\n        replaced in the destination. If 'skip' then any existing arrays will\n        not be copied. If 'skip_initialized' then any existing arrays with\n        all chunks initialized will not be copied (not available when copying to\n        h5py).\n    dry_run : bool, optional\n        If True, don't actually copy anything, just log what would have\n        happened.\n    **create_kws\n        Passed through to the create_dataset method when copying an array/dataset.\n\n    Returns\n    -------\n    n_copied : int\n        Number of items copied.\n    n_skipped : int\n        Number of items skipped.\n    n_bytes_copied : int\n        Number of bytes of data that were actually copied.\n\n    Examples\n    --------\n    Here's an example of copying a group named 'foo' from an HDF5 file to a\n    Zarr group::\n\n        >>> import h5py\n        >>> import zarr\n        >>> import numpy as np\n        >>> source = h5py.File('data/example.h5', mode='w')\n        >>> foo = source.create_group('foo')\n        >>> baz = foo.create_dataset('bar/baz', data=np.arange(100), chunks=(50,))\n        >>> spam = source.create_dataset('spam', data=np.arange(100, 200), chunks=(30,))\n        >>> zarr.tree(source)\n        /\n         \u251c\u2500\u2500 foo\n         \u2502   \u2514\u2500\u2500 bar\n         \u2502       \u2514\u2500\u2500 baz (100,) int64\n         \u2514\u2500\u2500 spam (100,) int64\n        >>> dest = zarr.group()\n        >>> from sys import stdout\n        >>> zarr.copy(source['foo'], dest, log=stdout)\n        copy /foo\n        copy /foo/bar\n        copy /foo/bar/baz (100,) int64\n        all done: 3 copied, 0 skipped, 800 bytes copied\n        (3, 0, 800)\n        >>> dest.tree()  # N.B., no spam\n        /\n         \u2514\u2500\u2500 foo\n             \u2514\u2500\u2500 bar\n                 \u2514\u2500\u2500 baz (100,) int64\n        >>> source.close()\n\n    The ``if_exists`` parameter provides options for how to handle pre-existing data in\n    the destination. Here are some examples of these options, also using\n    ``dry_run=True`` to find out what would happen without actually copying anything::\n\n        >>> source = zarr.group()\n        >>> dest = zarr.group()\n        >>> baz = source.create_dataset('foo/bar/baz', data=np.arange(100))\n        >>> spam = source.create_dataset('foo/spam', data=np.arange(1000))\n        >>> existing_spam = dest.create_dataset('foo/spam', data=np.arange(1000))\n        >>> from sys import stdout\n        >>> try:\n        ...     zarr.copy(source['foo'], dest, log=stdout, dry_run=True)\n        ... except zarr.CopyError as e:\n        ...     print(e)\n        ...\n        copy /foo\n        copy /foo/bar\n        copy /foo/bar/baz (100,) int64\n        an object 'spam' already exists in destination '/foo'\n        >>> zarr.copy(source['foo'], dest, log=stdout, if_exists='replace', dry_run=True)\n        copy /foo\n        copy /foo/bar\n        copy /foo/bar/baz (100,) int64\n        copy /foo/spam (1000,) int64\n        dry run: 4 copied, 0 skipped\n        (4, 0, 0)\n        >>> zarr.copy(source['foo'], dest, log=stdout, if_exists='skip', dry_run=True)\n        copy /foo\n        copy /foo/bar\n        copy /foo/bar/baz (100,) int64\n        skip /foo/spam (1000,) int64\n        dry run: 3 copied, 1 skipped\n        (3, 1, 0)\n\n    Notes\n    -----\n    Please note that this is an experimental feature. The behaviour of this\n    function is still evolving and the default behaviour and/or parameters may change\n    in future versions.\n\n    \"\"\"\n\n    # value checks\n    _check_dest_is_group(dest)\n\n    # setup logging\n    with _LogWriter(log) as log:\n\n        # do the copying\n        n_copied, n_skipped, n_bytes_copied = _copy(\n            log, source, dest, name=name, root=True, shallow=shallow,\n            without_attrs=without_attrs, if_exists=if_exists, dry_run=dry_run,\n            **create_kws\n        )\n\n        # log a final message with a summary of what happened\n        _log_copy_summary(log, dry_run, n_copied, n_skipped, n_bytes_copied)\n\n        return n_copied, n_skipped, n_bytes_copied"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy_all(source, dest, shallow=False, without_attrs=False, log=None,\n             if_exists='raise', dry_run=False, **create_kws):\n    \"\"\"Copy all children of the `source` group into the `dest` group.\n\n    Parameters\n    ----------\n    source : group or array/dataset\n        A zarr group or array, or an h5py group or dataset.\n    dest : group\n        A zarr or h5py group.\n    shallow : bool, optional\n        If True, only copy immediate children of `source`.\n    without_attrs : bool, optional\n        Do not copy user attributes.\n    log : callable, file path or file-like object, optional\n        If provided, will be used to log progress information.\n    if_exists : {'raise', 'replace', 'skip', 'skip_initialized'}, optional\n        How to handle arrays that already exist in the destination group. If\n        'raise' then a CopyError is raised on the first array already present\n        in the destination group. If 'replace' then any array will be\n        replaced in the destination. If 'skip' then any existing arrays will\n        not be copied. If 'skip_initialized' then any existing arrays with\n        all chunks initialized will not be copied (not available when copying to\n        h5py).\n    dry_run : bool, optional\n        If True, don't actually copy anything, just log what would have\n        happened.\n    **create_kws\n        Passed through to the create_dataset method when copying an\n        array/dataset.\n\n    Returns\n    -------\n    n_copied : int\n        Number of items copied.\n    n_skipped : int\n        Number of items skipped.\n    n_bytes_copied : int\n        Number of bytes of data that were actually copied.\n\n    Examples\n    --------\n    >>> import h5py\n    >>> import zarr\n    >>> import numpy as np\n    >>> source = h5py.File('data/example.h5', mode='w')\n    >>> foo = source.create_group('foo')\n    >>> baz = foo.create_dataset('bar/baz', data=np.arange(100), chunks=(50,))\n    >>> spam = source.create_dataset('spam', data=np.arange(100, 200), chunks=(30,))\n    >>> zarr.tree(source)\n    /\n     \u251c\u2500\u2500 foo\n     \u2502   \u2514\u2500\u2500 bar\n     \u2502       \u2514\u2500\u2500 baz (100,) int64\n     \u2514\u2500\u2500 spam (100,) int64\n    >>> dest = zarr.group()\n    >>> import sys\n    >>> zarr.copy_all(source, dest, log=sys.stdout)\n    copy /foo\n    copy /foo/bar\n    copy /foo/bar/baz (100,) int64\n    copy /spam (100,) int64\n    all done: 4 copied, 0 skipped, 1,600 bytes copied\n    (4, 0, 1600)\n    >>> dest.tree()\n    /\n     \u251c\u2500\u2500 foo\n     \u2502   \u2514\u2500\u2500 bar\n     \u2502       \u2514\u2500\u2500 baz (100,) int64\n     \u2514\u2500\u2500 spam (100,) int64\n    >>> source.close()\n\n    Notes\n    -----\n    Please note that this is an experimental feature. The behaviour of this\n    function is still evolving and the default behaviour and/or parameters may change\n    in future versions.\n\n    \"\"\"\n\n    # value checks\n    _check_dest_is_group(dest)\n\n    # setup counting variables\n    n_copied = n_skipped = n_bytes_copied = 0\n\n    # setup logging\n    with _LogWriter(log) as log:\n\n        for k in source.keys():\n            c, s, b = _copy(\n                log, source[k], dest, name=k, root=False, shallow=shallow,\n                without_attrs=without_attrs, if_exists=if_exists,\n                dry_run=dry_run, **create_kws)\n            n_copied += c\n            n_skipped += s\n            n_bytes_copied += b\n\n        # log a final message with a summary of what happened\n        _log_copy_summary(log, dry_run, n_copied, n_skipped, n_bytes_copied)\n\n    return n_copied, n_skipped, n_bytes_copied", "response": "Copy all children of source into dest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consolidate_metadata(store, metadata_key='.zmetadata'):\n    store = normalize_store_arg(store)\n\n    def is_zarr_key(key):\n        return (key.endswith('.zarray') or key.endswith('.zgroup') or\n                key.endswith('.zattrs'))\n\n    out = {\n        'zarr_consolidated_format': 1,\n        'metadata': {\n            key: json_loads(store[key])\n            for key in store if is_zarr_key(key)\n        }\n    }\n    store[metadata_key] = json_dumps(out)\n    return open_consolidated(store, metadata_key=metadata_key)", "response": "Consolidate all metadata for groups and arrays within the given store and put it under the given key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening a group using consolidated metadata.", "response": "def open_consolidated(store, metadata_key='.zmetadata', mode='r+', **kwargs):\n    \"\"\"Open group using metadata previously consolidated into a single key.\n\n    This is an optimised method for opening a Zarr group, where instead of\n    traversing the group/array hierarchy by accessing the metadata keys at\n    each level, a single key contains all of the metadata for everything.\n    For remote data sources where the overhead of accessing a key is large\n    compared to the time to read data.\n\n    The group accessed must have already had its metadata consolidated into a\n    single key using the function :func:`consolidate_metadata`.\n\n    This optimised method only works in modes which do not change the\n    metadata, although the data may still be written/updated.\n\n    Parameters\n    ----------\n    store : MutableMapping or string\n        Store or path to directory in file system or name of zip file.\n    metadata_key : str\n        Key to read the consolidated metadata from. The default (.zmetadata)\n        corresponds to the default used by :func:`consolidate_metadata`.\n    mode : {'r', 'r+'}, optional\n        Persistence mode: 'r' means read only (must exist); 'r+' means\n        read/write (must exist) although only writes to data are allowed,\n        changes to metadata including creation of new arrays or group\n        are not allowed.\n    **kwargs\n        Additional parameters are passed through to :func:`zarr.creation.open_array` or\n        :func:`zarr.hierarchy.open_group`.\n\n    Returns\n    -------\n    g : :class:`zarr.hierarchy.Group`\n        Group instance, opened with the consolidated metadata.\n\n    See Also\n    --------\n    consolidate_metadata\n\n    \"\"\"\n\n    from .storage import ConsolidatedMetadataStore\n\n    # normalize parameters\n    store = normalize_store_arg(store)\n    if mode not in {'r', 'r+'}:\n        raise ValueError(\"invalid mode, expected either 'r' or 'r+'; found {!r}\"\n                         .format(mode))\n\n    # setup metadata sotre\n    meta_store = ConsolidatedMetadataStore(store, metadata_key=metadata_key)\n\n    # pass through\n    return open(store=meta_store, chunk_store=store, mode=mode, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist volumes with pagination.", "response": "def volume_list_paged(request, search_opts=None, marker=None, paginate=False,\n                      sort_dir=\"desc\"):\n    \"\"\"List volumes with pagination.\n\n    To see all volumes in the cloud as an admin you can pass in a special\n    search option: {'all_tenants': 1}\n    \"\"\"\n    has_more_data = False\n    has_prev_data = False\n    volumes = []\n\n    # To support filtering with group_id, we need to use the microversion.\n    c_client = _cinderclient_with_generic_groups(request)\n    if c_client is None:\n        return volumes, has_more_data, has_prev_data\n\n    # build a dictionary of volume_id -> transfer\n    transfers = {t.volume_id: t\n                 for t in transfer_list(request, search_opts=search_opts)}\n\n    if VERSIONS.active > 1 and paginate:\n        page_size = utils.get_page_size(request)\n        # sort_key and sort_dir deprecated in kilo, use sort\n        # if pagination is true, we use a single sort parameter\n        # by default, it is \"created_at\"\n        sort = 'created_at:' + sort_dir\n        for v in c_client.volumes.list(search_opts=search_opts,\n                                       limit=page_size + 1,\n                                       marker=marker,\n                                       sort=sort):\n            v.transfer = transfers.get(v.id)\n            volumes.append(Volume(v))\n        volumes, has_more_data, has_prev_data = update_pagination(\n            volumes, page_size, marker, sort_dir)\n    else:\n        for v in c_client.volumes.list(search_opts=search_opts):\n            v.transfer = transfers.get(v.id)\n            volumes.append(Volume(v))\n\n    return volumes, has_more_data, has_prev_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting volume transfers. To see all volumes transfers as an admin pass in a special search option: {'all_tenants': 1}", "response": "def transfer_list(request, detailed=True, search_opts=None):\n    \"\"\"List volume transfers.\n\n    To see all volumes transfers as an admin pass in a special\n    search option: {'all_tenants': 1}\n    \"\"\"\n    c_client = cinderclient(request)\n    try:\n        return [VolumeTransfer(v) for v in c_client.transfers.list(\n            detailed=detailed, search_opts=search_opts)]\n    except cinder_exception.Forbidden as error:\n        LOG.error(error)\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tenant_quota_usages(request, tenant_id=None, targets=None):\n    if not tenant_id:\n        tenant_id = request.user.project_id\n\n    disabled_quotas = get_disabled_quotas(request, targets)\n    usages = QuotaUsage()\n\n    futurist_utils.call_functions_parallel(\n        (_get_tenant_compute_usages,\n         [request, usages, disabled_quotas, tenant_id]),\n        (_get_tenant_network_usages,\n         [request, usages, disabled_quotas, tenant_id]),\n        (_get_tenant_volume_usages,\n         [request, usages, disabled_quotas, tenant_id]))\n\n    return usages", "response": "Get our quotas and construct our usage object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_quota(self, quota):\n        if quota.limit in (None, -1, float('inf')):\n            # Handle \"unlimited\" quotas.\n            self.usages[quota.name]['quota'] = float(\"inf\")\n            self.usages[quota.name]['available'] = float(\"inf\")\n        else:\n            self.usages[quota.name]['quota'] = int(quota.limit)", "response": "Adds an internal tracking reference for the given quota."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd to the used metric for the given quota.", "response": "def tally(self, name, value):\n        \"\"\"Adds to the \"used\" metric for the given quota.\"\"\"\n        value = value or 0  # Protection against None.\n        # Start at 0 if this is the first value.\n        if 'used' not in self.usages[name]:\n            self.usages[name]['used'] = 0\n        # Increment our usage and update the \"available\" metric.\n        self.usages[name]['used'] += int(value)  # Fail if can't coerce to int.\n        self.update_available(name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the available metric for the given quota.", "response": "def update_available(self, name):\n        \"\"\"Updates the \"available\" metric for the given quota.\"\"\"\n        quota = self.usages.get(name, {}).get('quota', float('inf'))\n        available = quota - self.usages[name]['used']\n        if available < 0:\n            available = 0\n        self.usages[name]['available'] = available"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses filters by non - API fields", "response": "def process_non_api_filters(search_opts, non_api_filter_info):\n    \"\"\"Process filters by non-API fields\n\n    There are cases where it is useful to provide a filter field\n    which does not exist in a resource in a backend service.\n    For example, nova server list provides 'image' field with image ID\n    but 'image name' is more useful for GUI users.\n    This function replaces fake fields into corresponding real fields.\n\n    The format of non_api_filter_info is a tuple/list of\n    (fake_field, real_field, resources).\n\n    This returns True if further lookup is required.\n    It returns False if there are no matching resources,\n    for example, if no corresponding real field exists.\n    \"\"\"\n    for fake_field, real_field, resources in non_api_filter_info:\n        if not _swap_filter(resources, search_opts, fake_field, real_field):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\naccepts a port number or a single - colon separated range.", "response": "def validate_port_or_colon_separated_port_range(port_range):\n    \"\"\"Accepts a port number or a single-colon separated range.\"\"\"\n    if port_range.count(':') > 1:\n        raise ValidationError(_(\"One colon allowed in port range\"))\n    ports = port_range.split(':')\n    for port in ports:\n        validate_port_range(port)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_submodules(module):\n    submodules = {}\n    for loader, name, ispkg in pkgutil.iter_modules(module.__path__,\n                                                    module.__name__ + '.'):\n        try:\n            submodule = import_module(name)\n        except ImportError as e:\n            # FIXME: Make the errors non-fatal (do we want that?).\n            logging.warning(\"Error importing %s\", name)\n            logging.exception(e)\n        else:\n            parent, child = name.rsplit('.', 1)\n            submodules[child] = submodule\n    return submodules", "response": "Import all submodules and make them available in a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports configuration from all the modules and merges it.", "response": "def import_dashboard_config(modules):\n    \"\"\"Imports configuration from all the modules and merges it.\"\"\"\n    config = collections.defaultdict(dict)\n    for module in modules:\n        for submodule in import_submodules(module).values():\n            if hasattr(submodule, 'DASHBOARD'):\n                dashboard = submodule.DASHBOARD\n                config[dashboard].update(submodule.__dict__)\n            elif (hasattr(submodule, 'PANEL') or\n                  hasattr(submodule, 'PANEL_GROUP') or\n                  hasattr(submodule, 'FEATURE')):\n                # If enabled and local.enabled contains a same filename,\n                # the file loaded later (i.e., local.enabled) will be used.\n                name = submodule.__name__.rsplit('.', 1)[1]\n                config[name] = submodule.__dict__\n            else:\n                logging.warning(\"Skipping %s because it doesn't have DASHBOARD\"\n                                \", PANEL, PANEL_GROUP, or FEATURE defined.\",\n                                submodule.__name__)\n    return sorted(config.items(),\n                  key=lambda c: c[1]['__name__'].rsplit('.', 1)[1])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_dashboards(modules, horizon_config, installed_apps):\n    config_dashboards = horizon_config.get('dashboards', [])\n    if config_dashboards or horizon_config.get('default_dashboard'):\n        logging.warning(\n            '\"dashboards\" and \"default_dashboard\" in (local_)settings is '\n            'DEPRECATED now and may be unsupported in some future release. '\n            'The preferred way to specify the order of dashboards and the '\n            'default dashboard is the pluggable dashboard mechanism (in %s).',\n            ', '.join([os.path.abspath(module.__path__[0])\n                       for module in modules])\n        )\n\n    enabled_dashboards = []\n    disabled_dashboards = []\n    exceptions = horizon_config.get('exceptions', {})\n    apps = []\n    angular_modules = []\n    js_files = []\n    js_spec_files = []\n    scss_files = []\n    xstatic_modules = []\n    panel_customization = []\n    header_sections = []\n    extra_tabs = collections.defaultdict(tuple)\n    extra_steps = collections.defaultdict(tuple)\n    update_horizon_config = {}\n    for key, config in import_dashboard_config(modules):\n        if config.get('DISABLED', False):\n            if config.get('DASHBOARD'):\n                disabled_dashboards.append(config.get('DASHBOARD'))\n            continue\n\n        _apps = config.get('ADD_INSTALLED_APPS', [])\n        apps.extend(_apps)\n\n        _header_sections = config.get('ADD_HEADER_SECTIONS', [])\n        header_sections.extend(_header_sections)\n\n        if config.get('AUTO_DISCOVER_STATIC_FILES', False):\n            for _app in _apps:\n                module = import_module(_app)\n                base_path = os.path.join(module.__path__[0], 'static/')\n                file_discovery.populate_horizon_config(horizon_config,\n                                                       base_path)\n\n        add_exceptions = config.get('ADD_EXCEPTIONS', {}).items()\n        for category, exc_list in add_exceptions:\n            exceptions[category] = tuple(set(exceptions.get(category, ()) +\n                                             exc_list))\n\n        angular_modules.extend(config.get('ADD_ANGULAR_MODULES', []))\n        # avoid pulling in dashboard javascript dependencies multiple times\n        existing = set(js_files)\n        js_files.extend([f for f in config.get('ADD_JS_FILES', [])\n                         if f not in existing])\n        js_spec_files.extend(config.get('ADD_JS_SPEC_FILES', []))\n        scss_files.extend(config.get('ADD_SCSS_FILES', []))\n        xstatic_modules.extend(config.get('ADD_XSTATIC_MODULES', []))\n        update_horizon_config.update(\n            config.get('UPDATE_HORIZON_CONFIG', {}))\n        if config.get('DASHBOARD'):\n            dashboard = key\n            enabled_dashboards.append(dashboard)\n            if config.get('DEFAULT', False):\n                horizon_config['default_dashboard'] = dashboard\n        elif config.get('PANEL') or config.get('PANEL_GROUP'):\n            config.pop(\"__builtins__\", None)\n            panel_customization.append(config)\n        _extra_tabs = config.get('EXTRA_TABS', {})\n        for tab_key, tab_defs in _extra_tabs.items():\n            extra_tabs[tab_key] += tuple(tab_defs)\n        _extra_steps = config.get('EXTRA_STEPS', {})\n        for step_key, step_defs in _extra_steps.items():\n            extra_steps[step_key] += tuple(step_defs)\n    # Preserve the dashboard order specified in settings\n    dashboards = ([d for d in config_dashboards\n                   if d not in disabled_dashboards] +\n                  [d for d in enabled_dashboards\n                   if d not in config_dashboards])\n\n    horizon_config['panel_customization'] = panel_customization\n    horizon_config['header_sections'] = header_sections\n    horizon_config['dashboards'] = tuple(dashboards)\n    horizon_config.setdefault('exceptions', {}).update(exceptions)\n    horizon_config.update(update_horizon_config)\n    horizon_config.setdefault('angular_modules', []).extend(angular_modules)\n    horizon_config.setdefault('js_files', []).extend(js_files)\n    horizon_config.setdefault('js_spec_files', []).extend(js_spec_files)\n    horizon_config.setdefault('scss_files', []).extend(scss_files)\n    horizon_config.setdefault('xstatic_modules', []).extend(xstatic_modules)\n    horizon_config['extra_tabs'] = extra_tabs\n    horizon_config['extra_steps'] = extra_steps\n\n    # apps contains reference to applications declared in the enabled folder\n    # basically a list of applications that are internal and external plugins\n    # installed_apps contains reference to applications declared in settings\n    # such as django.contribe.*, django_pyscss, compressor, horizon, etc...\n    # for translation, we are only interested in the list of external plugins\n    # so we save the reference to it before we append to installed_apps\n    horizon_config.setdefault('plugins', []).extend(apps)\n    installed_apps[0:0] = apps", "response": "Imports dashboards and panel configurations from modules and applies them to the Horizon base."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of static files directories that are used by the xstatic modules.", "response": "def get_xstatic_dirs(XSTATIC_MODULES, HORIZON_CONFIG):\n    \"\"\"Discover static file configuration of the xstatic modules.\n\n    For each entry in the XSTATIC_MODULES list we determine the entry\n    point files (which may come from the xstatic MAIN var) and then\n    determine where in the Django static tree the xstatic package's contents\n    should be placed.\n\n    For jquery.bootstrap.wizard.js the module name is None the static file is\n    actually a 3rd-party file but resides in the Horizon source tree and not\n    an xstatic package.\n\n    The xstatic.pkg.jquery_ui package had its contents moved by packagers so\n    it must be handled as a special case.\n    \"\"\"\n    STATICFILES_DIRS = []\n    HORIZON_CONFIG.setdefault('xstatic_lib_files', [])\n    for module_name, files in XSTATIC_MODULES:\n        module = import_module(module_name)\n        if module_name == 'xstatic.pkg.jquery_ui':\n            # determine the correct path for jquery-ui which packagers moved\n            if module.VERSION.startswith('1.10.'):\n                # The 1.10.x versions already contain 'ui' directory.\n                files = ['ui/' + files[0]]\n\n        STATICFILES_DIRS.append(\n            ('horizon/lib/' + module.NAME, module.BASE_DIR)\n        )\n\n        # pull the file entry points from the xstatic package MAIN if possible\n        if hasattr(module, 'MAIN'):\n            files = module.MAIN\n            if not isinstance(files, list):\n                files = [files]\n\n            # just the Javascript files, please (don't <script> css, etc\n            # which is explicitly included in style/themes as appropriate)\n            files = [file for file in files if file.endswith('.js')]\n\n        # add to the list of files to link in the HTML\n        try:\n            for file in files:\n                file = 'horizon/lib/' + module.NAME + '/' + file\n                HORIZON_CONFIG['xstatic_lib_files'].append(file)\n        except TypeError:\n            raise Exception(\n                '%s: Nothing to include because files to include are not '\n                'defined (i.e., None) in BASE_XSTATIC_MODULES list and '\n                'a corresponding XStatic module does not define MAIN list.'\n                % module_name)\n\n    return STATICFILES_DIRS"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flavor_list(request, is_public=True, get_extras=False):\n    flavors = _nova.novaclient(request).flavors.list(is_public=is_public)\n    if get_extras:\n        for flavor in flavors:\n            flavor.extras = flavor_get_extras(request, flavor.id, True, flavor)\n    return flavors", "response": "Get the list of available instance sizes ( flavors )."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the list of available instance sizes.", "response": "def flavor_list_paged(request, is_public=True, get_extras=False, marker=None,\n                      paginate=False, sort_key=\"name\", sort_dir=\"desc\",\n                      reversed_order=False):\n    \"\"\"Get the list of available instance sizes (flavors).\"\"\"\n    has_more_data = False\n    has_prev_data = False\n\n    if paginate:\n        if reversed_order:\n            sort_dir = 'desc' if sort_dir == 'asc' else 'asc'\n        page_size = utils.get_page_size(request)\n        flavors = _nova.novaclient(request).flavors.list(is_public=is_public,\n                                                         marker=marker,\n                                                         limit=page_size + 1,\n                                                         sort_key=sort_key,\n                                                         sort_dir=sort_dir)\n        flavors, has_more_data, has_prev_data = update_pagination(\n            flavors, page_size, marker, reversed_order)\n    else:\n        flavors = _nova.novaclient(request).flavors.list(is_public=is_public)\n\n    if get_extras:\n        for flavor in flavors:\n            flavor.extras = flavor_get_extras(request, flavor.id, True, flavor)\n\n    return (flavors, has_more_data, has_prev_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flavor_access_list(request, flavor=None):\n    return _nova.novaclient(request).flavor_access.list(flavor=flavor)", "response": "Get the list of access instance sizes ( flavors."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a tenant to the given flavor access list.", "response": "def add_tenant_to_flavor(request, flavor, tenant):\n    \"\"\"Add a tenant to the given flavor access list.\"\"\"\n    return _nova.novaclient(request).flavor_access.add_tenant_access(\n        flavor=flavor, tenant=tenant)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a tenant from the given flavor access list.", "response": "def remove_tenant_from_flavor(request, flavor, tenant):\n    \"\"\"Remove a tenant from the given flavor access list.\"\"\"\n    return _nova.novaclient(request).flavor_access.remove_tenant_access(\n        flavor=flavor, tenant=tenant)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flavor_get_extras(request, flavor_id, raw=False, flavor=None):\n    if flavor is None:\n        flavor = _nova.novaclient(request).flavors.get(flavor_id)\n    extras = flavor.get_keys()\n    if raw:\n        return extras\n    return [FlavorExtraSpec(flavor_id, key, value) for\n            key, value in extras.items()]", "response": "Get flavor extra specs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flavor_extra_delete(request, flavor_id, keys):\n    flavor = _nova.novaclient(request).flavors.get(flavor_id)\n    return flavor.unset_keys(keys)", "response": "Unset the flavor extra spec keys."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the flavor extra spec keys.", "response": "def flavor_extra_set(request, flavor_id, metadata):\n    \"\"\"Set the flavor extra spec keys.\"\"\"\n    flavor = _nova.novaclient(request).flavors.get(flavor_id)\n    if (not metadata):  # not a way to delete keys\n        return None\n    return flavor.set_keys(metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the console output of an instance.", "response": "def server_console_output(request, instance_id, tail_length=None):\n    \"\"\"Gets console output of an instance.\"\"\"\n    nc = _nova.novaclient(request)\n    return nc.servers.get_console_output(instance_id, length=tail_length)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all nova extensions except the ones in the blacklist.", "response": "def list_extensions(request):\n    \"\"\"List all nova extensions, except the ones in the blacklist.\"\"\"\n    blacklist = set(getattr(settings,\n                            'OPENSTACK_NOVA_EXTENSIONS_BLACKLIST', []))\n    nova_api = _nova.novaclient(request)\n    return tuple(\n        extension for extension in\n        nova_list_extensions.ListExtManager(nova_api).show_all()\n        if extension.name not in blacklist\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extension_supported(extension_name, request):\n    for ext in list_extensions(request):\n        if ext.name == extension_name:\n            return True\n    return False", "response": "Determine if nova supports a given extension name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call_functions_parallel(*worker_defs):\n    # TODO(amotoki): Needs to figure out what max_workers can be specified.\n    # According to e0ne, the apache default configuration in devstack  allows\n    # only 10 threads. What happens if max_worker=11 is specified?\n    max_workers = len(worker_defs)\n    # Prepare a list with enough length.\n    futures = [None] * len(worker_defs)\n    with futurist.ThreadPoolExecutor(max_workers=max_workers) as e:\n        for index, func_def in enumerate(worker_defs):\n            if callable(func_def):\n                func_def = [func_def]\n            args = func_def[1] if len(func_def) > 1 else []\n            kwargs = func_def[2] if len(func_def) > 2 else {}\n            func = functools.partial(func_def[0], *args, **kwargs)\n            futures[index] = e.submit(fn=func)\n\n    return tuple(f.result() for f in futures)", "response": "Call functions in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_key(key_length=64):\n    if hasattr(random, 'SystemRandom'):\n        logging.info('Generating a secure random key using SystemRandom.')\n        choice = random.SystemRandom().choice\n    else:\n        msg = \"WARNING: SystemRandom not present. Generating a random \"\\\n              \"key using random.choice (NOT CRYPTOGRAPHICALLY SECURE).\"\n        logging.warning(msg)\n        choice = random.choice\n    return ''.join(map(lambda x: choice(string.digits + string.ascii_letters),\n                   range(key_length)))", "response": "Generate a secure random key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_or_read_from_file(key_file='.secret_key', key_length=64):\n    abspath = os.path.abspath(key_file)\n    # check, if key_file already exists\n    # if yes, then just read and return key\n    if os.path.exists(key_file):\n        key = read_from_file(key_file)\n        return key\n\n    # otherwise, first lock to make sure only one process\n    lock = lockutils.external_lock(key_file + \".lock\",\n                                   lock_path=os.path.dirname(abspath))\n    with lock:\n        if not os.path.exists(key_file):\n            key = generate_key(key_length)\n            old_umask = os.umask(0o177)  # Use '0600' file permissions\n            with open(key_file, 'w') as f:\n                f.write(key)\n            os.umask(old_umask)\n        else:\n            key = read_from_file(key_file)\n        return key", "response": "Generate a new key from a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a view to respond to a regex pattern.", "response": "def register(view):\n    \"\"\"Register API views to respond to a regex pattern.\n\n    ``url_regex`` on a wrapped view class is used as the regex pattern.\n    The view should be a standard Django class-based view implementing an\n    as_view() method. The url_regex attribute of the view should be a standard\n    Django URL regex pattern.\n    \"\"\"\n    p = urls.url(view.url_regex, view.as_view())\n    urlpatterns.append(p)\n    return view"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shellfilter(value):\n    replacements = {'\\\\': '\\\\\\\\',\n                    '`': '\\\\`',\n                    \"'\": \"\\\\'\",\n                    '\"': '\\\\\"'}\n    for search, repl in replacements.items():\n        value = value.replace(search, repl)\n    return safestring.mark_safe(value)", "response": "Replace HTML chars for shell usage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog a user in using the login form.", "response": "def login(request, template_name=None, extra_context=None, **kwargs):\n    \"\"\"Logs a user in using the :class:`~openstack_auth.forms.Login` form.\"\"\"\n\n    # If the user enabled websso and the default redirect\n    # redirect to the default websso url\n    if (request.method == 'GET' and utils.is_websso_enabled and\n            utils.is_websso_default_redirect()):\n        protocol = utils.get_websso_default_redirect_protocol()\n        region = utils.get_websso_default_redirect_region()\n        origin = utils.build_absolute_uri(request, '/auth/websso/')\n        url = ('%s/auth/OS-FEDERATION/websso/%s?origin=%s' %\n               (region, protocol, origin))\n        return shortcuts.redirect(url)\n\n    # If the user enabled websso and selects default protocol\n    # from the dropdown, We need to redirect user to the websso url\n    if request.method == 'POST':\n        auth_type = request.POST.get('auth_type', 'credentials')\n        if utils.is_websso_enabled() and auth_type != 'credentials':\n            region_id = request.POST.get('region')\n            auth_url = getattr(settings, 'WEBSSO_KEYSTONE_URL',\n                               forms.get_region_endpoint(region_id))\n            url = utils.get_websso_url(request, auth_url, auth_type)\n            return shortcuts.redirect(url)\n\n    if not request.is_ajax():\n        # If the user is already authenticated, redirect them to the\n        # dashboard straight away, unless the 'next' parameter is set as it\n        # usually indicates requesting access to a page that requires different\n        # permissions.\n        if (request.user.is_authenticated and\n                auth.REDIRECT_FIELD_NAME not in request.GET and\n                auth.REDIRECT_FIELD_NAME not in request.POST):\n            return shortcuts.redirect(settings.LOGIN_REDIRECT_URL)\n\n    # Get our initial region for the form.\n    initial = {}\n    current_region = request.session.get('region_endpoint', None)\n    requested_region = request.GET.get('region', None)\n    regions = dict(getattr(settings, \"AVAILABLE_REGIONS\", []))\n    if requested_region in regions and requested_region != current_region:\n        initial.update({'region': requested_region})\n\n    if request.method == \"POST\":\n        form = functional.curry(forms.Login)\n    else:\n        form = functional.curry(forms.Login, initial=initial)\n\n    if extra_context is None:\n        extra_context = {'redirect_field_name': auth.REDIRECT_FIELD_NAME}\n\n    extra_context['csrf_failure'] = request.GET.get('csrf_failure')\n\n    choices = getattr(settings, 'WEBSSO_CHOICES', ())\n    extra_context['show_sso_opts'] = (utils.is_websso_enabled() and\n                                      len(choices) > 1)\n\n    if not template_name:\n        if request.is_ajax():\n            template_name = 'auth/_login.html'\n            extra_context['hide'] = True\n        else:\n            template_name = 'auth/login.html'\n\n    res = django_auth_views.login(request,\n                                  template_name=template_name,\n                                  authentication_form=form,\n                                  extra_context=extra_context,\n                                  **kwargs)\n    # Save the region in the cookie, this is used as the default\n    # selected region next time the Login form loads.\n    if request.method == \"POST\":\n        utils.set_response_cookie(res, 'login_region',\n                                  request.POST.get('region', ''))\n        utils.set_response_cookie(res, 'login_domain',\n                                  request.POST.get('domain', ''))\n\n    # Set the session data here because django's session key rotation\n    # will erase it if we set it earlier.\n    if request.user.is_authenticated:\n        auth_user.set_session_from_user(request, request.user)\n        regions = dict(forms.Login.get_region_choices())\n        region = request.user.endpoint\n        login_region = request.POST.get('region')\n        region_name = regions.get(login_region)\n        request.session['region_endpoint'] = region\n        request.session['region_name'] = region_name\n        expiration_time = request.user.time_until_expiration()\n        threshold_days = getattr(\n            settings, 'PASSWORD_EXPIRES_WARNING_THRESHOLD_DAYS', -1)\n        if expiration_time is not None and \\\n                expiration_time.days <= threshold_days:\n            expiration_time = str(expiration_time).rsplit(':', 1)[0]\n            msg = (_('Please consider changing your password, it will expire'\n                     ' in %s minutes') %\n                   expiration_time).replace(':', ' Hours and ')\n            messages.warning(request, msg)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef websso(request):\n    referer = request.META.get('HTTP_REFERER', settings.OPENSTACK_KEYSTONE_URL)\n    auth_url = utils.clean_up_auth_url(referer)\n    token = request.POST.get('token')\n    try:\n        request.user = auth.authenticate(request=request, auth_url=auth_url,\n                                         token=token)\n    except exceptions.KeystoneAuthException as exc:\n        if utils.is_websso_default_redirect():\n            res = django_http.HttpResponseRedirect(settings.LOGIN_ERROR)\n        else:\n            msg = 'Login failed: %s' % six.text_type(exc)\n            res = django_http.HttpResponseRedirect(settings.LOGIN_URL)\n            res.set_cookie('logout_reason', msg, max_age=10)\n        return res\n\n    auth_user.set_session_from_user(request, request.user)\n    auth.login(request, request.user)\n    if request.session.test_cookie_worked():\n        request.session.delete_test_cookie()\n    return django_http.HttpResponseRedirect(settings.LOGIN_REDIRECT_URL)", "response": "Logs a user in using a token from Keystone s POST."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs out the user if he is logged in.", "response": "def logout(request, login_url=None, **kwargs):\n    \"\"\"Logs out the user if he is logged in. Then redirects to the log-in page.\n\n    :param login_url:\n        Once logged out, defines the URL where to redirect after login\n\n    :param kwargs:\n        see django.contrib.auth.views.logout_then_login extra parameters.\n\n    \"\"\"\n    msg = 'Logging out user \"%(username)s\".' % \\\n        {'username': request.user.username}\n    LOG.info(msg)\n\n    \"\"\" Securely logs a user out. \"\"\"\n    if (utils.is_websso_enabled and utils.is_websso_default_redirect() and\n            utils.get_websso_default_redirect_logout()):\n        auth_user.unset_session_user_variables(request)\n        return django_http.HttpResponseRedirect(\n            utils.get_websso_default_redirect_logout())\n    else:\n        return django_auth_views.logout_then_login(request,\n                                                   login_url=login_url,\n                                                   **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nswitches an authenticated user from one project to another.", "response": "def switch(request, tenant_id, redirect_field_name=auth.REDIRECT_FIELD_NAME):\n    \"\"\"Switches an authenticated user from one project to another.\"\"\"\n    LOG.debug('Switching to tenant %s for user \"%s\".',\n              tenant_id, request.user.username)\n\n    endpoint, __ = utils.fix_auth_url_version_prefix(request.user.endpoint)\n    session = utils.get_session()\n    # Keystone can be configured to prevent exchanging a scoped token for\n    # another token. Always use the unscoped token for requesting a\n    # scoped token.\n    unscoped_token = request.user.unscoped_token\n    auth = utils.get_token_auth_plugin(auth_url=endpoint,\n                                       token=unscoped_token,\n                                       project_id=tenant_id)\n\n    try:\n        auth_ref = auth.get_access(session)\n        msg = 'Project switch successful for user \"%(username)s\".' % \\\n            {'username': request.user.username}\n        LOG.info(msg)\n    except keystone_exceptions.ClientException:\n        msg = (\n            _('Project switch failed for user \"%(username)s\".') %\n            {'username': request.user.username})\n        messages.error(request, msg)\n        auth_ref = None\n        LOG.exception('An error occurred while switching sessions.')\n\n    # Ensure the user-originating redirection url is safe.\n    # Taken from django.contrib.auth.views.login()\n    redirect_to = request.GET.get(redirect_field_name, '')\n    if not is_safe_url(url=redirect_to, host=request.get_host()):\n        redirect_to = settings.LOGIN_REDIRECT_URL\n\n    if auth_ref:\n        user = auth_user.create_user_from_token(\n            request,\n            auth_user.Token(auth_ref, unscoped_token=unscoped_token),\n            endpoint)\n        auth_user.set_session_from_user(request, user)\n        message = (\n            _('Switch to project \"%(project_name)s\" successful.') %\n            {'project_name': request.user.project_name})\n        messages.success(request, message)\n    response = shortcuts.redirect(redirect_to)\n    utils.set_response_cookie(response, 'recent_project',\n                              request.user.project_id)\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswitching the user s region for all services except Identity service.", "response": "def switch_region(request, region_name,\n                  redirect_field_name=auth.REDIRECT_FIELD_NAME):\n    \"\"\"Switches the user's region for all services except Identity service.\n\n    The region will be switched if the given region is one of the regions\n    available for the scoped project. Otherwise the region is not switched.\n    \"\"\"\n    if region_name in request.user.available_services_regions:\n        request.session['services_region'] = region_name\n        LOG.debug('Switching services region to %s for user \"%s\".',\n                  region_name, request.user.username)\n\n    redirect_to = request.GET.get(redirect_field_name, '')\n    if not is_safe_url(url=redirect_to, host=request.get_host()):\n        redirect_to = settings.LOGIN_REDIRECT_URL\n\n    response = shortcuts.redirect(redirect_to)\n    utils.set_response_cookie(response, 'services_region',\n                              request.session['services_region'])\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nswitches the user s keystone provider using K2K federation.", "response": "def switch_keystone_provider(request, keystone_provider=None,\n                             redirect_field_name=auth.REDIRECT_FIELD_NAME):\n    \"\"\"Switches the user's keystone provider using K2K Federation\n\n    If keystone_provider is given then we switch the user to\n    the keystone provider using K2K federation. Otherwise if keystone_provider\n    is None then we switch the user back to the Identity Provider Keystone\n    which a non federated token auth will be used.\n    \"\"\"\n    base_token = request.session.get('k2k_base_unscoped_token', None)\n    k2k_auth_url = request.session.get('k2k_auth_url', None)\n    keystone_providers = request.session.get('keystone_providers', None)\n    recent_project = request.COOKIES.get('recent_project')\n\n    if not base_token or not k2k_auth_url:\n        msg = _('K2K Federation not setup for this session')\n        raise exceptions.KeystoneAuthException(msg)\n\n    redirect_to = request.GET.get(redirect_field_name, '')\n    if not is_safe_url(url=redirect_to, host=request.get_host()):\n        redirect_to = settings.LOGIN_REDIRECT_URL\n\n    unscoped_auth_ref = None\n    keystone_idp_id = getattr(\n        settings, 'KEYSTONE_PROVIDER_IDP_ID', 'localkeystone')\n\n    if keystone_provider == keystone_idp_id:\n        current_plugin = plugin.TokenPlugin()\n        unscoped_auth = current_plugin.get_plugin(auth_url=k2k_auth_url,\n                                                  token=base_token)\n    else:\n        # Switch to service provider using K2K federation\n        plugins = [plugin.TokenPlugin()]\n        current_plugin = plugin.K2KAuthPlugin()\n\n        unscoped_auth = current_plugin.get_plugin(\n            auth_url=k2k_auth_url, service_provider=keystone_provider,\n            plugins=plugins, token=base_token, recent_project=recent_project)\n\n    try:\n        # Switch to identity provider using token auth\n        unscoped_auth_ref = current_plugin.get_access_info(unscoped_auth)\n    except exceptions.KeystoneAuthException as exc:\n        msg = 'Switching to Keystone Provider %s has failed. %s' \\\n              % (keystone_provider, (six.text_type(exc)))\n        messages.error(request, msg)\n\n    if unscoped_auth_ref:\n        try:\n            request.user = auth.authenticate(\n                request=request, auth_url=unscoped_auth.auth_url,\n                token=unscoped_auth_ref.auth_token)\n        except exceptions.KeystoneAuthException as exc:\n            msg = 'Keystone provider switch failed: %s' % six.text_type(exc)\n            res = django_http.HttpResponseRedirect(settings.LOGIN_URL)\n            res.set_cookie('logout_reason', msg, max_age=10)\n            return res\n        auth.login(request, request.user)\n        auth_user.set_session_from_user(request, request.user)\n        request.session['keystone_provider_id'] = keystone_provider\n        request.session['keystone_providers'] = keystone_providers\n        request.session['k2k_base_unscoped_token'] = base_token\n        request.session['k2k_auth_url'] = k2k_auth_url\n        message = (\n            _('Switch to Keystone Provider \"%(keystone_provider)s\" '\n              'successful.') % {'keystone_provider': keystone_provider})\n        messages.success(request, message)\n\n    response = shortcuts.redirect(redirect_to)\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_link_url(self, datum=None):\n        if not self.url:\n            raise NotImplementedError('A LinkAction class must have a '\n                                      'url attribute or define its own '\n                                      'get_link_url method.')\n        if callable(self.url):\n            return self.url(datum, **self.kwargs)\n        try:\n            if datum:\n                obj_id = self.table.get_object_id(datum)\n                return urls.reverse(self.url, args=(obj_id,))\n            else:\n                return urls.reverse(self.url)\n        except urls.NoReverseMatch as ex:\n            LOG.info('No reverse found for \"%(url)s\": %(exception)s',\n                     {'url': self.url, 'exception': ex})\n            return self.url", "response": "Returns the final URL based on the value of url."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_param_name(self):\n        return \"__\".join([self.table.name, self.name, self.param_name])", "response": "Returns the full query parameter name for this action. Defaults to\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if agiven filter field should be used as an API filter.", "response": "def is_api_filter(self, filter_field):\n        \"\"\"Determine if agiven filter field should be used as an API filter.\"\"\"\n        if self.filter_type == 'server':\n            for choice in self.filter_choices:\n                if (choice[0] == filter_field and len(choice) > 2 and\n                        choice[2]):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_select_options(self):\n        if self.filter_choices:\n            return [choice[:4] for choice in self.filter_choices\n                    # Display it If the fifth element is True or does not exist\n                    if len(choice) < 5 or choice[4]]", "response": "Provide the value string and help_text for the template to render."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_action_name(self, items=None, past=False):\n        action_type = \"past\" if past else \"present\"\n        if items is None:\n            # Called without items parameter (by a single instance.)\n            count = 1\n        else:\n            count = len(items)\n\n        action_attr = getattr(self, \"action_%s\" % action_type)(count)\n        if isinstance(action_attr, (six.string_types, Promise)):\n            action = action_attr\n        else:\n            toggle_selection = getattr(self, \"current_%s_action\" % action_type)\n            action = action_attr[toggle_selection]\n\n        return action", "response": "Retreive action name based on the number of items and past flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, request, datum):\n        if getattr(self, 'action_present', False):\n            self.verbose_name = self._get_action_name()\n            self.verbose_name_plural = self._get_action_name('plural')", "response": "Switches the action verbose name if needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of the default HTML attributes for the action.", "response": "def get_default_attrs(self):\n        \"\"\"Returns a list of the default HTML attributes for the action.\"\"\"\n        attrs = super(BatchAction, self).get_default_attrs()\n        attrs.update({'data-batch-action': 'true'})\n        return attrs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tabs(self, request, **kwargs):\n        if self._tab_group is None:\n            self._tab_group = self.tab_group_class(request, **kwargs)\n        return self._tab_group", "response": "Returns the initialized tab group for this view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the tab_group variable to the context data.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"Adds the ``tab_group`` variable to the context data.\"\"\"\n        context = super(TabView, self).get_context_data(**kwargs)\n        try:\n            tab_group = self.get_tabs(self.request, **kwargs)\n            context[\"tab_group\"] = tab_group\n            # Make sure our data is pre-loaded to capture errors.\n            context[\"tab_group\"].load_tab_data()\n        except Exception:\n            exceptions.handle(self.request)\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_tabbed_response(self, tab_group, context):\n        if self.request.is_ajax():\n            if tab_group.selected:\n                return http.HttpResponse(tab_group.selected.render())\n            else:\n                return http.HttpResponse(tab_group.render())\n        return self.render_to_response(context)", "response": "Sends back an AJAX - appropriate response for the tab group."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the tab group.", "response": "def load_tabs(self):\n        \"\"\"Loads the tab group.\n\n        It compiles the table instances for each table attached to\n        any :class:`horizon.tabs.TableTab` instances on the tab group.\n        This step is necessary before processing any tab or table actions.\n        \"\"\"\n        tab_group = self.get_tabs(self.request, **self.kwargs)\n        tabs = tab_group.get_tabs()\n        for tab in [t for t in tabs if issubclass(t.__class__, TableTab)]:\n            self.table_classes.extend(tab.table_classes)\n            for table in tab._tables.values():\n                self._table_dict[table._meta.name] = {'table': table,\n                                                      'tab': tab}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads the table data based on a given table_dict and handles them.", "response": "def handle_table(self, table_dict):\n        \"\"\"Loads the table data based on a given table_dict and handles them.\n\n        For the given dict containing a ``DataTable`` and a ``TableTab``\n        instance, it loads the table data for that tab and calls the\n        table's :meth:`~horizon.tables.DataTable.maybe_handle` method.\n        The return value will be the result of ``maybe_handle``.\n        \"\"\"\n        table = table_dict['table']\n        tab = table_dict['tab']\n        tab.load_table_data()\n        table_name = table._meta.name\n        tab._tables[table_name]._meta.has_prev_data = self.has_prev_data(table)\n        tab._tables[table_name]._meta.has_more_data = self.has_more_data(table)\n        handled = tab._tables[table_name].maybe_handle()\n        return handled"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef openstack(request):\n    context = {}\n\n    # Auth/Keystone context\n    context.setdefault('authorized_tenants', [])\n    if request.user.is_authenticated:\n        context['authorized_tenants'] = [\n            tenant for tenant in\n            request.user.authorized_tenants if tenant.enabled]\n\n    # Region context/support\n    available_regions = getattr(settings, 'AVAILABLE_REGIONS', [])\n    regions = {'support': len(available_regions) > 1,\n               'current': {'endpoint': request.session.get('region_endpoint'),\n                           'name': request.session.get('region_name')},\n               'available': [{'endpoint': region[0], 'name':region[1]} for\n                             region in available_regions]}\n\n    # K2K Federation Service Providers context/support\n    available_providers = request.session.get('keystone_providers', [])\n    if available_providers:\n        provider_id = request.session.get('keystone_provider_id', None)\n        provider_name = None\n        for provider in available_providers:\n            if provider['id'] == provider_id:\n                provider_name = provider.get('name')\n\n        keystone_providers = {\n            'support': len(available_providers) > 1,\n            'current': {\n                'name': provider_name,\n                'id': provider_id\n            },\n            'available': [\n                {'name': keystone_provider['name'],\n                 'id': keystone_provider['id']}\n                for keystone_provider in available_providers]\n        }\n    else:\n        keystone_providers = {'support': False}\n\n    context['keystone_providers'] = keystone_providers\n    context['regions'] = regions\n\n    # Adding webroot access\n    context['WEBROOT'] = getattr(settings, \"WEBROOT\", \"/\")\n\n    user_menu_links = getattr(settings, \"USER_MENU_LINKS\", [])\n\n    if not getattr(settings, \"SHOW_KEYSTONE_V2_RC\", False):\n        user_menu_links = [\n            link for link in user_menu_links\n            if link['url'] != 'horizon:project:api_access:openrcv2']\n\n    context['USER_MENU_LINKS'] = user_menu_links\n\n    # Adding profiler support flag\n    profiler_settings = getattr(settings, 'OPENSTACK_PROFILER', {})\n    profiler_enabled = profiler_settings.get('enabled', False)\n    context['profiler_enabled'] = profiler_enabled\n    if profiler_enabled and 'profile_page' in request.COOKIES:\n        index_view_id = request.META.get(profiler.ROOT_HEADER, '')\n        hmac_keys = profiler_settings.get('keys', [])\n        context['x_trace_info'] = profiler.update_trace_headers(\n            hmac_keys, parent_id=index_view_id)\n\n    context['JS_CATALOG'] = get_js_catalog(conf)\n\n    return context", "response": "Context processor necessary for OpenStack Dashboard functionality."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_token_valid(token, margin=None):\n    expiration = token.expires\n    # In case we get an unparseable expiration timestamp, return False\n    # so you can't have a \"forever\" token just by breaking the expires param.\n    if expiration is None:\n        return False\n    if margin is None:\n        margin = getattr(settings, 'TOKEN_TIMEOUT_MARGIN', 0)\n    expiration = expiration - datetime.timedelta(seconds=margin)\n    if settings.USE_TZ and timezone.is_naive(expiration):\n        # Presumes that the Keystone is using UTC.\n        expiration = timezone.make_aware(expiration, timezone.utc)\n    return expiration > timezone.now()", "response": "Returns True if the token is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the url is a safe redirection.", "response": "def is_safe_url(url, host=None):\n    \"\"\"Return ``True`` if the url is a safe redirection.\n\n    The safe redirection means that it doesn't point to a different host.\n    Always returns ``False`` on an empty url.\n    \"\"\"\n    if not url:\n        return False\n    netloc = urlparse.urlparse(url)[1]\n    return not netloc or netloc == host"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_absolute_uri(request, relative_url):\n    webroot = getattr(settings, 'WEBROOT', '')\n    if webroot.endswith(\"/\") and relative_url.startswith(\"/\"):\n        webroot = webroot[:-1]\n\n    return request.build_absolute_uri(webroot + relative_url)", "response": "Ensure absolute_uri are relative to WEBROOT."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the keystone endpoint for initiating WebSSO.", "response": "def get_websso_url(request, auth_url, websso_auth):\n    \"\"\"Return the keystone endpoint for initiating WebSSO.\n\n    Generate the keystone WebSSO endpoint that will redirect the user\n    to the login page of the federated identity provider.\n\n    Based on the authentication type selected by the user in the login\n    form, it will construct the keystone WebSSO endpoint.\n\n    :param request: Django http request object.\n    :type request: django.http.HttpRequest\n    :param auth_url: Keystone endpoint configured in the horizon setting.\n                     If WEBSSO_KEYSTONE_URL is defined, its value will be\n                     used. Otherwise, the value is derived from:\n                     - OPENSTACK_KEYSTONE_URL\n                     - AVAILABLE_REGIONS\n    :type auth_url: string\n    :param websso_auth: Authentication type selected by the user from the\n                        login form. The value is derived from the horizon\n                        setting WEBSSO_CHOICES.\n    :type websso_auth: string\n\n    Example of horizon WebSSO setting::\n\n        WEBSSO_CHOICES = (\n            (\"credentials\", \"Keystone Credentials\"),\n            (\"oidc\", \"OpenID Connect\"),\n            (\"saml2\", \"Security Assertion Markup Language\"),\n            (\"acme_oidc\", \"ACME - OpenID Connect\"),\n            (\"acme_saml2\", \"ACME - SAML2\")\n        )\n\n        WEBSSO_IDP_MAPPING = {\n            \"acme_oidc\": (\"acme\", \"oidc\"),\n            \"acme_saml2\": (\"acme\", \"saml2\")\n            }\n        }\n\n    The value of websso_auth will be looked up in the WEBSSO_IDP_MAPPING\n    dictionary, if a match is found it will return a IdP specific WebSSO\n    endpoint using the values found in the mapping.\n\n    The value in WEBSSO_IDP_MAPPING is expected to be a tuple formatted as\n    (<idp_id>, <protocol_id>). Using the values found, a IdP/protocol\n    specific URL will be constructed::\n\n        /auth/OS-FEDERATION/identity_providers/<idp_id>\n        /protocols/<protocol_id>/websso\n\n    If no value is found from the WEBSSO_IDP_MAPPING dictionary, it will\n    treat the value as the global WebSSO protocol <protocol_id> and\n    construct the WebSSO URL by::\n\n        /auth/OS-FEDERATION/websso/<protocol_id>\n\n    :returns: Keystone WebSSO endpoint.\n    :rtype: string\n\n    \"\"\"\n    origin = build_absolute_uri(request, '/auth/websso/')\n    idp_mapping = getattr(settings, 'WEBSSO_IDP_MAPPING', {})\n    idp_id, protocol_id = idp_mapping.get(websso_auth,\n                                          (None, websso_auth))\n\n    if idp_id:\n        # Use the IDP specific WebSSO endpoint\n        url = ('%s/auth/OS-FEDERATION/identity_providers/%s'\n               '/protocols/%s/websso?origin=%s' %\n               (auth_url, idp_id, protocol_id, origin))\n    else:\n        # If no IDP mapping found for the identifier,\n        # perform WebSSO by protocol.\n        url = ('%s/auth/OS-FEDERATION/websso/%s?origin=%s' %\n               (auth_url, protocol_id, origin))\n\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if any of subs strings is present in the url path.", "response": "def has_in_url_path(url, subs):\n    \"\"\"Test if any of `subs` strings is present in the `url` path.\"\"\"\n    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)\n    return any([sub in path for sub in subs])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a copy of url with replaced path.", "response": "def url_path_replace(url, old, new, count=None):\n    \"\"\"Return a copy of url with replaced path.\n\n    Return a copy of url with all occurrences of old replaced by new in the url\n    path.  If the optional argument count is given, only the first count\n    occurrences are replaced.\n    \"\"\"\n    args = []\n    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)\n    if count is not None:\n        args.append(count)\n    return urlparse.urlunsplit((\n        scheme, netloc, path.replace(old, new, *args), query, fragment))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfix up the auth url if an invalid or no version prefix was given.", "response": "def fix_auth_url_version_prefix(auth_url):\n    \"\"\"Fix up the auth url if an invalid or no version prefix was given.\n\n    People still give a v2 auth_url even when they specify that they want v3\n    authentication. Fix the URL to say v3 in this case and add version if it is\n    missing entirely. This should be smarter and use discovery.\n    \"\"\"\n    auth_url = _augment_url_with_version(auth_url)\n\n    url_fixed = False\n    if get_keystone_version() >= 3 and has_in_url_path(auth_url, [\"/v2.0\"]):\n        url_fixed = True\n        auth_url = url_path_replace(auth_url, \"/v2.0\", \"/v3\", 1)\n\n    return auth_url, url_fixed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_up_auth_url(auth_url):\n\n    # NOTE(mnaser): This drops the query and fragment because we're only\n    #               trying to extract the Keystone URL.\n    scheme, netloc, path, query, fragment = urlparse.urlsplit(auth_url)\n    return urlparse.urlunsplit((\n        scheme, netloc, re.sub(r'/auth.*', '', path), '', ''))", "response": "Clean up the auth url to extract the exact Keystone URL"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the default service region from the service catalog.", "response": "def default_services_region(service_catalog, request=None,\n                            ks_endpoint=None):\n    \"\"\"Return the default service region.\n\n    Order of precedence:\n    1. 'services_region' cookie value\n    2. Matching endpoint in DEFAULT_SERVICE_REGIONS\n    3. '*' key in DEFAULT_SERVICE_REGIONS\n    4. First valid region from catalog\n\n    In each case the value must also be present in available_regions or\n    we move to the next level of precedence.\n    \"\"\"\n    if service_catalog:\n        available_regions = [get_endpoint_region(endpoint) for service\n                             in service_catalog for endpoint\n                             in service.get('endpoints', [])\n                             if (service.get('type') is not None and\n                                 service.get('type') != 'identity')]\n        if not available_regions:\n            # this is very likely an incomplete keystone setup\n            LOG.warning('No regions could be found excluding identity.')\n            available_regions = [get_endpoint_region(endpoint) for service\n                                 in service_catalog for endpoint\n                                 in service.get('endpoints', [])]\n\n            if not available_regions:\n                # if there are no region setup for any service endpoint,\n                # this is a critical problem and it's not clear how this occurs\n                LOG.error('No regions can be found in the service catalog.')\n                return None\n\n        region_options = []\n        if request:\n            region_options.append(request.COOKIES.get('services_region'))\n        if ks_endpoint:\n            default_service_regions = getattr(\n                settings, 'DEFAULT_SERVICE_REGIONS', {})\n            region_options.append(default_service_regions.get(ks_endpoint))\n        region_options.append(\n            getattr(settings, 'DEFAULT_SERVICE_REGIONS', {}).get('*'))\n\n        for region in region_options:\n            if region in available_regions:\n                return region\n        return available_regions[0]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_client_ip(request):\n    _SECURE_PROXY_ADDR_HEADER = getattr(\n        settings, 'SECURE_PROXY_ADDR_HEADER', False\n    )\n    if _SECURE_PROXY_ADDR_HEADER:\n        return request.META.get(\n            _SECURE_PROXY_ADDR_HEADER,\n            request.META.get('REMOTE_ADDR')\n        )\n    return request.META.get('REMOTE_ADDR')", "response": "Returns the client ip address using SECURE_PROXY_ADDR_HEADER variable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef store_initial_k2k_session(auth_url, request, scoped_auth_ref,\n                              unscoped_auth_ref):\n    \"\"\"Stores session variables if there are k2k service providers\n\n    This stores variables related to Keystone2Keystone federation. This\n    function gets skipped if there are no Keystone service providers.\n    An unscoped token to the identity provider keystone gets stored\n    so that it can be used to do federated login into the service\n    providers when switching keystone providers.\n    The settings file can be configured to set the display name\n    of the local (identity provider) keystone by setting\n    KEYSTONE_PROVIDER_IDP_NAME. The KEYSTONE_PROVIDER_IDP_ID settings\n    variable is used for comparison against the service providers.\n    It should not conflict with any of the service provider ids.\n\n    :param auth_url: base token auth url\n    :param request: Django http request object\n    :param scoped_auth_ref: Scoped Keystone access info object\n    :param unscoped_auth_ref: Unscoped Keystone access info object\n    \"\"\"\n    keystone_provider_id = request.session.get('keystone_provider_id', None)\n    if keystone_provider_id:\n        return None\n\n    providers = getattr(scoped_auth_ref, 'service_providers', None)\n    if providers:\n        providers = getattr(providers, '_service_providers', None)\n\n    if providers:\n        keystone_idp_name = getattr(settings, 'KEYSTONE_PROVIDER_IDP_NAME',\n                                    'Local Keystone')\n        keystone_idp_id = getattr(\n            settings, 'KEYSTONE_PROVIDER_IDP_ID', 'localkeystone')\n        keystone_identity_provider = {'name': keystone_idp_name,\n                                      'id': keystone_idp_id}\n        # (edtubill) We will use the IDs as the display names\n        # We may want to be able to set display names in the future.\n        keystone_providers = [\n            {'name': provider_id, 'id': provider_id}\n            for provider_id in providers]\n\n        keystone_providers.append(keystone_identity_provider)\n\n        # We treat the Keystone idp ID as None\n        request.session['keystone_provider_id'] = keystone_idp_id\n        request.session['keystone_providers'] = keystone_providers\n        request.session['k2k_base_unscoped_token'] =\\\n            unscoped_auth_ref.auth_token\n        request.session['k2k_auth_url'] = auth_url", "response": "Stores the initial k2k session variables related to Keystone2Keystone federation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_int_or_uuid(value):\n    try:\n        uuid.UUID(value)\n        return value\n    except (ValueError, AttributeError):\n        return int(value)", "response": "Check if a value is valid as UUID or an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_display_label(choices, status):\n\n    for (value, label) in choices:\n        if value == (status or '').lower():\n            display_label = label\n            break\n    else:\n        display_label = status\n\n    return display_label", "response": "Get a display label for a resource status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the user object based on the user ID and the session data.", "response": "def get_user(self, user_id):\n        \"\"\"Returns the current user from the session data.\n\n        If authenticated, this return the user object based on the user ID\n        and session data.\n\n        .. note::\n\n          This required monkey-patching the ``contrib.auth`` middleware\n          to make the ``request`` object available to the auth backend class.\n\n        \"\"\"\n        if (hasattr(self, 'request') and\n                user_id == self.request.session[\"user_id\"]):\n            token = self.request.session['token']\n            endpoint = self.request.session['region_endpoint']\n            services_region = self.request.session['services_region']\n            user = auth_user.create_user_from_token(self.request, token,\n                                                    endpoint, services_region)\n            return user\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate(self, auth_url=None, **kwargs):\n        LOG.debug('Beginning user authentication')\n\n        if not auth_url:\n            auth_url = settings.OPENSTACK_KEYSTONE_URL\n\n        auth_url, url_fixed = utils.fix_auth_url_version_prefix(auth_url)\n        if url_fixed:\n            LOG.warning(\"The OPENSTACK_KEYSTONE_URL setting points to a v2.0 \"\n                        \"Keystone endpoint, but v3 is specified as the API \"\n                        \"version to use by Horizon. Using v3 endpoint for \"\n                        \"authentication.\")\n\n        plugin, unscoped_auth = self._get_auth_backend(auth_url, **kwargs)\n\n        # the recent project id a user might have set in a cookie\n        recent_project = None\n        request = kwargs.get('request')\n        if request:\n            # Grab recent_project found in the cookie, try to scope\n            # to the last project used.\n            recent_project = request.COOKIES.get('recent_project')\n        unscoped_auth_ref = plugin.get_access_info(unscoped_auth)\n\n        # Check expiry for our unscoped auth ref.\n        self.check_auth_expiry(unscoped_auth_ref)\n\n        domain_name = kwargs.get('user_domain_name', None)\n        domain_auth, domain_auth_ref = plugin.get_domain_scoped_auth(\n            unscoped_auth, unscoped_auth_ref, domain_name)\n        scoped_auth, scoped_auth_ref = plugin.get_project_scoped_auth(\n            unscoped_auth, unscoped_auth_ref, recent_project=recent_project)\n\n        # Abort if there are no projects for this user and a valid domain\n        # token has not been obtained\n        #\n        # The valid use cases for a user login are:\n        #    Keystone v2: user must have a role on a project and be able\n        #                 to obtain a project scoped token\n        #    Keystone v3: 1) user can obtain a domain scoped token (user\n        #                    has a role on the domain they authenticated to),\n        #                    only, no roles on a project\n        #                 2) user can obtain a domain scoped token and has\n        #                    a role on a project in the domain they\n        #                    authenticated to (and can obtain a project scoped\n        #                    token)\n        #                 3) user cannot obtain a domain scoped token, but can\n        #                    obtain a project scoped token\n        if not scoped_auth_ref and domain_auth_ref:\n            # if the user can't obtain a project scoped token, set the scoped\n            # token to be the domain token, if valid\n            scoped_auth = domain_auth\n            scoped_auth_ref = domain_auth_ref\n        elif not scoped_auth_ref and not domain_auth_ref:\n            msg = _('You are not authorized for any projects.')\n            if utils.get_keystone_version() >= 3:\n                msg = _('You are not authorized for any projects or domains.')\n            raise exceptions.KeystoneAuthException(msg)\n\n        # Check expiry for our new scoped token.\n        self.check_auth_expiry(scoped_auth_ref)\n\n        # We want to try to use the same region we just logged into\n        # which may or may not be the default depending upon the order\n        # keystone uses\n        region_name = None\n        id_endpoints = scoped_auth_ref.service_catalog.\\\n            get_endpoints(service_type='identity')\n        for id_endpoint in [cat for cat in id_endpoints['identity']]:\n            if auth_url in id_endpoint.values():\n                region_name = id_endpoint['region']\n                break\n\n        interface = getattr(settings, 'OPENSTACK_ENDPOINT_TYPE', 'public')\n\n        endpoint, url_fixed = utils.fix_auth_url_version_prefix(\n            scoped_auth_ref.service_catalog.url_for(\n                service_type='identity',\n                interface=interface,\n                region_name=region_name))\n        if url_fixed:\n            LOG.warning(\"The Keystone URL in service catalog points to a v2.0 \"\n                        \"Keystone endpoint, but v3 is specified as the API \"\n                        \"version to use by Horizon. Using v3 endpoint for \"\n                        \"authentication.\")\n\n        # If we made it here we succeeded. Create our User!\n        unscoped_token = unscoped_auth_ref.auth_token\n\n        user = auth_user.create_user_from_token(\n            request,\n            auth_user.Token(scoped_auth_ref, unscoped_token=unscoped_token),\n            endpoint,\n            services_region=region_name)\n\n        if request is not None:\n            # if no k2k providers exist then the function returns quickly\n            utils.store_initial_k2k_session(auth_url, request, scoped_auth_ref,\n                                            unscoped_auth_ref)\n            request.session['unscoped_token'] = unscoped_token\n            if domain_auth_ref:\n                # check django session engine, if using cookies, this will not\n                # work, as it will overflow the cookie so don't add domain\n                # scoped token to the session and put error in the log\n                if utils.using_cookie_backed_sessions():\n                    LOG.error('Using signed cookies as SESSION_ENGINE with '\n                              'OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT is '\n                              'enabled. This disables the ability to '\n                              'perform identity operations due to cookie size '\n                              'constraints.')\n                else:\n                    request.session['domain_token'] = domain_auth_ref\n\n            request.user = user\n            timeout = getattr(settings, \"SESSION_TIMEOUT\", 3600)\n            token_life = user.token.expires - datetime.datetime.now(pytz.utc)\n            session_time = min(timeout, int(token_life.total_seconds()))\n            request.session.set_expiry(session_time)\n\n            keystone_client_class = utils.get_keystone_client().Client\n            session = utils.get_session()\n            scoped_client = keystone_client_class(session=session,\n                                                  auth=scoped_auth)\n\n            # Support client caching to save on auth calls.\n            setattr(request, KEYSTONE_CLIENT_ATTR, scoped_client)\n\n        LOG.debug('Authentication completed.')\n        return user", "response": "Authenticates a user via Keystone Identity API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_permissions(self, user, obj=None):\n        if user.is_anonymous or obj is not None:\n            return set()\n        # TODO(gabrielhurley): Integrate policy-driven RBAC\n        #                      when supported by Keystone.\n        role_perms = {utils.get_role_permission(role['name'])\n                      for role in user.roles}\n\n        services = []\n        for service in user.service_catalog:\n            try:\n                service_type = service['type']\n            except KeyError:\n                continue\n            service_regions = [utils.get_endpoint_region(endpoint) for endpoint\n                               in service.get('endpoints', [])]\n            if user.services_region in service_regions:\n                services.append(service_type.lower())\n        service_perms = {\"openstack.services.%s\" % service\n                         for service in services}\n        return role_perms | service_perms", "response": "Returns a set of permission strings that the user has permission for all services and roles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the given user has the specified permission.", "response": "def has_perm(self, user, perm, obj=None):\n        \"\"\"Returns True if the given user has the specified permission.\"\"\"\n        if not user.is_active:\n            return False\n        return perm in self.get_all_permissions(user, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle(request, message=None, redirect=None, ignore=False,\n           escalate=False, log_level=None, force_log=None):\n    \"\"\"Centralized error handling for Horizon.\n\n    Because Horizon consumes so many different APIs with completely\n    different ``Exception`` types, it's necessary to have a centralized\n    place for handling exceptions which may be raised.\n\n    Exceptions are roughly divided into 3 types:\n\n    #. ``UNAUTHORIZED``: Errors resulting from authentication or authorization\n       problems. These result in being logged out and sent to the login screen.\n    #. ``NOT_FOUND``: Errors resulting from objects which could not be\n       located via the API. These generally result in a user-facing error\n       message, but are otherwise returned to the normal code flow. Optionally\n       a redirect value may be passed to the error handler so users are\n       returned to a different view than the one requested in addition to the\n       error message.\n    #. ``RECOVERABLE``: Generic API errors which generate a user-facing message\n       but drop directly back to the regular code flow.\n\n    All other exceptions bubble the stack as normal unless the ``ignore``\n    argument is passed in as ``True``, in which case only unrecognized\n    errors are bubbled.\n\n    If the exception is not re-raised, an appropriate wrapper exception\n    class indicating the type of exception that was encountered will be\n    returned.\n    \"\"\"\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    log_method = getattr(LOG, log_level or \"exception\")\n    force_log = force_log or os.environ.get(\"HORIZON_TEST_RUN\", False)\n    force_silence = getattr(exc_value, \"silence_logging\", False)\n\n    # Because the same exception may travel through this method more than\n    # once (if it's re-raised) we may want to treat it differently\n    # the second time (e.g. no user messages/logging).\n    handled = issubclass(exc_type, HandledException)\n    wrap = False\n\n    # Restore our original exception information, but re-wrap it at the end\n    if handled:\n        exc_type, exc_value, exc_traceback = exc_value.wrapped\n        wrap = True\n\n    log_entry = encoding.force_text(exc_value)\n\n    user_message = \"\"\n    # We trust messages from our own exceptions\n    if issubclass(exc_type, HorizonException):\n        user_message = log_entry\n    # If the message has a placeholder for the exception, fill it in\n    elif message and \"%(exc)s\" in message:\n        user_message = encoding.force_text(message) % {\"exc\": log_entry}\n    elif message:\n        user_message = encoding.force_text(message)\n\n    for exc_handler in HANDLE_EXC_METHODS:\n        if issubclass(exc_type, exc_handler['exc']):\n            if exc_handler['set_wrap']:\n                wrap = True\n            handler = exc_handler['handler']\n            ret = handler(request, user_message, redirect, ignore,\n                          exc_handler.get('escalate', escalate),\n                          handled, force_silence, force_log,\n                          log_method, log_entry, log_level)\n            if ret:\n                return ret  # return to normal code flow\n\n    # If we've gotten here, time to wrap and/or raise our exception.\n    if wrap:\n        raise HandledException([exc_type, exc_value, exc_traceback])\n\n    # assume exceptions handled in the code that pass in a message are already\n    # handled appropriately and treat as recoverable\n    if message:\n        ret = handle_recoverable(request, user_message, redirect, ignore,\n                                 escalate, handled, force_silence, force_log,\n                                 log_method, log_entry, log_level)\n        # pylint: disable=using-constant-test\n        if ret:\n            return ret\n\n    six.reraise(exc_type, exc_value, exc_traceback)", "response": "This function handles the error handling for Horizon."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_config(files=None, root_path=None, local_path=None):\n\n    config = cfg.ConfigOpts()\n    config.register_opts([\n        cfg.Opt('root_path', default=root_path),\n        cfg.Opt('local_path', default=local_path),\n    ])\n    # XXX register actual config groups here\n    # theme_group = config_theme.register_config(config)\n    if files is not None:\n        config(args=[], default_config_files=files)\n    return config", "response": "Load the configuration from specified files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_user_roles_names_from_roles_id(self, user, users_roles,\n                                               roles_list):\n        \"\"\"Add roles names to user.roles, based on users_roles.\n\n        :param user: user to update\n        :param users_roles: list of roles ID\n        :param roles_list: list of roles obtained with keystone\n        \"\"\"\n        user_roles_names = [role.name for role in roles_list\n                            if role.id in users_roles]\n        current_user_roles_names = set(getattr(user, \"roles\", []))\n        user.roles = list(current_user_roles_names.union(user_roles_names))", "response": "Add roles names to user. roles based on users_roles."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget users from project and update project_users with the list of users which have role on project through a group.", "response": "def _get_users_from_project(self, project_id, roles, project_users):\n        \"\"\"Update with users which have role on project NOT through a group.\n\n        :param project_id: ID of the project\n        :param roles: list of roles from keystone\n        :param project_users: list to be updated with the users found\n        \"\"\"\n\n        # For keystone.user_list project_id is not passed as argument because\n        # it is ignored when using admin credentials\n        # Get all users (to be able to find user name)\n        users = api.keystone.user_list(self.request)\n        users = {user.id: user for user in users}\n\n        # Get project_users_roles ({user_id: [role_id_1, role_id_2]})\n        project_users_roles = api.keystone.get_project_users_roles(\n            self.request,\n            project=project_id)\n\n        for user_id in project_users_roles:\n\n            if user_id not in project_users:\n                # Add user to the project_users\n                project_users[user_id] = users[user_id]\n                project_users[user_id].roles = []\n                project_users[user_id].roles_from_groups = []\n\n            # Update the project_user role in order to get:\n            # project_users[user_id].roles = [role_name1, role_name2]\n            self._update_user_roles_names_from_roles_id(\n                user=project_users[user_id],\n                users_roles=project_users_roles[user_id],\n                roles_list=roles\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_users_from_groups(self, project_id, roles, project_users):\n\n        # For keystone.group_list project_id is not passed as argument because\n        # it is ignored when using admin credentials\n        # Get all groups (to be able to find group name)\n        groups = api.keystone.group_list(self.request)\n        group_names = {group.id: group.name for group in groups}\n\n        # Get a dictionary {group_id: [role_id_1, role_id_2]}\n        project_groups_roles = api.keystone.get_project_groups_roles(\n            self.request,\n            project=project_id)\n\n        for group_id in project_groups_roles:\n            group_users = api.keystone.user_list(self.request,\n                                                 group=group_id)\n            group_roles_names = [\n                role.name for role in roles\n                if role.id in project_groups_roles[group_id]]\n\n            roles_from_group = [(role_name, group_names[group_id])\n                                for role_name in group_roles_names]\n\n            for user in group_users:\n                if user.id not in project_users:\n                    # New user: Add the user to the list\n                    project_users[user.id] = user\n                    project_users[user.id].roles = []\n                    project_users[user.id].roles_from_groups = []\n\n                # Add roles from group\n                project_users[user.id].roles_from_groups.extend(\n                    roles_from_group)", "response": "Get users which have role on project through a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets users with roles on the project or through a group.", "response": "def get_userstable_data(self):\n        \"\"\"Get users with roles on the project.\n\n        Roles can be applied directly on the project or through a group.\n        \"\"\"\n        project_users = {}\n        project = self.tab_group.kwargs['project']\n\n        try:\n            # Get all global roles once to avoid multiple requests.\n            roles = api.keystone.role_list(self.request)\n\n            # Update project_users with users which have role directly on\n            # the project, (NOT through a group)\n            self._get_users_from_project(project_id=project.id,\n                                         roles=roles,\n                                         project_users=project_users)\n\n            # Update project_users with users which have role indirectly on\n            # the project, (through a group)\n            self._get_users_from_groups(project_id=project.id,\n                                        roles=roles,\n                                        project_users=project_users)\n\n        except Exception:\n            exceptions.handle(self.request,\n                              _(\"Unable to display the users of this project.\")\n                              )\n\n        return project_users.values()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a message to the Horizon app.", "response": "def add_message(request, level, message, extra_tags='', fail_silently=False):\n    \"\"\"Attempts to add a message to the request using the 'messages' app.\"\"\"\n    if not horizon_message_already_queued(request, message):\n        if request.is_ajax():\n            tag = constants.DEFAULT_TAGS[level]\n            # if message is marked as safe, pass \"safe\" tag as extra_tags so\n            # that client can skip HTML escape for the message when rendering\n            if isinstance(message, SafeData):\n                extra_tags = extra_tags + ' safe'\n            request.horizon['async_messages'].append([tag,\n                                                      force_text(message),\n                                                      extra_tags])\n        else:\n            return _messages.add_message(request, level, message,\n                                         extra_tags, fail_silently)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef debug(request, message, extra_tags='', fail_silently=False):\n    add_message(request, constants.DEBUG, message, extra_tags=extra_tags,\n                fail_silently=fail_silently)", "response": "Adds a message with the DEBUG level."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef info(request, message, extra_tags='', fail_silently=False):\n    add_message(request, constants.INFO, message, extra_tags=extra_tags,\n                fail_silently=fail_silently)", "response": "Adds a message with the INFO level."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef success(request, message, extra_tags='', fail_silently=False):\n    add_message(request, constants.SUCCESS, message, extra_tags=extra_tags,\n                fail_silently=fail_silently)", "response": "Adds a message with the SUCCESS level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef warning(request, message, extra_tags='', fail_silently=False):\n    add_message(request, constants.WARNING, message, extra_tags=extra_tags,\n                fail_silently=fail_silently)", "response": "Adds a message with the WARNING level."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef error(request, message, extra_tags='', fail_silently=False):\n    add_message(request, constants.ERROR, message, extra_tags=extra_tags,\n                fail_silently=fail_silently)", "response": "Adds a message with the ERROR level."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting a listing of objects into their appropriate wrapper classes.", "response": "def _objectify(items, container_name):\n    \"\"\"Splits a listing of objects into their appropriate wrapper classes.\"\"\"\n    objects = []\n\n    # Deal with objects and object pseudo-folders first, save subdirs for later\n    for item in items:\n        if item.get(\"subdir\", None) is not None:\n            object_cls = PseudoFolder\n        else:\n            object_cls = StorageObject\n\n        objects.append(object_cls(item, container_name))\n\n    return objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, result, spec):  # noqa Yes, it's too complex.\n        if spec is None:\n            # None matches anything.\n            return\n        if isinstance(spec, dict):\n            if not isinstance(result, dict):\n                raise ValueError('Dictionary expected, but %r found.' % result)\n            if spec:\n                spec_value = next(iter(spec.values()))  # Yay Python 3!\n                for value in result.values():\n                    self.validate(value, spec_value)\n                spec_key = next(iter(spec.keys()))\n                for key in result:\n                    self.validate(key, spec_key)\n        if isinstance(spec, list):\n            if not isinstance(result, list):\n                raise ValueError('List expected, but %r found.' % result)\n            if spec:\n                for value in result:\n                    self.validate(value, spec[0])\n        if isinstance(spec, tuple):\n            if not isinstance(result, tuple):\n                raise ValueError('Tuple expected, but %r found.' % result)\n            if len(result) != len(spec):\n                raise ValueError('Expected %d elements in tuple %r.' %\n                                 (len(spec), result))\n            for s, value in zip(spec, result):\n                self.validate(value, s)\n        if isinstance(spec, six.string_types):\n            if not isinstance(result, six.string_types):\n                raise ValueError('String expected, but %r found.' % result)\n        if isinstance(spec, int):\n            if not isinstance(result, int):\n                raise ValueError('Integer expected, but %r found.' % result)\n        if isinstance(spec, bool):\n            if not isinstance(result, bool):\n                raise ValueError('Boolean expected, but %r found.' % result)", "response": "Validate that the result has the correct structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, result, spec):\n        if isinstance(spec, dict):\n            if spec:\n                spec_value = next(iter(spec.values()))\n                for key, value in result.items():\n                    result[key] = self.update(value, spec_value)\n        if isinstance(spec, list):\n            if spec:\n                for i, value in enumerate(result):\n                    result[i] = self.update(value, spec[0])\n        if isinstance(spec, tuple):\n            return tuple(self.update(value, s)\n                         for s, value in zip(spec, result))\n        if callable(spec):\n            return spec(result)\n        return result", "response": "Replace elements with results of calling callables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of available images for the given project_id.", "response": "def get_available_images(request, project_id=None, images_cache=None):\n    \"\"\"Returns a list of available images\n\n    Returns a list of images that are public, shared, community or owned by\n    the given project_id. If project_id is not specified, only public and\n    community images are returned.\n\n    :param images_cache: An optional dict-like object in which to\n    cache public and per-project id image metadata.\n    \"\"\"\n\n    if images_cache is None:\n        images_cache = {}\n    public_images = images_cache.get('public_images', [])\n    community_images = images_cache.get('community_images', [])\n    images_by_project = images_cache.get('images_by_project', {})\n    shared_images = images_cache.get('shared_images', [])\n    if 'public_images' not in images_cache:\n        public = {\"is_public\": True,\n                  \"status\": \"active\"}\n        try:\n            images, _more, _prev = glance.image_list_detailed(\n                request, filters=public)\n            public_images += images\n            images_cache['public_images'] = public_images\n        except Exception:\n            exceptions.handle(request,\n                              _(\"Unable to retrieve public images.\"))\n\n    # Preempt if we don't have a project_id yet.\n    if project_id is None:\n        images_by_project[project_id] = []\n\n    if project_id not in images_by_project:\n        owner = {\"property-owner_id\": project_id,\n                 \"status\": \"active\"}\n        try:\n            owned_images, _more, _prev = glance.image_list_detailed(\n                request, filters=owner)\n            images_by_project[project_id] = owned_images\n        except Exception:\n            owned_images = []\n            exceptions.handle(request,\n                              _(\"Unable to retrieve images for \"\n                                \"the current project.\"))\n    else:\n        owned_images = images_by_project[project_id]\n\n    if 'community_images' not in images_cache:\n        community = {\"visibility\": \"community\",\n                     \"status\": \"active\"}\n        try:\n            images, _more, _prev = glance.image_list_detailed(\n                request, filters=community)\n            community_images += images\n            images_cache['community_images'] = community_images\n        except Exception:\n            exceptions.handle(request,\n                              _(\"Unable to retrieve community images.\"))\n\n    if 'shared_images' not in images_cache:\n        shared = {\"visibility\": \"shared\",\n                  \"status\": \"active\"}\n        try:\n            shared_images, _more, _prev = \\\n                glance.image_list_detailed(request, filters=shared)\n            images_cache['shared_images'] = shared_images\n        except Exception:\n            exceptions.handle(request,\n                              _(\"Unable to retrieve shared images.\"))\n\n    if 'images_by_project' not in images_cache:\n        images_cache['images_by_project'] = images_by_project\n\n    images = owned_images + public_images + community_images + shared_images\n\n    image_ids = []\n    final_images = []\n    for image in images:\n        if image.id not in image_ids and \\\n                image.container_format not in ('aki', 'ari'):\n            image_ids.append(image.id)\n            final_images.append(image)\n    return final_images"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_field_data(request, include_empty_option=False):\n    try:\n        images = get_available_images(request, request.user.project_id)\n    except Exception:\n        exceptions.handle(request, _('Unable to retrieve images'))\n    images.sort(key=lambda c: c.name)\n    images_list = [('', _('Select Image'))]\n    for image in images:\n        image_label = u\"{} ({})\".format(image.name, filesizeformat(image.size))\n        images_list.append((image.id, image_label))\n\n    if not images:\n        return [(\"\", _(\"No images available\")), ]\n\n    return images_list", "response": "Generates a sorted list of images available and returns a list of tuples of all images."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef availability_zone_list(request):\n    try:\n        if api.cinder.extension_supported(request, 'AvailabilityZones'):\n            return api.cinder.availability_zone_list(request)\n        return []\n    except Exception:\n        exceptions.handle(request,\n                          _('Unable to retrieve volumes availability zones.'))\n        return []", "response": "Utility method to retrieve a list of availability zones."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef horizon_main_nav(context):\n    if 'request' not in context:\n        return {}\n    current_dashboard = context['request'].horizon.get('dashboard', None)\n    dashboards = []\n    for dash in Horizon.get_dashboards():\n        if dash.can_access(context):\n            if callable(dash.nav) and dash.nav(context):\n                dashboards.append(dash)\n            elif dash.nav:\n                dashboards.append(dash)\n    return {'components': dashboards,\n            'user': context['request'].user,\n            'current': current_dashboard,\n            'request': context['request']}", "response": "Generates top - level dashboard navigation entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates sub - navigation entries for the current dashboard.", "response": "def horizon_dashboard_nav(context):\n    \"\"\"Generates sub-navigation entries for the current dashboard.\"\"\"\n    if 'request' not in context:\n        return {}\n    dashboard = context['request'].horizon['dashboard']\n    panel_groups = dashboard.get_panel_groups()\n    non_empty_groups = []\n\n    for group in panel_groups.values():\n        allowed_panels = []\n        for panel in group:\n            if (callable(panel.nav) and panel.nav(context) and\n                    panel.can_access(context)):\n                allowed_panels.append(panel)\n            elif (not callable(panel.nav) and panel.nav and\n                    panel.can_access(context)):\n                allowed_panels.append(panel)\n        if allowed_panels:\n            if group.name is None:\n                non_empty_groups.append((dashboard.name, allowed_panels))\n            else:\n                non_empty_groups.append((group.name, allowed_panels))\n\n    return {'components': OrderedDict(non_empty_groups),\n            'user': context['request'].user,\n            'current': context['request'].horizon['panel'].slug,\n            'request': context['request']}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jstemplate(parser, token):\n    nodelist = parser.parse(('endjstemplate',))\n    parser.delete_first_token()\n    return JSTemplateNode(nodelist)", "response": "This is a template tag that can be used to handle any of the Mustache - based templates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef minifyspace(parser, token):\n    nodelist = parser.parse(('endminifyspace',))\n    parser.delete_first_token()\n    return MinifiedNode(nodelist)", "response": "Removes whitespace including tab and newline characters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_message_notification(request, messages_path):\n    if not messages_path:\n        return\n\n    global _MESSAGES_CACHE\n    global _MESSAGES_MTIME\n\n    # NOTE (lhcheng): Cache the processed messages to avoid parsing\n    # the files every time. Check directory modification time if\n    # reload is necessary.\n    if (_MESSAGES_CACHE is None or\n            _MESSAGES_MTIME != os.path.getmtime(messages_path)):\n        _MESSAGES_CACHE = _get_processed_messages(messages_path)\n        _MESSAGES_MTIME = os.path.getmtime(messages_path)\n\n    for msg in _MESSAGES_CACHE:\n        msg.send_message(request)", "response": "Process all the msg files found in the message directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self):\n        try:\n            self._read()\n            self._parse()\n        except Exception as exc:\n            self.failed = True\n\n            params = {'path': self._path, 'exception': exc}\n            if self.fail_silently:\n                LOG.warning(\"Error processing message json file '%(path)s': \"\n                            \"%(exception)s\", params)\n            else:\n                raise exceptions.MessageFailure(\n                    _(\"Error processing message json file '%(path)s': \"\n                      \"%(exception)s\") % params)", "response": "Read and parse the message file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the table server filter information in the session. Returns True if the filter has been changed.", "response": "def handle_server_filter(self, request, table=None):\n        \"\"\"Update the table server filter information in the session.\n\n        Returns True if the filter has been changed.\n        \"\"\"\n        if not table:\n            table = self.get_table()\n        filter_info = self.get_server_filter_info(request, table)\n        if filter_info is None:\n            return False\n        request.session[filter_info['value_param']] = filter_info['value']\n        if filter_info['field_param']:\n            request.session[filter_info['field_param']] = filter_info['field']\n        return filter_info['changed']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the table server side filter action.", "response": "def update_server_filter_action(self, request, table=None):\n        \"\"\"Update the table server side filter action.\n\n        It is done based on the current filter. The filter info may be stored\n        in the session and this will restore it.\n        \"\"\"\n        if not table:\n            table = self.get_table()\n        filter_info = self.get_server_filter_info(request, table)\n        if filter_info is not None:\n            action = filter_info['action']\n            setattr(action, 'filter_string', filter_info['value'])\n            if filter_info['field_param']:\n                setattr(action, 'filter_field', filter_info['field'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a string given by the user into a valid api filter value.", "response": "def get_filters(self, filters=None, filters_map=None):\n        \"\"\"Converts a string given by the user into a valid api filter value.\n\n        :filters: Default filter values.\n            {'filter1': filter_value, 'filter2': filter_value}\n        :filters_map: mapping between user input and valid api filter values.\n            {'filter_name':{_(\"true_value\"):True, _(\"false_value\"):False}\n        \"\"\"\n        filters = filters or {}\n        filters_map = filters_map or {}\n        filter_action = self.table._meta._filter_action\n        if filter_action:\n            filter_field = self.table.get_filter_field()\n            if filter_action.is_api_filter(filter_field):\n                filter_string = self.table.get_filter_string().strip()\n                if filter_field and filter_string:\n                    filter_map = filters_map.get(filter_field, {})\n                    filters[filter_field] = filter_string\n                    for k, v in filter_map.items():\n                        # k is django.utils.functional.__proxy__\n                        # and could not be searched in dict\n                        if filter_string.lower() == k:\n                            filters[filter_field] = v\n                            break\n        return filters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _delete_network(self, request, network):\n        try:\n            api.neutron.network_delete(request, network.id)\n            LOG.debug('Delete the created network %s '\n                      'due to subnet creation failure.', network.id)\n            msg = _('Delete the created network \"%s\" '\n                    'due to subnet creation failure.') % network.name\n            redirect = self.get_failure_url()\n            messages.info(request, msg)\n            raise exceptions.Http302(redirect)\n        except Exception as e:\n            LOG.info('Failed to delete network %(id)s: %(exc)s',\n                     {'id': network.id, 'exc': e})\n            msg = _('Failed to delete network \"%s\"') % network.name\n            redirect = self.get_failure_url()\n            exceptions.handle(request, msg, redirect=redirect)", "response": "Delete the created network when subnet creation failed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bs_progress_bar(*args, **kwargs):\n\n    bars = []\n    contexts = kwargs.get(\n        'contexts',\n        ['', 'success', 'info', 'warning', 'danger']\n    )\n\n    for ndx, arg in enumerate(args):\n        bars.append(\n            dict(percent=arg,\n                 context=kwargs.get('context', contexts[ndx % len(contexts)]))\n        )\n\n    return {\n        'bars': bars,\n        'text': kwargs.pop('text', False),\n        'striped': kwargs.pop('striped', False),\n        'animated': kwargs.pop('animated', False),\n        'min_val': kwargs.pop('min_val', 0),\n        'max_val': kwargs.pop('max_val', 100),\n    }", "response": "A basic Bootstrap Progress Bar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the string that is used to filter the given datetime object or the default string.", "response": "def timesince_or_never(dt, default=None):\n    \"\"\"Call the Django ``timesince`` filter or a given default string.\n\n    It returns the string *default* if *dt* is not a valid ``date``\n    or ``datetime`` object.\n    When *default* is None, \"Never\" is returned.\n    \"\"\"\n    if default is None:\n        default = _(\"Never\")\n\n    if isinstance(dt, datetime.date):\n        return timesince(dt)\n    else:\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_plugin(self, service_provider=None, auth_url=None, plugins=None,\n                   **kwargs):\n        \"\"\"Authenticate using keystone to keystone federation.\n\n        This plugin uses other v3 plugins to authenticate a user to a\n        identity provider in order to authenticate the user to a service\n        provider\n\n        :param service_provider: service provider ID\n        :param auth_url: Keystone auth url\n        :param plugins: list of openstack_auth plugins to check\n        :returns Keystone2Keystone keystone auth plugin\n        \"\"\"\n\n        # Avoid mutable default arg for plugins\n        plugins = plugins or []\n\n        # service_provider being None prevents infinite recursion\n        if utils.get_keystone_version() < 3 or not service_provider:\n            return None\n\n        keystone_idp_id = getattr(settings, 'KEYSTONE_PROVIDER_IDP_ID',\n                                  'localkeystone')\n        if service_provider == keystone_idp_id:\n            return None\n\n        for plugin in plugins:\n            unscoped_idp_auth = plugin.get_plugin(plugins=plugins,\n                                                  auth_url=auth_url, **kwargs)\n            if unscoped_idp_auth:\n                break\n        else:\n            LOG.debug('Could not find base authentication backend for '\n                      'K2K plugin with the provided credentials.')\n            return None\n\n        idp_exception = None\n        scoped_idp_auth = None\n        unscoped_auth_ref = base.BasePlugin.get_access_info(\n            self, unscoped_idp_auth)\n        try:\n            scoped_idp_auth, __ = self.get_project_scoped_auth(\n                unscoped_idp_auth, unscoped_auth_ref,\n                recent_project=kwargs['recent_project'])\n        except exceptions.KeystoneAuthException as idp_excp:\n            idp_exception = idp_excp\n\n        if not scoped_idp_auth or idp_exception:\n            msg = _('Identity provider authentication failed.')\n            raise exceptions.KeystoneAuthException(msg)\n\n        session = utils.get_session()\n\n        if scoped_idp_auth.get_sp_auth_url(session, service_provider) is None:\n            msg = _('Could not find service provider ID on keystone.')\n            raise exceptions.KeystoneAuthException(msg)\n\n        unscoped_auth = v3_auth.Keystone2Keystone(\n            base_plugin=scoped_idp_auth,\n            service_provider=service_provider)\n        return unscoped_auth", "response": "Authenticate using Keystone to Keystone federation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_access_info(self, unscoped_auth):\n        try:\n            unscoped_auth_ref = base.BasePlugin.get_access_info(\n                self, unscoped_auth)\n        except exceptions.KeystoneAuthException as excp:\n            msg = _('Service provider authentication failed. %s')\n            raise exceptions.KeystoneAuthException(msg % str(excp))\n        return unscoped_auth_ref", "response": "Get the access info object for the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_token_expired(self, margin=None):\n        if self.token is None:\n            return None\n        return not utils.is_token_valid(self.token, margin)", "response": "Returns True if the token is expired False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating whether this user has admin privileges.", "response": "def is_superuser(self):\n        \"\"\"Evaluates whether this user has admin privileges.\n\n        :returns: ``True`` or ``False``.\n        \"\"\"\n        admin_roles = utils.get_admin_roles()\n        user_roles = {role['name'].lower() for role in self.roles}\n        return not admin_roles.isdisjoint(user_roles)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a memoized list of tenants this user may access.", "response": "def authorized_tenants(self):\n        \"\"\"Returns a memoized list of tenants this user may access.\"\"\"\n        if self.is_authenticated and self._authorized_tenants is None:\n            endpoint = self.endpoint\n            try:\n                self._authorized_tenants = utils.get_project_list(\n                    user_id=self.id,\n                    auth_url=endpoint,\n                    token=self.unscoped_token,\n                    is_federated=self.is_federated)\n            except (keystone_exceptions.ClientException,\n                    keystone_exceptions.AuthorizationFailure):\n                LOG.exception('Unable to retrieve project list.')\n        return self._authorized_tenants or []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of unique region name values in service catalog.", "response": "def available_services_regions(self):\n        \"\"\"Returns list of unique region name values in service catalog.\"\"\"\n        regions = []\n        if self.service_catalog:\n            for service in self.service_catalog:\n                service_type = service.get('type')\n                if service_type is None or service_type == 'identity':\n                    continue\n                for endpoint in service.get('endpoints', []):\n                    region = utils.get_endpoint_region(endpoint)\n                    if region not in regions:\n                        regions.append(region)\n        return regions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_a_matching_perm(self, perm_list, obj=None):\n        # If there are no permissions to check, just return true\n        if not perm_list:\n            return True\n        # Check that user has at least one of the required permissions.\n        for perm in perm_list:\n            if self.has_perm(perm, obj):\n                return True\n        return False", "response": "Returns True if the user has one of the specified permissions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the user has all of the specified permissions.", "response": "def has_perms(self, perm_list, obj=None):\n        \"\"\"Returns True if the user has all of the specified permissions.\n\n        Tuples in the list will possess the required permissions if\n        the user has a permissions matching one of the elements of\n        that tuple\n        \"\"\"\n        # If there are no permissions to check, just return true\n        if not perm_list:\n            return True\n        for perm in perm_list:\n            if isinstance(perm, six.string_types):\n                # check that the permission matches\n                if not self.has_perm(perm, obj):\n                    return False\n            else:\n                # check that a permission in the tuple matches\n                if not self.has_a_matching_perm(perm, obj):\n                    return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the number of days until the user s password expires.", "response": "def time_until_expiration(self):\n        \"\"\"Returns the number of remaining days until user's password expires.\n\n        Calculates the number days until the user must change their password,\n        once the password expires the user will not able to log in until an\n        admin changes its password.\n        \"\"\"\n        if self.password_expires_at is not None:\n            expiration_date = datetime.datetime.strptime(\n                self.password_expires_at, \"%Y-%m-%dT%H:%M:%S.%f\")\n            return expiration_date - datetime.datetime.now()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpire any cookie that initiated profiling request.", "response": "def clear_profiling_cookies(request, response):\n        \"\"\"Expire any cookie that initiated profiling request.\"\"\"\n        if 'profile_page' in request.COOKIES:\n            path = request.path\n            response.set_cookie('profile_page', max_age=0, path=path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a page title and insert it into the context.", "response": "def render_context_with_title(self, context):\n        \"\"\"Render a page title and insert it into the context.\n\n        This function takes in a context dict and uses it to render the\n        page_title variable. It then appends this title to the context using\n        the 'page_title' key. If there is already a page_title key defined in\n        context received then this function will do nothing.\n        \"\"\"\n\n        if \"page_title\" not in context:\n            con = template.Context(context)\n            # NOTE(sambetts): Use force_text to ensure lazy translations\n            # are handled correctly.\n            temp = template.Template(encoding.force_text(self.page_title))\n            context[\"page_title\"] = temp.render(con)\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a context with a page title.", "response": "def render_to_response(self, context):\n        \"\"\"render_to_response() with a page title.\n\n        This is an override of the default render_to_response function that\n        exists in the django generic views. This is here to inject the\n        page title into the context before the main template is rendered.\n        \"\"\"\n\n        context = self.render_context_with_title(context)\n        return super(PageTitleMixin, self).render_to_response(context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(actions, request, target=None):\n    if target is None:\n        target = {}\n    user = auth_utils.get_user(request)\n\n    # Several service policy engines default to a project id check for\n    # ownership. Since the user is already scoped to a project, if a\n    # different project id has not been specified use the currently scoped\n    # project's id.\n    #\n    # The reason is the operator can edit the local copies of the service\n    # policy file. If a rule is removed, then the default rule is used. We\n    # don't want to block all actions because the operator did not fully\n    # understand the implication of editing the policy file. Additionally,\n    # the service APIs will correct us if we are too permissive.\n    if target.get('project_id') is None:\n        target['project_id'] = user.project_id\n    if target.get('tenant_id') is None:\n        target['tenant_id'] = target['project_id']\n    # same for user_id\n    if target.get('user_id') is None:\n        target['user_id'] = user.id\n\n    domain_id_keys = [\n        'domain_id',\n        'project.domain_id',\n        'user.domain_id',\n        'group.domain_id'\n    ]\n    # populates domain id keys with user's current domain id\n    for key in domain_id_keys:\n        if target.get(key) is None:\n            target[key] = user.user_domain_id\n\n    credentials = _user_to_credentials(user)\n    domain_credentials = _domain_to_credentials(request, user)\n    # if there is a domain token use the domain_id instead of the user's domain\n    if domain_credentials:\n        credentials['domain_id'] = domain_credentials.get('domain_id')\n\n    enforcer = _get_enforcer()\n\n    for action in actions:\n        scope, action = action[0], action[1]\n        if scope in enforcer:\n            # this is for handling the v3 policy file and will only be\n            # needed when a domain scoped token is present\n            if scope == 'identity' and domain_credentials:\n                # use domain credentials\n                if not _check_credentials(enforcer[scope],\n                                          action,\n                                          target,\n                                          domain_credentials):\n                    return False\n\n            # use project credentials\n            if not _check_credentials(enforcer[scope],\n                                      action, target, credentials):\n                return False\n\n        # if no policy for scope, allow action, underlying API will\n        # ultimately block the action if not permitted, treat as though\n        # allowed\n    return True", "response": "Checks if the user has permission to the action according to the policy setting."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves servers networking information from Neutron if enabled.", "response": "def servers_update_addresses(request, servers, all_tenants=False):\n    \"\"\"Retrieve servers networking information from Neutron if enabled.\n\n       Should be used when up to date networking information is required,\n       and Nova's networking info caching mechanism is not fast enough.\n\n    \"\"\"\n    # NOTE(amotoki): This check is still needed because 'instances' panel\n    # calls this method. We dropped security group and floating IP support\n    # through Nova API (due to novaclient 8.0.0 drops their supports),\n    # but we can still support 'Instances' panel with nova-network.\n    # TODO(amotoki): Nova networkinfo info caching mechanism is now fast enough\n    # as they are updated by Neutron via Nova event callback mechasm,\n    # so servers_update_addresses is no longer needed.\n    # We can reduce API calls by dropping it.\n    neutron_enabled = base.is_service_enabled(request, 'network')\n    if neutron_enabled:\n        neutron.servers_update_addresses(request, servers, all_tenants)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logout_with_message(request, msg, redirect=True, status='success'):\n    logout(request)\n    if redirect:\n        response = http.HttpResponseRedirect(\n            '%s?next=%s' % (settings.LOGOUT_URL, request.path))\n    else:\n        response = http.HttpResponseRedirect(settings.LOGOUT_URL)\n    add_logout_reason(request, response, msg, status)\n    return response", "response": "Send HttpResponseRedirect to LOGOUT_URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_config_value(request, key, default, search_in_settings=True):\n    value = request.session.get(key, request.COOKIES.get(key))\n\n    if value is None:\n        if search_in_settings:\n            value = getattr(settings, key, default)\n        else:\n            value = default\n\n    if isinstance(default, int):\n        try:\n            value = int(value)\n        except ValueError:\n            value = request.session[key] = int(default)\n\n    return value", "response": "Retrieves the value of key from the configuration file or from the request session cookies or settings file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_config_value(request, response, key, value):\n    request.session[key] = value\n    response.set_cookie(key, value, expires=one_year_from_now())\n    return response", "response": "Sets value of key to value in both session and cookies."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next_key(tuple_of_tuples, key):\n    for i, t in enumerate(tuple_of_tuples):\n        if t[0] == key:\n            try:\n                return tuple_of_tuples[i + 1][0]\n            except IndexError:\n                return None", "response": "Returns the next key in the given tuple of 2 - element tuples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the key which comes before the given key.", "response": "def previous_key(tuple_of_tuples, key):\n    \"\"\"Returns the key which comes before the give key.\n\n    It Processes a tuple of 2-element tuples and returns the key which comes\n    before the given key.\n    \"\"\"\n    for i, t in enumerate(tuple_of_tuples):\n        if t[0] == key:\n            try:\n                return tuple_of_tuples[i - 1][0]\n            except IndexError:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the given value rounded to one decimal place if deciaml. Returns the integer if an integer is given.", "response": "def format_value(value):\n    \"\"\"Returns the given value rounded to one decimal place if deciaml.\n\n    Returns the integer if an integer is given.\n    \"\"\"\n    value = decimal.Decimal(str(value))\n    if int(value) == value:\n        return int(value)\n\n    # On Python 3, an explicit cast to float is required\n    return float(round(value, 1))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(self, table, cg_snapshots, filter_string):\n        query = filter_string.lower()\n        return [cg_snapshot for cg_snapshot in cg_snapshots\n                if query in cg_snapshot.name.lower()]", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_request(self, request):\n\n        request.horizon = {'dashboard': None,\n                           'panel': None,\n                           'async_messages': []}\n        if not hasattr(request, \"user\") or not request.user.is_authenticated:\n            # proceed no further if the current request is already known\n            # not to be authenticated\n            # it is CRITICAL to perform this check as early as possible\n            # to avoid creating too many sessions\n            return None\n\n        # Since we know the user is present and authenticated, lets refresh the\n        # session expiry if configured to do so.\n        if getattr(settings, \"SESSION_REFRESH\", True):\n            timeout = getattr(settings, \"SESSION_TIMEOUT\", 3600)\n            token_life = request.user.token.expires - datetime.datetime.now(\n                pytz.utc)\n            session_time = min(timeout, int(token_life.total_seconds()))\n            request.session.set_expiry(session_time)\n\n        if request.is_ajax():\n            # if the request is Ajax we do not want to proceed, as clients can\n            #  1) create pages with constant polling, which can create race\n            #     conditions when a page navigation occurs\n            #  2) might leave a user seemingly left logged in forever\n            #  3) thrashes db backed session engines with tons of changes\n            return None\n        # If we use cookie-based sessions, check that the cookie size does not\n        # reach the max size accepted by common web browsers.\n        if (\n            settings.SESSION_ENGINE ==\n            'django.contrib.sessions.backends.signed_cookies'\n        ):\n            max_cookie_size = getattr(\n                settings, 'SESSION_COOKIE_MAX_SIZE', None)\n            session_cookie_name = getattr(\n                settings, 'SESSION_COOKIE_NAME', None)\n            session_key = request.COOKIES.get(session_cookie_name)\n            if max_cookie_size is not None and session_key is not None:\n                cookie_size = sum((\n                    len(key) + len(value)\n                    for key, value in request.COOKIES.items()\n                ))\n                if cookie_size >= max_cookie_size:\n                    LOG.error(\n                        'Total Cookie size for user_id: %(user_id)s is '\n                        '%(cookie_size)sB >= %(max_cookie_size)sB. '\n                        'You need to configure file-based or database-backed '\n                        'sessions instead of cookie-based sessions: '\n                        'https://docs.openstack.org/horizon/latest/'\n                        'admin/sessions.html',\n                        {\n                            'user_id': request.session.get(\n                                'user_id', 'Unknown'),\n                            'cookie_size': cookie_size,\n                            'max_cookie_size': max_cookie_size,\n                        }\n                    )\n\n        tz = utils.get_timezone(request)\n        if tz:\n            timezone.activate(tz)", "response": "Adds data necessary for Horizon to function to the request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_exception(self, request, exception):\n        if isinstance(exception, (exceptions.NotAuthorized,\n                                  exceptions.NotAuthenticated)):\n            auth_url = settings.LOGIN_URL\n            next_url = iri_to_uri(request.get_full_path())\n            if next_url != auth_url:\n                field_name = REDIRECT_FIELD_NAME\n            else:\n                field_name = None\n            login_url = request.build_absolute_uri(auth_url)\n            response = redirect_to_login(next_url, login_url=login_url,\n                                         redirect_field_name=field_name)\n            if isinstance(exception, exceptions.NotAuthorized):\n                response.delete_cookie('messages')\n                return shortcuts.render(request, 'not_authorized.html',\n                                        status=403)\n\n            if request.is_ajax():\n                response_401 = http.HttpResponse(status=401)\n                response_401['X-Horizon-Location'] = response['location']\n                return response_401\n\n            return response\n\n        # If an internal \"NotFound\" error gets this far, return a real 404.\n        if isinstance(exception, exceptions.NotFound):\n            raise http.Http404(exception)\n\n        if isinstance(exception, exceptions.Http302):\n            # TODO(gabriel): Find a way to display an appropriate message to\n            # the user *on* the login form...\n            return shortcuts.redirect(exception.location)", "response": "Catches internal Horizon exception classes and handles them gracefully."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_response(self, request, response):\n        if request.is_ajax() and hasattr(request, 'horizon'):\n            queued_msgs = request.horizon['async_messages']\n            if type(response) == http.HttpResponseRedirect:\n                # Drop our messages back into the session as per usual so they\n                # don't disappear during the redirect. Not that we explicitly\n                # use django's messages methods here.\n                for tag, message, extra_tags in queued_msgs:\n                    getattr(django_messages, tag)(request, message, extra_tags)\n                if response['location'].startswith(settings.LOGOUT_URL):\n                    redirect_response = http.HttpResponse(status=401)\n                    # This header is used for handling the logout in JS\n                    redirect_response['logout'] = True\n                    if self.logout_reason is not None:\n                        utils.add_logout_reason(\n                            request, redirect_response, self.logout_reason,\n                            'error')\n                else:\n                    redirect_response = http.HttpResponse()\n                # Use a set while checking if we want a cookie's attributes\n                # copied\n                cookie_keys = {'max_age', 'expires', 'path', 'domain',\n                               'secure', 'httponly', 'logout_reason'}\n                # Copy cookies from HttpResponseRedirect towards HttpResponse\n                for cookie_name, cookie in response.cookies.items():\n                    cookie_kwargs = dict((\n                        (key, value) for key, value in cookie.items()\n                        if key in cookie_keys and value\n                    ))\n                    redirect_response.set_cookie(\n                        cookie_name, cookie.value, **cookie_kwargs)\n                redirect_response['X-Horizon-Location'] = response['location']\n                upload_url_key = 'X-File-Upload-URL'\n                if upload_url_key in response:\n                    self._copy_headers(response, redirect_response,\n                                       (upload_url_key, 'X-Auth-Token'))\n                return redirect_response\n            if queued_msgs:\n                # TODO(gabriel): When we have an async connection to the\n                # client (e.g. websockets) this should be pushed to the\n                # socket queue rather than being sent via a header.\n                # The header method has notable drawbacks (length limits,\n                # etc.) and is not meant as a long-term solution.\n                response['X-Horizon-Messages'] = json.dumps(queued_msgs)\n        return response", "response": "Convert HttpResponseRedirect to HttpResponse if request is via ajax."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the ID of the domain that the current operation should happen.", "response": "def get_domain_id_for_operation(request):\n    \"\"\"Get the ID of the domain in which the current operation should happen.\n\n    If the user has a domain context set, use that, otherwise use the user's\n    effective domain.\n    \"\"\"\n\n    domain_context = request.session.get('domain_context')\n    if domain_context:\n        return domain_context\n    return api.keystone.get_effective_domain_id(request)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the module path without importing anything.", "response": "def get_module_path(module_name):\n    \"\"\"Gets the module path without importing anything.\n\n    Avoids conflicts with package dependencies.\n    (taken from http://github.com/sitkatech/pypatch)\n    \"\"\"\n    path = sys.path\n    for name in module_name.split('.'):\n        file_pointer, path, desc = imp.find_module(name, path)\n        path = [path, ]\n        if file_pointer is not None:\n            file_pointer.close()\n\n    return path[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a diff between self. local_settings and the example file.", "response": "def gendiff(self, force=False):\n        \"\"\"Generate a diff between self.local_settings and the example file.\n\n        \"\"\"\n\n        with DirContext(self.local_settings_dir) as dircontext:\n            if not os.path.exists(self.local_settings_diff) or force:\n                with open(self.local_settings_example, 'r') as fp:\n                    example_lines = fp.readlines()\n                with open(self.local_settings_file, 'r') as fp:\n                    local_settings_lines = fp.readlines()\n                local_settings_example_mtime = time.strftime(\n                    self.time_fmt,\n                    time.localtime(\n                        os.stat(self.local_settings_example).st_mtime)\n                )\n\n                local_settings_mtime = time.strftime(\n                    self.time_fmt,\n                    time.localtime(os.stat(self.local_settings_file).st_mtime)\n                )\n\n                print('generating \"%s\"...' % os.path.join(\n                    dircontext.curdir,\n                    self.local_settings_diff)\n                )\n                with open(self.local_settings_diff, 'w') as fp:\n                    for line in difflib.unified_diff(\n                        example_lines, local_settings_lines,\n                        fromfile=self.local_settings_example,\n                        tofile=self.local_settings_file,\n                        fromfiledate=local_settings_example_mtime,\n                        tofiledate=local_settings_mtime\n                    ):\n                        fp.write(line)\n                print('\\tDONE.')\n                sys.exit(0)\n            else:\n                sys.exit(\n                    '\"%s\" already exists.' %\n                    os.path.join(dircontext.curdir,\n                                 self.local_settings_diff)\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch(self, force=False):\n\n        with DirContext(self.local_settings_dir) as dircontext:\n            if os.path.exists(self.local_settings_diff):\n                if not os.path.exists(self.local_settings_file) or force:\n                    local_settings_reject = \\\n                        self.local_settings_reject_pattern % (\n                            time.strftime(self.file_time_fmt, time.localtime())\n                        )\n                    patch_cmd = shlex.split(\n                        'patch %s %s -o %s -r %s' % (\n                            self.local_settings_example,\n                            self.local_settings_diff,\n                            self.local_settings_file,\n                            local_settings_reject\n                        )\n                    )\n                    try:\n                        subprocess.check_call(patch_cmd)\n                    except subprocess.CalledProcessError:\n                        if os.path.exists(local_settings_reject):\n                            sys.exit(\n                                'Some conflict(s) occurred. Please check \"%s\" '\n                                'to find unapplied parts of the diff.\\n'\n                                'Once conflicts are solved, it is safer to '\n                                'regenerate a newer diff with the \"--gendiff\" '\n                                'option.' %\n                                os.path.join(\n                                    dircontext.curdir,\n                                    local_settings_reject)\n                            )\n                        else:\n                            sys.exit('An unhandled error occurred.')\n                    print('Generation of \"%s\" successful.' % os.path.join(\n                        dircontext.curdir,\n                        self.local_settings_file)\n                    )\n                    sys.exit(0)\n                else:\n                    sys.exit(\n                        '\"%s\" already exists.' %\n                        os.path.join(dircontext.curdir,\n                                     self.local_settings_file)\n                    )\n            else:\n                sys.exit('No diff file found, please generate one with the '\n                         '\"--gendiff\" option.')", "response": "Patch the local_settings. py. example with local_settings. diff."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap of the configurable policy method.", "response": "def check(actions, request, target=None):\n    \"\"\"Wrapper of the configurable policy method.\"\"\"\n\n    policy_check = utils_settings.import_setting(\"POLICY_CHECK_FUNCTION\")\n\n    if policy_check:\n        return policy_check(actions, request, target)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_auth_params_from_request(request):\n    return (\n        request.user.username,\n        request.user.token.id,\n        request.user.tenant_id,\n        request.user.token.project.get('domain_id'),\n        base.url_for(request, 'compute'),\n        base.url_for(request, 'identity')\n    )", "response": "Extracts the auth params needed by novaclient from the request object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_context(request, context=None):\n    if context is None:\n        context = {}\n\n    network_config = getattr(settings, 'OPENSTACK_NEUTRON_NETWORK', {})\n\n    context['launch_instance_allowed'] = policy.check(\n        ((\"compute\", \"os_compute_api:servers:create\"),), request)\n    context['instance_quota_exceeded'] = _quota_exceeded(request, 'instances')\n    context['create_network_allowed'] = policy.check(\n        ((\"network\", \"create_network\"),), request)\n    context['network_quota_exceeded'] = _quota_exceeded(request, 'network')\n    context['create_router_allowed'] = (\n        network_config.get('enable_router', True) and\n        policy.check(((\"network\", \"create_router\"),), request))\n    context['router_quota_exceeded'] = _quota_exceeded(request, 'router')\n    context['console_type'] = getattr(settings, 'CONSOLE_TYPE', 'AUTO')\n    context['show_ng_launch'] = (\n        base.is_service_enabled(request, 'compute') and\n        getattr(settings, 'LAUNCH_INSTANCE_NG_ENABLED', True))\n    context['show_legacy_launch'] = (\n        base.is_service_enabled(request, 'compute') and\n        getattr(settings, 'LAUNCH_INSTANCE_LEGACY_ENABLED', False))\n    return context", "response": "Returns common context data for network topology views."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_object_id(self, datum):\n\n        # Role assignment doesn't have identifier so one will be created\n        # from the identifier of scope, user and role. This will guaranty the\n        # unicity.\n        scope_id = \"\"\n        if \"project\" in datum.scope:\n            scope_id = datum.scope[\"project\"][\"id\"]\n        elif \"domain\" in datum.scope:\n            scope_id = datum.scope[\"domain\"][\"id\"]\n\n        assignee_id = \"\"\n        if hasattr(datum, \"user\"):\n            assignee_id = datum.user[\"id\"]\n        elif hasattr(datum, \"group\"):\n            assignee_id = datum.group[\"id\"]\n\n        return \"%s%s%s\" % (assignee_id, datum.role[\"id\"], scope_id)", "response": "Return the object identifier of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, name=None, value=None, attrs=None):\n        attrs = {} or attrs\n        self.attrs = attrs\n        self.name = name\n        self.value = value\n\n        id_ = self.attrs.get('id')\n        output = []\n        for i, choice in enumerate(self.choices):\n            choice_value, choice_label = choice\n            if isinstance(choice_label, (tuple, list)):\n                attrs_plus = self.attrs.copy()\n                if id_:\n                    attrs_plus['id'] += '_{}'.format(i)\n                sub_ul_renderer = self.__class__(\n                    attrs=attrs_plus,\n                    choices=choice_label,\n                )\n                sub_ul_renderer.choice_input_class = self.choice_input_class\n                output.append(html.format_html(\n                    self.inner_html, choice_value=choice_value,\n                    sub_widgets=sub_ul_renderer.render(),\n                ))\n            else:\n                w = self.choice_input_class(\n                    self.name, self.value, self.attrs.copy(), choice, i)\n                output.append(html.format_html(\n                    self.inner_html,\n                    choice_value=force_text(w),\n                    sub_widgets=''))\n        return html.format_html(\n            self.outer_html,\n            id_attr=html.format_html(' id=\"{}\"', id_) if id_ else '',\n            content=mark_safe('\\n'.join(output)),\n        )", "response": "Renders the list of items for this field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the URL for the given service and region.", "response": "def get_url_for_service(service, region, endpoint_type):\n    if 'type' not in service:\n        return None\n\n    identity_version = get_version_from_service(service)\n    service_endpoints = service.get('endpoints', [])\n    available_endpoints = [endpoint for endpoint in service_endpoints\n                           if region == _get_endpoint_region(endpoint)]\n    \"\"\"if we are dealing with the identity service and there is no endpoint\n    in the current region, it is okay to use the first endpoint for any\n    identity service endpoints and we can assume that it is global\n    \"\"\"\n    if service['type'] == 'identity' and not available_endpoints:\n        available_endpoints = [endpoint for endpoint in service_endpoints]\n\n    for endpoint in available_endpoints:\n        try:\n            if identity_version < 3:\n                return endpoint.get(endpoint_type)\n            else:\n                interface = \\\n                    ENDPOINT_TYPE_TO_INTERFACE.get(endpoint_type, '')\n                if endpoint.get('interface') == interface:\n                    return endpoint.get('url')\n        except (IndexError, KeyError):\n            # it could be that the current endpoint just doesn't match the\n            # type, continue trying the next one\n            pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a boolean indicating whether unit_1 is larger than unit_2.", "response": "def is_larger(unit_1, unit_2):\n    \"\"\"Returns a boolean indicating whether unit_1 is larger than unit_2.\n\n    E.g:\n\n    >>> is_larger('KB', 'B')\n    True\n    >>> is_larger('min', 'day')\n    False\n    \"\"\"\n    unit_1 = functions.value_for_key(INFORMATION_UNITS, unit_1)\n    unit_2 = functions.value_for_key(INFORMATION_UNITS, unit_2)\n\n    return ureg.parse_expression(unit_1) > ureg.parse_expression(unit_2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert value from source_unit to target_unit.", "response": "def convert(value, source_unit, target_unit, fmt=False):\n    \"\"\"Converts value from source_unit to target_unit.\n\n    Returns a tuple containing the converted value and target_unit.\n    Having fmt set to True causes the value to be formatted to 1 decimal digit\n    if it's a decimal or be formatted as integer if it's an integer.\n\n    E.g:\n\n    >>> convert(2, 'hr', 'min')\n    (120.0, 'min')\n    >>> convert(2, 'hr', 'min', fmt=True)\n    (120, 'min')\n    >>> convert(30, 'min', 'hr', fmt=True)\n    (0.5, 'hr')\n    \"\"\"\n    orig_target_unit = target_unit\n    source_unit = functions.value_for_key(INFORMATION_UNITS, source_unit)\n    target_unit = functions.value_for_key(INFORMATION_UNITS, target_unit)\n\n    q = ureg.Quantity(value, source_unit)\n    q = q.to(ureg.parse_expression(target_unit))\n    value = functions.format_value(q.magnitude) if fmt else q.magnitude\n    return value, orig_target_unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize(value, unit):\n    if value < 0:\n        raise ValueError('Negative value: %s %s.' % (value, unit))\n\n    if unit in functions.get_keys(INFORMATION_UNITS):\n        return _normalize_information(value, unit)\n    elif unit in TIME_UNITS:\n        return _normalize_time(value, unit)\n    else:\n        # Unknown unit, just return it\n        return functions.format_value(value), unit", "response": "Converts the value so that it belongs to some expected range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_filters_kwargs(request, client_keywords=None):\n    filters = {}\n    kwargs = {}\n    client_keywords = client_keywords or {}\n    for param in request.GET:\n        param_value = PARAM_MAPPING.get(request.GET[param], request.GET[param])\n        if param in client_keywords:\n            kwargs[param] = param_value\n        else:\n            filters[param] = param_value\n    return filters, kwargs", "response": "Extract REST filter parameters from the request GET args."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post2data(func):\n    def wrapper(self, request):\n        request.DATA = request.POST\n        if '$$originalJSON' in request.POST:\n            request.DATA = jsonutils.loads(request.POST['$$originalJSON'])\n\n        return func(self, request)\n\n    return wrapper", "response": "Decorator to restore original form values along with their types stored on client - side under key $$originalJSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean(self):\n        '''Check to make sure password fields match.'''\n        data = super(PasswordForm, self).clean()\n        if 'new_password' in data:\n            if data['new_password'] != data.get('confirm_password', None):\n                raise ValidationError(_('Passwords do not match.'))\n            if data.get('confirm_password', None) == \\\n                    data.get('current_password', None):\n                raise ValidationError(_('Old password and new password '\n                                        'must be different'))\n        return data", "response": "Check to make sure password fields match."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_workflow(self):\n        extra_context = self.get_initial()\n        entry_point = self.request.GET.get(\"step\", None)\n        workflow = self.workflow_class(self.request,\n                                       context_seed=extra_context,\n                                       entry_point=entry_point)\n        return workflow", "response": "Returns the instantiated workflow class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the template context including the workflow class.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"Returns the template context, including the workflow class.\n\n        This method should be overridden in subclasses to provide additional\n        context data to the template.\n        \"\"\"\n        context = super(WorkflowView, self).get_context_data(**kwargs)\n        workflow = self.get_workflow()\n        workflow.verify_integrity()\n        context[self.context_object_name] = workflow\n        next = self.request.GET.get(workflow.redirect_param_name)\n        context['REDIRECT_URL'] = next\n        context['layout'] = self.get_layout()\n        # For consistency with Workflow class\n        context['modal'] = 'modal' in context['layout']\n\n        if ADD_TO_FIELD_HEADER in self.request.META:\n            context['add_to_field'] = self.request.META[ADD_TO_FIELD_HEADER]\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_layout(self):\n        if self.request.is_ajax():\n            layout = ['modal', ]\n        else:\n            layout = ['static_page', ]\n\n        if self.workflow_class.wizard:\n            layout += ['wizard', ]\n\n        return layout", "response": "Returns classes for the workflow element in template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the template name to use for this request.", "response": "def get_template_names(self):\n        \"\"\"Returns the template name to use for this request.\"\"\"\n        if self.request.is_ajax():\n            template = self.ajax_template_name\n        else:\n            template = self.template_name\n        return template"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, request, *args, **kwargs):\n        try:\n            context = self.get_context_data(**kwargs)\n        except exceptions.NotAvailable:\n            exceptions.handle(request)\n        self.set_workflow_step_errors(context)\n        return self.render_to_response(context)", "response": "Handler for HTTP GET requests."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the workflow steps from start to end inclusive. Returns a dictionary describing the validation state of the workflow.", "response": "def validate_steps(self, request, workflow, start, end):\n        \"\"\"Validates the workflow steps from ``start`` to ``end``, inclusive.\n\n        Returns a dict describing the validation state of the workflow.\n        \"\"\"\n        errors = {}\n        for step in workflow.steps[start:end + 1]:\n            if not step.action.is_valid():\n                errors[step.slug] = dict(\n                    (field, [six.text_type(error) for error in errors])\n                    for (field, errors) in step.action.errors.items())\n        return {\n            'has_errors': bool(errors),\n            'workflow_slug': workflow.slug,\n            'errors': errors,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling HTTP POST requests.", "response": "def post(self, request, *args, **kwargs):\n        \"\"\"Handler for HTTP POST requests.\"\"\"\n        context = self.get_context_data(**kwargs)\n        workflow = context[self.context_object_name]\n        try:\n            # Check for the VALIDATE_STEP* headers, if they are present\n            # and valid integers, return validation results as JSON,\n            # otherwise proceed normally.\n            validate_step_start = int(self.request.META.get(\n                'HTTP_X_HORIZON_VALIDATE_STEP_START', ''))\n            validate_step_end = int(self.request.META.get(\n                'HTTP_X_HORIZON_VALIDATE_STEP_END', ''))\n        except ValueError:\n            # No VALIDATE_STEP* headers, or invalid values. Just proceed\n            # with normal workflow handling for POSTs.\n            pass\n        else:\n            # There are valid VALIDATE_STEP* headers, so only do validation\n            # for the specified steps and return results.\n            data = self.validate_steps(request, workflow,\n                                       validate_step_start,\n                                       validate_step_end)\n            return http.HttpResponse(json.dumps(data),\n                                     content_type=\"application/json\")\n        if not workflow.is_valid():\n            return self.render_to_response(context)\n        try:\n            success = workflow.finalize()\n        except forms.ValidationError:\n            return self.render_to_response(context)\n        except Exception:\n            success = False\n            exceptions.handle(request)\n        if success:\n            msg = workflow.format_status_message(workflow.success_message)\n            messages.success(request, msg)\n        else:\n            msg = workflow.format_status_message(workflow.failure_message)\n            messages.error(request, msg)\n        if \"HTTP_X_HORIZON_ADD_TO_FIELD\" in self.request.META:\n            field_id = self.request.META[\"HTTP_X_HORIZON_ADD_TO_FIELD\"]\n            response = http.HttpResponse()\n            if workflow.object:\n                data = [self.get_object_id(workflow.object),\n                        self.get_object_display(workflow.object)]\n                response.content = json.dumps(data)\n                response[\"X-Horizon-Add-To-Field\"] = field_id\n            return response\n        next_url = self.request.POST.get(workflow.redirect_param_name)\n        return shortcuts.redirect(next_url or workflow.get_success_url())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_angular_template_hash(sender, **kwargs):\n    context = kwargs['context']  # context the compressor is working with\n    compressed = context['compressed']  # the compressed content\n    compressed_name = compressed['name']  # name of the compressed content\n    if compressed_name == 'angular_template_cache_preloads':\n        # The compressor has modified the angular template cache preloads\n        # which are cached in the 'default' Django cache. Fetch that cache.\n        cache = caches['default']\n\n        # generate the same key as used in _scripts.html when caching the\n        # preloads\n        theme = context['THEME']  # current theme being compressed\n        key = make_template_fragment_key(\n            \"angular\",\n            ['template_cache_preloads', theme]\n        )\n\n        # if template preloads have been cached, clear them\n        if cache.get(key):\n            cache.delete(key)", "response": "Listen for compress events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a dictionary of template contents for all static HTML templates.", "response": "def angular_templates(context):\n    \"\"\"Generate a dictionary of template contents for all static HTML templates.\n\n    If the template has been overridden by a theme, load the\n    override contents instead of the original HTML file.\n    One use for this is to pre-populate the angular template cache.\n\n    Args:\n        context: the context of the current Django template\n\n    Returns: an object containing\n     angular_templates: dictionary of angular template contents\n      - key is the template's static path,\n      - value is a string of HTML template contents\n    \"\"\"\n    template_paths = context['HORIZON_CONFIG']['external_templates']\n    all_theme_static_files = context['HORIZON_CONFIG']['theme_static_files']\n    this_theme_static_files = all_theme_static_files[context['THEME']]\n    template_overrides = this_theme_static_files['template_overrides']\n    angular_templates = {}\n\n    for relative_path in template_paths:\n\n        template_static_path = context['STATIC_URL'] + relative_path\n\n        # If the current theme overrides this template, use the theme\n        # content instead of the original file content\n        if relative_path in template_overrides:\n            relative_path = template_overrides[relative_path]\n\n        result = []\n        for finder in finders.get_finders():\n            result.extend(finder.find(relative_path, True))\n        path = result[-1]\n        try:\n            if six.PY3:\n                with open(path, encoding='utf-8') as template_file:\n                    angular_templates[template_static_path] = template_file.\\\n                        read()\n            else:\n                with open(path) as template_file:\n                    angular_templates[template_static_path] = template_file.\\\n                        read()\n        except (OSError, IOError):\n            # Failed to read template, leave the template dictionary blank\n            # If the caller is using this dictionary to pre-populate a cache\n            # there will simply be no pre-loaded version for this template.\n            pass\n\n    templates = [(key, value) for key, value in angular_templates.items()]\n    templates.sort(key=lambda item: item[0])\n\n    return {\n        'angular_templates': templates\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_microversion_for_features(service, features, wrapper_class,\n                                  min_ver, max_ver):\n    \"\"\"Retrieves that highest known functional microversion for features\"\"\"\n    feature_versions = get_requested_versions(service, features)\n    if not feature_versions:\n        return None\n\n    for version in feature_versions:\n        microversion = wrapper_class(version)\n        if microversion.matches(min_ver, max_ver):\n            return microversion\n    return None", "response": "Retrieves that highest known functional microversion for features"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrapped_include(arg):\n    pattern_list, app_namespace, instance_namespace = arg\n    return include((pattern_list, app_namespace), namespace=instance_namespace)", "response": "Wrap the arg for include in a new format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _register(self, cls):\n        if not inspect.isclass(cls):\n            raise ValueError('Only classes may be registered.')\n        elif not issubclass(cls, self._registerable_class):\n            raise ValueError('Only %s classes or subclasses may be registered.'\n                             % self._registerable_class.__name__)\n\n        if cls not in self._registry:\n            cls._registered_with = self\n            self._registry[cls] = cls()\n\n        return self._registry[cls]", "response": "Registers the given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _unregister(self, cls):\n        if not issubclass(cls, self._registerable_class):\n            raise ValueError('Only %s classes or subclasses may be '\n                             'unregistered.' % self._registerable_class)\n\n        if cls not in self._registry:\n            raise NotRegistered('%s is not registered' % cls)\n\n        del self._registry[cls]\n\n        return True", "response": "Unregisters the given class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_absolute_url(self):\n        try:\n            return reverse('horizon:%s:%s:%s' % (self._registered_with.slug,\n                                                 self.slug,\n                                                 self.index_url_name))\n        except Exception as exc:\n            # Logging here since this will often be called in a template\n            # where the exception would be hidden.\n            LOG.info(\"Error reversing absolute URL for %(self)s: %(exc)s\",\n                     {'self': self, 'exc': exc})\n            raise", "response": "Returns the absolute URL for this panel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_panels(self):\n        all_panels = []\n        panel_groups = self.get_panel_groups()\n        for panel_group in panel_groups.values():\n            all_panels.extend(panel_group)\n        return all_panels", "response": "Returns the Panel instances registered with this dashboard in order."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default URL for this dashboard.", "response": "def get_absolute_url(self):\n        \"\"\"Returns the default URL for this dashboard.\n\n        The default URL is defined as the URL pattern with ``name=\"index\"``\n        in the URLconf for the :class:`~horizon.Panel` specified by\n        :attr:`~horizon.Dashboard.default_panel`.\n        \"\"\"\n        try:\n            return self._registered(self.default_panel).get_absolute_url()\n        except Exception:\n            # Logging here since this will often be called in a template\n            # where the exception would be hidden.\n            LOG.exception(\"Error reversing absolute URL for %s.\", self)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _autodiscover(self):\n        if getattr(self, \"_autodiscover_complete\", False):\n            return\n\n        panels_to_discover = []\n        panel_groups = []\n        # If we have a flat iterable of panel names, wrap it again so\n        # we have a consistent structure for the next step.\n        if all([isinstance(i, six.string_types) for i in self.panels]):\n            self.panels = [self.panels]\n\n        # Now iterate our panel sets.\n        default_created = False\n        for panel_set in self.panels:\n            # Instantiate PanelGroup classes.\n            if not isinstance(panel_set, collections.Iterable) and \\\n                    issubclass(panel_set, PanelGroup):\n                panel_group = panel_set(self)\n            # Check for nested tuples, and convert them to PanelGroups\n            elif not isinstance(panel_set, PanelGroup):\n                panel_group = PanelGroup(self, panels=panel_set)\n\n            # Put our results into their appropriate places\n            panels_to_discover.extend(panel_group.panels)\n            panel_groups.append((panel_group.slug, panel_group))\n            if panel_group.slug == DEFAULT_PANEL_GROUP:\n                default_created = True\n\n        # Plugin panels can be added to a default panel group. Make sure such a\n        # default group exists.\n        if not default_created:\n            default_group = PanelGroup(self)\n            panel_groups.insert(0, (default_group.slug, default_group))\n        self._panel_groups = collections.OrderedDict(panel_groups)\n\n        # Do the actual discovery\n        package = '.'.join(self.__module__.split('.')[:-1])\n        mod = import_module(package)\n        for panel in panels_to_discover:\n            try:\n                before_import_registry = copy.copy(self._registry)\n                import_module('.%s.panel' % panel, package)\n            except Exception:\n                self._registry = before_import_registry\n                if module_has_submodule(mod, panel):\n                    raise\n        self._autodiscover_complete = True", "response": "Automatically discover panels from the current dashboard module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register(cls, panel):\n        panel_class = Horizon.register_panel(cls, panel)\n        # Support template loading from panel template directories.\n        panel_mod = import_module(panel.__module__)\n        panel_dir = os.path.dirname(panel_mod.__file__)\n        template_dir = os.path.join(panel_dir, \"templates\")\n        if os.path.exists(template_dir):\n            key = os.path.join(cls.slug, panel.slug)\n            loaders.panel_template_dirs[key] = template_dir\n        return panel_class", "response": "Registers a : class ~horizon. Panel with this dashboard."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unregister(cls, panel):\n        success = Horizon.unregister_panel(cls, panel)\n        if success:\n            # Remove the panel's template directory.\n            key = os.path.join(cls.slug, panel.slug)\n            if key in loaders.panel_template_dirs:\n                del loaders.panel_template_dirs[key]\n        return success", "response": "Unregisters a : class : ~horizon. Panel from this dashboard."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the user is allowed to access this dashboard. Returns True if the user can access this dashboard False otherwise.", "response": "def allowed(self, context):\n        \"\"\"Checks for role based access for this dashboard.\n\n        Checks for access to any panels in the dashboard and of the\n        dashboard itself.\n\n        This method should be overridden to return the result of\n        any policy checks required for the user to access this dashboard\n        when more complex checks are required.\n        \"\"\"\n\n        # if the dashboard has policy rules, honor those above individual\n        # panels\n        if not self._can_access(context['request']):\n            return False\n\n        # check if access is allowed to a single panel,\n        # the default for each panel is True\n        for panel in self.get_panels():\n            if panel.can_access(context):\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an ordered tuple of dashboards according to the HORIZON_CONFIG.", "response": "def get_dashboards(self):\n        \"\"\"Returns an ordered tuple of :class:`~horizon.Dashboard` modules.\n\n        Orders dashboards according to the ``\"dashboards\"`` key in\n        ``HORIZON_CONFIG`` or else returns all registered dashboards\n        in alphabetical order.\n\n        Any remaining :class:`~horizon.Dashboard` classes registered with\n        Horizon but not listed in ``HORIZON_CONFIG['dashboards']``\n        will be appended to the end of the list alphabetically.\n        \"\"\"\n        if self.dashboards:\n            registered = copy.copy(self._registry)\n            dashboards = []\n            for item in self.dashboards:\n                dashboard = self._registered(item)\n                dashboards.append(dashboard)\n                registered.pop(dashboard.__class__)\n            if registered:\n                extra = sorted(registered.values())\n                dashboards.extend(extra)\n            return dashboards\n        else:\n            return sorted(self._registry.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_dashboard(self):\n        if self.default_dashboard:\n            return self._registered(self.default_dashboard)\n        elif self._registry:\n            return self.get_dashboards()[0]\n        else:\n            raise NotRegistered(\"No dashboard modules have been registered.\")", "response": "Returns the default dashboard instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default URL for a particular user.", "response": "def get_user_home(self, user):\n        \"\"\"Returns the default URL for a particular user.\n\n        This method can be used to customize where a user is sent when\n        they log in, etc. By default it returns the value of\n        :meth:`get_absolute_url`.\n\n        An alternative function can be supplied to customize this behavior\n        by specifying a either a URL or a function which returns a URL via\n        the ``\"user_home\"`` key in ``HORIZON_CONFIG``. Each of these\n        would be valid::\n\n            {\"user_home\": \"/home\",}  # A URL\n            {\"user_home\": \"my_module.get_user_home\",}  # Path to a function\n            {\"user_home\": lambda user: \"/\" + user.name,}  # A function\n            {\"user_home\": None,}  # Will always return the default dashboard\n\n        This can be useful if the default dashboard may not be accessible\n        to all users. When user_home is missing from HORIZON_CONFIG,\n        it will default to the settings.LOGIN_REDIRECT_URL value.\n        \"\"\"\n        user_home = self._conf['user_home']\n        if user_home:\n            if callable(user_home):\n                return user_home(user)\n            elif isinstance(user_home, six.string_types):\n                # Assume we've got a URL if there's a slash in it\n                if '/' in user_home:\n                    return user_home\n                else:\n                    mod, func = user_home.rsplit(\".\", 1)\n                    return getattr(import_module(mod), func)(user)\n            # If it's not callable and not a string, it's wrong.\n            raise ValueError('The user_home setting must be either a string '\n                             'or a callable object (e.g. a function).')\n        else:\n            return self.get_absolute_url()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _lazy_urls(self):\n        def url_patterns():\n            return self._urls()[0]\n\n        return LazyURLPattern(url_patterns), self.namespace, self.slug", "response": "Lazy loading for URL patterns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct the URLconf for Horizon from registered Dashboards.", "response": "def _urls(self):\n        \"\"\"Constructs the URLconf for Horizon from registered Dashboards.\"\"\"\n        urlpatterns = self._get_default_urlpatterns()\n        self._autodiscover()\n\n        # Discover each dashboard's panels.\n        for dash in self._registry.values():\n            dash._autodiscover()\n\n        # Load the plugin-based panel configuration\n        self._load_panel_customization()\n\n        # Allow for override modules\n        if self._conf.get(\"customization_module\", None):\n            customization_module = self._conf[\"customization_module\"]\n            bits = customization_module.split('.')\n            mod_name = bits.pop()\n            package = '.'.join(bits)\n            mod = import_module(package)\n            try:\n                before_import_registry = copy.copy(self._registry)\n                import_module('%s.%s' % (package, mod_name))\n            except Exception:\n                self._registry = before_import_registry\n                if module_has_submodule(mod, mod_name):\n                    raise\n\n        # Compile the dynamic urlconf.\n        for dash in self._registry.values():\n            urlpatterns.append(url(r'^%s/' % dash.slug,\n                                   _wrapped_include(dash._decorated_urls)))\n\n        # Return the three arguments to django.conf.urls.include\n        return urlpatterns, self.namespace, self.slug"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndiscovering modules to register from settings. INSTALLED_APPS.", "response": "def _autodiscover(self):\n        \"\"\"Discovers modules to register from ``settings.INSTALLED_APPS``.\n\n        This makes sure that the appropriate modules get imported to register\n        themselves with Horizon.\n        \"\"\"\n        if not getattr(self, '_registerable_class', None):\n            raise ImproperlyConfigured('You must set a '\n                                       '\"_registerable_class\" property '\n                                       'in order to use autodiscovery.')\n        # Discover both dashboards and panels, in that order\n        for mod_name in ('dashboard', 'panel'):\n            for app in settings.INSTALLED_APPS:\n                mod = import_module(app)\n                try:\n                    before_import_registry = copy.copy(self._registry)\n                    import_module('%s.%s' % (app, mod_name))\n                except Exception:\n                    self._registry = before_import_registry\n                    if module_has_submodule(mod, mod_name):\n                        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_panel_customization(self):\n        panel_customization = self._conf.get(\"panel_customization\", [])\n\n        # Process all the panel groups first so that they exist before panels\n        # are added to them and Dashboard._autodiscover() doesn't wipe out any\n        # panels previously added when its panel groups are instantiated.\n        panel_configs = []\n        for config in panel_customization:\n            if config.get('PANEL'):\n                panel_configs.append(config)\n            elif config.get('PANEL_GROUP'):\n                self._process_panel_group_configuration(config)\n            else:\n                LOG.warning(\"Skipping %s because it doesn't have PANEL or \"\n                            \"PANEL_GROUP defined.\", config.__name__)\n        # Now process the panels.\n        for config in panel_configs:\n            self._process_panel_configuration(config)", "response": "Applies the plugin - based panel configurations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_panel_configuration(self, config):\n        try:\n            dashboard = config.get('PANEL_DASHBOARD')\n            if not dashboard:\n                LOG.warning(\"Skipping %s because it doesn't have \"\n                            \"PANEL_DASHBOARD defined.\", config.__name__)\n                return\n            panel_slug = config.get('PANEL')\n            dashboard_cls = self.get_dashboard(dashboard)\n            panel_group = config.get('PANEL_GROUP')\n            default_panel = config.get('DEFAULT_PANEL')\n\n            # Set the default panel\n            if default_panel:\n                dashboard_cls.default_panel = default_panel\n\n            # Remove the panel\n            if config.get('REMOVE_PANEL', False):\n                for panel in dashboard_cls.get_panels():\n                    if panel_slug == panel.slug:\n                        dashboard_cls.unregister(panel.__class__)\n            elif config.get('ADD_PANEL', None):\n                # Add the panel to the dashboard\n                panel_path = config['ADD_PANEL']\n                mod_path, panel_cls = panel_path.rsplit(\".\", 1)\n                try:\n                    mod = import_module(mod_path)\n                except ImportError as e:\n                    LOG.warning(\"Could not import panel module %(module)s: \"\n                                \"%(exc)s\", {'module': mod_path, 'exc': e})\n                    return\n                panel = getattr(mod, panel_cls)\n                # test is can_register method is present and call method if\n                # it is to determine if the panel should be loaded\n                if hasattr(panel, 'can_register') and \\\n                   callable(getattr(panel, 'can_register')):\n                    if not panel.can_register():\n                        LOG.debug(\"Load condition failed for panel: %(panel)s\",\n                                  {'panel': panel_slug})\n                        return\n                dashboard_cls.register(panel)\n                if panel_group:\n                    dashboard_cls.get_panel_group(panel_group).\\\n                        panels.append(panel.slug)\n                else:\n                    panels = list(dashboard_cls.panels)\n                    panels.append(panel)\n                    dashboard_cls.panels = tuple(panels)\n        except Exception as e:\n            LOG.warning('Could not process panel %(panel)s: %(exc)s',\n                        {'panel': panel_slug, 'exc': e})", "response": "Process the panel configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a panel group to the dashboard.", "response": "def _process_panel_group_configuration(self, config):\n        \"\"\"Adds a panel group to the dashboard.\"\"\"\n        panel_group_slug = config.get('PANEL_GROUP')\n        try:\n            dashboard = config.get('PANEL_GROUP_DASHBOARD')\n            if not dashboard:\n                LOG.warning(\"Skipping %s because it doesn't have \"\n                            \"PANEL_GROUP_DASHBOARD defined.\", config.__name__)\n                return\n            dashboard_cls = self.get_dashboard(dashboard)\n\n            panel_group_name = config.get('PANEL_GROUP_NAME')\n            if not panel_group_name:\n                LOG.warning(\"Skipping %s because it doesn't have \"\n                            \"PANEL_GROUP_NAME defined.\", config.__name__)\n                return\n            # Create the panel group class\n            panel_group = type(panel_group_slug,\n                               (PanelGroup, ),\n                               {'slug': panel_group_slug,\n                                'name': panel_group_name,\n                                'panels': []},)\n            # Add the panel group to dashboard\n            panels = list(dashboard_cls.panels)\n            panels.append(panel_group)\n            dashboard_cls.panels = tuple(panels)\n            # Trigger the autodiscovery to completely load the new panel group\n            dashboard_cls._autodiscover_complete = False\n            dashboard_cls._autodiscover()\n        except Exception as e:\n            LOG.warning('Could not process panel group %(panel_group)s: '\n                        '%(exc)s',\n                        {'panel_group': panel_group_slug, 'exc': e})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_cells(self, datum=None):\n        # Compile all the cells on instantiation.\n        table = self.table\n        if datum:\n            self.datum = datum\n        else:\n            datum = self.datum\n        cells = []\n        for column in table.columns.values():\n            cell = table._meta.cell_class(datum, column, self)\n            cells.append((column.name or column.auto, cell))\n        self.cells = collections.OrderedDict(cells)\n\n        if self.ajax:\n            interval = conf.HORIZON_CONFIG['ajax_poll_interval']\n            self.attrs['data-update-interval'] = interval\n            self.attrs['data-update-url'] = self.get_ajax_update_url()\n            self.classes.append(\"ajax-update\")\n\n        self.attrs['data-object-id'] = table.get_object_id(datum)\n\n        # Add the row's status class and id to the attributes to be rendered.\n        self.classes.append(self.status_class)\n        id_vals = {\"table\": self.table.name,\n                   \"sep\": STRING_SEPARATOR,\n                   \"id\": table.get_object_id(datum)}\n        self.id = \"%(table)s%(sep)srow%(sep)s%(id)s\" % id_vals\n        self.attrs['id'] = self.id\n\n        # Add the row's display name if available\n        display_name = table.get_object_display(datum)\n        display_name_key = table.get_object_display_key(datum)\n\n        if display_name:\n            self.attrs['data-display'] = escape(display_name)\n            self.attrs['data-display-key'] = escape(display_name_key)", "response": "Load the row s data and initialize all the cells in the row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch the data to be displayed in this cell.", "response": "def get_data(self, datum, column, row):\n        \"\"\"Fetches the data to be displayed in this cell.\"\"\"\n        table = row.table\n        if column.auto == \"multi_select\":\n            data = \"\"\n            if row.can_be_selected(datum):\n                widget = ThemableCheckboxInput(check_test=lambda value: False)\n                # Convert value to string to avoid accidental type conversion\n                data = widget.render('object_ids',\n                                     six.text_type(table.get_object_id(datum)),\n                                     {'class': 'table-row-multi-select'})\n            table._data_cache[column][table.get_object_id(datum)] = data\n        elif column.auto == \"form_field\":\n            widget = column.form_field\n            if issubclass(widget.__class__, forms.Field):\n                widget = widget.widget\n\n            widget_name = \"%s__%s\" % \\\n                (column.name,\n                 six.text_type(table.get_object_id(datum)))\n\n            # Create local copy of attributes, so it don't change column\n            # class form_field_attributes\n            form_field_attributes = {}\n            form_field_attributes.update(column.form_field_attributes)\n            # Adding id of the input so it pairs with label correctly\n            form_field_attributes['id'] = widget_name\n\n            if (template.defaultfilters.urlize in column.filters or\n                    template.defaultfilters.yesno in column.filters):\n                data = widget.render(widget_name,\n                                     column.get_raw_data(datum),\n                                     form_field_attributes)\n            else:\n                data = widget.render(widget_name,\n                                     column.get_data(datum),\n                                     form_field_attributes)\n            table._data_cache[column][table.get_object_id(datum)] = data\n        elif column.auto == \"actions\":\n            data = table.render_row_actions(datum)\n            table._data_cache[column][table.get_object_id(datum)] = data\n        else:\n            data = column.get_data(datum)\n            if column.cell_attributes_getter:\n                cell_attributes = column.cell_attributes_getter(data) or {}\n                self.attrs.update(cell_attributes)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef value(self):\n        try:\n            data = self.column.get_data(self.datum)\n            if data is None:\n                if callable(self.column.empty_value):\n                    data = self.column.empty_value(self.datum)\n                else:\n                    data = self.column.empty_value\n        except Exception:\n            data = None\n            exc_info = sys.exc_info()\n            raise six.reraise(template.TemplateSyntaxError, exc_info[1],\n                              exc_info[2])\n\n        if self.url and not self.column.auto == \"form_field\":\n            link_attrs = ' '.join(['%s=\"%s\"' % (k, v) for (k, v) in\n                                  self.column.link_attrs.items()])\n            # Escape the data inside while allowing our HTML to render\n            data = mark_safe('<a href=\"%s\" %s>%s</a>' % (\n                             (escape(self.url),\n                              link_attrs,\n                              escape(six.text_type(data)))))\n        return data", "response": "Returns a formatted version of the data for final output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the status for the column based on the data.", "response": "def status(self):\n        \"\"\"Gets the status for the column based on the cell's data.\"\"\"\n        # Deal with status column mechanics based in this cell's data\n        if hasattr(self, '_status'):\n            # pylint: disable=access-member-before-definition\n            return self._status\n\n        if self.column.status or \\\n                self.column.name in self.column.table._meta.status_columns:\n            # returns the first matching status found\n            data_status_lower = six.text_type(\n                self.column.get_raw_data(self.datum)).lower()\n            for status_name, status_value in self.column.status_choices:\n                if six.text_type(status_name).lower() == data_status_lower:\n                    self._status = status_value\n                    return self._status\n        self._status = None\n        return self._status"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_classes(self):\n        if not self.url:\n            self.column.classes = [cls for cls in self.column.classes\n                                   if cls != \"anchor\"]\n        column_class_string = self.column.get_final_attrs().get('class', \"\")\n        classes = set(column_class_string.split(\" \"))\n        if self.column.status:\n            classes.add(self.get_status_class(self.status))\n\n        if self.inline_edit_available:\n            classes.add(\"inline_edit_available\")\n\n        return list(classes)", "response": "Returns a flattened string of the cell s CSS classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines whether update of given cell is allowed.", "response": "def update_allowed(self):\n        \"\"\"Determines whether update of given cell is allowed.\n\n        Calls allowed action of defined UpdateAction of the Column.\n        \"\"\"\n        return self.update_action.allowed(self.column.table.request,\n                                          self.datum,\n                                          self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter(self, table, volumes, filter_string):\n        q = filter_string.lower()\n        return [volume for volume in volumes\n                if q in volume.name.lower()]", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a tuple of console url and console type.", "response": "def get_console(request, console_type, instance):\n    \"\"\"Get a tuple of console url and console type.\"\"\"\n    if console_type == 'AUTO':\n        check_consoles = CONSOLES\n    else:\n        try:\n            check_consoles = {console_type: CONSOLES[console_type]}\n        except KeyError:\n            msg = _('Console type \"%s\" not supported.') % console_type\n            raise exceptions.NotAvailable(msg)\n\n    # Ugly workaround due novaclient API change from 2.17 to 2.18.\n    try:\n        httpnotimplemented = nova_exception.HttpNotImplemented\n    except AttributeError:\n        httpnotimplemented = nova_exception.HTTPNotImplemented\n\n    for con_type, api_call in check_consoles.items():\n        try:\n            console = api_call(request, instance.id)\n        # If not supported, don't log it to avoid lot of errors in case\n        # of AUTO.\n        except httpnotimplemented:\n            continue\n        except Exception:\n            LOG.debug('Console not available', exc_info=True)\n            continue\n\n        if con_type == 'SERIAL':\n            console_url = console.url\n        else:\n            console_url = \"%s&%s(%s)\" % (\n                          console.url,\n                          urlencode({'title': getattr(instance, \"name\", \"\")}),\n                          instance.id)\n\n        return (con_type, console_url)\n\n    raise exceptions.NotAvailable(_('No available console found.'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck to make sure password fields match.", "response": "def clean(self):\n        '''Check to make sure password fields match.'''\n        data = super(PasswordMixin, self).clean()\n        if 'password' in data and 'confirm_password' in data:\n            if data['password'] != data['confirm_password']:\n                raise ValidationError(_('Passwords do not match.'))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking to make sure password fields match.", "response": "def clean(self):\n        '''Check to make sure password fields match.'''\n        cleaned_data = super(SetAccessControlsAction, self).clean()\n        if 'admin_pass' in cleaned_data:\n            if cleaned_data['admin_pass'] != cleaned_data.get(\n                    'confirm_admin_pass', None):\n                raise forms.ValidationError(_('Passwords do not match.'))\n        return cleaned_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_exception(self, request, exception):\n        log_format = self._get_log_format(request)\n        if log_format is None:\n            return\n\n        params = self._get_parameters_from_request(request, True)\n        params['message'] = exception\n        params['http_status'] = '-'\n\n        self.OPERATION_LOG.info(log_format, params)", "response": "Log error info when exception occurred."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_log_format(self, request):\n        user = getattr(request, 'user', None)\n        if not user:\n            return\n        if not request.user.is_authenticated:\n            return\n        method = request.method.upper()\n        if not (method in self.target_methods):\n            return\n        request_url = urlparse.unquote(request.path)\n        for rule in self._ignored_urls:\n            if rule.search(request_url):\n                return\n        return self.format", "response": "Return operation log format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting parameters to log in OPERATION_LOG.", "response": "def _get_parameters_from_request(self, request, exception=False):\n        \"\"\"Get parameters to log in OPERATION_LOG.\"\"\"\n        user = request.user\n        referer_url = None\n        try:\n            referer_dic = urlparse.urlsplit(\n                urlparse.unquote(request.META.get('HTTP_REFERER')))\n            referer_url = referer_dic[2]\n            if referer_dic[3]:\n                referer_url += \"?\" + referer_dic[3]\n            if isinstance(referer_url, str):\n                referer_url = referer_url.decode('utf-8')\n        except Exception:\n            pass\n        request_url = urlparse.unquote(request.path)\n        if request.META['QUERY_STRING']:\n            request_url += '?' + request.META['QUERY_STRING']\n        return {\n            'client_ip': request.META.get('REMOTE_ADDR', None),\n            'domain_name': getattr(user, 'domain_name', None),\n            'domain_id': getattr(user, 'domain_id', None),\n            'project_name': getattr(user, 'project_name', None),\n            'project_id': getattr(user, 'project_id', None),\n            'user_name': getattr(user, 'username', None),\n            'user_id': request.session.get('user_id', None),\n            'request_scheme': request.scheme,\n            'referer_url': referer_url,\n            'request_url': request_url,\n            'method': request.method if not exception else None,\n            'param': self._get_request_param(request),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging POST data to JSON string and mask data.", "response": "def _get_request_param(self, request):\n        \"\"\"Change POST data to JSON string and mask data.\"\"\"\n        params = {}\n        try:\n            params = request.POST.copy()\n            if not params:\n                params = json.loads(request.body)\n        except Exception:\n            pass\n        for key in params:\n            # replace a value to a masked characters\n            if key in self.mask_fields:\n                params[key] = '*' * 8\n\n        # when a file uploaded (E.g create image)\n        files = request.FILES.values()\n        if list(files):\n            filenames = ', '.join(\n                [up_file.name for up_file in files])\n            params['file_name'] = filenames\n\n        try:\n            return json.dumps(params, ensure_ascii=False)\n        except Exception:\n            return 'Unserializable Object'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists names of columns that have required fields.", "response": "def get_required_columns(self):\n        \"\"\"Lists names of columns that have required fields.\"\"\"\n        required_columns = []\n        if self.formset_class:\n            empty_form = self.get_formset().empty_form\n            for column in self.columns.values():\n                field = empty_form.fields.get(column.name)\n                if field and field.required:\n                    required_columns.append(column.name)\n        return required_columns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_formset_data(self):\n        data = []\n        for datum in self.filtered_data:\n            form_data = {}\n            for column in self.columns.values():\n                value = column.get_data(datum)\n                form_data[column.name] = value\n            form_data['id'] = self.get_object_id(datum)\n            data.append(form_data)\n        return data", "response": "Formats the self. filtered_data in a way suitable for a formset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_formset(self):\n        if self._formset is None:\n            self._formset = self.formset_class(\n                self.request.POST or None,\n                initial=self._get_formset_data(),\n                prefix=self._meta.name)\n        return self._formset", "response": "Provide the formset corresponding to this DataTable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the rows for this table broken out by columns.", "response": "def get_rows(self):\n        \"\"\"Return the row data for this table broken out by columns.\n\n        The row objects get an additional ``form`` parameter, with the\n        formset form corresponding to that row.\n        \"\"\"\n        try:\n            rows = []\n            if self.formset_class is None:\n                formset = []\n            else:\n                formset = self.get_formset()\n                formset.is_valid()\n            for datum, form in six.moves.zip_longest(self.filtered_data,\n                                                     formset):\n                row = self._meta.row_class(self, datum, form)\n                if self.get_object_id(datum) == self.current_item_id:\n                    self.selected = True\n                    row.classes.append('current_selected')\n                rows.append(row)\n        except Exception:\n            # Exceptions can be swallowed at the template level here,\n            # re-raising as a TemplateSyntaxError makes them visible.\n            LOG.exception(\"Error while rendering table rows.\")\n            exc_info = sys.exc_info()\n            raise six.reraise(template.TemplateSyntaxError, exc_info[1],\n                              exc_info[2])\n        return rows"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(self, table, mappings, filter_string):\n        q = filter_string.lower()\n        return [mapping for mapping in mappings\n                if q in mapping.ud.lower()]", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new client connected to Keystone backend.", "response": "def keystoneclient(request, admin=False):\n    \"\"\"Returns a client connected to the Keystone backend.\n\n    Several forms of authentication are supported:\n\n        * Username + password -> Unscoped authentication\n        * Username + password + tenant id -> Scoped authentication\n        * Unscoped token -> Unscoped authentication\n        * Unscoped token + tenant id -> Scoped authentication\n        * Scoped token -> Scoped authentication\n\n    Available services and data from the backend will vary depending on\n    whether the authentication was scoped or unscoped.\n\n    Lazy authentication if an ``endpoint`` parameter is provided.\n\n    Calls requiring the admin endpoint should have ``admin=True`` passed in\n    as a keyword argument.\n\n    The client is cached so that subsequent API calls during the same\n    request/response cycle don't have to be re-authenticated.\n    \"\"\"\n    client_version = VERSIONS.get_active_version()\n    user = request.user\n    token_id = user.token.id\n\n    if is_multi_domain_enabled():\n        # Cloud Admin, Domain Admin or Mixed Domain Admin\n        if is_domain_admin(request):\n            domain_token = request.session.get('domain_token')\n            if domain_token:\n                token_id = getattr(domain_token, 'auth_token', None)\n\n    if admin:\n        if not policy.check(((\"identity\", \"admin_required\"),), request):\n            raise exceptions.NotAuthorized\n        endpoint_type = 'adminURL'\n    else:\n        endpoint_type = getattr(settings,\n                                'OPENSTACK_ENDPOINT_TYPE',\n                                'publicURL')\n\n    # Take care of client connection caching/fetching a new client.\n    # Admin vs. non-admin clients are cached separately for token matching.\n    cache_attr = \"_keystoneclient_admin\" if admin \\\n        else backend.KEYSTONE_CLIENT_ATTR\n    if (hasattr(request, cache_attr) and\n        (not user.token.id or\n         getattr(request, cache_attr).auth_token == user.token.id)):\n        conn = getattr(request, cache_attr)\n    else:\n        endpoint = _get_endpoint_url(request, endpoint_type)\n        verify = not getattr(settings, 'OPENSTACK_SSL_NO_VERIFY', False)\n        cacert = getattr(settings, 'OPENSTACK_SSL_CACERT', None)\n        verify = verify and cacert\n        LOG.debug(\"Creating a new keystoneclient connection to %s.\", endpoint)\n        remote_addr = request.environ.get('REMOTE_ADDR', '')\n        token_auth = token_endpoint.Token(endpoint=endpoint,\n                                          token=token_id)\n        keystone_session = session.Session(auth=token_auth,\n                                           original_ip=remote_addr,\n                                           verify=verify)\n        conn = client_version['client'].Client(session=keystone_session,\n                                               debug=settings.DEBUG)\n        setattr(request, cache_attr, conn)\n    return conn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the default domain object to use when creating Identity object.", "response": "def get_default_domain(request, get_name=True):\n    \"\"\"Gets the default domain object to use when creating Identity object.\n\n    Returns the domain context if is set, otherwise return the domain\n    of the logon user.\n\n    :param get_name: Whether to get the domain name from Keystone if the\n        context isn't set.  Setting this to False prevents an unnecessary call\n        to Keystone if only the domain ID is needed.\n    \"\"\"\n    domain_id = request.session.get(\"domain_context\", None)\n    domain_name = request.session.get(\"domain_context_name\", None)\n    # if running in Keystone V3 or later\n    if VERSIONS.active >= 3 and domain_id is None:\n        # if no domain context set, default to user's domain\n        domain_id = request.user.user_domain_id\n        domain_name = request.user.user_domain_name\n        if get_name and not request.user.is_federated:\n            try:\n                domain = domain_get(request, domain_id)\n                domain_name = domain.name\n            except exceptions.NotAuthorized:\n                # NOTE (knasim-wrs): Retrieving domain information\n                # is an admin URL operation. As a pre-check, such\n                # operations would be Forbidden if the logon user does\n                # not have an 'admin' role on the current project.\n                #\n                # Since this can be a common occurence and can cause\n                # incessant warning logging in the horizon logs,\n                # we recognize this condition and return the user's\n                # domain information instead.\n                LOG.debug(\"Cannot retrieve domain information for \"\n                          \"user (%(user)s) that does not have an admin role \"\n                          \"on project (%(project)s)\",\n                          {'user': request.user.username,\n                           'project': request.user.project_name})\n            except Exception:\n                LOG.warning(\"Unable to retrieve Domain: %s\", domain_id)\n    domain = base.APIDictWrapper({\"id\": domain_id,\n                                  \"name\": domain_name})\n    return domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the id of the default domain.", "response": "def get_effective_domain_id(request):\n    \"\"\"Gets the id of the default domain.\n\n    If the requests default domain is the same as DEFAULT_DOMAIN,\n    return None.\n    \"\"\"\n    default_domain = get_default_domain(request)\n    domain_id = default_domain.get('id')\n    domain_name = default_domain.get('name')\n    return None if domain_name == DEFAULT_DOMAIN else domain_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the groups roles in a given project.", "response": "def get_project_groups_roles(request, project):\n    \"\"\"Gets the groups roles in a given project.\n\n    :param request: the request entity containing the login user information\n    :param project: the project to filter the groups roles. It accepts both\n                    project object resource or project ID\n\n    :returns group_roles: a dictionary mapping the groups and their roles in\n                          given project\n\n    \"\"\"\n    groups_roles = collections.defaultdict(list)\n    project_role_assignments = role_assignments_list(request,\n                                                     project=project)\n\n    for role_assignment in project_role_assignments:\n        if not hasattr(role_assignment, 'group'):\n            continue\n        group_id = role_assignment.group['id']\n        role_id = role_assignment.role['id']\n\n        # filter by project_id\n        if ('project' in role_assignment.scope and\n                role_assignment.scope['project']['id'] == project):\n            groups_roles[group_id].append(role_id)\n    return groups_roles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a global list of available roles.", "response": "def role_list(request, filters=None):\n    \"\"\"Returns a global list of available roles.\"\"\"\n    manager = keystoneclient(request, admin=True).roles\n    roles = []\n    kwargs = {}\n    if filters is not None:\n        kwargs.update(filters)\n    if 'id' in kwargs:\n        try:\n            roles = [manager.get(kwargs['id'])]\n        except keystone_exceptions.NotFound:\n            roles = []\n        except Exception:\n            exceptions.handle(request)\n    else:\n        roles = manager.list(**kwargs)\n    return roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of user roles scoped to a project or domain.", "response": "def roles_for_user(request, user, project=None, domain=None):\n    \"\"\"Returns a list of user roles scoped to a project or domain.\"\"\"\n    manager = keystoneclient(request, admin=True).roles\n    if VERSIONS.active < 3:\n        return manager.roles_for_user(user, project)\n    else:\n        return manager.list(user=user, domain=domain, project=project)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_domain_user_role(request, user, role, domain):\n    manager = keystoneclient(request, admin=True).roles\n    return manager.grant(role, user=user, domain=domain)", "response": "Adds a role for a user on a domain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_domain_user_role(request, user, role, domain=None):\n    manager = keystoneclient(request, admin=True).roles\n    return manager.revoke(role, user=user, domain=domain)", "response": "Removes a given single role for a given user from a domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a role for a user on a tenant.", "response": "def add_tenant_user_role(request, project=None, user=None, role=None,\n                         group=None, domain=None):\n    \"\"\"Adds a role for a user on a tenant.\"\"\"\n    manager = keystoneclient(request, admin=True).roles\n    if VERSIONS.active < 3:\n        manager.add_user_role(user, role, project)\n    else:\n        manager.grant(role, user=user, project=project,\n                      group=group, domain=domain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_tenant_user_role(request, project=None, user=None, role=None,\n                            group=None, domain=None):\n    \"\"\"Removes a given single role for a user from a tenant.\"\"\"\n    manager = keystoneclient(request, admin=True).roles\n    if VERSIONS.active < 3:\n        return manager.remove_user_role(user, role, project)\n    else:\n        return manager.revoke(role, user=user, project=project,\n                              group=group, domain=domain)", "response": "Removes a given single role for a user from a tenant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_tenant_user(request, project=None, user=None, domain=None):\n    client = keystoneclient(request, admin=True)\n    roles = client.roles.roles_for_user(user, project)\n    for role in roles:\n        remove_tenant_user_role(request, user=user, role=role.id,\n                                project=project, domain=domain)", "response": "Removes all roles from a user on a tenant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a role for a group on a domain or project.", "response": "def add_group_role(request, role, group, domain=None, project=None):\n    \"\"\"Adds a role for a group on a domain or project.\"\"\"\n    manager = keystoneclient(request, admin=True).roles\n    return manager.grant(role=role, group=group, domain=domain,\n                         project=project)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_group_role(request, role, group, domain=None, project=None):\n    manager = keystoneclient(request, admin=True).roles\n    return manager.revoke(role=role, group=group, project=project,\n                          domain=domain)", "response": "Removes a given single role for a group from a domain or project."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_group_roles(request, group, domain=None, project=None):\n    client = keystoneclient(request, admin=True)\n    roles = client.roles.list(group=group, domain=domain, project=project)\n    for role in roles:\n        remove_group_role(request, role=role.id, group=group,\n                          domain=domain, project=project)", "response": "Removes all roles from a group on a domain or project."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the default role object from Keystone and saves it as a global global.", "response": "def get_default_role(request):\n    \"\"\"Gets the default role object from Keystone and saves it as a global.\n\n    Since this is configured in settings and should not change from request\n    to request. Supports lookup by name or id.\n    \"\"\"\n    global DEFAULT_ROLE\n    default = getattr(settings, \"OPENSTACK_KEYSTONE_DEFAULT_ROLE\", None)\n    if default and DEFAULT_ROLE is None:\n        try:\n            roles = keystoneclient(request, admin=True).roles.list()\n        except Exception:\n            roles = []\n            exceptions.handle(request)\n        for role in roles:\n            if default in (role.id, role.name):\n                DEFAULT_ROLE = role\n                break\n    return DEFAULT_ROLE"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender the table using the template from the table options.", "response": "def render(self):\n        \"\"\"Renders the table using the template from the table options.\"\"\"\n        breadcrumb_template = template.loader.get_template(self.template)\n        extra_context = {\"breadcrumb\": self}\n        return breadcrumb_template.render(extra_context, self.request)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_tab_data(self):\n        for tab in self._tabs.values():\n            if tab.load and not tab.data_loaded:\n                try:\n                    tab._data = tab.get_context_data(self.request)\n                except Exception:\n                    tab._data = False\n                    exceptions.handle(self.request)", "response": "Preload all data that for the tabs that will be displayed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of the default classes for the tab group.", "response": "def get_default_classes(self):\n        \"\"\"Returns a list of the default classes for the tab group.\n\n        Defaults to ``[\"nav\", \"nav-tabs\", \"ajax-tabs\"]``.\n        \"\"\"\n        default_classes = super(TabGroup, self).get_default_classes()\n        default_classes.extend(CSS_TAB_GROUP_CLASSES)\n        return default_classes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a specific tab from this tab group.", "response": "def get_tab(self, tab_name, allow_disabled=False):\n        \"\"\"Returns a specific tab from this tab group.\n\n        If the tab is not allowed or not enabled this method returns ``None``.\n\n        If the tab is disabled but you wish to return it anyway, you can pass\n        ``True`` to the allow_disabled argument.\n        \"\"\"\n        tab = self._tabs.get(tab_name, None)\n        if tab and tab._allowed and (tab._enabled or allow_disabled):\n            return tab\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_selected_tab(self):\n        selected = self.request.GET.get(self.param_name, None)\n        if selected:\n            try:\n                tab_group, tab_name = selected.split(SEPARATOR)\n            except ValueError:\n                return None\n            if tab_group == self.get_id():\n                self._selected = self.get_tab(tab_name)\n        return self._selected", "response": "Returns the tab specific by the GET request parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders the tab to HTML.", "response": "def render(self):\n        \"\"\"Renders the tab to HTML.\n\n        :meth:`~horizon.tabs.Tab.get_context_data` method and\n        the :meth:`~horizon.tabs.Tab.get_template_name` method are called.\n\n        If :attr:`~horizon.tabs.Tab.preload` is ``False`` and ``force_load``\n        is not ``True``, or\n        either :meth:`~horizon.tabs.Tab.allowed` or\n        :meth:`~horizon.tabs.Tab.enabled` returns ``False`` this method will\n        return an empty string.\n        \"\"\"\n        if not self.load:\n            return ''\n        try:\n            context = self.data\n        except exceptions.Http302:\n            raise\n        except Exception:\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            raise six.reraise(TemplateSyntaxError, exc_value, exc_traceback)\n        return render_to_string(self.get_template_name(self.request), context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_classes(self):\n        default_classes = super(Tab, self).get_default_classes()\n        if self.is_active():\n            default_classes.extend(CSS_ACTIVE_TAB_CLASSES)\n        if not self._enabled:\n            default_classes.extend(CSS_DISABLED_TAB_CLASSES)\n        return default_classes", "response": "Returns a list of the default classes for the tab."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_template_name(self, request):\n        if not hasattr(self, \"template_name\"):\n            raise AttributeError(\"%s must have a template_name attribute or \"\n                                 \"override the get_template_name method.\"\n                                 % self.__class__.__name__)\n        return self.template_name", "response": "Returns the name of the template to be used for rendering this tab."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the get_{{ table_name}_data methods for each table class. When returning the loaded data is set on the tables.", "response": "def load_table_data(self):\n        \"\"\"Calls the ``get_{{ table_name }}_data`` methods for each table class.\n\n        When returning, the loaded data is set on the tables.\n        \"\"\"\n        # We only want the data to be loaded once, so we track if we have...\n        if not self._table_data_loaded:\n            for table_name, table in self._tables.items():\n                # Fetch the data function.\n                func_name = \"get_%s_data\" % table_name\n                data_func = getattr(self, func_name, None)\n                if data_func is None:\n                    cls_name = self.__class__.__name__\n                    raise NotImplementedError(\n                        \"You must define a %(func_name)s method on\"\n                        \" %(cls_name)s.\"\n                        % {'func_name': func_name, 'cls_name': cls_name})\n\n                # Load the data.\n                table.data = data_func()\n                table._meta.has_prev_data = self.has_prev_data(table)\n                table._meta.has_more_data = self.has_more_data(table)\n            # Mark our data as loaded so we don't run the loaders again.\n            self._table_data_loaded = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new entry to the context for each table.", "response": "def get_context_data(self, request, **kwargs):\n        \"\"\"Adds a ``{{ table_name }}_table`` item to the context for each table.\n\n        The target tables are specified by\n        the :attr:`~horizon.tabs.TableTab.table_classes` attribute.\n\n        If only one table class is provided, a shortcut ``table`` context\n        variable is also added containing the single table.\n        \"\"\"\n        context = super(TableTab, self).get_context_data(request, **kwargs)\n        # If the data hasn't been manually loaded before now,\n        # make certain it's loaded before setting the context.\n        self.load_table_data()\n        for table_name, table in self._tables.items():\n            # If there's only one table class, add a shortcut name as well.\n            if len(self.table_classes) == 1:\n                context[\"table\"] = table\n            context[\"%s_table\" % table_name] = table\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_final_attrs(self, classes=True):\n        final_attrs = copy.copy(self.get_default_attrs())\n        final_attrs.update(self.attrs)\n        if classes:\n            final_attrs['class'] = self.get_final_css()\n        else:\n            final_attrs.pop('class', None)\n\n        return final_attrs", "response": "Returns a dict containing the final attributes to be rendered."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a final css class concatenated string.", "response": "def get_final_css(self):\n        \"\"\"Returns a final css class concatenated string.\"\"\"\n        default = \" \".join(self.get_default_classes())\n        defined = self.attrs.get('class', '')\n        additional = \" \".join(getattr(self, \"classes\", []))\n        non_empty = [test for test in (defined, default, additional) if test]\n        final_classes = \" \".join(non_empty).strip()\n        return final_classes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flavor_list(request):\n    try:\n        return api.nova.flavor_list(request)\n    except Exception:\n        exceptions.handle(request,\n                          _('Unable to retrieve instance flavors.'))\n        return []", "response": "Utility method to retrieve a list of flavors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef availability_zone_list(request):\n    try:\n        return api.nova.availability_zone_list(request)\n    except Exception:\n        exceptions.handle(request,\n                          _('Unable to retrieve Nova availability zones.'))\n        return []", "response": "Utility method to retrieve a list of availability zones."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef network_field_data(request, include_empty_option=False, with_cidr=False,\n                       for_launch=False):\n    \"\"\"Returns a list of tuples of all networks.\n\n    Generates a list of networks available to the user (request). And returns\n    a list of (id, name) tuples.\n\n    :param request: django http request object\n    :param include_empty_option: flag to include a empty tuple in the front of\n         the list\n    :param with_cidr: flag to include subnets cidr in field name\n    :return: list of (id, name) tuples\n    \"\"\"\n    tenant_id = request.user.tenant_id\n    networks = []\n    if api.base.is_service_enabled(request, 'network'):\n        extra_params = {}\n        if for_launch:\n            extra_params['include_pre_auto_allocate'] = True\n        try:\n            networks = api.neutron.network_list_for_tenant(\n                request, tenant_id, **extra_params)\n        except Exception as e:\n            msg = _('Failed to get network list {0}').format(six.text_type(e))\n            exceptions.handle(request, msg)\n\n        _networks = []\n        for n in networks:\n            if not n['subnets']:\n                continue\n            v = n.name_or_id\n            if with_cidr:\n                cidrs = ([subnet.cidr for subnet in n['subnets']\n                          if subnet.ip_version == 4] +\n                         [subnet.cidr for subnet in n['subnets']\n                          if subnet.ip_version == 6])\n                v += ' (%s)' % ', '.join(cidrs)\n            _networks.append((n.id, v))\n        networks = sorted(_networks, key=itemgetter(1))\n\n    if not networks:\n        if include_empty_option:\n            return [(\"\", _(\"No networks available\")), ]\n        return []\n\n    if include_empty_option:\n        return [(\"\", _(\"Select Network\")), ] + networks\n    return networks", "response": "Returns a list of tuples of all networks available to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keypair_field_data(request, include_empty_option=False):\n    keypair_list = []\n    try:\n        keypairs = api.nova.keypair_list(request)\n        keypair_list = [(kp.name, kp.name) for kp in keypairs]\n    except Exception:\n        exceptions.handle(request, _('Unable to retrieve key pairs.'))\n\n    if not keypair_list:\n        if include_empty_option:\n            return [(\"\", _(\"No key pairs available\")), ]\n        return []\n\n    if include_empty_option:\n        return [(\"\", _(\"Select a key pair\")), ] + keypair_list\n    return keypair_list", "response": "Returns a list of tuples of all keypairs available to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flavor_field_data(request, include_empty_option=False):\n    flavors = flavor_list(request)\n    if flavors:\n        flavors_list = sort_flavor_list(request, flavors)\n        if include_empty_option:\n            return [(\"\", _(\"Select Flavor\")), ] + flavors_list\n        return flavors_list\n\n    if include_empty_option:\n        return [(\"\", _(\"No flavors available\")), ]\n    return []", "response": "Returns a list of tuples of all image flavors available."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef port_field_data(request, with_network=False):\n\n    def add_more_info_port_name(port, network):\n        # add more info to the port for the display\n        port_name = \"{} ({})\".format(\n            port.name_or_id, \",\".join(\n                [ip['ip_address'] for ip in port['fixed_ips']]))\n        if with_network and network:\n            port_name += \" - {}\".format(network.name_or_id)\n        return port_name\n\n    ports = []\n    if api.base.is_service_enabled(request, 'network'):\n        network_list = api.neutron.network_list_for_tenant(\n            request, request.user.tenant_id)\n        for network in network_list:\n            ports.extend(\n                [(port.id, add_more_info_port_name(port, network))\n                 for port in api.neutron.port_list_with_trunk_types(\n                     request, network_id=network.id,\n                     tenant_id=request.user.tenant_id)\n                 if (not port.device_owner and\n                     not isinstance(port, api.neutron.PortTrunkSubport))])\n    ports.sort(key=lambda obj: obj[1])\n    return ports", "response": "Returns a list of tuples of all ports available for the tenant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a list of tuples of all server groups available. And returns a list of tuples of all server groups.", "response": "def server_group_field_data(request):\n    \"\"\"Returns a list of tuples of all server groups.\n\n    Generates a list of server groups available. And returns a list of\n    (id, name) tuples.\n\n    :param request: django http request object\n    :return: list of (id, name) tuples\n    \"\"\"\n    server_groups = server_group_list(request)\n    if server_groups:\n        server_groups_list = [(sg.id, sg.name) for sg in server_groups]\n        server_groups_list.sort(key=lambda obj: obj[1])\n        return [(\"\", _(\"Select Server Group\")), ] + server_groups_list\n\n    return [(\"\", _(\"No server groups available\")), ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying ExtraRoutes when deleted.", "response": "def get_object_display(self, datum):\n        \"\"\"Display ExtraRoutes when deleted.\"\"\"\n        return (super(ExtraRoutesTable, self).get_object_display(datum) or\n                datum.destination + \" -> \" + datum.nexthop)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the values in a JSON object.", "response": "def iterencode(self, o, _one_shot=False):\n        \"\"\"JSON encoder with NaN and float inf support.\n\n        The sole purpose of defining a custom JSONEncoder class is to\n        override floatstr() inner function, or more specifically the\n        representation of NaN and +/-float('inf') values in a JSON. Although\n        Infinity values are not supported by JSON standard, we still can\n        convince Javascript JSON.parse() to create a Javascript Infinity\n        object if we feed a token `1e+999` to it.\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n\n        if self.ensure_ascii:\n            _encoder = encoder.encode_basestring_ascii\n        else:\n            _encoder = encoder.encode_basestring\n\n        # On Python 3, JSONEncoder has no more encoding attribute, it produces\n        # an Unicode string\n        if six.PY2 and self.encoding != 'utf-8':\n            def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\n                if isinstance(o, str):\n                    o = o.decode(_encoding)\n                return _orig_encoder(o)\n\n        def floatstr(o, allow_nan=self.allow_nan, _repr=float.__repr__,\n                     _inf=encoder.INFINITY, _neginf=-encoder.INFINITY):\n            # Check for specials.  Note that this type of test is processor\n            # and/or platform-specific, so do tests which don't depend on the\n            # internals.\n\n            # NOTE: In Python, NaN == NaN returns False and it can be used\n            # to detect NaN.\n            # pylint: disable=comparison-with-itself\n            if o != o:\n                text = self.nan_str\n            elif o == _inf:\n                text = self.inf_str\n            elif o == _neginf:\n                text = '-' + self.inf_str\n            else:\n                return _repr(o)\n\n            if not allow_nan:\n                raise ValueError(\n                    _(\"Out of range float values are not JSON compliant: %r\") %\n                    o)\n\n            return text\n\n        _iterencode = json.encoder._make_iterencode(\n            markers, self.default, _encoder, self.indent, floatstr,\n            self.key_separator, self.item_separator, self.sort_keys,\n            self.skipkeys, _one_shot)\n        return _iterencode(o, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(self, table, keypairs, filter_string):\n        query = filter_string.lower()\n        return [keypair for keypair in keypairs\n                if query in keypair.name.lower()]", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_project_name(project_id, projects):\n    for project in projects:\n        if project_id == project.id:\n            return project.name", "response": "Retrieves the project name for a given project id"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, datestring):\n        formats = [\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S.%f\",\n                   \"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%d %H:%M:%S\"]\n        if datestring:\n            for format in formats:\n                try:\n                    parsed = datetime.strptime(datestring, format)\n                    if not timezone.is_aware(parsed):\n                        parsed = timezone.make_aware(parsed, timezone.utc)\n                    return parsed\n                except Exception:\n                    pass\n        return None", "response": "Parses a date - like string into a timezone aware Python datetime."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _try_weakref(arg, remove_callback):\n    try:\n        arg = weakref.ref(arg, remove_callback)\n    except TypeError:\n        # Not all types can have a weakref. That includes strings\n        # and floats and such, so just pass them through directly.\n        pass\n    return arg", "response": "Return a weak reference to arg if possible or arg itself if not."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the cache key using weak references where possible.", "response": "def _get_key(args, kwargs, remove_callback):\n    \"\"\"Calculate the cache key, using weak references where possible.\"\"\"\n    # Use tuples, because lists are not hashable.\n    weak_args = tuple(_try_weakref(arg, remove_callback) for arg in args)\n    # Use a tuple of (key, values) pairs, because dict is not hashable.\n    # Sort it, so that we don't depend on the order of keys.\n    weak_kwargs = tuple(sorted(\n        (key, _try_weakref(value, remove_callback))\n        for (key, value) in kwargs.items()))\n    return weak_args, weak_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef memoized(func=None, max_size=None):\n\n    def decorate(func):\n\n        # The dictionary in which all the data will be cached. This is a\n        # separate instance for every decorated function, and it's stored in a\n        # closure of the wrapped function.\n        cache = collections.OrderedDict()\n        locks = collections.defaultdict(threading.Lock)\n        if max_size:\n            max_cache_size = max_size\n        else:\n            max_cache_size = getattr(settings, 'MEMOIZED_MAX_SIZE_DEFAULT', 25)\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # We need to have defined key early, to be able to use it in the\n            # remove() function, but we calculate the actual value of the key\n            # later on, because we need the remove() function for that.\n            key = None\n\n            def remove(ref):\n                \"\"\"A callback to remove outdated items from cache.\"\"\"\n                try:\n                    # The key here is from closure, and is calculated later.\n                    del cache[key]\n                    del locks[key]\n                except KeyError:\n                    # Some other weak reference might have already removed that\n                    # key -- in that case we don't need to do anything.\n                    pass\n\n            key = _get_key(args, kwargs, remove)\n            try:\n                with locks[key]:\n                    try:\n                        # We want cache hit to be as fast as possible, and\n                        # don't really care much about the speed of a cache\n                        # miss, because it will only happen once and likely\n                        # calls some external API, database, or some other slow\n                        # thing. That's why the hit is in straightforward code,\n                        # and the miss is in an exception.\n                        # We also want to pop the key and reset it to make sure\n                        # the position it has in the order updates.\n                        value = cache[key] = cache.pop(key)\n                    except KeyError:\n                        value = cache[key] = func(*args, **kwargs)\n            except TypeError:\n                # The calculated key may be unhashable when an unhashable\n                # object, such as a list, is passed as one of the arguments. In\n                # that case, we can't cache anything and simply always call the\n                # decorated function.\n                warnings.warn(\n                    \"The key of %s %s is not hashable and cannot be memoized: \"\n                    \"%r\\n\" % (func.__module__, func.__name__, key),\n                    UnhashableKeyWarning, 2)\n                value = func(*args, **kwargs)\n\n            while len(cache) > max_cache_size:\n                try:\n                    popped_tuple = cache.popitem(last=False)\n                    locks.pop(popped_tuple[0], None)\n                except KeyError:\n                    pass\n\n            return value\n        return wrapped\n    if func and callable(func):\n        return decorate(func)\n    return decorate", "response": "Decorator that caches the return value of a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter(self, table, domains, filter_string):\n        q = filter_string.lower()\n\n        def comp(domain):\n            if q in domain.name.lower():\n                return True\n            return False\n\n        return filter(comp, domains)", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, table, idps, filter_string):\n        q = filter_string.lower()\n        return [idp for idp in idps\n                if q in idp.ud.lower()]", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlikes getattr but return default if attr does not exist", "response": "def _getattr(obj, name, default):\n    \"\"\"Like getattr but return `default` if None or False.\n\n    By default, getattr(obj, name, default) returns default only if\n    attr does not exist, here, we return `default` even if attr evaluates to\n    None or False.\n    \"\"\"\n    value = getattr(obj, name, default)\n    if value:\n        return value\n    else:\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist neutron resources with handling RequestURITooLong exception.", "response": "def list_resources_with_long_filters(list_method,\n                                     filter_attr, filter_values, **params):\n    \"\"\"List neutron resources with handling RequestURITooLong exception.\n\n    If filter parameters are long, list resources API request leads to\n    414 error (URL is too long). For such case, this method split\n    list parameters specified by a list_field argument into chunks\n    and call the specified list_method repeatedly.\n\n    :param list_method: Method used to retrieve resource list.\n    :param filter_attr: attribute name to be filtered. The value corresponding\n        to this attribute is specified by \"filter_values\".\n        If you want to specify more attributes for a filter condition,\n        pass them as keyword arguments like \"attr2=values2\".\n    :param filter_values: values of \"filter_attr\" to be filtered.\n        If filter_values are too long and the total URI length exceed the\n        maximum length supported by the neutron server, filter_values will\n        be split into sub lists if filter_values is a list.\n    :param params: parameters to pass a specified listing API call\n        without any changes. You can specify more filter conditions\n        in addition to a pair of filter_attr and filter_values.\n    \"\"\"\n    try:\n        params[filter_attr] = filter_values\n        return list_method(**params)\n    except neutron_exc.RequestURITooLong as uri_len_exc:\n        # The URI is too long because of too many filter values.\n        # Use the excess attribute of the exception to know how many\n        # filter values can be inserted into a single request.\n\n        # We consider only the filter condition from (filter_attr,\n        # filter_values) and do not consider other filter conditions\n        # which may be specified in **params.\n        if not isinstance(filter_values, (list, tuple, set, frozenset)):\n            filter_values = [filter_values]\n\n        # Length of each query filter is:\n        # <key>=<value>& (e.g., id=<uuid>)\n        # The length will be key_len + value_maxlen + 2\n        all_filter_len = sum(len(filter_attr) + len(val) + 2\n                             for val in filter_values)\n        allowed_filter_len = all_filter_len - uri_len_exc.excess\n\n        val_maxlen = max(len(val) for val in filter_values)\n        filter_maxlen = len(filter_attr) + val_maxlen + 2\n        chunk_size = allowed_filter_len // filter_maxlen\n\n        resources = []\n        for i in range(0, len(filter_values), chunk_size):\n            params[filter_attr] = filter_values[i:i + chunk_size]\n            resources.extend(list_method(**params))\n        return resources"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle an update to a trunk in a single openstack network trunk.", "response": "def trunk_update(request, trunk_id, old_trunk, new_trunk):\n    \"\"\"Handle update to a trunk in (at most) three neutron calls.\n\n    The JavaScript side should know only about the old and new state of a\n    trunk. However it should not know anything about how the old and new are\n    meant to be diffed and sent to neutron. We handle that here.\n\n    This code was adapted from Heat, see: https://review.opendev.org/442496\n\n    Call #1) Update all changed properties but 'sub_ports'.\n        PUT /v2.0/trunks/TRUNK_ID\n        openstack network trunk set\n\n    Call #2) Delete subports not needed anymore.\n        PUT /v2.0/trunks/TRUNK_ID/remove_subports\n        openstack network trunk unset --subport\n\n    Call #3) Create new subports.\n        PUT /v2.0/trunks/TRUNK_ID/add_subports\n        openstack network trunk set --subport\n\n    A single neutron port cannot be two subports at the same time (ie.\n    have two segmentation (type, ID)s on the same trunk or to belong to\n    two trunks). Therefore we have to delete old subports before creating\n    new ones to avoid conflicts.\n    \"\"\"\n    LOG.debug(\"trunk_update(): trunk_id=%s\", trunk_id)\n\n    # NOTE(bence romsics): We want to do set operations on the subports,\n    # however we receive subports represented as dicts. In Python\n    # mutable objects like dicts are not hashable so they cannot be\n    # inserted into sets. So we convert subport dicts to (immutable)\n    # frozensets in order to do the set operations.\n    def dict2frozenset(d):\n        \"\"\"Convert a dict to a frozenset.\n\n        Create an immutable equivalent of a dict, so it's hashable\n        therefore can be used as an element of a set or a key of another\n        dictionary.\n        \"\"\"\n        return frozenset(d.items())\n\n    # cf. neutron_lib/api/definitions/trunk.py\n    updatable_props = ('admin_state_up', 'description', 'name')\n    prop_diff = {\n        k: new_trunk[k]\n        for k in updatable_props\n        if old_trunk[k] != new_trunk[k]}\n\n    subports_old = {dict2frozenset(d): d\n                    for d in old_trunk.get('sub_ports', [])}\n    subports_new = {dict2frozenset(d): d\n                    for d in new_trunk.get('sub_ports', [])}\n\n    old_set = set(subports_old.keys())\n    new_set = set(subports_new.keys())\n\n    delete = old_set - new_set\n    create = new_set - old_set\n\n    dicts_delete = [subports_old[fs] for fs in delete]\n    dicts_create = [subports_new[fs] for fs in create]\n\n    trunk = old_trunk\n    if prop_diff:\n        LOG.debug('trunk_update(): update properties of trunk %s: %s',\n                  trunk_id, prop_diff)\n        body = _prepare_body_update_trunk(prop_diff)\n        trunk = neutronclient(request).update_trunk(\n            trunk_id, body=body).get('trunk')\n\n    if dicts_delete:\n        LOG.debug('trunk_update(): delete subports of trunk %s: %s',\n                  trunk_id, dicts_delete)\n        body = _prepare_body_remove_subports(dicts_delete)\n        trunk = neutronclient(request).trunk_remove_subports(\n            trunk_id, body=body)\n\n    if dicts_create:\n        LOG.debug('trunk_update(): create subports of trunk %s: %s',\n                  trunk_id, dicts_create)\n        body = _prepare_body_add_subports(dicts_create)\n        trunk = neutronclient(request).trunk_add_subports(\n            trunk_id, body=body)\n\n    return Trunk(trunk)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef network_list_for_tenant(request, tenant_id, include_external=False,\n                            include_pre_auto_allocate=False,\n                            **params):\n    \"\"\"Return a network list available for the tenant.\n\n    The list contains networks owned by the tenant and public networks.\n    If requested_networks specified, it searches requested_networks only.\n    \"\"\"\n    LOG.debug(\"network_list_for_tenant(): tenant_id=%(tenant_id)s, \"\n              \"params=%(params)s\", {'tenant_id': tenant_id, 'params': params})\n\n    networks = []\n    shared = params.get('shared')\n    if shared is not None:\n        del params['shared']\n\n    if shared in (None, False):\n        # If a user has admin role, network list returned by Neutron API\n        # contains networks that do not belong to that tenant.\n        # So we need to specify tenant_id when calling network_list().\n        networks += network_list(request, tenant_id=tenant_id,\n                                 shared=False, **params)\n\n    if shared in (None, True):\n        # In the current Neutron API, there is no way to retrieve\n        # both owner networks and public networks in a single API call.\n        networks += network_list(request, shared=True, **params)\n\n    # Hack for auto allocated network\n    if include_pre_auto_allocate and not networks:\n        if _is_auto_allocated_network_supported(request):\n            networks.append(PreAutoAllocateNetwork(request))\n\n    params['router:external'] = params.get('router:external', True)\n    if params['router:external'] and include_external:\n        if shared is not None:\n            params['shared'] = shared\n        fetched_net_ids = [n.id for n in networks]\n        # Retrieves external networks when router:external is not specified\n        # in (filtering) params or router:external=True filter is specified.\n        # When router:external=False is specified there is no need to query\n        # networking API because apparently nothing will match the filter.\n        ext_nets = network_list(request, **params)\n        networks += [n for n in ext_nets if\n                     n.id not in fetched_net_ids]\n\n    return networks", "response": "Return a list of network resources owned by the tenant."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a network object.", "response": "def network_create(request, **kwargs):\n    \"\"\"Create a  network object.\n\n    :param request: request context\n    :param tenant_id: (optional) tenant id of the network created\n    :param name: (optional) name of the network created\n    :returns: Network object\n    \"\"\"\n    LOG.debug(\"network_create(): kwargs = %s\", kwargs)\n    if 'tenant_id' not in kwargs:\n        kwargs['tenant_id'] = request.user.project_id\n    body = {'network': kwargs}\n    network = neutronclient(request).create_network(body=body).get('network')\n    return Network(network)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subnet_create(request, network_id, **kwargs):\n    LOG.debug(\"subnet_create(): netid=%(network_id)s, kwargs=%(kwargs)s\",\n              {'network_id': network_id, 'kwargs': kwargs})\n    body = {'subnet': {'network_id': network_id}}\n    if 'tenant_id' not in kwargs:\n        kwargs['tenant_id'] = request.user.project_id\n    body['subnet'].update(kwargs)\n    subnet = neutronclient(request).create_subnet(body=body).get('subnet')\n    return Subnet(subnet)", "response": "Create a subnet on a specified network."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a subnetpool. ip_version is auto-detected in back-end. Parameters: request -- Request context name -- Name for subnetpool prefixes -- List of prefixes for pool Keyword Arguments (optional): min_prefixlen -- Minimum prefix length for allocations from pool max_prefixlen -- Maximum prefix length for allocations from pool default_prefixlen -- Default prefix length for allocations from pool default_quota -- Default quota for allocations from pool shared -- Subnetpool should be shared (Admin-only) tenant_id -- Owner of subnetpool Returns: SubnetPool object", "response": "def subnetpool_create(request, name, prefixes, **kwargs):\n    \"\"\"Create a subnetpool.\n\n    ip_version is auto-detected in back-end.\n\n    Parameters:\n    request           -- Request context\n    name              -- Name for subnetpool\n    prefixes          -- List of prefixes for pool\n\n    Keyword Arguments (optional):\n    min_prefixlen     -- Minimum prefix length for allocations from pool\n    max_prefixlen     -- Maximum prefix length for allocations from pool\n    default_prefixlen -- Default prefix length for allocations from pool\n    default_quota     -- Default quota for allocations from pool\n    shared            -- Subnetpool should be shared (Admin-only)\n    tenant_id         -- Owner of subnetpool\n\n    Returns:\n    SubnetPool object\n    \"\"\"\n    LOG.debug(\"subnetpool_create(): name=%(name)s, prefixes=%(prefixes)s, \"\n              \"kwargs=%(kwargs)s\", {'name': name, 'prefixes': prefixes,\n                                    'kwargs': kwargs})\n    body = {'subnetpool':\n            {'name': name,\n             'prefixes': prefixes,\n             }\n            }\n    if 'tenant_id' not in kwargs:\n        kwargs['tenant_id'] = request.user.project_id\n    body['subnetpool'].update(kwargs)\n    subnetpool = \\\n        neutronclient(request).create_subnetpool(body=body).get('subnetpool')\n    return SubnetPool(subnetpool)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting neutron Ports for this tenant with possible TrunkPort indicated .", "response": "def port_list_with_trunk_types(request, **params):\n    \"\"\"List neutron Ports for this tenant with possible TrunkPort indicated\n\n    :param request: request context\n\n    NOTE Performing two API calls is not atomic, but this is not worse\n         than the original idea when we call port_list repeatedly for\n         each network to perform identification run-time. We should\n         handle the inconsistencies caused by non-atomic API requests\n         gracefully.\n    \"\"\"\n    LOG.debug(\"port_list_with_trunk_types(): params=%s\", params)\n\n    # When trunk feature is disabled in neutron, we have no need to fetch\n    # trunk information and port_list() is enough.\n    if not is_extension_supported(request, 'trunk'):\n        return port_list(request, **params)\n\n    ports = neutronclient(request).list_ports(**params)['ports']\n    trunk_filters = {}\n    if 'tenant_id' in params:\n        trunk_filters['tenant_id'] = params['tenant_id']\n    trunks = neutronclient(request).list_trunks(**trunk_filters)['trunks']\n    parent_ports = set(t['port_id'] for t in trunks)\n    # Create a dict map for child ports (port ID to trunk info)\n    child_ports = dict((s['port_id'],\n                        {'trunk_id': t['id'],\n                         'segmentation_type': s['segmentation_type'],\n                         'segmentation_id': s['segmentation_id']})\n                       for t in trunks\n                       for s in t['sub_ports'])\n\n    def _get_port_info(port):\n        if port['id'] in parent_ports:\n            return PortTrunkParent(port)\n        elif port['id'] in child_ports:\n            return PortTrunkSubport(port, child_ports[port['id']])\n        else:\n            return Port(port)\n\n    return [_get_port_info(p) for p in ports]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef port_create(request, network_id, **kwargs):\n    LOG.debug(\"port_create(): netid=%(network_id)s, kwargs=%(kwargs)s\",\n              {'network_id': network_id, 'kwargs': kwargs})\n    kwargs = unescape_port_kwargs(**kwargs)\n    body = {'port': {'network_id': network_id}}\n    if 'tenant_id' not in kwargs:\n        kwargs['tenant_id'] = request.user.project_id\n    body['port'].update(kwargs)\n    port = neutronclient(request).create_port(body=body).get('port')\n    return Port(port)", "response": "Create a port on a specified network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef servers_update_addresses(request, servers, all_tenants=False):\n\n    # NOTE(e0ne): we don't need to call neutron if we have no instances\n    if not servers:\n        return\n\n    # Get all (filtered for relevant servers) information from Neutron\n    try:\n        # NOTE(e0ne): we need tuple here to work with @memoized decorator.\n        # @memoized works with hashable arguments only.\n        ports = list_resources_with_long_filters(\n            port_list, 'device_id',\n            tuple([instance.id for instance in servers]),\n            request=request)\n        fips = FloatingIpManager(request)\n        if fips.is_supported():\n            floating_ips = list_resources_with_long_filters(\n                fips.list, 'port_id', tuple([port.id for port in ports]),\n                all_tenants=all_tenants)\n        else:\n            floating_ips = []\n        # NOTE(e0ne): we need frozenset here to work with @memoized decorator.\n        # @memoized works with hashable arguments only\n        networks = list_resources_with_long_filters(\n            network_list, 'id', frozenset([port.network_id for port in ports]),\n            request=request)\n    except neutron_exc.NotFound as e:\n        LOG.error('Neutron resource does not exist. %s', e)\n        return\n    except Exception as e:\n        LOG.error('Unable to connect to Neutron: %s', e)\n        error_message = _('Unable to connect to Neutron.')\n        messages.error(request, error_message)\n        return\n\n    # Map instance to its ports\n    instances_ports = collections.defaultdict(list)\n    for port in ports:\n        instances_ports[port.device_id].append(port)\n\n    # Map port to its floating ips\n    ports_floating_ips = collections.defaultdict(list)\n    for fip in floating_ips:\n        ports_floating_ips[fip.port_id].append(fip)\n\n    # Map network id to its name\n    network_names = dict((network.id, network.name_or_id)\n                         for network in networks)\n\n    for server in servers:\n        try:\n            addresses = _server_get_addresses(\n                request,\n                server,\n                instances_ports,\n                ports_floating_ips,\n                network_names)\n        except Exception as e:\n            LOG.error(six.text_type(e))\n        else:\n            server.addresses = addresses", "response": "Retrieve servers networking information from Neutron if enabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists neutron extensions. :param request: django request object", "response": "def list_extensions(request):\n    \"\"\"List neutron extensions.\n\n    :param request: django request object\n    \"\"\"\n    neutron_api = neutronclient(request)\n    try:\n        extensions_list = neutron_api.list_extensions()\n    except exceptions.ServiceCatalogException:\n        return {}\n    if 'extensions' in extensions_list:\n        return tuple(extensions_list['extensions'])\n    else:\n        return ()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a specified extension is supported.", "response": "def is_extension_supported(request, extension_alias):\n    \"\"\"Check if a specified extension is supported.\n\n    :param request: django request object\n    :param extension_alias: neutron extension alias\n    \"\"\"\n    extensions = list_extensions(request)\n    for extension in extensions:\n        if extension['alias'] == extension_alias:\n            return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a feature - specific field can be displayed.", "response": "def get_feature_permission(request, feature, operation=None):\n    \"\"\"Check if a feature-specific field can be displayed.\n\n    This method check a permission for a feature-specific field.\n    Such field is usually provided through Neutron extension.\n\n    :param request: Request Object\n    :param feature: feature name defined in FEATURE_MAP\n    :param operation (optional): Operation type. The valid value should be\n        defined in FEATURE_MAP[feature]['policies']\n        It must be specified if FEATURE_MAP[feature] has 'policies'.\n    \"\"\"\n    network_config = getattr(settings, 'OPENSTACK_NEUTRON_NETWORK', {})\n    feature_info = FEATURE_MAP.get(feature)\n    if not feature_info:\n        raise ValueError(\"The requested feature '%(feature)s' is unknown. \"\n                         \"Please make sure to specify a feature defined \"\n                         \"in FEATURE_MAP.\")\n\n    # Check dashboard settings\n    feature_config = feature_info.get('config')\n    if feature_config:\n        if not network_config.get(feature_config['name'],\n                                  feature_config['default']):\n            return False\n\n    # Check policy\n    feature_policies = feature_info.get('policies')\n    if feature_policies:\n        policy_name = feature_policies.get(operation)\n        if not policy_name:\n            raise ValueError(\"The 'operation' parameter for \"\n                             \"get_feature_permission '%(feature)s' \"\n                             \"is invalid. It should be one of %(allowed)s\"\n                             % {'feature': feature,\n                                'allowed': ' '.join(feature_policies.keys())})\n        role = (('network', policy_name),)\n        if not policy.check(role, request):\n            return False\n\n    # Check if a required extension is enabled\n    feature_extension = feature_info.get('extension')\n    if feature_extension:\n        try:\n            return is_extension_supported(request, feature_extension)\n        except Exception:\n            LOG.info(\"Failed to check Neutron '%s' extension is not supported\",\n                     feature_extension)\n            return False\n\n    # If all checks are passed, now a given feature is allowed.\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef policy_create(request, **kwargs):\n    body = {'policy': kwargs}\n    policy = neutronclient(request).create_qos_policy(body=body).get('policy')\n    return QoSPolicy(policy)", "response": "Create a QoS Policy."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting of QoS Policies.", "response": "def policy_list(request, **kwargs):\n    \"\"\"List of QoS Policies.\"\"\"\n    policies = neutronclient(request).list_qos_policies(\n        **kwargs).get('policies')\n    return [QoSPolicy(p) for p in policies]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef policy_get(request, policy_id, **kwargs):\n    policy = neutronclient(request).show_qos_policy(\n        policy_id, **kwargs).get('policy')\n    return QoSPolicy(policy)", "response": "Get a QoS policy for a given policy id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a RBAC Policy.", "response": "def rbac_policy_create(request, **kwargs):\n    \"\"\"Create a RBAC Policy.\n\n    :param request: request context\n    :param target_tenant: target tenant of the policy\n    :param tenant_id: owner tenant of the policy(Not recommended)\n    :param object_type: network or qos_policy\n    :param object_id: object id of policy\n    :param action: access_as_shared or access_as_external\n    :return: RBACPolicy object\n    \"\"\"\n    body = {'rbac_policy': kwargs}\n    rbac_policy = neutronclient(request).create_rbac_policy(\n        body=body).get('rbac_policy')\n    return RBACPolicy(rbac_policy)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rbac_policy_list(request, **kwargs):\n    policies = neutronclient(request).list_rbac_policies(\n        **kwargs).get('rbac_policies')\n    return [RBACPolicy(p) for p in policies]", "response": "List of RBAC Policies."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating an existing RBAC Policy.", "response": "def rbac_policy_update(request, policy_id, **kwargs):\n    \"\"\"Update a RBAC Policy.\n\n    :param request: request context\n    :param policy_id: target policy id\n    :param target_tenant: target tenant of the policy\n    :return: RBACPolicy object\n    \"\"\"\n    body = {'rbac_policy': kwargs}\n    rbac_policy = neutronclient(request).update_rbac_policy(\n        policy_id, body=body).get('rbac_policy')\n    return RBACPolicy(rbac_policy)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a RBAC policy for a given policy id.", "response": "def rbac_policy_get(request, policy_id, **kwargs):\n    \"\"\"Get RBAC policy for a given policy id.\"\"\"\n    policy = neutronclient(request).show_rbac_policy(\n        policy_id, **kwargs).get('rbac_policy')\n    return RBACPolicy(policy)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(self, **params):\n        # This is to ensure tenant_id key is not populated\n        # if tenant_id=None is specified.\n        tenant_id = params.pop('tenant_id', self.request.user.tenant_id)\n        if tenant_id:\n            params['tenant_id'] = tenant_id\n        return self._list(**params)", "response": "Fetches a list of security groups."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a mapping dict from secgroup id to its name.", "response": "def _sg_name_dict(self, sg_id, rules):\n        \"\"\"Create a mapping dict from secgroup id to its name.\"\"\"\n        related_ids = set([sg_id])\n        related_ids |= set(filter(None, [r['remote_group_id'] for r in rules]))\n        related_sgs = self.client.list_security_groups(id=related_ids,\n                                                       fields=['id', 'name'])\n        related_sgs = related_sgs.get('security_groups')\n        return dict((sg['id'], sg['name']) for sg in related_sgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, sg_id):\n        secgroup = self.client.show_security_group(sg_id).get('security_group')\n        sg_dict = self._sg_name_dict(sg_id, secgroup['security_group_rules'])\n        return SecurityGroup(secgroup, sg_dict)", "response": "Fetches the security group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new security group.", "response": "def create(self, name, desc):\n        \"\"\"Create a new security group.\n\n        :returns: SecurityGroup object created\n        \"\"\"\n        body = {'security_group': {'name': name,\n                                   'description': desc,\n                                   'tenant_id': self.request.user.project_id}}\n        secgroup = self.client.create_security_group(body)\n        return SecurityGroup(secgroup.get('security_group'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new security group rule.", "response": "def rule_create(self, parent_group_id,\n                    direction=None, ethertype=None,\n                    ip_protocol=None, from_port=None, to_port=None,\n                    cidr=None, group_id=None, description=None):\n        \"\"\"Create a new security group rule.\n\n        :param parent_group_id: security group id a rule is created to\n        :param direction: ``ingress`` or ``egress``\n        :param ethertype: ``IPv4`` or ``IPv6``\n        :param ip_protocol: tcp, udp, icmp\n        :param from_port: L4 port range min\n        :param to_port: L4 port range max\n        :param cidr: Remote IP CIDR\n        :param group_id: ID of Source Security Group\n        :returns: SecurityGroupRule object\n        \"\"\"\n        if not cidr:\n            cidr = None\n        if isinstance(from_port, int) and from_port < 0:\n            from_port = None\n        if isinstance(to_port, int) and to_port < 0:\n            to_port = None\n        if isinstance(ip_protocol, int) and ip_protocol < 0:\n            ip_protocol = None\n\n        params = {'security_group_id': parent_group_id,\n                  'direction': direction,\n                  'ethertype': ethertype,\n                  'protocol': ip_protocol,\n                  'port_range_min': from_port,\n                  'port_range_max': to_port,\n                  'remote_ip_prefix': cidr,\n                  'remote_group_id': group_id}\n        if description is not None:\n            params['description'] = description\n        body = {'security_group_rule': params}\n        try:\n            rule = self.client.create_security_group_rule(body)\n        except neutron_exc.OverQuotaClient:\n            raise exceptions.Conflict(\n                _('Security group rule quota exceeded.'))\n        except neutron_exc.Conflict:\n            raise exceptions.Conflict(\n                _('Security group rule already exists.'))\n        rule = rule.get('security_group_rule')\n        sg_dict = self._sg_name_dict(parent_group_id, [rule])\n        return SecurityGroupRule(rule, sg_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_by_instance(self, instance_id):\n        ports = port_list(self.request, device_id=instance_id)\n        sg_ids = []\n        for p in ports:\n            sg_ids += p.security_groups\n        return self._list(id=set(sg_ids)) if sg_ids else []", "response": "Gets the security groups associated with an instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_instance_security_group(self, instance_id,\n                                       new_security_group_ids):\n        \"\"\"Update security groups of a specified instance.\"\"\"\n        ports = port_list(self.request, device_id=instance_id)\n        for p in ports:\n            params = {'security_groups': new_security_group_ids}\n            port_update(self.request, p.id, **params)", "response": "Update security groups of a specified instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_pools(self):\n        search_opts = {'router:external': True}\n        return [FloatingIpPool(pool) for pool\n                in self.client.list_networks(**search_opts).get('networks')]", "response": "Fetches a list of all floating IP pools."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, all_tenants=False, **search_opts):\n        if not all_tenants:\n            tenant_id = self.request.user.tenant_id\n            # In Neutron, list_floatingips returns Floating IPs from\n            # all tenants when the API is called with admin role, so\n            # we need to filter them with tenant_id.\n            search_opts['tenant_id'] = tenant_id\n            port_search_opts = {'tenant_id': tenant_id}\n        else:\n            port_search_opts = {}\n        fips = self.client.list_floatingips(**search_opts)\n        fips = fips.get('floatingips')\n        # Get port list to add instance_id to floating IP list\n        # instance_id is stored in device_id attribute\n        ports = port_list(self.request, **port_search_opts)\n        port_dict = collections.OrderedDict([(p['id'], p) for p in ports])\n        for fip in fips:\n            self._set_instance_info(fip, port_dict.get(fip['port_id']))\n        return [FloatingIp(fip) for fip in fips]", "response": "Fetches a list of all floating IPs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the floating IP.", "response": "def get(self, floating_ip_id):\n        \"\"\"Fetches the floating IP.\n\n        :returns: FloatingIp object corresponding to floating_ip_id\n        \"\"\"\n        fip = self.client.show_floatingip(floating_ip_id).get('floatingip')\n        self._set_instance_info(fip)\n        return FloatingIp(fip)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef allocate(self, pool, tenant_id=None, **params):\n        if not tenant_id:\n            tenant_id = self.request.user.project_id\n        create_dict = {'floating_network_id': pool,\n                       'tenant_id': tenant_id}\n        if 'subnet_id' in params:\n            create_dict['subnet_id'] = params['subnet_id']\n        if 'floating_ip_address' in params:\n            create_dict['floating_ip_address'] = params['floating_ip_address']\n        if 'description' in params:\n            create_dict['description'] = params['description']\n        if 'dns_domain' in params:\n            create_dict['dns_domain'] = params['dns_domain']\n        if 'dns_name' in params:\n            create_dict['dns_name'] = params['dns_name']\n        fip = self.client.create_floatingip(\n            {'floatingip': create_dict}).get('floatingip')\n        self._set_instance_info(fip)\n        return FloatingIp(fip)", "response": "Allocates a floating IP to the tenant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef associate(self, floating_ip_id, port_id):\n        # NOTE: In Neutron Horizon floating IP support, port_id is\n        # \"<port_id>_<ip_address>\" format to identify multiple ports.\n        pid, ip_address = port_id.split('_', 1)\n        update_dict = {'port_id': pid,\n                       'fixed_ip_address': ip_address}\n        self.client.update_floatingip(floating_ip_id,\n                                      {'floatingip': update_dict})", "response": "Associates the floating IP with the port."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of association targets of instance VIFs.", "response": "def list_targets(self):\n        \"\"\"Returns a list of association targets of instance VIFs.\n\n        Each association target is represented as FloatingIpTarget object.\n        FloatingIpTarget is a APIResourceWrapper/APIDictWrapper and\n        'id' and 'name' attributes must be defined in each object.\n        FloatingIpTarget.id can be passed as port_id in associate().\n        FloatingIpTarget.name is displayed in Floating Ip Association Form.\n        \"\"\"\n        tenant_id = self.request.user.tenant_id\n        ports = port_list(self.request, tenant_id=tenant_id)\n        servers, has_more = nova.server_list(self.request, detailed=False)\n        server_dict = collections.OrderedDict(\n            [(s.id, s.name) for s in servers])\n        reachable_subnets = self._get_reachable_subnets(ports)\n\n        targets = []\n        for p in ports:\n            # Remove network ports from Floating IP targets\n            if p.device_owner.startswith('network:'):\n                continue\n            server_name = server_dict.get(p.device_id)\n\n            for ip in p.fixed_ips:\n                if ip['subnet_id'] not in reachable_subnets:\n                    continue\n                # Floating IPs can only target IPv4 addresses.\n                if netaddr.IPAddress(ip['ip_address']).version != 4:\n                    continue\n                targets.append(FloatingIpTarget(p, ip['ip_address'],\n                                                server_name))\n        return targets"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of FloatingIpTarget objects of FIP association.", "response": "def list_targets_by_instance(self, instance_id, target_list=None):\n        \"\"\"Returns a list of FloatingIpTarget objects of FIP association.\n\n        :param instance_id: ID of target VM instance\n        :param target_list: (optional) a list returned by list_targets().\n            If specified, looking up is done against the specified list\n            to save extra API calls to a back-end. Otherwise target list\n            is retrieved from a back-end inside the method.\n        \"\"\"\n        if target_list is not None:\n            # We assume that target_list was returned by list_targets()\n            # so we can assume checks for subnet reachability and IP version\n            # have been done already. We skip all checks here.\n            return [target for target in target_list\n                    if target['instance_id'] == instance_id]\n        else:\n            ports = self._target_ports_by_instance(instance_id)\n            reachable_subnets = self._get_reachable_subnets(\n                ports, fetch_router_ports=True)\n            name = self._get_server_name(instance_id)\n            targets = []\n            for p in ports:\n                for ip in p.fixed_ips:\n                    if ip['subnet_id'] not in reachable_subnets:\n                        continue\n                    # Floating IPs can only target IPv4 addresses.\n                    if netaddr.IPAddress(ip['ip_address']).version != 4:\n                        continue\n                    targets.append(FloatingIpTarget(p, ip['ip_address'], name))\n            return targets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _current_component(view_func, dashboard=None, panel=None):\n    @functools.wraps(view_func, assigned=available_attrs(view_func))\n    def dec(request, *args, **kwargs):\n        if dashboard:\n            request.horizon['dashboard'] = dashboard\n        if panel:\n            request.horizon['panel'] = panel\n        return view_func(request, *args, **kwargs)\n    return dec", "response": "Decorator that sets the currently - active dashboard and / or panel on the request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef require_auth(view_func):\n    from horizon.exceptions import NotAuthenticated\n\n    @functools.wraps(view_func, assigned=available_attrs(view_func))\n    def dec(request, *args, **kwargs):\n        if request.user.is_authenticated:\n            return view_func(request, *args, **kwargs)\n        raise NotAuthenticated(_(\"Please log in to continue.\"))\n    return dec", "response": "Decorator that checks that the user is logged in and raises a NotAuthenticated exception."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecorating a view function to require permission - based access controls.", "response": "def require_perms(view_func, required):\n    \"\"\"Enforces permission-based access controls.\n\n    :param list required: A tuple of permission names, all of which the request\n                          user must possess in order access the decorated view.\n\n    Example usage::\n\n        from horizon.decorators import require_perms\n\n\n        @require_perms(['foo.admin', 'foo.member'])\n        def my_view(request):\n            ...\n\n    Raises a :exc:`~horizon.exceptions.NotAuthorized` exception if the\n    requirements are not met.\n    \"\"\"\n    from horizon.exceptions import NotAuthorized\n    # We only need to check each permission once for a view, so we'll use a set\n    current_perms = getattr(view_func, '_required_perms', set([]))\n    view_func._required_perms = current_perms | set(required)\n\n    @functools.wraps(view_func, assigned=available_attrs(view_func))\n    def dec(request, *args, **kwargs):\n        if request.user.is_authenticated:\n            if request.user.has_perms(view_func._required_perms):\n                return view_func(request, *args, **kwargs)\n        raise NotAuthorized(_(\"You are not authorized to access %s\")\n                            % request.path)\n\n    # If we don't have any permissions, just return the original view.\n    if required:\n        return dec\n    else:\n        return view_func"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef require_component_access(view_func, component):\n    from horizon.exceptions import NotAuthorized\n\n    @functools.wraps(view_func, assigned=available_attrs(view_func))\n    def dec(request, *args, **kwargs):\n        if not component.can_access({'request': request}):\n            raise NotAuthorized(_(\"You are not authorized to access %s\")\n                                % request.path)\n\n        return view_func(request, *args, **kwargs)\n\n    return dec", "response": "Decorator that checks that the component can access the view."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_angular(fileobj, keywords, comment_tags, options):\n\n    parser = AngularGettextHTMLParser()\n\n    for line in fileobj:\n        parser.feed(encodeutils.safe_decode(line))\n\n    for string in parser.strings:\n        yield(string)", "response": "This function extracts messages from angular template files that use\n    angular - gettext translate directive as per\n   . It returns an iterator over tuples of the first line of the file - like object that is passed to the function extract_angular method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef image_get(request, image_id):\n    image = glanceclient(request).images.get(image_id)\n    return Image(image)", "response": "Returns an Image object populated with metadata for a given image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_list_detailed(request, marker=None, sort_dir='desc',\n                        sort_key='created_at', filters=None, paginate=False,\n                        reversed_order=False, **kwargs):\n    \"\"\"Thin layer above glanceclient, for handling pagination issues.\n\n    It provides iterating both forward and backward on top of ascetic\n    OpenStack pagination API - which natively supports only iterating forward\n    through the entries. Thus in order to retrieve list of objects at previous\n    page, a request with the reverse entries order had to be made to Glance,\n    using the first object id on current page as the marker - restoring\n    the original items ordering before sending them back to the UI.\n\n    :param request:\n\n        The request object coming from browser to be passed further into\n        Glance service.\n\n    :param marker:\n\n        The id of an object which defines a starting point of a query sent to\n        Glance service.\n\n    :param sort_dir:\n\n        The direction by which the resulting image list throughout all pages\n        (if pagination is enabled) will be sorted. Could be either 'asc'\n        (ascending) or 'desc' (descending), defaults to 'desc'.\n\n    :param sort_key:\n\n        The name of key by by which the resulting image list throughout all\n        pages (if pagination is enabled) will be sorted. Defaults to\n        'created_at'.\n\n    :param filters:\n\n        A dictionary of filters passed as is to Glance service.\n\n    :param paginate:\n\n        Whether the pagination is enabled. If it is, then the number of\n        entries on a single page of images table is limited to the specific\n        number stored in browser cookies.\n\n    :param reversed_order:\n\n        Set this flag to True when it's necessary to get a reversed list of\n        images from Glance (used for navigating the images list back in UI).\n    \"\"\"\n    limit = getattr(settings, 'API_RESULT_LIMIT', 1000)\n    page_size = utils.get_page_size(request)\n\n    if paginate:\n        request_size = page_size + 1\n    else:\n        request_size = limit\n\n    _normalize_list_input(filters, **kwargs)\n    kwargs = {'filters': filters or {}}\n\n    if marker:\n        kwargs['marker'] = marker\n    kwargs['sort_key'] = sort_key\n\n    if not reversed_order:\n        kwargs['sort_dir'] = sort_dir\n    else:\n        kwargs['sort_dir'] = 'desc' if sort_dir == 'asc' else 'asc'\n\n    images_iter = glanceclient(request).images.list(page_size=request_size,\n                                                    limit=limit,\n                                                    **kwargs)\n    has_prev_data = False\n    has_more_data = False\n    if paginate:\n        images = list(itertools.islice(images_iter, request_size))\n        # first and middle page condition\n        if len(images) > page_size:\n            images.pop(-1)\n            has_more_data = True\n            # middle page condition\n            if marker is not None:\n                has_prev_data = True\n        # first page condition when reached via prev back\n        elif reversed_order and marker is not None:\n            has_more_data = True\n        # last page condition\n        elif marker is not None:\n            has_prev_data = True\n\n        # restore the original ordering here\n        if reversed_order:\n            images = sorted(images, key=lambda image:\n                            (getattr(image, sort_key) or '').lower(),\n                            reverse=(sort_dir == 'desc'))\n    else:\n        images = list(images_iter)\n\n    # TODO(jpichon): Do it better\n    wrapped_images = []\n    for image in images:\n        wrapped_images.append(Image(image))\n\n    return wrapped_images, has_more_data, has_prev_data", "response": "This function returns a detailed list of images for the current object id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_image_metadata(data):\n\n    # Default metadata\n    meta = {'protected': data.get('protected', False),\n            'disk_format': data.get('disk_format', 'raw'),\n            'container_format': data.get('container_format', 'bare'),\n            'min_disk': data.get('min_disk') or 0,\n            'min_ram': data.get('min_ram') or 0,\n            'name': data.get('name', '')}\n\n    # Glance does not really do anything with container_format at the\n    # moment. It requires it is set to the same disk_format for the three\n    # Amazon image types, otherwise it just treats them as 'bare.' As such\n    # we will just set that to be that here instead of bothering the user\n    # with asking them for information we can already determine.\n    if meta['disk_format'] in ('ami', 'aki', 'ari',):\n        meta['container_format'] = meta['disk_format']\n    elif meta['disk_format'] == 'docker':\n        # To support docker containers we allow the user to specify\n        # 'docker' as the format. In that case we really want to use\n        # 'raw' as the disk format and 'docker' as the container format.\n        meta['disk_format'] = 'raw'\n        meta['container_format'] = 'docker'\n    elif meta['disk_format'] == 'ova':\n        # If the user wishes to upload an OVA using Horizon, then\n        # 'ova' must be the container format and 'vmdk' must be the disk\n        # format.\n        meta['container_format'] = 'ova'\n        meta['disk_format'] = 'vmdk'\n\n    properties = {}\n\n    for prop, key in [('description', 'description'),\n                      ('kernel_id', 'kernel'),\n                      ('ramdisk_id', 'ramdisk'),\n                      ('architecture', 'architecture')]:\n        if data.get(key):\n            properties[prop] = data[key]\n\n    _handle_unknown_properties(data, properties)\n\n    if ('visibility' in data and\n            data['visibility'] not in ['public', 'private', 'community',\n                                       'shared']):\n        raise KeyError('invalid visibility option: %s' % data['visibility'])\n    _normalize_is_public_filter(data)\n\n    if VERSIONS.active < 2:\n        meta['properties'] = properties\n        meta['is_public'] = data.get('is_public', False)\n    else:\n        meta['visibility'] = data.get('visibility', 'private')\n        meta.update(properties)\n\n    return meta", "response": "Generate metadata dict for a new image from a given form data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an image in the current thread.", "response": "def image_create(request, **kwargs):\n    \"\"\"Create image.\n\n    :param kwargs:\n        * copy_from: URL from which Glance server should immediately copy\n            the data and store it in its configured image store.\n        * data: Form data posted from client.\n        * location: URL where the data for this image already resides.\n\n    In the case of 'copy_from' and 'location', the Glance server\n    will give us a immediate response from create and handle the data\n    asynchronously.\n\n    In the case of 'data' the process of uploading the data may take\n    some time and is handed off to a separate thread.\n    \"\"\"\n    data = kwargs.pop('data', None)\n    location = None\n    if VERSIONS.active >= 2:\n        location = kwargs.pop('location', None)\n\n    image = glanceclient(request).images.create(**kwargs)\n    if location is not None:\n        glanceclient(request).images.add_location(image.id, location, {})\n\n    if data:\n        if isinstance(data, six.string_types):\n            # The image data is meant to be uploaded externally, return a\n            # special wrapper to bypass the web server in a subsequent upload\n            return ExternallyUploadedImage(image, request)\n        elif isinstance(data, TemporaryUploadedFile):\n            # Hack to fool Django, so we can keep file open in the new thread.\n            if six.PY2:\n                data.file.close_called = True\n            else:\n                data.file._closer.close_called = True\n        elif isinstance(data, InMemoryUploadedFile):\n            # Clone a new file for InMemeoryUploadedFile.\n            # Because the old one will be closed by Django.\n            data = SimpleUploadedFile(data.name,\n                                      data.read(),\n                                      data.content_type)\n        if VERSIONS.active < 2:\n            thread.start_new_thread(image_update,\n                                    (request, image.id),\n                                    {'data': data})\n        else:\n            def upload():\n                try:\n                    return glanceclient(request).images.upload(image.id, data)\n                finally:\n                    filename = str(data.file.name)\n                    try:\n                        os.remove(filename)\n                    except OSError as e:\n                        LOG.warning('Failed to remove temporary image file '\n                                    '%(file)s (%(e)s)',\n                                    {'file': filename, 'e': e})\n            thread.start_new_thread(upload, ())\n\n    return Image(image)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd or update a custom property of an image.", "response": "def image_update_properties(request, image_id, remove_props=None, **kwargs):\n    \"\"\"Add or update a custom property of an image.\"\"\"\n    return glanceclient(request, '2').images.update(image_id,\n                                                    remove_props,\n                                                    **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting custom properties for an image.", "response": "def image_delete_properties(request, image_id, keys):\n    \"\"\"Delete custom properties for an image.\"\"\"\n    return glanceclient(request, '2').images.update(image_id, keys)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter metadata namespaces based on given resource types and properties target.", "response": "def filter_properties_target(namespaces_iter,\n                             resource_types,\n                             properties_target):\n    \"\"\"Filter metadata namespaces.\n\n    Filtering is done based ongiven resource types and a properties target.\n\n    :param namespaces_iter: Metadata namespaces iterable.\n    :param resource_types: List of resource type names.\n    :param properties_target: Name of the properties target.\n    \"\"\"\n    def filter_namespace(namespace):\n        for asn in namespace.get('resource_type_associations'):\n            if (asn.get('name') in resource_types and\n                    asn.get('properties_target') == properties_target):\n                return True\n        return False\n    return filter(filter_namespace, namespaces_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef metadefs_namespace_list(request,\n                            filters=None,\n                            sort_dir='asc',\n                            sort_key='namespace',\n                            marker=None,\n                            paginate=False):\n    \"\"\"Retrieve a listing of Namespaces\n\n    :param paginate: If true will perform pagination based on settings.\n    :param marker: Specifies the namespace of the last-seen namespace.\n             The typical pattern of limit and marker is to make an\n             initial limited request and then to use the last\n             namespace from the response as the marker parameter\n             in a subsequent limited request. With paginate, limit\n             is automatically set.\n    :param sort_dir: The sort direction ('asc' or 'desc').\n    :param sort_key: The field to sort on (for example, 'created_at'). Default\n             is namespace. The way base namespaces are loaded into glance\n             typically at first deployment is done in a single transaction\n             giving them a potentially unpredictable sort result when using\n             create_at.\n    :param filters: specifies addition fields to filter on such as\n             resource_types.\n    :returns A tuple of three values:\n        1) Current page results\n        2) A boolean of whether or not there are previous page(s).\n        3) A boolean of whether or not there are more page(s).\n\n    \"\"\"\n    # Listing namespaces requires the v2 API. If not supported we return an\n    # empty array so callers don't need to worry about version checking.\n    if get_version() < 2:\n        return [], False, False\n\n    if filters is None:\n        filters = {}\n    limit = getattr(settings, 'API_RESULT_LIMIT', 1000)\n    page_size = utils.get_page_size(request)\n\n    if paginate:\n        request_size = page_size + 1\n    else:\n        request_size = limit\n\n    kwargs = {'filters': filters}\n    if marker:\n        kwargs['marker'] = marker\n    kwargs['sort_dir'] = sort_dir\n    kwargs['sort_key'] = sort_key\n\n    namespaces_iter = glanceclient(request, '2').metadefs_namespace.list(\n        page_size=request_size, limit=limit, **kwargs)\n\n    # Filter the namespaces based on the provided properties_target since this\n    # is not supported by the metadata namespaces API.\n    resource_types = filters.get('resource_types')\n    properties_target = filters.get('properties_target')\n    if resource_types and properties_target:\n        namespaces_iter = filter_properties_target(namespaces_iter,\n                                                   resource_types,\n                                                   properties_target)\n\n    has_prev_data = False\n    has_more_data = False\n    if paginate:\n        namespaces = list(itertools.islice(namespaces_iter, request_size))\n        # first and middle page condition\n        if len(namespaces) > page_size:\n            namespaces.pop(-1)\n            has_more_data = True\n            # middle page condition\n            if marker is not None:\n                has_prev_data = True\n        # first page condition when reached via prev back\n        elif sort_dir == 'desc' and marker is not None:\n            has_more_data = True\n        # last page condition\n        elif marker is not None:\n            has_prev_data = True\n    else:\n        namespaces = list(namespaces_iter)\n\n    namespaces = [Namespace(namespace) for namespace in namespaces]\n    return namespaces, has_more_data, has_prev_data", "response": "Retrieve a listing of all namespaces in the current base namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef discover_files(base_path, sub_path='', ext='', trim_base_path=False):\n    file_list = []\n    for root, dirs, files in walk(path.join(base_path, sub_path)):\n        if trim_base_path:\n            root = path.relpath(root, base_path)\n        file_list.extend([path.join(root, file_name)\n                          for file_name in files\n                          if file_name.endswith(ext)])\n    return sorted(file_list)", "response": "Discovers all files in given paths."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsort JavaScript files in a given list of files.", "response": "def sort_js_files(js_files):\n    \"\"\"Sorts JavaScript files in `js_files`.\n\n    It sorts JavaScript files in a given `js_files`\n    into source files, mock files and spec files based on file extension.\n\n    Output:\n\n    * sources: source files for production.  The order of source files\n      is significant and should be listed in the below order:\n\n      - First, all the that defines the other application's angular module.\n        Those files have extension of `.module.js`.  The order among them is\n        not significant.\n\n      - Followed by all other source code files.  The order among them\n        is not significant.\n\n    * mocks: mock files provide mock data/services for tests.  They have\n      extension of `.mock.js`. The order among them is not significant.\n\n    * specs: spec files for testing.  They have extension of `.spec.js`.\n      The order among them is not significant.\n\n    \"\"\"\n    modules = [f for f in js_files if f.endswith(MODULE_EXT)]\n    mocks = [f for f in js_files if f.endswith(MOCK_EXT)]\n    specs = [f for f in js_files if f.endswith(SPEC_EXT)]\n\n    other_sources = [f for f in js_files\n                     if (not f.endswith(MODULE_EXT) and\n                         not f.endswith(MOCK_EXT) and\n                         not f.endswith(SPEC_EXT))]\n\n    sources = modules + other_sources\n    return sources, mocks, specs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndiscovering static files in given paths. It returns JavaScript sources mocks specs and HTML templates.", "response": "def discover_static_files(base_path, sub_path=''):\n    \"\"\"Discovers static files in given paths.\n\n    It returns JavaScript sources, mocks, specs and HTML templates,\n    all grouped in lists.\n    \"\"\"\n    js_files = discover_files(base_path, sub_path=sub_path,\n                              ext='.js', trim_base_path=True)\n    sources, mocks, specs = sort_js_files(js_files)\n    html_files = discover_files(base_path, sub_path=sub_path,\n                                ext='.html', trim_base_path=True)\n\n    p = path.join(base_path, sub_path)\n    _log(sources, 'JavaScript source', p)\n    _log(mocks, 'JavaScript mock', p)\n    _log(specs, 'JavaScript spec', p)\n    _log(html_files, 'HTML template', p)\n\n    return sources, mocks, specs, html_files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _log(file_list, list_name, in_path):\n    file_names = '\\n'.join(file_list)\n    LOG.debug(\"\\nDiscovered %(size)d %(name)s file(s) in %(path)s:\\n\"\n              \"%(files)s\\n\",\n              {'size': len(file_list), 'name': list_name, 'path': in_path,\n               'files': file_names})", "response": "Logs result at debug level"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter(self, table, security_groups, filter_string):\n        query = filter_string.lower()\n        return [security_group for security_group in security_groups\n                if query in security_group.name.lower()]", "response": "Naive case - insensitive search."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_tables(self, tables):\n        self.navigation_table = tables[self.navigation_table_class._meta.name]\n        self.content_table = tables[self.content_table_class._meta.name]\n        navigation_item = self.kwargs.get(self.navigation_kwarg_name)\n        content_path = self.kwargs.get(self.content_kwarg_name)\n        if self.has_breadcrumb:\n            self.prepare_breadcrumb(tables, navigation_item, content_path)", "response": "Sets the tables on the browser."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_account_history(self, account_id, **kwargs):\n        endpoint = '/accounts/{}/ledger'.format(account_id)\n        return self._send_paginated_message(endpoint, params=kwargs)", "response": "Get the history of an account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_account_holds(self, account_id, **kwargs):\n        endpoint = '/accounts/{}/holds'.format(account_id)\n        return self._send_paginated_message(endpoint, params=kwargs)", "response": "This method returns a generator which returns a list of dicts containing the information about the holds for the account."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef place_order(self, product_id, side, order_type, **kwargs):\n        # Margin parameter checks\n        if kwargs.get('overdraft_enabled') is not None and \\\n                kwargs.get('funding_amount') is not None:\n            raise ValueError('Margin funding must be specified through use of '\n                             'overdraft or by setting a funding amount, but not'\n                             ' both')\n\n        # Limit order checks\n        if order_type == 'limit':\n            if kwargs.get('cancel_after') is not None and \\\n                    kwargs.get('time_in_force') != 'GTT':\n                raise ValueError('May only specify a cancel period when time '\n                                 'in_force is `GTT`')\n            if kwargs.get('post_only') is not None and kwargs.get('time_in_force') in \\\n                    ['IOC', 'FOK']:\n                raise ValueError('post_only is invalid when time in force is '\n                                 '`IOC` or `FOK`')\n\n        # Market and stop order checks\n        if order_type == 'market' or order_type == 'stop':\n            if not (kwargs.get('size') is None) ^ (kwargs.get('funds') is None):\n                raise ValueError('Either `size` or `funds` must be specified '\n                                 'for market/stop orders (but not both).')\n\n        # Build params dict\n        params = {'product_id': product_id,\n                  'side': side,\n                  'type': order_type}\n        params.update(kwargs)\n        return self._send_message('post', '/orders', data=json.dumps(params))", "response": "Place an order in the current Public TeleSign Core."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef buy(self, product_id, order_type, **kwargs):\n        return self.place_order(product_id, 'buy', order_type, **kwargs)", "response": "Place a buy order."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplace a sell order.", "response": "def sell(self, product_id, order_type, **kwargs):\n        \"\"\"Place a sell order.\n\n        This is included to maintain backwards compatibility with older versions\n        of cbpro-Python. For maximum support from docstrings and function\n        signatures see the order type-specific functions place_limit_order,\n        place_market_order, and place_stop_order.\n\n        Args:\n            product_id (str): Product to order (eg. 'BTC-USD')\n            order_type (str): Order type ('limit', 'market', or 'stop')\n            **kwargs: Additional arguments can be specified for different order\n                types.\n\n        Returns:\n            dict: Order details. See `place_order` for example.\n\n        \"\"\"\n        return self.place_order(product_id, 'sell', order_type, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef place_limit_order(self, product_id, side, price, size,\n                          client_oid=None,\n                          stp=None,\n                          time_in_force=None,\n                          cancel_after=None,\n                          post_only=None,\n                          overdraft_enabled=None,\n                          funding_amount=None):\n        \"\"\"Place a limit order.\n\n        Args:\n            product_id (str): Product to order (eg. 'BTC-USD')\n            side (str): Order side ('buy' or 'sell)\n            price (Decimal): Price per cryptocurrency\n            size (Decimal): Amount of cryptocurrency to buy or sell\n            client_oid (Optional[str]): User-specified Order ID\n            stp (Optional[str]): Self-trade prevention flag. See `place_order`\n                for details.\n            time_in_force (Optional[str]): Time in force. Options:\n                'GTC'   Good till canceled\n                'GTT'   Good till time (set by `cancel_after`)\n                'IOC'   Immediate or cancel\n                'FOK'   Fill or kill\n            cancel_after (Optional[str]): Cancel after this period for 'GTT'\n                orders. Options are 'min', 'hour', or 'day'.\n            post_only (Optional[bool]): Indicates that the order should only\n                make liquidity. If any part of the order results in taking\n                liquidity, the order will be rejected and no part of it will\n                execute.\n            overdraft_enabled (Optional[bool]): If true funding above and\n                beyond the account balance will be provided by margin, as\n                necessary.\n            funding_amount (Optional[Decimal]): Amount of margin funding to be\n                provided for the order. Mutually exclusive with\n                `overdraft_enabled`.\n\n        Returns:\n            dict: Order details. See `place_order` for example.\n\n        \"\"\"\n        params = {'product_id': product_id,\n                  'side': side,\n                  'order_type': 'limit',\n                  'price': price,\n                  'size': size,\n                  'client_oid': client_oid,\n                  'stp': stp,\n                  'time_in_force': time_in_force,\n                  'cancel_after': cancel_after,\n                  'post_only': post_only,\n                  'overdraft_enabled': overdraft_enabled,\n                  'funding_amount': funding_amount}\n        params = dict((k, v) for k, v in params.items() if v is not None)\n\n        return self.place_order(**params)", "response": "Place a limit order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplace market order. Args: product_id (str): Product to order (eg. 'BTC-USD') side (str): Order side ('buy' or 'sell) size (Optional[Decimal]): Desired amount in crypto. Specify this or `funds`. funds (Optional[Decimal]): Desired amount of quote currency to use. Specify this or `size`. client_oid (Optional[str]): User-specified Order ID stp (Optional[str]): Self-trade prevention flag. See `place_order` for details. overdraft_enabled (Optional[bool]): If true funding above and beyond the account balance will be provided by margin, as necessary. funding_amount (Optional[Decimal]): Amount of margin funding to be provided for the order. Mutually exclusive with `overdraft_enabled`. Returns: dict: Order details. See `place_order` for example.", "response": "def place_market_order(self, product_id, side, size=None, funds=None,\n                           client_oid=None,\n                           stp=None,\n                           overdraft_enabled=None,\n                           funding_amount=None):\n        \"\"\" Place market order.\n\n        Args:\n            product_id (str): Product to order (eg. 'BTC-USD')\n            side (str): Order side ('buy' or 'sell)\n            size (Optional[Decimal]): Desired amount in crypto. Specify this or\n                `funds`.\n            funds (Optional[Decimal]): Desired amount of quote currency to use.\n                Specify this or `size`.\n            client_oid (Optional[str]): User-specified Order ID\n            stp (Optional[str]): Self-trade prevention flag. See `place_order`\n                for details.\n            overdraft_enabled (Optional[bool]): If true funding above and\n                beyond the account balance will be provided by margin, as\n                necessary.\n            funding_amount (Optional[Decimal]): Amount of margin funding to be\n                provided for the order. Mutually exclusive with\n                `overdraft_enabled`.\n\n        Returns:\n            dict: Order details. See `place_order` for example.\n\n        \"\"\"\n        params = {'product_id': product_id,\n                  'side': side,\n                  'order_type': 'market',\n                  'size': size,\n                  'funds': funds,\n                  'client_oid': client_oid,\n                  'stp': stp,\n                  'overdraft_enabled': overdraft_enabled,\n                  'funding_amount': funding_amount}\n        params = dict((k, v) for k, v in params.items() if v is not None)\n\n        return self.place_order(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cancel_all(self, product_id=None):\n        if product_id is not None:\n            params = {'product_id': product_id}\n        else:\n            params = None\n        return self._send_message('delete', '/orders', params=params)", "response": "Cancel all open orders for a specific product."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist your current open orders. This method returns a generator which may make multiple HTTP requests while iterating through it. Only open or un-settled orders are returned. As soon as an order is no longer open and settled, it will no longer appear in the default request. Orders which are no longer resting on the order book, will be marked with the 'done' status. There is a small window between an order being 'done' and 'settled'. An order is 'settled' when all of the fills have settled and the remaining holds (if any) have been removed. For high-volume trading it is strongly recommended that you maintain your own list of open orders and use one of the streaming market data feeds to keep it updated. You should poll the open orders endpoint once when you start trading to obtain the current state of any open orders. Args: product_id (Optional[str]): Only list orders for this product status (Optional[list/str]): Limit list of orders to this status or statuses. Passing 'all' returns orders of all statuses. ** Options: 'open', 'pending', 'active', 'done', 'settled' ** default: ['open', 'pending', 'active'] Returns: list: Containing information on orders. Example:: [ { \"id\": \"d0c5340b-6d6c-49d9-b567-48c4bfca13d2\", \"price\": \"0.10000000\", \"size\": \"0.01000000\", \"product_id\": \"BTC-USD\", \"side\": \"buy\", \"stp\": \"dc\", \"type\": \"limit\", \"time_in_force\": \"GTC\", \"post_only\": false, \"created_at\": \"2016-12-08T20:02:28.53864Z\", \"fill_fees\": \"0.0000000000000000\", \"filled_size\": \"0.00000000\", \"executed_value\": \"0.0000000000000000\", \"status\": \"open\", \"settled\": false }, { ... } ]", "response": "def get_orders(self, product_id=None, status=None, **kwargs):\n        \"\"\" List your current open orders.\n\n        This method returns a generator which may make multiple HTTP requests\n        while iterating through it.\n\n        Only open or un-settled orders are returned. As soon as an\n        order is no longer open and settled, it will no longer appear\n        in the default request.\n\n        Orders which are no longer resting on the order book, will be\n        marked with the 'done' status. There is a small window between\n        an order being 'done' and 'settled'. An order is 'settled' when\n        all of the fills have settled and the remaining holds (if any)\n        have been removed.\n\n        For high-volume trading it is strongly recommended that you\n        maintain your own list of open orders and use one of the\n        streaming market data feeds to keep it updated. You should poll\n        the open orders endpoint once when you start trading to obtain\n        the current state of any open orders.\n\n        Args:\n            product_id (Optional[str]): Only list orders for this\n                product\n            status (Optional[list/str]): Limit list of orders to\n                this status or statuses. Passing 'all' returns orders\n                of all statuses.\n                ** Options: 'open', 'pending', 'active', 'done',\n                    'settled'\n                ** default: ['open', 'pending', 'active']\n\n        Returns:\n            list: Containing information on orders. Example::\n                [\n                    {\n                        \"id\": \"d0c5340b-6d6c-49d9-b567-48c4bfca13d2\",\n                        \"price\": \"0.10000000\",\n                        \"size\": \"0.01000000\",\n                        \"product_id\": \"BTC-USD\",\n                        \"side\": \"buy\",\n                        \"stp\": \"dc\",\n                        \"type\": \"limit\",\n                        \"time_in_force\": \"GTC\",\n                        \"post_only\": false,\n                        \"created_at\": \"2016-12-08T20:02:28.53864Z\",\n                        \"fill_fees\": \"0.0000000000000000\",\n                        \"filled_size\": \"0.00000000\",\n                        \"executed_value\": \"0.0000000000000000\",\n                        \"status\": \"open\",\n                        \"settled\": false\n                    },\n                    {\n                        ...\n                    }\n                ]\n\n        \"\"\"\n        params = kwargs\n        if product_id is not None:\n            params['product_id'] = product_id\n        if status is not None:\n            params['status'] = status\n        return self._send_paginated_message('/orders', params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a generator which returns a list of recent fills.", "response": "def get_fills(self, product_id=None, order_id=None, **kwargs):\n        \"\"\" Get a list of recent fills.\n\n        As of 8/23/18 - Requests without either order_id or product_id\n        will be rejected\n\n        This method returns a generator which may make multiple HTTP requests\n        while iterating through it.\n\n        Fees are recorded in two stages. Immediately after the matching\n        engine completes a match, the fill is inserted into our\n        datastore. Once the fill is recorded, a settlement process will\n        settle the fill and credit both trading counterparties.\n\n        The 'fee' field indicates the fees charged for this fill.\n\n        The 'liquidity' field indicates if the fill was the result of a\n        liquidity provider or liquidity taker. M indicates Maker and T\n        indicates Taker.\n\n        Args:\n            product_id (str): Limit list to this product_id\n            order_id (str): Limit list to this order_id\n            kwargs (dict): Additional HTTP request parameters.\n\n        Returns:\n            list: Containing information on fills. Example::\n                [\n                    {\n                        \"trade_id\": 74,\n                        \"product_id\": \"BTC-USD\",\n                        \"price\": \"10.00\",\n                        \"size\": \"0.01\",\n                        \"order_id\": \"d50ec984-77a8-460a-b958-66f114b0de9b\",\n                        \"created_at\": \"2014-11-07T22:19:28.578544Z\",\n                        \"liquidity\": \"T\",\n                        \"fee\": \"0.00025\",\n                        \"settled\": true,\n                        \"side\": \"buy\"\n                    },\n                    {\n                        ...\n                    }\n                ]\n\n        \"\"\"\n        if (product_id is None) and (order_id is None):\n            raise ValueError('Either product_id or order_id must be specified.')\n\n        params = {}\n        if product_id:\n            params['product_id'] = product_id\n        if order_id:\n            params['order_id'] = order_id\n        params.update(kwargs)\n\n        return self._send_paginated_message('/fills', params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fundings(self, status=None, **kwargs):\n        params = {}\n        if status is not None:\n            params['status'] = status\n        params.update(kwargs)\n        return self._send_paginated_message('/funding', params=params)", "response": "This method returns a generator which returns a generator which will make multiple HTTP requests to get the information on the margin profile that draws funding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrepaying the older funding records first.", "response": "def repay_funding(self, amount, currency):\n        \"\"\" Repay funding. Repays the older funding records first.\n\n        Args:\n            amount (int): Amount of currency to repay\n            currency (str): The currency, example USD\n\n        Returns:\n            Not specified by cbpro.\n\n        \"\"\"\n        params = {\n            'amount': amount,\n            'currency': currency  # example: USD\n            }\n        return self._send_message('post', '/funding/repay',\n                                  data=json.dumps(params))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef margin_transfer(self, margin_profile_id, transfer_type, currency,\n                        amount):\n        \"\"\" Transfer funds between your standard profile and a margin profile.\n\n        Args:\n            margin_profile_id (str): Margin profile ID to withdraw or deposit\n                from.\n            transfer_type (str): 'deposit' or 'withdraw'\n            currency (str): Currency to transfer (eg. 'USD')\n            amount (Decimal): Amount to transfer\n\n        Returns:\n            dict: Transfer details. Example::\n                {\n                  \"created_at\": \"2017-01-25T19:06:23.415126Z\",\n                  \"id\": \"80bc6b74-8b1f-4c60-a089-c61f9810d4ab\",\n                  \"user_id\": \"521c20b3d4ab09621f000011\",\n                  \"profile_id\": \"cda95996-ac59-45a3-a42e-30daeb061867\",\n                  \"margin_profile_id\": \"45fa9e3b-00ba-4631-b907-8a98cbdf21be\",\n                  \"type\": \"deposit\",\n                  \"amount\": \"2\",\n                  \"currency\": \"USD\",\n                  \"account_id\": \"23035fc7-0707-4b59-b0d2-95d0c035f8f5\",\n                  \"margin_account_id\": \"e1d9862c-a259-4e83-96cd-376352a9d24d\",\n                  \"margin_product_id\": \"BTC-USD\",\n                  \"status\": \"completed\",\n                  \"nonce\": 25\n                }\n\n        \"\"\"\n        params = {'margin_profile_id': margin_profile_id,\n                  'type': transfer_type,\n                  'currency': currency,  # example: USD\n                  'amount': amount}\n        return self._send_message('post', '/profiles/margin-transfer',\n                                  data=json.dumps(params))", "response": "Transfer funds between your standard profile and a margin profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close_position(self, repay_only):\n        params = {'repay_only': repay_only}\n        return self._send_message('post', '/position/close',\n                                  data=json.dumps(params))", "response": "Close the current position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwithdrawing funds to a payment method.", "response": "def withdraw(self, amount, currency, payment_method_id):\n        \"\"\" Withdraw funds to a payment method.\n\n        See AuthenticatedClient.get_payment_methods() to receive\n        information regarding payment methods.\n\n        Args:\n            amount (Decimal): The amount to withdraw.\n            currency (str): Currency type (eg. 'BTC')\n            payment_method_id (str): ID of the payment method.\n\n        Returns:\n            dict: Withdraw details. Example::\n                {\n                    \"id\":\"593533d2-ff31-46e0-b22e-ca754147a96a\",\n                    \"amount\": \"10.00\",\n                    \"currency\": \"USD\",\n                    \"payout_at\": \"2016-08-20T00:31:09Z\"\n                }\n\n        \"\"\"\n        params = {'amount': amount,\n                  'currency': currency,\n                  'payment_method_id': payment_method_id}\n        return self._send_message('post', '/withdrawals/payment-method',\n                                  data=json.dumps(params))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwithdraws funds to a coinbase account.", "response": "def coinbase_withdraw(self, amount, currency, coinbase_account_id):\n        \"\"\" Withdraw funds to a coinbase account.\n\n        You can move funds between your Coinbase accounts and your cbpro\n        trading accounts within your daily limits. Moving funds between\n        Coinbase and cbpro is instant and free.\n\n        See AuthenticatedClient.get_coinbase_accounts() to receive\n        information regarding your coinbase_accounts.\n\n        Args:\n            amount (Decimal): The amount to withdraw.\n            currency (str): The type of currency (eg. 'BTC')\n            coinbase_account_id (str): ID of the coinbase account.\n\n        Returns:\n            dict: Information about the deposit. Example::\n                {\n                    \"id\":\"593533d2-ff31-46e0-b22e-ca754147a96a\",\n                    \"amount\":\"10.00\",\n                    \"currency\": \"BTC\",\n                }\n\n        \"\"\"\n        params = {'amount': amount,\n                  'currency': currency,\n                  'coinbase_account_id': coinbase_account_id}\n        return self._send_message('post', '/withdrawals/coinbase-account',\n                                  data=json.dumps(params))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwithdraws funds to a crypto address.", "response": "def crypto_withdraw(self, amount, currency, crypto_address):\n        \"\"\" Withdraw funds to a crypto address.\n\n        Args:\n            amount (Decimal): The amount to withdraw\n            currency (str): The type of currency (eg. 'BTC')\n            crypto_address (str): Crypto address to withdraw to.\n\n        Returns:\n            dict: Withdraw details. Example::\n                {\n                    \"id\":\"593533d2-ff31-46e0-b22e-ca754147a96a\",\n                    \"amount\":\"10.00\",\n                    \"currency\": \"BTC\",\n                }\n\n        \"\"\"\n        params = {'amount': amount,\n                  'currency': currency,\n                  'crypto_address': crypto_address}\n        return self._send_message('post', '/withdrawals/crypto',\n                                  data=json.dumps(params))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new report of historic information about your account.", "response": "def create_report(self, report_type, start_date, end_date, product_id=None,\n                      account_id=None, report_format='pdf', email=None):\n        \"\"\" Create report of historic information about your account.\n\n        The report will be generated when resources are available. Report status\n        can be queried via `get_report(report_id)`.\n\n        Args:\n            report_type (str): 'fills' or 'account'\n            start_date (str): Starting date for the report in ISO 8601\n            end_date (str): Ending date for the report in ISO 8601\n            product_id (Optional[str]): ID of the product to generate a fills\n                report for. Required if account_type is 'fills'\n            account_id (Optional[str]): ID of the account to generate an account\n                report for. Required if report_type is 'account'.\n            report_format (Optional[str]): 'pdf' or 'csv'. Default is 'pdf'.\n            email (Optional[str]): Email address to send the report to.\n\n        Returns:\n            dict: Report details. Example::\n                {\n                    \"id\": \"0428b97b-bec1-429e-a94c-59232926778d\",\n                    \"type\": \"fills\",\n                    \"status\": \"pending\",\n                    \"created_at\": \"2015-01-06T10:34:47.000Z\",\n                    \"completed_at\": undefined,\n                    \"expires_at\": \"2015-01-13T10:35:47.000Z\",\n                    \"file_url\": undefined,\n                    \"params\": {\n                        \"start_date\": \"2014-11-01T00:00:00.000Z\",\n                        \"end_date\": \"2014-11-30T23:59:59.000Z\"\n                    }\n                }\n\n        \"\"\"\n        params = {'type': report_type,\n                  'start_date': start_date,\n                  'end_date': end_date,\n                  'format': report_format}\n        if product_id is not None:\n            params['product_id'] = product_id\n        if account_id is not None:\n            params['account_id'] = account_id\n        if email is not None:\n            params['email'] = email\n\n        return self._send_message('post', '/reports',\n                                  data=json.dumps(params))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_product_order_book(self, product_id, level=1):\n        params = {'level': level}\n        return self._send_message('get',\n                                  '/products/{}/book'.format(product_id),\n                                  params=params)", "response": "Get a list of open orders for a product."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the latest trades for a product. This method returns a generator which may make multiple HTTP requests while iterating through it. Args: product_id (str): Product before (Optional[str]): start time in ISO 8601 after (Optional[str]): end time in ISO 8601 limit (Optional[int]): the desired number of trades (can be more than 100, automatically paginated) results (Optional[list]): list of results that is used for the pagination Returns: list: Latest trades. Example:: [{ \"time\": \"2014-11-07T22:19:28.578544Z\", \"trade_id\": 74, \"price\": \"10.00000000\", \"size\": \"0.01000000\", \"side\": \"buy\" }, { \"time\": \"2014-11-07T01:08:43.642366Z\", \"trade_id\": 73, \"price\": \"100.00000000\", \"size\": \"0.01000000\", \"side\": \"sell\" }]", "response": "def get_product_trades(self, product_id, before='', after='', limit=None, result=None):\n        \"\"\"List the latest trades for a product.\n\n        This method returns a generator which may make multiple HTTP requests\n        while iterating through it.\n\n        Args:\n             product_id (str): Product\n             before (Optional[str]): start time in ISO 8601\n             after (Optional[str]): end time in ISO 8601\n             limit (Optional[int]): the desired number of trades (can be more than 100,\n                          automatically paginated)\n             results (Optional[list]): list of results that is used for the pagination\n        Returns:\n             list: Latest trades. Example::\n                 [{\n                     \"time\": \"2014-11-07T22:19:28.578544Z\",\n                     \"trade_id\": 74,\n                     \"price\": \"10.00000000\",\n                     \"size\": \"0.01000000\",\n                     \"side\": \"buy\"\n                 }, {\n                     \"time\": \"2014-11-07T01:08:43.642366Z\",\n                     \"trade_id\": 73,\n                     \"price\": \"100.00000000\",\n                     \"size\": \"0.01000000\",\n                     \"side\": \"sell\"\n         }]\n        \"\"\"\n        return self._send_paginated_message('/products/{}/trades'\n                                            .format(product_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_product_historic_rates(self, product_id, start=None, end=None,\n                                   granularity=None):\n        \"\"\"Historic rates for a product.\n\n        Rates are returned in grouped buckets based on requested\n        `granularity`. If start, end, and granularity aren't provided,\n        the exchange will assume some (currently unknown) default values.\n\n        Historical rate data may be incomplete. No data is published for\n        intervals where there are no ticks.\n\n        **Caution**: Historical rates should not be polled frequently.\n        If you need real-time information, use the trade and book\n        endpoints along with the websocket feed.\n\n        The maximum number of data points for a single request is 200\n        candles. If your selection of start/end time and granularity\n        will result in more than 200 data points, your request will be\n        rejected. If you wish to retrieve fine granularity data over a\n        larger time range, you will need to make multiple requests with\n        new start/end ranges.\n\n        Args:\n            product_id (str): Product\n            start (Optional[str]): Start time in ISO 8601\n            end (Optional[str]): End time in ISO 8601\n            granularity (Optional[int]): Desired time slice in seconds\n\n        Returns:\n            list: Historic candle data. Example:\n                [\n                    [ time, low, high, open, close, volume ],\n                    [ 1415398768, 0.32, 4.2, 0.35, 4.2, 12.3 ],\n                    ...\n                ]\n\n        \"\"\"\n        params = {}\n        if start is not None:\n            params['start'] = start\n        if end is not None:\n            params['end'] = end\n        if granularity is not None:\n            acceptedGrans = [60, 300, 900, 3600, 21600, 86400]\n            if granularity not in acceptedGrans:\n                raise ValueError( 'Specified granularity is {}, must be in approved values: {}'.format(\n                        granularity, acceptedGrans) )\n\n            params['granularity'] = granularity\n        return self._send_message('get',\n                                  '/products/{}/candles'.format(product_id),\n                                  params=params)", "response": "This method returns a list of candle data for a specific product."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a request to the specified endpoint and return the JSON response.", "response": "def _send_message(self, method, endpoint, params=None, data=None):\n        \"\"\"Send API request.\n\n        Args:\n            method (str): HTTP method (get, post, delete, etc.)\n            endpoint (str): Endpoint (to be added to base URL)\n            params (Optional[dict]): HTTP request parameters\n            data (Optional[str]): JSON-encoded string payload for POST\n\n        Returns:\n            dict/list: JSON response\n\n        \"\"\"\n        url = self.url + endpoint\n        r = self.session.request(method, url, params=params, data=data,\n                                 auth=self.auth, timeout=30)\n        return r.json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a paginated API message to the given endpoint.", "response": "def _send_paginated_message(self, endpoint, params=None):\n        \"\"\" Send API message that results in a paginated response.\n\n        The paginated responses are abstracted away by making API requests on\n        demand as the response is iterated over.\n\n        Paginated API messages support 3 additional parameters: `before`,\n        `after`, and `limit`. `before` and `after` are mutually exclusive. To\n        use them, supply an index value for that endpoint (the field used for\n        indexing varies by endpoint - get_fills() uses 'trade_id', for example).\n            `before`: Only get data that occurs more recently than index\n            `after`: Only get data that occurs further in the past than index\n            `limit`: Set amount of data per HTTP response. Default (and\n                maximum) of 100.\n\n        Args:\n            endpoint (str): Endpoint (to be added to base URL)\n            params (Optional[dict]): HTTP request parameters\n\n        Yields:\n            dict: API response objects\n\n        \"\"\"\n        if params is None:\n            params = dict()\n        url = self.url + endpoint\n        while True:\n            r = self.session.get(url, params=params, auth=self.auth, timeout=30)\n            results = r.json()\n            for result in results:\n                yield result\n            # If there are no more pages, we're done. Otherwise update `after`\n            # param to get next page.\n            # If this request included `before` don't get any more pages - the\n            # cbpro API doesn't support multiple pages in that case.\n            if not r.headers.get('cb-after') or \\\n                    params.get('before') is not None:\n                break\n            else:\n                params['after'] = r.headers['cb-after']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_params(params):\n    # Collect a set of all fields\n    fields = set()\n    for p in params:\n        fields.update(p)\n    fields = sorted(fields)\n\n    params2 = list(pluck(fields, params, MISSING))\n    # Non-basic types (including MISSING) are unique to their id\n    tokens = [\n        tuple(x if isinstance(x, (int, float, str)) else id(x) for x in p)\n        for p in params2\n    ]\n\n    return fields, tokens, params2", "response": "Take a list of dictionaries and tokenize and normalize."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_n_splits(cv, X, y=None, groups=None):\n    if not any(is_dask_collection(i) for i in (X, y, groups)):\n        return cv.get_n_splits(X, y, groups)\n\n    if isinstance(cv, (_BaseKFold, BaseShuffleSplit)):\n        return cv.n_splits\n\n    elif isinstance(cv, PredefinedSplit):\n        return len(cv.unique_folds)\n\n    elif isinstance(cv, _CVIterableWrapper):\n        return len(cv.cv)\n\n    elif isinstance(cv, (LeaveOneOut, LeavePOut)) and not is_dask_collection(X):\n        # Only `X` is referenced for these classes\n        return cv.get_n_splits(X, None, None)\n\n    elif isinstance(cv, (LeaveOneGroupOut, LeavePGroupsOut)) and not is_dask_collection(\n        groups\n    ):\n        # Only `groups` is referenced for these classes\n        return cv.get_n_splits(None, None, groups)\n\n    else:\n        return delayed(cv).get_n_splits(X, y, groups).compute()", "response": "Compute the number of splits in a base cross validator."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders the task graph for this parameter search using graphviz.", "response": "def visualize(self, filename=\"mydask\", format=None, **kwargs):\n        \"\"\"Render the task graph for this parameter search using ``graphviz``.\n\n        Requires ``graphviz`` to be installed.\n\n        Parameters\n        ----------\n        filename : str or None, optional\n            The name (without an extension) of the file to write to disk.  If\n            `filename` is None, no file will be written, and we communicate\n            with dot using only pipes.\n        format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n            Format in which to write output file.  Default is 'png'.\n        **kwargs\n            Additional keyword arguments to forward to\n            ``dask.dot.to_graphviz``.\n\n        Returns\n        -------\n        result : IPython.diplay.Image, IPython.display.SVG, or None\n            See ``dask.dot.dot_graph`` for more information.\n        \"\"\"\n        check_is_fitted(self, \"dask_graph_\")\n        return dask.visualize(\n            self.dask_graph_, filename=filename, format=format, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfitting the estimator with all sets of parameters and return the result.", "response": "def fit(self, X, y=None, groups=None, **fit_params):\n        \"\"\"Run fit with all sets of parameters.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n        groups : array-like, shape = [n_samples], optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        **fit_params\n            Parameters passed to the ``fit`` method of the estimator\n        \"\"\"\n        estimator = self.estimator\n        from sklearn.metrics.scorer import _check_multimetric_scoring\n\n        scorer, multimetric = _check_multimetric_scoring(\n            estimator, scoring=self.scoring\n        )\n        if not multimetric:\n            scorer = scorer[\"score\"]\n        self.multimetric_ = multimetric\n\n        if self.multimetric_:\n            if self.refit is not False and (\n                not isinstance(self.refit, str)\n                # This will work for both dict / list (tuple)\n                or self.refit not in scorer\n            ):\n                raise ValueError(\n                    \"For multi-metric scoring, the parameter \"\n                    \"refit must be set to a scorer key \"\n                    \"to refit an estimator with the best \"\n                    \"parameter setting on the whole data and \"\n                    \"make the best_* attributes \"\n                    \"available for that metric. If this is \"\n                    \"not needed, refit should be set to \"\n                    \"False explicitly. %r was .\"\n                    \"passed.\" % self.refit\n                )\n        self.scorer_ = scorer\n\n        error_score = self.error_score\n        if not (isinstance(error_score, numbers.Number) or error_score == \"raise\"):\n            raise ValueError(\n                \"error_score must be the string 'raise' or a\" \" numeric value.\"\n            )\n\n        dsk, keys, n_splits = build_graph(\n            estimator,\n            self.cv,\n            self.scorer_,\n            list(self._get_param_iterator()),\n            X,\n            y,\n            groups,\n            fit_params,\n            iid=self.iid,\n            refit=self.refit,\n            error_score=error_score,\n            return_train_score=self.return_train_score,\n            cache_cv=self.cache_cv,\n            multimetric=multimetric,\n        )\n        self.dask_graph_ = dsk\n        self.n_splits_ = n_splits\n\n        n_jobs = _normalize_n_jobs(self.n_jobs)\n        scheduler = dask.base.get_scheduler(scheduler=self.scheduler)\n\n        if not scheduler:\n            scheduler = dask.threaded.get\n        if scheduler is dask.threaded.get and n_jobs == 1:\n            scheduler = dask.local.get_sync\n\n        out = scheduler(dsk, keys, num_workers=n_jobs)\n\n        results = handle_deprecated_train_score(out[0], self.return_train_score)\n        self.cv_results_ = results\n\n        if self.refit:\n            if self.multimetric_:\n                key = self.refit\n            else:\n                key = \"score\"\n            self.best_index_ = np.flatnonzero(results[\"rank_test_{}\".format(key)] == 1)[\n                0\n            ]\n\n            self.best_estimator_ = out[1]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an iterator for the given distributions", "response": "def _get_param_iterator(self):\n        \"\"\"Return ParameterSampler instance for the given distributions\"\"\"\n        return model_selection.ParameterSampler(\n            self.param_distributions, self.n_iter, random_state=self.random_state\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls partial_fit on a classifiers with training data X and y Arguments --------- model_and_meta : Tuple[Estimator, dict] X, y : np.ndarray, np.ndarray Training data fit_params : dict Extra keyword arguments to pass to partial_fit Returns ------- Results A namedtuple with four fields: info, models, history, best * info : Dict[model_id, List[Dict]] Keys are integers identifying each model. Values are a List of Dict * models : Dict[model_id, Future[Estimator]] A dictionary with the same keys as `info`. The values are futures to the fitted models. * history : List[Dict] The history of model fitting for each model. Each element of the list is a dictionary with the following elements: * model_id : int A superset of the keys for `info` and `models`. * params : Dict[str, Any] Parameters this model was trained with. * partial_fit_calls : int The number of *consecutive* partial fit calls at this stage in this models training history. * partial_fit_time : float Time (in seconds) spent on this partial fit * score : float Score on the test set for the model at this point in history * score_time : float Time (in seconds) spent on this scoring. * best : Tuple[model_id, Future[Estimator]]] The estimator with the highest validation score in the final round.", "response": "def _partial_fit(model_and_meta, X, y, fit_params):\n    \"\"\"\n    Call partial_fit on a classifiers with training data X and y\n\n    Arguments\n    ---------\n    model_and_meta : Tuple[Estimator, dict]\n    X, y : np.ndarray, np.ndarray\n        Training data\n    fit_params : dict\n        Extra keyword arguments to pass to partial_fit\n\n    Returns\n    -------\n    Results\n        A namedtuple with four fields: info, models, history, best\n\n        * info : Dict[model_id, List[Dict]]\n            Keys are integers identifying each model. Values are a\n            List of Dict\n        * models : Dict[model_id, Future[Estimator]]\n            A dictionary with the same keys as `info`. The values\n            are futures to the fitted models.\n        * history : List[Dict]\n            The history of model fitting for each model. Each element\n            of the list is a dictionary with the following elements:\n\n            * model_id : int\n                A superset of the keys for `info` and `models`.\n            * params : Dict[str, Any]\n                Parameters this model was trained with.\n            * partial_fit_calls : int\n                The number of *consecutive* partial fit calls at this stage in\n                this models training history.\n            * partial_fit_time : float\n                Time (in seconds) spent on this partial fit\n            * score : float\n                Score on the test set for the model at this point in history\n            * score_time : float\n                Time (in seconds) spent on this scoring.\n        * best : Tuple[model_id, Future[Estimator]]]\n            The estimator with the highest validation score in the final\n            round.\n    \"\"\"\n    with log_errors():\n        start = time()\n        model, meta = model_and_meta\n\n        if len(X):\n            model = deepcopy(model)\n            model.partial_fit(X, y, **(fit_params or {}))\n\n        meta = dict(meta)\n        meta[\"partial_fit_calls\"] += 1\n        meta[\"partial_fit_time\"] = time() - start\n\n        return model, meta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_model(model, ident, **params):\n    with log_errors(pdb=True):\n        model = clone(model).set_params(**params)\n        return model, {\"model_id\": ident, \"params\": params, \"partial_fit_calls\": 0}", "response": "Create a new model by cloning and setting params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(\n    model,\n    params,\n    X_train,\n    y_train,\n    X_test,\n    y_test,\n    additional_calls,\n    fit_params=None,\n    scorer=None,\n    random_state=None,\n):\n    \"\"\" Find a good model and search among a space of hyper-parameters\n\n    This does a hyper-parameter search by creating many models and then fitting\n    them incrementally on batches of data and reducing the number of models based\n    on the scores computed during training.  Over time fewer and fewer models\n    remain.  We train these models for increasingly long times.\n\n    The model, number of starting parameters, and decay can all be provided as\n    configuration parameters.\n\n    Training data should be given as Dask arrays.  It can be large.  Testing\n    data should be given either as a small dask array or as a numpy array.  It\n    should fit on a single worker.\n\n    Parameters\n    ----------\n    model : Estimator\n    params : List[Dict]\n        Parameters to start training on model\n    X_train : dask Array\n    y_train : dask Array\n    X_test : Array\n        Numpy array or small dask array.  Should fit in single node's memory.\n    y_test : Array\n        Numpy array or small dask array.  Should fit in single node's memory.\n    additional_calls : callable\n        A function that takes information about scoring history per model and\n        returns the number of additional partial fit calls to run on each model\n    fit_params : dict\n        Extra parameters to give to partial_fit\n    scorer : callable\n        A scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n    random_state : int, RandomState instance or None, optional, default: None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from dask_ml.datasets import make_classification\n    >>> X, y = make_classification(n_samples=5000000, n_features=20,\n    ...                            chunks=100000, random_state=0)\n\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> model = SGDClassifier(tol=1e-3, penalty='elasticnet', random_state=0)\n\n    >>> from sklearn.model_selection import ParameterSampler\n    >>> params = {'alpha': np.logspace(-2, 1, num=1000),\n    ...           'l1_ratio': np.linspace(0, 1, num=1000),\n    ...           'average': [True, False]}\n    >>> params = list(ParameterSampler(params, 10, random_state=0))\n\n    >>> X_test, y_test = X[:100000], y[:100000]\n    >>> X_train = X[100000:]\n    >>> y_train = y[100000:]\n\n    >>> def remove_worst(scores):\n    ...    last_score = {model_id: info[-1]['score']\n    ...                  for model_id, info in scores.items()}\n    ...    worst_score = min(last_score.values())\n    ...    out = {}\n    ...    for model_id, score in last_score.items():\n    ...        if score != worst_score:\n    ...            out[model_id] = 1  # do one more training step\n    ...    if len(out) == 1:\n    ...        out = {k: 0 for k in out}  # no more work to do, stops execution\n    ...    return out\n\n    >>> from dask.distributed import Client\n    >>> client = Client(processes=False)\n\n    >>> from dask_ml.model_selection._incremental import fit\n    >>> info, models, history, best = fit(model, params,\n    ...                                   X_train, y_train,\n    ...                                   X_test, y_test,\n    ...                                   additional_calls=remove_worst,\n    ...                                   fit_params={'classes': [0, 1]},\n    ...                                   random_state=0)\n\n    >>> models\n    {2: <Future: status: finished, type: SGDClassifier, key: ...}\n    >>> models[2].result()\n    SGDClassifier(...)\n    >>> info[2][-1]  # doctest: +SKIP\n    {'model_id': 2,\n     'params': {'l1_ratio': 0.9529529529529529, 'average': False,\n                'alpha': 0.014933932161242525},\n     'partial_fit_calls': 8,\n     'partial_fit_time': 0.17334818840026855,\n     'score': 0.58765,\n     'score_time': 0.031442880630493164}\n\n    Returns\n    -------\n    info : Dict[int, List[Dict]]\n        Scoring history of each successful model, keyed by model ID.\n        This has the parameters, scores, and timing information over time\n    models : Dict[int, Future]\n        Dask futures pointing to trained models\n    history : List[Dict]\n        A history of all models scores over time\n    \"\"\"\n    return default_client().sync(\n        _fit,\n        model,\n        params,\n        X_train,\n        y_train,\n        X_test,\n        y_test,\n        additional_calls,\n        fit_params=fit_params,\n        scorer=scorer,\n        random_state=random_state,\n    )", "response": "Find a good model and search among a space of hyper - parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the data arguments X and y.", "response": "def _check_array(self, X, **kwargs):\n        \"\"\"Validate the data arguments X and y.\n\n        By default, NumPy arrays are converted to 1-block dask arrays.\n\n        Parameters\n        ----------\n        X, y : array-like\n        \"\"\"\n        if isinstance(X, np.ndarray):\n            X = da.from_array(X, X.shape)\n        X = check_array(X, **kwargs)\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfit the model to the data.", "response": "def fit(self, X, y, **fit_params):\n        \"\"\"Find the best parameters for a particular model.\n\n        Parameters\n        ----------\n        X, y : array-like\n        **fit_params\n            Additional partial fit keyword arguments for the estimator.\n        \"\"\"\n        return default_client().sync(self._fit, X, y, **fit_params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the first block from a dask object", "response": "def _first_block(dask_object):\n    \"\"\"Extract the first block / partition from a dask object\n    \"\"\"\n    if isinstance(dask_object, da.Array):\n        if dask_object.ndim > 1 and dask_object.numblocks[-1] != 1:\n            raise NotImplementedError(\n                \"IID estimators require that the array \"\n                \"blocked only along the first axis. \"\n                \"Rechunk your array before fitting.\"\n            )\n        shape = (dask_object.chunks[0][0],)\n        if dask_object.ndim > 1:\n            shape = shape + (dask_object.chunks[1][0],)\n\n        return da.from_delayed(\n            dask_object.to_delayed().flatten()[0], shape, dask_object.dtype\n        )\n\n    if isinstance(dask_object, dd._Frame):\n        return dask_object.get_partition(0)\n\n    else:\n        return dask_object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_array(self, X):\n        if isinstance(X, da.Array):\n            if X.ndim == 2 and X.numblocks[1] > 1:\n                logger.debug(\"auto-rechunking 'X'\")\n                if not np.isnan(X.chunks[0]).any():\n                    X = X.rechunk({0: \"auto\", 1: -1})\n                else:\n                    X = X.rechunk({1: -1})\n        return X", "response": "Validate an array for post - fit tasks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self, X, y=None, **kwargs):\n        logger.info(\"Starting fit\")\n        with _timer(\"fit\", _logger=logger):\n            result = self.estimator.fit(X, y, **kwargs)\n\n        # Copy over learned attributes\n        copy_learned_attributes(result, self)\n        copy_learned_attributes(result, self.estimator)\n        return self", "response": "Fit the underlying estimator and return the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms block - wise for dask inputs.", "response": "def transform(self, X):\n        \"\"\"Transform block or partition-wise for dask inputs.\n\n        For dask inputs, a dask array or dataframe is returned. For other\n        inputs (NumPy array, pandas dataframe, scipy sparse matrix), the\n        regular return value is returned.\n\n        If the underlying estimator does not have a ``transform`` method, then\n        an ``AttributeError`` is raised.\n\n        Parameters\n        ----------\n        X : array-like\n\n        Returns\n        -------\n        transformed : array-like\n        \"\"\"\n        self._check_method(\"transform\")\n        X = self._check_array(X)\n\n        if isinstance(X, da.Array):\n            return X.map_blocks(_transform, estimator=self._postfit_estimator)\n        elif isinstance(X, dd._Frame):\n            return X.map_partitions(_transform, estimator=self._postfit_estimator)\n        else:\n            return _transform(X, estimator=self._postfit_estimator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the score on the given data.", "response": "def score(self, X, y, compute=True):\n        \"\"\"Returns the score on the given data.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        Returns\n        -------\n        score : float\n                return self.estimator.score(X, y)\n        \"\"\"\n        scoring = self.scoring\n        X = self._check_array(X)\n        y = self._check_array(y)\n\n        if not scoring:\n            if type(self._postfit_estimator).score == sklearn.base.RegressorMixin.score:\n                scoring = \"r2\"\n            elif (\n                type(self._postfit_estimator).score\n                == sklearn.base.ClassifierMixin.score\n            ):\n                scoring = \"accuracy\"\n        else:\n            scoring = self.scoring\n\n        if scoring:\n            if not dask.is_dask_collection(X) and not dask.is_dask_collection(y):\n                scorer = sklearn.metrics.get_scorer(scoring)\n            else:\n                scorer = get_scorer(scoring, compute=compute)\n            return scorer(self, X, y)\n        else:\n            return self._postfit_estimator.score(X, y)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npredict for X. For dask inputs, a dask array or dataframe is returned. For other inputs (NumPy array, pandas dataframe, scipy sparse matrix), the regular return value is returned. Parameters ---------- X : array-like Returns ------- y : array-like", "response": "def predict(self, X):\n        \"\"\"Predict for X.\n\n        For dask inputs, a dask array or dataframe is returned. For other\n        inputs (NumPy array, pandas dataframe, scipy sparse matrix), the\n        regular return value is returned.\n\n        Parameters\n        ----------\n        X : array-like\n\n        Returns\n        -------\n        y : array-like\n        \"\"\"\n        self._check_method(\"predict\")\n        X = self._check_array(X)\n\n        if isinstance(X, da.Array):\n            result = X.map_blocks(\n                _predict, dtype=\"int\", estimator=self._postfit_estimator, drop_axis=1\n            )\n            return result\n\n        elif isinstance(X, dd._Frame):\n            return X.map_partitions(\n                _predict, estimator=self._postfit_estimator, meta=np.array([1])\n            )\n\n        else:\n            return _predict(X, estimator=self._postfit_estimator)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npredicting probability estimates for a set of items in X.", "response": "def predict_proba(self, X):\n        \"\"\"Probability estimates.\n\n        For dask inputs, a dask array or dataframe is returned. For other\n        inputs (NumPy array, pandas dataframe, scipy sparse matrix), the\n        regular return value is returned.\n\n        If the underlying estimator does not have a ``predict_proba``\n        method, then an ``AttributeError`` is raised.\n\n        Parameters\n        ----------\n        X : array or dataframe\n\n        Returns\n        -------\n        y : array-like\n        \"\"\"\n        X = self._check_array(X)\n\n        self._check_method(\"predict_proba\")\n\n        if isinstance(X, da.Array):\n            # XXX: multiclass\n            return X.map_blocks(\n                _predict_proba,\n                estimator=self._postfit_estimator,\n                dtype=\"float\",\n                chunks=(X.chunks[0], len(self._postfit_estimator.classes_)),\n            )\n        elif isinstance(X, dd._Frame):\n            return X.map_partitions(_predict_proba, estimator=self._postfit_estimator)\n        else:\n            return _predict_proba(X, estimator=self._postfit_estimator)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging of proability estimates for the given data set X.", "response": "def predict_log_proba(self, X):\n        \"\"\"Log of proability estimates.\n\n        For dask inputs, a dask array or dataframe is returned. For other\n        inputs (NumPy array, pandas dataframe, scipy sparse matrix), the\n        regular return value is returned.\n\n        If the underlying estimator does not have a ``predict_proba``\n        method, then an ``AttributeError`` is raised.\n\n        Parameters\n        ----------\n        X : array or dataframe\n\n        Returns\n        -------\n        y : array-like\n        \"\"\"\n        self._check_method(\"predict_log_proba\")\n        return da.log(self.predict_proba(X))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_method(self, method):\n        estimator = self._postfit_estimator\n        if not hasattr(estimator, method):\n            msg = \"The wrapped estimator '{}' does not have a '{}' method.\".format(\n                estimator, method\n            )\n            raise AttributeError(msg)\n        return getattr(estimator, method)", "response": "Check if self. estimator has method. Raises AttributeError if not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partial_fit(self, X, y=None, **fit_kwargs):\n        estimator = getattr(self, \"estimator_\", None)\n        if estimator is None:\n            estimator = sklearn.base.clone(self.estimator)\n        return self._fit_for_estimator(estimator, X, y, **fit_kwargs)", "response": "Fit the underlying estimator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfit the model on the training data X y.", "response": "def fit(self, X, y=None):\n        \"\"\"Fit the model on the training data\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n        y : array-like, shape (n_samples,)\n\n        Returns\n        -------\n        self : objectj\n        \"\"\"\n        X = self._check_array(X)\n\n        solver_kwargs = self._get_solver_kwargs()\n\n        self._coef = algorithms._solvers[self.solver](X, y, **solver_kwargs)\n        if self.fit_intercept:\n            self.coef_ = self._coef[:-1]\n            self.intercept_ = self._coef[-1]\n        else:\n            self.coef_ = self._coef\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npredicts the probability of samples in X.", "response": "def predict_proba(self, X):\n        \"\"\"Probability estimates for samples in X.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Returns\n        -------\n        T : array-like, shape = [n_samples, n_classes]\n            The probability of the sample for each class in the model.\n        \"\"\"\n        X_ = self._check_array(X)\n        return sigmoid(dot(X_, self._coef))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, X):\n        X_ = self._check_array(X)\n        return exp(dot(X_, self._coef))", "response": "Predict count for samples in X."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logsumexp(arr, axis=0):\n    if axis == 0:\n        pass\n    elif axis == 1:\n        arr = arr.T\n    else:\n        raise NotImplementedError\n    # Use the max to normalize, as with the log this is what accumulates\n    # the less errors\n    vmax = arr.max(axis=0)\n    out = da.log(da.sum(da.exp(arr - vmax), axis=0))\n    out += vmax\n    return out", "response": "Computes the sum of arr assuming arr is in the log domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms classification on an array of test vectors X and return the predicted target values for X.", "response": "def predict(self, X):\n        \"\"\"\n        Perform classification on an array of test vectors X.\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n        Returns\n        -------\n        C : array, shape = [n_samples]\n            Predicted target values for X\n        \"\"\"\n        jll = self._joint_log_likelihood(X)\n        return delayed(self.classes_)[da.argmax(jll, axis=1)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict_log_proba(self, X):\n        jll = self._joint_log_likelihood(X)\n        # normalize by P(x) = P(f_1, ..., f_n)\n        log_prob_x = logsumexp(jll, axis=1)\n        return jll - log_prob_x.reshape(-1, 1)", "response": "Predicts the log - probability of the test vector X."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchoosing the initial centers for K - Means.", "response": "def k_init(\n    X,\n    n_clusters,\n    init=\"k-means||\",\n    random_state=None,\n    max_iter=None,\n    oversampling_factor=2,\n):\n    \"\"\"Choose the initial centers for K-Means.\n\n    Parameters\n    ----------\n    X : da.Array (n_samples, n_features)\n    n_clusters : int\n        Number of clusters to end up with\n    init : {'k-means||', 'k-means++', 'random'} or numpy.ndarray\n        Initialization method, or pass a NumPy array to use\n    random_state : int, optional\n    max_iter : int, optional\n        Only used for ``init='k-means||'``.\n    oversampling_factor : int, optional\n        Only used for ``init='k-means||`''. Controls the additional number of\n        candidate centers in each iteration.\n\n    Return\n    ------\n    centers : np.ndarray (n_clusters, n_features)\n\n    Notes\n    -----\n    The default strategy is ``k-means||``, which tends to be slower than\n    ``k-means++`` for small (in-memory) datasets, but works better in a\n    distributed setting.\n\n    .. warning::\n\n       Using ``init='k-means++'`` assumes that the entire dataset fits\n       in RAM.\n    \"\"\"\n    if isinstance(init, np.ndarray):\n        K, P = init.shape\n\n        if K != n_clusters:\n            msg = (\n                \"Number of centers in provided 'init' ({}) does \"\n                \"not match 'n_clusters' ({})\"\n            )\n            raise ValueError(msg.format(K, n_clusters))\n\n        if P != X.shape[1]:\n            msg = (\n                \"Number of features in the provided 'init' ({}) do not \"\n                \"match the number of features in 'X'\"\n            )\n            raise ValueError(msg.format(P, X.shape[1]))\n\n        return init\n\n    elif not isinstance(init, str):\n        raise TypeError(\"'init' must be an array or str, got {}\".format(type(init)))\n\n    valid = {\"k-means||\", \"k-means++\", \"random\"}\n\n    if isinstance(random_state, Integral) or random_state is None:\n        if init == \"k-means||\":\n            random_state = da.random.RandomState(random_state)\n        else:\n            random_state = np.random.RandomState(random_state)\n\n    if init == \"k-means||\":\n        return init_scalable(X, n_clusters, random_state, max_iter, oversampling_factor)\n    elif init == \"k-means++\":\n        return init_pp(X, n_clusters, random_state)\n\n    elif init == \"random\":\n        return init_random(X, n_clusters, random_state)\n\n    else:\n        raise ValueError(\"'init' must be one of {}, got {}\".format(valid, init))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the k - means or cost cluster and cluster.", "response": "def init_scalable(\n    X, n_clusters, random_state=None, max_iter=None, oversampling_factor=2\n):\n    \"\"\"K-Means initialization using k-means||\n\n    This is algorithm 2 in Scalable K-Means++ (2012).\n    \"\"\"\n\n    logger.info(\"Initializing with k-means||\")\n    # Step 1: Initialize Centers\n    idx = 0\n    centers = da.compute(X[idx, np.newaxis])[0]\n    c_idx = {idx}\n\n    # Step 2: Initialize cost\n    cost, = compute(evaluate_cost(X, centers))\n\n    if cost == 0:\n        n_iter = 0\n    else:\n        n_iter = int(np.round(np.log(cost)))\n\n    if max_iter is not None:\n        n_iter = min(max_iter, n_iter)\n\n    # Steps 3 - 6: update candidate Centers\n    for i in range(n_iter):\n        with _timer(\n            \"init iteration %2d/%2d , %2d centers\" % (i + 1, n_iter, len(c_idx)),\n            _logger=logger,\n        ):\n            new_idxs = _sample_points(X, centers, oversampling_factor, random_state)\n            new_idxs = set(*compute(new_idxs))\n            c_idx |= new_idxs\n\n        # Sort before slicing, for better performance / memory\n        # usage with the scheduler.\n        # See https://github.com/dask/dask-ml/issues/39\n        centers = X[sorted(c_idx)].compute()\n\n    # XXX: scikit-learn doesn't have weighted k-means.\n    # The paper weights each center by the number of points closest to it.\n    # https://stackoverflow.com/a/37198799/1889400 claims you can scale the\n    # features before clustering, but that doesn't seem right.\n    # I think that replicating the *points*, proportional to the number of\n    # original points closest to the candidate centers, would be a better way\n    # to do that.\n\n    if len(centers) < n_clusters:\n        logger.warning(\"Found fewer than %d clusters in init.\", n_clusters)\n        # supplement with random\n        need = n_clusters - len(centers)\n        locs = sorted(\n            random_state.choice(\n                np.arange(0, len(X)), size=need, replace=False, chunks=len(X)\n            )\n        )\n        extra = X[locs].compute()\n        return np.vstack([centers, extra])\n    else:\n        # Step 7, 8 without weights\n        # dask RandomState objects aren't valid for scikit-learn\n        rng2 = (\n            random_state.randint(0, np.iinfo(\"i4\").max - 1, chunks=())\n            .compute(scheduler=\"single-threaded\")\n            .item()\n        )\n        km = sk_k_means.KMeans(n_clusters, random_state=rng2)\n        km.fit(centers)\n\n    return km.cluster_centers_"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sample_points(X, centers, oversampling_factor, random_state):\n    # re-implement evaluate_cost here, to avoid redundant computation\n    distances = pairwise_distances(X, centers).min(1) ** 2\n    denom = distances.sum()\n    p = oversampling_factor * distances / denom\n\n    draws = random_state.uniform(size=len(p), chunks=p.chunks)\n    picked = p > draws\n\n    new_idxs, = da.where(picked)\n    return new_idxs", "response": "r Sample points independently with probability\nWorkItem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npredict the closest cluster each sample in X belongs to.", "response": "def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            New data to predict.\n\n        Returns\n        -------\n        labels : array, shape [n_samples,]\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self, \"cluster_centers_\")\n        X = self._check_array(X)\n        labels = pairwise_distances_argmin_min(X, self.cluster_centers_)[0].astype(\n            np.int32\n        )\n        return labels"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncenter and scale the data.", "response": "def transform(self, X):\n        \"\"\"Center and scale the data.\n\n        Can be called on sparse input, provided that ``RobustScaler`` has been\n        fitted to dense input and ``with_centering=False``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}\n            The data used to scale along the specified axis.\n\n        This implementation was copied and modified from Scikit-Learn.\n\n        See License information here:\n        https://github.com/scikit-learn/scikit-learn/blob/master/README.rst\n        \"\"\"\n        if self.with_centering:\n            check_is_fitted(self, \"center_\")\n        if self.with_scaling:\n            check_is_fitted(self, \"scale_\")\n        X = self._check_array(X, self.copy)\n\n        # if sparse.issparse(X):\n        #     if self.with_scaling:\n        #         inplace_column_scale(X, 1.0 / self.scale_)\n        # else:\n        if self.with_centering:\n            X -= self.center_\n        if self.with_scaling:\n            X /= self.scale_\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscale back the data to the original representation.", "response": "def inverse_transform(self, X):\n        \"\"\"Scale back the data to the original representation\n\n        Parameters\n        ----------\n        X : array-like\n            The data used to scale along the specified axis.\n\n        This implementation was copied and modified from Scikit-Learn.\n\n        See License information here:\n        https://github.com/scikit-learn/scikit-learn/blob/master/README.rst\n        \"\"\"\n        check_is_fitted(self, \"center_\", \"scale_\")\n\n        # if sparse.issparse(X):\n        #     if self.with_scaling:\n        #         inplace_column_scale(X, self.scale_)\n        # else:\n        if self.with_scaling:\n            X *= self.scale_\n        if self.with_centering:\n            X += self.center_\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X, y=None):\n        X = self._check_array(X)\n\n        if self.categories is not None:\n            # some basic validation\n            columns = pd.Index(self.categories)\n            categories = self.categories\n\n        elif isinstance(X, pd.DataFrame):\n            columns, categories = self._fit(X)\n        else:\n            columns, categories = self._fit_dask(X)\n\n        self.columns_ = columns\n        self.categories_ = categories\n        return self", "response": "Fit the categorical columns."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform the columns in X according to self. categories_.", "response": "def transform(self, X, y=None):\n        \"\"\"Transform the columns in ``X`` according to ``self.categories_``.\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or dask.DataFrame\n        y : ignored\n\n        Returns\n        -------\n        X_trn : pandas.DataFrame or dask.DataFrame\n            Same type as the input. The columns in ``self.categories_`` will\n            be converted to categorical dtype.\n        \"\"\"\n        check_is_fitted(self, \"categories_\")\n        X = self._check_array(X).copy()\n        categories = self.categories_\n\n        for k, dtype in categories.items():\n            if _HAS_CTD:\n                if not isinstance(dtype, pd.api.types.CategoricalDtype):\n                    dtype = pd.api.types.CategoricalDtype(*dtype)\n                X[k] = X[k].astype(dtype)\n            else:\n                cat, ordered = dtype\n                X[k] = X[k].astype(\"category\").cat.set_categories(cat, ordered)\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X, y=None):\n        self.columns_ = X.columns\n        columns = self.columns\n        if columns is None:\n            columns = X.select_dtypes(include=[\"category\"]).columns\n        else:\n            for column in columns:\n                assert is_categorical_dtype(X[column]), \"Must be categorical\"\n\n        self.categorical_columns_ = columns\n        self.non_categorical_columns_ = X.columns.drop(self.categorical_columns_)\n\n        if _HAS_CTD:\n            self.dtypes_ = {col: X[col].dtype for col in self.categorical_columns_}\n        else:\n            self.dtypes_ = {\n                col: (X[col].cat.categories, X[col].cat.ordered)\n                for col in self.categorical_columns_\n            }\n\n        left = len(self.non_categorical_columns_)\n        self.categorical_blocks_ = {}\n        for col in self.categorical_columns_:\n            right = left + len(X[col].cat.categories)\n            if self.drop_first:\n                right -= 1\n            self.categorical_blocks_[col], left = slice(left, right), right\n\n        if isinstance(X, pd.DataFrame):\n            sample = X.iloc[:1]\n        else:\n            sample = X._meta_nonempty\n\n        self.transformed_columns_ = pd.get_dummies(\n            sample, drop_first=self.drop_first\n        ).columns\n        return self", "response": "Fits the DataFrame to get the categorical columns that are dummy encoded."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inverse_transform(self, X):\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X, columns=self.transformed_columns_)\n\n        elif isinstance(X, da.Array):\n            # later on we concat(..., axis=1), which requires\n            # known divisions. Suboptimal, but I think unavoidable.\n            unknown = np.isnan(X.chunks[0]).any()\n            if unknown:\n                lengths = blockwise(len, \"i\", X[:, 0], \"i\", dtype=\"i8\").compute()\n                X = X.copy()\n                chunks = (tuple(lengths), X.chunks[1])\n                X._chunks = chunks\n\n            X = dd.from_dask_array(X, columns=self.transformed_columns_)\n\n        big = isinstance(X, dd.DataFrame)\n\n        if big:\n            chunks = np.array(X.divisions)\n            chunks[-1] = chunks[-1] + 1\n            chunks = tuple(chunks[1:] - chunks[:-1])\n\n        non_cat = X[list(self.non_categorical_columns_)]\n\n        cats = []\n        for col in self.categorical_columns_:\n            slice_ = self.categorical_blocks_[col]\n            if _HAS_CTD:\n                dtype = self.dtypes_[col]\n                categories, ordered = dtype.categories, dtype.ordered\n            else:\n                categories, ordered = self.dtypes_[col]\n\n            # use .values to avoid warning from pandas\n            cols_slice = list(X.columns[slice_])\n            if big:\n                inds = X[cols_slice].to_dask_array(lengths=chunks)\n            else:\n                inds = X[cols_slice].values\n            codes = inds.argmax(1)\n\n            if self.drop_first:\n                codes += 1\n                codes[(inds == 0).all(1)] = 0\n\n            if big:\n                # dask\n                codes._chunks = (chunks,)\n                # Need a Categorical.from_codes for dask\n                series = (\n                    dd.from_dask_array(codes, columns=col)\n                    .astype(\"category\")\n                    .cat.set_categories(np.arange(len(categories)), ordered=ordered)\n                    .cat.rename_categories(categories)\n                )\n                # Bug in pandas <= 0.20.3 lost name\n                if series.name is None:\n                    series.name = col\n                series.divisions = X.divisions\n            else:\n                # pandas\n                series = pd.Series(\n                    pd.Categorical.from_codes(codes, categories, ordered=ordered),\n                    name=col,\n                )\n\n            cats.append(series)\n        if big:\n            df = dd.concat([non_cat] + cats, axis=1)[list(self.columns_)]\n        else:\n            df = pd.concat([non_cat] + cats, axis=1)[self.columns_]\n        return df", "response": "Inverse - encode the columns in X"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the object to get the categorical columns to be encoded.", "response": "def fit(self, X, y=None):\n        \"\"\"Determine the categorical columns to be encoded.\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or dask.dataframe.DataFrame\n        y : ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.columns_ = X.columns\n        columns = self.columns\n        if columns is None:\n            columns = X.select_dtypes(include=[\"category\"]).columns\n        else:\n            for column in columns:\n                assert is_categorical_dtype(X[column]), \"Must be categorical\"\n\n        self.categorical_columns_ = columns\n        self.non_categorical_columns_ = X.columns.drop(self.categorical_columns_)\n\n        if _HAS_CTD:\n            self.dtypes_ = {col: X[col].dtype for col in self.categorical_columns_}\n        else:\n            self.dtypes_ = {\n                col: (X[col].cat.categories, X[col].cat.ordered)\n                for col in self.categorical_columns_\n            }\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform(self, X, y=None):\n        if not X.columns.equals(self.columns_):\n            raise ValueError(\n                \"Columns of 'X' do not match the training \"\n                \"columns. Got {!r}, expected {!r}\".format(X.columns, self.columns)\n            )\n        if not isinstance(X, (pd.DataFrame, dd.DataFrame)):\n            raise TypeError(\"Unexpected type {}\".format(type(X)))\n\n        X = X.copy()\n        for col in self.categorical_columns_:\n            X[col] = X[col].cat.codes\n        return X", "response": "Ordinal encode the categorical columns in X"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inverse_transform(self, X):\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X, columns=self.columns_)\n\n        elif isinstance(X, da.Array):\n            # later on we concat(..., axis=1), which requires\n            # known divisions. Suboptimal, but I think unavoidable.\n            unknown = np.isnan(X.chunks[0]).any()\n            if unknown:\n                lengths = blockwise(len, \"i\", X[:, 0], \"i\", dtype=\"i8\").compute()\n                X = X.copy()\n                chunks = (tuple(lengths), X.chunks[1])\n                X._chunks = chunks\n\n            X = dd.from_dask_array(X, columns=self.columns_)\n\n        big = isinstance(X, dd.DataFrame)\n\n        if big:\n            chunks = np.array(X.divisions)\n            chunks[-1] = chunks[-1] + 1\n            chunks = tuple(chunks[1:] - chunks[:-1])\n\n        X = X.copy()\n        for col in self.categorical_columns_:\n            if _HAS_CTD:\n                dtype = self.dtypes_[col]\n                categories, ordered = dtype.categories, dtype.ordered\n            else:\n                categories, ordered = self.dtypes_[col]\n\n            # use .values to avoid warning from pandas\n            codes = X[col].values\n\n            if big:\n                # dask\n                codes._chunks = (chunks,)\n                # Need a Categorical.from_codes for dask\n                series = (\n                    dd.from_dask_array(codes, columns=col)\n                    .astype(\"category\")\n                    .cat.set_categories(np.arange(len(categories)), ordered=ordered)\n                    .cat.rename_categories(categories)\n                )\n                # Bug in pandas <= 0.20.3 lost name\n                if series.name is None:\n                    series.name = col\n                series.divisions = X.divisions\n            else:\n                # pandas\n                series = pd.Series(\n                    pd.Categorical.from_codes(codes, categories, ordered=ordered),\n                    name=col,\n                )\n\n            X[col] = series\n\n        return X", "response": "Inverse - encode the columns in X into a Dask DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(model, x, y, compute=True, shuffle_blocks=True, random_state=None, **kwargs):\n    if not hasattr(x, \"chunks\") and hasattr(x, \"to_dask_array\"):\n        x = x.to_dask_array()\n    assert x.ndim == 2\n    if y is not None:\n        if not hasattr(y, \"chunks\") and hasattr(y, \"to_dask_array\"):\n            y = y.to_dask_array()\n\n        assert y.ndim == 1\n        assert x.chunks[0] == y.chunks[0]\n\n    assert hasattr(model, \"partial_fit\")\n    if len(x.chunks[1]) > 1:\n        x = x.rechunk(chunks=(x.chunks[0], sum(x.chunks[1])))\n\n    nblocks = len(x.chunks[0])\n    order = list(range(nblocks))\n    if shuffle_blocks:\n        rng = sklearn.utils.check_random_state(random_state)\n        rng.shuffle(order)\n\n    name = \"fit-\" + dask.base.tokenize(model, x, y, kwargs, order)\n    dsk = {(name, -1): model}\n    dsk.update(\n        {\n            (name, i): (\n                _partial_fit,\n                (name, i - 1),\n                (x.name, order[i], 0),\n                (getattr(y, \"name\", \"\"), order[i]),\n                kwargs,\n            )\n            for i in range(nblocks)\n        }\n    )\n\n    graphs = {x.name: x.__dask_graph__(), name: dsk}\n    if hasattr(y, \"__dask_graph__\"):\n        graphs[y.name] = y.__dask_graph__()\n\n    try:\n        from dask.highlevelgraph import HighLevelGraph\n\n        new_dsk = HighLevelGraph.merge(*graphs.values())\n    except ImportError:\n        from dask import sharedict\n\n        new_dsk = sharedict.merge(*graphs.values())\n\n    value = Delayed((name, nblocks - 1), new_dsk)\n\n    if compute:\n        return value.compute()\n    else:\n        return value", "response": "Fit a sklearn model against dask arrays and return the result as a new object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict(model, x):\n    if not hasattr(x, \"chunks\") and hasattr(x, \"to_dask_array\"):\n        x = x.to_dask_array()\n    assert x.ndim == 2\n    if len(x.chunks[1]) > 1:\n        x = x.rechunk(chunks=(x.chunks[0], sum(x.chunks[1])))\n    func = partial(_predict, model)\n    xx = np.zeros((1, x.shape[1]), dtype=x.dtype)\n    dt = model.predict(xx).dtype\n    return x.map_blocks(func, chunks=(x.chunks[0], (1,)), dtype=dt).squeeze()", "response": "Predict with a scikit learn model and return the result as a dask array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nslicing dask array that is almost entirely sorted already.", "response": "def _slice_mostly_sorted(array, keep, rest, ind=None):\n    \"\"\"Slice dask array `array` that is almost entirely sorted already.\n\n    We perform approximately `2 * len(keep)` slices on `array`.\n    This is OK, since `keep` is small. Individually, each of these slices\n    is entirely sorted.\n\n    Parameters\n    ----------\n    array : dask.array.Array\n    keep : ndarray[Int]\n        This must be sorted.\n    rest : ndarray[Bool]\n    ind : ndarray[Int], optional\n\n    Returns\n    -------\n    sliced : dask.array.Array\n    \"\"\"\n    if ind is None:\n        ind = np.arange(len(array))\n    idx = np.argsort(np.concatenate([keep, ind[rest]]))\n\n    slices = []\n    if keep[0] > 0:  # avoid creating empty slices\n        slices.append(slice(None, keep[0]))\n    slices.append([keep[0]])\n    windows = zip(keep[:-1], keep[1:])\n\n    for l, r in windows:\n        if r > l + 1:  # avoid creating empty slices\n            slices.append(slice(l + 1, r))\n        slices.append([r])\n\n    if keep[-1] < len(array) - 1:  # avoid creating empty slices\n        slices.append(slice(keep[-1] + 1, None))\n    result = da.concatenate([array[idx[slice_]] for slice_ in slices])\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a dummy dataset for modeling count data.", "response": "def make_counts(\n    n_samples=1000,\n    n_features=100,\n    n_informative=2,\n    scale=1.0,\n    chunks=100,\n    random_state=None,\n):\n    \"\"\"\n    Generate a dummy dataset for modeling count data.\n\n    Parameters\n    ----------\n    n_samples : int\n        number of rows in the output array\n    n_features : int\n        number of columns (features) in the output array\n    n_informative : int\n        number of features that are correlated with the outcome\n    scale : float\n        Scale the true coefficient array by this\n    chunks : int\n        Number of rows per dask array block.\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : dask.array, size ``(n_samples, n_features)``\n    y : dask.array, size ``(n_samples,)``\n        array of non-negative integer-valued data\n\n    Examples\n    --------\n    >>> X, y = make_counts()\n    \"\"\"\n    rng = dask_ml.utils.check_random_state(random_state)\n\n    X = rng.normal(0, 1, size=(n_samples, n_features), chunks=(chunks, n_features))\n    informative_idx = rng.choice(n_features, n_informative, chunks=n_informative)\n    beta = (rng.random(n_features, chunks=n_features) - 1) * scale\n\n    informative_idx, beta = dask.compute(informative_idx, beta)\n\n    z0 = X[:, informative_idx].dot(beta[informative_idx])\n    rate = da.exp(z0)\n    y = rng.poisson(rate, size=1, chunks=(chunks,))\n    return X, y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_blobs(\n    n_samples=100,\n    n_features=2,\n    centers=None,\n    cluster_std=1.0,\n    center_box=(-10.0, 10.0),\n    shuffle=True,\n    random_state=None,\n    chunks=None,\n):\n    \"\"\"\n    Generate isotropic Gaussian blobs for clustering.\n\n    This can be used to generate very large Dask arrays on a cluster of\n    machines. When using Dask in distributed mode, the client machine\n    only needs to allocate a single block's worth of data.\n\n    Parameters\n    ----------\n    n_samples : int or array-like, optional (default=100)\n        If int, it is the total number of points equally divided among\n        clusters.\n        If array-like, each element of the sequence indicates\n        the number of samples per cluster.\n\n    n_features : int, optional (default=2)\n        The number of features for each sample.\n\n    centers : int or array of shape [n_centers, n_features], optional\n        (default=None)\n        The number of centers to generate, or the fixed center locations.\n        If n_samples is an int and centers is None, 3 centers are generated.\n        If n_samples is array-like, centers must be\n        either None or an array of length equal to the length of n_samples.\n\n    cluster_std : float or sequence of floats, optional (default=1.0)\n        The standard deviation of the clusters.\n\n    center_box : pair of floats (min, max), optional (default=(-10.0, 10.0))\n        The bounding box for each cluster center when centers are\n        generated at random.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    chunks : int, tuple\n        How to chunk the array. Must be one of the following forms:\n        -   A blocksize like 1000.\n        -   A blockshape like (1000, 1000).\n        -   Explicit sizes of all blocks along all dimensions like\n            ((1000, 1000, 500), (400, 400)).\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The generated samples.\n\n    y : array of shape [n_samples]\n        The integer labels for cluster membership of each sample.\n\n    Examples\n    --------\n    >>> from dask_ml.datasets import make_blobs\n    >>> X, y = make_blobs(n_samples=100000, chunks=10000)\n    >>> X\n    dask.array<..., shape=(100000, 2), dtype=float64, chunksize=(10000, 2)>\n    >>> y\n    dask.array<concatenate, shape=(100000,), dtype=int64, chunksize=(10000,)>\n\n    See Also\n    --------\n    make_classification: a more intricate variant\n    \"\"\"\n    chunks = da.core.normalize_chunks(chunks, (n_samples, n_features))\n    _check_axis_partitioning(chunks, n_features)\n\n    if centers is None:\n        # TODO: non-int n_samples?\n        centers = 3\n    if isinstance(centers, numbers.Integral):\n        # Make a prototype\n        n_centers = centers\n        X, y = sklearn.datasets.make_blobs(\n            n_samples=chunks[0][0],\n            n_features=n_features,\n            centers=centers,\n            shuffle=shuffle,\n            cluster_std=cluster_std,\n            center_box=center_box,\n            random_state=random_state,\n        )\n        centers = []\n        centers = np.zeros((n_centers, n_features))\n\n        for i in range(n_centers):\n            centers[i] = X[y == i].mean(0)\n\n    objs = [\n        dask.delayed(sklearn.datasets.make_blobs, nout=2)(\n            n_samples=n_samples_per_block,\n            n_features=n_features,\n            centers=centers,\n            cluster_std=cluster_std,\n            shuffle=shuffle,\n            center_box=center_box,\n            random_state=i,\n        )\n        for i, n_samples_per_block in enumerate(chunks[0])\n    ]\n    Xobjs, yobjs = zip(*objs)\n\n    Xarrs = [\n        da.from_delayed(arr, shape=(n, n_features), dtype=\"f8\")\n        for arr, n in zip(Xobjs, chunks[0])\n    ]\n    X_big = da.vstack(Xarrs)\n\n    yarrs = [\n        da.from_delayed(arr, shape=(n,), dtype=np.dtype(\"int\"))\n        for arr, n in zip(yobjs, chunks[0])\n    ]\n    y_big = da.hstack(yarrs)\n    return X_big, y_big", "response": "Generates isotropic Gaussian blobs for clustering."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_regression(\n    n_samples=100,\n    n_features=100,\n    n_informative=10,\n    n_targets=1,\n    bias=0.0,\n    effective_rank=None,\n    tail_strength=0.5,\n    noise=0.0,\n    shuffle=True,\n    coef=False,\n    random_state=None,\n    chunks=None,\n):\n    \"\"\"\n    Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See\n    :func:`sklearn.datasets.make_low_rank_matrix` for more details.\n\n    This can be used to generate very large Dask arrays on a cluster of\n    machines. When using Dask in distributed mode, the client machine\n    only needs to allocate a single block's worth of data.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    chunks : int, tuple\n        How to chunk the array. Must be one of the following forms:\n        -   A blocksize like 1000.\n        -   A blockshape like (1000, 1000).\n        -   Explicit sizes of all blocks along all dimensions like\n            ((1000, 1000, 500), (400, 400)).\n\n    Returns\n    -------\n    X : Dask array of shape [n_samples, n_features]\n        The input samples.\n\n    y : Dask array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"\n    chunks = da.core.normalize_chunks(chunks, (n_samples, n_features))\n    _check_axis_partitioning(chunks, n_features)\n\n    rng = sklearn.utils.check_random_state(random_state)\n    return_coef = coef is True\n\n    if chunks[1][0] != n_features:\n        raise ValueError(\n            \"Can only generate arrays partitioned along the \"\n            \"first axis. Specifying a larger chunksize for \"\n            \"the second axis.\"\n        )\n    _, _, coef = sklearn.datasets.make_regression(\n        n_samples=chunks[0][0],\n        n_features=n_features,\n        n_informative=n_informative,\n        n_targets=n_targets,\n        bias=bias,\n        effective_rank=effective_rank,\n        tail_strength=tail_strength,\n        noise=noise,\n        shuffle=shuffle,\n        coef=True,  # hardcode here\n        random_state=rng,\n    )\n    seed = da.random.random_state_data(1, random_state=rng)\n    da_rng = da.random.RandomState(seed[0])\n\n    X_big = da_rng.normal(size=(n_samples, n_features), chunks=(chunks[0], n_features))\n    y_big = da.dot(X_big, coef) + bias\n\n    if noise > 0:\n        y_big = y_big + da_rng.normal(\n            scale=noise, size=y_big.shape, chunks=y_big.chunks\n        )\n\n    y_big = y_big.squeeze()\n\n    if return_coef:\n        return X_big, y_big, coef\n    else:\n        return X_big, y_big", "response": "Generates a random regression problem for a single cluster of Dasks."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit model to X and perform dimensionality reduction on X.", "response": "def fit_transform(self, X, y=None):\n        \"\"\"Fit model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array, of the\n            same type as the input array. If ``X`` was a ``dask.array``, then\n            ``X_new`` will be a ``dask.array`` with the same chunks along the\n            first dimension.\n        \"\"\"\n        X = self._check_array(X)\n        if self.algorithm not in {\"tsqr\", \"randomized\"}:\n            raise ValueError()\n        if self.algorithm == \"tsqr\":\n            u, s, v = da.linalg.svd(X)\n            u = u[:, : self.n_components]\n            s = s[: self.n_components]\n            v = v[: self.n_components]\n        else:\n            u, s, v = da.linalg.svd_compressed(\n                X, self.n_components, self.n_iter, seed=self.random_state\n            )\n        u, v = svd_flip(u, v)\n\n        X_transformed = u * s\n        explained_var = X_transformed.var(axis=0)\n        full_var = X.var(axis=0).sum()\n        explained_variance_ratio = explained_var / full_var\n\n        components, ev, evr, sv = compute(v, explained_var, explained_variance_ratio, s)\n        self.components_ = components\n        self.explained_variance_ = ev\n        self.explained_variance_ratio_ = evr\n        self.singular_values_ = sv\n        return X_transformed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _hstack(self, Xs):\n        types = set(type(X) for X in Xs)\n\n        if self.sparse_output_:\n            return sparse.hstack(Xs).tocsr()\n        elif dd.Series in types or dd.DataFrame in types:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", \"Concatenating\", UserWarning)\n                return dd.concat(Xs, axis=\"columns\")\n        elif da.Array in types:\n            # To allow compatibility with dask core 1.0.0, this is the `else`\n            # part of the definition of the dask.array.hstack inlined.\n            # The `then` branch is removed because _validate_output in\n            # sklearn.compose.ColumnTransformer ensures ndim == 2, so the\n            # check `all(x.ndim == 1 for x in Xs)` should always fail.\n            #\n            # Once dask.array.hstack supports allow_unknown_chunksizes,\n            # changed this to da.hstack(Xs, allow_unknown_chunksizes=True)\n            return da.concatenate(Xs, axis=1, allow_unknown_chunksizes=True)\n        elif self.preserve_dataframe and (pd.Series in types or pd.DataFrame in types):\n            return pd.concat(Xs, axis=\"columns\")\n        else:\n            return np.hstack(Xs)", "response": "Stacks X horizontally.\n\n        Supports input types (X): list of\n            numpy arrays, sparse arrays and DataFrames"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply dimensionality reduction on X.", "response": "def transform(self, X):\n        \"\"\"Apply dimensionality reduction on X.\n\n        X is projected on the first principal components previous extracted\n        from a training set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n\n        \"\"\"\n        check_is_fitted(self, [\"mean_\", \"components_\"], all_or_any=all)\n\n        # X = check_array(X)\n        if self.mean_ is not None:\n            X = X - self.mean_\n        X_transformed = da.dot(X, self.components_.T)\n        if self.whiten:\n            X_transformed /= np.sqrt(self.explained_variance_)\n        return X_transformed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the model with X and apply the dimensionality reduction on X.", "response": "def fit_transform(self, X, y=None):\n        \"\"\"Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n\n        \"\"\"\n        # X = check_array(X)\n        U, S, V = self._fit(X)\n        U = U[:, : self.n_components_]\n\n        if self.whiten:\n            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n            U *= np.sqrt(X.shape[0] - 1)\n        else:\n            # X_new = X * V = U * S * V^T * V = U * S\n            U *= S[: self.n_components_]\n\n        return U"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms data back to its original space.", "response": "def inverse_transform(self, X):\n        \"\"\"Transform data back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n            New data, where n_samples in the number of samples\n            and n_components is the number of components.\n\n        Returns\n        -------\n        X_original array-like, shape (n_samples, n_features)\n\n        Notes\n        -----\n        If whitening is enabled, inverse_transform does not compute the\n        exact inverse operation of transform.\n        \"\"\"\n        check_is_fitted(self, \"mean_\")\n\n        if self.whiten:\n            return (\n                da.dot(\n                    X,\n                    np.sqrt(self.explained_variance_[:, np.newaxis]) * self.components_,\n                )\n                + self.mean_\n            )\n        else:\n            return da.dot(X, self.components_) + self.mean_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the log - likelihood of each sample under the current model.", "response": "def score_samples(self, X):\n        \"\"\"Return the log-likelihood of each sample.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array, shape(n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : array, shape (n_samples,)\n            Log-likelihood of each sample under the current model\n        \"\"\"\n        check_is_fitted(self, \"mean_\")\n\n        # X = check_array(X)\n        Xr = X - self.mean_\n        n_features = X.shape[1]\n        precision = self.get_precision()  # [n_features, n_features]\n        log_like = -0.5 * (Xr * (da.dot(Xr, precision))).sum(axis=1)\n        log_like -= 0.5 * (n_features * da.log(2.0 * np.pi) - fast_logdet(precision))\n        return log_like"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that two Estimators are equal.", "response": "def assert_estimator_equal(left, right, exclude=None, **kwargs):\n    \"\"\"Check that two Estimators are equal\n\n    Parameters\n    ----------\n    left, right : Estimators\n    exclude : str or sequence of str\n        attributes to skip in the check\n    kwargs : dict\n        Passed through to the dask `assert_eq` method.\n\n    \"\"\"\n    left_attrs = [x for x in dir(left) if x.endswith(\"_\") and not x.startswith(\"_\")]\n    right_attrs = [x for x in dir(right) if x.endswith(\"_\") and not x.startswith(\"_\")]\n    if exclude is None:\n        exclude = set()\n    elif isinstance(exclude, str):\n        exclude = {exclude}\n    else:\n        exclude = set(exclude)\n\n    assert (set(left_attrs) - exclude) == set(right_attrs) - exclude\n\n    for attr in set(left_attrs) - exclude:\n        l = getattr(left, attr)\n        r = getattr(right, attr)\n        _assert_eq(l, r, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_array(array, *args, **kwargs):\n    accept_dask_array = kwargs.pop(\"accept_dask_array\", True)\n    preserve_pandas_dataframe = kwargs.pop(\"preserve_pandas_dataframe\", False)\n    accept_dask_dataframe = kwargs.pop(\"accept_dask_dataframe\", False)\n    accept_unknown_chunks = kwargs.pop(\"accept_unknown_chunks\", False)\n    accept_multiple_blocks = kwargs.pop(\"accept_multiple_blocks\", False)\n\n    if isinstance(array, da.Array):\n        if not accept_dask_array:\n            raise TypeError\n        if not accept_unknown_chunks:\n            if np.isnan(array.shape[0]):\n                raise TypeError(\n                    \"Cannot operate on Dask array with unknown chunk sizes.\"\n                )\n        if not accept_multiple_blocks and array.ndim > 1:\n            if len(array.chunks[1]) > 1:\n                msg = (\n                    \"Chunking is only allowed on the first axis. \"\n                    \"Use 'array.rechunk({1: array.shape[1]})' to \"\n                    \"rechunk to a single block along the second axis.\"\n                )\n                raise TypeError(msg)\n\n        # hmmm, we want to catch things like shape errors.\n        # I'd like to make a small sample somehow\n        shape = array.shape\n        if len(shape) == 2:\n            shape = (min(10, shape[0]), shape[1])\n        elif shape == 1:\n            shape = min(10, shape[0])\n\n        sample = np.ones(shape=shape, dtype=array.dtype)\n        sk_validation.check_array(sample, *args, **kwargs)\n        return array\n\n    elif isinstance(array, dd.DataFrame):\n        if not accept_dask_dataframe:\n            raise TypeError(\"This estimator does not support dask dataframes.\")\n        # TODO: sample?\n        return array\n    elif isinstance(array, pd.DataFrame) and preserve_pandas_dataframe:\n        # TODO: validation?\n        return array\n    else:\n        return sk_validation.check_array(array, *args, **kwargs)", "response": "Validate input array and return a new object of type tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the partitioning structure for many arrays matches.", "response": "def check_matching_blocks(*arrays):\n    \"\"\"Check that the partitioning structure for many arrays matches.\n\n    Parameters\n    ----------\n    *arrays : Sequence of array-likes\n        This includes\n\n        * Dask Array\n        * Dask DataFrame\n        * Dask Series\n    \"\"\"\n    if len(arrays) <= 1:\n        return\n    if all(isinstance(x, da.Array) for x in arrays):\n        # TODO: unknown chunks, ensure blocks match, or just raise (configurable)\n        chunks = arrays[0].chunks\n        for array in arrays[1:]:\n            if array.chunks != chunks:\n                raise ValueError(\n                    \"Mismatched chunks. {} != {}\".format(chunks, array.chunks)\n                )\n\n    elif all(isinstance(x, (dd.Series, dd.DataFrame)) for x in arrays):\n        divisions = arrays[0].divisions\n        for array in arrays[1:]:\n            if array.divisions != divisions:\n                raise ValueError(\n                    \"Mismatched divisions. {} != {}\".format(divisions, array.divisions)\n                )\n    else:\n        raise ValueError(\"Unexpected types {}.\".format({type(x) for x in arrays}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates and normalize the chunks argument for a dask. array .", "response": "def check_chunks(n_samples, n_features, chunks=None):\n    \"\"\"Validate and normalize the chunks argument for a dask.array\n\n    Parameters\n    ----------\n    n_samples, n_features : int\n        Give the shape of the array\n    chunks : int, sequence, optional, default None\n        * For 'chunks=None', this picks a \"good\" default number of chunks based\n          on the number of CPU cores. The default results in a block structure\n          with one block per core along the first dimension (of roughly equal\n          lengths) and a single block along the second dimension. This may or\n          may not be appropriate for your use-case. The chunk size will be at\n          least 100 along the first dimension.\n\n        * When chunks is an int, we split the ``n_samples`` into ``chunks``\n          blocks along the first dimension, and a single block along the\n          second. Again, the chunksize will be at least 100 along the first\n          dimension.\n\n        * When chunks is a sequence, we validate that it's length two and turn\n          it into a tuple.\n\n    Returns\n    -------\n    chunks : tuple\n    \"\"\"\n    if chunks is None:\n        chunks = (max(100, n_samples // cpu_count()), n_features)\n    elif isinstance(chunks, Integral):\n        chunks = (max(100, n_samples // chunks), n_features)\n    elif isinstance(chunks, Sequence):\n        chunks = tuple(chunks)\n        if len(chunks) != 2:\n            raise AssertionError(\"Chunks should be a 2-tuple.\")\n    else:\n        raise ValueError(\"Unknown type of chunks: '{}'\".format(type(chunks)))\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _timer(name, _logger=None, level=\"info\"):\n    start = tic()\n    _logger = _logger or logger\n    _logger.info(\"Starting %s\", name)\n    yield\n    stop = tic()\n    delta = datetime.timedelta(seconds=stop - start)\n    _logger_level = getattr(_logger, level)\n    _logger_level(\"Finished %s in %s\", name, delta)", "response": "A context manager that logs the execution time of a function to the given logger level"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _timed(_logger=None, level=\"info\"):\n\n    def fun_wrapper(f):\n        @functools.wraps(f)\n        def wraps(*args, **kwargs):\n            with _timer(f.__name__, _logger=logger, level=level):\n                results = f(*args, **kwargs)\n            return results\n\n        return wraps\n\n    return fun_wrapper", "response": "Decorator to log execution time of a function to the given logger level"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a scorer from string or scorer.", "response": "def get_scorer(scoring, compute=True):\n    \"\"\"Get a scorer from string\n\n    Parameters\n    ----------\n    scoring : str | callable\n        scoring method as string. If callable it is returned as is.\n\n    Returns\n    -------\n    scorer : callable\n        The scorer.\n    \"\"\"\n    # This is the same as sklearns, only we use our SCORERS dict,\n    # and don't have back-compat code\n    if isinstance(scoring, six.string_types):\n        try:\n            scorer, kwargs = SCORERS[scoring]\n        except KeyError:\n            raise ValueError(\n                \"{} is not a valid scoring value. \"\n                \"Valid options are {}\".format(scoring, sorted(SCORERS))\n            )\n    else:\n        scorer = scoring\n        kwargs = {}\n\n    kwargs[\"compute\"] = compute\n\n    return make_scorer(scorer, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a list of dask objects to indexable types.", "response": "def to_indexable(*args, **kwargs):\n    \"\"\"Ensure that all args are an indexable type.\n\n    Conversion runs lazily for dask objects, immediately otherwise.\n\n    Parameters\n    ----------\n    args : array_like or scalar\n    allow_scalars : bool, optional\n        Whether to allow scalars in args. Default is False.\n    \"\"\"\n    if kwargs.get(\"allow_scalars\", False):\n        indexable = _maybe_indexable\n    else:\n        indexable = _indexable\n    for x in args:\n        if x is None or isinstance(x, (da.Array, dd.DataFrame)):\n            yield x\n        elif is_dask_collection(x):\n            yield delayed(indexable, pure=True)(x)\n        else:\n            yield indexable(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _index_param_value(num_samples, v, indices):\n    if not _is_arraylike(v) or _num_samples(v) != num_samples:\n        # pass through: skip indexing\n        return v\n    if sp.issparse(v):\n        v = v.tocsr()\n    return safe_indexing(v, indices)", "response": "Private helper function for parameter value indexing."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a warning to be triggered when the specified key is read", "response": "def add_warning(self, key, *args, **kwargs):\n        \"\"\"Add a warning to be triggered when the specified key is read\n\n        Parameters\n        ----------\n        key : any hashable object\n            The key\n        \"\"\"\n        self._deprecations[key] = (args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a sparse matrix from an encoded array.", "response": "def _construct(x, categories):\n    \"\"\"Make a sparse matrix from an encoded array.\n\n    >>> construct(np.array([0, 1, 0]), np.array([0, 1])).toarray()\n    array([[1., 0.],\n           [0., 1.],\n           [1., 0.]])\n    \"\"\"\n    # type: (np.ndarray, np.ndarray) -> scipy.sparse.csr_matrix\n    data = np.ones(len(x))\n    rows = np.arange(len(x))\n    columns = x.ravel()\n    return scipy.sparse.csr_matrix(\n        (data, (rows, columns)), shape=(len(x), len(categories))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pipeline(names, steps):\n    steps, times = zip(*map(_maybe_timed, steps))\n    fit_time = sum(times)\n    if any(s is FIT_FAILURE for s in steps):\n        fit_est = FIT_FAILURE\n    else:\n        fit_est = Pipeline(list(zip(names, steps)))\n    return fit_est, fit_time", "response": "Reconstruct a Pipeline from names and steps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef feature_union(names, steps, weights):\n    steps, times = zip(*map(_maybe_timed, steps))\n    fit_time = sum(times)\n    if any(s is FIT_FAILURE for s in steps):\n        fit_est = FIT_FAILURE\n    else:\n        fit_est = FeatureUnion(list(zip(names, steps)), transformer_weights=weights)\n    return fit_est, fit_time", "response": "Reconstruct a FeatureUnion from names steps and weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies weights and concatenate outputs from a FeatureUnion", "response": "def feature_union_concat(Xs, nsamples, weights):\n    \"\"\"Apply weights and concatenate outputs from a FeatureUnion\"\"\"\n    if any(x is FIT_FAILURE for x in Xs):\n        return FIT_FAILURE\n    Xs = [X if w is None else X * w for X, w in zip(Xs, weights) if X is not None]\n    if not Xs:\n        return np.zeros((nsamples, 0))\n    if any(sparse.issparse(f) for f in Xs):\n        return sparse.hstack(Xs).tocsr()\n    return np.hstack(Xs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _store(\n    results,\n    key_name,\n    array,\n    n_splits,\n    n_candidates,\n    weights=None,\n    splits=False,\n    rank=False,\n):\n    \"\"\"A small helper to store the scores/times to the cv_results_\"\"\"\n    # When iterated first by n_splits and then by parameters\n    array = np.array(array, dtype=np.float64).reshape(n_splits, n_candidates).T\n    if splits:\n        for split_i in range(n_splits):\n            results[\"split%d_%s\" % (split_i, key_name)] = array[:, split_i]\n\n    array_means = np.average(array, axis=1, weights=weights)\n    results[\"mean_%s\" % key_name] = array_means\n    # Weighted std is not directly available in numpy\n    array_stds = np.sqrt(\n        np.average((array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights)\n    )\n    results[\"std_%s\" % key_name] = array_stds\n\n    if rank:\n        results[\"rank_%s\" % key_name] = np.asarray(\n            rankdata(-array_means, method=\"min\"), dtype=np.int32\n        )", "response": "A small helper to store the scores and times in cv_results_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate train and test indices for a length - n array.", "response": "def _generate_idx(n, seed, n_train, n_test):\n    \"\"\"Generate train, test indices for a length-n array.\n\n    Parameters\n    ----------\n    n : int\n        The length of the array\n    seed : int\n        Seed for a RandomState\n    n_train, n_test : int, 0 < n_train, n_test < n\n        Number of samples to use for the train or\n        test index.\n\n    Notes\n    -----\n    \"\"\"\n    idx = check_random_state(seed).permutation(n)\n\n    ind_test = idx[:n_test]\n    ind_train = idx[n_test : n_train + n_test]\n    return ind_train, ind_test"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _blockwise_slice(arr, idx):\n    objs = []\n    offsets = np.hstack([0, np.cumsum(arr.chunks[0])[:-1]])\n\n    for i, (x, idx2) in enumerate(\n        zip(arr.to_delayed().ravel(), idx.to_delayed().ravel())\n    ):\n        idx3 = idx2 - offsets[i]\n        objs.append(x[idx3])\n\n    shapes = idx.chunks[0]\n    if arr.ndim == 2:\n        P = arr.shape[1]\n        shapes = [(x, P) for x in shapes]\n    else:\n        shapes = [(x,) for x in shapes]\n\n    sliced = da.concatenate(\n        [\n            da.from_delayed(x, shape=shape, dtype=arr.dtype)\n            for x, shape in zip(objs, shapes)\n        ]\n    )\n    return sliced", "response": "Slice an array that is blockwise - aligned with idx."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a sequence of text documents to a document - term matrix.", "response": "def transform(self, X):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Transformation is done in parallel, and correctly handles dask\n        collections.\n\n        Parameters\n        ----------\n        X : dask.Bag of raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n\n        Returns\n        -------\n        X : dask.array.Array, shape = (n_samples, self.n_features)\n            Document-term matrix. Each block of the array is a scipy sparse\n            matrix.\n\n        Notes\n        -----\n        The returned dask Array is composed scipy sparse matricies. If you need\n        to compute on the result immediately, you may need to convert the individual\n        blocks to ndarrays or pydata/sparse matricies.\n\n        >>> import sparse\n        >>> X.map_blocks(sparse.COO.from_scipy_sparse, dtype=X.dtype)  # doctest: +SKIP\n\n        See the :doc:`examples/text-vectorization` for more.\n        \"\"\"\n        msg = \"'X' should be a 1-dimensional array with length 'num_samples'.\"\n\n        if not dask.is_dask_collection(X):\n            return super(HashingVectorizer, self).transform(X)\n\n        if isinstance(X, db.Bag):\n            bag2 = X.map_partitions(_transform, estimator=self)\n            objs = bag2.to_delayed()\n            arrs = [\n                da.from_delayed(obj, (np.nan, self.n_features), self.dtype)\n                for obj in objs\n            ]\n            result = da.concatenate(arrs, axis=0)\n        elif isinstance(X, dd.Series):\n            result = X.map_partitions(_transform, self)\n        elif isinstance(X, da.Array):\n            # dask.Array\n            chunks = ((np.nan,) * X.numblocks[0], (self.n_features,))\n            if X.ndim == 1:\n                result = X.map_blocks(\n                    _transform, estimator=self, dtype=\"f8\", chunks=chunks, new_axis=1\n                )\n            else:\n                raise ValueError(msg)\n        else:\n            raise ValueError(msg)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_peaks(sig):\n    if len(sig) == 0:\n        return np.empty([0]), np.empty([0])\n\n    tmp = sig[1:]\n    tmp = np.append(tmp, [sig[-1]])\n    tmp = sig - tmp\n    tmp[np.where(tmp>0)] = 1\n    tmp[np.where(tmp==0)] = 0\n    tmp[np.where(tmp<0)] = -1\n    tmp2 = tmp[1:]\n    tmp2 = np.append(tmp2, [0])\n    tmp = tmp-tmp2\n\n    hard_peaks = np.where(np.logical_or(tmp==-2, tmp==+2))[0] + 1\n    soft_peaks = []\n\n    for iv in np.where(np.logical_or(tmp==-1,tmp==+1))[0]:\n        t = tmp[iv]\n        i = iv+1\n        while True:\n            if i==len(tmp) or tmp[i] == -t or tmp[i] == -2 or tmp[i] == 2:\n                break\n            if tmp[i] == t:\n                soft_peaks.append(int(iv + (i - iv)/2))\n                break\n            i += 1\n    soft_peaks = np.array(soft_peaks, dtype='int') + 1\n\n    return hard_peaks, soft_peaks", "response": "Find hard and soft peaks in a 1d signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_local_peaks(sig, radius):\n    # TODO: Fix flat mountain scenarios.\n    if np.min(sig) == np.max(sig):\n        return np.empty(0)\n\n    peak_inds = []\n\n    i = 0\n    while i < radius + 1:\n        if sig[i] == max(sig[:i + radius]):\n            peak_inds.append(i)\n            i += radius\n        else:\n            i += 1\n\n    while i < len(sig):\n        if sig[i] == max(sig[i - radius:i + radius]):\n            peak_inds.append(i)\n            i += radius\n        else:\n            i += 1\n\n    while i < len(sig):\n        if sig[i] == max(sig[i - radius:]):\n            peak_inds.append(i)\n            i += radius\n        else:\n            i += 1\n\n    return (np.array(peak_inds))", "response": "Find all local peaks in a signal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef correct_peaks(sig, peak_inds, search_radius, smooth_window_size,\n                  peak_dir='compare'):\n    \"\"\"\n    Adjust a set of detected peaks to coincide with local signal maxima,\n    and\n\n    Parameters\n    ----------\n    sig : numpy array\n        The 1d signal array\n    peak_inds : np array\n        Array of the original peak indices\n    max_gap : int\n        The radius within which the original peaks may be shifted.\n    smooth_window_size : int\n        The window size of the moving average filter applied on the\n        signal. Peak distance is calculated on the difference between\n        the original and smoothed signal.\n    peak_dir : str, optional\n        The expected peak direction: 'up' or 'down', 'both', or\n        'compare'.\n\n        - If 'up', the peaks will be shifted to local maxima\n        - If 'down', the peaks will be shifted to local minima\n        - If 'both', the peaks will be shifted to local maxima of the\n          rectified signal\n        - If 'compare', the function will try both 'up' and 'down'\n          options, and choose the direction that gives the largest mean\n          distance from the smoothed signal.\n\n    Returns\n    -------\n    corrected_peak_inds : numpy array\n        Array of the corrected peak indices\n\n\n    Examples\n    --------\n\n    \"\"\"\n    sig_len = sig.shape[0]\n    n_peaks = len(peak_inds)\n\n    # Subtract the smoothed signal from the original\n    sig = sig - smooth(sig=sig, window_size=smooth_window_size)\n\n\n    # Shift peaks to local maxima\n    if peak_dir == 'up':\n        shifted_peak_inds = shift_peaks(sig=sig,\n                                        peak_inds=peak_inds,\n                                        search_radius=search_radius,\n                                        peak_up=True)\n    elif peak_dir == 'down':\n        shifted_peak_inds = shift_peaks(sig=sig,\n                                        peak_inds=peak_inds,\n                                        search_radius=search_radius,\n                                        peak_up=False)\n    elif peak_dir == 'both':\n        shifted_peak_inds = shift_peaks(sig=np.abs(sig),\n                                        peak_inds=peak_inds,\n                                        search_radius=search_radius,\n                                        peak_up=True)\n    else:\n        shifted_peak_inds_up = shift_peaks(sig=sig,\n                                           peak_inds=peak_inds,\n                                           search_radius=search_radius,\n                                           peak_up=True)\n        shifted_peak_inds_down = shift_peaks(sig=sig,\n                                             peak_inds=peak_inds,\n                                             search_radius=search_radius,\n                                             peak_up=False)\n\n        # Choose the direction with the biggest deviation\n        up_dist = np.mean(np.abs(sig[shifted_peak_inds_up]))\n        down_dist = np.mean(np.abs(sig[shifted_peak_inds_down]))\n\n        if up_dist >= down_dist:\n            shifted_peak_inds = shifted_peak_inds_up\n        else:\n            shifted_peak_inds = shifted_peak_inds_down\n\n    return shifted_peak_inds", "response": "This function takes a 1d signal and returns a set of detected peaks to coincide with local maxima and local minima."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_items(signal=None, ann_samp=None, ann_sym=None, fs=None,\n               time_units='samples', sig_name=None, sig_units=None,\n               ylabel=None, title=None, sig_style=[''], ann_style=['r*'],\n               ecg_grids=[], figsize=None, return_fig=False):\n    \"\"\"\n    Subplot individual channels of signals and/or annotations.\n\n    Parameters\n    ----------\n    signal : 1d or 2d numpy array, optional\n        The uniformly sampled signal to be plotted. If signal.ndim is 1, it is\n        assumed to be a one channel signal. If it is 2, axes 0 and 1, must\n        represent time and channel number respectively.\n    ann_samp: list, optional\n        A list of annotation locations to plot, with each list item\n        corresponding to a different channel. List items may be:\n\n        - 1d numpy array, with values representing sample indices. Empty\n          arrays are skipped.\n        - list, with values representing sample indices. Empty lists\n          are skipped.\n        - None. For channels in which nothing is to be plotted.\n\n        If `signal` is defined, the annotation locations will be overlaid on\n        the signals, with the list index corresponding to the signal channel.\n        The length of `annotation` does not have to match the number of\n        channels of `signal`.\n    ann_sym: list, optional\n        A list of annotation symbols to plot, with each list item\n        corresponding to a different channel. List items should be lists of\n        strings. The symbols are plotted over the corresponding `ann_samp`\n        index locations.\n    fs : int or float, optional\n        The sampling frequency of the signals and/or annotations. Used to\n        calculate time intervals if `time_units` is not 'samples'. Also\n        required for plotting ecg grids.\n    time_units : str, optional\n        The x axis unit. Allowed options are: 'samples', 'seconds', 'minutes',\n        and 'hours'.\n    sig_name : list, optional\n        A list of strings specifying the signal names. Used with `sig_units`\n        to form y labels, if `ylabel` is not set.\n    sig_units : list, optional\n        A list of strings specifying the units of each signal channel. Used\n        with `sig_name` to form y labels, if `ylabel` is not set. This\n        parameter is required for plotting ecg grids.\n    ylabel : list, optional\n        A list of strings specifying the final y labels. If this option is\n        present, `sig_name` and `sig_units` will not be used for labels.\n    title : str, optional\n        The title of the graph.\n    sig_style : list, optional\n        A list of strings, specifying the style of the matplotlib plot\n        for each signal channel. The list length should match the number\n        of signal channels. If the list has a length of 1, the style\n        will be used for all channels.\n    ann_style : list, optional\n        A list of strings, specifying the style of the matplotlib plot for each\n        annotation channel. If the list has a length of 1, the style will be\n        used for all channels.\n    ecg_grids : list, optional\n        A list of integers specifying channels in which to plot ecg grids. May\n        also be set to 'all' for all channels. Major grids at 0.5mV, and minor\n        grids at 0.125mV. All channels to be plotted with grids must have\n        `sig_units` equal to 'uV', 'mV', or 'V'.\n    figsize : tuple, optional\n        Tuple pair specifying the width, and height of the figure. It is the\n        'figsize' argument passed into matplotlib.pyplot's `figure` function.\n    return_fig : bool, optional\n        Whether the figure is to be returned as an output argument.\n\n    Returns\n    -------\n    figure : matplotlib figure, optional\n        The matplotlib figure generated. Only returned if the 'return_fig'\n        parameter is set to True.\n\n    Examples\n    --------\n    >>> record = wfdb.rdrecord('sample-data/100', sampto=3000)\n    >>> ann = wfdb.rdann('sample-data/100', 'atr', sampto=3000)\n\n    >>> wfdb.plot_items(signal=record.p_signal,\n                        annotation=[ann.sample, ann.sample],\n                        title='MIT-BIH Record 100', time_units='seconds',\n                        figsize=(10,4), ecg_grids='all')\n\n    \"\"\"\n\n    # Figure out number of subplots required\n    sig_len, n_sig, n_annot, n_subplots = get_plot_dims(signal, ann_samp)\n\n    # Create figure\n    fig, axes = create_figure(n_subplots, figsize)\n\n    if signal is not None:\n        plot_signal(signal, sig_len, n_sig, fs, time_units, sig_style, axes)\n\n    if ann_samp is not None:\n        plot_annotation(ann_samp, n_annot, ann_sym, signal, n_sig, fs,\n                        time_units, ann_style, axes)\n\n    if ecg_grids:\n        plot_ecg_grids(ecg_grids, fs, sig_units, time_units, axes)\n\n    # Add title and axis labels.\n    label_figure(axes, n_subplots, time_units, sig_name, sig_units, ylabel,\n                 title)\n\n    plt.show(fig)\n\n    if return_fig:\n        return fig", "response": "Plot the items of the signal and annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_plot_dims(signal, ann_samp):\n    \"Figure out the number of plot channels\"\n    if signal is not None:\n        if signal.ndim == 1:\n            sig_len = len(signal)\n            n_sig = 1\n        else:\n            sig_len = signal.shape[0]\n            n_sig = signal.shape[1]\n    else:\n        sig_len = 0\n        n_sig = 0\n\n    if ann_samp is not None:\n        n_annot = len(ann_samp)\n    else:\n        n_annot = 0\n\n    return sig_len, n_sig, n_annot, max(n_sig, n_annot)", "response": "Figure out the number of plot channels"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the plot figure and subplot axes", "response": "def create_figure(n_subplots, figsize):\n    \"Create the plot figure and subplot axes\"\n    fig = plt.figure(figsize=figsize)\n    axes = []\n\n    for i in range(n_subplots):\n        axes.append(fig.add_subplot(n_subplots, 1, i+1))\n\n    return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the annotations possibly overlaid on signals", "response": "def plot_annotation(ann_samp, n_annot, ann_sym, signal, n_sig, fs, time_units,\n                    ann_style, axes):\n    \"Plot annotations, possibly overlaid on signals\"\n    # Extend annotation style if necesary\n    if len(ann_style) == 1:\n        ann_style = n_annot * ann_style\n\n    # Figure out downsample factor for time indices\n    if time_units == 'samples':\n        downsample_factor = 1\n    else:\n        downsample_factor = {'seconds':float(fs), 'minutes':float(fs)*60,\n                             'hours':float(fs)*3600}[time_units]\n\n    # Plot the annotations\n    for ch in range(n_annot):\n        if ann_samp[ch] is not None and len(ann_samp[ch]):\n            # Figure out the y values to plot on a channel basis\n\n            # 1 dimensional signals\n            if n_sig > ch:\n                if signal.ndim == 1:\n                    y = signal[ann_samp[ch]]\n                else:\n                    y = signal[ann_samp[ch], ch]\n            else:\n                y = np.zeros(len(ann_samp[ch]))\n\n            axes[ch].plot(ann_samp[ch] / downsample_factor, y, ann_style[ch])\n\n            # Plot the annotation symbols if any\n            if ann_sym is not None and ann_sym[ch] is not None:\n                for i, s in enumerate(ann_sym[ch]):\n                    axes[ch].annotate(s, (ann_samp[ch][i] / downsample_factor,\n                                          y[i]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_ecg_grids(ecg_grids, fs, units, time_units, axes):\n    \"Add ecg grids to the axes\"\n    if ecg_grids == 'all':\n        ecg_grids = range(0, len(axes))\n\n\n    for ch in ecg_grids:\n        # Get the initial plot limits\n        auto_xlims = axes[ch].get_xlim()\n        auto_ylims= axes[ch].get_ylim()\n\n        (major_ticks_x, minor_ticks_x, major_ticks_y,\n            minor_ticks_y) = calc_ecg_grids(auto_ylims[0], auto_ylims[1],\n                                            units[ch], fs, auto_xlims[1],\n                                            time_units)\n\n        min_x, max_x = np.min(minor_ticks_x), np.max(minor_ticks_x)\n        min_y, max_y = np.min(minor_ticks_y), np.max(minor_ticks_y)\n\n        for tick in minor_ticks_x:\n            axes[ch].plot([tick, tick], [min_y,  max_y], c='#ededed',\n                          marker='|', zorder=1)\n        for tick in major_ticks_x:\n            axes[ch].plot([tick, tick], [min_y, max_y], c='#bababa',\n                          marker='|', zorder=2)\n        for tick in minor_ticks_y:\n            axes[ch].plot([min_x, max_x], [tick, tick], c='#ededed',\n                          marker='_', zorder=1)\n        for tick in major_ticks_y:\n            axes[ch].plot([min_x, max_x], [tick, tick], c='#bababa',\n                          marker='_', zorder=2)\n\n        # Plotting the lines changes the graph. Set the limits back\n        axes[ch].set_xlim(auto_xlims)\n        axes[ch].set_ylim(auto_ylims)", "response": "Add ecg grids to the axes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate tick intervals for the ECG grids.", "response": "def calc_ecg_grids(minsig, maxsig, sig_units, fs, maxt, time_units):\n    \"\"\"\n    Calculate tick intervals for ecg grids\n\n    - 5mm 0.2s major grids, 0.04s minor grids\n    - 0.5mV major grids, 0.125 minor grids\n\n    10 mm is equal to 1mV in voltage.\n    \"\"\"\n    # Get the grid interval of the x axis\n    if time_units == 'samples':\n        majorx = 0.2 * fs\n        minorx = 0.04 * fs\n    elif time_units == 'seconds':\n        majorx = 0.2\n        minorx = 0.04\n    elif time_units == 'minutes':\n        majorx = 0.2 / 60\n        minorx = 0.04/60\n    elif time_units == 'hours':\n        majorx = 0.2 / 3600\n        minorx = 0.04 / 3600\n\n    # Get the grid interval of the y axis\n    if sig_units.lower()=='uv':\n        majory = 500\n        minory = 125\n    elif sig_units.lower()=='mv':\n        majory = 0.5\n        minory = 0.125\n    elif sig_units.lower()=='v':\n        majory = 0.0005\n        minory = 0.000125\n    else:\n        raise ValueError('Signal units must be uV, mV, or V to plot ECG grids.')\n\n    major_ticks_x = np.arange(0, upround(maxt, majorx) + 0.0001, majorx)\n    minor_ticks_x = np.arange(0, upround(maxt, majorx) + 0.0001, minorx)\n\n    major_ticks_y = np.arange(downround(minsig, majory),\n                              upround(maxsig, majory) + 0.0001, majory)\n    minor_ticks_y = np.arange(downround(minsig, majory),\n                              upround(maxsig, majory) + 0.0001, minory)\n\n    return (major_ticks_x, minor_ticks_x, major_ticks_y, minor_ticks_y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds title and axes labels", "response": "def label_figure(axes, n_subplots, time_units, sig_name, sig_units, ylabel,\n                 title):\n    \"Add title, and axes labels\"\n    if title:\n        axes[0].set_title(title)\n\n    # Determine y label\n    # Explicit labels take precedence if present. Otherwise, construct labels\n    # using signal names and units\n    if not ylabel:\n        ylabel = []\n        # Set default channel and signal names if needed\n        if not sig_name:\n            sig_name = ['ch_'+str(i) for i in range(n_subplots)]\n        if not sig_units:\n            sig_units = n_subplots * ['NU']\n\n        ylabel = ['/'.join(pair) for pair in zip(sig_name, sig_units)]\n\n        # If there are annotations with channels outside of signal range\n        # put placeholders\n        n_missing_labels = n_subplots - len(ylabel)\n        if n_missing_labels:\n            ylabel = ylabel + ['ch_%d/NU' % i for i in range(len(ylabel),\n                                                             n_subplots)]\n\n    for ch in range(n_subplots):\n        axes[ch].set_ylabel(ylabel[ch])\n\n    axes[-1].set_xlabel('/'.join(['time', time_units[:-1]]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_wfdb(record=None, annotation=None, plot_sym=False,\n              time_units='samples', title=None, sig_style=[''],\n              ann_style=['r*'], ecg_grids=[], figsize=None, return_fig=False):\n    \"\"\"\n    Subplot individual channels of a wfdb record and/or annotation.\n\n    This function implements the base functionality of the `plot_items`\n    function, while allowing direct input of wfdb objects.\n\n    If the record object is input, the function will extract from it:\n      - signal values, from the `p_signal` (priority) or `d_signal` attribute\n      - sampling frequency, from the `fs` attribute\n      - signal names, from the `sig_name` attribute\n      - signal units, from the `units` attribute\n\n    If the annotation object is input, the function will extract from it:\n      - sample locations, from the `sample` attribute\n      - symbols, from the `symbol` attribute\n      - the annotation channels, from the `chan` attribute\n      - the sampling frequency, from the `fs` attribute if present, and if fs\n        was not already extracted from the `record` argument.\n\n    Parameters\n    ----------\n    record : wfdb Record, optional\n        The Record object to be plotted\n    annotation : wfdb Annotation, optional\n        The Annotation object to be plotted\n    plot_sym : bool, optional\n        Whether to plot the annotation symbols on the graph.\n    time_units : str, optional\n        The x axis unit. Allowed options are: 'samples', 'seconds',\n        'minutes', and 'hours'.\n    title : str, optional\n        The title of the graph.\n    sig_style : list, optional\n        A list of strings, specifying the style of the matplotlib plot\n        for each signal channel. The list length should match the number\n        of signal channels. If the list has a length of 1, the style\n        will be used for all channels.\n    ann_style : list, optional\n        A list of strings, specifying the style of the matplotlib plot\n        for each annotation channel. The list length should match the\n        number of annotation channels. If the list has a length of 1,\n        the style will be used for all channels.\n    ecg_grids : list, optional\n        A list of integers specifying channels in which to plot ecg grids. May\n        also be set to 'all' for all channels. Major grids at 0.5mV, and minor\n        grids at 0.125mV. All channels to be plotted with grids must have\n        `sig_units` equal to 'uV', 'mV', or 'V'.\n    figsize : tuple, optional\n        Tuple pair specifying the width, and height of the figure. It is the\n        'figsize' argument passed into matplotlib.pyplot's `figure` function.\n    return_fig : bool, optional\n        Whether the figure is to be returned as an output argument.\n\n    Returns\n    -------\n    figure : matplotlib figure, optional\n        The matplotlib figure generated. Only returned if the 'return_fig'\n        option is set to True.\n\n    Examples\n    --------\n    >>> record = wfdb.rdrecord('sample-data/100', sampto=3000)\n    >>> annotation = wfdb.rdann('sample-data/100', 'atr', sampto=3000)\n\n    >>> wfdb.plot_wfdb(record=record, annotation=annotation, plot_sym=True\n                       time_units='seconds', title='MIT-BIH Record 100',\n                       figsize=(10,4), ecg_grids='all')\n\n    \"\"\"\n    (signal, ann_samp, ann_sym, fs,\n        ylabel, record_name) = get_wfdb_plot_items(record=record,\n                                                   annotation=annotation,\n                                                   plot_sym=plot_sym)\n\n    return plot_items(signal=signal, ann_samp=ann_samp, ann_sym=ann_sym, fs=fs,\n                      time_units=time_units, ylabel=ylabel,\n                      title=(title or record_name),\n                      sig_style=sig_style,\n                      ann_style=ann_style, ecg_grids=ecg_grids,\n                      figsize=figsize, return_fig=return_fig)", "response": "This function plots individual channels of a wfdb record or annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets items to plot from wfdb objects", "response": "def get_wfdb_plot_items(record, annotation, plot_sym):\n    \"\"\"\n    Get items to plot from wfdb objects\n    \"\"\"\n    # Get record attributes\n    if record:\n        if record.p_signal is not None:\n            signal = record.p_signal\n        elif record.d_signal is not None:\n            signal = record.d_signal\n        else:\n            raise ValueError('The record has no signal to plot')\n\n        fs = record.fs\n        sig_name = record.sig_name\n        sig_units = record.units\n        record_name = 'Record: %s' % record.record_name\n        ylabel = ['/'.join(pair) for pair in zip(sig_name, sig_units)]\n    else:\n        signal = fs = ylabel = record_name = None\n\n    # Get annotation attributes\n    if annotation:\n        # Get channels\n        ann_chans = set(annotation.chan)\n        n_ann_chans = max(ann_chans) + 1\n\n        # Indices for each channel\n        chan_inds = n_ann_chans * [np.empty(0, dtype='int')]\n\n        for chan in ann_chans:\n            chan_inds[chan] = np.where(annotation.chan == chan)[0]\n\n        ann_samp = [annotation.sample[ci] for ci in chan_inds]\n\n        if plot_sym:\n            ann_sym = n_ann_chans * [None]\n            for ch in ann_chans:\n                ann_sym[ch] = [annotation.symbol[ci] for ci in chan_inds[ch]]\n        else:\n            ann_sym = None\n\n        # Try to get fs from annotation if not already in record\n        if fs is None:\n            fs = annotation.fs\n\n        record_name = record_name or annotation.record_name\n    else:\n        ann_samp = None\n        ann_sym = None\n\n    # Cleaning: remove empty channels and set labels and styles.\n\n    # Wrangle together the signal and annotation channels if necessary\n    if record and annotation:\n        # There may be instances in which the annotation `chan`\n        # attribute has non-overlapping channels with the signal.\n        # In this case, omit empty middle channels. This function should\n        # already process labels and arrangements before passing into\n        # `plot_items`\n        sig_chans = set(range(signal.shape[1]))\n        all_chans = sorted(sig_chans.union(ann_chans))\n\n        # Need to update ylabels and annotation values\n        if sig_chans != all_chans:\n            compact_ann_samp = []\n            if plot_sym:\n                compact_ann_sym = []\n            else:\n                compact_ann_sym = None\n            ylabel = []\n            for ch in all_chans: # ie. 0, 1, 9\n                if ch in ann_chans:\n                    compact_ann_samp.append(ann_samp[ch])\n                    if plot_sym:\n                        compact_ann_sym.append(ann_sym[ch])\n                if ch in sig_chans:\n                    ylabel.append(''.join([sig_name[ch], sig_units[ch]]))\n                else:\n                    ylabel.append('ch_%d/NU' % ch)\n            ann_samp = compact_ann_samp\n            ann_sym = compact_ann_sym\n        # Signals encompass annotations\n        else:\n            ylabel = ['/'.join(pair) for pair in zip(sig_name, sig_units)]\n\n    # Remove any empty middle channels from annotations\n    elif annotation:\n        ann_samp = [a for a in ann_samp if a.size]\n        if ann_sym is not None:\n            ann_sym = [a for a in ann_sym if a]\n        ylabel = ['ch_%d/NU' % ch for ch in ann_chans]\n\n    return signal, ann_samp, ann_sym, fs, ylabel, record_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot all wfdb records in a directory.", "response": "def plot_all_records(directory=''):\n    \"\"\"\n    Plot all wfdb records in a directory (by finding header files), one at\n    a time, until the 'enter' key is pressed.\n\n    Parameters\n    ----------\n    directory : str, optional\n        The directory in which to search for WFDB records. Defaults to\n        current working directory.\n\n    \"\"\"\n    directory = directory or os.getcwd()\n\n    headers = [f for f in os.listdir(directory) if os.path.isfile(\n        os.path.join(directory, f))]\n    headers = [f for f in headers if f.endswith('.hea')]\n\n    records = [h.split('.hea')[0] for h in headers]\n    records.sort()\n\n    for record_name in records:\n        record = rdrecord(os.path.join(directory, record_name))\n\n        plot_wfdb(record, title='Record - %s' % record.record_name)\n        input('Press enter to continue...')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remote_file_size(url=None, file_name=None, pb_dir=None):\n\n    # Option to construct the url\n    if file_name and pb_dir:\n        url = posixpath.join(config.db_index_url, pb_dir, file_name)\n\n    response = requests.head(url, headers={'Accept-Encoding': 'identity'})\n    # Raise HTTPError if invalid url\n    response.raise_for_status()\n\n    # Supposed size of the file\n    remote_file_size = int(response.headers['content-length'])\n\n    return remote_file_size", "response": "Returns the size of the remote file in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _stream_header(file_name, pb_dir):\n    # Full url of header location\n    url = posixpath.join(config.db_index_url, pb_dir, file_name)\n    response = requests.get(url)\n\n    # Raise HTTPError if invalid url\n    response.raise_for_status()\n\n    # Get each line as a string\n    filelines = response.content.decode('iso-8859-1').splitlines()\n\n    # Separate content into header and comment lines\n    header_lines = []\n    comment_lines = []\n\n    for line in filelines:\n        line = str(line.strip())\n        # Comment line\n        if line.startswith('#'):\n            comment_lines.append(line)\n        # Non-empty non-comment line = header line.\n        elif line:\n            # Look for a comment in the line\n            ci = line.find('#')\n            if ci > 0:\n                header_lines.append(line[:ci])\n                # comment on same line as header line\n                comment_lines.append(line[ci:])\n            else:\n                header_lines.append(line)\n\n    return (header_lines, comment_lines)", "response": "Stream the lines of a remote header file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstreaming data from a remote dat file into a 1d numpy array.", "response": "def _stream_dat(file_name, pb_dir, byte_count, start_byte, dtype):\n    \"\"\"\n    Stream data from a remote dat file, into a 1d numpy array.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the dat file to be read.\n    pb_dir : str\n        The physiobank directory where the dat file is located.\n    byte_count : int\n        The number of bytes to be read.\n    start_byte : int\n        The starting byte number to read from.\n    dtype : str\n        The numpy dtype to load the data into.\n\n    Returns\n    -------\n    sig_data : numpy array\n        The data read from the dat file.\n\n    \"\"\"\n\n    # Full url of dat file\n    url = posixpath.join(config.db_index_url, pb_dir, file_name)\n\n    # Specify the byte range\n    end_byte = start_byte + byte_count - 1\n    headers = {\"Range\":\"bytes=%d-%d\" % (start_byte, end_byte),\n               'Accept-Encoding': '*'}\n\n    # Get the content\n    response = requests.get(url, headers=headers, stream=True)\n\n    # Raise HTTPError if invalid url\n    response.raise_for_status()\n\n    # Convert to numpy array\n    sig_data = np.fromstring(response.content, dtype=dtype)\n\n    return sig_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstream an entire remote annotation file from physiobank.", "response": "def _stream_annotation(file_name, pb_dir):\n    \"\"\"\n    Stream an entire remote annotation file from physiobank\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the annotation file to be read.\n    pb_dir : str\n        The physiobank directory where the annotation file is located.\n\n    \"\"\"\n    # Full url of annotation file\n    url = posixpath.join(config.db_index_url, pb_dir, file_name)\n\n    # Get the content\n    response = requests.get(url)\n    # Raise HTTPError if invalid url\n    response.raise_for_status()\n\n    # Convert to numpy array\n    ann_data = np.fromstring(response.content, dtype=np.dtype('<u1'))\n\n    return ann_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dbs():\n    url = posixpath.join(config.db_index_url, 'DBS')\n    response = requests.get(url)\n\n    dbs = response.content.decode('ascii').splitlines()\n    dbs = [re.sub('\\t{2,}', '\\t', line).split('\\t') for line in dbs]\n\n    return dbs", "response": "Get a list of all the Physiobank databases available."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of records belonging to a database.", "response": "def get_record_list(db_dir, records='all'):\n    \"\"\"\n    Get a list of records belonging to a database.\n\n    Parameters\n    ----------\n    db_dir : str\n        The database directory, usually the same as the database slug.\n        The location to look for a RECORDS file.\n    records : list, optional\n        A Option used when this function acts as a helper function.\n        Leave as default 'all' to get all records.\n\n    Examples\n    --------\n    >>> wfdb.get_record_list('mitdb')\n\n    \"\"\"\n    # Full url physiobank database\n    db_url = posixpath.join(config.db_index_url, db_dir)\n\n    # Check for a RECORDS file\n    if records == 'all':\n        response = requests.get(posixpath.join(db_url, 'RECORDS'))\n        if response.status_code == 404:\n            raise ValueError('The database %s has no WFDB files to download' % db_url)\n\n        # Get each line as a string\n        record_list = response.content.decode('ascii').splitlines()\n    # Otherwise the records are input manually\n    else:\n        record_list = records\n\n    return record_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_local_dirs(dl_dir, dl_inputs, keep_subdirs):\n\n    # Make the local download dir if it doesn't exist\n    if not os.path.isdir(dl_dir):\n        os.makedirs(dl_dir)\n        print('Created local base download directory: %s' % dl_dir)\n    # Create all required local subdirectories\n    # This must be out of dl_pb_file to\n    # avoid clash in multiprocessing\n    if keep_subdirs:\n        dl_dirs = set([os.path.join(dl_dir, d[1]) for d in dl_inputs])\n        for d in dl_dirs:\n            if not os.path.isdir(d):\n                os.makedirs(d)\n    return", "response": "Make any required local directories to download the entire tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload a file from physiobank and return the ID of the file.", "response": "def dl_pb_file(inputs):\n    \"\"\"\n    Download a file from physiobank.\n\n    The input args are to be unpacked for the use of multiprocessing\n    map, because python2 doesn't have starmap...\n\n    \"\"\"\n\n    basefile, subdir, db, dl_dir, keep_subdirs, overwrite = inputs\n\n    # Full url of file\n    url = posixpath.join(config.db_index_url, db, subdir, basefile)\n\n    # Supposed size of the file\n    remote_file_size = _remote_file_size(url)\n\n    # Figure out where the file should be locally\n    if keep_subdirs:\n        dldir = os.path.join(dl_dir, subdir)\n    else:\n        dldir = dl_dir\n\n    local_file = os.path.join(dldir, basefile)\n\n    # The file exists locally.\n    if os.path.isfile(local_file):\n        # Redownload regardless\n        if overwrite:\n            dl_full_file(url, local_file)\n        # Process accordingly.\n        else:\n            local_file_size = os.path.getsize(local_file)\n            # Local file is smaller than it should be. Append it.\n            if local_file_size < remote_file_size:\n                print('Detected partially downloaded file: %s Appending file...' % local_file)\n                headers = {\"Range\": \"bytes=\"+str(local_file_size)+\"-\", 'Accept-Encoding': '*'}\n                r = requests.get(url, headers=headers, stream=True)\n                print('headers: ', headers)\n                print('r content length: ', len(r.content))\n                with open(local_file, 'ba') as writefile:\n                    writefile.write(r.content)\n                print('Done appending.')\n            # Local file is larger than it should be. Redownload.\n            elif local_file_size > remote_file_size:\n                dl_full_file(url, local_file)\n            # If they're the same size, do nothing.\n\n    # The file doesn't exist. Download it.\n    else:\n        dl_full_file(url, local_file)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a file. No checks are performed.", "response": "def dl_full_file(url, save_file_name):\n    \"\"\"\n    Download a file. No checks are performed.\n\n    Parameters\n    ----------\n    url : str\n        The url of the file to download\n    save_file_name : str\n        The name to save the file as\n\n    \"\"\"\n    response = requests.get(url)\n    with open(save_file_name, 'wb') as writefile:\n        writefile.write(response.content)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dl_files(db, dl_dir, files, keep_subdirs=True, overwrite=False):\n\n    # Full url physiobank database\n    db_url = posixpath.join(config.db_index_url, db)\n    # Check if the database is valid\n    response = requests.get(db_url)\n    response.raise_for_status()\n\n    # Construct the urls to download\n    dl_inputs = [(os.path.split(file)[1], os.path.split(file)[0], db, dl_dir, keep_subdirs, overwrite) for file in files]\n\n    # Make any required local directories\n    make_local_dirs(dl_dir, dl_inputs, keep_subdirs)\n\n    print('Downloading files...')\n    # Create multiple processes to download files.\n    # Limit to 2 connections to avoid overloading the server\n    pool = multiprocessing.Pool(processes=2)\n    pool.map(dl_pb_file, dl_inputs)\n    print('Finished downloading files')\n\n    return", "response": "Download specified files from a Physiobank database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a tuple triplets into a pd dataframe", "response": "def label_triplets_to_df(triplets):\n    \"\"\"\n    Get a pd dataframe from a tuple triplets\n    used to define annotation labels.\n\n    The triplets should come in the\n    form: (label_store, symbol, description)\n    \"\"\"\n\n    label_df = pd.DataFrame({'label_store':np.array([t[0] for t in triplets],\n                                                    dtype='int'),\n                             'symbol':[t[1] for t in triplets],\n                             'description':[t[2] for t in triplets]})\n\n    label_df.set_index(label_df['label_store'].values, inplace=True)\n    label_df = label_df[list(ann_label_fields)]\n\n    return label_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef custom_triplet_bytes(custom_triplet):\n    # Structure: 0, NOTE, len(aux_note), aux_note, codenumber, space, codesymbol, space, description, (0 null if necessary)\n    # Remember, aux_note string includes 'number(s)<space><symbol><space><description>''\n    annbytes = [0, 88, len(custom_triplet[2]) + 3 + len(str(custom_triplet[0])), 252] + [ord(c) for c in str(custom_triplet[0])] \\\n               + [32] + [ord(custom_triplet[1])] + [32] + [ord(c) for c in custom_triplet[2]]\n\n    if len(annbytes) % 2:\n        annbytes.append(0)\n\n    return annbytes", "response": "Convert a custom triplet of label_store symbol description into a byte array for defining custom labels in the annotation file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compact_carry_field(full_field):\n\n    # Keep in mind that the field may already be compact or None\n\n    if full_field is None:\n        return None\n\n    # List of same length. Place None where element\n    # does not need to be written\n    compact_field = [None]*len(full_field)\n\n    prev_field = 0\n\n    for i in range(len(full_field)):\n        current_field = full_field[i]\n        if current_field != prev_field:\n            compact_field[i] = current_field\n            prev_field = current_field\n\n    # May further simplify\n    if np.array_equal(compact_field, [None]*len(full_field)):\n        compact_field = None\n\n    return compact_field", "response": "Return the compact list version of a list of anomalous anomal."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wrann(record_name, extension, sample, symbol=None, subtype=None, chan=None,\n          num=None, aux_note=None, label_store=None, fs=None,\n          custom_labels=None, write_dir=''):\n    \"\"\"\n    Write a WFDB annotation file.\n\n    Specify at least the following:\n\n    - The record name of the WFDB record (record_name)\n    - The annotation file extension (extension)\n    - The annotation locations in samples relative to the beginning of\n      the record (sample)\n    - Either the numerical values used to store the labels\n      (`label_store`), or more commonly, the display symbols of each\n      label (`symbol`).\n\n    Parameters\n    ----------\n    record_name : str\n        The string name of the WFDB record to be written (without any file\n        extensions).\n    extension : str\n        The string annotation file extension.\n    sample : numpy array\n        A numpy array containing the annotation locations in samples relative to\n        the beginning of the record.\n    symbol : list, or numpy array, optional\n        The symbols used to display the annotation labels. List or numpy array.\n        If this field is present, `label_store` must not be present.\n    subtype : numpy array, optional\n        A numpy array containing the marked class/category of each annotation.\n    chan : numpy array, optional\n        A numpy array containing the signal channel associated with each\n        annotation.\n    num : numpy array, optional\n        A numpy array containing the labelled annotation number for each\n        annotation.\n    aux_note : list, optional\n        A list containing the auxiliary information string (or None for\n        annotations without notes) for each annotation.\n    label_store : numpy array, optional\n        A numpy array containing the integer values used to store the\n        annotation labels. If this field is present, `symbol` must not be\n        present.\n    fs : int, or float, optional\n        The numerical sampling frequency of the record to be written to the file.\n    custom_labels : pandas dataframe, optional\n        The map of custom defined annotation labels used for this annotation, in\n        addition to the standard WFDB annotation labels. Custom labels are\n        defined by two or three fields:\n\n        - The integer values used to store custom annotation labels in the file\n          (optional)\n        - Their short display symbols\n        - Their long descriptions.\n\n        This input argument may come in four formats:\n\n        1. A pandas.DataFrame object with columns:\n           ['label_store', 'symbol', 'description']\n        2. A pandas.DataFrame object with columns: ['symbol', 'description']\n           If this option is chosen, label_store values are automatically chosen.\n        3. A list or tuple of tuple triplets, with triplet elements\n           representing: (label_store, symbol, description).\n        4. A list or tuple of tuple pairs, with pair elements representing:\n           (symbol, description). If this option is chosen, label_store values\n           are automatically chosen.\n\n        If the `label_store` field is given for this function, and\n        `custom_labels` is defined, `custom_labels` must contain `label_store`\n        in its mapping. ie. it must come in format 1 or 3 above.\n    write_dir : str, optional\n        The directory in which to write the annotation file\n\n    Notes\n    -----\n    This is a gateway function, written as a simple way to write WFDB annotation\n    files without needing to explicity create an Annotation object. You may also\n    create an Annotation object, manually set its attributes, and call its\n    `wrann` instance method.\n\n    Each annotation stored in a WFDB annotation file contains a sample field and\n    a label field. All other fields may or may not be present.\n\n    Examples\n    --------\n    >>> # Read an annotation as an Annotation object\n    >>> annotation = wfdb.rdann('b001', 'atr', pb_dir='cebsdb')\n    >>> # Write a copy of the annotation file\n    >>> wfdb.wrann('b001', 'cpy', annotation.sample, annotation.symbol)\n\n    \"\"\"\n\n    # Create Annotation object\n    annotation = Annotation(record_name=record_name, extension=extension,\n                            sample=sample, symbol=symbol, subtype=subtype,\n                            chan=chan, num=num, aux_note=aux_note,\n                            label_store=label_store, fs=fs,\n                            custom_labels=custom_labels)\n\n    # Find out which input field describes the labels\n    if symbol is None:\n        if label_store is None:\n            raise Exception(\"Either the 'symbol' field or the 'label_store' field must be set\")\n    else:\n        if label_store is None:\n            annotation.sym_to_aux()\n        else:\n            raise Exception(\"Only one of the 'symbol' and 'label_store' fields may be input, for describing annotation labels\")\n\n    # Perform field checks and write the annotation file\n    annotation.wrann(write_fs=True, write_dir=write_dir)", "response": "Writes a WFDB annotation file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rdann(record_name, extension, sampfrom=0, sampto=None, shift_samps=False,\n          pb_dir=None, return_label_elements=['symbol'],\n          summarize_labels=False):\n    \"\"\"\n    Read a WFDB annotation file record_name.extension and return an\n    Annotation object.\n\n    Parameters\n    ----------\n    record_name : str\n        The record name of the WFDB annotation file. ie. for file '100.atr',\n        record_name='100'.\n    extension : str\n        The annotatator extension of the annotation file. ie. for  file\n        '100.atr', extension='atr'.\n    sampfrom : int, optional\n        The minimum sample number for annotations to be returned.\n    sampto : int, optional\n        The maximum sample number for annotations to be returned.\n    shift_samps : bool, optional\n        Specifies whether to return the sample indices relative to `sampfrom`\n        (True), or sample 0 (False).\n    pb_dir : str, optional\n        Option used to stream data from Physiobank. The Physiobank database\n        directory from which to find the required annotation file. eg. For\n        record '100' in 'http://physionet.org/physiobank/database/mitdb':\n        pb_dir='mitdb'.\n    return_label_elements : list, optional\n        The label elements that are to be returned from reading the annotation\n        file. A list with at least one of the following options: 'symbol',\n        'label_store', 'description'.\n    summarize_labels : bool, optional\n        If True, assign a summary table of the set of annotation labels\n        contained in the file to the 'contained_labels' attribute of the\n        returned object. This table will contain the columns:\n        ['label_store', 'symbol', 'description', 'n_occurrences']\n\n    Returns\n    -------\n    annotation : Annotation\n        The Annotation object. Call help(wfdb.Annotation) for the attribute\n        descriptions.\n\n    Notes\n    -----\n    For every annotation sample, the annotation file explictly stores the\n    'sample' and 'symbol' fields, but not necessarily the others. When reading\n    annotation files using this function, fields which are not stored in the\n    file will either take their default values of 0 or None, or will be carried\n    over from their previous values if any.\n\n    Examples\n    --------\n    >>> ann = wfdb.rdann('sample-data/100', 'atr', sampto=300000)\n\n    \"\"\"\n\n    return_label_elements = check_read_inputs(sampfrom, sampto,\n                                              return_label_elements)\n\n    # Read the file in byte pairs\n    filebytes = load_byte_pairs(record_name, extension, pb_dir)\n\n    # Get wfdb annotation fields from the file bytes\n    (sample, label_store, subtype,\n     chan, num, aux_note) = proc_ann_bytes(filebytes, sampto)\n\n    # Get the indices of annotations that hold definition information about\n    # the entire annotation file, and other empty annotations to be removed.\n    potential_definition_inds, rm_inds = get_special_inds(sample, label_store,\n                                                          aux_note)\n\n    # Try to extract information describing the annotation file\n    (fs,\n     custom_labels) = interpret_defintion_annotations(potential_definition_inds,\n                                                      aux_note)\n\n    # Remove annotations that do not store actual sample and label information\n    (sample, label_store, subtype,\n     chan, num, aux_note) = rm_empty_indices(rm_inds, sample, label_store,\n                                             subtype, chan, num, aux_note)\n\n    # Convert lists to numpy arrays dtype='int'\n    (sample, label_store, subtype,\n     chan, num) = lists_to_int_arrays(sample, label_store, subtype, chan, num)\n\n    # Try to get fs from the header file if it is not contained in the\n    # annotation file\n    if fs is None:\n        try:\n            rec = record.rdheader(record_name, pb_dir)\n            fs = rec.fs\n        except:\n            pass\n\n    # Create the annotation object\n    annotation = Annotation(record_name=os.path.split(record_name)[1],\n                            extension=extension, sample=sample,\n                            label_store=label_store,  subtype=subtype,\n                            chan=chan, num=num, aux_note=aux_note, fs=fs,\n                            custom_labels=custom_labels)\n\n    # Apply the desired index range\n    if sampfrom > 0 and sampto is not None:\n        annotation.apply_range(sampfrom=sampfrom, sampto=sampto)\n\n    # If specified, obtain annotation samples relative to the starting\n    # index\n    if shift_samps and len(sample) > 0 and sampfrom:\n        annotation.sample = annotation.sample - sampfrom\n\n    # Get the set of unique label definitions contained in this\n    # annotation\n    if summarize_labels:\n        annotation.get_contained_labels(inplace=True)\n\n    # Set/unset the desired label values\n    annotation.set_label_elements(return_label_elements)\n\n    return annotation", "response": "Read a WFDB annotation file record_name. extension and return an Annotation object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses extra field of the current annotation.", "response": "def proc_extra_field(label_store, filebytes, bpi, subtype, chan, num, aux_note, update):\n    \"\"\"\n    Process extra fields belonging to the current annotation.\n    Potential updated fields: subtype, chan, num, aux_note\n    \"\"\"\n\n    # aux_note and sub are reset between annotations. chan and num copy over\n    # previous value if missing.\n\n    # SUB\n    if label_store == 61:\n        # sub is interpreted as signed char.\n        subtype.append(filebytes[bpi, 0].astype('i1'))\n        update['subtype'] = False\n        bpi = bpi + 1\n    # CHAN\n    elif label_store == 62:\n        # chan is interpreted as un_signed char\n        chan.append(filebytes[bpi, 0])\n        update['chan'] = False\n        bpi = bpi + 1\n    # NUM\n    elif label_store == 60:\n        # num is interpreted as signed char\n        num.append(filebytes[bpi, 0].astype('i1'))\n        update['num'] = False\n        bpi = bpi + 1\n    # aux_note\n    elif label_store == 63:\n        # length of aux_note string. Max 256? No need to check other bits of\n        # second byte?\n        aux_notelen = filebytes[bpi, 0]\n        aux_notebytes = filebytes[bpi + 1:bpi + 1 + int(np.ceil(aux_notelen / 2.)),:].flatten()\n        if aux_notelen & 1:\n            aux_notebytes = aux_notebytes[:-1]\n        # The aux_note string\n        aux_note.append(\"\".join([chr(char) for char in aux_notebytes]))\n        update['aux_note'] = False\n        bpi = bpi + 1 + int(np.ceil(aux_notelen / 2.))\n\n    return subtype, chan, num, aux_note, update, bpi"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_extra_fields(subtype, chan, num, aux_note, update):\n\n    if update['subtype']:\n        subtype.append(0)\n\n    if update['chan']:\n        if chan == []:\n            chan.append(0)\n        else:\n            chan.append(chan[-1])\n    if update['num']:\n        if num == []:\n            num.append(0)\n        else:\n            num.append(num[-1])\n\n    if update['aux_note']:\n        aux_note.append('')\n\n    return subtype, chan, num, aux_note", "response": "Update the fields in the current annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the indices of annotations that hold definition information about the entire annotation file and other empty annotations to be removed.", "response": "def get_special_inds(sample, label_store, aux_note):\n    \"\"\"\n    Get the indices of annotations that hold definition information about\n    the entire annotation file, and other empty annotations to be removed.\n\n    Note: There is no need to deal with SKIP annotations (label_store=59)\n          which were already dealt with in proc_core_fields and hence not\n          included here.\n    \"\"\"\n\n    s0_inds = np.where(sample == np.int64(0))[0]\n    note_inds = np.where(label_store == np.int64(22))[0]\n\n    # sample = 0 with aux_note means there should be an fs or custom label definition.\n    # Either way, they are to be removed.\n    potential_definition_inds = set(s0_inds).intersection(note_inds)\n\n    # Other indices which are not actual annotations.\n    notann_inds = np.where(label_store == np.int64(0))[0]\n\n    rm_inds = potential_definition_inds.union(set(notann_inds))\n\n    return potential_definition_inds, rm_inds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninterpret the potential_definition_inds and aux_note strings to extract the information that may be contained in the annotation notes.", "response": "def interpret_defintion_annotations(potential_definition_inds, aux_note):\n    \"\"\"\n    Try to extract annotation definition information from annotation notes.\n    Information that may be contained:\n    - fs - sample=0, label_state=22, aux_note='## time resolution: XXX'\n    - custom annotation label definitions\n    \"\"\"\n\n    fs = None\n    custom_labels = []\n\n    if len(potential_definition_inds) > 0:\n        i = 0\n        while i<len(potential_definition_inds):\n            if aux_note[i].startswith('## '):\n                if not fs:\n                    search_fs = rx_fs.findall(aux_note[i])\n                    if search_fs:\n                        fs = float(search_fs[0])\n                        if round(fs, 8) == float(int(fs)):\n                            fs = int(fs)\n                        i += 1\n                        continue\n                if aux_note[i] == '## annotation type definitions':\n                    i += 1\n                    while aux_note[i] != '## end of definitions':\n                        label_store, symbol, description = rx_custom_label.findall(aux_note[i])[0]\n                        custom_labels.append((int(label_store), symbol, description))\n                        i += 1\n                    i += 1\n            else:\n                i += 1\n\n    if not custom_labels:\n        custom_labels = None\n\n    return fs, custom_labels"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove unwanted list indices from the list.", "response": "def rm_empty_indices(*args):\n    \"\"\"\n    Remove unwanted list indices. First argument is the list\n    of indices to remove. Other elements are the lists\n    to trim.\n    \"\"\"\n    rm_inds = args[0]\n\n    if not rm_inds:\n        return args[1:]\n\n    keep_inds = [i for i in range(len(args[1])) if i not in rm_inds]\n\n    return [[a[i] for i in keep_inds] for a in args[1:]]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter the annotation attributes to keep only items between the sample values sampfrom and sampto", "response": "def apply_range(self, sampfrom=0, sampto=None):\n        \"\"\"\n        Filter the annotation attributes to keep only items between the\n        desired sample values\n\n        \"\"\"\n        sampto = sampto or self.sample[-1]\n\n        kept_inds = np.intersect1d(np.where(self.sample>=sampfrom),\n                                   np.where(self.sample<=sampto))\n\n\n        for field in ['sample', 'label_store', 'subtype', 'chan', 'num']:\n            setattr(self, field, getattr(self, field)[kept_inds])\n\n        self.aux_note = [self.aux_note[i] for i in kept_inds]\n\n        self.ann_len = len(self.sample)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a WFDB annotation file from this object.", "response": "def wrann(self, write_fs=False, write_dir=''):\n        \"\"\"\n        Write a WFDB annotation file from this object.\n\n        Parameters\n        ----------\n        write_fs : bool, optional\n            Whether to write the `fs` attribute to the file.\n\n        \"\"\"\n        for field in ['record_name', 'extension']:\n            if getattr(self, field) is None:\n                raise Exception('Missing required field for writing annotation file: ',field)\n\n        present_label_fields = self.get_label_fields()\n        if not present_label_fields:\n            raise Exception('At least one annotation label field is required to write the annotation: ', ann_label_fields)\n\n        # Check the validity of individual fields\n        self.check_fields()\n\n        # Standardize the format of the custom_labels field\n        self.standardize_custom_labels()\n\n        # Create the label map used in this annotaion\n        self.create_label_map()\n\n        # Check the cohesion of fields\n        self.check_field_cohesion(present_label_fields)\n\n        # Calculate the label_store field if necessary\n        if 'label_store' not in present_label_fields:\n            self.convert_label_attribute(source_field=present_label_fields[0],\n                                         target_field='label_store')\n\n        # Write the header file using the specified fields\n        self.wr_ann_file(write_fs=write_fs, write_dir=write_dir)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the present label fields in the object", "response": "def get_label_fields(self):\n        \"\"\"\n        Get the present label fields in the object\n        \"\"\"\n        present_label_fields = []\n        for field in ann_label_fields:\n            if getattr(self, field) is not None:\n                present_label_fields.append(field)\n\n        return present_label_fields"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_field(self, field):\n\n        item = getattr(self, field)\n\n        if not isinstance(item, ALLOWED_TYPES[field]):\n            raise TypeError('The '+field+' field must be one of the following types:', ALLOWED_TYPES[field])\n\n        # Numerical integer annotation fields: sample, label_store, sub,\n        # chan, num\n        if ALLOWED_TYPES[field] == (np.ndarray):\n            record.check_np_array(item=item, field_name=field, ndim=1,\n                                  parent_class=np.integer, channel_num=None)\n\n        # Field specific checks\n        if field == 'record_name':\n            if bool(re.search('[^-\\w]', self.record_name)):\n                raise ValueError('record_name must only comprise of letters, digits, hyphens, and underscores.')\n        elif field == 'extension':\n            if bool(re.search('[^a-zA-Z]', self.extension)):\n                raise ValueError('extension must only comprise of letters.')\n        elif field == 'fs':\n            if self.fs <=0:\n                raise ValueError('The fs field must be a non-negative number')\n        elif field == 'custom_labels':\n            \"\"\"\n            The role of this section is just to check the\n            elements of this item, without utilizing\n            any other fields. No format conversion\n            or free value looksups etc are done.\n            \"\"\"\n\n            # Check the structure of the subelements\n            if isinstance(item, pd.DataFrame):\n                column_names = list(item)\n                if 'symbol' in column_names and 'description' in column_names:\n                    if 'label_store' in column_names:\n                        label_store = list(item['label_store'].values)\n                    else:\n                        label_store = None\n                    symbol = item['symbol'].values\n                    description = item['description'].values\n                else:\n                    raise ValueError(''.join(['If the '+field+' field is pandas dataframe, its columns',\n                                             ' must be one of the following:\\n-[label_store, symbol, description]',\n                                             '\\n-[symbol, description]']))\n            else:\n                if set([len(i) for i in item]) == {2}:\n                    label_store = None\n                    symbol = [i[0] for i in item]\n                    description = [i[1] for i in item]\n                elif set([len(i) for i in item]) == {3}:\n                    label_store = [i[0] for i in item]\n                    symbol = [i[1] for i in item]\n                    description = [i[2] for i in item]\n                else:\n                    raise ValueError(''.join(['If the '+field+' field is an array-like object, its subelements',\n                                             ' must be one of the following:\\n- tuple triplets storing: ',\n                                             '(label_store, symbol, description)\\n- tuple pairs storing: ',\n                                             '(symbol, description)']))\n\n            # Check the values of the subelements\n            if label_store:\n                if len(item) != len(set(label_store)):\n                    raise ValueError('The label_store values of the '+field+' field must be unique')\n\n                if min(label_store) < 1 or max(label_store) > 49:\n                    raise ValueError('The label_store values of the custom_labels field must be between 1 and 49')\n\n            if len(item) != len(set(symbol)):\n                raise ValueError('The symbol values of the '+field+' field must be unique')\n\n            for i in range(len(item)):\n                if label_store:\n                    if not hasattr(label_store[i], '__index__'):\n                        raise TypeError('The label_store values of the '+field+' field must be integer-like')\n\n                if not isinstance(symbol[i], str_types) or len(symbol[i]) not in [1,2,3]:\n                    raise ValueError('The symbol values of the '+field+' field must be strings of length 1 to 3')\n\n                if bool(re.search('[ \\t\\n\\r\\f\\v]', symbol[i])):\n                    raise ValueError('The symbol values of the '+field+' field must not contain whitespace characters')\n\n                if not isinstance(description[i], str_types):\n                    raise TypeError('The description values of the '+field+' field must be strings')\n\n                # Would be good to enfore this but existing garbage annotations have tabs and newlines...\n                #if bool(re.search('[\\t\\n\\r\\f\\v]', description[i])):\n                #    raise ValueError('The description values of the '+field+' field must not contain tabs or newlines')\n\n        # The string fields\n        elif field in ['symbol', 'description', 'aux_note']:\n            uniq_elements = set(item)\n\n            for e in uniq_elements:\n                if not isinstance(e, str_types):\n                    raise TypeError('Subelements of the '+field+' field must be strings')\n\n            if field == 'symbol':\n                for e in uniq_elements:\n                    if len(e) not in [1,2,3]:\n                        raise ValueError('Subelements of the '+field+' field must be strings of length 1 to 3')\n                    if bool(re.search('[ \\t\\n\\r\\f\\v]', e)):\n                        raise ValueError('Subelements of the '+field+' field may not contain whitespace characters')\n            else:\n                for e in uniq_elements:\n                    if bool(re.search('[\\t\\n\\r\\f\\v]', e)):\n                        raise ValueError('Subelements of the '+field+' field must not contain tabs or newlines')\n\n        elif field == 'sample':\n            if len(self.sample) == 1:\n                sampdiffs = np.array([self.sample[0]])\n            elif len(self.sample) > 1:\n                sampdiffs = np.concatenate(([self.sample[0]], np.diff(self.sample)))\n            else:\n                raise ValueError(\"The 'sample' field must be a numpy array with length greater than 0\")\n            if min(self.sample) < 0 :\n                raise ValueError(\"The 'sample' field must only contain non-negative integers\")\n            if min(sampdiffs) < 0 :\n                raise ValueError(\"The 'sample' field must contain monotonically increasing sample numbers\")\n            if max(sampdiffs) > 2147483648:\n                raise ValueError('WFDB annotation files cannot store sample differences greater than 2**31')\n\n        elif field == 'label_store':\n            if min(item) < 1 or max(item) > 49:\n                raise ValueError('The label_store values must be between 1 and 49')\n\n        # The C WFDB library stores num/sub/chan as chars.\n        elif field == 'subtype':\n            # signed character\n            if min(self.subtype) < 0 or max(self.subtype) >127:\n                raise ValueError(\"The 'subtype' field must only contain non-negative integers up to 127\")\n        elif field == 'chan':\n            # un_signed character\n            if min(self.chan) < 0 or max(self.chan) >255:\n                raise ValueError(\"The 'chan' field must only contain non-negative integers up to 255\")\n        elif field == 'num':\n            # signed character\n            if min(self.num) < 0 or max(self.num) >127:\n                raise ValueError(\"The 'num' field must only contain non-negative integers up to 127\")\n\n        return", "response": "Check a particular annotation field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_field_cohesion(self, present_label_fields):\n        # Ensure all written annotation fields have the same length\n        nannots = len(self.sample)\n\n        for field in ['sample', 'num', 'subtype', 'chan', 'aux_note']+present_label_fields:\n            if getattr(self, field) is not None:\n                if len(getattr(self, field)) != nannots:\n                    raise ValueError(\"The lengths of the 'sample' and '\"+field+\"' fields do not match\")\n\n        # Ensure all label fields are defined by the label map. This has to be checked because\n        # it is possible the user defined (or lack of) custom_labels does not capture all the\n        # labels present.\n        for field in present_label_fields:\n            defined_values = self.__label_map__[field].values\n\n            if set(getattr(self, field)) - set(defined_values) != set():\n                raise ValueError('\\n'.join(['\\nThe '+field+' field contains elements not encoded in the stardard WFDB annotation labels, or this object\\'s custom_labels field',\n                                      '- To see the standard WFDB annotation labels, call: show_ann_labels()',\n                                      '- To transfer non-encoded symbol items into the aux_note field, call: self.sym_to_aux()',\n                                      '- To define custom labels, set the custom_labels field as a list of tuple triplets with format: (label_store, symbol, description)']))\n\n        return", "response": "Checks that the content and structure of different fields are consistent with one another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the custom_labels field of the object to a standardized format: 3 column pandas df with ann_label_fields as columns. Does nothing if there are no custom labels defined. Does nothing if custom_labels is already a df with all 3 columns If custom_labels is an iterable of pairs/triplets, this function will convert it into a df. If the label_store attribute is not already defined, this function will automatically choose values by trying to use: 1. The undefined store values from the standard wfdb annotation label map. 2. The unused label store values. This is extracted by finding the set of all labels contained in this annotation object and seeing which symbols/descriptions are not used. If there are more custom labels defined than there are enough spaces, even in condition 2 from above, this function will raise an error. This function must work when called as a standalone.", "response": "def standardize_custom_labels(self):\n        \"\"\"\n        Set the custom_labels field of the object to a standardized format:\n        3 column pandas df with ann_label_fields as columns.\n\n        Does nothing if there are no custom labels defined.\n        Does nothing if custom_labels is already a df with all 3 columns\n\n        If custom_labels is an iterable of pairs/triplets, this\n        function will convert it into a df.\n\n        If the label_store attribute is not already defined, this\n        function will automatically choose values by trying to use:\n        1. The undefined store values from the standard wfdb annotation\n           label map.\n        2. The unused label store values. This is extracted by finding the\n           set of all labels contained in this annotation object and seeing\n           which symbols/descriptions are not used.\n\n        If there are more custom labels defined than there are enough spaces,\n        even in condition 2 from above, this function will raise an error.\n\n        This function must work when called as a standalone.\n        \"\"\"\n        custom_labels = self.custom_labels\n\n        if custom_labels is None:\n            return\n\n        self.check_field('custom_labels')\n\n        # Convert to dataframe if not already\n        if not isinstance(custom_labels, pd.DataFrame):\n            if len(self.custom_labels[0]) == 2:\n                symbol = self.get_custom_label_attribute('symbol')\n                description = self.get_custom_label_attribute('description')\n                custom_labels = pd.DataFrame({'symbol': symbol, 'description': description})\n            else:\n                label_store = self.get_custom_label_attribute('label_store')\n                symbol = self.get_custom_label_attribute('symbol')\n                description = self.get_custom_label_attribute('description')\n                custom_labels = pd.DataFrame({'label_store':label_store, 'symbol': symbol, 'description': description})\n\n        # Assign label_store values to the custom labels if not defined\n        if 'label_store' not in list(custom_labels):\n            undefined_label_stores = self.get_undefined_label_stores()\n\n            if len(custom_labels) > len(undefined_label_stores):\n                available_label_stores = self.get_available_label_stores()\n            else:\n                available_label_stores = undefined_label_stores\n\n            n_custom_labels = custom_labels.shape[0]\n\n            if n_custom_labels > len(available_label_stores):\n                raise ValueError('There are more custom_label definitions than storage values available for them.')\n\n            custom_labels['label_store'] = available_label_stores[:n_custom_labels]\n\n        custom_labels.set_index(custom_labels['label_store'].values, inplace=True)\n        custom_labels = custom_labels[list(ann_label_fields)]\n\n        self.custom_labels = custom_labels\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of available label stores for writing this annotation.", "response": "def get_available_label_stores(self, usefield='tryall'):\n        \"\"\"\n        Get the label store values that may be used\n        for writing this annotation.\n\n        Available store values include:\n        - the undefined values in the standard wfdb labels\n        - the store values not used in the current\n          annotation object.\n        - the store values whose standard wfdb symbols/descriptions\n          match those of the custom labels (if custom_labels exists)\n\n        If 'usefield' is explicitly specified, the function will use that\n        field to figure out available label stores. If 'usefield'\n        is set to 'tryall', the function will choose one of the contained\n        attributes by checking availability in the order: label_store, symbol, description\n        \"\"\"\n\n\n        # Figure out which field to use to get available labels stores.\n        if usefield == 'tryall':\n            if self.label_store is not None:\n                usefield = 'label_store'\n            elif self.symbol is not None:\n                usefield = 'symbol'\n            elif self.description is not None:\n                usefield = 'description'\n            else:\n                raise ValueError('No label fields are defined. At least one of the following is required: ', ann_label_fields)\n            return self.get_available_label_stores(usefield = usefield)\n        # Use the explicitly stated field to get available stores.\n        else:\n            # If usefield == 'label_store', there are slightly fewer/different steps\n            # compared to if it were another option\n\n            contained_field = getattr(self, usefield)\n\n            # Get the unused label_store values\n            if usefield == 'label_store':\n                unused_label_stores = set(ann_label_table['label_store'].values) - contained_field\n            else:\n                # the label_store values from the standard wfdb annotation labels\n                # whose symbols are not contained in this annotation\n                unused_field = set(ann_label_table[usefield].values) - contained_field\n                unused_label_stores = ann_label_table.loc[ann_label_table[usefield] in unused_field, 'label_store'].values\n\n            # Get the standard wfdb label_store values overwritten by the\n            # custom_labels if any\n            if self.custom_symbols is not None:\n                custom_field = set(self.get_custom_label_attribute(usefield))\n                if usefield == 'label_store':\n                    overwritten_label_stores = set(custom_field).intersection(set(ann_label_table['label_store']))\n                else:\n                    overwritten_fields = set(custom_field).intersection(set(ann_label_table[usefield]))\n                    overwritten_label_stores = ann_label_table.loc[ann_label_table[usefield] in overwritten_fields, 'label_store'].values\n            else:\n                overwritten_label_stores = set()\n\n\n            # The undefined values in the standard wfdb labels\n            undefined_label_stores = self.get_undefined_label_stores()\n            # Final available label stores = undefined + unused + overwritten\n            available_label_stores = set(undefined_label_stores).union(set(unused_label_stores)).union(overwritten_label_stores)\n\n            return available_label_stores"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_custom_label_attribute(self, attribute):\n\n        if attribute not in ann_label_fields:\n            raise ValueError('Invalid attribute specified')\n\n        if isinstance(self.custom_labels, pd.DataFrame):\n            if 'label_store' not in list(self.custom_labels):\n                raise ValueError('label_store not defined in custom_labels')\n            a = list(self.custom_labels[attribute].values)\n        else:\n            if len(self.custom_labels[0]) == 2:\n                if attribute == 'label_store':\n                    raise ValueError('label_store not defined in custom_labels')\n                elif attribute == 'symbol':\n                    a = [l[0] for l in self.custom_labels]\n                elif attribute == 'description':\n                    a = [l[1] for l in self.custom_labels]\n            else:\n                if attribute == 'label_store':\n                    a = [l[0] for l in self.custom_labels]\n                elif attribute == 'symbol':\n                    a = [l[1] for l in self.custom_labels]\n                elif attribute == 'description':\n                    a = [l[2] for l in self.custom_labels]\n\n        return a", "response": "Get a list of the custom_labels attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a mapping df based on ann_label_table and self. custom_labels.", "response": "def create_label_map(self, inplace=True):\n        \"\"\"\n        Creates mapping df based on ann_label_table and self.custom_labels.\n\n        Table composed of entire WFDB standard annotation table, overwritten/appended\n        with custom_labels if any. Sets __label_map__ attribute, or returns value.\n        \"\"\"\n\n        label_map =  ann_label_table.copy()\n\n        if self.custom_labels is not None:\n            self.standardize_custom_labels()\n            for i in self.custom_labels.index:\n                label_map.loc[i] = self.custom_labels.loc[i]\n\n        if inplace:\n            self.__label_map__ = label_map\n        else:\n            return label_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the annotation file for the current record.", "response": "def wr_ann_file(self, write_fs, write_dir=''):\n        \"\"\"\n        Calculate the bytes used to encode an annotation set and\n        write them to an annotation file\n        \"\"\"\n\n        # Calculate the fs bytes to write if present and desired to write\n        if write_fs:\n            fs_bytes = self.calc_fs_bytes()\n        else:\n            fs_bytes = []\n        # Calculate the custom_labels bytes to write if present\n        cl_bytes = self.calc_cl_bytes()\n        # Calculate the core field bytes to write\n        core_bytes = self.calc_core_bytes()\n\n        # Mark the end of the special annotation types if needed\n        if fs_bytes == [] and cl_bytes == []:\n            end_special_bytes = []\n        else:\n            end_special_bytes = [0, 236, 255, 255, 255, 255, 1, 0]\n\n        # Write the file\n        with open(os.path.join(write_dir, self.record_name+'.'+self.extension),\n                  'wb') as f:\n            # Combine all bytes to write: fs (if any), custom annotations (if any), main content, file terminator\n            np.concatenate((fs_bytes, cl_bytes, end_special_bytes, core_bytes,\n                            np.array([0,0]))).astype('u1').tofile(f)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_core_bytes(self):\n        # The difference sample to write\n        if len(self.sample) == 1:\n            sampdiff = np.array([self.sample[0]])\n        else:\n            sampdiff = np.concatenate(([self.sample[0]], np.diff(self.sample)))\n\n        # Create a copy of the annotation object with a\n        # compact version of fields to write\n        compact_annotation = copy.deepcopy(self)\n        compact_annotation.compact_fields()\n\n\n        # The optional fields to be written. Write if they are not None or all empty\n        extra_write_fields = []\n\n        for field in ['num', 'subtype', 'chan', 'aux_note']:\n            if not isblank(getattr(compact_annotation, field)):\n                extra_write_fields.append(field)\n\n        data_bytes = []\n\n        # Iterate across all fields one index at a time\n        for i in range(len(sampdiff)):\n\n            # Process the samp (difference) and sym items\n            data_bytes.append(field2bytes('samptype', [sampdiff[i], self.symbol[i]]))\n\n            # Process the extra optional fields\n            for field in extra_write_fields:\n                value = getattr(compact_annotation, field)[i]\n                if value is not None:\n                    data_bytes.append(field2bytes(field, value))\n\n        # Flatten and convert to correct format\n        data_bytes = np.array([item for sublist in data_bytes for item in sublist]).astype('u1')\n\n        return data_bytes", "response": "Convert all used annotation fields into bytes to write\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a pandas dataframe or sets the contained_labels_ attribute of the object.", "response": "def get_contained_labels(self, inplace=True):\n        \"\"\"\n        Get the set of unique labels contained in this annotation.\n        Returns a pandas dataframe or sets the contained_labels\n        attribute of the object.\n\n\n        Requires the label_store field to be set.\n\n        Function will try to use attributes contained in the order:\n        1. label_store\n        2. symbol\n        3. description\n\n        This function should also be called\n        to summarize information about an\n        annotation after it has been\n        read. Should not be a helper function\n        to others except rdann.\n        \"\"\"\n        if self.custom_labels is not None:\n            self.check_field('custom_labels')\n\n        # Create the label map\n        label_map = ann_label_table.copy()\n\n        # Convert the tuple triplets into a pandas dataframe if needed\n        if isinstance(self.custom_labels, (list, tuple)):\n            custom_labels = label_triplets_to_df(self.custom_labels)\n        elif isinstance(self.custom_labels, pd.DataFrame):\n            # Set the index just in case it doesn't already match the label_store\n            self.custom_labels.set_index(\n                self.custom_labels['label_store'].values, inplace=True)\n            custom_labels = self.custom_labels\n        else:\n            custom_labels = None\n\n        # Merge the standard wfdb labels with the custom labels.\n        # custom labels values overwrite standard wfdb if overlap.\n        if custom_labels is not None:\n            for i in custom_labels.index:\n                label_map.loc[i] = custom_labels.loc[i]\n            # This doesn't work...\n            # label_map.loc[custom_labels.index] = custom_labels.loc[custom_labels.index]\n\n        # Get the labels using one of the features\n        if self.label_store is not None:\n            index_vals = set(self.label_store)\n            reset_index = False\n            counts = np.unique(self.label_store, return_counts=True)\n        elif self.symbol is not None:\n            index_vals = set(self.symbol)\n            label_map.set_index(label_map['symbol'].values, inplace=True)\n            reset_index = True\n            counts = np.unique(self.symbol, return_counts=True)\n        elif self.description is not None:\n            index_vals = set(self.description)\n            label_map.set_index(label_map['description'].values, inplace=True)\n            reset_index = True\n            counts = np.unique(self.description, return_counts=True)\n        else:\n            raise Exception('No annotation labels contained in object')\n\n        contained_labels = label_map.loc[index_vals, :]\n\n        # Add the counts\n        for i in range(len(counts[0])):\n            contained_labels.loc[counts[0][i], 'n_occurrences'] = counts[1][i]\n        contained_labels['n_occurrences'] = pd.to_numeric(contained_labels['n_occurrences'], downcast='integer')\n\n        if reset_index:\n            contained_labels.set_index(contained_labels['label_store'].values,\n                                       inplace=True)\n\n        if inplace:\n            self.contained_labels = contained_labels\n            return\n        else:\n            return contained_labels"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets one or more label elements based on the ones in the object.", "response": "def set_label_elements(self, wanted_label_elements):\n        \"\"\"\n        Set one or more label elements based on\n        at least one of the others\n        \"\"\"\n        if isinstance(wanted_label_elements, str):\n            wanted_label_elements = [wanted_label_elements]\n\n        # Figure out which desired label elements are missing\n        missing_elements = [e for e in wanted_label_elements if getattr(self, e) is None]\n\n        contained_elements = [e for e in ann_label_fields if getattr(self, e )is not None]\n\n        if not contained_elements:\n            raise Exception('No annotation labels contained in object')\n\n        for e in missing_elements:\n            self.convert_label_attribute(contained_elements[0], e)\n\n        unwanted_label_elements = list(set(ann_label_fields)\n                                       - set(wanted_label_elements))\n\n        self.rm_attributes(unwanted_label_elements)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_label_attribute(self, source_field, target_field, inplace=True,\n                                overwrite=True):\n        \"\"\"\n        Convert one label attribute (label_store, symbol, or description) to another.\n        Input arguments:\n        - inplace - If True, sets the object attribute. If False, returns the value.\n        - overwrite - if True, performs conversion and replaces target field attribute even if the\n          target attribute already has a value. If False, does not perform conversion in the aforementioned case.\n          Set to True (do conversion) if inplace=False.\n\n        Creates mapping df on the fly based on ann_label_table and self.custom_labels\n        \"\"\"\n        if inplace and not overwrite:\n            if getattr(self, target_field) is not None:\n                return\n\n        label_map = self.create_label_map(inplace=False)\n        label_map.set_index(label_map[source_field].values, inplace=True)\n\n        target_item = label_map.loc[getattr(self, source_field), target_field].values\n\n        if target_field != 'label_store':\n            # Should already be int64 dtype if target is label_store\n            target_item = list(target_item)\n\n        if inplace:\n            setattr(self, target_field, target_item)\n        else:\n            return target_item", "response": "Convert one label attribute to another."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _rd_segment(file_name, dir_name, pb_dir, fmt, n_sig, sig_len, byte_offset,\n                samps_per_frame, skew, sampfrom, sampto, channels,\n                smooth_frames, ignore_skew):\n    \"\"\"\n    Read the digital samples from a single segment record's associated\n    dat file(s).\n\n    Parameters\n    ----------\n    file_name : list\n        The names of the dat files to be read.\n    dir_name : str\n        The full directory where the dat file(s) are located, if the dat\n        file(s) are local.\n    pb_dir : str\n        The physiobank directory where the dat file(s) are located, if\n        the dat file(s) are remote.\n    fmt : list\n        The formats of the dat files\n    n_sig : int\n        The number of signals contained in the dat file\n    sig_len : int\n        The signal length (per channel) of the dat file\n    byte_offset : int\n        The byte offset of the dat file\n    samps_per_frame : list\n        The samples/frame for each signal of the dat file\n    skew : list\n        The skew for the signals of the dat file\n    sampfrom : int\n        The starting sample number to be read from the signals\n    sampto : int\n        The final sample number to be read from the signals\n    smooth_frames : bool\n        Whether to smooth channels with multiple samples/frame\n    ignore_skew : bool, optional\n        Used when reading records with at least one skewed signal.\n        Specifies whether to apply the skew to align the signals in the\n        output variable (False), or to ignore the skew field and load in\n        all values contained in the dat files unaligned (True).\n\n    Returns\n    -------\n    signals : numpy array, or list\n        The signals read from the dat file(s). A 2d numpy array is\n        returned if the signals have uniform samples/frame or if\n        `smooth_frames` is True. Otherwise a list of 1d numpy arrays\n        is returned.\n\n    Notes\n    -----\n    'channels', 'sampfrom', 'sampto', 'smooth_frames', and 'ignore_skew'\n    are user desired input fields. All other parameters are\n    specifications of the segment\n\n    \"\"\"\n\n    # Avoid changing outer variables\n    byte_offset = byte_offset[:]\n    samps_per_frame = samps_per_frame[:]\n    skew = skew[:]\n\n    # Set defaults for empty fields\n    for i in range(n_sig):\n        if byte_offset[i] == None:\n            byte_offset[i] = 0\n        if samps_per_frame[i] == None:\n            samps_per_frame[i] = 1\n        if skew[i] == None:\n            skew[i] = 0\n\n    # If skew is to be ignored, set all to 0\n    if ignore_skew:\n        skew = [0]*n_sig\n\n    # Get the set of dat files, and the\n    # channels that belong to each file.\n    file_name, datchannel = describe_list_indices(file_name)\n\n    # Some files will not be read depending on input channels.\n    # Get the the wanted fields only.\n    w_file_name = [] # one scalar per dat file\n    w_fmt = {} # one scalar per dat file\n    w_byte_offset = {} # one scalar per dat file\n    w_samps_per_frame = {} # one list per dat file\n    w_skew = {} # one list per dat file\n    w_channel = {} # one list per dat file\n\n    for fn in file_name:\n        # intersecting dat channels between the input channels and the channels of the file\n        idc = [c for c in datchannel[fn] if c in channels]\n\n        # There is at least one wanted channel in the dat file\n        if idc != []:\n            w_file_name.append(fn)\n            w_fmt[fn] = fmt[datchannel[fn][0]]\n            w_byte_offset[fn] = byte_offset[datchannel[fn][0]]\n            w_samps_per_frame[fn] = [samps_per_frame[c] for c in datchannel[fn]]\n            w_skew[fn] = [skew[c] for c in datchannel[fn]]\n            w_channel[fn] = idc\n\n    # Wanted dat channels, relative to the dat file itself\n    r_w_channel =  {}\n    # The channels in the final output array that correspond to the read channels in each dat file\n    out_dat_channel = {}\n    for fn in w_channel:\n        r_w_channel[fn] = [c - min(datchannel[fn]) for c in w_channel[fn]]\n        out_dat_channel[fn] = [channels.index(c) for c in w_channel[fn]]\n\n    # Signals with multiple samples/frame are smoothed, or all signals have 1 sample/frame.\n    # Return uniform numpy array\n    if smooth_frames or sum(samps_per_frame) == n_sig:\n        # Figure out the largest required dtype for the segment to minimize memory usage\n        max_dtype = _np_dtype(_fmt_res(fmt, max_res=True), discrete=True)\n        # Allocate signal array. Minimize dtype\n        signals = np.zeros([sampto-sampfrom, len(channels)], dtype=max_dtype)\n\n        # Read each wanted dat file and store signals\n        for fn in w_file_name:\n            signals[:, out_dat_channel[fn]] = _rd_dat_signals(fn, dir_name, pb_dir,\n                w_fmt[fn], len(datchannel[fn]), sig_len, w_byte_offset[fn],\n                w_samps_per_frame[fn], w_skew[fn], sampfrom, sampto,\n                smooth_frames)[:, r_w_channel[fn]]\n\n    # Return each sample in signals with multiple samples/frame, without smoothing.\n    # Return a list of numpy arrays for each signal.\n    else:\n        signals = [None] * len(channels)\n\n        for fn in w_file_name:\n            # Get the list of all signals contained in the dat file\n            datsignals = _rd_dat_signals(fn, dir_name, pb_dir, w_fmt[fn],\n                len(datchannel[fn]), sig_len, w_byte_offset[fn],\n                w_samps_per_frame[fn], w_skew[fn], sampfrom, sampto,\n                smooth_frames)\n\n            # Copy over the wanted signals\n            for cn in range(len(out_dat_channel[fn])):\n                signals[out_dat_channel[fn][cn]] = datsignals[r_w_channel[fn][cn]]\n\n    return signals", "response": "Read the digital samples from a single segment record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading all signals from a WFDB dat file.", "response": "def _rd_dat_signals(file_name, dir_name, pb_dir, fmt, n_sig, sig_len,\n                   byte_offset, samps_per_frame, skew, sampfrom, sampto,\n                   smooth_frames):\n    \"\"\"\n    Read all signals from a WFDB dat file.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the dat file\n    * other params\n        See docstring for `_rd_segment`.\n\n    Returns\n    -------\n    signals : numpy array, or list\n        See docstring for `_rd_segment`.\n\n    Notes\n    -----\n    See docstring notes for `_rd_segment`.\n\n    \"\"\"\n\n    # Total number of samples per frame\n    tsamps_per_frame = sum(samps_per_frame)\n    # The signal length to read (per channel)\n    read_len = sampto - sampfrom\n\n    # Calculate parameters used to read and process the dat file\n    (start_byte, n_read_samples, block_floor_samples,\n     extra_flat_samples, nan_replace) = _dat_read_params(fmt, sig_len,\n                                                         byte_offset, skew,\n                                                         tsamps_per_frame,\n                                                         sampfrom, sampto)\n\n    # Number of bytes to be read from the dat file\n    total_read_bytes = _required_byte_num('read', fmt, n_read_samples)\n\n    # Total samples to be processed in intermediate step. Includes extra\n    # padded samples beyond dat file\n    total_process_samples = n_read_samples + extra_flat_samples\n\n    # Total number of bytes to be processed in intermediate step.\n    total_process_bytes = _required_byte_num('read', fmt,\n                                             total_process_samples)\n\n    # Get the intermediate bytes or samples to process. Bit of a\n    # discrepancy. Recall special formats load uint8 bytes, other formats\n    # already load samples.\n\n    # Read values from dat file. Append bytes/samples if needed.\n    if extra_flat_samples:\n        if fmt in UNALIGNED_FMTS:\n            # Extra number of bytes to append onto the bytes read from\n            # the dat file.\n            n_extra_bytes = total_process_bytes - total_read_bytes\n\n            sig_data = np.concatenate((_rd_dat_file(file_name, dir_name,\n                                                     pb_dir, fmt, start_byte,\n                                                     n_read_samples),\n                                        np.zeros(n_extra_bytes,\n                                                 dtype=np.dtype(DATA_LOAD_TYPES[fmt]))))\n        else:\n            sig_data = np.concatenate((_rd_dat_file(file_name, dir_name,\n                                                     pb_dir, fmt, start_byte,\n                                                     n_read_samples),\n                                        np.zeros(extra_flat_samples,\n                                                 dtype=np.dtype(DATA_LOAD_TYPES[fmt]))))\n    else:\n        sig_data = _rd_dat_file(file_name, dir_name, pb_dir, fmt, start_byte,\n                                 n_read_samples)\n\n    # Finish processing the read data into proper samples if not already\n\n    # For unaligned fmts, turn the uint8 blocks into actual samples\n    if fmt in UNALIGNED_FMTS:\n        sig_data = _blocks_to_samples(sig_data, total_process_samples, fmt)\n        # Remove extra leading sample read within the byte block if any\n        if block_floor_samples:\n            sig_data = sig_data[block_floor_samples:]\n\n    # Adjust samples values for byte offset formats\n    if fmt in OFFSET_FMTS:\n        if fmt == '80':\n            sig_data = (sig_data.astype('int16') - 128).astype('int8')\n        elif fmt == '160':\n            sig_data = (sig_data.astype('int32') - 32768).astype('int16')\n\n    # At this point, dtype of sig_data is the minimum integer format\n    # required for storing the final digital samples.\n\n    # No extra samples/frame. Obtain original uniform numpy array\n    if tsamps_per_frame == n_sig:\n        # Reshape into multiple channels\n        signal = sig_data.reshape(-1, n_sig)\n        # Skew the signal\n        signal = _skew_sig(signal, skew, n_sig, read_len, fmt, nan_replace)\n    # Extra frames present to be smoothed. Obtain averaged uniform numpy array\n    elif smooth_frames:\n        # Allocate memory for smoothed signal.\n        signal = np.zeros((int(len(sig_data) / tsamps_per_frame) , n_sig),\n                       dtype=sig_data.dtype)\n\n        # Transfer and average samples\n        for ch in range(n_sig):\n            if samps_per_frame[ch] == 1:\n                signal[:, ch] = sig_data[sum(([0] + samps_per_frame)[:ch + 1])::tsamps_per_frame]\n            else:\n                if ch == 0:\n                    startind = 0\n                else:\n                    startind = np.sum(samps_per_frame[:ch])\n                signal[:,ch] = [np.average(sig_data[ind:ind+samps_per_frame[ch]]) for ind in range(startind,len(sig_data),tsamps_per_frame)]\n        # Skew the signal\n        signal = _skew_sig(signal, skew, n_sig, read_len, fmt, nan_replace)\n\n    # Extra frames present without wanting smoothing. Return all\n    # expanded samples.\n    else:\n        # List of 1d numpy arrays\n        signal = []\n        # Transfer over samples\n        for ch in range(n_sig):\n            # Indices of the flat signal that belong to the channel\n            ch_indices = np.concatenate([np.array(range(samps_per_frame[ch]))\n                                         + sum([0] + samps_per_frame[:ch])\n                                         + tsamps_per_frame * framenum for framenum in range(int(len(sig_data)/tsamps_per_frame))])\n            signal.append(sig_data[ch_indices])\n        # Skew the signal\n        signal = _skew_sig(signal, skew, n_sig, read_len, fmt, nan_replace, samps_per_frame)\n\n    # Integrity check of signal shape after reading\n    _check_sig_dims(signal, read_len, n_sig, samps_per_frame)\n\n    return signal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dat_read_params(fmt, sig_len, byte_offset, skew, tsamps_per_frame,\n                     sampfrom, sampto):\n    \"\"\"\n    Calculate the parameters used to read and process a dat file, given\n    its layout, and the desired sample range.\n\n    Parameters\n    ----------\n    fmt : str\n        The format of the dat file\n    sig_len : int\n        The signal length (per channel) of the dat file\n    byte_offset : int\n        The byte offset of the dat file\n    skew : list\n        The skew for the signals of the dat file\n    tsamps_per_frame : int\n        The total samples/frame for all channels of the dat file\n    sampfrom : int\n        The starting sample number to be read from the signals\n    sampto : int\n        The final sample number to be read from the signals\n\n    Returns\n    -------\n    start_byte : int\n        The starting byte to read the dat file from. Always points to\n        the start of a byte block for special formats.\n    n_read_samples : int\n        The number of flat samples to read from the dat file.\n    block_floor_samples : int\n        The extra samples read prior to the first desired sample, for\n        special formats, in order to ensure entire byte blocks are read.\n    extra_flat_samples : int\n        The extra samples desired beyond what is contained in the file.\n    nan_replace : list\n        The number of samples to replace with nan at the end of each\n        signal, due to skew wanting samples beyond the file.\n\n    Examples\n    --------\n    sig_len=100, t = 4 (total samples/frame), skew = [0, 2, 4, 5]\n    sampfrom=0, sampto=100 --> read_len = 100, n_sampread = 100*t, extralen = 5, nan_replace = [0, 2, 4, 5]\n    sampfrom=50, sampto=100 --> read_len = 50, n_sampread = 50*t, extralen = 5, nan_replace = [0, 2, 4, 5]\n    sampfrom=0, sampto=50 --> read_len = 50, n_sampread = 55*t, extralen = 0, nan_replace = [0, 0, 0, 0]\n    sampfrom=95, sampto=99 --> read_len = 4, n_sampread = 5*t, extralen = 4, nan_replace = [0, 1, 3, 4]\n\n    \"\"\"\n\n    # First flat sample number to read (if all channels were flattened)\n    start_flat_sample = sampfrom * tsamps_per_frame\n\n    # Calculate the last flat sample number to read.\n    # Cannot exceed sig_len * tsamps_per_frame, the number of samples\n    # stored in the file. If extra 'samples' are desired by the skew,\n    # keep track.\n    # Where was the -sampfrom derived from? Why was it in the formula?\n    if (sampto + max(skew)) > sig_len:\n        end_flat_sample = sig_len * tsamps_per_frame\n        extra_flat_samples = (sampto + max(skew) - sig_len) * tsamps_per_frame\n    else:\n        end_flat_sample = (sampto + max(skew)) * tsamps_per_frame\n        extra_flat_samples = 0\n\n    # Adjust the starting sample number to read from start of blocks for special fmts.\n    # Keep track of how many preceeding samples are read, to be discarded later.\n    if fmt == '212':\n        # Samples come in groups of 2, in 3 byte blocks\n        block_floor_samples = start_flat_sample % 2\n        start_flat_sample = start_flat_sample - block_floor_samples\n    elif fmt in ['310', '311']:\n        # Samples come in groups of 3, in 4 byte blocks\n        block_floor_samples = start_flat_sample % 3\n        start_flat_sample = start_flat_sample - block_floor_samples\n    else:\n        block_floor_samples = 0\n\n    # The starting byte to read from\n    start_byte = byte_offset + int(start_flat_sample * BYTES_PER_SAMPLE[fmt])\n\n    # The number of samples to read\n    n_read_samples = end_flat_sample - start_flat_sample\n\n    # The number of samples to replace with nan at the end of each signal\n    # due to skew wanting samples beyond the file\n    nan_replace = [max(0, sampto + s - sig_len) for s in skew]\n\n    return (start_byte, n_read_samples, block_floor_samples,\n            extra_flat_samples, nan_replace)", "response": "Calculate the parameters used to read a dat file and return the parameters used to process the dat file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine how many bytes needed to read or write a log file.", "response": "def _required_byte_num(mode, fmt, n_samp):\n    \"\"\"\n    Determine how many signal bytes are needed to read or write a\n    number of desired samples from a dat file.\n\n    Parameters\n    ----------\n    mode : str\n        Whether the file is to be read or written: 'read' or 'write'.\n    fmt : str\n        The wfdb dat format.\n    n_samp : int\n        The number of samples wanted.\n\n    Returns\n    -------\n    n_bytes : int\n        The number of bytes required to read or write the file\n\n    Notes\n    -----\n    Read and write require the same number in most cases. An exception\n    is fmt 311 for n_extra==2.\n\n    \"\"\"\n\n    if fmt == '212':\n        n_bytes = math.ceil(n_samp*1.5)\n    elif fmt in ['310', '311']:\n        n_extra = n_samp % 3\n\n        if n_extra == 2:\n            if fmt == '310':\n                n_bytes = upround(n_samp * 4/3, 4)\n            # 311\n            else:\n                if mode == 'read':\n                    n_bytes = math.ceil(n_samp * 4/3)\n                # Have to write more bytes for wfdb c to work\n                else:\n                    n_bytes = upround(n_samp * 4/3, 4)\n        # 0 or 1\n        else:\n            n_bytes = math.ceil(n_samp * 4/3 )\n    else:\n        n_bytes = n_samp * BYTES_PER_SAMPLE[fmt]\n\n    return int(n_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread data from a dat file into a 1d numpy array.", "response": "def _rd_dat_file(file_name, dir_name, pb_dir, fmt, start_byte, n_samp):\n    \"\"\"\n    Read data from a dat file, either local or remote, into a 1d numpy\n    array.\n\n    This is the lowest level dat reading function (along with\n    `_stream_dat` which this function may call), and is called by\n    `_rd_dat_signals`.\n\n    Parameters\n    ----------\n    start_byte : int\n        The starting byte number to read from.\n    n_samp : int\n        The total number of samples to read. Does NOT need to create\n        whole blocks for special format. Any number of samples should be\n        readable.\n    * other params\n        See docstring for `_rd_dat_signals`\n\n    Returns\n    -------\n    sig_data : numpy array\n        The data read from the dat file. The dtype varies depending on\n        fmt. Byte aligned fmts are read in their final required format.\n        Unaligned formats are read as uint8 to be further processed.\n\n    Notes\n    -----\n    See docstring notes for `_rd_dat_signals`\n\n    \"\"\"\n\n    # element_count is the number of elements to read using np.fromfile\n    # for local files\n    # byte_count is the number of bytes to read for streaming files\n    if fmt == '212':\n        byte_count = _required_byte_num('read', '212', n_samp)\n        element_count = byte_count\n    elif fmt in ['310', '311']:\n        byte_count = _required_byte_num('read', fmt, n_samp)\n        element_count = byte_count\n    else:\n        element_count = n_samp\n        byte_count = n_samp * BYTES_PER_SAMPLE[fmt]\n\n    # Local dat file\n    if pb_dir is None:\n        with open(os.path.join(dir_name, file_name), 'rb') as fp:\n            fp.seek(start_byte)\n            sig_data = np.fromfile(fp, dtype=np.dtype(DATA_LOAD_TYPES[fmt]),\n                                   count=element_count)\n    # Stream dat file from physiobank\n    else:\n        sig_data = download._stream_dat(file_name, pb_dir, byte_count,\n                                        start_byte,\n                                        np.dtype(DATA_LOAD_TYPES[fmt]))\n\n    return sig_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _blocks_to_samples(sig_data, n_samp, fmt):\n    if fmt == '212':\n        # Easier to process when dealing with whole blocks\n        if n_samp % 2:\n            n_samp += 1\n            added_samps = 1\n            sig_data = np.append(sig_data, np.zeros(1, dtype='uint8'))\n        else:\n            added_samps = 0\n\n        sig_data = sig_data.astype('int16')\n        sig = np.zeros(n_samp, dtype='int16')\n\n        # One sample pair is stored in one byte triplet.\n\n        # Even numbered samples\n        sig[0::2] = sig_data[0::3] + 256 * np.bitwise_and(sig_data[1::3], 0x0f)\n        # Odd numbered samples (len(sig) always > 1 due to processing of\n        # whole blocks)\n        sig[1::2] = sig_data[2::3] + 256*np.bitwise_and(sig_data[1::3] >> 4, 0x0f)\n\n        # Remove trailing sample read within the byte block if\n        # originally odd sampled\n        if added_samps:\n            sig = sig[:-added_samps]\n\n        # Loaded values as un_signed. Convert to 2's complement form:\n        # values > 2^11-1 are negative.\n        sig[sig > 2047] -= 4096\n\n    elif fmt == '310':\n        # Easier to process when dealing with whole blocks\n        if n_samp % 3:\n            n_samp = upround(n_samp,3)\n            added_samps = n_samp % 3\n            sig_data = np.append(sig_data, np.zeros(added_samps, dtype='uint8'))\n        else:\n            added_samps = 0\n\n        sig_data = sig_data.astype('int16')\n        sig = np.zeros(n_samp, dtype='int16')\n\n        # One sample triplet is stored in one byte quartet\n        # First sample is 7 msb of first byte and 3 lsb of second byte.\n        sig[0::3] = (sig_data[0::4] >> 1)[0:len(sig[0::3])] + 128 * np.bitwise_and(sig_data[1::4], 0x07)[0:len(sig[0::3])]\n        # Second signal is 7 msb of third byte and 3 lsb of forth byte\n        sig[1::3] = (sig_data[2::4] >> 1)[0:len(sig[1::3])] + 128 * np.bitwise_and(sig_data[3::4], 0x07)[0:len(sig[1::3])]\n        # Third signal is 5 msb of second byte and 5 msb of forth byte\n        sig[2::3] = np.bitwise_and((sig_data[1::4] >> 3), 0x1f)[0:len(sig[2::3])] + 32 * np.bitwise_and(sig_data[3::4] >> 3, 0x1f)[0:len(sig[2::3])]\n\n        # Remove trailing samples read within the byte block if\n        # originally not 3n sampled\n        if added_samps:\n            sig = sig[:-added_samps]\n\n        # Loaded values as un_signed. Convert to 2's complement form:\n        # values > 2^9-1 are negative.\n        sig[sig > 511] -= 1024\n\n    elif fmt == '311':\n        # Easier to process when dealing with whole blocks\n        if n_samp % 3:\n            n_samp = upround(n_samp,3)\n            added_samps = n_samp % 3\n            sig_data = np.append(sig_data, np.zeros(added_samps, dtype='uint8'))\n        else:\n            added_samps = 0\n\n        sig_data = sig_data.astype('int16')\n        sig = np.zeros(n_samp, dtype='int16')\n\n        # One sample triplet is stored in one byte quartet\n        # First sample is first byte and 2 lsb of second byte.\n        sig[0::3] = sig_data[0::4][0:len(sig[0::3])] + 256 * np.bitwise_and(sig_data[1::4], 0x03)[0:len(sig[0::3])]\n        # Second sample is 6 msb of second byte and 4 lsb of third byte\n        sig[1::3] = (sig_data[1::4] >> 2)[0:len(sig[1::3])] + 64 * np.bitwise_and(sig_data[2::4], 0x0f)[0:len(sig[1::3])]\n        # Third sample is 4 msb of third byte and 6 msb of forth byte\n        sig[2::3] = (sig_data[2::4] >> 4)[0:len(sig[2::3])] + 16 * np.bitwise_and(sig_data[3::4], 0x7f)[0:len(sig[2::3])]\n\n        # Remove trailing samples read within the byte block if\n        # originally not 3n sampled\n        if added_samps:\n            sig = sig[:-added_samps]\n\n        # Loaded values as un_signed. Convert to 2's complement form.\n        # Values > 2^9-1 are negative.\n        sig[sig > 511] -= 1024\n    return sig", "response": "Convert uint8 blocks into signal samples for unaligned dat formats."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nskew the signal and insert nans where skewed signal overran dat file.", "response": "def _skew_sig(sig, skew, n_sig, read_len, fmt, nan_replace, samps_per_frame=None):\n    \"\"\"\n    Skew the signal, insert nans and shave off end of array if needed.\n\n    Parameters\n    ----------\n    sig : numpy array\n        The original signal\n    skew : list\n        List of samples to skew for each signal\n    n_sig : int\n        The number of signals\n\n\n    Notes\n    -----\n    `fmt` is just for the correct nan value.\n    `samps_per_frame` is only used for skewing expanded signals.\n\n    \"\"\"\n    if max(skew)>0:\n\n        # Expanded frame samples. List of arrays.\n        if isinstance(sig, list):\n            # Shift the channel samples\n            for ch in range(n_sig):\n                if skew[ch]>0:\n                    sig[ch][:read_len*samps_per_frame[ch]] = sig[ch][skew[ch]*samps_per_frame[ch]:]\n\n            # Shave off the extra signal length at the end\n            for ch in range(n_sig):\n                sig[ch] = sig[ch][:read_len*samps_per_frame[ch]]\n\n            # Insert nans where skewed signal overran dat file\n            for ch in range(n_sig):\n                if nan_replace[ch]>0:\n                    sig[ch][-nan_replace[ch]:] = _digi_nan(fmt)\n        # Uniform array\n        else:\n            # Shift the channel samples\n            for ch in range(n_sig):\n                if skew[ch]>0:\n                    sig[:read_len, ch] = sig[skew[ch]:, ch]\n            # Shave off the extra signal length at the end\n            sig = sig[:read_len, :]\n\n            # Insert nans where skewed signal overran dat file\n            for ch in range(n_sig):\n                if nan_replace[ch]>0:\n                    sig[-nan_replace[ch]:, ch] = _digi_nan(fmt)\n\n    return sig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the signal s shape is correct.", "response": "def _check_sig_dims(sig, read_len, n_sig, samps_per_frame):\n    \"\"\"\n    Integrity check of a signal's shape after reading.\n\n    \"\"\"\n    if isinstance(sig, np.ndarray):\n        if sig.shape != (read_len, n_sig):\n            raise ValueError('Samples were not loaded correctly')\n    else:\n        if len(sig) != n_sig:\n            raise ValueError('Samples were not loaded correctly')\n        for ch in range(n_sig):\n            if len(sig[ch]) != samps_per_frame[ch] * read_len:\n                raise ValueError('Samples were not loaded correctly')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn min and max digital values for each format type.", "response": "def _digi_bounds(fmt):\n    \"\"\"\n    Return min and max digital values for each format type.\n    Accepts lists.\n\n    Parmeters\n    ---------\n    fmt : str, or list\n        The wfdb dat format, or a list of them.\n\n    \"\"\"\n    if isinstance(fmt, list):\n        return [_digi_bounds(f) for f in fmt]\n\n    if fmt == '80':\n        return (-128, 127)\n    elif fmt == '212':\n        return (-2048, 2047)\n    elif fmt == '16':\n        return (-32768, 32767)\n    elif fmt == '24':\n        return (-8388608, 8388607)\n    elif fmt == '32':\n        return (-2147483648, 2147483647)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _digi_nan(fmt):\n    if isinstance(fmt, list):\n        return [_digi_nan(f) for f in fmt]\n\n    if fmt == '80':\n        return -128\n    if fmt == '310':\n        return -512\n    if fmt == '311':\n        return -512\n    elif fmt == '212':\n        return -2048\n    elif fmt == '16':\n        return -32768\n    elif fmt == '61':\n        return -32768\n    elif fmt == '160':\n        return -32768\n    elif fmt == '24':\n        return -8388608\n    elif fmt == '32':\n        return -2147483648", "response": "Returns the wfdb digital value used to store nan for the format type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the most suitable wfdb format for the given signal and resolutions.", "response": "def _wfdb_fmt(bit_res, single_fmt=True):\n    \"\"\"\n    Return the most suitable wfdb format(s) to use given signal\n    resolutions.\n\n    Parameters\n    ----------\n    bit_res : int, or list\n        The resolution of the signal, or a list of resolutions, in bits.\n    single_fmt : bool, optional\n        Whether to return the format for the maximum resolution signal.\n\n    Returns\n    -------\n    fmt : str or list\n        The most suitable wfdb format(s) used to encode the signal(s).\n\n    \"\"\"\n    if isinstance(bit_res, list):\n        # Return a single format\n        if single_fmt:\n            bit_res = [max(bit_res)] * len(bit_res)\n\n        return [wfdb_fmt(r) for r in bit_res]\n\n    if bit_res <= 8:\n        return '80'\n    elif bit_res <= 12:\n        return '212'\n    elif bit_res <= 16:\n        return '16'\n    elif bit_res <= 24:\n        return '24'\n    else:\n        return '32'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fmt_res(fmt, max_res=False):\n    if isinstance(fmt, list):\n        if max_res:\n            # Allow None\n            bit_res = np.max([_fmt_res(f) for f in fmt if f is not None])\n        else:\n            bit_res = [_fmt_res(f) for f in fmt]\n        return bit_res\n\n    return BIT_RES[fmt]", "response": "Returns the resolution of the WFDB dat format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _np_dtype(bit_res, discrete):\n    bit_res = min(bit_res, 64)\n\n    for np_res in [8, 16, 32, 64]:\n        if bit_res <= np_res:\n            break\n\n    if discrete is True:\n        return 'int' + str(np_res)\n    else:\n        # No float8 dtype\n        return 'float' + str(max(np_res, 16))", "response": "Returns the numpy dtype used to store the signal of the given bit resolution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a dat file.", "response": "def wr_dat_file(file_name, fmt, d_signal, byte_offset, expanded=False,\n                e_d_signal=None, samps_per_frame=None, write_dir=''):\n    \"\"\"\n    Write a dat file. All bytes are written one at a time to avoid\n    endianness issues.\n\n    \"\"\"\n    # Combine list of arrays into single array\n    if expanded:\n        n_sig = len(e_d_signal)\n        sig_len = int(len(e_d_signal[0])/samps_per_frame[0])\n        # Effectively create MxN signal, with extra frame samples acting\n        # like extra channels\n        d_signal = np.zeros((sig_len, sum(samps_per_frame)), dtype = 'int64')\n        # Counter for channel number\n        expand_ch = 0\n        for ch in range(n_sig):\n            spf = samps_per_frame[ch]\n            for framenum in range(spf):\n                d_signal[:, expand_ch] = e_d_signal[ch][framenum::spf]\n                expand_ch = expand_ch + 1\n\n    # This n_sig is used for making list items.\n    # Does not necessarily represent number of signals (ie. for expanded=True)\n    n_sig = d_signal.shape[1]\n\n    if fmt == '80':\n        # convert to 8 bit offset binary form\n        d_signal = d_signal + 128\n        # Concatenate into 1D\n        d_signal = d_signal.reshape(-1)\n        # Convert to un_signed 8 bit dtype to write\n        b_write = d_signal.astype('uint8')\n\n    elif fmt == '212':\n        # Each sample is represented by a 12 bit two's complement\n        # amplitude. The first sample is obtained from the 12 least\n        # significant bits of the first byte pair (stored least\n        # significant byte first). The second sample is formed from the\n        # 4 remaining bits of the first byte pair (which are the 4 high\n        # bits of the 12-bit sample) and the next byte (which contains\n        # the remaining 8 bits of the second sample). The process is\n        # repeated for each successive pair of samples.\n\n        # convert to 12 bit two's complement\n        d_signal[d_signal<0] = d_signal[d_signal<0] + 4096\n\n        # Concatenate into 1D\n        d_signal = d_signal.reshape(-1)\n\n        n_samp = len(d_signal)\n        # use this for byte processing\n        processn_samp = n_samp\n\n        # Odd numbered number of samples. Fill in extra blank for\n        # following byte calculation.\n        if processn_samp % 2:\n            d_signal = np.concatenate([d_signal, np.array([0])])\n            processn_samp +=1\n\n        # The individual bytes to write\n        b_write = np.zeros([int(1.5*processn_samp)], dtype = 'uint8')\n\n        # Fill in the byte triplets\n\n        # Triplet 1 from lowest 8 bits of sample 1\n        b_write[0::3] = d_signal[0::2] & 255\n        # Triplet 2 from highest 4 bits of samples 1 (lower) and 2 (upper)\n        b_write[1::3] = ((d_signal[0::2] & 3840) >> 8) + ((d_signal[1::2] & 3840) >> 4)\n        # Triplet 3 from lowest 8 bits of sample 2\n        b_write[2::3] = d_signal[1::2] & 255\n\n        # If we added an extra sample for byte calculation, remove the last byte (don't write)\n        if n_samp % 2:\n            b_write = b_write[:-1]\n\n    elif fmt == '16':\n        # convert to 16 bit two's complement\n        d_signal[d_signal<0] = d_signal[d_signal<0] + 65536\n        # Split samples into separate bytes using binary masks\n        b1 = d_signal & [255]*n_sig\n        b2 = ( d_signal & [65280]*n_sig ) >> 8\n        # Interweave the bytes so that the same samples' bytes are consecutive\n        b1 = b1.reshape((-1, 1))\n        b2 = b2.reshape((-1, 1))\n        b_write = np.concatenate((b1, b2), axis=1)\n        b_write = b_write.reshape((1,-1))[0]\n        # Convert to un_signed 8 bit dtype to write\n        b_write = b_write.astype('uint8')\n    elif fmt == '24':\n        # convert to 24 bit two's complement\n        d_signal[d_signal<0] = d_signal[d_signal<0] + 16777216\n        # Split samples into separate bytes using binary masks\n        b1 = d_signal & [255]*n_sig\n        b2 = ( d_signal & [65280]*n_sig ) >> 8\n        b3 = ( d_signal & [16711680]*n_sig ) >> 16\n        # Interweave the bytes so that the same samples' bytes are consecutive\n        b1 = b1.reshape((-1, 1))\n        b2 = b2.reshape((-1, 1))\n        b3 = b3.reshape((-1, 1))\n        b_write = np.concatenate((b1, b2, b3), axis=1)\n        b_write = b_write.reshape((1,-1))[0]\n        # Convert to un_signed 8 bit dtype to write\n        b_write = b_write.astype('uint8')\n\n    elif fmt == '32':\n        # convert to 32 bit two's complement\n        d_signal[d_signal<0] = d_signal[d_signal<0] + 4294967296\n        # Split samples into separate bytes using binary masks\n        b1 = d_signal & [255]*n_sig\n        b2 = ( d_signal & [65280]*n_sig ) >> 8\n        b3 = ( d_signal & [16711680]*n_sig ) >> 16\n        b4 = ( d_signal & [4278190080]*n_sig ) >> 24\n        # Interweave the bytes so that the same samples' bytes are consecutive\n        b1 = b1.reshape((-1, 1))\n        b2 = b2.reshape((-1, 1))\n        b3 = b3.reshape((-1, 1))\n        b4 = b4.reshape((-1, 1))\n        b_write = np.concatenate((b1, b2, b3, b4), axis=1)\n        b_write = b_write.reshape((1,-1))[0]\n        # Convert to un_signed 8 bit dtype to write\n        b_write = b_write.astype('uint8')\n    else:\n        raise ValueError('This library currently only supports writing the following formats: 80, 16, 24, 32')\n\n    # Byte offset in the file\n    if byte_offset is not None and byte_offset>0:\n        print('Writing file '+file_name+' with '+str(byte_offset)+' empty leading bytes')\n        b_write = np.append(np.zeros(byte_offset, dtype = 'uint8'), b_write)\n\n    # Write the bytes to the file\n    with open(os.path.join(write_dir, file_name),'wb') as f:\n        b_write.tofile(f)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef describe_list_indices(full_list):\n    unique_elements = []\n    element_indices = {}\n\n    for i in range(len(full_list)):\n        item = full_list[i]\n        # new item\n        if item not in unique_elements:\n            unique_elements.append(item)\n            element_indices[item] = [i]\n        # previously seen item\n        else:\n            element_indices[item].append(i)\n    return unique_elements, element_indices", "response": "Takes a list of items and returns a list of unique elements and a list of element indices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _infer_sig_len(file_name, fmt, n_sig, dir_name, pb_dir=None):\n    if pb_dir is None:\n        file_size = os.path.getsize(os.path.join(dir_name, file_name))\n    else:\n        file_size = download._remote_file_size(file_name=file_name,\n                                               pb_dir=pb_dir)\n\n    sig_len = int(file_size / (BYTES_PER_SAMPLE[fmt] * n_sig))\n\n    return sig_len", "response": "Infer the length of a signal from a dat file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the cohesion of the d_signal and e_d_signal fields with the other_signal fields used to write the record .", "response": "def check_sig_cohesion(self, write_fields, expanded):\n        \"\"\"\n        Check the cohesion of the d_signal/e_d_signal field with the other\n        fields used to write the record\n        \"\"\"\n        # Using list of arrays e_d_signal\n        if expanded:\n            # Set default samps_per_frame\n            spf = self.samps_per_frame\n            for ch in range(len(spf)):\n                if spf[ch] is None:\n                    spf[ch] = 1\n\n            # Match the actual signal shape against stated length and number of channels\n            if self.n_sig != len(self.e_d_signal):\n                raise ValueError('n_sig does not match the length of e_d_signal')\n            for ch in range(self.n_sig):\n                if len(self.e_d_signal[ch]) != spf[ch]*self.sig_len:\n                    raise ValueError('Length of channel '+str(ch)+'does not match samps_per_frame['+str(ch+']*sig_len'))\n\n            # For each channel (if any), make sure the digital format has no values out of bounds\n            for ch in range(self.n_sig):\n                fmt = self.fmt[ch]\n                dmin, dmax = _digi_bounds(self.fmt[ch])\n\n                chmin = min(self.e_d_signal[ch])\n                chmax = max(self.e_d_signal[ch])\n                if (chmin < dmin) or (chmax > dmax):\n                    raise IndexError(\"Channel \"+str(ch)+\" contain values outside allowed range [\"+str(dmin)+\", \"+str(dmax)+\"] for fmt \"+str(fmt))\n\n            # Ensure that the checksums and initial value fields match the digital signal (if the fields are present)\n            if self.n_sig > 0:\n                if 'checksum' in write_fields:\n                    realchecksum = self.calc_checksum(expanded)\n                    if self.checksum != realchecksum:\n                        print(\"The actual checksum of e_d_signal is: \", realchecksum)\n                        raise ValueError(\"checksum field does not match actual checksum of e_d_signal\")\n                if 'init_value' in write_fields:\n                    realinit_value = [self.e_d_signal[ch][0] for ch in range(self.n_sig)]\n                    if self.init_value != realinit_value:\n                        print(\"The actual init_value of e_d_signal is: \", realinit_value)\n                        raise ValueError(\"init_value field does not match actual init_value of e_d_signal\")\n\n        # Using uniform d_signal\n        else:\n            # Match the actual signal shape against stated length and number of channels\n            if (self.sig_len, self.n_sig) != self.d_signal.shape:\n                print('sig_len: ', self.sig_len)\n                print('n_sig: ', self.n_sig)\n                print('d_signal.shape: ', self.d_signal.shape)\n                raise ValueError('sig_len and n_sig do not match shape of d_signal')\n\n            # For each channel (if any), make sure the digital format has no values out of bounds\n            for ch in range(self.n_sig):\n                fmt = self.fmt[ch]\n                dmin, dmax = _digi_bounds(self.fmt[ch])\n\n                chmin = min(self.d_signal[:,ch])\n                chmax = max(self.d_signal[:,ch])\n                if (chmin < dmin) or (chmax > dmax):\n                    raise IndexError(\"Channel \"+str(ch)+\" contain values outside allowed range [\"+str(dmin)+\", \"+str(dmax)+\"] for fmt \"+str(fmt))\n\n            # Ensure that the checksums and initial value fields match the digital signal (if the fields are present)\n            if self.n_sig>0:\n                if 'checksum' in write_fields:\n                    realchecksum = self.calc_checksum()\n                    if self.checksum != realchecksum:\n                        print(\"The actual checksum of d_signal is: \", realchecksum)\n                        raise ValueError(\"checksum field does not match actual checksum of d_signal\")\n                if 'init_value' in write_fields:\n                    realinit_value = list(self.d_signal[0,:])\n                    if self.init_value != realinit_value:\n                        print(\"The actual init_value of d_signal is: \", realinit_value)\n                        raise ValueError(\"init_value field does not match actual init_value of d_signal\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the physical signal field features of the current instance of ADC and DAC.", "response": "def set_p_features(self, do_dac=False, expanded=False):\n        \"\"\"\n        Use properties of the physical signal field to set the following\n        features: n_sig, sig_len.\n\n        Parameters\n        ----------\n        do_dac : bool\n            Whether to use the digital signal field to perform dac\n            conversion to get the physical signal field beforehand.\n        expanded : bool\n            Whether to use the `e_p_signal` or `p_signal` field. If\n            True, the `samps_per_frame` attribute is also required.\n\n        Notes\n        -----\n        Regarding dac conversion:\n          - fmt, gain, and baseline must all be set in order to perform\n            dac.\n          - Unlike with adc, there is no way to infer these fields.\n          - Using the fmt, gain and baseline fields, dac is performed,\n            and (e_)p_signal is set.\n\n        *Developer note: Seems this function will be very infrequently used.\n         The set_d_features function seems far more useful.\n\n        \"\"\"\n\n        if expanded:\n            if do_dac:\n                self.check_field('e_d_signal')\n                self.check_field('fmt', 'all')\n                self.check_field('adc_gain', 'all')\n                self.check_field('baseline', 'all')\n                self.check_field('samps_per_frame', 'all')\n\n                # All required fields are present and valid. Perform DAC\n                self.e_p_signal = self.dac(expanded)\n\n            # Use e_p_signal to set fields\n            self.check_field('e_p_signal', channels = 'all')\n            self.sig_len = int(len(self.e_p_signal[0])/self.samps_per_frame[0])\n            self.n_sig = len(self.e_p_signal)\n        else:\n            if do_dac:\n                self.check_field('d_signal')\n                self.check_field('fmt', 'all')\n                self.check_field('adc_gain', 'all')\n                self.check_field('baseline', 'all')\n\n                # All required fields are present and valid. Perform DAC\n                self.p_signal = self.dac()\n\n            # Use p_signal to set fields\n            self.check_field('p_signal')\n            self.sig_len = self.p_signal.shape[0]\n            self.n_sig = self.p_signal.shape[1]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the digital features of the file.", "response": "def set_d_features(self, do_adc=False, single_fmt=True, expanded=False):\n        \"\"\"\n        Use properties of the digital signal field to set the following\n        features: n_sig, sig_len, init_value, checksum, and possibly\n        *(fmt, adc_gain, baseline).\n\n        Parameters\n        ----------\n        do_adc : bools\n            Whether to use the physical signal field to perform adc\n            conversion to get the digital signal field beforehand.\n        single_fmt : bool\n            Whether to use a single digital format during adc, if it is\n            performed.\n        expanded : bool\n            Whether to use the `e_d_signal` or `d_signal` field.\n\n        Notes\n        -----\n        Regarding adc conversion:\n        - If fmt is unset:\n          - Neither adc_gain nor baseline may be set. If the digital values\n            used to store the signal are known, then the file format should\n            also be known.\n          - The most appropriate fmt for the signals will be calculated and the\n            `fmt` attribute will be set. Given that neither `adc_gain` nor\n            `baseline` is allowed to be set, optimal values for those fields will\n            then be calculated and set as well.\n        - If fmt is set:\n          - If both adc_gain and baseline are unset, optimal values for those\n            fields will be calculated the fields will be set.\n          - If both adc_gain and baseline are set, the function will continue.\n          - If only one of adc_gain and baseline are set, this function will\n            raise an error. It makes no sense to know only one of those fields.\n        - ADC will occur after valid values for fmt, adc_gain, and baseline are\n          present, using all three fields.\n\n        \"\"\"\n        if expanded:\n            # adc is performed.\n            if do_adc:\n                self.check_field('e_p_signal', channels='all')\n\n                # If there is no fmt set it, adc_gain, and baseline\n                if self.fmt is None:\n                    # Make sure that neither adc_gain nor baseline are set\n                    if self.adc_gain is not None or self.baseline is not None:\n                        raise Exception('If fmt is not set, gain and baseline may not be set either.')\n                    # Choose appropriate fmts based on estimated signal resolutions.\n                    res = est_res(self.e_p_signal)\n                    self.fmt = _wfdb_fmt(res, single_fmt)\n                # If there is a fmt set\n                else:\n                    self.check_field('fmt', 'all')\n                    # Neither field set\n                    if self.adc_gain is None and self.baseline is None:\n                        # Calculate and set optimal gain and baseline values to convert physical signals\n                        self.adc_gain, self.baseline = self.calc_adc_params()\n                    # Exactly one field set\n                    elif (self.adc_gain is None) ^ (self.baseline is None):\n                        raise Exception('If fmt is set, gain and baseline should both be set or not set.')\n\n                self.check_field('adc_gain', 'all')\n                self.check_field('baseline', 'all')\n\n                # All required fields are present and valid. Perform ADC\n                self.d_signal = self.adc(expanded)\n\n            # Use e_d_signal to set fields\n            self.check_field('e_d_signal', channels='all')\n            self.sig_len = int(len(self.e_d_signal[0])/self.samps_per_frame[0])\n            self.n_sig = len(self.e_d_signal)\n            self.init_value = [sig[0] for sig in self.e_d_signal]\n            self.checksum = self.calc_checksum(expanded)\n        else:\n            # adc is performed.\n            if do_adc:\n                self.check_field('p_signal')\n\n                # If there is no fmt set\n                if self.fmt is None:\n                    # Make sure that neither adc_gain nor baseline are set\n                    if self.adc_gain is not None or self.baseline is not None:\n                        raise Exception('If fmt is not set, gain and baseline may not be set either.')\n                    # Choose appropriate fmts based on estimated signal resolutions.\n                    res = est_res(self.p_signal)\n                    self.fmt = _wfdb_fmt(res, single_fmt)\n                    # Calculate and set optimal gain and baseline values to convert physical signals\n                    self.adc_gain, self.baseline = self.calc_adc_params()\n\n                # If there is a fmt set\n                else:\n                    self.check_field('fmt', 'all')\n                    # Neither field set\n                    if self.adc_gain is None and self.baseline is None:\n                        # Calculate and set optimal gain and baseline values to convert physical signals\n                        self.adc_gain, self.baseline = self.calc_adc_params()\n                    # Exactly one field set\n                    elif (self.adc_gain is None) ^ (self.baseline is None):\n                        raise Exception('If fmt is set, gain and baseline should both be set or not set.')\n\n                self.check_field('adc_gain', 'all')\n                self.check_field('baseline', 'all')\n\n                # All required fields are present and valid. Perform ADC\n                self.d_signal = self.adc()\n\n            # Use d_signal to set fields\n            self.check_field('d_signal')\n            self.sig_len = self.d_signal.shape[0]\n            self.n_sig = self.d_signal.shape[1]\n            self.init_value = list(self.d_signal[0,:])\n            self.checksum = self.calc_checksum()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adc(self, expanded=False, inplace=False):\n\n        # The digital nan values for each channel\n        d_nans = _digi_nan(self.fmt)\n\n        # To do: choose the minimum return res needed\n        intdtype = 'int64'\n\n        # Do inplace conversion and set relevant variables.\n        if inplace:\n            if expanded:\n                for ch in range(self.n_sig):\n                    # nan locations for the channel\n                    ch_nanlocs = np.isnan(self.e_p_signal[ch])\n                    np.multiply(self.e_p_signal[ch], self.adc_gain[ch],\n                                self.e_p_signal[ch])\n                    np.add(e_p_signal[ch], self.baseline[ch],\n                           self.e_p_signal[ch])\n                    self.e_p_signal[ch] = self.e_p_signal[ch].astype(intdtype,\n                                                                     copy=False)\n                    self.e_p_signal[ch][ch_nanlocs] = d_nans[ch]\n                self.e_d_signal = self.e_p_signal\n                self.e_p_signal = None\n            else:\n                nanlocs = np.isnan(self.p_signal)\n                np.multiply(self.p_signal, self.adc_gain, self.p_signal)\n                np.add(self.p_signal, self.baseline, self.p_signal)\n                self.p_signal = self.p_signal.astype(intdtype, copy=False)\n                self.d_signal = self.p_signal\n                self.p_signal = None\n\n        # Return the variable\n        else:\n            if expanded:\n                d_signal = []\n                for ch in range(self.n_sig):\n                    # nan locations for the channel\n                    ch_nanlocs = np.isnan(self.e_p_signal[ch])\n                    ch_d_signal = self.e_p_signal.copy()\n                    np.multiply(ch_d_signal, self.adc_gain[ch], ch_d_signal)\n                    np.add(ch_d_signal, self.baseline[ch], ch_d_signal)\n                    ch_d_signal = ch_d_signal.astype(intdtype, copy=False)\n                    ch_d_signal[ch_nanlocs] = d_nans[ch]\n                    d_signal.append(ch_d_signal)\n\n            else:\n                nanlocs = np.isnan(self.p_signal)\n                # Cannot cast dtype to int now because gain is float.\n                d_signal = self.p_signal.copy()\n                np.multiply(d_signal, self.adc_gain, d_signal)\n                np.add(d_signal, self.baseline, d_signal)\n                d_signal = d_signal.astype(intdtype, copy=False)\n\n                if nanlocs.any():\n                    for ch in range(d_signal.shape[1]):\n                        if nanlocs[:,ch].any():\n                            d_signal[nanlocs[:,ch],ch] = d_nans[ch]\n\n            return d_signal", "response": "Returns the physical signal as a 2d numpy array and the digital nan values for each channel stored in the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dac(self, expanded=False, return_res=64, inplace=False):\n\n        # The digital nan values for each channel\n        d_nans = _digi_nan(self.fmt)\n\n        # Get the appropriate float dtype\n        if return_res == 64:\n            floatdtype = 'float64'\n        elif return_res == 32:\n            floatdtype = 'float32'\n        else:\n            floatdtype = 'float16'\n\n        # Do inplace conversion and set relevant variables.\n        if inplace:\n            if expanded:\n                for ch in range(self.n_sig):\n                    # nan locations for the channel\n                    ch_nanlocs = self.e_d_signal[ch] == d_nans[ch]\n                    self.e_d_signal[ch] = self.e_d_signal[ch].astype(floatdtype, copy=False)\n                    np.subtract(self.e_d_signal[ch], self.baseline[ch], self.e_d_signal[ch])\n                    np.divide(self.e_d_signal[ch], self.adc_gain[ch], self.e_d_signal[ch])\n                    self.e_d_signal[ch][ch_nanlocs] = np.nan\n                self.e_p_signal = self.e_d_signal\n                self.e_d_signal = None\n            else:\n                nanlocs = self.d_signal == d_nans\n                # Do float conversion immediately to avoid potential under/overflow\n                # of efficient int dtype\n                self.d_signal = self.d_signal.astype(floatdtype, copy=False)\n                np.subtract(self.d_signal, self.baseline, self.d_signal)\n                np.divide(self.d_signal, self.adc_gain, self.d_signal)\n                self.d_signal[nanlocs] = np.nan\n                self.p_signal = self.d_signal\n                self.d_signal = None\n\n        # Return the variable\n        else:\n            if expanded:\n                p_signal = []\n                for ch in range(self.n_sig):\n                    # nan locations for the channel\n                    ch_nanlocs = self.e_d_signal[ch] == d_nans[ch]\n                    ch_p_signal = self.e_d_signal[ch].astype(floatdtype, copy=False)\n                    np.subtract(ch_p_signal, self.baseline[ch], ch_p_signal)\n                    np.divide(ch_p_signal, self.adc_gain[ch], ch_p_signal)\n                    ch_p_signal[ch_nanlocs] = np.nan\n                    p_signal.append(ch_p_signal)\n            else:\n                nanlocs = self.d_signal == d_nans\n                p_signal = self.d_signal.astype(floatdtype, copy=False)\n                np.subtract(p_signal, self.baseline, p_signal)\n                np.divide(p_signal, self.adc_gain, p_signal)\n                p_signal[nanlocs] = np.nan\n\n            return p_signal", "response": "Returns the digital analogue conversion of the signal stored in the object s d_signal and e_d_signal fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates appropriate adc_gain and baseline parameters for the given physical signal and the format values.", "response": "def calc_adc_params(self):\n        \"\"\"\n        Compute appropriate adc_gain and baseline parameters for adc\n        conversion, given the physical signal and the fmts.\n\n        Returns\n        -------\n        adc_gains : list\n            List of calculated `adc_gain` values for each channel.\n        baselines : list\n            List of calculated `baseline` values for each channel.\n\n        Notes\n        -----\n        This is the mapping equation:\n            `digital - baseline / adc_gain = physical`\n            `physical * adc_gain + baseline = digital`\n\n        The original WFDB library stores `baseline` as int32.\n        Constrain abs(adc_gain) <= 2**31 == 2147483648\n\n        This function does carefully deal with overflow for calculated\n        int32 `baseline` values, but does not consider over/underflow\n        for calculated float `adc_gain` values.\n\n        \"\"\"\n        adc_gains = []\n        baselines = []\n\n        if np.where(np.isinf(self.p_signal))[0].size:\n            raise ValueError('Signal contains inf. Cannot perform adc.')\n\n        # min and max ignoring nans, unless whole channel is nan.\n        # Should suppress warning message.\n        minvals = np.nanmin(self.p_signal, axis=0)\n        maxvals = np.nanmax(self.p_signal, axis=0)\n\n        for ch in range(np.shape(self.p_signal)[1]):\n            # Get the minimum and maximum (valid) storage values\n            dmin, dmax = _digi_bounds(self.fmt[ch])\n            # add 1 because the lowest value is used to store nans\n            dmin = dmin + 1\n\n            pmin = minvals[ch]\n            pmax = maxvals[ch]\n\n            # Figure out digital samples used to store physical samples\n\n            # If the entire signal is nan, gain/baseline won't be used\n            if pmin == np.nan:\n                adc_gain = 1\n                baseline = 1\n            # If the signal is just one value, store one digital value.\n            elif pmin == pmax:\n                if pmin == 0:\n                    adc_gain = 1\n                    baseline = 1\n                else:\n                    # All digital values are +1 or -1. Keep adc_gain > 0\n                    adc_gain = abs(1 / pmin)\n                    baseline = 0\n            # Regular varied signal case.\n            else:\n                # The equation is: p = (d - b) / g\n\n                # Approximately, pmax maps to dmax, and pmin maps to\n                # dmin. Gradient will be equal to, or close to\n                # delta(d) / delta(p), since intercept baseline has\n                # to be an integer.\n\n                # Constraint: baseline must be between +/- 2**31\n                adc_gain = (dmax-dmin) / (pmax-pmin)\n                baseline = dmin - adc_gain*pmin\n\n                # Make adjustments for baseline to be an integer\n                # This up/down round logic of baseline is to ensure\n                # there is no overshoot of dmax. Now pmax will map\n                # to dmax or dmax-1 which is also fine.\n                if pmin > 0:\n                    baseline = int(np.ceil(baseline))\n                else:\n                    baseline = int(np.floor(baseline))\n\n                # After baseline is set, adjust gain correspondingly.Set\n                # the gain to map pmin to dmin, and p==0 to baseline.\n                # In the case where pmin == 0 and dmin == baseline,\n                # adc_gain is already correct. Avoid dividing by 0.\n                if dmin != baseline:\n                    adc_gain = (dmin - baseline) / pmin\n\n            # Remap signal if baseline exceeds boundaries.\n            # This may happen if pmax < 0\n            if baseline > MAX_I32:\n                # pmin maps to dmin, baseline maps to 2**31 - 1\n                # pmax will map to a lower value than before\n                adc_gain = (MAX_I32) - dmin / abs(pmin)\n                baseline = MAX_I32\n            # This may happen if pmin > 0\n            elif baseline < MIN_I32:\n                # pmax maps to dmax, baseline maps to -2**31 + 1\n                adc_gain = (dmax - MIN_I32) / pmax\n                baseline = MIN_I32\n\n            adc_gains.append(adc_gain)\n            baselines.append(baseline)\n\n        return (adc_gains, baselines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_checksum(self, expanded=False):\n        if expanded:\n            cs = [int(np.sum(self.e_d_signal[ch]) % 65536) for ch in range(self.n_sig)]\n        else:\n            cs = np.sum(self.d_signal, 0) % 65536\n            cs = [int(c) for c in cs]\n        return cs", "response": "Calculate the checksum of the d_signal or e_d_signal field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite each of the specified dat files to the specified directory.", "response": "def wr_dat_files(self, expanded=False, write_dir=''):\n        \"\"\"\n        Write each of the specified dat files\n\n        \"\"\"\n        # Get the set of dat files to be written, and\n        # the channels to be written to each file.\n        file_names, dat_channels = describe_list_indices(self.file_name)\n\n        # Get the fmt and byte offset corresponding to each dat file\n        DAT_FMTS = {}\n        dat_offsets = {}\n        for fn in file_names:\n            DAT_FMTS[fn] = self.fmt[dat_channels[fn][0]]\n\n            # byte_offset may not be present\n            if self.byte_offset is None:\n                dat_offsets[fn] = 0\n            else:\n                dat_offsets[fn] = self.byte_offset[dat_channels[fn][0]]\n\n        # Write the dat files\n        if expanded:\n            for fn in file_names:\n                wr_dat_file(fn, DAT_FMTS[fn], None , dat_offsets[fn], True,\n                            [self.e_d_signal[ch] for ch in dat_channels[fn]],\n                            self.samps_per_frame, write_dir=write_dir)\n        else:\n            # Create a copy to prevent overwrite\n            dsig = self.d_signal.copy()\n            for fn in file_names:\n                wr_dat_file(fn, DAT_FMTS[fn],\n                            dsig[:, dat_channels[fn][0]:dat_channels[fn][-1]+1],\n                            dat_offsets[fn], write_dir=write_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts expanded signals with different samples/frame into a uniform numpy array. Input parameters - sigtype (default='physical'): Specifies whether to mooth the e_p_signal field ('physical'), or the e_d_signal field ('digital').", "response": "def smooth_frames(self, sigtype='physical'):\n        \"\"\"\n        Convert expanded signals with different samples/frame into\n        a uniform numpy array.\n\n        Input parameters\n        - sigtype (default='physical'): Specifies whether to mooth\n          the e_p_signal field ('physical'), or the e_d_signal\n          field ('digital').\n        \"\"\"\n        spf = self.samps_per_frame[:]\n        for ch in range(len(spf)):\n            if spf[ch] is None:\n                spf[ch] = 1\n\n        # Total samples per frame\n        tspf = sum(spf)\n\n        if sigtype == 'physical':\n            n_sig = len(self.e_p_signal)\n            sig_len = int(len(self.e_p_signal[0])/spf[0])\n            signal = np.zeros((sig_len, n_sig), dtype='float64')\n\n            for ch in range(n_sig):\n                if spf[ch] == 1:\n                    signal[:, ch] = self.e_p_signal[ch]\n                else:\n                    for frame in range(spf[ch]):\n                        signal[:, ch] += self.e_p_signal[ch][frame::spf[ch]]\n                    signal[:, ch] = signal[:, ch] / spf[ch]\n\n        elif sigtype == 'digital':\n            n_sig = len(self.e_d_signal)\n            sig_len = int(len(self.e_d_signal[0])/spf[0])\n            signal = np.zeros((sig_len, n_sig), dtype='int64')\n\n            for ch in range(n_sig):\n                if spf[ch] == 1:\n                    signal[:, ch] = self.e_d_signal[ch]\n                else:\n                    for frame in range(spf[ch]):\n                        signal[:, ch] += self.e_d_signal[ch][frame::spf[ch]]\n                    signal[:, ch] = signal[:, ch] / spf[ch]\n        else:\n            raise ValueError(\"sigtype must be 'physical' or 'digital'\")\n\n        return signal"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wfdb_strptime(time_string):\n    n_colons = time_string.count(':')\n\n    if n_colons == 0:\n        time_fmt = '%S'\n    elif n_colons == 1:\n        time_fmt = '%M:%S'\n    elif n_colons == 2:\n        time_fmt = '%H:%M:%S'\n\n    if '.' in time_string:\n        time_fmt += '.%f'\n\n    return datetime.datetime.strptime(time_string, time_fmt).time()", "response": "Given a time string in an acceptable wfdb format return a datetime. time object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the lines in a local or remote header file.", "response": "def _read_header_lines(base_record_name, dir_name, pb_dir):\n    \"\"\"\n    Read the lines in a local or remote header file.\n\n    Parameters\n    ----------\n    base_record_name : str\n        The base name of the WFDB record to be read, without any file\n        extensions.\n    dir_name : str\n        The local directory location of the header file. This parameter\n        is ignored if `pb_dir` is set.\n    pb_dir : str\n        Option used to stream data from Physiobank. The Physiobank\n        database directory from which to find the required record files.\n        eg. For record '100' in 'http://physionet.org/physiobank/database/mitdb'\n        pb_dir='mitdb'.\n\n    Returns\n    -------\n    header_lines : list\n        List of strings corresponding to the header lines.\n    comment_lines : list\n        List of strings corresponding to the comment lines.\n\n    \"\"\"\n    file_name = base_record_name + '.hea'\n\n    # Read local file\n    if pb_dir is None:\n        with open(os.path.join(dir_name, file_name), 'r') as fp:\n            # Record line followed by signal/segment lines if any\n            header_lines = []\n            # Comment lines\n            comment_lines = []\n            for line in fp:\n                line = line.strip()\n                # Comment line\n                if line.startswith('#'):\n                    comment_lines.append(line)\n                # Non-empty non-comment line = header line.\n                elif line:\n                    # Look for a comment in the line\n                    ci = line.find('#')\n                    if ci > 0:\n                        header_lines.append(line[:ci])\n                        # comment on same line as header line\n                        comment_lines.append(line[ci:])\n                    else:\n                        header_lines.append(line)\n    # Read online header file\n    else:\n        header_lines, comment_lines = download._stream_header(file_name,\n                                                              pb_dir)\n\n    return header_lines, comment_lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a record line into a dictionary of fields.", "response": "def _parse_record_line(record_line):\n    \"\"\"\n    Extract fields from a record line string into a dictionary\n\n    \"\"\"\n    # Dictionary for record fields\n    record_fields = {}\n\n    # Read string fields from record line\n    (record_fields['record_name'], record_fields['n_seg'],\n     record_fields['n_sig'], record_fields['fs'],\n     record_fields['counter_freq'], record_fields['base_counter'],\n     record_fields['sig_len'], record_fields['base_time'],\n     record_fields['base_date']) = re.findall(_rx_record, record_line)[0]\n\n    for field in RECORD_SPECS.index:\n        # Replace empty strings with their read defaults (which are\n        # mostly None)\n        if record_fields[field] == '':\n            record_fields[field] = RECORD_SPECS.loc[field, 'read_default']\n        # Typecast non-empty strings for non-string (numerical/datetime)\n        # fields\n        else:\n            if RECORD_SPECS.loc[field, 'allowed_types'] == int_types:\n                record_fields[field] = int(record_fields[field])\n            elif RECORD_SPECS.loc[field, 'allowed_types'] == float_types:\n                record_fields[field] = float(record_fields[field])\n                # cast fs to an int if it is close\n                if field == 'fs':\n                    fs = float(record_fields['fs'])\n                    if round(fs, 8) == float(int(fs)):\n                        fs = int(fs)\n                    record_fields['fs'] = fs\n            elif field == 'base_time':\n                record_fields['base_time'] = wfdb_strptime(record_fields['base_time'])\n            elif field == 'base_date':\n                record_fields['base_date'] = datetime.datetime.strptime(\n                    record_fields['base_date'], '%d/%m/%Y').date()\n\n    # This is not a standard wfdb field, but is useful to set.\n    if record_fields['base_date'] and record_fields['base_time']:\n        record_fields['base_datetime'] = datetime.datetime.combine(\n            record_fields['base_date'], record_fields['base_time'])\n\n    return record_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_signal_lines(signal_lines):\n    n_sig = len(signal_lines)\n    # Dictionary for signal fields\n    signal_fields = {}\n\n    # Each dictionary field is a list\n    for field in SIGNAL_SPECS.index:\n        signal_fields[field] = n_sig * [None]\n\n    # Read string fields from signal line\n    for ch in range(n_sig):\n        (signal_fields['file_name'][ch], signal_fields['fmt'][ch],\n         signal_fields['samps_per_frame'][ch], signal_fields['skew'][ch],\n         signal_fields['byte_offset'][ch], signal_fields['adc_gain'][ch],\n         signal_fields['baseline'][ch], signal_fields['units'][ch],\n         signal_fields['adc_res'][ch], signal_fields['adc_zero'][ch],\n         signal_fields['init_value'][ch], signal_fields['checksum'][ch],\n         signal_fields['block_size'][ch],\n         signal_fields['sig_name'][ch]) = _rx_signal.findall(signal_lines[ch])[0]\n\n        for field in SIGNAL_SPECS.index:\n            # Replace empty strings with their read defaults (which are mostly None)\n            # Note: Never set a field to None. [None]* n_sig is accurate, indicating\n            # that different channels can be present or missing.\n            if signal_fields[field][ch] == '':\n                signal_fields[field][ch] = SIGNAL_SPECS.loc[field, 'read_default']\n\n                # Special case: missing baseline defaults to ADCzero if present\n                if field == 'baseline' and signal_fields['adc_zero'][ch] != '':\n                    signal_fields['baseline'][ch] = int(signal_fields['adc_zero'][ch])\n            # Typecast non-empty strings for numerical fields\n            else:\n                if SIGNAL_SPECS.loc[field, 'allowed_types'] is int_types:\n                    signal_fields[field][ch] = int(signal_fields[field][ch])\n                elif SIGNAL_SPECS.loc[field, 'allowed_types'] is float_types:\n                    signal_fields[field][ch] = float(signal_fields[field][ch])\n                    # Special case: adc_gain of 0 means 200\n                    if field == 'adc_gain' and signal_fields['adc_gain'][ch] == 0:\n                        signal_fields['adc_gain'][ch] = 200.\n\n    return signal_fields", "response": "Parses the signal lines into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract fields from segment line strings into a dictionary", "response": "def _read_segment_lines(segment_lines):\n    \"\"\"\n    Extract fields from segment line strings into a dictionary\n\n    \"\"\"\n    # Dictionary for segment fields\n    segment_fields = {}\n\n    # Each dictionary field is a list\n    for field in SEGMENT_SPECS.index:\n        segment_fields[field] = [None] * len(segment_lines)\n\n    # Read string fields from signal line\n    for i in range(len(segment_lines)):\n        (segment_fields['seg_name'][i], segment_fields['seg_len'][i]) = _rx_segment.findall(segment_lines[i])[0]\n\n        # Typecast strings for numerical field\n        if field == 'seg_len':\n            segment_fields['seg_len'][i] = int(segment_fields['seg_len'][i])\n\n    return segment_fields"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_write_subset(self, spec_type):\n        if spec_type == 'record':\n            write_fields = []\n            record_specs = RECORD_SPECS.copy()\n\n            # Remove the n_seg requirement for single segment items\n            if not hasattr(self, 'n_seg'):\n                record_specs.drop('n_seg', inplace=True)\n\n            for field in record_specs.index[-1::-1]:\n                # Continue if the field has already been included\n                if field in write_fields:\n                    continue\n                # If the field is required by default or has been\n                # defined by the user\n                if (record_specs.loc[field, 'write_required']\n                        or getattr(self, field) is not None):\n                    req_field = field\n                    # Add the field and its recursive dependencies\n                    while req_field is not None:\n                        write_fields.append(req_field)\n                        req_field = record_specs.loc[req_field, 'dependency']\n            # Add comments if any\n            if getattr(self, 'comments') is not None:\n                write_fields.append('comments')\n\n        # signal spec field. Need to return a potentially different list for each channel.\n        elif spec_type == 'signal':\n            # List of lists for each channel\n            write_fields = []\n            signal_specs = SIGNAL_SPECS.copy()\n\n            for ch in range(self.n_sig):\n                # The fields needed for this channel\n                write_fields_ch = []\n                for field in signal_specs.index[-1::-1]:\n                    if field in write_fields_ch:\n                        continue\n\n                    item = getattr(self, field)\n                    # If the field is required by default or has been defined by the user\n                    if signal_specs.loc[field, 'write_required'] or (item is not None and item[ch] is not None):\n                        req_field = field\n                        # Add the field and its recursive dependencies\n                        while req_field is not None:\n                            write_fields_ch.append(req_field)\n                            req_field = signal_specs.loc[req_field, 'dependency']\n\n                write_fields.append(write_fields_ch)\n\n            # Convert the list of lists to a single dictionary.\n            # keys = field and value = list of channels in which the\n            # field is required.\n            dict_write_fields = {}\n\n            # For fields present in any channel:\n            for field in set([i for write_fields_ch in write_fields for i in write_fields_ch]):\n                dict_write_fields[field] = []\n\n                for ch in range(self.n_sig):\n                    if field in write_fields[ch]:\n                        dict_write_fields[field].append(ch)\n\n            write_fields = dict_write_fields\n\n        return write_fields", "response": "Returns a set of fields used to write the header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the default values for the fields needed to write the header if they have no defaults.", "response": "def set_defaults(self):\n        \"\"\"\n        Set defaults for fields needed to write the header if they have\n        defaults.\n\n        Notes\n        -----\n        - This is NOT called by `rdheader`. It is only automatically\n          called by the gateway `wrsamp` for convenience.\n        - This is also not called by `wrheader` since it is supposed to\n          be an explicit function.\n        - This is not responsible for initializing the attributes. That\n          is done by the constructor.\n\n        See also `set_p_features` and `set_d_features`.\n\n        \"\"\"\n        rfields, sfields = self.get_write_fields()\n        for f in rfields:\n            self.set_default(f)\n        for f in sfields:\n            self.set_default(f)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a header file for the current instance of the class.", "response": "def wrheader(self, write_dir=''):\n        \"\"\"\n        Write a wfdb header file. The signals are not used. Before\n        writing:\n        - Get the fields used to write the header for this instance.\n        - Check each required field.\n        - Check that the fields are cohesive with one another.\n\n        Parameters\n        ----------\n        write_dir : str, optional\n            The output directory in which the header is written.\n\n        Notes\n        -----\n        This function does NOT call `set_defaults`. Essential fields\n        must be set beforehand.\n\n        \"\"\"\n\n        # Get all the fields used to write the header\n        # sig_write_fields is a dictionary of\n        # {field_name:required_channels}\n        rec_write_fields, sig_write_fields = self.get_write_fields()\n\n        # Check the validity of individual fields used to write the header\n\n        # Record specification fields (and comments)\n        for field in rec_write_fields:\n            self.check_field(field)\n\n        # Signal specification fields.\n        for field in sig_write_fields:\n            self.check_field(field, required_channels=sig_write_fields[field])\n\n        # Check the cohesion of fields used to write the header\n        self.check_field_cohesion(rec_write_fields, list(sig_write_fields))\n\n        # Write the header file using the specified fields\n        self.wr_header_file(rec_write_fields, sig_write_fields, write_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the list of fields used to write the header separating the record and signal specification fields. Returns the default record specification fields the user defined fields the user defined fields the dependencies the user defined fields and their dependencies.", "response": "def get_write_fields(self):\n        \"\"\"\n        Get the list of fields used to write the header, separating\n        record and signal specification fields. Returns the default\n        required fields, the user defined fields,\n        and their dependencies.\n\n        Does NOT include `d_signal` or `e_d_signal`.\n\n        Returns\n        -------\n        rec_write_fields : list\n            Record specification fields to be written. Includes\n            'comment' if present.\n        sig_write_fields : dict\n            Dictionary of signal specification fields to be written,\n            with values equal to the channels that need to be present\n            for each field.\n\n        \"\"\"\n\n        # Record specification fields\n        rec_write_fields = self.get_write_subset('record')\n\n        # Add comments if any\n        if self.comments != None:\n            rec_write_fields.append('comments')\n\n        # Get required signal fields if signals are present.\n        self.check_field('n_sig')\n\n        if self.n_sig >  0:\n            sig_write_fields = self.get_write_subset('signal')\n        else:\n            sig_write_fields = None\n\n        return rec_write_fields, sig_write_fields"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_default(self, field):\n\n        # Record specification fields\n        if field in RECORD_SPECS.index:\n            # Return if no default to set, or if the field is already\n            # present.\n            if RECORD_SPECS.loc[field, 'write_default'] is None or getattr(self, field) is not None:\n                return\n            setattr(self, field, RECORD_SPECS.loc[field, 'write_default'])\n\n        # Signal specification fields\n        # Setting entire list default, not filling in blanks in lists.\n        elif field in SIGNAL_SPECS.index:\n\n            # Specific dynamic case\n            if field == 'file_name' and self.file_name is None:\n                self.file_name = self.n_sig * [self.record_name + '.dat']\n                return\n\n            item = getattr(self, field)\n\n            # Return if no default to set, or if the field is already\n            # present.\n            if SIGNAL_SPECS.loc[field, 'write_default'] is None or item is not None:\n                return\n\n            # Set more specific defaults if possible\n            if field == 'adc_res' and self.fmt is not None:\n                self.adc_res = _signal._fmt_res(self.fmt)\n                return\n\n            setattr(self, field,\n                   [SIGNAL_SPECS.loc[field, 'write_default']] * self.n_sig)", "response": "Set the object s attribute to its default value if it is missing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_field_cohesion(self, rec_write_fields, sig_write_fields):\n        # If there are no signal specification fields, there is nothing to check.\n        if self.n_sig>0:\n\n            # The length of all signal specification fields must match n_sig\n            # even if some of its elements are None.\n            for f in sig_write_fields:\n                if len(getattr(self, f)) != self.n_sig:\n                    raise ValueError('The length of field: '+f+' must match field n_sig.')\n\n            # Each file_name must correspond to only one fmt, (and only one byte offset if defined).\n            datfmts = {}\n            for ch in range(self.n_sig):\n                if self.file_name[ch] not in datfmts:\n                    datfmts[self.file_name[ch]] = self.fmt[ch]\n                else:\n                    if datfmts[self.file_name[ch]] != self.fmt[ch]:\n                        raise ValueError('Each file_name (dat file) specified must have the same fmt')\n\n            datoffsets = {}\n            if self.byte_offset is not None:\n                # At least one byte offset value exists\n                for ch in range(self.n_sig):\n                    if self.byte_offset[ch] is None:\n                        continue\n                    if self.file_name[ch] not in datoffsets:\n                        datoffsets[self.file_name[ch]] = self.byte_offset[ch]\n                    else:\n                        if datoffsets[self.file_name[ch]] != self.byte_offset[ch]:\n                            raise ValueError('Each file_name (dat file) specified must have the same byte offset')", "response": "Check the cohesion of fields used to write the header."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wr_header_file(self, rec_write_fields, sig_write_fields, write_dir):\n        # Create record specification line\n        record_line = ''\n        # Traverse the ordered dictionary\n        for field in RECORD_SPECS.index:\n            # If the field is being used, add it with its delimiter\n            if field in rec_write_fields:\n                string_field = str(getattr(self, field))\n\n                # Certain fields need extra processing\n\n                if field == 'fs' and isinstance(self.fs, float):\n                    if round(self.fs, 8) == float(int(self.fs)):\n                        string_field = str(int(self.fs))\n                elif field == 'base_time' and '.' in string_field:\n                    string_field = string_field.rstrip('0')\n                elif field == 'base_date':\n                    string_field = '/'.join((string_field[8:],\n                                             string_field[5:7],\n                                             string_field[:4]))\n\n                record_line += RECORD_SPECS.loc[field, 'delimiter'] + string_field\n                # The 'base_counter' field needs to be closed with ')'\n                if field == 'base_counter':\n                    record_line += ')'\n\n        header_lines = [record_line]\n\n        # Create signal specification lines (if any) one channel at a time\n        if self.n_sig > 0:\n            signal_lines = self.n_sig * ['']\n            for ch in range(self.n_sig):\n                # Traverse the signal fields\n                for field in SIGNAL_SPECS.index:\n                    # If the field is being used, add each of its\n                    # elements with the delimiter to the appropriate\n                    # line\n                    if field in sig_write_fields and ch in sig_write_fields[field]:\n                        signal_lines[ch] += SIGNAL_SPECS.loc[field, 'delimiter'] + str(getattr(self, field)[ch])\n                    # The 'baseline' field needs to be closed with ')'\n                    if field == 'baseline':\n                        signal_lines[ch] += ')'\n\n            header_lines += signal_lines\n\n        # Create comment lines (if any)\n        if 'comments' in rec_write_fields:\n            comment_lines = ['# ' + comment for comment in self.comments]\n            header_lines += comment_lines\n\n        lines_to_file(self.record_name + '.hea', write_dir, header_lines)", "response": "Writes a header file using the specified fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of fields used to write the multi - segment header.", "response": "def get_write_fields(self):\n        \"\"\"\n        Get the list of fields used to write the multi-segment header.\n        Returns the default required fields, the user defined fields,\n        and their dependencies.\n\n        \"\"\"\n\n        # Record specification fields\n        write_fields = self.get_write_subset('record')\n\n        # Segment specification fields are all mandatory\n        write_fields = write_fields + ['seg_name', 'seg_len']\n\n        # Comments\n        if self.comments !=None:\n            write_fields.append('comments')\n        return write_fields"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wr_header_file(self, write_fields, write_dir):\n        # Create record specification line\n        record_line = ''\n        # Traverse the ordered dictionary\n        for field in RECORD_SPECS.index:\n            # If the field is being used, add it with its delimiter\n            if field in write_fields:\n                record_line += RECORD_SPECS.loc[field, 'delimiter'] + str(getattr(self, field))\n\n        header_lines = [record_line]\n\n        # Create segment specification lines\n        segment_lines = self.n_seg * ['']\n        # For both fields, add each of its elements with the delimiter\n        # to the appropriate line\n        for field in SEGMENT_SPECS.index:\n            for seg_num in range(self.n_seg):\n                segment_lines[seg_num] += SEGMENT_SPECS.loc[field, 'delimiter'] + str(getattr(self, field)[seg_num])\n\n        header_lines = header_lines + segment_lines\n\n        # Create comment lines (if any)\n        if 'comments' in write_fields:\n            comment_lines = ['# '+ comment for comment in self.comments]\n            header_lines += comment_lines\n\n        lines_to_file(self.record_name + '.hea', header_lines, write_dir)", "response": "Write a header file using the specified fields and write_dir."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of the segment numbers that contain a particular signal.", "response": "def get_sig_segments(self, sig_name=None):\n        \"\"\"\n        Get a list of the segment numbers that contain a particular signal\n        (or a dictionary of segment numbers for a list of signals)\n        Only works if information about the segments has been read in\n        \"\"\"\n        if self.segments is None:\n            raise Exception(\"The MultiRecord's segments must be read in before this method is called. ie. Call rdheader() with rsegment_fieldsments=True\")\n\n        # Default value = all signal names.\n        if sig_name is None:\n            sig_name = self.get_sig_name()\n\n        if isinstance(sig_name, list):\n            sigdict = {}\n            for sig in sig_name:\n                sigdict[sig] = self.get_sig_segments(sig)\n            return sigdict\n        elif isinstance(sig_name, str):\n            sigsegs = []\n            for i in range(self.n_seg):\n                if self.seg_name[i] != '~' and sig_name in self.segments[i].sig_name:\n                    sigsegs.append(i)\n            return sigsegs\n        else:\n            raise TypeError('sig_name must be a string or a list of strings')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resample_ann(resampled_t, ann_sample):\n    tmp = np.zeros(len(resampled_t), dtype='int16')\n    j = 0\n    tprec = resampled_t[j]\n    for i, v in enumerate(ann_sample):\n        while True:\n            d = False\n            if v < tprec:\n                j -= 1\n                tprec = resampled_t[j]\n\n            if j+1 == len(resampled_t):\n                tmp[j] += 1\n                break\n\n            tnow = resampled_t[j+1]\n            if tprec <= v and v <= tnow:\n                if v-tprec < tnow-v:\n                    tmp[j] += 1\n                else:\n                    tmp[j+1] += 1\n                d = True\n            j += 1\n            tprec = tnow\n            if d:\n                break\n\n    idx = np.where(tmp>0)[0].astype('int64')\n    res = []\n    for i in idx:\n        for j in range(tmp[i]):\n            res.append(i)\n    assert len(res) == len(ann_sample)\n\n    return np.asarray(res, dtype='int64')", "response": "Resample the annotation locations of a new language tree."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resample_singlechan(x, ann, fs, fs_target):\n\n    resampled_x, resampled_t = resample_sig(x, fs, fs_target)\n\n    new_sample = resample_ann(resampled_t, ann.sample)\n    assert ann.sample.shape == new_sample.shape\n\n    resampled_ann = Annotation(record_name=ann.record_name,\n                               extension=ann.extension,\n                               sample=new_sample,\n                               symbol=ann.symbol,\n                               subtype=ann.subtype,\n                               chan=ann.chan,\n                               num=ann.num,\n                               aux_note=ann.aux_note,\n                               fs=fs_target)\n\n    return resampled_x, resampled_ann", "response": "Resample a single - channel signal with its annotations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resample_multichan(xs, ann, fs, fs_target, resamp_ann_chan=0):\n    assert resamp_ann_chan < xs.shape[1]\n\n    lx = []\n    lt = None\n    for chan in range(xs.shape[1]):\n        resampled_x, resampled_t = resample_sig(xs[:, chan], fs, fs_target)\n        lx.append(resampled_x)\n        if chan == resamp_ann_chan:\n            lt = resampled_t\n\n    new_sample = resample_ann(lt, ann.sample)\n    assert ann.sample.shape == new_sample.shape\n\n    resampled_ann = Annotation(record_name=ann.record_name,\n                               extension=ann.extension,\n                               sample=new_sample,\n                               symbol=ann.symbol,\n                               subtype=ann.subtype,\n                               chan=ann.chan,\n                               num=ann.num,\n                               aux_note=ann.aux_note,\n                               fs=fs_target)\n\n    return np.column_stack(lx), resampled_ann", "response": "Resample multiple channels with their annotations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_bound(sig, lb=0, ub=1):\n\n    mid = ub - (ub - lb) / 2\n    min_v = np.min(sig)\n    max_v = np.max(sig)\n    mid_v =  max_v - (max_v - min_v) / 2\n    coef = (ub - lb) / (max_v - min_v)\n    return sig * coef - (mid_v * coef) + mid", "response": "Normalizes a signal between the lower and upper bound"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply a uniform moving average filter to a signal.", "response": "def smooth(sig, window_size):\n    \"\"\"\n    Apply a uniform moving average filter to a signal\n\n    Parameters\n    ----------\n    sig : numpy array\n        The signal to smooth.\n    window_size : int\n        The width of the moving average filter.\n\n    \"\"\"\n    box = np.ones(window_size)/window_size\n    return np.convolve(sig, box, mode='same')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives filter coefficients return the gain at a particular frequency.", "response": "def get_filter_gain(b, a, f_gain, fs):\n    \"\"\"\n    Given filter coefficients, return the gain at a particular\n    frequency.\n\n    Parameters\n    ----------\n    b : list\n        List of linear filter b coefficients\n    a : list\n        List of linear filter a coefficients\n    f_gain : int or float, optional\n        The frequency at which to calculate the gain\n    fs : int or float, optional\n        The sampling frequency of the system\n\n    \"\"\"\n    # Save the passband gain\n    w, h = signal.freqz(b, a)\n    w_gain = f_gain * 2 * np.pi / fs\n\n    ind = np.where(w >= w_gain)[0][0]\n    gain = abs(h[ind])\n\n    return gain"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_item_type(item, field_name, allowed_types, expect_list=False,\n                    required_channels='all'):\n    \"\"\"\n    Check the item's type against a set of allowed types.\n    Vary the print message regarding whether the item can be None.\n    Helper to `BaseRecord.check_field`.\n\n    Parameters\n    ----------\n    item : any\n        The item to check.\n    field_name : str\n        The field name.\n    allowed_types : iterable\n        Iterable of types the item is allowed to be.\n    expect_list : bool, optional\n        Whether the item is expected to be a list.\n    required_channels : list, optional\n        List of integers specifying which channels of the item must be\n        present. May be set to 'all' to indicate all channels. Only used\n        if `expect_list` is True, ie. item is a list, and its\n        subelements are to be checked.\n\n    Notes\n    -----\n    This is called by `check_field`, which determines whether the item\n    should be a list or not. This function should generally not be\n    called by the user directly.\n\n    \"\"\"\n    if expect_list:\n        if not isinstance(item, list):\n            raise TypeError('Field `%s` must be a list.' % field_name)\n\n        # All channels of the field must be present.\n        if required_channels == 'all':\n            required_channels = list(range(len(item)))\n\n        for ch in range(len(item)):\n            # Check whether the field may be None\n            if ch in required_channels:\n                allowed_types_ch = allowed_types\n            else:\n                allowed_types_ch = allowed_types + (type(None),)\n\n            if not isinstance(item[ch], allowed_types_ch):\n                raise TypeError('Channel %d of field `%s` must be one of the following types:' % (ch, field_name),\n                                allowed_types_ch)\n    else:\n        if not isinstance(item, allowed_types):\n            raise TypeError('Field `%s` must be one of the following types:',\n                            allowed_types)", "response": "Checks the item s type against a set of allowed types."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks a numpy array s shape and dtype against required array specifications.", "response": "def check_np_array(item, field_name, ndim, parent_class, channel_num=None):\n    \"\"\"\n    Check a numpy array's shape and dtype against required\n    specifications.\n\n    Parameters\n    ----------\n    item : numpy array\n        The numpy array to check\n    field_name : str\n        The name of the field to check\n    ndim : int\n        The required number of dimensions\n    parent_class : type\n        The parent class of the dtype. ie. np.integer, np.floating.\n    channel_num : int, optional\n        If not None, indicates that the item passed in is a subelement\n        of a list. Indicate this in the error message if triggered.\n\n    \"\"\"\n    # Check shape\n    if item.ndim != ndim:\n        error_msg = 'Field `%s` must have ndim == %d' % (field_name, ndim)\n        if channel_num is not None:\n            error_msg = ('Channel %d of f' % channel_num) + error_msg[1:]\n        raise TypeError(error_msg)\n\n    # Check dtype\n    if not np.issubdtype(item.dtype, parent_class):\n        error_msg = 'Field `%s` must have a dtype that subclasses %s' % (field_name, parent_class)\n        if channel_num is not None:\n            error_msg = ('Channel %d of f' % channel_num) + error_msg[1:]\n        raise TypeError(error_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rdheader(record_name, pb_dir=None, rd_segments=False):\n    dir_name, base_record_name = os.path.split(record_name)\n    dir_name = os.path.abspath(dir_name)\n\n    # Read the header file. Separate comment and non-comment lines\n    header_lines, comment_lines = _header._read_header_lines(base_record_name,\n                                                             dir_name, pb_dir)\n\n    # Get fields from record line\n    record_fields = _header._parse_record_line(header_lines[0])\n\n    # Single segment header - Process signal specification lines\n    if record_fields['n_seg'] is None:\n        # Create a single-segment WFDB record object\n        record = Record()\n\n        # There are signals\n        if len(header_lines)>1:\n            # Read the fields from the signal lines\n            signal_fields = _header._parse_signal_lines(header_lines[1:])\n            # Set the object's signal fields\n            for field in signal_fields:\n                setattr(record, field, signal_fields[field])\n\n        # Set the object's record line fields\n        for field in record_fields:\n            if field == 'n_seg':\n                continue\n            setattr(record, field, record_fields[field])\n    # Multi segment header - Process segment specification lines\n    else:\n        # Create a multi-segment WFDB record object\n        record = MultiRecord()\n        # Read the fields from the segment lines\n        segment_fields = _header._read_segment_lines(header_lines[1:])\n        # Set the object's segment fields\n        for field in segment_fields:\n            setattr(record, field, segment_fields[field])\n        # Set the objects' record fields\n        for field in record_fields:\n            setattr(record, field, record_fields[field])\n\n        # Determine whether the record is fixed or variable\n        if record.seg_len[0] == 0:\n            record.layout = 'variable'\n        else:\n            record.layout = 'fixed'\n\n        # If specified, read the segment headers\n        if rd_segments:\n            record.segments = []\n            # Get the base record name (could be empty)\n            for s in record.seg_name:\n                if s == '~':\n                    record.segments.append(None)\n                else:\n                    record.segments.append(rdheader(os.path.join(dir_name, s),\n                                                    pb_dir))\n            # Fill in the sig_name attribute\n            record.sig_name = record.get_sig_name()\n            # Fill in the sig_segments attribute\n            record.sig_segments = record.get_sig_segments()\n\n    # Set the comments field\n    record.comments = [line.strip(' \\t#') for line in comment_lines]\n\n    return record", "response": "Read a WFDB header file and return a Record or MultiRecord object representing the contents of the header file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a WFDB record and returns the signal and record descriptors as a Record or MultiRecord object.", "response": "def rdrecord(record_name, sampfrom=0, sampto=None, channels=None,\n             physical=True, pb_dir=None, m2s=True, smooth_frames=True,\n             ignore_skew=False, return_res=64, force_channels=True,\n             channel_names=None, warn_empty=False):\n    \"\"\"\n    Read a WFDB record and return the signal and record descriptors as\n    attributes in a Record or MultiRecord object.\n\n    Parameters\n    ----------\n    record_name : str\n        The name of the WFDB record to be read, without any file\n        extensions. If the argument contains any path delimiter\n        characters, the argument will be interpreted as PATH/BASE_RECORD.\n        Both relative and absolute paths are accepted. If the `pb_dir`\n        parameter is set, this parameter should contain just the base\n        record name, and the files fill be searched for remotely.\n        Otherwise, the data files will be searched for in the local path.\n    sampfrom : int, optional\n        The starting sample number to read for all channels.\n    sampto : int, or 'end', optional\n        The sample number at which to stop reading for all channels.\n        Reads the entire duration by default.\n    channels : list, optional\n        List of integer indices specifying the channels to be read.\n        Reads all channels by default.\n    physical : bool, optional\n        Specifies whether to return signals in physical units in the\n        `p_signal` field (True), or digital units in the `d_signal`\n        field (False).\n    pb_dir : str, optional\n        Option used to stream data from Physiobank. The Physiobank\n        database directory from which to find the required record files.\n        eg. For record '100' in 'http://physionet.org/physiobank/database/mitdb'\n        pb_dir='mitdb'.\n    m2s : bool, optional\n        Used when reading multi-segment records. Specifies whether to\n        directly return a wfdb MultiRecord object (False), or to convert\n        it into and return a wfdb Record object (True).\n    smooth_frames : bool, optional\n        Used when reading records with signals having multiple samples\n        per frame. Specifies whether to smooth the samples in signals\n        with more than one sample per frame and return an (MxN) uniform\n        numpy array as the `d_signal` or `p_signal` field (True), or to\n        return a list of 1d numpy arrays containing every expanded\n        sample as the `e_d_signal` or `e_p_signal` field (False).\n    ignore_skew : bool, optional\n        Used when reading records with at least one skewed signal.\n        Specifies whether to apply the skew to align the signals in the\n        output variable (False), or to ignore the skew field and load in\n        all values contained in the dat files unaligned (True).\n    return_res : int, optional\n        The numpy array dtype of the returned signals. Options are: 64,\n        32, 16, and 8, where the value represents the numpy int or float\n        dtype. Note that the value cannot be 8 when physical is True\n        since there is no float8 format.\n    force_channels : bool, optional\n        Used when reading multi-segment variable layout records. Whether\n        to update the layout specification record, and the converted\n        Record object if `m2s` is True, to match the input `channels`\n        argument, or to omit channels in which no read segment contains\n        the signals.\n    channel_names : list, optional\n        List of channel names to return. If this parameter is specified,\n        it takes precedence over `channels`.\n    warn_empty : bool, optional\n        Whether to display a warning if the specified channel indices\n        or names are not contained in the record, and no signal is\n        returned.\n\n    Returns\n    -------\n    record : Record or MultiRecord\n        The wfdb Record or MultiRecord object representing the contents\n        of the record read.\n\n    Notes\n    -----\n    If a signal range or channel selection is specified when calling\n    this function, the resulting attributes of the returned object will\n    be set to reflect the section of the record that is actually read,\n    rather than necessarily the entire record. For example, if\n    `channels=[0, 1, 2]` is specified when reading a 12 channel record,\n    the 'n_sig' attribute will be 3, not 12.\n\n    The `rdsamp` function exists as a simple alternative to `rdrecord`\n    for the common purpose of extracting the physical signals and a few\n    important descriptor fields.\n\n    Examples\n    --------\n    >>> record = wfdb.rdrecord('sample-data/test01_00s', sampfrom=800,\n                               channels=[1, 3])\n\n    \"\"\"\n\n    dir_name, base_record_name = os.path.split(record_name)\n    dir_name = os.path.abspath(dir_name)\n\n    # Read the header fields\n    record = rdheader(record_name, pb_dir=pb_dir, rd_segments=False)\n\n    # Set defaults for sampto and channels input variables\n    if sampto is None:\n        # If the header does not contain the signal length, figure it\n        # out from the first dat file. This is only possible for single\n        # segment records. If there are no signals, sig_len is 0.\n        if record.sig_len is None:\n            if record.n_sig == 0:\n                record.sig_len = 0\n            else:\n                record.sig_len = _signal._infer_sig_len(\n                    file_name=record.file_name[0], fmt=record.fmt[0],\n                    n_sig=record.file_name.count(record.file_name[0]),\n                    dir_name=dir_name, pb_dir=pb_dir)\n        sampto = record.sig_len\n\n    # channel_names takes precedence over channels\n    if channel_names is not None:\n        # Figure out the channel indices matching the record, if any.\n        if isinstance(record, Record):\n            reference_record = record\n        else:\n            if record.layout == 'fixed':\n                # Find the first non-empty segment to get the signal\n                # names\n                first_seg_name = [n for n in record.seg_name if n != '~'][0]\n                reference_record = rdheader(os.path.join(dir_name,\n                                                         record.seg_name[0]),\n                                            pb_dir=pb_dir)\n            else:\n                # Use the layout specification header to get the signal\n                # names\n                reference_record = rdheader(os.path.join(dir_name,\n                                                         record.seg_name[0]),\n                                            pb_dir=pb_dir)\n\n        channels = _get_wanted_channels(wanted_sig_names=channel_names,\n                                       record_sig_names=reference_record.sig_name)\n\n    elif channels is None:\n        channels = list(range(record.n_sig))\n\n    # Ensure that input fields are valid for the record\n    record.check_read_inputs(sampfrom, sampto, channels, physical,\n                             smooth_frames, return_res)\n\n    # If the signal doesn't have the specified channels, there will be\n    # no signal. Recall that `rdsamp` is not called on segments of multi\n    # segment records if the channels are not present, so this won't\n    # break anything.\n    if not len(channels):\n        old_record = record\n        record = Record()\n        for attr in _header.RECORD_SPECS.index:\n            if attr == 'n_seg':\n                continue\n            elif attr in ['n_sig', 'sig_len']:\n                setattr(record, attr, 0)\n            else:\n                setattr(record, attr, getattr(old_record, attr))\n        if warn_empty:\n            print('None of the specified signals were contained in the record')\n\n    # A single segment record\n    elif isinstance(record, Record):\n\n        # Only 1 sample/frame, or frames are smoothed. Return uniform numpy array\n        if smooth_frames or max([record.samps_per_frame[c] for c in channels]) == 1:\n            # Read signals from the associated dat files that contain\n            # wanted channels\n            record.d_signal = _signal._rd_segment(record.file_name, dir_name,\n                                                  pb_dir, record.fmt,\n                                                  record.n_sig, record.sig_len,\n                                                  record.byte_offset,\n                                                  record.samps_per_frame,\n                                                  record.skew, sampfrom, sampto,\n                                                  channels, smooth_frames,\n                                                  ignore_skew)\n\n            # Arrange/edit the object fields to reflect user channel\n            # and/or signal range input\n            record._arrange_fields(channels=channels, sampfrom=sampfrom,\n                                   expanded=False)\n\n            if physical:\n                # Perform inplace dac to get physical signal\n                record.dac(expanded=False, return_res=return_res, inplace=True)\n\n        # Return each sample of the signals with multiple samples per frame\n        else:\n            record.e_d_signal = _signal._rd_segment(record.file_name, dir_name,\n                                                    pb_dir, record.fmt,\n                                                    record.n_sig,\n                                                    record.sig_len,\n                                                    record.byte_offset,\n                                                    record.samps_per_frame,\n                                                    record.skew, sampfrom,\n                                                    sampto, channels,\n                                                    smooth_frames, ignore_skew)\n\n            # Arrange/edit the object fields to reflect user channel\n            # and/or signal range input\n            record._arrange_fields(channels=channels, sampfrom=sampfrom,\n                                   expanded=True)\n\n            if physical:\n                # Perform dac to get physical signal\n                record.dac(expanded=True, return_res=return_res, inplace=True)\n\n    # A multi segment record\n    else:\n        # Strategy:\n        # 1. Read the required segments and store them in\n        # Record objects.\n        # 2. Update the parameters of the objects to reflect\n        # the state of the sections read.\n        # 3. Update the parameters of the overall MultiRecord\n        # object to reflect the state of the individual segments.\n        # 4. If specified, convert the MultiRecord object\n        # into a single Record object.\n\n        # Segments field is a list of Record objects\n        # Empty segments store None.\n\n        record.segments = [None] * record.n_seg\n\n        # Variable layout, read the layout specification header\n        if record.layout == 'variable':\n            record.segments[0] = rdheader(os.path.join(dir_name,\n                                                       record.seg_name[0]),\n                                          pb_dir=pb_dir)\n\n        # The segment numbers and samples within each segment to read.\n        seg_numbers, seg_ranges  = record._required_segments(sampfrom, sampto)\n        # The channels within each segment to read\n        seg_channels = record._required_channels(seg_numbers, channels,\n                                                 dir_name, pb_dir)\n\n        # Read the desired samples in the relevant segments\n        for i in range(len(seg_numbers)):\n            seg_num = seg_numbers[i]\n            # Empty segment or segment with no relevant channels\n            if record.seg_name[seg_num] == '~' or len(seg_channels[i]) == 0:\n                record.segments[seg_num] = None\n            else:\n                record.segments[seg_num] = rdrecord(\n                    os.path.join(dir_name, record.seg_name[seg_num]),\n                    sampfrom=seg_ranges[i][0], sampto=seg_ranges[i][1],\n                    channels=seg_channels[i], physical=physical, pb_dir=pb_dir)\n\n        # Arrange the fields of the layout specification segment, and\n        # the overall object, to reflect user input.\n        record._arrange_fields(seg_numbers=seg_numbers, seg_ranges=seg_ranges,\n                               channels=channels, sampfrom=sampfrom,\n                               force_channels=force_channels)\n\n        # Convert object into a single segment Record object\n        if m2s:\n            record = record.multi_to_single(physical=physical,\n                                            return_res=return_res)\n\n    # Perform dtype conversion if necessary\n    if isinstance(record, Record) and record.n_sig > 0:\n        record.convert_dtype(physical, return_res, smooth_frames)\n\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rdsamp(record_name, sampfrom=0, sampto=None, channels=None, pb_dir=None,\n           channel_names=None, warn_empty=False):\n    \"\"\"\n    Read a WFDB record, and return the physical signals and a few important\n    descriptor fields.\n\n    Parameters\n    ----------\n    record_name : str\n        The name of the WFDB record to be read (without any file\n        extensions). If the argument contains any path delimiter\n        characters, the argument will be interpreted as PATH/baserecord\n        and the data files will be searched for in the local path.\n    sampfrom : int, optional\n        The starting sample number to read for all channels.\n    sampto : int, or 'end', optional\n        The sample number at which to stop reading for all channels.\n        Reads the entire duration by default.\n    channels : list, optional\n        List of integer indices specifying the channels to be read.\n        Reads all channels by default.\n    pb_dir : str, optional\n        Option used to stream data from Physiobank. The Physiobank\n        database directory from which to find the required record files.\n        eg. For record '100' in 'http://physionet.org/physiobank/database/mitdb'\n        pb_dir='mitdb'.\n    channel_names : list, optional\n        List of channel names to return. If this parameter is specified,\n        it takes precedence over `channels`.\n    warn_empty : bool, optional\n        Whether to display a warning if the specified channel indices\n        or names are not contained in the record, and no signal is\n        returned.\n\n    Returns\n    -------\n    signals : numpy array\n        A 2d numpy array storing the physical signals from the record.\n    fields : dict\n        A dictionary containing several key attributes of the read\n        record:\n          - fs: The sampling frequency of the record\n          - units: The units for each channel\n          - sig_name: The signal name for each channel\n          - comments: Any comments written in the header\n\n    Notes\n    -----\n    If a signal range or channel selection is specified when calling\n    this function, the resulting attributes of the returned object will\n    be set to reflect the section of the record that is actually read,\n    rather than necessarily the entire record. For example, if\n    `channels=[0, 1, 2]` is specified when reading a 12 channel record,\n    the 'n_sig' attribute will be 3, not 12.\n\n    The `rdrecord` function is the base function upon which this one is\n    built. It returns all attributes present, along with the signals, as\n    attributes in a `Record` object. The function, along with the\n    returned data type, has more options than `rdsamp` for users who\n    wish to more directly manipulate WFDB content.\n\n    Examples\n    --------\n    >>> signals, fields = wfdb.rdsamp('sample-data/test01_00s',\n                                      sampfrom=800,\n                                      channel =[1,3])\n\n    \"\"\"\n    record = rdrecord(record_name=record_name, sampfrom=sampfrom,\n                      sampto=sampto, channels=channels, physical=True,\n                      pb_dir=pb_dir, m2s=True, channel_names=channel_names,\n                      warn_empty=warn_empty)\n\n    signals = record.p_signal\n    fields = {}\n    for field in ['fs','sig_len', 'n_sig', 'base_date', 'base_time',\n                  'units','sig_name', 'comments']:\n        fields[field] = getattr(record, field)\n\n    return signals, fields", "response": "Reads a WFDB record and returns the physical signals and descriptor fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive some wanted signal names and the record names return the indices of the record channels that intersect.", "response": "def _get_wanted_channels(wanted_sig_names, record_sig_names, pad=False):\n    \"\"\"\n    Given some wanted signal names, and the signal names contained in a\n    record, return the indices of the record channels that intersect.\n\n    Parameters\n    ----------\n    wanted_sig_names : list\n        List of desired signal name strings\n    record_sig_names : list\n        List of signal names for a single record\n    pad : bool, optional\n        Whether the output channels is to always have the same number\n        of elements and the wanted channels. If True, pads missing\n        signals with None.\n\n    Returns\n    -------\n    wanted_channel_inds\n\n    \"\"\"\n    if pad:\n        return [record_sig_names.index(s) if s in record_sig_names else None for s in wanted_sig_names]\n    else:\n        return [record_sig_names.index(s) for s in wanted_sig_names if s in record_sig_names]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the list of items in a list are monotonic.", "response": "def is_monotonic(full_list):\n    \"\"\"\n    Determine whether elements in a list are monotonic. ie. unique\n    elements are clustered together.\n\n    ie. [5,5,3,4] is, [5,3,5] is not.\n    \"\"\"\n    prev_elements = set({full_list[0]})\n    prev_item = full_list[0]\n\n    for item in full_list:\n        if item != prev_item:\n            if item in prev_elements:\n                return False\n            prev_item = item\n            prev_elements.add(item)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dl_database(db_dir, dl_dir, records='all', annotators='all',\n                keep_subdirs=True, overwrite=False):\n    \"\"\"\n    Download WFDB record (and optionally annotation) files from a\n    Physiobank database. The database must contain a 'RECORDS' file in\n    its base directory which lists its WFDB records.\n\n    Parameters\n    ----------\n    db_dir : str\n        The Physiobank database directory to download. eg. For database:\n        'http://physionet.org/physiobank/database/mitdb', db_dir='mitdb'.\n    dl_dir : str\n        The full local directory path in which to download the files.\n    records : list, or 'all', optional\n        A list of strings specifying the WFDB records to download. Leave\n        as 'all' to download all records listed in the database's\n        RECORDS file.\n        eg. records=['test01_00s', test02_45s] for database:\n        https://physionet.org/physiobank/database/macecgdb/\n    annotators : list, 'all', or None, optional\n        A list of strings specifying the WFDB annotation file types to\n        download along with the record files. Is either None to skip\n        downloading any annotations, 'all' to download all annotation\n        types as specified by the ANNOTATORS file, or a list of strings\n        which each specify an annotation extension.\n        eg. annotators = ['anI'] for database:\n        https://physionet.org/physiobank/database/prcp/\n    keep_subdirs : bool, optional\n        Whether to keep the relative subdirectories of downloaded files\n        as they are organized in Physiobank (True), or to download all\n        files into the same base directory (False).\n    overwrite : bool, optional\n        If True, all files will be redownloaded regardless. If False,\n        existing files with the same name and relative subdirectory will\n        be checked. If the local file is the same size as the online\n        file, the download is skipped. If the local file is larger, it\n        will be deleted and the file will be redownloaded. If the local\n        file is smaller, the file will be assumed to be partially\n        downloaded and the remaining bytes will be downloaded and\n        appended.\n\n    Examples\n    --------\n    >>> wfdb.dl_database('ahadb', os.getcwd())\n\n    \"\"\"\n    # Full url physiobank database\n    db_url = posixpath.join(download.config.db_index_url, db_dir)\n    # Check if the database is valid\n    r = requests.get(db_url)\n    r.raise_for_status()\n\n    # Get the list of records\n    recordlist = download.get_record_list(db_dir, records)\n    # Get the annotator extensions\n    annotators = download.get_annotators(db_dir, annotators)\n\n    # All files to download (relative to the database's home directory)\n    allfiles = []\n\n    for rec in recordlist:\n        # Check out whether each record is in MIT or EDF format\n        if rec.endswith('.edf'):\n            allfiles.append(rec)\n        else:\n            # May be pointing to directory\n            if rec.endswith('/'):\n                rec = rec + rec[:-1]\n            # If MIT format, have to figure out all associated files\n            allfiles.append(rec+'.hea')\n            dir_name, baserecname = os.path.split(rec)\n            record = rdheader(baserecname, pb_dir=posixpath.join(db_dir, dir_name))\n\n            # Single segment record\n            if isinstance(record, Record):\n                # Add all dat files of the segment\n                for file in (record.file_name if record.file_name else []):\n                    allfiles.append(posixpath.join(dir_name, file))\n\n            # Multi segment record\n            else:\n                for seg in record.seg_name:\n                    # Skip empty segments\n                    if seg == '~':\n                        continue\n                    # Add the header\n                    allfiles.append(posixpath.join(dir_name, seg+'.hea'))\n                    # Layout specifier has no dat files\n                    if seg.endswith('_layout'):\n                        continue\n                    # Add all dat files of the segment\n                    recseg = rdheader(seg, pb_dir=posixpath.join(db_dir, dir_name))\n                    for file in recseg.file_name:\n                        allfiles.append(posixpath.join(dir_name, file))\n        # check whether the record has any requested annotation files\n        if annotators is not None:\n            for a in annotators:\n                annfile = rec+'.'+a\n                url = posixpath.join(download.config.db_index_url, db_dir, annfile)\n                rh = requests.head(url)\n\n                if rh.status_code != 404:\n                    allfiles.append(annfile)\n\n    dlinputs = [(os.path.split(file)[1], os.path.split(file)[0], db_dir, dl_dir, keep_subdirs, overwrite) for file in allfiles]\n\n    # Make any required local directories\n    download.make_local_dirs(dl_dir, dlinputs, keep_subdirs)\n\n    print('Downloading files...')\n    # Create multiple processes to download files.\n    # Limit to 2 connections to avoid overloading the server\n    pool = multiprocessing.Pool(processes=2)\n    pool.map(download.dl_pb_file, dlinputs)\n    print('Finished downloading files')\n\n    return", "response": "Download WFDB record files from Physiobank database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_field(self, field, required_channels='all'):\n        item = getattr(self, field)\n        if item is None:\n            raise Exception('Missing field required: %s' % field)\n\n        # We should have a list specifying these automatically.\n\n        # Whether the item should be a list. Watch out for required_channels for `segments`\n        expect_list = True if field in LIST_FIELDS else False\n\n        # Check the type of the field (and of its elements if it should\n        # be a list)\n        _check_item_type(item, field_name=field,\n                        allowed_types=ALLOWED_TYPES[field],\n                        expect_list=expect_list,\n                        required_channels=required_channels)\n\n        # Individual specific field checks\n\n        if field in ['d_signal', 'p_signal']:\n            check_np_array(item=item, field_name=field, ndim=2,\n                           parent_class=(lambda f: np.integer if f == 'd_signal' else np.floating)(field))\n        elif field in ['e_d_signal', 'e_p_signal']:\n            for ch in range(len(item)):\n                check_np_array(item=item[ch], field_name=field,\n                               ndim=1, parent_class=(lambda f: np.integer if f == 'e_d_signal' else np.floating)(field),\n                               channel_num=ch)\n\n        # Record specification fields\n\n        elif field == 'record_name':\n            # Allow letters, digits, hyphens, and underscores.\n            accepted_string = re.match('[-\\w]+', self.record_name)\n            if not accepted_string or accepted_string.string != self.record_name:\n                raise ValueError('record_name must only comprise of letters, digits, hyphens, and underscores.')\n        elif field == 'n_seg':\n            if self.n_seg <= 0:\n                raise ValueError('n_seg must be a positive integer')\n        elif field == 'n_sig':\n            if self.n_sig <= 0:\n                raise ValueError('n_sig must be a positive integer')\n        elif field == 'fs':\n            if self.fs <= 0:\n                raise ValueError('fs must be a positive number')\n        elif field == 'counter_freq':\n            if self.counter_freq <= 0:\n                raise ValueError('counter_freq must be a positive number')\n        elif field == 'base_counter':\n            if self.base_counter <= 0:\n                raise ValueError('base_counter must be a positive number')\n        elif field == 'sig_len':\n            if self.sig_len < 0:\n                raise ValueError('sig_len must be a non-negative integer')\n\n        # Signal specification fields\n        elif field in _header.SIGNAL_SPECS.index:\n            if required_channels == 'all':\n                required_channels = range(len(item))\n\n            for ch in range(len(item)):\n                # If the element is allowed to be None\n                if ch not in required_channels:\n                    if item[ch] is None:\n                        continue\n\n                if field == 'file_name':\n                    # Check for file_name characters\n                    accepted_string = re.match('[-\\w]+\\.?[\\w]+', item[ch])\n                    if not accepted_string or accepted_string.string != item[ch]:\n                        raise ValueError('File names should only contain alphanumerics, hyphens, and an extension. eg. record-100.dat')\n                    # Check that dat files are grouped together\n                    if not is_monotonic(self.file_name):\n                        raise ValueError('Signals in a record that share a given file must be consecutive.')\n                elif field == 'fmt':\n                    if item[ch] not in _signal.DAT_FMTS:\n                        raise ValueError('File formats must be valid WFDB dat formats:', _signal.DAT_FMTS)\n                elif field == 'samps_per_frame':\n                    if item[ch] < 1:\n                        raise ValueError('samps_per_frame values must be positive integers')\n                elif field == 'skew':\n                    if item[ch] < 0:\n                        raise ValueError('skew values must be non-negative integers')\n                elif field == 'byte_offset':\n                    if item[ch] < 0:\n                        raise ValueError('byte_offset values must be non-negative integers')\n                elif field == 'adc_gain':\n                    if item[ch] <= 0:\n                        raise ValueError('adc_gain values must be positive')\n                elif field == 'baseline':\n                    # Original WFDB library 10.5.24 only has 4 bytes for\n                    # baseline.\n                    if item[ch] < -2147483648 or item[ch] > 2147483648:\n                        raise ValueError('baseline values must be between -2147483648 (-2^31) and 2147483647 (2^31 -1)')\n                elif field == 'units':\n                    if re.search('\\s', item[ch]):\n                        raise ValueError('units strings may not contain whitespaces.')\n                elif field == 'adc_res':\n                    if item[ch] < 0:\n                        raise ValueError('adc_res values must be non-negative integers')\n                elif field == 'block_size':\n                    if item[ch] < 0:\n                        raise ValueError('block_size values must be non-negative integers')\n                elif field == 'sig_name':\n                    if re.search('\\s', item[ch]):\n                        raise ValueError('sig_name strings may not contain whitespaces.')\n                    if len(set(item)) != len(item):\n                        raise ValueError('sig_name strings must be unique.')\n\n        # Segment specification fields and comments\n        elif field in _header.SEGMENT_SPECS.index:\n            for ch in range(len(item)):\n                if field == 'seg_name':\n                    # Segment names must be alphanumerics or just a\n                    # single '~'\n                    if item[ch] == '~':\n                        continue\n                    accepted_string = re.match('[-\\w]+', item[ch])\n                    if not accepted_string or accepted_string.string != item[ch]:\n                        raise ValueError(\"Non-null segment names may only contain alphanumerics and dashes. Null segment names must be set to '~'\")\n                elif field == 'seg_len':\n                    # For records with more than 1 segment, the first\n                    # segment may be the layout specification segment\n                    # with a length of 0\n                    min_len = 0 if ch == 0 else 1\n                    if item[ch] < min_len:\n                        raise ValueError('seg_len values must be positive integers. Only seg_len[0] may be 0 to indicate a layout segment')\n                # Comment field\n                elif field == 'comments':\n                    if item[ch].startswith('#'):\n                        print(\"Note: comment strings do not need to begin with '#'. This library adds them automatically.\")\n                    if re.search('[\\t\\n\\r\\f\\v]', item[ch]):\n                        raise ValueError('comments may not contain tabs or newlines (they may contain spaces and underscores).')", "response": "Checks whether a single field is valid in its basic form. Does not check compatibility with other fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the input parameters for the record are valid for the record.", "response": "def check_read_inputs(self, sampfrom, sampto, channels, physical,\n                          smooth_frames, return_res):\n        \"\"\"\n        Ensure that input read parameters (from rdsamp) are valid for\n        the record\n\n        \"\"\"\n\n        # Data Type Check\n        if not hasattr(sampfrom, '__index__'):\n            raise TypeError('sampfrom must be an integer')\n        if not hasattr(sampto, '__index__'):\n            raise TypeError('sampto must be an integer')\n        if not isinstance(channels, list):\n            raise TypeError('channels must be a list of integers')\n\n        # Duration Ranges\n        if sampfrom < 0:\n            raise ValueError('sampfrom must be a non-negative integer')\n        if sampfrom > self.sig_len:\n            raise ValueError('sampfrom must be shorter than the signal length')\n        if sampto < 0:\n            raise ValueError('sampto must be a non-negative integer')\n        if sampto > self.sig_len:\n            raise ValueError('sampto must be shorter than the signal length')\n        if sampto <= sampfrom:\n            raise ValueError('sampto must be greater than sampfrom')\n\n        # Channel Ranges\n        if len(channels):\n            if min(channels) < 0:\n                raise ValueError('Input channels must all be non-negative integers')\n            if max(channels) > self.n_sig - 1:\n                raise ValueError('Input channels must all be lower than the total number of channels')\n\n        if return_res not in [64, 32, 16, 8]:\n            raise ValueError(\"return_res must be one of the following: 64, 32, 16, 8\")\n        if physical is True and return_res == 8:\n            raise ValueError(\"return_res must be one of the following when physical is True: 64, 32, 16\")\n\n        # Cannot expand multiple samples/frame for multi-segment records\n        if isinstance(self, MultiRecord):\n            if smooth_frames is False:\n                raise ValueError('This package version cannot expand all samples when reading multi-segment records. Must enable frame smoothing.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadjusts date and time fields to reflect user input if possible.", "response": "def _adjust_datetime(self, sampfrom):\n        \"\"\"\n        Adjust date and time fields to reflect user input if possible.\n\n        Helper function for the `_arrange_fields` of both Record and\n        MultiRecord objects.\n        \"\"\"\n        if sampfrom:\n            dt_seconds = sampfrom / self.fs\n            if self.base_date and self.base_time:\n                self.base_datetime = datetime.datetime.combine(self.base_date,\n                                                               self.base_time)\n                self.base_datetime += datetime.timedelta(seconds=dt_seconds)\n                self.base_date = self.base_datetime.date()\n                self.base_time = self.base_datetime.time()\n            # We can calculate the time even if there is no date\n            elif self.base_time:\n                tmp_datetime = datetime.datetime.combine(\n                    datetime.datetime.today().date(), self.base_time)\n                self.base_time = (tmp_datetime\n                                  + datetime.timedelta(seconds=dt_seconds)).time()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrsamp(self, expanded=False, write_dir=''):\n        # Perform field validity and cohesion checks, and write the\n        # header file.\n        self.wrheader(write_dir=write_dir)\n        if self.n_sig > 0:\n            # Perform signal validity and cohesion checks, and write the\n            # associated dat files.\n            self.wr_dats(expanded=expanded, write_dir=write_dir)", "response": "Writes a wfdb header file and any associated dat files from this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _arrange_fields(self, channels, sampfrom=0, expanded=False):\n\n        # Rearrange signal specification fields\n        for field in _header.SIGNAL_SPECS.index:\n            item = getattr(self, field)\n            setattr(self, field, [item[c] for c in channels])\n\n        # Expanded signals - multiple samples per frame.\n        if expanded:\n            # Checksum and init_value to be updated if present\n            # unless the whole signal length was input\n            if self.sig_len != int(len(self.e_d_signal[0]) / self.samps_per_frame[0]):\n                self.checksum = self.calc_checksum(expanded)\n                self.init_value = [s[0] for s in self.e_d_signal]\n\n            self.n_sig = len(channels)\n            self.sig_len = int(len(self.e_d_signal[0]) / self.samps_per_frame[0])\n\n        # MxN numpy array d_signal\n        else:\n            # Checksum and init_value to be updated if present\n            # unless the whole signal length was input\n            if self.sig_len != self.d_signal.shape[0]:\n\n                if self.checksum is not None:\n                    self.checksum = self.calc_checksum()\n                if self.init_value is not None:\n                    ival = list(self.d_signal[0, :])\n                    self.init_value = [int(i) for i in ival]\n\n            # Update record specification parameters\n            # Important that these get updated after^^\n            self.n_sig = len(channels)\n            self.sig_len = self.d_signal.shape[0]\n\n        # Adjust date and time if necessary\n        self._adjust_datetime(sampfrom=sampfrom)", "response": "Arrange object fields to reflect user channel and signal and signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a multi - segment header along with headers and dat files for all segments and the object itself.", "response": "def wrsamp(self, write_dir=''):\n        \"\"\"\n        Write a multi-segment header, along with headers and dat files\n        for all segments, from this object.\n        \"\"\"\n        # Perform field validity and cohesion checks, and write the\n        # header file.\n        self.wrheader(write_dir=write_dir)\n        # Perform record validity and cohesion checks, and write the\n        # associated segments.\n        for seg in self.segments:\n            seg.wrsamp(write_dir=write_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_segment_cohesion(self):\n\n        if self.n_seg != len(self.segments):\n            raise ValueError(\"Length of segments must match the 'n_seg' field\")\n\n        for i in range(n_seg):\n            s = self.segments[i]\n\n            # If segment 0 is a layout specification record, check that its file names are all == '~''\n            if i == 0 and self.seg_len[0] == 0:\n                for file_name in s.file_name:\n                    if file_name != '~':\n                        raise ValueError(\"Layout specification records must have all file_names named '~'\")\n\n            # Sampling frequencies must all match the one in the master header\n            if s.fs != self.fs:\n                raise ValueError(\"The 'fs' in each segment must match the overall record's 'fs'\")\n\n            # Check the signal length of the segment against the corresponding seg_len field\n            if s.sig_len != self.seg_len[i]:\n                raise ValueError('The signal length of segment '+str(i)+' does not match the corresponding segment length')\n\n            totalsig_len = totalsig_len + getattr(s, 'sig_len')", "response": "Checks the cohesion of the segments field with other fields used\n        to write the record."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the segments and the samples within each segment in a multi - segment record that lie within a sample range.", "response": "def _required_segments(self, sampfrom, sampto):\n        \"\"\"\n        Determine the segments and the samples within each segment in a\n        multi-segment record, that lie within a sample range.\n\n        Parameters\n        ----------\n        sampfrom : int\n            The starting sample number to read for each channel.\n        sampto : int\n            The sample number at which to stop reading for each channel.\n\n        \"\"\"\n\n        # The starting segment with actual samples\n        if self.layout == 'fixed':\n            startseg = 0\n        else:\n            startseg = 1\n\n        # Cumulative sum of segment lengths (ignoring layout segment)\n        cumsumlengths = list(np.cumsum(self.seg_len[startseg:]))\n        # Get first segment\n        seg_numbers = [[sampfrom < cs for cs in cumsumlengths].index(True)]\n        # Get final segment\n        if sampto == cumsumlengths[len(cumsumlengths) - 1]:\n            seg_numbers.append(len(cumsumlengths) - 1)\n        else:\n            seg_numbers.append([sampto <= cs for cs in cumsumlengths].index(True))\n\n        # Add 1 for variable layout records\n        seg_numbers = list(np.add(seg_numbers,startseg))\n\n        # Obtain the sampfrom and sampto to read for each segment\n        if seg_numbers[1] == seg_numbers[0]:\n            # Only one segment to read\n            seg_numbers = [seg_numbers[0]]\n            # The segment's first sample number relative to the entire record\n            segstartsamp = sum(self.seg_len[0:seg_numbers[0]])\n            readsamps = [[sampfrom-segstartsamp, sampto-segstartsamp]]\n\n        else:\n            # More than one segment to read\n            seg_numbers = list(range(seg_numbers[0], seg_numbers[1]+1))\n            readsamps = [[0, self.seg_len[s]] for s in seg_numbers]\n\n            # Starting sample for first segment.\n            readsamps[0][0] = sampfrom - ([0] + cumsumlengths)[seg_numbers[0]-startseg]\n\n            # End sample for last segment\n            readsamps[-1][1] = sampto - ([0] + cumsumlengths)[seg_numbers[-1]-startseg]\n\n        return (seg_numbers, readsamps)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the channel numbers to be read from each specified segment and the channel numbers specified for the entire record.", "response": "def _required_channels(self, seg_numbers, channels, dir_name, pb_dir):\n        \"\"\"\n        Get the channel numbers to be read from each specified segment,\n        given the channel numbers specified for the entire record.\n\n        Parameters\n        ----------\n        seg_numbers : list\n            List of segment numbers to read.\n        channels : list\n            The channel indices to read for the whole record. Same one\n            specified by user input.\n\n        Returns\n        -------\n        required_channels : list\n            List of lists, containing channel indices to read for each\n            desired segment.\n\n        \"\"\"\n\n        # Fixed layout. All channels are the same.\n        if self.layout == 'fixed':\n            required_channels = [channels] * len(seg_numbers)\n        # Variable layout: figure out channels by matching record names\n        else:\n            required_channels = []\n            # The overall layout signal names\n            l_sig_names = self.segments[0].sig_name\n            # The wanted signals\n            w_sig_names = [l_sig_names[c] for c in channels]\n\n            # For each segment\n            for i in range(len(seg_numbers)):\n                # Skip empty segments\n                if self.seg_name[seg_numbers[i]] == '~':\n                    required_channels.append([])\n                else:\n                    # Get the signal names of the current segment\n                    s_sig_names = rdheader(\n                        os.path.join(dir_name, self.seg_name[seg_numbers[i]]),\n                        pb_dir=pb_dir).sig_name\n                    required_channels.append(_get_wanted_channels(\n                        w_sig_names, s_sig_names))\n\n        return required_channels"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\narrange and edit object fields to reflect user channel and or signal range inputs.", "response": "def _arrange_fields(self, seg_numbers, seg_ranges, channels,\n                        sampfrom=0, force_channels=True):\n        \"\"\"\n        Arrange/edit object fields to reflect user channel and/or\n        signal range inputs. Updates layout specification header if\n        necessary.\n\n        Parameters\n        ----------\n        seg_numbers : list\n            List of integer segment numbers read.\n        seg_ranges: list\n            List of integer pairs, giving the sample ranges for each\n            segment number read.\n        channels : list\n            List of channel numbers specified\n        sampfrom : int\n            Starting sample read.\n        force_channels : bool, optional\n            Used when reading multi-segment variable layout records.\n            Whether to update the layout specification record to match\n            the input `channels` argument, or to omit channels in which\n            no read segment contains the signals.\n\n        \"\"\"\n        # Update seg_len values for relevant segments\n        for i in range(len(seg_numbers)):\n            self.seg_len[seg_numbers[i]] = seg_ranges[i][1] - seg_ranges[i][0]\n\n        # Get rid of the segments and segment line parameters\n        # outside the desired segment range\n        if self.layout == 'fixed':\n            self.n_sig = len(channels)\n            self.segments = self.segments[seg_numbers[0]:seg_numbers[-1]+1]\n            self.seg_name = self.seg_name[seg_numbers[0]:seg_numbers[-1]+1]\n            self.seg_len = self.seg_len[seg_numbers[0]:seg_numbers[-1]+1]\n        else:\n            self.segments = [self.segments[0]] + self.segments[seg_numbers[0]:seg_numbers[-1]+1]\n            self.seg_name = [self.seg_name[0]] + self.seg_name[seg_numbers[0]:seg_numbers[-1]+1]\n            self.seg_len = [self.seg_len[0]] + self.seg_len[seg_numbers[0]:seg_numbers[-1]+1]\n\n            # Update the layout specification segment. At this point it\n            # should match the full original header\n\n            # Have to inspect existing channels of segments; requested\n            # input channels will not be enough on its own because not\n            # all signals may be present, depending on which section of\n            # the signal was read.\n            if not force_channels:\n                # The desired signal names.\n                desired_sig_names = [self.segments[0].sig_name[ch] for ch in channels]\n                # Actual contained signal names of individual segments\n                #contained_sig_names = [seg.sig_name for seg in self.segments[1:]]\n                contained_sig_names = set([name for seg in self.segments[1:] if seg is not None for name in seg.sig_name])\n                # Remove non-present names. Keep the order.\n                sig_name = [name for name in desired_sig_names if name in contained_sig_names]\n                # Channel indices to keep for signal specification fields\n                channels = [self.segments[0].sig_name.index(name) for name in sig_name]\n\n            # Rearrange signal specification fields\n            for field in _header.SIGNAL_SPECS.index:\n                item = getattr(self.segments[0], field)\n                setattr(self.segments[0], field, [item[c] for c in channels])\n\n            self.segments[0].n_sig = self.n_sig = len(channels)\n            if self.n_sig == 0:\n                print('No signals of the desired channels are contained in the specified sample range.')\n\n        # Update record specification parameters\n        self.sig_len = sum([sr[1]-sr[0] for sr in seg_ranges])\n        self.n_seg = len(self.segments)\n        self._adjust_datetime(sampfrom=sampfrom)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multi_to_single(self, physical, return_res=64):\n\n        # The fields to transfer to the new object\n        fields = self.__dict__.copy()\n\n        # Remove multirecord fields\n        for attr in ['segments', 'seg_name', 'seg_len', 'n_seg']:\n            del(fields[attr])\n\n        # Figure out single segment fields to set for the new Record\n        if self.layout == 'fixed':\n            # Get the fields from the first segment\n            for attr in ['fmt', 'adc_gain', 'baseline', 'units', 'sig_name']:\n                fields[attr] = getattr(self.segments[0], attr)\n        else:\n            # For variable layout records, inspect the segments for the\n            # attribute values.\n\n            # Coincidentally, if physical=False, figure out if this\n            # conversion can be performed. All signals of the same name\n            # must have the same fmt, gain, baseline, and units for all\n            # segments.\n\n            # The layout header should be updated at this point to\n            # reflect channels. We can depend on it for sig_name, but\n            # not for fmt, adc_gain, units, and baseline.\n\n            # These signal names will be the key\n            signal_names = self.segments[0].sig_name\n            n_sig = len(signal_names)\n\n            # This will be the field dictionary to copy over.\n            reference_fields = {'fmt':n_sig*[None], 'adc_gain':n_sig*[None],\n                                'baseline':n_sig*[None],\n                                'units':n_sig*[None]}\n\n            # For physical signals, mismatched fields will not be copied\n            # over. For digital, mismatches will cause an exception.\n            mismatched_fields = []\n            for seg in self.segments[1:]:\n                if seg is None:\n                    continue\n                # For each signal, check fmt, adc_gain, baseline, and\n                # units of each signal\n                for seg_ch in range(seg.n_sig):\n                    sig_name = seg.sig_name[seg_ch]\n                    # The overall channel\n                    ch = signal_names.index(sig_name)\n\n                    for field in reference_fields:\n                        item_ch = getattr(seg, field)[seg_ch]\n                        if reference_fields[field][ch] is None:\n                            reference_fields[field][ch] = item_ch\n                        # mismatch case\n                        elif reference_fields[field][ch] != item_ch:\n                            if physical:\n                                mismatched_fields.append(field)\n                            else:\n                                raise Exception('This variable layout multi-segment record cannot be converted to single segment, in digital format.')\n            # Remove mismatched signal fields for physical signals\n            for field in set(mismatched_fields):\n                del(reference_fields[field])\n            # At this point, the fields should be set for all channels\n            fields.update(reference_fields)\n            fields['sig_name'] = signal_names\n\n        # Figure out signal attribute to set, and its dtype.\n        if physical:\n            sig_attr = 'p_signal'\n            # Figure out the largest required dtype\n            dtype = _signal._np_dtype(return_res, discrete=False)\n            nan_vals = np.array([self.n_sig * [np.nan]], dtype=dtype)\n        else:\n            sig_attr = 'd_signal'\n            # Figure out the largest required dtype\n            dtype = _signal._np_dtype(return_res, discrete=True)\n            nan_vals = np.array([_signal._digi_nan(fields['fmt'])], dtype=dtype)\n\n        # Initialize the full signal array\n        combined_signal = np.repeat(nan_vals, self.sig_len, axis=0)\n\n        # Start and end samples in the overall array to place the\n        # segment samples into\n        start_samps = [0] + list(np.cumsum(self.seg_len)[0:-1])\n        end_samps = list(np.cumsum(self.seg_len))\n\n        if self.layout == 'fixed':\n            # Copy over the signals directly. Recall there are no\n            # empty segments in fixed layout records.\n            for i in range(self.n_seg):\n                combined_signal[start_samps[i]:end_samps[i], :] = getattr(self.segments[i], sig_attr)\n        else:\n            # Copy over the signals into the matching channels\n            for i in range(1, self.n_seg):\n                seg = self.segments[i]\n                if seg is not None:\n                    # Get the segment channels to copy over for each\n                    # overall channel\n                    segment_channels = _get_wanted_channels(fields['sig_name'],\n                                                           seg.sig_name,\n                                                           pad=True)\n                    for ch in range(self.n_sig):\n                        # Copy over relevant signal\n                        if segment_channels[ch] is not None:\n                            combined_signal[start_samps[i]:end_samps[i], ch] = getattr(seg, sig_attr)[:, segment_channels[ch]]\n\n        # Create the single segment Record object and set attributes\n        record = Record()\n        for field in fields:\n            setattr(record, field, fields[field])\n        setattr(record, sig_attr, combined_signal)\n\n        # Use the signal to set record features\n        if physical:\n            record.set_p_features()\n        else:\n            record.set_d_features()\n\n        return record", "response": "This function creates a single record from a MultiRecord object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading values from a tff file into a new nparray tree.", "response": "def rdtff(file_name, cut_end=False):\n    \"\"\"\n    Read values from a tff file\n\n    Parameters\n    ----------\n    file_name : str\n        Name of the .tff file to read\n    cut_end : bool, optional\n        If True, cuts out the last sample for all channels. This is for\n        reading files which appear to terminate with the incorrect\n        number of samples (ie. sample not present for all channels).\n\n    Returns\n    -------\n    signal : numpy array\n        A 2d numpy array storing the physical signals from the record.\n    fields : dict\n        A dictionary containing several key attributes of the read record.\n    markers : numpy array\n        A 1d numpy array storing the marker locations.\n    triggers : numpy array\n        A 1d numpy array storing the trigger locations.\n\n    Notes\n    -----\n    This function is slow because tff files may contain any number of\n    escape sequences interspersed with the signals. There is no way to\n    know the number of samples/escape sequences beforehand, so the file\n    is inefficiently parsed a small chunk at a time.\n\n    It is recommended that you convert your tff files to wfdb format.\n\n    \"\"\"\n    file_size = os.path.getsize(file_name)\n    with open(file_name, 'rb') as fp:\n        fields, file_fields = _rdheader(fp)\n        signal, markers, triggers = _rdsignal(fp, file_size=file_size,\n                                              header_size=file_fields['header_size'],\n                                              n_sig=file_fields['n_sig'],\n                                              bit_width=file_fields['bit_width'],\n                                              is_signed=file_fields['is_signed'],\n                                              cut_end=cut_end)\n    return signal, fields, markers, triggers"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the header info of the windaq file and return the header info.", "response": "def _rdheader(fp):\n    \"\"\"\n    Read header info of the windaq file\n    \"\"\"\n    tag = None\n    # The '2' tag indicates the end of tags.\n    while tag != 2:\n        # For each header element, there is a tag indicating data type,\n        # followed by the data size, followed by the data itself. 0's\n        # pad the content to the nearest 4 bytes. If data_len=0, no pad.\n        tag = struct.unpack('>H', fp.read(2))[0]\n        data_size = struct.unpack('>H', fp.read(2))[0]\n        pad_len = (4 - (data_size % 4)) % 4\n        pos = fp.tell()\n        # Currently, most tags will be ignored...\n        # storage method\n        if tag == 1001:\n            storage_method = fs = struct.unpack('B', fp.read(1))[0]\n            storage_method = {0:'recording', 1:'manual', 2:'online'}[storage_method]\n        # fs, unit16\n        elif tag == 1003:\n            fs = struct.unpack('>H', fp.read(2))[0]\n        # sensor type\n        elif tag == 1007:\n            # Each byte contains information for one channel\n            n_sig = data_size\n            channel_data = struct.unpack('>%dB' % data_size, fp.read(data_size))\n            # The documentation states: \"0 : Channel is not used\"\n            # This means the samples are NOT saved.\n            channel_map = ((1, 1, 'emg'),\n                           (15, 30, 'goniometer'), (31, 46, 'accelerometer'),\n                           (47, 62, 'inclinometer'),\n                           (63, 78, 'polar_interface'), (79, 94, 'ecg'),\n                           (95, 110, 'torque'), (111, 126, 'gyrometer'),\n                           (127, 142, 'sensor'))\n            sig_name = []\n            # The number range that the data lies between gives the\n            # channel\n            for data in channel_data:\n                # Default case if byte value falls outside of channel map\n                base_name = 'unknown'\n                # Unused channel\n                if data == 0:\n                    n_sig -= 1\n                    break\n                for item in channel_map:\n                    if item[0] <= data <= item[1]:\n                        base_name = item[2]\n                        break\n                existing_count = [base_name in name for name in sig_name].count(True)\n                sig_name.append('%s_%d' % (base_name, existing_count))\n        # Display scale. Probably not useful.\n        elif tag == 1009:\n            # 100, 500, 1000, 2500, or 8500uV\n            display_scale = struct.unpack('>I', fp.read(4))[0]\n        # sample format, uint8\n        elif tag == 3:\n            sample_fmt = struct.unpack('B', fp.read(1))[0]\n            is_signed = bool(sample_fmt >> 7)\n            # ie. 8 or 16 bits\n            bit_width = sample_fmt & 127\n        # Measurement start time - seconds from 1.1.1970 UTC\n        elif tag == 101:\n            n_seconds = struct.unpack('>I', fp.read(4))[0]\n            base_datetime = datetime.datetime.utcfromtimestamp(n_seconds)\n            base_date = base_datetime.date()\n            base_time = base_datetime.time()\n        # Measurement start time - minutes from UTC\n        elif tag == 102:\n            n_minutes = struct.unpack('>h', fp.read(2))[0]\n        # Go to the next tag\n        fp.seek(pos + data_size + pad_len)\n    header_size = fp.tell()\n    # For interpreting the waveforms\n    fields = {'fs':fs, 'n_sig':n_sig, 'sig_name':sig_name,\n              'base_time':base_time, 'base_date':base_date}\n    # For reading the signal samples\n    file_fields = {'header_size':header_size, 'n_sig':n_sig,\n                   'bit_width':bit_width, 'is_signed':is_signed}\n    return fields, file_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the signal from the file and return the signal array.", "response": "def _rdsignal(fp, file_size, header_size, n_sig, bit_width, is_signed, cut_end):\n    \"\"\"\n    Read the signal\n\n    Parameters\n    ----------\n    cut_end : bool, optional\n        If True, enables reading the end of files which appear to terminate\n        with the incorrect number of samples (ie. sample not present for all channels),\n        by checking and skipping the reading the end of such files.\n        Checking this option makes reading slower.\n    \"\"\"\n    # Cannot initially figure out signal length because there\n    # are escape sequences.\n    fp.seek(header_size)\n    signal_size = file_size - header_size\n    byte_width = int(bit_width / 8)\n    # numpy dtype\n    dtype = str(byte_width)\n    if is_signed:\n        dtype = 'i' + dtype\n    else:\n        dtype = 'u' + dtype\n    # big endian\n    dtype = '>' + dtype\n    # The maximum possible samples given the file size\n    # All channels must be present\n    max_samples = int(signal_size / byte_width)\n    max_samples = max_samples - max_samples % n_sig\n    # Output information\n    signal = np.empty(max_samples, dtype=dtype)\n    markers = []\n    triggers = []\n    # Number of (total) samples read\n    sample_num = 0\n\n    # Read one sample for all channels at a time\n    if cut_end:\n        stop_byte = file_size - n_sig * byte_width + 1\n        while fp.tell() < stop_byte:\n            chunk = fp.read(2)\n            sample_num = _get_sample(fp, chunk, n_sig, dtype, signal, markers, triggers, sample_num)\n    else:\n        while True:\n            chunk = fp.read(2)\n            if not chunk:\n                break\n            sample_num = _get_sample(fp, chunk, n_sig, dtype, signal, markers, triggers, sample_num)\n\n    # No more bytes to read. Reshape output arguments.\n    signal = signal[:sample_num]\n    signal = signal.reshape((-1, n_sig))\n    markers = np.array(markers, dtype='int')\n    triggers = np.array(triggers, dtype='int')\n    return signal, markers, triggers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef xqrs_detect(sig, fs, sampfrom=0, sampto='end', conf=None,\n                learn=True, verbose=True):\n    \"\"\"\n    Run the 'xqrs' qrs detection algorithm on a signal. See the\n    docstring of the XQRS class for algorithm details.\n\n    Parameters\n    ----------\n    sig : numpy array\n        The input ecg signal to apply the qrs detection on.\n    fs : int or float\n        The sampling frequency of the input signal.\n    sampfrom : int, optional\n        The starting sample number to run the detection on.\n    sampto :\n        The final sample number to run the detection on. Set as 'end' to\n        run on the entire signal.\n    conf : XQRS.Conf object, optional\n        The configuration object specifying signal configuration\n        parameters. See the docstring of the XQRS.Conf class.\n    learn : bool, optional\n        Whether to apply learning on the signal before running the main\n        detection. If learning fails or is not conducted, the default\n        configuration parameters will be used to initialize these\n        variables.\n    verbose : bool, optional\n        Whether to display the stages and outcomes of the detection\n        process.\n\n    Returns\n    -------\n    qrs_inds : numpy array\n        The indices of the detected qrs complexes\n\n    Examples\n    --------\n    >>> import wfdb\n    >>> from wfdb import processing\n\n    >>> sig, fields = wfdb.rdsamp('sample-data/100', channels=[0])\n    >>> qrs_inds = processing.xqrs_detect(sig=sig[:,0], fs=fields['fs'])\n\n    \"\"\"\n    xqrs = XQRS(sig=sig, fs=fs, conf=conf)\n    xqrs.detect(sampfrom=sampfrom, sampto=sampto, verbose=verbose)\n    return xqrs.qrs_inds", "response": "Run the xqrs detection algorithm on a signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetect qrs locations in a single channel ecg. Functionally, a direct port of the gqrs algorithm from the original wfdb package. Accepts either a physical signal, or a digital signal with known adc_gain and adc_zero. See the notes below for a summary of the program. This algorithm is not being developed/supported. Parameters ---------- sig : 1d numpy array, optional The input physical signal. The detection algorithm which replicates the original, works using digital samples, and this physical option is provided as a convenient interface. If this is the specified input signal, automatic adc is performed using 24 bit precision, to obtain the `d_sig`, `adc_gain`, and `adc_zero` parameters. There may be minor differences in detection results (ie. an occasional 1 sample difference) between using `sig` and `d_sig`. To replicate the exact output of the original gqrs algorithm, use the `d_sig` argument instead. fs : int, or float The sampling frequency of the signal. d_sig : 1d numpy array, optional The input digital signal. If this is the specified input signal rather than `sig`, the `adc_gain` and `adc_zero` parameters must be specified. adc_gain : int, or float, optional The analogue to digital gain of the signal (the number of adus per physical unit). adc_zero: int, optional The value produced by the ADC given a 0 volt input. threshold : int, or float, optional The relative amplitude detection threshold. Used to initialize the peak and qrs detection threshold. hr : int, or float, optional Typical heart rate, in beats per minute. RRdelta : int or float, optional Typical difference between successive RR intervals in seconds. RRmin : int or float, optional Minimum RR interval (\"refractory period\"), in seconds. RRmax : int or float, optional Maximum RR interval, in seconds. Thresholds will be adjusted if no peaks are detected within this interval. QS : int or float, optional Typical QRS duration, in seconds. QT : int or float, optional Typical QT interval, in seconds. RTmin : int or float, optional Minimum interval between R and T peaks, in seconds. RTmax : int or float, optional Maximum interval between R and T peaks, in seconds. QRSa : int or float, optional Typical QRS peak-to-peak amplitude, in microvolts. QRSamin : int or float, optional Minimum QRS peak-to-peak amplitude, in microvolts. Returns ------- qrs_locs : numpy array Detected qrs locations Notes ----- This function should not be used for signals with fs <= 50Hz The algorithm theoretically works as follows: - Load in configuration parameters. They are used to set/initialize the: * allowed rr interval limits (fixed) * initial recent rr interval (running) * qrs width, used for detection filter widths (fixed) * allowed rt interval limits (fixed) * initial recent rt interval (running) * initial peak amplitude detection threshold (running) * initial qrs amplitude detection threshold (running) * `Note`: this algorithm does not normalize signal amplitudes, and hence is highly dependent on configuration amplitude parameters. - Apply trapezoid low-pass filtering to the signal - Convolve a QRS matched filter with the filtered signal - Run the learning phase using a calculated signal length: detect qrs and non-qrs peaks as in the main detection phase, without saving the qrs locations. During this phase, running parameters of recent intervals and peak/qrs thresholds are adjusted. - Run the detection:: if a sample is bigger than its immediate neighbors and larger than the peak detection threshold, it is a peak. if it is further than RRmin from the previous qrs, and is a *primary peak. if it is further than 2 standard deviations from the previous qrs, do a backsearch for a missed low amplitude beat return the primary peak between the current sample and the previous qrs if any. if it surpasses the qrs threshold, it is a qrs complex save the qrs location. update running rr and qrs amplitude parameters. look for the qrs complex's t-wave and mark it if found. else if it is not a peak lower the peak detection threshold if the last peak found was more than RRmax ago, and not already at its minimum. *A peak is secondary if there is a larger peak within its neighborhood (time +- rrmin), or if it has been identified as a T-wave associated with a previous primary peak. A peak is primary if it is largest in its neighborhood, or if the only larger peaks are secondary. The above describes how the algorithm should theoretically work, but there are bugs which make the program contradict certain parts of its supposed logic. A list of issues from the original c, code and hence this python implementation can be found here: https://github.com/bemoody/wfdb/issues/17 gqrs will not be supported/developed in this library. Examples -------- >>> import numpy as np >>> import wfdb >>> from wfdb import processing >>> # Detect using a physical input signal >>> record = wfdb.rdrecord('sample-data/100', channels=[0]) >>> qrs_locs = processing.gqrs_detect(record.p_signal[:,0], fs=record.fs) >>> # Detect using a digital input signal >>> record_2 = wfdb.rdrecord('sample-data/100', channels=[0], physical=False) >>> qrs_locs_2 = processing.gqrs_detect(d_sig=record_2.d_signal[:,0], fs=record_2.fs, adc_gain=record_2.adc_gain[0], adc_zero=record_2.adc_zero[0])", "response": "def gqrs_detect(sig=None, fs=None, d_sig=None, adc_gain=None, adc_zero=None,\n                threshold=1.0, hr=75, RRdelta=0.2, RRmin=0.28, RRmax=2.4,\n                QS=0.07, QT=0.35, RTmin=0.25, RTmax=0.33,\n                QRSa=750, QRSamin=130):\n    \"\"\"\n    Detect qrs locations in a single channel ecg. Functionally, a direct port\n    of the gqrs algorithm from the original wfdb package. Accepts either a\n    physical signal, or a digital signal with known adc_gain and adc_zero.\n\n    See the notes below for a summary of the program. This algorithm is not\n    being developed/supported.\n\n    Parameters\n    ----------\n    sig : 1d numpy array, optional\n        The input physical signal. The detection algorithm which replicates\n        the original, works using digital samples, and this physical option is\n        provided as a convenient interface. If this is the specified input\n        signal, automatic adc is performed using 24 bit precision, to obtain\n        the `d_sig`, `adc_gain`, and `adc_zero` parameters. There may be minor\n        differences in detection results (ie. an occasional 1 sample\n        difference) between using `sig` and `d_sig`. To replicate the exact\n        output of the original gqrs algorithm, use the `d_sig` argument\n        instead.\n    fs : int, or float\n        The sampling frequency of the signal.\n    d_sig : 1d numpy array, optional\n        The input digital signal. If this is the specified input signal rather\n        than `sig`, the `adc_gain` and `adc_zero` parameters must be specified.\n    adc_gain : int, or float, optional\n        The analogue to digital gain of the signal (the number of adus per\n        physical unit).\n    adc_zero: int, optional\n        The value produced by the ADC given a 0 volt input.\n    threshold : int, or float, optional\n        The relative amplitude detection threshold. Used to initialize the peak\n        and qrs detection threshold.\n    hr : int, or float, optional\n        Typical heart rate, in beats per minute.\n    RRdelta : int or float, optional\n        Typical difference between successive RR intervals in seconds.\n    RRmin : int or float, optional\n        Minimum RR interval (\"refractory period\"), in seconds.\n    RRmax : int or float, optional\n        Maximum RR interval, in seconds. Thresholds will be adjusted if no\n        peaks are detected within this interval.\n    QS : int or float, optional\n        Typical QRS duration, in seconds.\n    QT : int or float, optional\n        Typical QT interval, in seconds.\n    RTmin : int or float, optional\n        Minimum interval between R and T peaks, in seconds.\n    RTmax : int or float, optional\n        Maximum interval between R and T peaks, in seconds.\n    QRSa : int or float, optional\n        Typical QRS peak-to-peak amplitude, in microvolts.\n    QRSamin : int or float, optional\n        Minimum QRS peak-to-peak amplitude, in microvolts.\n\n    Returns\n    -------\n    qrs_locs : numpy array\n        Detected qrs locations\n\n\n    Notes\n    -----\n    This function should not be used for signals with fs <= 50Hz\n\n    The algorithm theoretically works as follows:\n\n    - Load in configuration parameters. They are used to set/initialize the:\n\n      * allowed rr interval limits (fixed)\n      * initial recent rr interval (running)\n      * qrs width, used for detection filter widths (fixed)\n      * allowed rt interval limits (fixed)\n      * initial recent rt interval (running)\n      * initial peak amplitude detection threshold (running)\n      * initial qrs amplitude detection threshold (running)\n      * `Note`: this algorithm does not normalize signal amplitudes, and\n        hence is highly dependent on configuration amplitude parameters.\n\n    - Apply trapezoid low-pass filtering to the signal\n    - Convolve a QRS matched filter with the filtered signal\n    - Run the learning phase using a calculated signal length: detect qrs and\n      non-qrs peaks as in the main detection phase, without saving the qrs\n      locations. During this phase, running parameters of recent intervals\n      and peak/qrs thresholds are adjusted.\n    - Run the detection::\n        if a sample is bigger than its immediate neighbors and larger\n        than the peak detection threshold, it is a peak.\n            if it is further than RRmin from the previous qrs, and is a\n            *primary peak.\n                if it is further than 2 standard deviations from the\n                previous qrs, do a backsearch for a missed low amplitude\n                beat\n                    return the primary peak between the current sample\n                    and the previous qrs if any.\n                if it surpasses the qrs threshold, it is a qrs complex\n                    save the qrs location.\n                    update running rr and qrs amplitude parameters.\n                    look for the qrs complex's t-wave and mark it if\n                    found.\n        else if it is not a peak\n            lower the peak detection threshold if the last peak found\n            was more than RRmax ago, and not already at its minimum.\n\n    *A peak is secondary if there is a larger peak within its neighborhood\n    (time +- rrmin), or if it has been identified as a T-wave associated with a\n    previous primary peak. A peak is primary if it is largest in its neighborhood,\n    or if the only larger peaks are secondary.\n\n    The above describes how the algorithm should theoretically work, but there\n    are bugs which make the program contradict certain parts of its supposed\n    logic. A list of issues from the original c, code and hence this python\n    implementation can be found here:\n\n    https://github.com/bemoody/wfdb/issues/17\n\n    gqrs will not be supported/developed in this library.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import wfdb\n    >>> from wfdb import processing\n\n    >>> # Detect using a physical input signal\n    >>> record = wfdb.rdrecord('sample-data/100', channels=[0])\n    >>> qrs_locs = processing.gqrs_detect(record.p_signal[:,0], fs=record.fs)\n\n    >>> # Detect using a digital input signal\n    >>> record_2 = wfdb.rdrecord('sample-data/100', channels=[0], physical=False)\n    >>> qrs_locs_2 = processing.gqrs_detect(d_sig=record_2.d_signal[:,0],\n                                            fs=record_2.fs,\n                                            adc_gain=record_2.adc_gain[0],\n                                            adc_zero=record_2.adc_zero[0])\n\n    \"\"\"\n    # Perform adc if input signal is physical\n    if sig is not None:\n        record = Record(p_signal=sig.reshape([-1,1]), fmt=['24'])\n        record.set_d_features(do_adc=True)\n        d_sig = record.d_signal[:,0]\n        adc_zero = 0\n        adc_gain = record.adc_gain[0]\n\n    conf = GQRS.Conf(fs=fs, adc_gain=adc_gain, hr=hr, RRdelta=RRdelta, RRmin=RRmin,\n                RRmax=RRmax, QS=QS, QT=QT, RTmin=RTmin, RTmax=RTmax, QRSa=QRSa,\n                QRSamin=QRSamin, thresh=threshold)\n    gqrs = GQRS()\n\n    annotations = gqrs.detect(x=d_sig, conf=conf, adc_zero=adc_zero)\n\n    return np.array([a.time for a in annotations])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_conf(self):\n        self.rr_init = 60 * self.fs / self.conf.hr_init\n        self.rr_max = 60 * self.fs / self.conf.hr_min\n        self.rr_min = 60 * self.fs / self.conf.hr_max\n\n        # Note: if qrs_width is odd, qrs_width == qrs_radius*2 + 1\n        self.qrs_width = int(self.conf.qrs_width * self.fs)\n        self.qrs_radius = int(self.conf.qrs_radius * self.fs)\n\n        self.qrs_thr_init = self.conf.qrs_thr_init\n        self.qrs_thr_min = self.conf.qrs_thr_min\n\n        self.ref_period = int(self.conf.ref_period * self.fs)\n        self.t_inspect_period = int(self.conf.t_inspect_period * self.fs)", "response": "Set the configuration parameters from the detector s conf object into the detector s base object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply a bandpass filter onto the signal and save the filtered signal.", "response": "def _bandpass(self, fc_low=5, fc_high=20):\n        \"\"\"\n        Apply a bandpass filter onto the signal, and save the filtered\n        signal.\n        \"\"\"\n        self.fc_low = fc_low\n        self.fc_high = fc_high\n\n        b, a = signal.butter(2, [float(fc_low) * 2 / self.fs,\n                                 float(fc_high) * 2 / self.fs], 'pass')\n        self.sig_f = signal.filtfilt(b, a, self.sig[self.sampfrom:self.sampto],\n                                     axis=0)\n        # Save the passband gain (x2 due to double filtering)\n        self.filter_gain = get_filter_gain(b, a, np.mean([fc_low, fc_high]),\n                                           self.fs) * 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _mwi(self):\n        wavelet_filter = signal.ricker(self.qrs_width, 4)\n\n        self.sig_i = signal.filtfilt(wavelet_filter, [1], self.sig_f,\n                                     axis=0) ** 2\n\n        # Save the mwi gain (x2 due to double filtering) and the total\n        # gain from raw to mwi\n        self.mwi_gain = get_filter_gain(wavelet_filter, [1],\n                         np.mean([self.fc_low, self.fc_high]), self.fs) * 2\n        self.transform_gain = self.filter_gain * self.mwi_gain\n        self.peak_inds_i = find_local_peaks(self.sig_i, radius=self.qrs_radius)\n        self.n_peaks_i = len(self.peak_inds_i)", "response": "Apply moving wave integration on the signal and save the square of the integrated signal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _learn_init_params(self, n_calib_beats=8):\n        if self.verbose:\n            print('Learning initial signal parameters...')\n\n        last_qrs_ind = -self.rr_max\n        qrs_inds = []\n        qrs_amps = []\n        noise_amps = []\n\n        ricker_wavelet = signal.ricker(self.qrs_radius * 2, 4).reshape(-1,1)\n\n        # Find the local peaks of the signal.\n        peak_inds_f = find_local_peaks(self.sig_f, self.qrs_radius)\n\n        # Peak numbers at least qrs_width away from signal boundaries\n        peak_nums_r = np.where(peak_inds_f > self.qrs_width)[0]\n        peak_nums_l = np.where(peak_inds_f <= self.sig_len - self.qrs_width)[0]\n\n        # Skip if no peaks in range\n        if (not peak_inds_f.size or not peak_nums_r.size\n                                 or not peak_nums_l.size):\n            if self.verbose:\n                print('Failed to find %d beats during learning.'\n                      % n_calib_beats)\n            self._set_default_init_params()\n            return\n\n        # Go through the peaks and find qrs peaks and noise peaks.\n        # only inspect peaks with at least qrs_radius around either side\n        for peak_num in range(peak_nums_r[0], peak_nums_l[-1]):\n            i = peak_inds_f[peak_num]\n            # Calculate cross-correlation between the filtered signal\n            # segment and a ricker wavelet\n\n            # Question: should the signal be squared? Case for inverse qrs\n            # complexes\n            sig_segment = normalize((self.sig_f[i - self.qrs_radius:\n                                                i + self.qrs_radius]).reshape(-1, 1), axis=0)\n\n            xcorr = np.correlate(sig_segment[:, 0], ricker_wavelet[:,0])\n\n            # Classify as qrs if xcorr is large enough\n            if xcorr > 0.6 and i-last_qrs_ind > self.rr_min:\n                last_qrs_ind = i\n                qrs_inds.append(i)\n                qrs_amps.append(self.sig_i[i])\n            else:\n                noise_amps.append(self.sig_i[i])\n\n            if len(qrs_inds) == n_calib_beats:\n                break\n\n        # Found enough calibration beats to initialize parameters\n        if len(qrs_inds) == n_calib_beats:\n\n            if self.verbose:\n                print('Found %d beats during learning.' % n_calib_beats\n                      + ' Initializing using learned parameters')\n\n            # QRS amplitude is most important.\n            qrs_amp = np.mean(qrs_amps)\n\n            # Set noise amplitude if found\n            if noise_amps:\n                noise_amp = np.mean(noise_amps)\n            else:\n                # Set default of 1/10 of qrs amplitude\n                noise_amp = qrs_amp / 10\n\n            # Get rr intervals of consecutive beats, if any.\n            rr_intervals = np.diff(qrs_inds)\n            rr_intervals = rr_intervals[rr_intervals < self.rr_max]\n            if rr_intervals.any():\n                rr_recent = np.mean(rr_intervals)\n            else:\n                rr_recent = self.rr_init\n\n            # If an early qrs was detected, set last_qrs_ind so that it can be\n            # picked up.\n            last_qrs_ind = min(0, qrs_inds[0] - self.rr_min - 1)\n\n            self._set_init_params(qrs_amp_recent=qrs_amp,\n                                  noise_amp_recent=noise_amp,\n                                  rr_recent=rr_recent,\n                                  last_qrs_ind=last_qrs_ind)\n            self.learned_init_params = True\n\n        # Failed to find enough calibration beats. Use default values.\n        else:\n            if self.verbose:\n                print('Failed to find %d beats during learning.'\n                      % n_calib_beats)\n\n            self._set_default_init_params()", "response": "This function learns the initial parameters of the local maxima of the signal and uses them to initialize the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_init_params(self, qrs_amp_recent, noise_amp_recent, rr_recent,\n                         last_qrs_ind):\n        \"\"\"\n        Set initial online parameters\n        \"\"\"\n        self.qrs_amp_recent = qrs_amp_recent\n        self.noise_amp_recent = noise_amp_recent\n        # What happens if qrs_thr is calculated to be less than the explicit\n        # min threshold? Should print warning?\n        self.qrs_thr = max(0.25*self.qrs_amp_recent\n                           + 0.75*self.noise_amp_recent,\n                           self.qrs_thr_min * self.transform_gain)\n        self.rr_recent = rr_recent\n        self.last_qrs_ind = last_qrs_ind\n\n        # No qrs detected initially\n        self.last_qrs_peak_num = None", "response": "Set initial online parameters for the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_default_init_params(self):\n        if self.verbose:\n            print('Initializing using default parameters')\n        # Multiply the specified ecg thresholds by the filter and mwi gain\n        # factors\n        qrs_thr_init = self.qrs_thr_init * self.transform_gain\n        qrs_thr_min = self.qrs_thr_min * self.transform_gain\n\n        qrs_amp = 27/40 * qrs_thr_init\n        noise_amp = qrs_amp / 10\n        rr_recent = self.rr_init\n        last_qrs_ind = 0\n\n        self._set_init_params(qrs_amp_recent=qrs_amp,\n                              noise_amp_recent=noise_amp,\n                              rr_recent=rr_recent,\n                              last_qrs_ind=last_qrs_ind)\n\n        self.learned_init_params = False", "response": "Set initial running parameters using default values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether a peak is a qrs complex.", "response": "def _is_qrs(self, peak_num, backsearch=False):\n        \"\"\"\n        Check whether a peak is a qrs complex. It is classified as qrs\n        if it:\n        - Comes after the refractory period\n        - Passes qrs threshold\n        - Is not a t-wave (check it if the peak is close to the previous\n          qrs).\n\n        Parameters\n        ----------\n        peak_num : int\n            The peak number of the mwi signal to be inspected\n        backsearch: bool, optional\n            Whether the peak is being inspected during backsearch\n\n        \"\"\"\n        i = self.peak_inds_i[peak_num]\n        if backsearch:\n            qrs_thr = self.qrs_thr / 2\n        else:\n            qrs_thr = self.qrs_thr\n\n        if (i-self.last_qrs_ind > self.ref_period\n           and self.sig_i[i] > qrs_thr):\n            if i-self.last_qrs_ind < self.t_inspect_period:\n                if self._is_twave(peak_num):\n                    return False\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the qrs parameters and the qrs threshold for the specified peak.", "response": "def _update_qrs(self, peak_num, backsearch=False):\n        \"\"\"\n        Update live qrs parameters. Adjust the recent rr-intervals and\n        qrs amplitudes, and the qrs threshold.\n\n        Parameters\n        ----------\n        peak_num : int\n            The peak number of the mwi signal where the qrs is detected\n        backsearch: bool, optional\n            Whether the qrs was found via backsearch\n        \"\"\"\n\n        i = self.peak_inds_i[peak_num]\n\n        # Update recent rr if the beat is consecutive (do this before\n        # updating self.last_qrs_ind)\n        rr_new = i - self.last_qrs_ind\n        if rr_new < self.rr_max:\n            self.rr_recent = 0.875*self.rr_recent + 0.125*rr_new\n\n        self.qrs_inds.append(i)\n        self.last_qrs_ind = i\n        # Peak number corresponding to last qrs\n        self.last_qrs_peak_num = self.peak_num\n\n        # qrs recent amplitude is adjusted twice as quickly if the peak\n        # was found via backsearch\n        if backsearch:\n            self.backsearch_qrs_inds.append(i)\n            self.qrs_amp_recent = (0.75*self.qrs_amp_recent\n                                   + 0.25*self.sig_i[i])\n        else:\n            self.qrs_amp_recent = (0.875*self.qrs_amp_recent\n                                   + 0.125*self.sig_i[i])\n\n        self.qrs_thr = max((0.25*self.qrs_amp_recent\n                            + 0.75*self.noise_amp_recent), self.qrs_thr_min)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking whether a segment is a t - wave.", "response": "def _is_twave(self, peak_num):\n        \"\"\"\n        Check whether a segment is a t-wave. Compare the maximum gradient of\n        the filtered signal segment with that of the previous qrs segment.\n\n        Parameters\n        ----------\n        peak_num : int\n            The peak number of the mwi signal where the qrs is detected\n\n        \"\"\"\n        i = self.peak_inds_i[peak_num]\n\n        # Due to initialization parameters, last_qrs_ind may be negative.\n        # No way to check in this instance.\n        if self.last_qrs_ind - self.qrs_radius < 0:\n            return False\n\n        # Get half the qrs width of the signal to the left.\n        # Should this be squared?\n        sig_segment = normalize((self.sig_f[i - self.qrs_radius:i]\n                                 ).reshape(-1, 1), axis=0)\n        last_qrs_segment = self.sig_f[self.last_qrs_ind - self.qrs_radius:\n                                      self.last_qrs_ind]\n\n        segment_slope = np.diff(sig_segment)\n        last_qrs_slope = np.diff(last_qrs_segment)\n\n        # Should we be using absolute values?\n        if max(segment_slope) < 0.5*max(abs(last_qrs_slope)):\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates live noise parameters", "response": "def _update_noise(self, peak_num):\n        \"\"\"\n        Update live noise parameters\n        \"\"\"\n        i = self.peak_inds_i[peak_num]\n        self.noise_amp_recent = (0.875*self.noise_amp_recent\n                                 + 0.125*self.sig_i[i])\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _require_backsearch(self):\n        if self.peak_num == self.n_peaks_i-1:\n            # If we just return false, we may miss a chance to backsearch.\n            # Update this?\n            return False\n\n        next_peak_ind = self.peak_inds_i[self.peak_num + 1]\n\n        if next_peak_ind-self.last_qrs_ind > self.rr_recent*1.66:\n            return True\n        else:\n            return False", "response": "Determine whether a backsearch should be performed on prior peaks\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _backsearch(self):\n        if self.last_qrs_peak_num is not None:\n            for peak_num in range(self.last_qrs_peak_num + 1, self.peak_num + 1):\n                if self._is_qrs(peak_num=peak_num, backsearch=True):\n                    self._update_qrs(peak_num=peak_num, backsearch=True)", "response": "Inspect previous peaks from the last detected qrs peak and update the QRS if necessary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_detection(self):\n        if self.verbose:\n            print('Running QRS detection...')\n\n        # Detected qrs indices\n        self.qrs_inds = []\n        # qrs indices found via backsearch\n        self.backsearch_qrs_inds = []\n\n        # Iterate through mwi signal peak indices\n        for self.peak_num in range(self.n_peaks_i):\n            if self._is_qrs(self.peak_num):\n                self._update_qrs(self.peak_num)\n            else:\n                self._update_noise(self.peak_num)\n\n            # Before continuing to the next peak, do backsearch if\n            # necessary\n            if self._require_backsearch():\n                self._backsearch()\n\n        # Detected indices are relative to starting sample\n        if self.qrs_inds:\n            self.qrs_inds = np.array(self.qrs_inds) + self.sampfrom\n        else:\n            self.qrs_inds = np.array(self.qrs_inds)\n\n        if self.verbose:\n            print('QRS detection complete.')", "response": "Run the qrs detection after all signals and parameters have been set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detect(self, sampfrom=0, sampto='end', learn=True, verbose=True):\n        if sampfrom < 0:\n            raise ValueError(\"'sampfrom' cannot be negative\")\n        self.sampfrom = sampfrom\n\n        if sampto == 'end':\n            sampto = self.sig_len\n        elif sampto > self.sig_len:\n            raise ValueError(\"'sampto' cannot exceed the signal length\")\n        self.sampto = sampto\n        self.verbose = verbose\n\n        # Don't attempt to run on a flat signal\n        if np.max(self.sig) == np.min(self.sig):\n            self.qrs_inds = np.empty(0)\n            if self.verbose:\n                print('Flat signal. Detection skipped.')\n            return\n\n        # Get/set signal configuration fields from Conf object\n        self._set_conf()\n        # Bandpass filter the signal\n        self._bandpass()\n        # Compute moving wave integration of filtered signal\n        self._mwi()\n\n        # Initialize the running parameters\n        if learn:\n            self._learn_init_params()\n        else:\n            self._set_default_init_params()\n\n        # Run the detection\n        self._run_detection()", "response": "Detect qrs locations between two samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detect(self, x, conf, adc_zero):\n        self.c = conf\n        self.annotations = []\n        self.sample_valid = False\n\n        if len(x) < 1:\n            return []\n\n        self.x = x\n        self.adc_zero = adc_zero\n\n        self.qfv = np.zeros((self.c._BUFLN), dtype=\"int64\")\n        self.smv = np.zeros((self.c._BUFLN), dtype=\"int64\")\n        self.v1 = 0\n\n        t0 = 0\n        self.tf = len(x) - 1\n        self.t = 0 - self.c.dt4\n\n        self.annot = GQRS.Annotation(0, \"NOTE\", 0, 0)\n\n        # Cicular buffer of Peaks\n        first_peak = GQRS.Peak(0, 0, 0)\n        tmp = first_peak\n        for _ in range(1, self.c._NPEAKS):\n            tmp.next_peak = GQRS.Peak(0, 0, 0)\n            tmp.next_peak.prev_peak = tmp\n            tmp = tmp.next_peak\n        tmp.next_peak = first_peak\n        first_peak.prev_peak = tmp\n        self.current_peak = first_peak\n\n        if self.c.spm > self.c._BUFLN:\n            if self.tf - t0 > self.c._BUFLN:\n                tf_learn = t0 + self.c._BUFLN - self.c.dt4\n            else:\n                tf_learn = self.tf - self.c.dt4\n        else:\n            if self.tf - t0 > self.c.spm:\n                tf_learn = t0 + self.c.spm - self.c.dt4\n            else:\n                tf_learn = self.tf - self.c.dt4\n\n        self.countdown = -1\n        self.state = \"LEARNING\"\n        self.gqrs(t0, tf_learn)\n\n        self.rewind_gqrs()\n\n        self.state = \"RUNNING\"\n        self.t = t0 - self.c.dt4\n        self.gqrs(t0, self.tf)\n\n        return self.annotations", "response": "Run detection. x is digital signal xconf adc_zero is boolean indicating whether the sample is valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing instantaneous heart rate from peak indices.", "response": "def compute_hr(sig_len, qrs_inds, fs):\n    \"\"\"\n    Compute instantaneous heart rate from peak indices.\n\n    Parameters\n    ----------\n    sig_len : int\n        The length of the corresponding signal\n    qrs_inds : numpy array\n        The qrs index locations\n    fs : int, or float\n        The corresponding signal's sampling frequency.\n\n    Returns\n    -------\n    heart_rate : numpy array\n        An array of the instantaneous heart rate, with the length of the\n        corresponding signal. Contains numpy.nan where heart rate could\n        not be computed.\n\n    \"\"\"\n    heart_rate = np.full(sig_len, np.nan, dtype='float32')\n\n    if len(qrs_inds) < 2:\n        return heart_rate\n\n    for i in range(0, len(qrs_inds)-2):\n        a = qrs_inds[i]\n        b = qrs_inds[i+1]\n        c = qrs_inds[i+2]\n        rr = (b-a) * (1.0 / fs) * 1000\n        hr = 60000.0 / rr\n        heart_rate[b+1:c+1] = hr\n\n    heart_rate[qrs_inds[-1]:] = heart_rate[qrs_inds[-1]]\n\n    return heart_rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_rr(qrs_locs, fs=None, min_rr=None, max_rr=None, qrs_units='samples',\n            rr_units='samples'):\n    \"\"\"\n    Compute rr intervals from qrs indices by extracting the time\n    differences.\n\n    Parameters\n    ----------\n    qrs_locs : numpy array\n        1d array of qrs locations.\n    fs : float, optional\n        Sampling frequency of the original signal. Needed if\n        `qrs_units` does not match `rr_units`.\n    min_rr : float, optional\n        The minimum allowed rr interval. Values below this are excluded\n        from the returned rr intervals. Units are in `rr_units`.\n    max_rr : float, optional\n        The maximum allowed rr interval. Values above this are excluded\n        from the returned rr intervals. Units are in `rr_units`.\n    qrs_units : str, optional\n        The time unit of `qrs_locs`. Must be one of: 'samples',\n        'seconds'.\n    rr_units : str, optional\n        The desired time unit of the returned rr intervals in. Must be\n        one of: 'samples', 'seconds'.\n\n    Returns\n    -------\n    rr : numpy array\n        Array of rr intervals.\n\n    \"\"\"\n    rr = np.diff(qrs_locs)\n\n    # Empty input qrs_locs\n    if not len(rr):\n        return rr\n\n    # Convert to desired output rr units if needed\n    if qrs_units == 'samples' and rr_units == 'seconds':\n        rr = rr / fs\n    elif qrs_units == 'seconds' and rr_units == 'samples':\n        rr = rr * fs\n\n    # Apply rr interval filters\n    if min_rr is not None:\n        rr = rr[rr > min_rr]\n\n    if max_rr is not None:\n        rr = rr[rr < max_rr]\n\n    return rr", "response": "Compute the rr intervals from qrs locations by extracting the time series differences."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the mean heart rate in beats per minute from a set of rr intervals.", "response": "def calc_mean_hr(rr, fs=None, min_rr=None, max_rr=None, rr_units='samples'):\n    \"\"\"\n    Compute mean heart rate in beats per minute, from a set of rr\n    intervals. Returns 0 if rr is empty.\n\n    Parameters\n    ----------\n    rr : numpy array\n        Array of rr intervals.\n    fs : int, or float\n        The corresponding signal's sampling frequency. Required if\n        'input_time_units' == 'samples'.\n    min_rr : float, optional\n        The minimum allowed rr interval. Values below this are excluded\n        when calculating the heart rate. Units are in `rr_units`.\n    max_rr : float, optional\n        The maximum allowed rr interval. Values above this are excluded\n        when calculating the heart rate. Units are in `rr_units`.\n    rr_units : str, optional\n        The time units of the input rr intervals. Must be one of:\n        'samples', 'seconds'.\n\n    Returns\n    -------\n    mean_hr : float\n        The mean heart rate in beats per minute\n\n    \"\"\"\n    if not len(rr):\n        return 0\n\n    if min_rr is not None:\n        rr = rr[rr > min_rr]\n\n    if max_rr is not None:\n        rr = rr[rr < max_rr]\n\n    mean_rr = np.mean(rr)\n\n    mean_hr = 60 / mean_rr\n\n    # Convert to bpm\n    if rr_units == 'samples':\n        mean_hr = mean_hr * fs\n\n    return mean_hr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare_annotations(ref_sample, test_sample, window_width, signal=None):\n    comparitor = Comparitor(ref_sample=ref_sample, test_sample=test_sample,\n                            window_width=window_width, signal=signal)\n    comparitor.compare()\n\n    return comparitor", "response": "Compare a set of reference annotation locations against a set of test annotation locations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbenchmark a qrs detector against mitdb s records.", "response": "def benchmark_mitdb(detector, verbose=False, print_results=False):\n    \"\"\"\n    Benchmark a qrs detector against mitdb's records.\n\n    Parameters\n    ----------\n    detector : function\n        The detector function.\n    verbose : bool, optional\n        The verbose option of the detector function.\n    print_results : bool, optional\n        Whether to print the overall performance, and the results for\n        each record.\n\n    Returns\n    -------\n    comparitors : dictionary\n        Dictionary of Comparitor objects run on the records, keyed on\n        the record names.\n    specificity : float\n        Aggregate specificity.\n    positive_predictivity : float\n        Aggregate positive_predictivity.\n    false_positive_rate : float\n        Aggregate false_positive_rate.\n\n    Notes\n    -----\n    TODO:\n    - remove non-qrs detections from reference annotations\n    - allow kwargs\n\n    Examples\n    --------\n    >>> import wfdb\n    >> from wfdb.processing import benchmark_mitdb, xqrs_detect\n\n    >>> comparitors, spec, pp, fpr = benchmark_mitdb(xqrs_detect)\n\n    \"\"\"\n    record_list = get_record_list('mitdb')\n    n_records = len(record_list)\n\n    # Function arguments for starmap\n    args = zip(record_list, n_records * [detector], n_records * [verbose])\n\n    # Run detector and compare against reference annotations for all\n    # records\n    with Pool(cpu_count() - 1) as p:\n        comparitors = p.starmap(benchmark_mitdb_record, args)\n\n    # Calculate aggregate stats\n    specificity = np.mean([c.specificity for c in comparitors])\n    positive_predictivity = np.mean(\n        [c.positive_predictivity for c in comparitors])\n    false_positive_rate = np.mean(\n        [c.false_positive_rate for c in comparitors])\n\n    comparitors = dict(zip(record_list, comparitors))\n\n    print('Benchmark complete')\n\n    if print_results:\n        print('\\nOverall MITDB Performance - Specificity: %.4f, Positive Predictivity: %.4f, False Positive Rate: %.4f\\n'\n              % (specificity, positive_predictivity, false_positive_rate))\n        for record_name in record_list:\n            print('Record %s:' % record_name)\n            comparitors[record_name].print_summary()\n            print('\\n\\n')\n\n    return comparitors, specificity, positive_predictivity, false_positive_rate"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef benchmark_mitdb_record(rec, detector, verbose):\n    sig, fields = rdsamp(rec, pb_dir='mitdb', channels=[0])\n    ann_ref = rdann(rec, pb_dir='mitdb', extension='atr')\n\n    qrs_inds = detector(sig=sig[:,0], fs=fields['fs'], verbose=verbose)\n\n    comparitor = compare_annotations(ref_sample=ann_ref.sample[1:],\n                                     test_sample=qrs_inds,\n                                     window_width=int(0.1 * fields['fs']))\n    if verbose:\n        print('Finished record %s' % rec)\n    return comparitor", "response": "Benchmark a single mitdb record"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _calc_stats(self):\n        # Reference annotation indices that were detected\n        self.matched_ref_inds = np.where(self.matching_sample_nums != -1)[0]\n        # Reference annotation indices that were missed\n        self.unmatched_ref_inds = np.where(self.matching_sample_nums == -1)[0]\n        # Test annotation indices that were matched to a reference annotation\n        self.matched_test_inds = self.matching_sample_nums[\n            self.matching_sample_nums != -1]\n        # Test annotation indices that were unmatched to a reference annotation\n        self.unmatched_test_inds = np.setdiff1d(np.array(range(self.n_test)),\n            self.matched_test_inds, assume_unique=True)\n\n        # Sample numbers that were matched and unmatched\n        self.matched_ref_sample = self.ref_sample[self.matched_ref_inds]\n        self.unmatched_ref_sample = self.ref_sample[self.unmatched_ref_inds]\n        self.matched_test_sample = self.test_sample[self.matched_test_inds]\n        self.unmatched_test_sample = self.test_sample[self.unmatched_test_inds]\n\n        # True positives = matched reference samples\n        self.tp = len(self.matched_ref_inds)\n        # False positives = extra test samples not matched\n        self.fp = self.n_test - self.tp\n        # False negatives = undetected reference samples\n        self.fn = self.n_ref - self.tp\n        # No tn attribute\n\n        self.specificity = float(self.tp) / self.n_ref\n        self.positive_predictivity = float(self.tp) / self.n_test\n        self.false_positive_rate = float(self.fp) / self.n_test", "response": "Calculates the performance statistics of the current set of annotations and returns a dictionary of the values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_closest_samp_num(self, ref_samp_num, start_test_samp_num):\n\n        if start_test_samp_num >= self.n_test:\n            raise ValueError('Invalid starting test sample number.')\n\n        ref_samp = self.ref_sample[ref_samp_num]\n        test_samp = self.test_sample[start_test_samp_num]\n        samp_diff = ref_samp - test_samp\n\n        # Initialize running parameters\n        closest_samp_num = start_test_samp_num\n        smallest_samp_diff = abs(samp_diff)\n\n        # Iterate through the testing samples\n        for test_samp_num in range(start_test_samp_num, self.n_test):\n            test_samp = self.test_sample[test_samp_num]\n            samp_diff = ref_samp - test_samp\n            abs_samp_diff = abs(samp_diff)\n\n            # Found a better match\n            if abs_samp_diff < smallest_samp_diff:\n                closest_samp_num = test_samp_num\n                smallest_samp_diff = abs_samp_diff\n\n            # Stop iterating when the ref sample is first passed or reached\n            if samp_diff <= 0:\n                break\n\n        return closest_samp_num, smallest_samp_diff", "response": "Return the closest testing sample number for the given reference sample number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints summary metrics of the annotation comparisons.", "response": "def print_summary(self):\n        \"\"\"\n        Print summary metrics of the annotation comparisons.\n        \"\"\"\n        # True positives = matched reference samples\n        self.tp = len(self.matched_ref_inds)\n        # False positives = extra test samples not matched\n        self.fp = self.n_test - self.tp\n        # False negatives = undetected reference samples\n        self.fn = self.n_ref - self.tp\n        # No tn attribute\n\n        self.specificity = self.tp / self.n_ref\n        self.positive_predictivity = self.tp / self.n_test\n        self.false_positive_rate = self.fp / self.n_test\n\n        print('%d reference annotations, %d test annotations\\n'\n            % (self.n_ref, self.n_test))\n        print('True Positives (matched samples): %d' % self.tp)\n        print('False Positives (unmatched test samples: %d' % self.fp)\n        print('False Negatives (unmatched reference samples): %d\\n' % self.fn)\n\n        print('Specificity: %.4f (%d/%d)'\n            % (self.specificity, self.tp, self.n_ref))\n        print('Positive Predictivity: %.4f (%d/%d)'\n            % (self.positive_predictivity, self.tp, self.n_test))\n        print('False Positive Rate: %.4f (%d/%d)'\n            % (self.false_positive_rate, self.fp, self.n_test))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot(self, sig_style='', title=None, figsize=None,\n             return_fig=False):\n        \"\"\"\n        Plot the comparison of two sets of annotations, possibly\n        overlaid on their original signal.\n\n        Parameters\n        ----------\n        sig_style : str, optional\n            The matplotlib style of the signal\n        title : str, optional\n            The title of the plot\n        figsize: tuple, optional\n            Tuple pair specifying the width, and height of the figure.\n            It is the'figsize' argument passed into matplotlib.pyplot's\n            `figure` function.\n        return_fig : bool, optional\n            Whether the figure is to be returned as an output argument.\n\n        \"\"\"\n        fig = plt.figure(figsize=figsize)\n        ax = fig.add_subplot(1, 1, 1)\n\n        legend = ['Signal',\n                  'Matched Reference Annotations (%d/%d)' % (self.tp, self.n_ref),\n                  'Unmatched Reference Annotations (%d/%d)' % (self.fn, self.n_ref),\n                  'Matched Test Annotations (%d/%d)' % (self.tp, self.n_test),\n                  'Unmatched Test Annotations (%d/%d)' % (self.fp, self.n_test)\n        ]\n\n        # Plot the signal if any\n        if self.signal is not None:\n            ax.plot(self.signal, sig_style)\n\n            # Plot reference annotations\n            ax.plot(self.matched_ref_sample,\n                    self.signal[self.matched_ref_sample], 'ko')\n            ax.plot(self.unmatched_ref_sample,\n                    self.signal[self.unmatched_ref_sample], 'ko',\n                    fillstyle='none')\n            # Plot test annotations\n            ax.plot(self.matched_test_sample,\n                    self.signal[self.matched_test_sample], 'g+')\n            ax.plot(self.unmatched_test_sample,\n                    self.signal[self.unmatched_test_sample], 'rx')\n\n            ax.legend(legend)\n\n        # Just plot annotations\n        else:\n           # Plot reference annotations\n            ax.plot(self.matched_ref_sample, np.ones(self.tp), 'ko')\n            ax.plot(self.unmatched_ref_sample, np.ones(self.fn), 'ko',\n                fillstyle='none')\n            # Plot test annotations\n            ax.plot(self.matched_test_sample, 0.5 * np.ones(self.tp), 'g+')\n            ax.plot(self.unmatched_test_sample, 0.5 * np.ones(self.fp), 'rx')\n            ax.legend(legend[1:])\n\n        if title:\n            ax.set_title(title)\n\n        ax.set_xlabel('time/sample')\n\n        fig.show()\n\n        if return_fig:\n            return fig, ax", "response": "Plots the comparison of two sets of annotations overlaid on their original signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a MCJIT ExecutionEngine from the given module and target_machine.", "response": "def create_mcjit_compiler(module, target_machine):\n    \"\"\"\n    Create a MCJIT ExecutionEngine from the given *module* and\n    *target_machine*.\n    \"\"\"\n    with ffi.OutputString() as outerr:\n        engine = ffi.lib.LLVMPY_CreateMCJITCompiler(\n                module, target_machine, outerr)\n        if not engine:\n            raise RuntimeError(str(outerr))\n\n    target_machine._owned = True\n    return ExecutionEngine(engine, module=module)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a module to the execution engine.", "response": "def add_module(self, module):\n        \"\"\"\n        Ownership of module is transferred to the execution engine\n        \"\"\"\n        if module in self._modules:\n            raise KeyError(\"module already added to this engine\")\n        ffi.lib.LLVMPY_AddModule(self, module)\n        module._owned = True\n        self._modules.add(module)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_module(self, module):\n        with ffi.OutputString() as outerr:\n            if ffi.lib.LLVMPY_RemoveModule(self, module, outerr):\n                raise RuntimeError(str(outerr))\n        self._modules.remove(module)\n        module._owned = False", "response": "Removes a module from the list of modules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef target_data(self):\n        if self._td is not None:\n            return self._td\n        ptr = ffi.lib.LLVMPY_GetExecutionEngineTargetData(self)\n        self._td = targets.TargetData(ptr)\n        self._td._owned = True\n        return self._td", "response": "Returns the TargetData for this execution engine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_module_ptr(self, module_ptr):\n        ptr = cast(module_ptr, c_void_p).value\n        for module in self._modules:\n            if cast(module._ptr, c_void_p).value == ptr:\n                return module\n        return None", "response": "Find the ModuleRef corresponding to the given pointer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_object_file(self, obj_file):\n        if isinstance(obj_file, str):\n            obj_file = object_file.ObjectFileRef.from_path(obj_file)\n\n        ffi.lib.LLVMPY_MCJITAddObjectFile(self, obj_file)", "response": "Add an object file to the jit."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the object cache callbacks to the given Python functions.", "response": "def set_object_cache(self, notify_func=None, getbuffer_func=None):\n        \"\"\"\n        Set the object cache \"notifyObjectCompiled\" and \"getBuffer\"\n        callbacks to the given Python functions.\n        \"\"\"\n        self._object_cache_notify = notify_func\n        self._object_cache_getbuffer = getbuffer_func\n        # Lifetime of the object cache is managed by us.\n        self._object_cache = _ObjectCacheRef(self)\n        # Note this doesn't keep a reference to self, to avoid reference\n        # cycles.\n        ffi.lib.LLVMPY_SetObjectCache(self, self._object_cache)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _raw_object_cache_notify(self, data):\n        if self._object_cache_notify is None:\n            return\n        module_ptr = data.contents.module_ptr\n        buf_ptr = data.contents.buf_ptr\n        buf_len = data.contents.buf_len\n        buf = string_at(buf_ptr, buf_len)\n        module = self._find_module_ptr(module_ptr)\n        if module is None:\n            # The LLVM EE should only give notifications for modules\n            # known by us.\n            raise RuntimeError(\"object compilation notification \"\n                               \"for unknown module %s\" % (module_ptr,))\n        self._object_cache_notify(module, buf)", "response": "Internal method to call the object cache notification hook."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _raw_object_cache_getbuffer(self, data):\n        if self._object_cache_getbuffer is None:\n            return\n        module_ptr = data.contents.module_ptr\n        module = self._find_module_ptr(module_ptr)\n        if module is None:\n            # The LLVM EE should only give notifications for modules\n            # known by us.\n            raise RuntimeError(\"object compilation notification \"\n                               \"for unknown module %s\" % (module_ptr,))\n\n        buf = self._object_cache_getbuffer(module)\n        if buf is not None:\n            # Create a copy, which will be freed by the caller\n            data[0].buf_ptr = ffi.lib.LLVMPY_CreateByteString(buf, len(buf))\n            data[0].buf_len = len(buf)", "response": "Internal method for object cache getbuffer hook."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a truncated version of the given label + suffix.", "response": "def _label_suffix(label, suffix):\n    \"\"\"Returns (label + suffix) or a truncated version if it's too long.\n    Parameters\n    ----------\n    label : str\n        Label name\n    suffix : str\n        Label suffix\n    \"\"\"\n    if len(label) > 50:\n        nhead = 25\n        return ''.join([label[:nhead], '..', suffix])\n    else:\n        return label + suffix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef position_before(self, instr):\n        self._block = instr.parent\n        self._anchor = self._block.instructions.index(instr)", "response": "Position immediately before the given instruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npositions immediately after the given instruction.", "response": "def position_after(self, instr):\n        \"\"\"\n        Position immediately after the given instruction.  The current block\n        is also changed to the instruction's basic block.\n        \"\"\"\n        self._block = instr.parent\n        self._anchor = self._block.instructions.index(instr) + 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npositioning at the end of the basic block.", "response": "def position_at_end(self, block):\n        \"\"\"\n        Position at the end of the basic *block*.\n        \"\"\"\n        self._block = block\n        self._anchor = len(block.instructions)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef if_then(self, pred, likely=None):\n        bb = self.basic_block\n        bbif = self.append_basic_block(name=_label_suffix(bb.name, '.if'))\n        bbend = self.append_basic_block(name=_label_suffix(bb.name, '.endif'))\n        br = self.cbranch(pred, bbif, bbend)\n        if likely is not None:\n            br.set_weights([99, 1] if likely else [1, 99])\n\n        with self._branch_helper(bbif, bbend):\n            yield bbend\n\n        self.position_at_end(bbend)", "response": "A context manager which sets up a conditional basic block based\n        on the predicate pred."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef if_else(self, pred, likely=None):\n        bb = self.basic_block\n        bbif = self.append_basic_block(name=_label_suffix(bb.name, '.if'))\n        bbelse = self.append_basic_block(name=_label_suffix(bb.name, '.else'))\n        bbend = self.append_basic_block(name=_label_suffix(bb.name, '.endif'))\n        br = self.cbranch(pred, bbif, bbelse)\n        if likely is not None:\n            br.set_weights([99, 1] if likely else [1, 99])\n\n        then = self._branch_helper(bbif, bbend)\n        otherwise = self._branch_helper(bbelse, bbend)\n\n        yield then, otherwise\n\n        self.position_at_end(bbend)", "response": "A generator that yields the first and last conditional context manager based on the predicate pred."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef not_(self, value, name=''):\n        if isinstance(value.type, types.VectorType):\n            rhs = values.Constant(value.type, (-1,) * value.type.count)\n        else:\n            rhs = values.Constant(value.type, -1)\n        return self.xor(value, rhs, name=name)", "response": "Bitwise integer complement:\n            name = ~value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef neg(self, value, name=''):\n        return self.sub(values.Constant(value.type, 0), value, name=name)", "response": "negative is a wrapper for subtract"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsign integer comparison: name = lhs <cmpop> rhs where cmpop can be '==', '!=', '<', '<=', '>', '>='", "response": "def icmp_signed(self, cmpop, lhs, rhs, name=''):\n        \"\"\"\n        Signed integer comparison:\n            name = lhs <cmpop> rhs\n\n        where cmpop can be '==', '!=', '<', '<=', '>', '>='\n        \"\"\"\n        return self._icmp('s', cmpop, lhs, rhs, name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare two sets of unsigned integers.", "response": "def icmp_unsigned(self, cmpop, lhs, rhs, name=''):\n        \"\"\"\n        Unsigned integer (or pointer) comparison:\n            name = lhs <cmpop> rhs\n\n        where cmpop can be '==', '!=', '<', '<=', '>', '>='\n        \"\"\"\n        return self._icmp('u', cmpop, lhs, rhs, name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a floating - point ordered comparison instruction.", "response": "def fcmp_ordered(self, cmpop, lhs, rhs, name='', flags=[]):\n        \"\"\"\n        Floating-point ordered comparison:\n            name = lhs <cmpop> rhs\n\n        where cmpop can be '==', '!=', '<', '<=', '>', '>=', 'ord', 'uno'\n        \"\"\"\n        if cmpop in _CMP_MAP:\n            op = 'o' + _CMP_MAP[cmpop]\n        else:\n            op = cmpop\n        instr = instructions.FCMPInstr(self.block, op, lhs, rhs, name=name, flags=flags)\n        self._insert(instr)\n        return instr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef alloca(self, typ, size=None, name=''):\n        if size is None:\n            pass\n        elif isinstance(size, (values.Value, values.Constant)):\n            assert isinstance(size.type, types.IntType)\n        else:\n            # If it is not a Value instance,\n            # assume to be a Python integer.\n            size = values.Constant(types.IntType(32), size)\n\n        al = instructions.AllocaInstr(self.block, typ, size, name)\n        self._insert(al)\n        return al", "response": "Allocate a new entry in the stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self, ptr, name='', align=None):\n        if not isinstance(ptr.type, types.PointerType):\n            raise TypeError(\"cannot load from value of type %s (%r): not a pointer\"\n                            % (ptr.type, str(ptr)))\n        ld = instructions.LoadInstr(self.block, ptr, name)\n        ld.align = align\n        self._insert(ld)\n        return ld", "response": "Load a value from a pointer to a new entry in the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstore a value to a pointer.", "response": "def store(self, value, ptr, align=None):\n        \"\"\"\n        Store value to pointer, with optional guaranteed alignment:\n            *ptr = name\n        \"\"\"\n        if not isinstance(ptr.type, types.PointerType):\n            raise TypeError(\"cannot store to value of type %s (%r): not a pointer\"\n                            % (ptr.type, str(ptr)))\n        if ptr.type.pointee != value.type:\n            raise TypeError(\"cannot store %s to %s: mismatching types\"\n                            % (value.type, ptr.type))\n        st = instructions.StoreInstr(self.block, value, ptr)\n        st.align = align\n        self._insert(st)\n        return st"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_atomic(self, ptr, ordering, align, name=''):\n        if not isinstance(ptr.type, types.PointerType):\n            raise TypeError(\"cannot load from value of type %s (%r): not a pointer\"\n                            % (ptr.type, str(ptr)))\n        ld = instructions.LoadAtomicInstr(self.block, ptr, ordering, align, name)\n        self._insert(ld)\n        return ld", "response": "Load atomic value from a pointer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef store_atomic(self, value, ptr, ordering, align):\n        if not isinstance(ptr.type, types.PointerType):\n            raise TypeError(\"cannot store to value of type %s (%r): not a pointer\"\n                            % (ptr.type, str(ptr)))\n        if ptr.type.pointee != value.type:\n            raise TypeError(\"cannot store %s to %s: mismatching types\"\n                            % (value.type, ptr.type))\n        st = instructions.StoreAtomicInstr(self.block, value, ptr, ordering, align)\n        self._insert(st)\n        return st", "response": "Store atomic value to a pointer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a switch - case with a single default target.", "response": "def switch(self, value, default):\n        \"\"\"\n        Create a switch-case with a single *default* target.\n        \"\"\"\n        swt = instructions.SwitchInstr(self.block, 'switch', value, default)\n        self._set_terminator(swt)\n        return swt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef branch(self, target):\n        br = instructions.Branch(self.block, \"br\", [target])\n        self._set_terminator(br)\n        return br", "response": "Return a branch instruction that will branch to target."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a conditional branch that is set to the current block.", "response": "def cbranch(self, cond, truebr, falsebr):\n        \"\"\"\n        Conditional branch to *truebr* if *cond* is true, else to *falsebr*.\n        \"\"\"\n        br = instructions.ConditionalBranch(self.block, \"br\",\n                                            [cond, truebr, falsebr])\n        self._set_terminator(br)\n        return br"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef branch_indirect(self, addr):\n        br = instructions.IndirectBranch(self.block, \"indirectbr\", addr)\n        self._set_terminator(br)\n        return br", "response": "Return an indirect branch to target addr."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ret(self, value):\n        return self._set_terminator(\n            instructions.Ret(self.block, \"ret\", value))", "response": "Set the return value of the current resource entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresume an in - flight exception.", "response": "def resume(self, landingpad):\n        \"\"\"\n        Resume an in-flight exception.\n        \"\"\"\n        br = instructions.Branch(self.block, \"resume\", [landingpad])\n        self._set_terminator(br)\n        return br"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call(self, fn, args, name='', cconv=None, tail=False, fastmath=()):\n        inst = instructions.CallInstr(self.block, fn, args, name=name,\n                                      cconv=cconv, tail=tail, fastmath=fastmath)\n        self._insert(inst)\n        return inst", "response": "Call a function with the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a register into an LLVM value.", "response": "def load_reg(self, reg_type, reg_name, name=''):\n        \"\"\"\n        Load a register value into an LLVM value.\n          Example: v = load_reg(IntType(32), \"eax\")\n        \"\"\"\n        ftype = types.FunctionType(reg_type, [])\n        return self.asm(ftype, \"\", \"={%s}\" % reg_name, [], False, name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef store_reg(self, value, reg_type, reg_name, name=''):\n        ftype = types.FunctionType(types.VoidType(), [reg_type])\n        return self.asm(ftype, \"\", \"{%s}\" % reg_name, [value], True, name)", "response": "Store an LLVM value inside a register."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing effective address of a getelementptr.", "response": "def gep(self, ptr, indices, inbounds=False, name=''):\n        \"\"\"\n        Compute effective address (getelementptr):\n            name = getelementptr ptr, <indices...>\n        \"\"\"\n        instr = instructions.GEPInstr(self.block, ptr, indices,\n                                      inbounds=inbounds, name=name)\n        self._insert(instr)\n        return instr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_element(self, vector, idx, name=''):\n        instr = instructions.ExtractElement(self.block, vector, idx, name=name)\n        self._insert(instr)\n        return instr", "response": "Extracts the value at the given position idx."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert an element into the set at the given index.", "response": "def insert_element(self, vector, value, idx, name=''):\n        \"\"\"\n        Returns vector with vector[idx] replaced by value.\n        The result is undefined if the idx is larger or equal the vector length.\n        \"\"\"\n        instr = instructions.InsertElement(self.block, vector, value, idx,\n                                           name=name)\n        self._insert(instr)\n        return instr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shuffle_vector(self, vector1, vector2, mask, name=''):\n        instr = instructions.ShuffleVector(self.block, vector1, vector2, mask,\n                                           name=name)\n        self._insert(instr)\n        return instr", "response": "Returns a new vector in the same length as vector1 and vector2."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the value of the named attribute from the aggregate.", "response": "def extract_value(self, agg, idx, name=''):\n        \"\"\"\n        Extract member number *idx* from aggregate.\n        \"\"\"\n        if not isinstance(idx, (tuple, list)):\n            idx = [idx]\n        instr = instructions.ExtractValue(self.block, agg, idx, name=name)\n        self._insert(instr)\n        return instr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert_value(self, agg, value, idx, name=''):\n        if not isinstance(idx, (tuple, list)):\n            idx = [idx]\n        instr = instructions.InsertValue(self.block, agg, value, idx, name=name)\n        self._insert(instr)\n        return instr", "response": "Insert value into the aggregate."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cmpxchg(self, ptr, cmp, val, ordering, failordering=None, name=''):\n        failordering = ordering if failordering is None else failordering\n        inst = instructions.CmpXchg(self.block, ptr, cmp, val, ordering,\n                                    failordering, name=name)\n        self._insert(inst)\n        return inst", "response": "Compare the value of a previously set element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noptimize hint : assume is always true.", "response": "def assume(self, cond):\n        \"\"\"\n        Optimizer hint: assume *cond* is always true.\n        \"\"\"\n        fn = self.module.declare_intrinsic(\"llvm.assume\")\n        return self.call(fn, [cond])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a memory barrier to prevent certain reorderings of load and store accesses.", "response": "def fence(self, ordering, targetscope=None, name=''):\n        \"\"\"\n        Add a memory barrier, preventing certain reorderings of load and/or store accesses with\n        respect to other processors and devices.\n        \"\"\"\n        inst = instructions.Fence(self.block, ordering, targetscope, name=name)\n        self._insert(inst)\n        return inst"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_metadata(self, operands):\n        if not isinstance(operands, (list, tuple)):\n            raise TypeError(\"expected a list or tuple of metadata values, \"\n                            \"got %r\" % (operands,))\n        operands = self._fix_metadata_operands(operands)\n        key = tuple(operands)\n        if key not in self._metadatacache:\n            n = len(self.metadata)\n            md = values.MDValue(self, operands, name=str(n))\n            self._metadatacache[key] = md\n        else:\n            md = self._metadatacache[key]\n        return md", "response": "Add an unnamed metadata to the module with the given operands."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_debug_info(self, kind, operands, is_distinct=False):\n        operands = tuple(sorted(self._fix_di_operands(operands.items())))\n        key = (kind, operands, is_distinct)\n        if key not in self._metadatacache:\n            n = len(self.metadata)\n            di = values.DIValue(self, is_distinct, kind, operands, name=str(n))\n            self._metadatacache[key] = di\n        else:\n            di = self._metadatacache[key]\n        return di", "response": "Add debug information metadata to the module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_named_metadata(self, name, element=None):\n        if name in self.namedmetadata:\n            nmd = self.namedmetadata[name]\n        else:\n            nmd = self.namedmetadata[name] = values.NamedMetaData(self)\n        if element is not None:\n            if not isinstance(element, values.Value):\n                element = self.add_metadata(element)\n            if not isinstance(element.type, types.MetaDataType):\n                raise TypeError(\"wrong type for metadata element: got %r\"\n                                % (element,))\n            nmd.add(element)\n        return nmd", "response": "Add a named metadata node to the module if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a new global value.", "response": "def add_global(self, globalvalue):\n        \"\"\"\n        Add a new global value.\n        \"\"\"\n        assert globalvalue.name not in self.globals\n        self.globals[globalvalue.name] = globalvalue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a command in a subshell return stdout. Stderr is unbuffered from the subshell. x", "response": "def sh2(cmd):\n    \"\"\"Execute command in a subshell, return stdout.\n\n    Stderr is unbuffered from the subshell.x\"\"\"\n    p = Popen(cmd, stdout=PIPE, shell=True, env=sub_environment())\n    out = p.communicate()[0]\n    retcode = p.returncode\n    if retcode:\n        raise CalledProcessError(retcode, cmd)\n    else:\n        return out.rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sh3(cmd):\n    p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True,\n              env=sub_environment())\n    out, err = p.communicate()\n    retcode = p.returncode\n    if retcode:\n        raise CalledProcessError(retcode, cmd)\n    else:\n        return out.rstrip(), err.rstrip()", "response": "Execute a command in a subshell return stdout stderr\n    If anything appears in stderr print it out to sys. stderr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncloning the gh - pages repo if we haven t already.", "response": "def init_repo(path):\n    \"\"\"clone the gh-pages repo if we haven't already.\"\"\"\n    sh(\"git clone %s %s\"%(pages_repo, path))\n    here = os.getcwd()\n    cd(path)\n    sh('git checkout gh-pages')\n    cd(here)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an ExecutionEngine suitable for JIT code generation on the host CPU.", "response": "def create_execution_engine():\n    \"\"\"\n    Create an ExecutionEngine suitable for JIT code generation on\n    the host CPU.  The engine is reusable for an arbitrary number of\n    modules.\n    \"\"\"\n    # Create a target machine representing the host\n    target = llvm.Target.from_default_triple()\n    target_machine = target.create_target_machine()\n    # And an execution engine with an empty backing module\n    backing_mod = llvm.parse_assembly(\"\")\n    engine = llvm.create_mcjit_compiler(backing_mod, target_machine)\n    return engine"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile_ir(engine, llvm_ir):\n    # Create a LLVM module object from the IR\n    mod = llvm.parse_assembly(llvm_ir)\n    mod.verify()\n    # Now add the module and make sure it is ready for execution\n    engine.add_module(mod)\n    engine.finalize_object()\n    engine.run_static_constructors()\n    return mod", "response": "Compile the LLVM IR string with the given engine and return a module object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a target triple suitable for generating code for the current process.", "response": "def get_process_triple():\n    \"\"\"\n    Return a target triple suitable for generating code for the current process.\n    An example when the default triple from ``get_default_triple()`` is not be\n    suitable is when LLVM is compiled for 32-bit but the process is executing\n    in 64-bit mode.\n    \"\"\"\n    with ffi.OutputString() as out:\n        ffi.lib.LLVMPY_GetProcessTriple(out)\n        return str(out)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_host_cpu_features():\n    with ffi.OutputString() as out:\n        outdict = FeatureMap()\n        if not ffi.lib.LLVMPY_GetHostCPUFeatures(out):\n            raise RuntimeError(\"failed to get host cpu features.\")\n        flag_map = {'+': True, '-': False}\n        content = str(out)\n        if content:  # protect against empty string\n            for feat in content.split(','):\n                if feat:  # protect against empty feature\n                    outdict[feat[1:]] = flag_map[feat[0]]\n        return outdict", "response": "Returns a dictionary - like object indicating the CPU features for the current LLVM and whether it is enabled for this CPU."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_triple():\n    with ffi.OutputString() as out:\n        ffi.lib.LLVMPY_GetDefaultTargetTriple(out)\n        return str(out)", "response": "Return the default target triple for the LLVM."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the name of the host s CPU suitable for using with .", "response": "def get_host_cpu_name():\n    \"\"\"\n    Get the name of the host's CPU, suitable for using with\n    :meth:`Target.create_target_machine()`.\n    \"\"\"\n    with ffi.OutputString() as out:\n        ffi.lib.LLVMPY_GetHostCPUName(out)\n        return str(out)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_object_format(triple=None):\n    if triple is None:\n        triple = get_default_triple()\n    res = ffi.lib.LLVMPY_GetTripleObjectFormat(_encode_string(triple))\n    return _object_formats[res]", "response": "Get the object format for the given triple."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string representation of the feature set.", "response": "def flatten(self, sort=True):\n        \"\"\"\n        Args\n        ----\n        sort: bool\n            Optional.  If True, the features are sorted by name; otherwise,\n            the ordering is unstable between python session due to hash\n            randomization.  Defaults to True.\n\n        Returns a string suitable for use as the ``features`` argument to\n        ``Target.create_target_machine()``.\n\n        \"\"\"\n        iterator = sorted(self.items()) if sort else iter(self.items())\n        flag_map = {True: '+', False: '-'}\n        return ','.join('{0}{1}'.format(flag_map[v], k)\n                        for k, v in iterator)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the byte offset of the type s ty element at the given position.", "response": "def get_element_offset(self, ty, position):\n        \"\"\"\n        Get byte offset of type's ty element at the given position\n        \"\"\"\n\n        offset = ffi.lib.LLVMPY_OffsetOfElement(self, ty, position)\n        if offset == -1:\n            raise ValueError(\"Could not determined offset of {}th \"\n                    \"element of the type '{}'. Is it a struct type?\".format(\n                    position, str(ty)))\n        return offset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_pointee_abi_size(self, ty):\n        size = ffi.lib.LLVMPY_ABISizeOfElementType(self, ty)\n        if size == -1:\n            raise RuntimeError(\"Not a pointer type: %s\" % (ty,))\n        return size", "response": "Get the size of the ABI of pointee type of LLVM pointer type ty."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the minimum ABI alignment of pointee type of LLVM pointer type ty.", "response": "def get_pointee_abi_alignment(self, ty):\n        \"\"\"\n        Get minimum ABI alignment of pointee type of LLVM pointer type *ty*.\n        \"\"\"\n        size = ffi.lib.LLVMPY_ABIAlignmentOfElementType(self, ty)\n        if size == -1:\n            raise RuntimeError(\"Not a pointer type: %s\" % (ty,))\n        return size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Target instance from a triple.", "response": "def from_triple(cls, triple):\n        \"\"\"\n        Create a Target instance for the given triple (a string).\n        \"\"\"\n        with ffi.OutputString() as outerr:\n            target = ffi.lib.LLVMPY_GetTargetFromTriple(triple.encode('utf8'),\n                                                        outerr)\n            if not target:\n                raise RuntimeError(str(outerr))\n            target = cls(target)\n            target._triple = triple\n            return target"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_target_machine(self, cpu='', features='',\n                              opt=2, reloc='default', codemodel='jitdefault',\n                              jitdebug=False, printmc=False):\n        \"\"\"\n        Create a new TargetMachine for this target and the given options.\n\n        Specifying codemodel='default' will result in the use of the \"small\"\n        code model. Specifying codemodel='jitdefault' will result in the code\n        model being picked based on platform bitness (32=\"small\", 64=\"large\").\n        \"\"\"\n        assert 0 <= opt <= 3\n        assert reloc in RELOC\n        assert codemodel in CODEMODEL\n        triple = self._triple\n        # MCJIT under Windows only supports ELF objects, see\n        # http://lists.llvm.org/pipermail/llvm-dev/2013-December/068341.html\n        # Note we still want to produce regular COFF files in AOT mode.\n        if os.name == 'nt' and codemodel == 'jitdefault':\n            triple += '-elf'\n        tm = ffi.lib.LLVMPY_CreateTargetMachine(self,\n                                                _encode_string(triple),\n                                                _encode_string(cpu),\n                                                _encode_string(features),\n                                                opt,\n                                                _encode_string(reloc),\n                                                _encode_string(codemodel),\n                                                int(jitdebug),\n                                                int(printmc),\n        )\n        if tm:\n            return TargetMachine(tm)\n        else:\n            raise RuntimeError(\"Cannot create target machine\")", "response": "Create a new target machine for this target."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the bytes of object code of the module.", "response": "def _emit_to_memory(self, module, use_object=False):\n        \"\"\"Returns bytes of object code of the module.\n\n        Args\n        ----\n        use_object : bool\n            Emit object code or (if False) emit assembly code.\n        \"\"\"\n        with ffi.OutputString() as outerr:\n            mb = ffi.lib.LLVMPY_TargetMachineEmitToMemory(self, module,\n                                                          int(use_object),\n                                                          outerr)\n            if not mb:\n                raise RuntimeError(str(outerr))\n\n        bufptr = ffi.lib.LLVMPY_GetBufferStart(mb)\n        bufsz = ffi.lib.LLVMPY_GetBufferSize(mb)\n        try:\n            return string_at(bufptr, bufsz)\n        finally:\n            ffi.lib.LLVMPY_DisposeMemoryBuffer(mb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_assembly(llvmir, context=None):\n    if context is None:\n        context = get_global_context()\n    llvmir = _encode_string(llvmir)\n    strbuf = c_char_p(llvmir)\n    with ffi.OutputString() as errmsg:\n        mod = ModuleRef(\n            ffi.lib.LLVMPY_ParseAssembly(context, strbuf, errmsg),\n            context)\n        if errmsg:\n            mod.close()\n            raise RuntimeError(\"LLVM IR parsing error\\n{0}\".format(errmsg))\n    return mod", "response": "Create a Module from a LLVM IR string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_bitcode(bitcode, context=None):\n    if context is None:\n        context = get_global_context()\n    buf = c_char_p(bitcode)\n    bufsize = len(bitcode)\n    with ffi.OutputString() as errmsg:\n        mod = ModuleRef(ffi.lib.LLVMPY_ParseBitcode(\n            context, buf, bufsize, errmsg), context)\n        if errmsg:\n            mod.close()\n            raise RuntimeError(\n                \"LLVM bitcode parsing error\\n{0}\".format(errmsg))\n    return mod", "response": "Parse a LLVM bitcode into a module."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the module s LLVM bitcode as a bytes object.", "response": "def as_bitcode(self):\n        \"\"\"\n        Return the module's LLVM bitcode, as a bytes object.\n        \"\"\"\n        ptr = c_char_p(None)\n        size = c_size_t(-1)\n        ffi.lib.LLVMPY_WriteBitcodeToString(self, byref(ptr), byref(size))\n        if not ptr:\n            raise MemoryError\n        try:\n            assert size.value >= 0\n            return string_at(ptr, size.value)\n        finally:\n            ffi.lib.LLVMPY_DisposeString(ptr)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_function(self, name):\n        p = ffi.lib.LLVMPY_GetNamedFunction(self, _encode_string(name))\n        if not p:\n            raise NameError(name)\n        return ValueRef(p, 'function', dict(module=self))", "response": "Get a ValueRef pointing to the function named name. Raises NameError is raised if the symbol isn t found."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_global_variable(self, name):\n        p = ffi.lib.LLVMPY_GetNamedGlobalVariable(self, _encode_string(name))\n        if not p:\n            raise NameError(name)\n        return ValueRef(p, 'global', dict(module=self))", "response": "Get a ValueRef pointing to the global variable named name. Raises NameError is raised if the symbol is not found."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a TypeRef pointing to a struct type named name. Raises NameError is raised if the struct type is not found.", "response": "def get_struct_type(self, name):\n        \"\"\"\n        Get a TypeRef pointing to a structure type named *name*.\n        NameError is raised if the struct type isn't found.\n        \"\"\"\n        p = ffi.lib.LLVMPY_GetNamedStructType(self, _encode_string(name))\n        if not p:\n            raise NameError(name)\n        return TypeRef(p)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying the module IR s correctness.", "response": "def verify(self):\n        \"\"\"\n        Verify the module IR's correctness.  RuntimeError is raised on error.\n        \"\"\"\n        with ffi.OutputString() as outmsg:\n            if ffi.lib.LLVMPY_VerifyModule(self, outmsg):\n                raise RuntimeError(str(outmsg))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_layout(self):\n        # LLVMGetDataLayout() points inside a std::string managed by LLVM.\n        with ffi.OutputString(owned=False) as outmsg:\n            ffi.lib.LLVMPY_GetDataLayout(self, outmsg)\n            return str(outmsg)", "response": "This module s data layout specification as a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the target triple of the current LLVM entry.", "response": "def triple(self):\n        \"\"\"\n        This module's target \"triple\" specification, as a string.\n        \"\"\"\n        # LLVMGetTarget() points inside a std::string managed by LLVM.\n        with ffi.OutputString(owned=False) as outmsg:\n            ffi.lib.LLVMPY_GetTarget(self, outmsg)\n            return str(outmsg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlink the other module into this one.", "response": "def link_in(self, other, preserve=False):\n        \"\"\"\n        Link the *other* module into this one.  The *other* module will\n        be destroyed unless *preserve* is true.\n        \"\"\"\n        if preserve:\n            other = other.clone()\n        link_modules(self, other)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef global_variables(self):\n        it = ffi.lib.LLVMPY_ModuleGlobalsIter(self)\n        return _GlobalsIterator(it, dict(module=self))", "response": "Return an iterator over this module s global variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an iterator over this module s functions.", "response": "def functions(self):\n        \"\"\"\n        Return an iterator over this module's functions.\n        The iterator will yield a ValueRef for each function.\n        \"\"\"\n        it = ffi.lib.LLVMPY_ModuleFunctionsIter(self)\n        return _FunctionsIterator(it, dict(module=self))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an iterator over the struct types defined in this module.", "response": "def struct_types(self):\n        \"\"\"\n        Return an iterator over the struct types defined in\n        the module. The iterator will yield a TypeRef.\n        \"\"\"\n        it = ffi.lib.LLVMPY_ModuleTypesIter(self)\n        return _TypesIterator(it, dict(module=self))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_option(name, option):\n    ffi.lib.LLVMPY_SetCommandLine(_encode_string(name),\n                                  _encode_string(option))", "response": "Set the given LLVM command - line option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing all targets in the current LLVM.", "response": "def initialize_all_targets():\n    \"\"\"\n    Initialize all targets. Necessary before targets can be looked up\n    via the :class:`Target` class.\n    \"\"\"\n    ffi.lib.LLVMPY_InitializeAllTargetInfos()\n    ffi.lib.LLVMPY_InitializeAllTargets()\n    ffi.lib.LLVMPY_InitializeAllTargetMCs()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert this type object to an LLVM type.", "response": "def _get_ll_pointer_type(self, target_data, context=None):\n        \"\"\"\n        Convert this type object to an LLVM type.\n        \"\"\"\n        from . import Module, GlobalVariable\n        from ..binding import parse_assembly\n\n        if context is None:\n            m = Module()\n        else:\n            m = Module(context=context)\n        foo = GlobalVariable(m, self, name=\"foo\")\n        with parse_assembly(str(m)) as llmod:\n            return llmod.get_global_variable(foo.name).type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the ABI size of this type according to data layout target_data.", "response": "def get_abi_size(self, target_data, context=None):\n        \"\"\"\n        Get the ABI size of this type according to data layout *target_data*.\n        \"\"\"\n        llty = self._get_ll_pointer_type(target_data, context)\n        return target_data.get_pointee_abi_size(llty)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_abi_alignment(self, target_data, context=None):\n        llty = self._get_ll_pointer_type(target_data, context)\n        return target_data.get_pointee_abi_alignment(llty)", "response": "Get the ABI alignment of this type according to data layout\n        target_data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve the type of the i - th element.", "response": "def gep(self, i):\n        \"\"\"\n        Resolve the type of the i-th element (for getelementptr lookups).\n        \"\"\"\n        if not isinstance(i.type, IntType):\n            raise TypeError(i.type)\n        return self.pointee"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gep(self, i):\n        if not isinstance(i.type, IntType):\n            raise TypeError(i.type)\n        return self.element", "response": "Resolve the type of the i - th element."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef structure_repr(self):\n        ret = '{%s}' % ', '.join([str(x) for x in self.elements])\n        return self._wrap_packed(ret)", "response": "Return the LLVM IR for the structure representation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gep(self, i):\n        if not isinstance(i.type, IntType):\n            raise TypeError(i.type)\n        return self.elements[i.constant]", "response": "Resolve the type of the i - th element."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_declaration(self):\n        if self.is_opaque:\n            out = \"{strrep} = type opaque\".format(strrep=str(self))\n        else:\n            out = \"{strrep} = type {struct}\".format(\n                strrep=str(self), struct=self.structure_repr())\n        return out", "response": "Returns the string for the declaration of the type\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose this object and do any required clean - up actions.", "response": "def close(self):\n        \"\"\"\n        Close this object and do any required clean-up actions.\n        \"\"\"\n        try:\n            if not self._closed and not self._owned:\n                self._dispose()\n        finally:\n            self.detach()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detach(self):\n        if not self._closed:\n            del self._as_parameter_\n            self._closed = True\n            self._ptr = None", "response": "Detach the underlying LLVM resource without disposing of it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering the *address* of the global symbol name.", "response": "def add_symbol(name, address):\n    \"\"\"\n    Register the *address* of global symbol *name*.  This will make\n    it usable (e.g. callable) from LLVM-compiled functions.\n    \"\"\"\n    ffi.lib.LLVMPY_AddSymbol(_encode_string(name), c_void_p(address))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_library_permanently(filename):\n    with ffi.OutputString() as outerr:\n        if ffi.lib.LLVMPY_LoadLibraryPermanently(\n                _encode_string(filename), outerr):\n            raise RuntimeError(str(outerr))", "response": "Loads an external library and raises a RuntimeError if it fails."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_win32_generator():\n    # XXX this assumes we will find a generator that's the same, or\n    # compatible with, the one which was used to compile LLVM... cmake\n    # seems a bit lacking here.\n    cmake_dir = os.path.join(here_dir, 'dummy')\n    # LLVM 4.0+ needs VS 2015 minimum.\n    generators = []\n    if os.environ.get(\"CMAKE_GENERATOR\"):\n        generators.append(os.environ.get(\"CMAKE_GENERATOR\"))\n\n    # Drop generators that are too old\n    vspat = re.compile(r'Visual Studio (\\d+)')\n    def drop_old_vs(g):\n        m = vspat.match(g)\n        if m is None:\n            return True  # keep those we don't recognize\n        ver = int(m.group(1))\n        return ver >= 14\n    generators = list(filter(drop_old_vs, generators))\n\n    generators.append('Visual Studio 14 2015' + (' Win64' if is_64bit else ''))\n    for generator in generators:\n        build_dir = tempfile.mkdtemp()\n        print(\"Trying generator %r\" % (generator,))\n        try:\n            try_cmake(cmake_dir, build_dir, generator)\n        except subprocess.CalledProcessError:\n            continue\n        else:\n            # Success\n            return generator\n        finally:\n            shutil.rmtree(build_dir)\n    raise RuntimeError(\"No compatible cmake generator installed on this machine\")", "response": "Find a suitable cmake generator under Windows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _escape_string(text, _map={}):\n    if isinstance(text, str):\n        text = text.encode('ascii')\n    assert isinstance(text, (bytes, bytearray))\n\n    if not _map:\n        for ch in range(256):\n            if ch in _VALID_CHARS:\n                _map[ch] = chr(ch)\n            else:\n                _map[ch] = '\\\\%02x' % ch\n            if six.PY2:\n                _map[chr(ch)] = _map[ch]\n\n    buf = [_map[ch] for ch in text]\n    return ''.join(buf)", "response": "Escape the given bytestring for safe use as a LLVM array constant."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inttoptr(self, typ):\n        if not isinstance(self.type, types.IntType):\n            raise TypeError(\"can only call inttoptr() on integer constants, not '%s'\"\n                            % (self.type,))\n        if not isinstance(typ, types.PointerType):\n            raise TypeError(\"can only inttoptr() to pointer type, not '%s'\"\n                            % (typ,))\n\n        op = \"inttoptr ({0} {1} to {2})\".format(self.type,\n                                                self.get_reference(),\n                                                typ)\n        return FormattedConstant(typ, op)", "response": "Cast this integer constant to the given pointer type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value of the getelementptr operation on this pointer constant.", "response": "def gep(self, indices):\n        \"\"\"\n        Call getelementptr on this pointer constant.\n        \"\"\"\n        if not isinstance(self.type, types.PointerType):\n            raise TypeError(\"can only call gep() on pointer constants, not '%s'\"\n                            % (self.type,))\n\n        outtype = self.type\n        for i in indices:\n            outtype = outtype.gep(i)\n\n        strindices = [\"{0} {1}\".format(idx.type, idx.get_reference())\n                      for idx in indices]\n\n        op = \"getelementptr ({0}, {1} {2}, {3})\".format(\n            self.type.pointee, self.type,\n            self.get_reference(), ', '.join(strindices))\n        return FormattedConstant(outtype.as_pointer(self.addrspace), op)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef literal_array(cls, elems):\n        tys = [el.type for el in elems]\n        if len(tys) == 0:\n            raise ValueError(\"need at least one element\")\n        ty = tys[0]\n        for other in tys:\n            if ty != other:\n                raise TypeError(\"all elements must have the same type\")\n        return cls(types.ArrayType(ty, len(elems)), elems)", "response": "Construct a literal array constant made of the given members."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a literal struct constant made of the given members.", "response": "def literal_struct(cls, elems):\n        \"\"\"\n        Construct a literal structure constant made of the given members.\n        \"\"\"\n        tys = [el.type for el in elems]\n        return cls(types.LiteralStructType(tys), elems)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert block before and return it", "response": "def insert_basic_block(self, before, name=''):\n        \"\"\"Insert block before\n        \"\"\"\n        blk = Block(parent=self, name=name)\n        self.blocks.insert(before, blk)\n        return blk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndescribe the prototype of the function.", "response": "def descr_prototype(self, buf):\n        \"\"\"\n        Describe the prototype (\"head\") of the function.\n        \"\"\"\n        state = \"define\" if self.blocks else \"declare\"\n        ret = self.return_value\n        args = \", \".join(str(a) for a in self.args)\n        name = self.get_reference()\n        attrs = self.attributes\n        if any(self.args):\n            vararg = ', ...' if self.ftype.var_arg else ''\n        else:\n            vararg = '...' if self.ftype.var_arg else ''\n        linkage = self.linkage\n        cconv = self.calling_convention\n        prefix = \" \".join(str(x) for x in [state, linkage, cconv, ret] if x)\n        metadata = self._stringify_metadata()\n        prototype = \"{prefix} {name}({args}{vararg}) {attrs}{metadata}\\n\".format(\n            prefix=prefix, name=name, args=args, vararg=vararg,\n            attrs=attrs, metadata=metadata)\n        buf.append(prototype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace(self, old, new):\n        if old.type != new.type:\n            raise TypeError(\"new instruction has a different type\")\n        pos = self.instructions.index(old)\n        self.instructions.remove(old)\n        self.instructions.insert(pos, new)\n\n        for bb in self.parent.basic_blocks:\n            for instr in bb.instructions:\n                instr.replace_usage(old, new)", "response": "Replace an instruction with a new instruction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string of the control - flow graph of the function in DOT format.", "response": "def get_function_cfg(func, show_inst=True):\n    \"\"\"Return a string of the control-flow graph of the function in DOT\n    format. If the input `func` is not a materialized function, the module\n    containing the function is parsed to create an actual LLVM module.\n    The `show_inst` flag controls whether the instructions of each block\n    are printed.\n    \"\"\"\n    assert func is not None\n    if isinstance(func, ir.Function):\n        mod = parse_assembly(str(func.module))\n        func = mod.get_function(func.name)\n\n    # Assume func is a materialized function\n    with ffi.OutputString() as dotstr:\n        ffi.lib.LLVMPY_WriteCFG(func, dotstr, show_inst)\n        return str(dotstr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef view_dot_graph(graph, filename=None, view=False):\n    # Optionally depends on graphviz package\n    import graphviz as gv\n\n    src = gv.Source(graph)\n    if view:\n        # Returns the output file path\n        return src.render(filename, view=view)\n    else:\n        # Attempts to show the graph in IPython notebook\n        try:\n            __IPYTHON__\n        except NameError:\n            return src\n        else:\n            import IPython.display as display\n            format = 'svg'\n            return display.SVG(data=src.pipe(format))", "response": "View the given DOT source in the specified file path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the pointed - to type. When the type is not a pointer an exception is raised.", "response": "def element_type(self):\n        \"\"\"\n        Returns the pointed-to type. When the type is not a pointer,\n        raises exception.\n        \"\"\"\n        if not self.is_pointer:\n            raise ValueError(\"Type {} is not a pointer\".format(self))\n        return TypeRef(ffi.lib.LLVMPY_GetElementType(self))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_function_attribute(self, attr):\n        if not self.is_function:\n            raise ValueError('expected function value, got %s' % (self._kind,))\n        attrname = str(attr)\n        attrval = ffi.lib.LLVMPY_GetEnumAttributeKindForName(\n            _encode_string(attrname), len(attrname))\n        if attrval == 0:\n            raise ValueError('no such attribute {!r}'.format(attrname))\n        ffi.lib.LLVMPY_AddFunctionAttr(self, attrval)", "response": "Only works on function value\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if the current value is a global or function value.", "response": "def is_declaration(self):\n        \"\"\"\n        Whether this value (presumably global) is defined in the current\n        module.\n        \"\"\"\n        if not (self.is_global or self.is_function):\n            raise ValueError('expected global or function value, got %s'\n                             % (self._kind,))\n        return ffi.lib.LLVMPY_IsDeclaration(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an iterator over this value s attributes.", "response": "def attributes(self):\n        \"\"\"\n        Return an iterator over this value's attributes.\n        The iterator will yield a string for each attribute.\n        \"\"\"\n        itr = iter(())\n        if self.is_function:\n            it = ffi.lib.LLVMPY_FunctionAttributesIter(self)\n            itr = _AttributeListIterator(it)\n        elif self.is_instruction:\n            if self.opcode == 'call':\n                it = ffi.lib.LLVMPY_CallInstAttributesIter(self)\n                itr = _AttributeListIterator(it)\n            elif self.opcode == 'invoke':\n                it = ffi.lib.LLVMPY_InvokeInstAttributesIter(self)\n                itr = _AttributeListIterator(it)\n        elif self.is_global:\n            it = ffi.lib.LLVMPY_GlobalAttributesIter(self)\n            itr = _AttributeSetIterator(it)\n        elif self.is_argument:\n            it = ffi.lib.LLVMPY_ArgumentAttributesIter(self)\n            itr = _AttributeSetIterator(it)\n        return itr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an iterator over this function s blocks.", "response": "def blocks(self):\n        \"\"\"\n        Return an iterator over this function's blocks.\n        The iterator will yield a ValueRef for each block.\n        \"\"\"\n        if not self.is_function:\n            raise ValueError('expected function value, got %s' % (self._kind,))\n        it = ffi.lib.LLVMPY_FunctionBlocksIter(self)\n        parents = self._parents.copy()\n        parents.update(function=self)\n        return _BlocksIterator(it, parents)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator over this function s arguments.", "response": "def arguments(self):\n        \"\"\"\n        Return an iterator over this function's arguments.\n        The iterator will yield a ValueRef for each argument.\n        \"\"\"\n        if not self.is_function:\n            raise ValueError('expected function value, got %s' % (self._kind,))\n        it = ffi.lib.LLVMPY_FunctionArgumentsIter(self)\n        parents = self._parents.copy()\n        parents.update(function=self)\n        return _ArgumentsIterator(it, parents)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an iterator over this block s instructions.", "response": "def instructions(self):\n        \"\"\"\n        Return an iterator over this block's instructions.\n        The iterator will yield a ValueRef for each instruction.\n        \"\"\"\n        if not self.is_block:\n            raise ValueError('expected block value, got %s' % (self._kind,))\n        it = ffi.lib.LLVMPY_BlockInstructionsIter(self)\n        parents = self._parents.copy()\n        parents.update(block=self)\n        return _InstructionsIterator(it, parents)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef operands(self):\n        if not self.is_instruction:\n            raise ValueError('expected instruction value, got %s'\n                             % (self._kind,))\n        it = ffi.lib.LLVMPY_InstructionOperandsIter(self)\n        parents = self._parents.copy()\n        parents.update(instruction=self)\n        return _OperandsIterator(it, parents)", "response": "Return an iterator over this instruction s operands."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces all calls to orig to repl in module mod.", "response": "def replace_all_calls(mod, orig, repl):\n    \"\"\"Replace all calls to `orig` to `repl` in module `mod`.\n    Returns the references to the returned calls\n    \"\"\"\n    rc = ReplaceCalls(orig, repl)\n    rc.visit(mod)\n    return rc.calls"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display(self, image):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        image = self.preprocess(image)\n\n        self.command(\n            # Column start/end address\n            self._const.COLUMNADDR, self._colstart, self._colend - 1,\n            # Page start/end address\n            self._const.PAGEADDR, 0x00, self._pages - 1)\n\n        buf = bytearray(self._w * self._pages)\n        off = self._offsets\n        mask = self._mask\n\n        idx = 0\n        for pix in image.getdata():\n            if pix > 0:\n                buf[off[idx]] |= mask[idx]\n            idx += 1\n\n        self.data(list(buf))", "response": "Prints the contents of the image to the OLED log."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a command and arguments through to the delegated serial interface.", "response": "def command(self, cmd, *args):\n        \"\"\"\n        Sends a command and an (optional) sequence of arguments through to the\n        delegated serial interface. Note that the arguments are passed through\n        as data.\n        \"\"\"\n        self._serial_interface.command(cmd)\n        if len(args) > 0:\n            self._serial_interface.data(list(args))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender the image to the OLED display.", "response": "def display(self, image):\n        \"\"\"\n        Takes a 1-bit monochrome or 24-bit RGB image and renders it\n        to the greyscale OLED display. RGB pixels are converted to 4-bit\n        greyscale values using a simplified Luma calculation, based on\n        *Y'=0.299R'+0.587G'+0.114B'*.\n\n        :param image: The image to render.\n        :type image: PIL.Image.Image\n        \"\"\"\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        image = self.preprocess(image)\n\n        if self.framebuffer.redraw_required(image):\n            left, top, right, bottom = self.framebuffer.inflate_bbox()\n            width = right - left\n            height = bottom - top\n\n            buf = bytearray(width * height >> 1)\n            self._set_position(top, right, bottom, left)\n            self._populate(buf, self.framebuffer.getdata())\n            self.data(list(buf))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef display(self, image):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n\n        image = self.preprocess(image)\n\n        if self.framebuffer.redraw_required(image):\n            left, top, right, bottom = self._apply_offsets(self.framebuffer.bounding_box)\n            width = right - left\n            height = bottom - top\n\n            self._set_position(top, right, bottom, left)\n\n            i = 0\n            buf = bytearray(width * height * 2)\n            for r, g, b in self.framebuffer.getdata():\n                if not(r == g == b == 0):\n                    # 65K format 1\n                    buf[i] = r & 0xF8 | g >> 5\n                    buf[i + 1] = g << 3 & 0xE0 | b >> 3\n                i += 2\n\n            self.data(list(buf))", "response": "Renders a 24 - bit RGB image to the Color OLED display."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nemitting the event for revealing the secret on - chain if the secret is settled off - chain.", "response": "def events_for_onchain_secretreveal(\n        target_state: TargetTransferState,\n        channel_state: NettingChannelState,\n        block_number: BlockNumber,\n        block_hash: BlockHash,\n) -> List[Event]:\n    \"\"\" Emits the event for revealing the secret on-chain if the transfer\n    can not be settled off-chain.\n    \"\"\"\n    transfer = target_state.transfer\n    expiration = transfer.lock.expiration\n\n    safe_to_wait, _ = is_safe_to_wait(\n        expiration,\n        channel_state.reveal_timeout,\n        block_number,\n    )\n    secret_known_offchain = channel.is_secret_known_offchain(\n        channel_state.partner_state,\n        transfer.lock.secrethash,\n    )\n    has_onchain_reveal_started = (\n        target_state.state == TargetTransferState.ONCHAIN_SECRET_REVEAL\n    )\n\n    if not safe_to_wait and secret_known_offchain and not has_onchain_reveal_started:\n        target_state.state = TargetTransferState.ONCHAIN_SECRET_REVEAL\n        secret = channel.get_secret(\n            channel_state.partner_state,\n            transfer.lock.secrethash,\n        )\n        assert secret, 'secret should be known at this point'\n        return secret_registry.events_for_onchain_secretreveal(\n            channel_state=channel_state,\n            secret=secret,\n            expiration=expiration,\n            block_hash=block_hash,\n        )\n\n    return list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles an ActionInitTarget state change.", "response": "def handle_inittarget(\n        state_change: ActionInitTarget,\n        channel_state: NettingChannelState,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\" Handles an ActionInitTarget state change. \"\"\"\n    transfer = state_change.transfer\n    route = state_change.route\n\n    assert channel_state.identifier == transfer.balance_proof.channel_identifier\n    is_valid, channel_events, errormsg = channel.handle_receive_lockedtransfer(\n        channel_state,\n        transfer,\n    )\n\n    if is_valid:\n        # A valid balance proof does not mean the payment itself is still valid.\n        # e.g. the lock may be near expiration or have expired. This is fine. The\n        # message with an unusable lock must be handled to properly synchronize the\n        # local view of the partner's channel state, allowing the next balance\n        # proofs to be handled. This however, must only be done once, which is\n        # enforced by the nonce increasing sequentially, which is verified by\n        # the handler handle_receive_lockedtransfer.\n        target_state = TargetTransferState(route, transfer)\n\n        safe_to_wait, _ = is_safe_to_wait(\n            transfer.lock.expiration,\n            channel_state.reveal_timeout,\n            block_number,\n        )\n\n        # If there is not enough time to safely unlock the lock on-chain\n        # silently let the transfer expire. The target task must be created to\n        # handle the ReceiveLockExpired state change, which will clear the\n        # expired lock.\n        if safe_to_wait:\n            message_identifier = message_identifier_from_prng(pseudo_random_generator)\n            recipient = transfer.initiator\n            secret_request = SendSecretRequest(\n                recipient=Address(recipient),\n                channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n                message_identifier=message_identifier,\n                payment_identifier=transfer.payment_identifier,\n                amount=transfer.lock.amount,\n                expiration=transfer.lock.expiration,\n                secrethash=transfer.lock.secrethash,\n            )\n            channel_events.append(secret_request)\n\n        iteration = TransitionResult(target_state, channel_events)\n    else:\n        # If the balance proof is not valid, do *not* create a task. Otherwise it's\n        # possible for an attacker to send multiple invalid transfers, and increase\n        # the memory usage of this Node.\n        unlock_failed = EventUnlockClaimFailed(\n            identifier=transfer.payment_identifier,\n            secrethash=transfer.lock.secrethash,\n            reason=errormsg,\n        )\n        channel_events.append(unlock_failed)\n        iteration = TransitionResult(None, channel_events)\n\n    return iteration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_offchain_secretreveal(\n        target_state: TargetTransferState,\n        state_change: ReceiveSecretReveal,\n        channel_state: NettingChannelState,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\" Validates and handles a ReceiveSecretReveal state change. \"\"\"\n    valid_secret = is_valid_secret_reveal(\n        state_change=state_change,\n        transfer_secrethash=target_state.transfer.lock.secrethash,\n        secret=state_change.secret,\n    )\n    has_transfer_expired = channel.is_transfer_expired(\n        transfer=target_state.transfer,\n        affected_channel=channel_state,\n        block_number=block_number,\n    )\n\n    if valid_secret and not has_transfer_expired:\n        channel.register_offchain_secret(\n            channel_state=channel_state,\n            secret=state_change.secret,\n            secrethash=state_change.secrethash,\n        )\n\n        route = target_state.route\n        message_identifier = message_identifier_from_prng(pseudo_random_generator)\n        target_state.state = TargetTransferState.OFFCHAIN_SECRET_REVEAL\n        target_state.secret = state_change.secret\n        recipient = route.node_address\n\n        reveal = SendSecretReveal(\n            recipient=recipient,\n            channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n            message_identifier=message_identifier,\n            secret=target_state.secret,\n        )\n\n        iteration = TransitionResult(target_state, [reveal])\n\n    else:\n        # TODO: event for byzantine behavior\n        iteration = TransitionResult(target_state, list())\n\n    return iteration", "response": "Validates and handles a ReceiveSecretReveal state change."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate and handles a ContractReceiveSecretReveal state change.", "response": "def handle_onchain_secretreveal(\n        target_state: TargetTransferState,\n        state_change: ContractReceiveSecretReveal,\n        channel_state: NettingChannelState,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\" Validates and handles a ContractReceiveSecretReveal state change. \"\"\"\n    valid_secret = is_valid_secret_reveal(\n        state_change=state_change,\n        transfer_secrethash=target_state.transfer.lock.secrethash,\n        secret=state_change.secret,\n    )\n\n    if valid_secret:\n        channel.register_onchain_secret(\n            channel_state=channel_state,\n            secret=state_change.secret,\n            secrethash=state_change.secrethash,\n            secret_reveal_block_number=state_change.block_number,\n        )\n\n        target_state.state = TargetTransferState.ONCHAIN_UNLOCK\n        target_state.secret = state_change.secret\n\n    return TransitionResult(target_state, list())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a ReceiveUnlock state change.", "response": "def handle_unlock(\n        target_state: TargetTransferState,\n        state_change: ReceiveUnlock,\n        channel_state: NettingChannelState,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\" Handles a ReceiveUnlock state change. \"\"\"\n    balance_proof_sender = state_change.balance_proof.sender\n\n    is_valid, events, _ = channel.handle_unlock(\n        channel_state,\n        state_change,\n    )\n    next_target_state: Optional[TargetTransferState] = target_state\n\n    if is_valid:\n        transfer = target_state.transfer\n        payment_received_success = EventPaymentReceivedSuccess(\n            payment_network_identifier=channel_state.payment_network_identifier,\n            token_network_identifier=TokenNetworkID(channel_state.token_network_identifier),\n            identifier=transfer.payment_identifier,\n            amount=TokenAmount(transfer.lock.amount),\n            initiator=transfer.initiator,\n        )\n\n        unlock_success = EventUnlockClaimSuccess(\n            transfer.payment_identifier,\n            transfer.lock.secrethash,\n        )\n\n        send_processed = SendProcessed(\n            recipient=balance_proof_sender,\n            channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n            message_identifier=state_change.message_identifier,\n        )\n\n        events.extend([payment_received_success, unlock_success, send_processed])\n        next_target_state = None\n\n    return TransitionResult(next_target_state, events)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_block(\n        target_state: TargetTransferState,\n        channel_state: NettingChannelState,\n        block_number: BlockNumber,\n        block_hash: BlockHash,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\" After Raiden learns about a new block this function must be called to\n    handle expiration of the hash time lock.\n    \"\"\"\n    transfer = target_state.transfer\n    events: List[Event] = list()\n    lock = transfer.lock\n\n    secret_known = channel.is_secret_known(\n        channel_state.partner_state,\n        lock.secrethash,\n    )\n    lock_has_expired, _ = channel.is_lock_expired(\n        end_state=channel_state.our_state,\n        lock=lock,\n        block_number=block_number,\n        lock_expiration_threshold=channel.get_receiver_expiration_threshold(lock),\n    )\n\n    if lock_has_expired and target_state.state != 'expired':\n        failed = EventUnlockClaimFailed(\n            identifier=transfer.payment_identifier,\n            secrethash=transfer.lock.secrethash,\n            reason=f'lock expired',\n        )\n        target_state.state = TargetTransferState.EXPIRED\n        events = [failed]\n    elif secret_known:\n        events = events_for_onchain_secretreveal(\n            target_state=target_state,\n            channel_state=channel_state,\n            block_number=block_number,\n            block_hash=block_hash,\n        )\n\n    return TransitionResult(target_state, events)", "response": "This function handles the block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving expired locks from channel states.", "response": "def handle_lock_expired(\n        target_state: TargetTransferState,\n        state_change: ReceiveLockExpired,\n        channel_state: NettingChannelState,\n        block_number: BlockNumber,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\"Remove expired locks from channel states.\"\"\"\n    result = channel.handle_receive_lock_expired(\n        channel_state=channel_state,\n        state_change=state_change,\n        block_number=block_number,\n    )\n    assert result.new_state, 'handle_receive_lock_expired should not delete the task'\n\n    if not channel.get_lock(result.new_state.partner_state, target_state.transfer.lock.secrethash):\n        transfer = target_state.transfer\n        unlock_failed = EventUnlockClaimFailed(\n            identifier=transfer.payment_identifier,\n            secrethash=transfer.lock.secrethash,\n            reason='Lock expired',\n        )\n        result.events.append(unlock_failed)\n        return TransitionResult(None, result.events)\n\n    return TransitionResult(target_state, result.events)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef state_transition(\n        target_state: Optional[TargetTransferState],\n        state_change: StateChange,\n        channel_state: NettingChannelState,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n) -> TransitionResult[TargetTransferState]:\n    \"\"\" State machine for the target node of a mediated transfer. \"\"\"\n    # pylint: disable=too-many-branches,unidiomatic-typecheck\n\n    iteration = TransitionResult(target_state, list())\n    if type(state_change) == ActionInitTarget:\n        assert isinstance(state_change, ActionInitTarget), MYPY_ANNOTATION\n        if target_state is None:\n            iteration = handle_inittarget(\n                state_change,\n                channel_state,\n                pseudo_random_generator,\n                block_number,\n            )\n    elif type(state_change) == Block:\n        assert isinstance(state_change, Block), MYPY_ANNOTATION\n        assert state_change.block_number == block_number\n        assert target_state, 'Block state changes should be accompanied by a valid target state'\n\n        iteration = handle_block(\n            target_state=target_state,\n            channel_state=channel_state,\n            block_number=state_change.block_number,\n            block_hash=state_change.block_hash,\n        )\n    elif type(state_change) == ReceiveSecretReveal:\n        assert isinstance(state_change, ReceiveSecretReveal), MYPY_ANNOTATION\n        assert target_state, 'ReceiveSecretReveal should be accompanied by a valid target state'\n        iteration = handle_offchain_secretreveal(\n            target_state=target_state,\n            state_change=state_change,\n            channel_state=channel_state,\n            pseudo_random_generator=pseudo_random_generator,\n            block_number=block_number,\n        )\n    elif type(state_change) == ContractReceiveSecretReveal:\n        assert isinstance(state_change, ContractReceiveSecretReveal), MYPY_ANNOTATION\n        msg = 'ContractReceiveSecretReveal should be accompanied by a valid target state'\n        assert target_state, msg\n        iteration = handle_onchain_secretreveal(\n            target_state,\n            state_change,\n            channel_state,\n        )\n    elif type(state_change) == ReceiveUnlock:\n        assert isinstance(state_change, ReceiveUnlock), MYPY_ANNOTATION\n        assert target_state, 'ReceiveUnlock should be accompanied by a valid target state'\n        iteration = handle_unlock(\n            target_state=target_state,\n            state_change=state_change,\n            channel_state=channel_state,\n        )\n    elif type(state_change) == ReceiveLockExpired:\n        assert isinstance(state_change, ReceiveLockExpired), MYPY_ANNOTATION\n        assert target_state, 'ReceiveLockExpired should be accompanied by a valid target state'\n        iteration = handle_lock_expired(\n            target_state=target_state,\n            state_change=state_change,\n            channel_state=channel_state,\n            block_number=block_number,\n        )\n\n    sanity_check(\n        old_state=target_state,\n        new_state=iteration.new_state,\n        channel_state=channel_state,\n    )\n\n    return iteration", "response": "State machine for the target node of a mediated transfer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrap(data):\n    try:\n        cmdid = data[0]\n    except IndexError:\n        log.warning('data is empty')\n        return None\n\n    try:\n        message_type = CMDID_MESSAGE[cmdid]\n    except KeyError:\n        log.error('unknown cmdid %s', cmdid)\n        return None\n\n    try:\n        message = message_type(data)\n    except ValueError:\n        log.error('trying to decode invalid message')\n        return None\n\n    return message", "response": "Try to decode data into a message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef solidity_resolve_address(hex_code, library_symbol, library_address):\n    if library_address.startswith('0x'):\n        raise ValueError('Address should not contain the 0x prefix')\n\n    try:\n        decode_hex(library_address)\n    except TypeError:\n        raise ValueError(\n            'library_address contains invalid characters, it must be hex encoded.')\n\n    if len(library_symbol) != 40 or len(library_address) != 40:\n        raise ValueError('Address with wrong length')\n\n    return hex_code.replace(library_symbol, library_address)", "response": "Change the bytecode to use the given library address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solidity_library_symbol(library_name):\n    # the symbol is always 40 characters in length with the minimum of two\n    # leading and trailing underscores\n    length = min(len(library_name), 36)\n\n    library_piece = library_name[:length]\n    hold_piece = '_' * (36 - length)\n\n    return '__{library}{hold}__'.format(\n        library=library_piece,\n        hold=hold_piece,\n    )", "response": "Return the symbol used in the bytecode to represent the library_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompiling the files in the current working directory and return the compiled contracts", "response": "def compile_files_cwd(*args, **kwargs):\n    \"\"\"change working directory to contract's dir in order to avoid symbol\n    name conflicts\"\"\"\n    # get root directory of the contracts\n    compile_wd = os.path.commonprefix(args[0])\n    # edge case - compiling a single file\n    if os.path.isfile(compile_wd):\n        compile_wd = os.path.dirname(compile_wd)\n    # remove prefix from the files\n    if compile_wd[-1] != '/':\n        compile_wd += '/'\n    file_list = [\n        x.replace(compile_wd, '')\n        for x in args[0]\n    ]\n    cwd = os.getcwd()\n    try:\n        os.chdir(compile_wd)\n        compiled_contracts = compile_files(\n            source_files=file_list,\n            # We need to specify output values here because py-solc by default\n            # provides them all and does not know that \"clone-bin\" does not exist\n            # in solidity >= v0.5.0\n            output_values=('abi', 'asm', 'ast', 'bin', 'bin-runtime'),\n            **kwargs,\n        )\n    finally:\n        os.chdir(cwd)\n    return compiled_contracts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the jsondata contains the structure of a keystore file version 3.", "response": "def check_keystore_json(jsondata: Dict) -> bool:\n    \"\"\" Check if ``jsondata`` has the structure of a keystore file version 3.\n\n    Note that this test is not complete, e.g. it doesn't check key derivation or cipher parameters.\n    Copied from https://github.com/vbuterin/pybitcointools\n\n    Args:\n        jsondata: Dictionary containing the data from the json file\n\n    Returns:\n        `True` if the data appears to be valid, otherwise `False`\n    \"\"\"\n    if 'crypto' not in jsondata and 'Crypto' not in jsondata:\n        return False\n    if 'version' not in jsondata:\n        return False\n    if jsondata['version'] != 3:\n        return False\n\n    crypto = jsondata.get('crypto', jsondata.get('Crypto'))\n    if 'cipher' not in crypto:\n        return False\n    if 'ciphertext' not in crypto:\n        return False\n    if 'kdf' not in crypto:\n        return False\n    if 'mac' not in crypto:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the keystore file for an account unlock it and get the private key associated with the address", "response": "def get_privkey(self, address: AddressHex, password: str) -> PrivateKey:\n        \"\"\"Find the keystore file for an account, unlock it and get the private key\n\n        Args:\n            address: The Ethereum address for which to find the keyfile in the system\n            password: Mostly for testing purposes. A password can be provided\n                           as the function argument here. If it's not then the\n                           user is interactively queried for one.\n        Returns\n            The private key associated with the address\n        \"\"\"\n        address = add_0x_prefix(address).lower()\n\n        if not self.address_in_keystore(address):\n            raise ValueError('Keystore file not found for %s' % address)\n\n        with open(self.accounts[address]) as data_file:\n            data = json.load(data_file)\n\n        acc = Account(data, password, self.accounts[address])\n        return acc.privkey"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload an account from a keystore file.", "response": "def load(cls, path: str, password: str = None) -> 'Account':\n        \"\"\"Load an account from a keystore file.\n\n        Args:\n            path: full path to the keyfile\n            password: the password to decrypt the key file or `None` to leave it encrypted\n        \"\"\"\n        with open(path) as f:\n            keystore = json.load(f)\n        if not check_keystore_json(keystore):\n            raise ValueError('Invalid keystore file')\n        return Account(keystore, password, path=path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping the keystore for later disk storage.", "response": "def dump(self, include_address=True, include_id=True) -> str:\n        \"\"\"Dump the keystore for later disk storage.\n\n        The result inherits the entries `'crypto'` and `'version`' from `account.keystore`, and\n        adds `'address'` and `'id'` in accordance with the parameters `'include_address'` and\n        `'include_id`'.\n\n        If address or id are not known, they are not added, even if requested.\n\n        Args:\n            include_address: flag denoting if the address should be included or not\n            include_id: flag denoting if the id should be included or not\n        \"\"\"\n        d = {\n            'crypto': self.keystore['crypto'],\n            'version': self.keystore['version'],\n        }\n        if include_address and self.address is not None:\n            d['address'] = remove_0x_prefix(encode_hex(self.address))\n        if include_id and self.uuid is not None:\n            d['id'] = self.uuid\n        return json.dumps(d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunlocks the account with a password.", "response": "def unlock(self, password: str):\n        \"\"\"Unlock the account with a password.\n\n        If the account is already unlocked, nothing happens, even if the password is wrong.\n\n        Raises:\n            ValueError: (originating in ethereum.keys) if the password is wrong\n            (and the account is locked)\n        \"\"\"\n        if self.locked:\n            self._privkey = decode_keyfile_json(self.keystore, password.encode('UTF-8'))\n            self.locked = False\n            # get address such that it stays accessible after a subsequent lock\n            self._fill_address()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the UUID. Set it to None in order to remove it.", "response": "def uuid(self, value):\n        \"\"\"Set the UUID. Set it to `None` in order to remove it.\"\"\"\n        if value is not None:\n            self.keystore['id'] = value\n        elif 'id' in self.keystore:\n            self.keystore.pop('id')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recover_chain_id(storage: SQLiteStorage) -> ChainID:\n    action_init_chain = json.loads(storage.get_state_changes(limit=1, offset=0)[0])\n\n    assert action_init_chain['_type'] == 'raiden.transfer.state_change.ActionInitChain'\n    return action_init_chain['chain_id']", "response": "Return the chain_id from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef determine_run_number(self) -> int:\n        run_number = 0\n        run_number_file = self.data_path.joinpath('run_number.txt')\n        if run_number_file.exists():\n            run_number = int(run_number_file.read_text()) + 1\n        run_number_file.write_text(str(run_number))\n        log.info('Run number', run_number=run_number)\n        return run_number", "response": "Determine the current run number."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_chain(self, chain_urls: Dict[str, List[str]]) -> Tuple[str, List[str]]:\n        chain_name = self.scenario.chain_name\n        if chain_name in ('any', 'Any', 'ANY'):\n            chain_name = random.choice(list(chain_urls.keys()))\n\n        log.info('Using chain', chain=chain_name)\n        try:\n            return chain_name, chain_urls[chain_name]\n        except KeyError:\n            raise ScenarioError(\n                f'The scenario requested chain \"{chain_name}\" for which no RPC-URL is known.',\n            )", "response": "Select a chain and return its name and RPC - URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npacks the balance proof data to be signed by the given message type.", "response": "def pack_balance_proof(\n        nonce: Nonce,\n        balance_hash: BalanceHash,\n        additional_hash: AdditionalHash,\n        canonical_identifier: CanonicalIdentifier,\n        msg_type: MessageTypeId = MessageTypeId.BALANCE_PROOF,\n) -> bytes:\n    \"\"\"Packs balance proof data to be signed\n\n    Packs the given arguments in a byte array in the same configuration the\n    contracts expect the signed data to have.\n    \"\"\"\n    return pack_data([\n        'address',\n        'uint256',\n        'uint256',\n        'uint256',\n        'bytes32',\n        'uint256',\n        'bytes32',\n    ], [\n        canonical_identifier.token_network_address,\n        canonical_identifier.chain_identifier,\n        msg_type,\n        canonical_identifier.channel_identifier,\n        balance_hash,\n        nonce,\n        additional_hash,\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pack_balance_proof_update(\n        nonce: Nonce,\n        balance_hash: BalanceHash,\n        additional_hash: AdditionalHash,\n        canonical_identifier: CanonicalIdentifier,\n        partner_signature: Signature,\n) -> bytes:\n    \"\"\"Packs balance proof data to be signed for updateNonClosingBalanceProof\n\n    Packs the given arguments in a byte array in the same configuration the\n    contracts expect the signed data for updateNonClosingBalanceProof to have.\n    \"\"\"\n    return pack_balance_proof(\n        nonce=nonce,\n        balance_hash=balance_hash,\n        additional_hash=additional_hash,\n        canonical_identifier=canonical_identifier,\n        msg_type=MessageTypeId.BALANCE_PROOF_UPDATE,\n    ) + partner_signature", "response": "Packs the balance proof data to be signed for updateNonClosingBalanceProof\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a periodical Ping to the recipient to check its health.", "response": "def healthcheck(\n        transport: 'UDPTransport',\n        recipient: Address,\n        stop_event: Event,\n        event_healthy: Event,\n        event_unhealthy: Event,\n        nat_keepalive_retries: int,\n        nat_keepalive_timeout: int,\n        nat_invitation_timeout: int,\n        ping_nonce: Dict[str, Nonce],\n):\n    \"\"\" Sends a periodical Ping to `recipient` to check its health. \"\"\"\n    # pylint: disable=too-many-branches\n\n    log.debug(\n        'starting healthcheck for',\n        node=pex(transport.address),\n        to=pex(recipient),\n    )\n\n    # The state of the node is unknown, the events are set to allow the tasks\n    # to do work.\n    last_state = NODE_NETWORK_UNKNOWN\n    transport.set_node_network_state(\n        recipient,\n        last_state,\n    )\n\n    # Always call `clear` before `set`, since only `set` does context-switches\n    # it's easier to reason about tasks that are waiting on both events.\n\n    # Wait for the end-point registration or for the node to quit\n    try:\n        transport.get_host_port(recipient)\n    except UnknownAddress:\n        log.debug(\n            'waiting for endpoint registration',\n            node=pex(transport.address),\n            to=pex(recipient),\n        )\n\n        event_healthy.clear()\n        event_unhealthy.set()\n\n        backoff = udp_utils.timeout_exponential_backoff(\n            nat_keepalive_retries,\n            nat_keepalive_timeout,\n            nat_invitation_timeout,\n        )\n        sleep = next(backoff)\n\n        while not stop_event.wait(sleep):\n            try:\n                transport.get_host_port(recipient)\n            except UnknownAddress:\n                sleep = next(backoff)\n            else:\n                break\n\n    # Don't wait to send the first Ping and to start sending messages if the\n    # endpoint is known\n    sleep = 0\n    event_unhealthy.clear()\n    event_healthy.set()\n\n    while not stop_event.wait(sleep):\n        sleep = nat_keepalive_timeout\n\n        ping_nonce['nonce'] = Nonce(ping_nonce['nonce'] + 1)\n        messagedata = transport.get_ping(ping_nonce['nonce'])\n        message_id = ('ping', ping_nonce['nonce'], recipient)\n\n        # Send Ping a few times before setting the node as unreachable\n        acknowledged = udp_utils.retry(\n            transport,\n            messagedata,\n            message_id,\n            recipient,\n            stop_event,\n            [nat_keepalive_timeout] * nat_keepalive_retries,\n        )\n\n        if stop_event.is_set():\n            return\n\n        if not acknowledged:\n            log.debug(\n                'node is unresponsive',\n                node=pex(transport.address),\n                to=pex(recipient),\n                current_state=last_state,\n                new_state=NODE_NETWORK_UNREACHABLE,\n                retries=nat_keepalive_retries,\n                timeout=nat_keepalive_timeout,\n            )\n\n            # The node is not healthy, clear the event to stop all queue\n            # tasks\n            last_state = NODE_NETWORK_UNREACHABLE\n            transport.set_node_network_state(\n                recipient,\n                last_state,\n            )\n            event_healthy.clear()\n            event_unhealthy.set()\n\n            # Retry until recovery, used for:\n            # - Checking node status.\n            # - Nat punching.\n            acknowledged = udp_utils.retry(\n                transport,\n                messagedata,\n                message_id,\n                recipient,\n                stop_event,\n                repeat(nat_invitation_timeout),\n            )\n\n        if acknowledged:\n            current_state = views.get_node_network_status(\n                views.state_from_raiden(transport.raiden),\n                recipient,\n            )\n\n            if last_state != NODE_NETWORK_REACHABLE:\n                log.debug(\n                    'node answered',\n                    node=pex(transport.raiden.address),\n                    to=pex(recipient),\n                    current_state=current_state,\n                    new_state=NODE_NETWORK_REACHABLE,\n                )\n\n                last_state = NODE_NETWORK_REACHABLE\n                transport.set_node_network_state(\n                    recipient,\n                    last_state,\n                )\n                event_unhealthy.clear()\n                event_healthy.set()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_token_network(\n            self,\n            token_address: TokenAddress,\n            block_identifier: BlockSpecification = 'latest',\n    ) -> Optional[Address]:\n        \"\"\" Return the token network address for the given token or None if\n        there is no correspoding address.\n        \"\"\"\n        if not isinstance(token_address, T_TargetAddress):\n            raise ValueError('token_address must be an address')\n\n        address = self.proxy.contract.functions.token_to_token_networks(\n            to_checksum_address(token_address),\n        ).call(block_identifier=block_identifier)\n        address = to_canonical_address(address)\n\n        if is_same_address(address, NULL_ADDRESS):\n            return None\n\n        return address", "response": "Returns the token network address for the given token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_token_with_limits(\n            self,\n            token_address: TokenAddress,\n            channel_participant_deposit_limit: TokenAmount,\n            token_network_deposit_limit: TokenAmount,\n    ) -> Address:\n        \"\"\"\n        Register token of `token_address` with the token network.\n        The limits apply for version 0.13.0 and above of raiden-contracts,\n        since instantiation also takes the limits as constructor arguments.\n        \"\"\"\n        return self._add_token(\n            token_address=token_address,\n            additional_arguments={\n                '_channel_participant_deposit_limit': channel_participant_deposit_limit,\n                '_token_network_deposit_limit': token_network_deposit_limit,\n            },\n        )", "response": "Adds a token to the token network with limits."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a token with the token network without limits.", "response": "def add_token_without_limits(\n            self,\n            token_address: TokenAddress,\n    ) -> Address:\n        \"\"\"\n        Register token of `token_address` with the token network.\n        This applies for versions prior to 0.13.0 of raiden-contracts,\n        since limits were hardcoded into the TokenNetwork contract.\n        \"\"\"\n        return self._add_token(\n            token_address=token_address,\n            additional_arguments=dict(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator that returns a free TCP port number.", "response": "def get_free_port(\n        initial_port: int = 0,\n        socket_kind: SocketKind = SocketKind.SOCK_STREAM,\n        reliable: bool = True,\n):\n    \"\"\"\n    Find an unused TCP port.\n\n    Unless the `reliable` parameter is set to `True` (the default) this is prone to race\n    conditions - some other process may grab the port before the caller of this function has\n    a chance to use it.\n    When using `reliable` the port is forced into TIME_WAIT mode, ensuring that it will not be\n    considered 'free' by the OS for the next 60 seconds. This does however require that the\n    process using the port sets SO_REUSEADDR on it's sockets. Most 'server' applications do.\n\n    If `initial_port` is passed the function will try to find a port as close as possible.\n    Otherwise a random port is chosen by the OS.\n\n    Returns an iterator that will return unused port numbers.\n    \"\"\"\n\n    def _port_generator():\n        if initial_port == 0:\n            next_port = repeat(0)\n        else:\n            next_port = count(start=initial_port)\n\n        for port_candidate in next_port:\n            # Don't inline the variable until https://github.com/PyCQA/pylint/issues/1437 is fixed\n            sock = socket.socket(socket.AF_INET, socket_kind)\n            with closing(sock):\n                if reliable:\n                    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n                try:\n                    sock.bind(('127.0.0.1', port_candidate))\n                except OSError as ex:\n                    if ex.errno == errno.EADDRINUSE:\n                        continue\n\n                sock_addr = sock.getsockname()\n                port = sock_addr[1]\n\n                if reliable:\n                    # Connect to the socket to force it into TIME_WAIT state\n                    sock.listen(1)\n                    # see above\n                    sock2 = socket.socket(socket.AF_INET, socket_kind)\n                    with closing(sock2):\n                        sock2.connect(sock_addr)\n                        sock.accept()\n            yield port\n    return _port_generator()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_http_rtt(\n        url: str,\n        samples: int = 3,\n        method: str = 'head',\n        timeout: int = 1,\n) -> Optional[float]:\n    \"\"\"\n    Determine the average HTTP RTT to `url` over the number of `samples`.\n    Returns `None` if the server is unreachable.\n    \"\"\"\n    durations = []\n    for _ in range(samples):\n        try:\n            durations.append(\n                requests.request(method, url, timeout=timeout).elapsed.total_seconds(),\n            )\n        except (RequestException, OSError):\n            return None\n        except Exception as ex:\n            print(ex)\n            return None\n        # Slight delay to avoid overloading\n        sleep(.125)\n    return sum(durations) / samples", "response": "Get the average HTTP RTT to url over the number of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds blockhash to ContractReceiveXXX and ActionInitChain state changes", "response": "def _add_blockhash_to_state_changes(storage: SQLiteStorage, cache: BlockHashCache) -> None:\n    \"\"\"Adds blockhash to ContractReceiveXXX and ActionInitChain state changes\"\"\"\n\n    batch_size = 50\n    batch_query = storage.batch_query_state_changes(\n        batch_size=batch_size,\n        filters=[\n            ('_type', 'raiden.transfer.state_change.ContractReceive%'),\n            ('_type', 'raiden.transfer.state_change.ActionInitChain'),\n        ],\n        logical_and=False,\n    )\n    for state_changes_batch in batch_query:\n        # Gather query records to pass to gevent pool imap to have concurrent RPC calls\n        query_records = []\n        for state_change in state_changes_batch:\n            data = json.loads(state_change.data)\n            assert 'block_hash' not in data, 'v18 state changes cant contain blockhash'\n            record = BlockQueryAndUpdateRecord(\n                block_number=int(data['block_number']),\n                data=data,\n                state_change_identifier=state_change.state_change_identifier,\n                cache=cache,\n            )\n            query_records.append(record)\n\n        # Now perform the queries in parallel with gevent.Pool.imap and gather the\n        # updated tuple entries that will update the DB\n        updated_state_changes = []\n        pool_generator = Pool(batch_size).imap(\n            _query_blocknumber_and_update_statechange_data,\n            query_records,\n        )\n        for entry in pool_generator:\n            updated_state_changes.append(entry)\n\n        # Finally update the DB with a batched executemany()\n        storage.update_state_changes(updated_state_changes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds blockhash to all ContractSendXXX events", "response": "def _add_blockhash_to_events(storage: SQLiteStorage, cache: BlockHashCache) -> None:\n    \"\"\"Adds blockhash to all ContractSendXXX events\"\"\"\n    batch_query = storage.batch_query_event_records(\n        batch_size=500,\n        filters=[('_type', 'raiden.transfer.events.ContractSend%')],\n    )\n    for events_batch in batch_query:\n        updated_events = []\n        for event in events_batch:\n            data = json.loads(event.data)\n            assert 'triggered_by_block_hash' not in data, 'v18 events cant contain blockhash'\n            # Get the state_change that triggered the event and get its hash\n            matched_state_changes = storage.get_statechanges_by_identifier(\n                from_identifier=event.state_change_identifier,\n                to_identifier=event.state_change_identifier,\n            )\n            result_length = len(matched_state_changes)\n            msg = 'multiple state changes should not exist for the same identifier'\n            assert result_length == 1, msg\n\n            statechange_data = json.loads(matched_state_changes[0])\n            if 'block_hash' in statechange_data:\n                data['triggered_by_block_hash'] = statechange_data['block_hash']\n            elif 'block_number' in statechange_data:\n                block_number = int(statechange_data['block_number'])\n                data['triggered_by_block_hash'] = cache.get(block_number)\n\n            updated_events.append((\n                json.dumps(data),\n                event.event_identifier,\n            ))\n\n        storage.update_events(updated_events)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupgrade a single snapshot by adding the blockhash to it and to any pending transactions.", "response": "def _transform_snapshot(\n        raw_snapshot: str,\n        storage: SQLiteStorage,\n        cache: BlockHashCache,\n) -> str:\n    \"\"\"Upgrades a single snapshot by adding the blockhash to it and to any pending transactions\"\"\"\n    snapshot = json.loads(raw_snapshot)\n    block_number = int(snapshot['block_number'])\n    snapshot['block_hash'] = cache.get(block_number)\n\n    pending_transactions = snapshot['pending_transactions']\n    new_pending_transactions = []\n    for transaction_data in pending_transactions:\n        if 'raiden.transfer.events.ContractSend' not in transaction_data['_type']:\n            raise InvalidDBData(\n                \"Error during v18 -> v19 upgrade. Chain state's pending transactions \"\n                \"should only contain ContractSend transactions\",\n            )\n\n        # For each pending transaction find the corresponding DB event record.\n        event_record = storage.get_latest_event_by_data_field(\n            filters=transaction_data,\n        )\n        if not event_record.data:\n            raise InvalidDBData(\n                'Error during v18 -> v19 upgrade. Could not find a database event '\n                'table entry for a pending transaction.',\n            )\n\n        event_record_data = json.loads(event_record.data)\n        transaction_data['triggered_by_block_hash'] = event_record_data['triggered_by_block_hash']\n        new_pending_transactions.append(transaction_data)\n\n    snapshot['pending_transactions'] = new_pending_transactions\n    return json.dumps(snapshot)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupgrade the snapshots by adding the blockhash to it and to any pending transactions", "response": "def _transform_snapshots_for_blockhash(storage: SQLiteStorage, cache: BlockHashCache) -> None:\n    \"\"\"Upgrades the snapshots by adding the blockhash to it and to any pending transactions\"\"\"\n\n    snapshots = storage.get_snapshots()\n    snapshot_records = [\n        TransformSnapshotRecord(\n            data=snapshot.data,\n            identifier=snapshot.identifier,\n            storage=storage,\n            cache=cache,\n        )\n        for snapshot in snapshots\n    ]\n\n    pool_generator = Pool(len(snapshots)).imap(_do_transform_snapshot, snapshot_records)\n\n    updated_snapshots_data = []\n    for result in pool_generator:\n        updated_snapshots_data.append(result)\n\n    storage.update_snapshots(updated_snapshots_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a block number returns the hex representation of the blockhash", "response": "def get(self, block_number: BlockNumber) -> str:\n        \"\"\"Given a block number returns the hex representation of the blockhash\"\"\"\n        if block_number in self.mapping:\n            return self.mapping[block_number]\n\n        block_hash = self.web3.eth.getBlock(block_number)['hash']\n        block_hash = block_hash.hex()\n        self.mapping[block_number] = block_hash\n        return block_hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconsumes the specified number of transport tokens.", "response": "def consume(self, tokens):\n        \"\"\"Consume tokens.\n        Args:\n            tokens (float): number of transport tokens to consume\n        Returns:\n            wait_time (float): waiting time for the consumer\n        \"\"\"\n        wait_time = 0.\n        self.tokens -= tokens\n        if self.tokens < 0:\n            self._get_tokens()\n        if self.tokens < 0:\n            wait_time = -self.tokens / self.fill_rate\n        return wait_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the account has enough gas reservation for the remaining lifecycles of the remaining channels.", "response": "def has_enough_gas_reserve(\n        raiden,\n        channels_to_open: int = 0,\n) -> Tuple[bool, int]:\n    \"\"\" Checks if the account has enough balance to handle the lifecycles of all\n    open channels as well as the to be created channels.\n\n    Note: This is just an estimation.\n\n    Args:\n        raiden: A raiden service instance\n        channels_to_open: The number of new channels that should be opened\n\n    Returns:\n        Tuple of a boolean denoting if the account has enough balance for\n        the remaining lifecycle events and the estimate for the remaining\n        lifecycle cost\n    \"\"\"\n    secure_reserve_estimate = get_reserve_estimate(raiden, channels_to_open)\n    current_account_balance = raiden.chain.client.balance(raiden.chain.client.address)\n\n    return secure_reserve_estimate <= current_account_balance, secure_reserve_estimate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hash_pair(first: Keccak256, second: Optional[Keccak256]) -> Keccak256:\n    assert first is not None\n\n    if second is None:\n        return first\n\n    if first > second:\n        return sha3(second + first)\n\n    return sha3(first + second)", "response": "Computes the keccak hash of the elements ordered topologically."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_layers(elements: List[Keccak256]) -> List[List[Keccak256]]:\n\n    elements = list(elements)  # consume generators\n    assert elements, 'Use make_empty_merkle_tree if there are no elements'\n\n    if not all(isinstance(item, bytes) for item in elements):\n        raise ValueError('all elements must be bytes')\n\n    if any(len(item) != 32 for item in elements):\n        raise HashLengthNot32()\n\n    if len(elements) != len(set(elements)):\n        raise ValueError('Duplicated element')\n\n    leaves = sorted(item for item in elements)\n    tree = [leaves]\n\n    layer = leaves\n    while len(layer) > 1:\n        paired_items = split_in_pairs(layer)\n        layer = [hash_pair(a, b) for a, b in paired_items]\n        tree.append(layer)\n\n    return tree", "response": "Computes the layers of the merkletree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_merkleproof_for(merkletree: 'MerkleTreeState', element: Keccak256) -> List[Keccak256]:\n    idx = merkletree.layers[LEAVES].index(element)\n\n    proof = []\n    for layer in merkletree.layers:\n        if idx % 2:\n            pair = idx - 1\n        else:\n            pair = idx + 1\n\n        # with an odd number of elements the rightmost one does not have a pair.\n        if pair < len(layer):\n            proof.append(layer[pair])\n\n        # the tree is binary and balanced\n        idx = idx // 2\n\n    return proof", "response": "Computes the containment proof for the given element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that leaf_element was contained in the tree represented by merkleroot.", "response": "def validate_proof(proof: List[Keccak256], root: Keccak256, leaf_element: Keccak256) -> bool:\n    \"\"\" Checks that `leaf_element` was contained in the tree represented by\n    `merkleroot`.\n    \"\"\"\n\n    hash_ = leaf_element\n    for pair in proof:\n        hash_ = hash_pair(hash_, pair)\n\n    return hash_ == root"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the root element of the merkle tree.", "response": "def merkleroot(merkletree: 'MerkleTreeState') -> Locksroot:\n    \"\"\" Return the root element of the merkle tree. \"\"\"\n    assert merkletree.layers, 'the merkle tree layers are empty'\n    assert merkletree.layers[MERKLEROOT], 'the root layer is empty'\n\n    return Locksroot(merkletree.layers[MERKLEROOT][0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds onchain_locksroot to our_state and partner_state objects.", "response": "def _add_onchain_locksroot_to_channel_new_state_changes(\n        storage: SQLiteStorage,\n) -> None:\n    \"\"\" Adds `onchain_locksroot` to our_state/partner_state in\n    ContractReceiveChannelNew's channel_state object. \"\"\"\n    batch_size = 50\n    batch_query = storage.batch_query_state_changes(\n        batch_size=batch_size,\n        filters=[\n            ('_type', 'raiden.transfer.state_change.ContractReceiveChannelNew'),\n        ],\n    )\n    for state_changes_batch in batch_query:\n        updated_state_changes = list()\n        for state_change in state_changes_batch:\n            state_change_data = json.loads(state_change.data)\n\n            channel_state = state_change_data['channel_state']\n            msg = 'v18 state changes cant contain onchain_locksroot'\n            assert 'onchain_locksroot' not in channel_state['our_state'], msg\n\n            msg = 'v18 state changes cant contain onchain_locksroot'\n            assert 'onchain_locksroot' not in channel_state['partner_state'], msg\n\n            channel_state['our_state']['onchain_locksroot'] = serialize_bytes(EMPTY_MERKLE_ROOT)\n            channel_state['partner_state']['onchain_locksroot'] = serialize_bytes(\n                EMPTY_MERKLE_ROOT,\n            )\n\n            updated_state_changes.append((\n                json.dumps(state_change_data),\n                state_change.state_change_identifier,\n            ))\n        storage.update_state_changes(updated_state_changes)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd our_onchain_locksroot and partner_onchain_locksroot to ContractReceiveChannelSettled.", "response": "def _add_onchain_locksroot_to_channel_settled_state_changes(\n        raiden: RaidenService,\n        storage: SQLiteStorage,\n) -> None:\n    \"\"\" Adds `our_onchain_locksroot` and `partner_onchain_locksroot` to\n    ContractReceiveChannelSettled. \"\"\"\n    batch_size = 50\n    batch_query = storage.batch_query_state_changes(\n        batch_size=batch_size,\n        filters=[\n            ('_type', 'raiden.transfer.state_change.ContractReceiveChannelSettled'),\n        ],\n    )\n    for state_changes_batch in batch_query:\n        updated_state_changes = list()\n        for state_change in state_changes_batch:\n            state_change_data = json.loads(state_change.data)\n            msg = 'v18 state changes cant contain our_onchain_locksroot'\n            assert 'our_onchain_locksroot' not in state_change_data, msg\n\n            msg = 'v18 state changes cant contain partner_onchain_locksroot'\n            assert 'partner_onchain_locksroot' not in state_change_data, msg\n\n            token_network_identifier = state_change_data['token_network_identifier']\n            channel_identifier = state_change_data['channel_identifier']\n\n            channel_new_state_change = _find_channel_new_state_change(\n                storage=storage,\n                token_network_address=token_network_identifier,\n                channel_identifier=channel_identifier,\n            )\n\n            if not channel_new_state_change.data:\n                raise RaidenUnrecoverableError(\n                    f'Could not find the state change for channel {channel_identifier}, '\n                    f'token network address: {token_network_identifier} being created. ',\n                )\n\n            channel_state_data = json.loads(channel_new_state_change.data)\n            new_channel_state = channel_state_data['channel_state']\n\n            canonical_identifier = CanonicalIdentifier(\n                chain_identifier=-1,\n                token_network_address=to_canonical_address(token_network_identifier),\n                channel_identifier=int(channel_identifier),\n            )\n            our_locksroot, partner_locksroot = get_onchain_locksroots(\n                chain=raiden.chain,\n                canonical_identifier=canonical_identifier,\n                participant1=to_canonical_address(new_channel_state['our_state']['address']),\n                participant2=to_canonical_address(new_channel_state['partner_state']['address']),\n                block_identifier='latest',\n            )\n\n            state_change_data['our_onchain_locksroot'] = serialize_bytes(\n                our_locksroot,\n            )\n            state_change_data['partner_onchain_locksroot'] = serialize_bytes(\n                partner_locksroot,\n            )\n\n            updated_state_changes.append((\n                json.dumps(state_change_data),\n                state_change.state_change_identifier,\n            ))\n        storage.update_state_changes(updated_state_changes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding onchain_locksroot to each NettingChannelEndState", "response": "def _add_onchain_locksroot_to_snapshot(\n        raiden: RaidenService,\n        storage: SQLiteStorage,\n        snapshot_record: StateChangeRecord,\n) -> str:\n    \"\"\"\n    Add `onchain_locksroot` to each NettingChannelEndState\n    \"\"\"\n    snapshot = json.loads(snapshot_record.data)\n\n    for payment_network in snapshot.get('identifiers_to_paymentnetworks', dict()).values():\n        for token_network in payment_network.get('tokennetworks', list()):\n            channelidentifiers_to_channels = token_network.get(\n                'channelidentifiers_to_channels',\n                dict(),\n            )\n            for channel in channelidentifiers_to_channels.values():\n                our_locksroot, partner_locksroot = _get_onchain_locksroots(\n                    raiden=raiden,\n                    storage=storage,\n                    token_network=token_network,\n                    channel=channel,\n                )\n                channel['our_state']['onchain_locksroot'] = serialize_bytes(our_locksroot)\n                channel['partner_state']['onchain_locksroot'] = serialize_bytes(partner_locksroot)\n\n    return json.dumps(snapshot, indent=4), snapshot_record.identifier"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _chain(first_func, *funcs) -> Callable:\n    @wraps(first_func)\n    def wrapper(*args, **kwargs):\n        result = first_func(*args, **kwargs)\n        for func in funcs:\n            result = func(result)\n        return result\n    return wrapper", "response": "Chain a given number of functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds greenlet_name to the event dict for greenlets that do not have a non - default name.", "response": "def add_greenlet_name(\n        _logger: str,\n        _method_name: str,\n        event_dict: Dict[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"Add greenlet_name to the event dict for greenlets that have a non-default name.\"\"\"\n    current_greenlet = gevent.getcurrent()\n    greenlet_name = getattr(current_greenlet, 'name', None)\n    if greenlet_name is not None and not greenlet_name.startswith('Greenlet-'):\n        event_dict['greenlet_name'] = greenlet_name\n    return event_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a function which transforms a str replacing all matches for its replacement", "response": "def redactor(blacklist: Dict[Pattern, str]) -> Callable[[str], str]:\n    \"\"\"Returns a function which transforms a str, replacing all matches for its replacement\"\"\"\n    def processor_wrapper(msg: str) -> str:\n        for regex, repl in blacklist.items():\n            if repl is None:\n                repl = '<redacted>'\n            msg = regex.sub(repl, msg)\n        return msg\n    return processor_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _wrap_tracebackexception_format(redact: Callable[[str], str]):\n    original_format = getattr(TracebackException, '_original', None)\n    if original_format is None:\n        original_format = TracebackException.format\n        setattr(TracebackException, '_original', original_format)\n\n    @wraps(original_format)\n    def tracebackexception_format(self, *, chain=True):\n        for line in original_format(self, chain=chain):\n            yield redact(line)\n\n    setattr(TracebackException, 'format', tracebackexception_format)", "response": "Monkey - patch TracebackException. format to redact printed lines."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef should_log(self, logger_name: str, level: str) -> bool:\n        if (logger_name, level) not in self._should_log:\n            log_level_per_rule = self._get_log_level(logger_name)\n            log_level_per_rule_numeric = getattr(logging, log_level_per_rule.upper(), 10)\n            log_level_event_numeric = getattr(logging, level.upper(), 10)\n\n            should_log = log_level_event_numeric >= log_level_per_rule_numeric\n            self._should_log[(logger_name, level)] = should_log\n        return self._should_log[(logger_name, level)]", "response": "Returns True if a message for the logger should be logged."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _transform_snapshot(raw_snapshot: str) -> str:\n    snapshot = json.loads(raw_snapshot)\n\n    for task in snapshot['payment_mapping']['secrethashes_to_task'].values():\n        if 'raiden.transfer.state.InitiatorTask' in task['_type']:\n            for initiator in task['manager_state']['initiator_transfers'].values():\n                msg = 'v20 initiator task should not contain allocated_fee'\n                assert 'allocated_fee' not in initiator['transfer_description'], msg\n                initiator['transfer_description']['allocated_fee'] = '0'\n\n    msg = 'v20 snapshots should not contain tokennetworkaddresses_to_paymentnetworkaddresses'\n    assert 'tokennetworkaddresses_to_paymentnetworkaddresses' not in snapshot, msg\n    ids_to_addrs = dict()\n    for payment_network in snapshot['identifiers_to_paymentnetworks'].values():\n        for token_network in payment_network['tokennetworks']:\n            ids_to_addrs[token_network['address']] = payment_network['address']\n    snapshot['tokennetworkaddresses_to_paymentnetworkaddresses'] = ids_to_addrs\n\n    for payment_network in snapshot['identifiers_to_paymentnetworks'].values():\n        for token_network in payment_network['tokennetworks']:\n            for channel_state in token_network['channelidentifiers_to_channels'].values():\n                msg = 'v20 channel state should not contain mediation_fee'\n                assert 'mediation_fee' not in channel_state, msg\n                channel_state['mediation_fee'] = '0'\n\n    return json.dumps(snapshot)", "response": "Transform a snapshot to a v20 tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate each ContractReceiveChannelNew s channel_state member by setting the medation_fee that was added to the NettingChannelState", "response": "def _update_statechanges(storage: SQLiteStorage):\n    \"\"\"\n    Update each ContractReceiveChannelNew's channel_state member\n    by setting the `mediation_fee` that was added to the NettingChannelState\n    \"\"\"\n    batch_size = 50\n    batch_query = storage.batch_query_state_changes(\n        batch_size=batch_size,\n        filters=[\n            ('_type', 'raiden.transfer.state_change.ContractReceiveChannelNew'),\n        ],\n    )\n\n    for state_changes_batch in batch_query:\n        updated_state_changes = list()\n        for state_change in state_changes_batch:\n            data = json.loads(state_change.data)\n            msg = 'v20 ContractReceiveChannelNew channel state should not contain medation_fee'\n            assert 'mediation_fee' not in data['channel_state'], msg\n            data['channel_state']['mediation_fee'] = '0'\n\n            updated_state_changes.append((\n                json.dumps(data),\n                state_change.state_change_identifier,\n            ))\n\n        storage.update_state_changes(updated_state_changes)\n\n    batch_query = storage.batch_query_state_changes(\n        batch_size=batch_size,\n        filters=[\n            ('_type', 'raiden.transfer.mediated_transfer.state_change.ActionInitInitiator'),\n        ],\n    )\n\n    for state_changes_batch in batch_query:\n        updated_state_changes = list()\n        for state_change in state_changes_batch:\n            data = json.loads(state_change.data)\n            msg = 'v20 ActionInitInitiator transfer should not contain allocated_fee'\n            assert 'allocated_fee' not in data['transfer'], msg\n            data['transfer']['allocated_fee'] = '0'\n\n            updated_state_changes.append((\n                json.dumps(data),\n                state_change.state_change_identifier,\n            ))\n\n        storage.update_state_changes(updated_state_changes)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_contract_events(\n        chain: BlockChainService,\n        abi: Dict,\n        contract_address: Address,\n        topics: Optional[List[str]],\n        from_block: BlockSpecification,\n        to_block: BlockSpecification,\n) -> List[Dict]:\n    \"\"\" Query the blockchain for all events of the smart contract at\n    `contract_address` that match the filters `topics`, `from_block`, and\n    `to_block`.\n    \"\"\"\n    verify_block_number(from_block, 'from_block')\n    verify_block_number(to_block, 'to_block')\n    events = chain.client.get_filter_events(\n        contract_address,\n        topics=topics,\n        from_block=from_block,\n        to_block=to_block,\n    )\n\n    result = []\n    for event in events:\n        decoded_event = dict(decode_event(abi, event))\n        if event.get('blockNumber'):\n            decoded_event['block_number'] = event['blockNumber']\n            del decoded_event['blockNumber']\n        result.append(decoded_event)\n    return result", "response": "Get all events of the smart contract at\n    contract_address that match the filters topics from_block and to_block."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_token_network_events(\n        chain: BlockChainService,\n        token_network_address: Address,\n        contract_manager: ContractManager,\n        events: Optional[List[str]] = ALL_EVENTS,\n        from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,\n        to_block: BlockSpecification = 'latest',\n) -> List[Dict]:\n    \"\"\" Helper to get all events of the ChannelManagerContract at `token_address`. \"\"\"\n\n    return get_contract_events(\n        chain,\n        contract_manager.get_contract_abi(CONTRACT_TOKEN_NETWORK),\n        token_network_address,\n        events,\n        from_block,\n        to_block,\n    )", "response": "Helper to get all events of the ChannelManagerContract at the given token_address."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_event_to_internal(abi, log_event):\n    # Note: All addresses inside the event_data must be decoded.\n\n    decoded_event = decode_event(abi, log_event)\n\n    if not decoded_event:\n        raise UnknownEventType()\n\n    # copy the attribute dict because that data structure is immutable\n    data = dict(decoded_event)\n    args = dict(data['args'])\n\n    data['args'] = args\n    # translate from web3's to raiden's name convention\n    data['block_number'] = log_event.pop('blockNumber')\n    data['transaction_hash'] = log_event.pop('transactionHash')\n    data['block_hash'] = bytes(log_event.pop('blockHash'))\n\n    assert data['block_number'], 'The event must have the block_number'\n    assert data['transaction_hash'], 'The event must have the transaction hash field'\n\n    event = data['event']\n    if event == EVENT_TOKEN_NETWORK_CREATED:\n        args['token_network_address'] = to_canonical_address(args['token_network_address'])\n        args['token_address'] = to_canonical_address(args['token_address'])\n\n    elif event == ChannelEvent.OPENED:\n        args['participant1'] = to_canonical_address(args['participant1'])\n        args['participant2'] = to_canonical_address(args['participant2'])\n\n    elif event == ChannelEvent.DEPOSIT:\n        args['participant'] = to_canonical_address(args['participant'])\n\n    elif event == ChannelEvent.BALANCE_PROOF_UPDATED:\n        args['closing_participant'] = to_canonical_address(args['closing_participant'])\n\n    elif event == ChannelEvent.CLOSED:\n        args['closing_participant'] = to_canonical_address(args['closing_participant'])\n\n    elif event == ChannelEvent.UNLOCKED:\n        args['participant'] = to_canonical_address(args['participant'])\n        args['partner'] = to_canonical_address(args['partner'])\n\n    return Event(\n        originating_contract=to_canonical_address(log_event['address']),\n        event_data=data,\n    )", "response": "Decode a log event into a new internal version of the internal version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npolling for new blockchain events up to block_number.", "response": "def poll_blockchain_events(self, block_number: typing.BlockNumber):\n        \"\"\" Poll for new blockchain events up to `block_number`. \"\"\"\n\n        for event_listener in self.event_listeners:\n            assert isinstance(event_listener.filter, StatelessFilter)\n\n            for log_event in event_listener.filter.get_new_entries(block_number):\n                yield decode_event_to_internal(event_listener.abi, log_event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_accounts(seeds):\n    return {\n        seed: {\n            'privatekey': encode_hex(sha3(seed)),\n            'address': encode_hex(privatekey_to_address(sha3(seed))),\n        }\n        for seed in seeds\n    }", "response": "Create private keys and addresses for all seeds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mk_genesis(accounts, initial_alloc=denoms.ether * 100000000):\n    genesis = GENESIS_STUB.copy()\n    genesis['extraData'] = encode_hex(CLUSTER_NAME)\n    genesis['alloc'].update({\n        account: {\n            'balance': str(initial_alloc),\n        }\n        for account in accounts\n    })\n    # add the one-privatekey account (\"1\" * 64) for convenience\n    genesis['alloc']['19e7e376e7c213b7e7e7e46cc70a5dd086daff2a'] = dict(balance=str(initial_alloc))\n    return genesis", "response": "Create a genesis - block dict with allocation for all accounts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the token of this manager.", "response": "def token_address(self) -> Address:\n        \"\"\" Return the token of this manager. \"\"\"\n        return to_canonical_address(self.proxy.contract.functions.token().call())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_netting_channel(\n            self,\n            partner: Address,\n            settle_timeout: int,\n            given_block_identifier: BlockSpecification,\n    ) -> ChannelID:\n        \"\"\" Creates a new channel in the TokenNetwork contract.\n\n        Args:\n            partner: The peer to open the channel with.\n            settle_timeout: The settle timeout to use for this channel.\n            given_block_identifier: The block identifier of the state change that\n                                    prompted this proxy action\n\n        Returns:\n            The ChannelID of the new netting channel.\n        \"\"\"\n        checking_block = self.client.get_checking_block()\n        self._new_channel_preconditions(\n            partner=partner,\n            settle_timeout=settle_timeout,\n            block_identifier=given_block_identifier,\n        )\n        log_details = {\n            'peer1': pex(self.node_address),\n            'peer2': pex(partner),\n        }\n        gas_limit = self.proxy.estimate_gas(\n            checking_block,\n            'openChannel',\n            participant1=self.node_address,\n            participant2=partner,\n            settle_timeout=settle_timeout,\n        )\n        if not gas_limit:\n            self.proxy.jsonrpc_client.check_for_insufficient_eth(\n                transaction_name='openChannel',\n                transaction_executed=False,\n                required_gas=GAS_REQUIRED_FOR_OPEN_CHANNEL,\n                block_identifier=checking_block,\n            )\n            self._new_channel_postconditions(\n                partner=partner,\n                block=checking_block,\n            )\n\n            log.critical('new_netting_channel call will fail', **log_details)\n            raise RaidenUnrecoverableError('Creating a new channel will fail')\n\n        log.debug('new_netting_channel called', **log_details)\n        # Prevent concurrent attempts to open a channel with the same token and\n        # partner address.\n        if gas_limit and partner not in self.open_channel_transactions:\n            new_open_channel_transaction = AsyncResult()\n            self.open_channel_transactions[partner] = new_open_channel_transaction\n            gas_limit = safe_gas_limit(gas_limit, GAS_REQUIRED_FOR_OPEN_CHANNEL)\n            try:\n                transaction_hash = self.proxy.transact(\n                    'openChannel',\n                    gas_limit,\n                    participant1=self.node_address,\n                    participant2=partner,\n                    settle_timeout=settle_timeout,\n                )\n                self.client.poll(transaction_hash)\n                receipt_or_none = check_transaction_threw(self.client, transaction_hash)\n                if receipt_or_none:\n                    self._new_channel_postconditions(\n                        partner=partner,\n                        block=receipt_or_none['blockNumber'],\n                    )\n                    log.critical('new_netting_channel failed', **log_details)\n                    raise RaidenUnrecoverableError('creating new channel failed')\n\n            except Exception as e:\n                log.critical('new_netting_channel failed', **log_details)\n                new_open_channel_transaction.set_exception(e)\n                raise\n            else:\n                new_open_channel_transaction.set(transaction_hash)\n            finally:\n                self.open_channel_transactions.pop(partner, None)\n        else:\n            # All other concurrent threads should block on the result of opening this channel\n            self.open_channel_transactions[partner].get()\n\n        channel_identifier: ChannelID = self._detail_channel(\n            participant1=self.node_address,\n            participant2=partner,\n            block_identifier='latest',\n        ).channel_identifier\n        log_details['channel_identifier'] = str(channel_identifier)\n        log.info('new_netting_channel successful', **log_details)\n\n        return channel_identifier", "response": "Creates a new channel in TokenNetwork contract with the given partner."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _channel_exists_and_not_settled(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID = None,\n    ) -> bool:\n        \"\"\"Returns if the channel exists and is in a non-settled state\"\"\"\n        try:\n            channel_state = self._get_channel_state(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return False\n        exists_and_not_settled = (\n            channel_state > ChannelState.NONEXISTENT and\n            channel_state < ChannelState.SETTLED\n        )\n        return exists_and_not_settled", "response": "Returns if the channel exists and is not settled."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _detail_participant(\n            self,\n            channel_identifier: ChannelID,\n            participant: Address,\n            partner: Address,\n            block_identifier: BlockSpecification,\n    ) -> ParticipantDetails:\n        \"\"\" Returns a dictionary with the channel participant information. \"\"\"\n\n        data = self._call_and_check_result(\n            block_identifier,\n            'getChannelParticipantInfo',\n            channel_identifier=channel_identifier,\n            participant=to_checksum_address(participant),\n            partner=to_checksum_address(partner),\n        )\n        return ParticipantDetails(\n            address=participant,\n            deposit=data[ParticipantInfoIndex.DEPOSIT],\n            withdrawn=data[ParticipantInfoIndex.WITHDRAWN],\n            is_closer=data[ParticipantInfoIndex.IS_CLOSER],\n            balance_hash=data[ParticipantInfoIndex.BALANCE_HASH],\n            nonce=data[ParticipantInfoIndex.NONCE],\n            locksroot=data[ParticipantInfoIndex.LOCKSROOT],\n            locked_amount=data[ParticipantInfoIndex.LOCKED_AMOUNT],\n        )", "response": "Returns a dictionary with the channel participant information."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a ChannelData instance with the channel specific information.", "response": "def _detail_channel(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID = None,\n    ) -> ChannelData:\n        \"\"\" Returns a ChannelData instance with the channel specific information.\n\n        If no specific channel_identifier is given then it tries to see if there\n        is a currently open channel and uses that identifier.\n\n        \"\"\"\n        channel_identifier = self._inspect_channel_identifier(\n            participant1=participant1,\n            participant2=participant2,\n            called_by_fn='_detail_channel(',\n            block_identifier=block_identifier,\n            channel_identifier=channel_identifier,\n        )\n\n        channel_data = self._call_and_check_result(\n            block_identifier,\n            'getChannelInfo',\n            channel_identifier=channel_identifier,\n            participant1=to_checksum_address(participant1),\n            participant2=to_checksum_address(participant2),\n        )\n\n        return ChannelData(\n            channel_identifier=channel_identifier,\n            settle_block_number=channel_data[ChannelInfoIndex.SETTLE_BLOCK],\n            state=channel_data[ChannelInfoIndex.STATE],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detail_participants(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID = None,\n    ) -> ParticipantsDetails:\n        \"\"\" Returns a ParticipantsDetails instance with the participants'\n            channel information.\n\n        Note:\n            For now one of the participants has to be the node_address\n        \"\"\"\n        if self.node_address not in (participant1, participant2):\n            raise ValueError('One participant must be the node address')\n\n        if self.node_address == participant2:\n            participant1, participant2 = participant2, participant1\n\n        channel_identifier = self._inspect_channel_identifier(\n            participant1=participant1,\n            participant2=participant2,\n            called_by_fn='details_participants',\n            block_identifier=block_identifier,\n            channel_identifier=channel_identifier,\n        )\n\n        our_data = self._detail_participant(\n            channel_identifier=channel_identifier,\n            participant=participant1,\n            partner=participant2,\n            block_identifier=block_identifier,\n        )\n        partner_data = self._detail_participant(\n            channel_identifier=channel_identifier,\n            participant=participant2,\n            partner=participant1,\n            block_identifier=block_identifier,\n        )\n        return ParticipantsDetails(our_details=our_data, partner_details=partner_data)", "response": "Returns a ParticipantsDetails instance with the participants' channel information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detail(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID = None,\n    ) -> ChannelDetails:\n        \"\"\" Returns a ChannelDetails instance with all the details of the\n            channel and the channel participants.\n\n        Note:\n            For now one of the participants has to be the node_address\n        \"\"\"\n        if self.node_address not in (participant1, participant2):\n            raise ValueError('One participant must be the node address')\n\n        if self.node_address == participant2:\n            participant1, participant2 = participant2, participant1\n\n        channel_data = self._detail_channel(\n            participant1=participant1,\n            participant2=participant2,\n            block_identifier=block_identifier,\n            channel_identifier=channel_identifier,\n        )\n        participants_data = self.detail_participants(\n            participant1=participant1,\n            participant2=participant2,\n            block_identifier=block_identifier,\n            channel_identifier=channel_data.channel_identifier,\n        )\n        chain_id = self.proxy.contract.functions.chain_id().call()\n\n        return ChannelDetails(\n            chain_id=chain_id,\n            channel_data=channel_data,\n            participants_data=participants_data,\n        )", "response": "Returns a ChannelDetails instance with all the details of the channel and the channel participants."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns true if the channel is in an open state.", "response": "def channel_is_opened(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ) -> bool:\n        \"\"\" Returns true if the channel is in an open state, false otherwise. \"\"\"\n        try:\n            channel_state = self._get_channel_state(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return False\n        return channel_state == ChannelState.OPENED"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns true if the channel is in a closed state.", "response": "def channel_is_closed(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ) -> bool:\n        \"\"\" Returns true if the channel is in a closed state, false otherwise. \"\"\"\n        try:\n            channel_state = self._get_channel_state(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return False\n        return channel_state == ChannelState.CLOSED"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef channel_is_settled(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ) -> bool:\n        \"\"\" Returns true if the channel is in a settled state, false otherwise. \"\"\"\n        try:\n            channel_state = self._get_channel_state(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return False\n        return channel_state >= ChannelState.SETTLED", "response": "Returns true if the channel is in settled state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef closing_address(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID = None,\n    ) -> Optional[Address]:\n        \"\"\" Returns the address of the closer, if the channel is closed and not settled. None\n        otherwise. \"\"\"\n\n        try:\n            channel_data = self._detail_channel(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return None\n\n        if channel_data.state >= ChannelState.SETTLED:\n            return None\n\n        participants_data = self.detail_participants(\n            participant1=participant1,\n            participant2=participant2,\n            block_identifier=block_identifier,\n            channel_identifier=channel_data.channel_identifier,\n        )\n\n        if participants_data.our_details.is_closer:\n            return participants_data.our_details.address\n        elif participants_data.partner_details.is_closer:\n            return participants_data.partner_details.address\n\n        return None", "response": "Returns the address of the closer if the channel is closed and settled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the channel is opened and the node has deposit in it.", "response": "def can_transfer(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ) -> bool:\n        \"\"\" Returns True if the channel is opened and the node has deposit in\n        it.\n\n        Note: Having a deposit does not imply having a balance for off-chain\n        transfers. \"\"\"\n        opened = self.channel_is_opened(\n            participant1=participant1,\n            participant2=participant2,\n            block_identifier=block_identifier,\n            channel_identifier=channel_identifier,\n        )\n\n        if opened is False:\n            return False\n\n        deposit = self._detail_participant(\n            channel_identifier=channel_identifier,\n            participant=participant1,\n            partner=participant2,\n            block_identifier=block_identifier,\n        ).deposit\n        return deposit > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the total deposit of a channel.", "response": "def set_total_deposit(\n            self,\n            given_block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n            total_deposit: TokenAmount,\n            partner: Address,\n    ):\n        \"\"\" Set channel's total deposit.\n\n        `total_deposit` has to be monotonically increasing, this is enforced by\n        the `TokenNetwork` smart contract. This is done for the same reason why\n        the balance proofs have a monotonically increasing transferred amount,\n        it simplifies the analysis of bad behavior and the handling code of\n        out-dated balance proofs.\n\n        Races to `set_total_deposit` are handled by the smart contract, where\n        largest total deposit wins. The end balance of the funding accounts is\n        undefined. E.g.\n\n        - Acc1 calls set_total_deposit with 10 tokens\n        - Acc2 calls set_total_deposit with 13 tokens\n\n        - If Acc2's transaction is mined first, then Acc1 token supply is left intact.\n        - If Acc1's transaction is mined first, then Acc2 will only move 3 tokens.\n\n        Races for the same account don't have any unexpeted side-effect.\n\n        Raises:\n            DepositMismatch: If the new request total deposit is lower than the\n                existing total deposit on-chain for the `given_block_identifier`.\n            RaidenRecoverableError: If the channel was closed meanwhile the\n                deposit was in transit.\n            RaidenUnrecoverableError: If the transaction was sucessful and the\n                deposit_amount is not as large as the requested value.\n            RuntimeError: If the token address is empty.\n            ValueError: If an argument is of the invalid type.\n        \"\"\"\n        if not isinstance(total_deposit, int):\n            raise ValueError('total_deposit needs to be an integer number.')\n        token_address = self.token_address()\n        token = Token(\n            jsonrpc_client=self.client,\n            token_address=token_address,\n            contract_manager=self.contract_manager,\n        )\n        checking_block = self.client.get_checking_block()\n        error_prefix = 'setTotalDeposit call will fail'\n        with self.channel_operations_lock[partner], self.deposit_lock:\n            previous_total_deposit = self._detail_participant(\n                channel_identifier=channel_identifier,\n                participant=self.node_address,\n                partner=partner,\n                block_identifier=given_block_identifier,\n            ).deposit\n            amount_to_deposit = TokenAmount(total_deposit - previous_total_deposit)\n            log_details = {\n                'token_network': pex(self.address),\n                'channel_identifier': channel_identifier,\n                'node': pex(self.node_address),\n                'partner': pex(partner),\n                'new_total_deposit': total_deposit,\n                'previous_total_deposit': previous_total_deposit,\n            }\n            try:\n                self._deposit_preconditions(\n                    channel_identifier=channel_identifier,\n                    total_deposit=total_deposit,\n                    partner=partner,\n                    token=token,\n                    previous_total_deposit=previous_total_deposit,\n                    log_details=log_details,\n                    block_identifier=given_block_identifier,\n                )\n            except NoStateForBlockIdentifier:\n                # If preconditions end up being on pruned state skip them. Estimate\n                # gas will stop us from sending a transaction that will fail\n                pass\n\n            # If there are channels being set up concurrenlty either the\n            # allowance must be accumulated *or* the calls to `approve` and\n            # `setTotalDeposit` must be serialized. This is necessary otherwise\n            # the deposit will fail.\n            #\n            # Calls to approve and setTotalDeposit are serialized with the\n            # deposit_lock to avoid transaction failure, because with two\n            # concurrent deposits, we may have the transactions executed in the\n            # following order\n            #\n            # - approve\n            # - approve\n            # - setTotalDeposit\n            # - setTotalDeposit\n            #\n            # in which case  the second `approve` will overwrite the first,\n            # and the first `setTotalDeposit` will consume the allowance,\n            #  making the second deposit fail.\n            token.approve(\n                allowed_address=Address(self.address),\n                allowance=amount_to_deposit,\n            )\n\n            gas_limit = self.proxy.estimate_gas(\n                checking_block,\n                'setTotalDeposit',\n                channel_identifier=channel_identifier,\n                participant=self.node_address,\n                total_deposit=total_deposit,\n                partner=partner,\n            )\n\n            if gas_limit:\n                gas_limit = safe_gas_limit(gas_limit, GAS_REQUIRED_FOR_SET_TOTAL_DEPOSIT)\n                error_prefix = 'setTotalDeposit call failed'\n\n                log.debug('setTotalDeposit called', **log_details)\n                transaction_hash = self.proxy.transact(\n                    'setTotalDeposit',\n                    gas_limit,\n                    channel_identifier=channel_identifier,\n                    participant=self.node_address,\n                    total_deposit=total_deposit,\n                    partner=partner,\n                )\n                self.client.poll(transaction_hash)\n                receipt_or_none = check_transaction_threw(self.client, transaction_hash)\n\n            transaction_executed = gas_limit is not None\n            if not transaction_executed or receipt_or_none:\n                if transaction_executed:\n                    block = receipt_or_none['blockNumber']\n                else:\n                    block = checking_block\n\n                self.proxy.jsonrpc_client.check_for_insufficient_eth(\n                    transaction_name='setTotalDeposit',\n                    transaction_executed=transaction_executed,\n                    required_gas=GAS_REQUIRED_FOR_SET_TOTAL_DEPOSIT,\n                    block_identifier=block,\n                )\n                error_type, msg = self._check_why_deposit_failed(\n                    channel_identifier=channel_identifier,\n                    partner=partner,\n                    token=token,\n                    amount_to_deposit=amount_to_deposit,\n                    total_deposit=total_deposit,\n                    transaction_executed=transaction_executed,\n                    block_identifier=block,\n                )\n\n                error_msg = f'{error_prefix}. {msg}'\n                if error_type == RaidenRecoverableError:\n                    log.warning(error_msg, **log_details)\n                else:\n                    log.critical(error_msg, **log_details)\n                raise error_type(error_msg)\n\n            log.info('setTotalDeposit successful', **log_details)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(\n            self,\n            channel_identifier: ChannelID,\n            partner: Address,\n            balance_hash: BalanceHash,\n            nonce: Nonce,\n            additional_hash: AdditionalHash,\n            signature: Signature,\n            given_block_identifier: BlockSpecification,\n    ):\n        \"\"\" Close the channel using the provided balance proof.\n\n        Note:\n            This method must *not* be called without updating the application\n            state, otherwise the node may accept new transfers which cannot be\n            used, because the closer is not allowed to update the balance proof\n            submitted on chain after closing\n\n        Raises:\n            RaidenRecoverableError: If the channel is already closed.\n            RaidenUnrecoverableError: If the channel does not exist or is settled.\n        \"\"\"\n\n        log_details = {\n            'token_network': pex(self.address),\n            'node': pex(self.node_address),\n            'partner': pex(partner),\n            'nonce': nonce,\n            'balance_hash': encode_hex(balance_hash),\n            'additional_hash': encode_hex(additional_hash),\n            'signature': encode_hex(signature),\n        }\n        log.debug('closeChannel called', **log_details)\n\n        checking_block = self.client.get_checking_block()\n        try:\n            self._close_preconditions(\n                channel_identifier,\n                partner=partner,\n                block_identifier=given_block_identifier,\n            )\n        except NoStateForBlockIdentifier:\n            # If preconditions end up being on pruned state skip them. Estimate\n            # gas will stop us from sending a transaction that will fail\n            pass\n\n        error_prefix = 'closeChannel call will fail'\n        with self.channel_operations_lock[partner]:\n            gas_limit = self.proxy.estimate_gas(\n                checking_block,\n                'closeChannel',\n                channel_identifier=channel_identifier,\n                partner=partner,\n                balance_hash=balance_hash,\n                nonce=nonce,\n                additional_hash=additional_hash,\n                signature=signature,\n            )\n\n            if gas_limit:\n                error_prefix = 'closeChannel call failed'\n                transaction_hash = self.proxy.transact(\n                    'closeChannel',\n                    safe_gas_limit(gas_limit, GAS_REQUIRED_FOR_CLOSE_CHANNEL),\n                    channel_identifier=channel_identifier,\n                    partner=partner,\n                    balance_hash=balance_hash,\n                    nonce=nonce,\n                    additional_hash=additional_hash,\n                    signature=signature,\n                )\n                self.client.poll(transaction_hash)\n                receipt_or_none = check_transaction_threw(self.client, transaction_hash)\n\n            transaction_executed = gas_limit is not None\n            if not transaction_executed or receipt_or_none:\n                if transaction_executed:\n                    block = receipt_or_none['blockNumber']\n                else:\n                    block = checking_block\n\n                self.proxy.jsonrpc_client.check_for_insufficient_eth(\n                    transaction_name='closeChannel',\n                    transaction_executed=transaction_executed,\n                    required_gas=GAS_REQUIRED_FOR_CLOSE_CHANNEL,\n                    block_identifier=block,\n                )\n                error_type, msg = self._check_channel_state_for_close(\n                    participant1=self.node_address,\n                    participant2=partner,\n                    block_identifier=block,\n                    channel_identifier=channel_identifier,\n                )\n                if not error_type:\n                    # error_type can also be None above in which case it's\n                    # unknown reason why we would fail.\n                    error_type = RaidenUnrecoverableError\n                error_msg = f'{error_prefix}. {msg}'\n                if error_type == RaidenRecoverableError:\n                    log.warning(error_msg, **log_details)\n                else:\n                    log.critical(error_msg, **log_details)\n\n                raise error_type(error_msg)\n\n        log.info('closeChannel successful', **log_details)", "response": "Closes the channel using the provided balance proof."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef events_filter(\n            self,\n            topics: List[str] = None,\n            from_block: BlockSpecification = None,\n            to_block: BlockSpecification = None,\n    ) -> StatelessFilter:\n        \"\"\" Install a new filter for an array of topics emitted by the contract.\n\n        Args:\n            topics: A list of event ids to filter for. Can also be None,\n                    in which case all events are queried.\n            from_block: The block number at which to start looking for events.\n            to_block: The block number at which to stop looking for events.\n        Return:\n            Filter: The filter instance.\n        \"\"\"\n        return self.client.new_filter(\n            self.address,\n            topics=topics,\n            from_block=from_block,\n            to_block=to_block,\n        )", "response": "Install a new filter for an array of topics emitted by the contract."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_events_filter(\n            self,\n            from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,\n            to_block: BlockSpecification = 'latest',\n    ) -> StatelessFilter:\n        \"\"\" Install a new filter for all the events emitted by the current token network contract\n\n        Args:\n            from_block: Create filter starting from this block number (default: 0).\n            to_block: Create filter stopping at this block number (default: 'latest').\n\n        Return:\n            The filter instance.\n        \"\"\"\n        return self.events_filter(None, from_block, to_block)", "response": "Install a filter for all the events emitted by the current token network contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_for_outdated_channel(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ) -> None:\n        \"\"\"\n        Checks whether an operation is being executed on a channel\n        between two participants using an old channel identifier\n        \"\"\"\n        try:\n            onchain_channel_details = self._detail_channel(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n            )\n        except RaidenRecoverableError:\n            return\n\n        onchain_channel_identifier = onchain_channel_details.channel_identifier\n\n        if onchain_channel_identifier != channel_identifier:\n            raise ChannelOutdatedError(\n                'Current channel identifier is outdated. '\n                f'current={channel_identifier}, '\n                f'new={onchain_channel_identifier}',\n            )", "response": "Checks whether an operation is being executed on a channel between two participants."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the state of the channel to see if it has been updated. If it has not yet been mined and the channel has not yet been updated return None.", "response": "def _check_channel_state_for_update(\n            self,\n            channel_identifier: ChannelID,\n            closer: Address,\n            update_nonce: Nonce,\n            block_identifier: BlockSpecification,\n    ) -> Optional[str]:\n        \"\"\"Check the channel state on chain to see if it has been updated.\n\n        Compare the nonce, we are about to update the contract with, with the\n        updated nonce in the onchain state and, if it's the same, return a\n        message with which the caller should raise a RaidenRecoverableError.\n\n        If all is okay return None.\n        \"\"\"\n        msg = None\n        closer_details = self._detail_participant(\n            channel_identifier=channel_identifier,\n            participant=closer,\n            partner=self.node_address,\n            block_identifier=block_identifier,\n        )\n        if closer_details.nonce == update_nonce:\n            msg = (\n                'updateNonClosingBalanceProof transaction has already '\n                'been mined and updated the channel succesfully.'\n            )\n\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef event_first_of(*events: _AbstractLinkable) -> Event:\n    first_finished = Event()\n\n    if not all(isinstance(e, _AbstractLinkable) for e in events):\n        raise ValueError('all events must be linkable')\n\n    for event in events:\n        event.rawlink(lambda _: first_finished.set())\n\n    return first_finished", "response": "Waits until one of the given events is set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timeout_two_stage(\n        retries: int,\n        timeout1: int,\n        timeout2: int,\n) -> Iterable[int]:\n    \"\"\" Timeouts generator with a two stage strategy\n\n    Timeouts start spaced by `timeout1`, after `retries` increase\n    to `timeout2` which is repeated indefinitely.\n    \"\"\"\n    for _ in range(retries):\n        yield timeout1\n    while True:\n        yield timeout2", "response": "Yields the two - stage timeout generator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef retry(\n        transport: 'UDPTransport',\n        messagedata: bytes,\n        message_id: UDPMessageID,\n        recipient: Address,\n        stop_event: Event,\n        timeout_backoff: Iterable[int],\n) -> bool:\n    \"\"\" Send messagedata until it's acknowledged.\n\n    Exit when:\n\n    - The message is delivered.\n    - Event_stop is set.\n    - The iterator timeout_backoff runs out.\n\n    Returns:\n        bool: True if the message was acknowledged, False otherwise.\n    \"\"\"\n\n    async_result = transport.maybe_sendraw_with_result(\n        recipient,\n        messagedata,\n        message_id,\n    )\n\n    event_quit = event_first_of(\n        async_result,\n        stop_event,\n    )\n\n    for timeout in timeout_backoff:\n\n        if event_quit.wait(timeout=timeout) is True:\n            break\n\n        log.debug(\n            'retrying message',\n            node=pex(transport.raiden.address),\n            recipient=pex(recipient),\n            msgid=message_id,\n        )\n\n        transport.maybe_sendraw_with_result(\n            recipient,\n            messagedata,\n            message_id,\n        )\n\n    return async_result.ready()", "response": "Send messagedata until it s acknowledged."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretry a message with recovery.", "response": "def retry_with_recovery(\n        transport: 'UDPTransport',\n        messagedata: bytes,\n        message_id: UDPMessageID,\n        recipient: Address,\n        stop_event: Event,\n        event_healthy: Event,\n        event_unhealthy: Event,\n        backoff: Iterable[int],\n) -> bool:\n    \"\"\" Send messagedata while the node is healthy until it's acknowledged.\n\n    Note:\n        backoff must be an infinite iterator, otherwise this task will\n        become a hot loop.\n    \"\"\"\n\n    # The underlying unhealthy will be cleared, care must be taken to properly\n    # clear stop_or_unhealthy too.\n    stop_or_unhealthy = event_first_of(\n        stop_event,\n        event_unhealthy,\n    )\n\n    acknowledged = False\n    while not stop_event.is_set() and not acknowledged:\n\n        # Packets must not be sent to an unhealthy node, nor should the task\n        # wait for it to become available if the message has been acknowledged.\n        if event_unhealthy.is_set():\n            log.debug(\n                'waiting for recipient to become available',\n                node=pex(transport.raiden.address),\n                recipient=pex(recipient),\n            )\n            wait_recovery(\n                stop_event,\n                event_healthy,\n            )\n\n            # Assume wait_recovery returned because unhealthy was cleared and\n            # continue execution, this is safe to do because stop_event is\n            # checked below.\n            stop_or_unhealthy.clear()\n\n            if stop_event.is_set():\n                return acknowledged\n\n        acknowledged = retry(\n            transport,\n            messagedata,\n            message_id,\n            recipient,\n\n            # retry will stop when this event is set, allowing this task to\n            # wait for recovery when the node becomes unhealthy or to quit if\n            # the stop event is set.\n            stop_or_unhealthy,\n\n            # Intentionally reusing backoff to restart from the last\n            # timeout/number of iterations.\n            backoff,\n        )\n\n    return acknowledged"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforcing joinall to use set", "response": "def _force_joinall_to_use_set(self, node):\n        \"\"\"This detect usages of the form:\n\n            >>> from gevent import joinall\n            >>> joinall(...)\n\n        or:\n\n            >>> import gevent\n            >>> gevent.joinall(...)\n        \"\"\"\n        for inferred_func in node.func.infer():\n            if is_joinall(inferred_func):\n\n                is_every_value_a_set = all(\n                    inferred_first_arg.pytype() == 'builtins.set'\n                    for inferred_first_arg in node.args[0].infer()\n                )\n                if not is_every_value_a_set:\n                    self.add_message(JOINALL_ID, node=node)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a batch of secrets.", "response": "def register_secret_batch(\n            self,\n            secrets: List[Secret],\n    ):\n        \"\"\"Register a batch of secrets. Check if they are already registered at\n        the given block identifier.\"\"\"\n        secrets_to_register = list()\n        secrethashes_to_register = list()\n        secrethashes_not_sent = list()\n        transaction_result = AsyncResult()\n        wait_for = set()\n\n        # secret registration has no preconditions:\n        #\n        # - The action does not depend on any state, it's always valid to call\n        #   it.\n        # - This action is always susceptible to race conditions.\n        #\n        # Therefore this proxy only needs to detect if the secret is already\n        # registered, to avoid sending obviously unecessary transactions, and\n        # it has to handle race conditions.\n\n        with self._open_secret_transactions_lock:\n            verification_block_hash = self.client.get_confirmed_blockhash()\n\n            for secret in secrets:\n                secrethash = sha3(secret)\n                secrethash_hex = encode_hex(secrethash)\n\n                # Do the local test on `open_secret_transactions` first, then\n                # if necessary do an RPC call.\n                #\n                # The call to `is_secret_registered` has two conflicting\n                # requirements:\n                #\n                # - Avoid sending duplicated transactions for the same lock\n                # - Operating on a consistent/confirmed view of the blockchain\n                #   (if a secret has been registered in a block that is not\n                #   confirmed it doesn't count yet, an optimization would be to\n                #   *not* send the transaction and wait for the confirmation)\n                #\n                # The code below respects the consistent blockchain view,\n                # meaning that if this proxy method is called with an old\n                # blockhash an unecessary transaction will be sent, and the\n                # error will be treated as a race-condition.\n                other_result = self.open_secret_transactions.get(secret)\n\n                if other_result is not None:\n                    wait_for.add(other_result)\n                    secrethashes_not_sent.append(secrethash_hex)\n                elif not self.is_secret_registered(secrethash, verification_block_hash):\n                    secrets_to_register.append(secret)\n                    secrethashes_to_register.append(secrethash_hex)\n                    self.open_secret_transactions[secret] = transaction_result\n\n        # From here on the lock is not required. Context-switches will happen\n        # for the gas estimation and the transaction, however the\n        # synchronization data is limited to the open_secret_transactions\n        log_details = {\n            'node': pex(self.node_address),\n            'contract': pex(self.address),\n            'secrethashes': secrethashes_to_register,\n            'secrethashes_not_sent': secrethashes_not_sent,\n        }\n\n        if not secrets_to_register:\n            log.debug(\n                'registerSecretBatch skipped, waiting for transactions',\n                **log_details,\n            )\n\n            gevent.joinall(wait_for, raise_error=True)\n\n            log.info(\n                'registerSecretBatch successful',\n                **log_details,\n            )\n            return\n\n        checking_block = self.client.get_checking_block()\n        gas_limit = self.proxy.estimate_gas(\n            checking_block,\n            'registerSecretBatch',\n            secrets_to_register,\n        )\n        receipt = None\n        transaction_hash = None\n        msg = None\n\n        if gas_limit:\n            gas_limit = safe_gas_limit(\n                gas_limit,\n                len(secrets_to_register) * GAS_REQUIRED_PER_SECRET_IN_BATCH,\n            )\n\n            log.debug('registerSecretBatch called', **log_details)\n\n            try:\n                transaction_hash = self.proxy.transact(\n                    'registerSecretBatch',\n                    gas_limit,\n                    secrets_to_register,\n                )\n                self.client.poll(transaction_hash)\n                receipt = self.client.get_transaction_receipt(transaction_hash)\n            except Exception as e:  # pylint: disable=broad-except\n                msg = f'Unexpected exception {e} at sending registerSecretBatch transaction.'\n\n        # Clear `open_secret_transactions` regardless of the transaction being\n        # successfully executed or not.\n        with self._open_secret_transactions_lock:\n            for secret in secrets_to_register:\n                self.open_secret_transactions.pop(secret)\n\n        # As of version `0.4.0` of the contract has *no* asserts or requires.\n        # Therefore the only reason for the transaction to fail is if there is\n        # a bug.\n        unrecoverable_error = (\n            gas_limit is None or\n            receipt is None or\n            receipt['status'] == RECEIPT_FAILURE_CODE\n        )\n\n        exception: Union[RaidenRecoverableError, RaidenUnrecoverableError]\n        if unrecoverable_error:\n            # If the transaction was sent it must not fail. If this happened\n            # some of our assumptions is broken therefore the error is\n            # unrecoverable\n            if receipt is not None:\n                if receipt['gasUsed'] == gas_limit:\n                    # The transaction failed and all gas was used. This can\n                    # happen because of:\n                    #\n                    # - A compiler bug if an invalid opcode was executed.\n                    # - A configuration bug if an assert was executed,\n                    # because version 0.4.0 of the secret registry does not have an\n                    # assert.\n                    # - An ethereum client bug if the gas_limit was\n                    # underestimated.\n                    #\n                    # Safety cannot be guaranteed under any of these cases,\n                    # this error is unrecoverable.\n                    error = (\n                        'Secret registration failed because of a bug in either '\n                        'the solidity compiler, the running ethereum client, or '\n                        'a configuration error in Raiden.'\n                    )\n                else:\n                    # The transaction failed and *not* all gas was used. This\n                    # can happen because of:\n                    #\n                    # - A compiler bug if a revert was introduced.\n                    # - A configuration bug, because for 0.4.0 the secret\n                    # registry does not have a revert.\n                    error = (\n                        'Secret registration failed because of a configuration '\n                        'bug or compiler bug. Please double check the secret '\n                        'smart contract is at version 0.4.0, if it is then a '\n                        'compiler bug was hit.'\n                    )\n\n                log.critical(error, **log_details)\n                exception = RaidenUnrecoverableError(error)\n                transaction_result.set_exception(exception)\n                raise exception\n\n            # If gas_limit is set and there is no receipt then an exception was\n            # raised while sending the transaction. This should only happen if\n            # the account is being used concurrently, which is not supported.\n            # This can happen because:\n            #\n            # - The nonce of the transaction was already used\n            # - The nonce was reused *and* the account didn't have enough ether\n            # to pay for the gas\n            #\n            # Safety cannot be guaranteed under any of these cases, this error\n            # is unrecoverable. *Note*: This assumes the ethereum client\n            # takes into account the current transactions in the pool.\n            if gas_limit:\n                assert msg, (\n                    'Unexpected control flow, an exception should have been raised.'\n                )\n                error = (\n                    f\"Sending the the transaction for registerSecretBatch failed with: `{msg}`. \"\n                    f\"This happens if the same ethereum account is being used by more than one \"\n                    f\"program which is not supported.\"\n                )\n\n                log.critical(error, **log_details)\n                exception = RaidenUnrecoverableError(error)\n                transaction_result.set_exception(exception)\n                raise exception\n\n            # gas_limit can fail because:\n            #\n            # - The Ethereum client detected the transaction could not\n            # successfully execute, this happens if an assert/revert is hit.\n            # - The account is lacking funds to pay for the gas.\n            #\n            # Either of these is a bug. The contract does not use\n            # assert/revert, and the account should always be funded\n            self.proxy.jsonrpc_client.check_for_insufficient_eth(\n                transaction_name='registerSecretBatch',\n                transaction_executed=True,\n                required_gas=gas_limit,\n                block_identifier=checking_block,\n            )\n            error = \"Call to registerSecretBatch couldn't be done\"\n\n            log.critical(error, **log_details)\n            exception = RaidenRecoverableError(error)\n            transaction_result.set_exception(exception)\n            raise exception\n\n        # The local **MUST** transaction_result be set before waiting for the\n        # other results, otherwise we have a dead-lock\n        transaction_result.set(transaction_hash)\n\n        if wait_for:\n            log.info('registerSecretBatch waiting for pending', **log_details)\n            gevent.joinall(wait_for, raise_error=True)\n\n        log.info('registerSecretBatch successful', **log_details)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the block number at which the secret for secrethash was registered.", "response": "def get_secret_registration_block_by_secrethash(\n            self,\n            secrethash: SecretHash,\n            block_identifier: BlockSpecification,\n    ) -> Optional[BlockNumber]:\n        \"\"\"Return the block number at which the secret for `secrethash` was\n        registered, None if the secret was never registered.\n        \"\"\"\n        result = self.proxy.contract.functions.getSecretRevealBlockHeight(\n            secrethash,\n        ).call(block_identifier=block_identifier)\n\n        # Block 0 either represents the genesis block or an empty entry in the\n        # secret mapping. This is important for custom genesis files used while\n        # testing. To avoid problems the smart contract can be added as part of\n        # the genesis file, however it's important for its storage to be\n        # empty.\n        if result == 0:\n            return None\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_secret_registered(\n            self,\n            secrethash: SecretHash,\n            block_identifier: BlockSpecification,\n    ) -> bool:\n        \"\"\"True if the secret for `secrethash` is registered at `block_identifier`.\n\n        Throws NoStateForBlockIdentifier if the given block_identifier\n        is older than the pruning limit\n        \"\"\"\n        if not self.client.can_query_state_for_block(block_identifier):\n            raise NoStateForBlockIdentifier()\n\n        block = self.get_secret_registration_block_by_secrethash(\n            secrethash=secrethash,\n            block_identifier=block_identifier,\n        )\n        return block is not None", "response": "Returns True if the secret for secrethash is registered at block_identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new token in the registry.", "response": "def create_token(\n            self,\n            registry_address,\n            initial_alloc=10 ** 6,\n            name='raidentester',\n            symbol='RDT',\n            decimals=2,\n            timeout=60,\n            auto_register=True,\n    ):\n        \"\"\" Create a proxy for a new HumanStandardToken (ERC20), that is\n        initialized with Args(below).\n        Per default it will be registered with 'raiden'.\n\n        Args:\n            initial_alloc (int): amount of initial tokens.\n            name (str): human readable token name.\n            symbol (str): token shorthand symbol.\n            decimals (int): decimal places.\n            timeout (int): timeout in seconds for creation.\n            auto_register (boolean): if True(default), automatically register\n                the token with raiden.\n\n        Returns:\n            token_address_hex: the hex encoded address of the new token/token.\n        \"\"\"\n        with gevent.Timeout(timeout):\n            token_address = deploy_contract_web3(\n                CONTRACT_HUMAN_STANDARD_TOKEN,\n                self._chain.client,\n                contract_manager=self._raiden.contract_manager,\n                constructor_arguments=(initial_alloc, name, decimals, symbol),\n            )\n\n        token_address_hex = to_checksum_address(token_address)\n        if auto_register:\n            self.register_token(registry_address, token_address_hex)\n        print(\"Successfully created {}the token '{}'.\".format(\n            'and registered ' if auto_register else ' ',\n            name,\n        ))\n        return token_address_hex"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a token with the token manager.", "response": "def register_token(\n            self,\n            registry_address_hex: typing.AddressHex,\n            token_address_hex: typing.AddressHex,\n            retry_timeout: typing.NetworkTimeout = DEFAULT_RETRY_TIMEOUT,\n    ) -> TokenNetwork:\n        \"\"\" Register a token with the raiden token manager.\n\n        Args:\n            registry_address: registry address\n            token_address_hex (string): a hex encoded token address.\n\n        Returns:\n\n            The token network proxy.\n        \"\"\"\n        registry_address = decode_hex(registry_address_hex)\n        token_address = decode_hex(token_address_hex)\n\n        registry = self._raiden.chain.token_network_registry(registry_address)\n        contracts_version = self._raiden.contract_manager.contracts_version\n\n        if contracts_version == DEVELOPMENT_CONTRACT_VERSION:\n            token_network_address = registry.add_token_with_limits(\n                token_address=token_address,\n                channel_participant_deposit_limit=UINT256_MAX,\n                token_network_deposit_limit=UINT256_MAX,\n            )\n        else:\n            token_network_address = registry.add_token_without_limits(\n                token_address=token_address,\n            )\n\n        # Register the channel manager with the raiden registry\n        waiting.wait_for_payment_network(\n            self._raiden,\n            registry.address,\n            token_address,\n            retry_timeout,\n        )\n\n        return self._raiden.chain.token_network(token_network_address)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen a channel with the given token and peer.", "response": "def open_channel_with_funding(\n            self,\n            registry_address_hex,\n            token_address_hex,\n            peer_address_hex,\n            total_deposit,\n            settle_timeout=None,\n    ):\n        \"\"\" Convenience method to open a channel.\n\n        Args:\n            registry_address_hex (str): hex encoded address of the registry for the channel.\n            token_address_hex (str): hex encoded address of the token for the channel.\n            peer_address_hex (str): hex encoded address of the channel peer.\n            total_deposit (int): amount of total funding for the channel.\n            settle_timeout (int): amount of blocks for the settle time (if None use app defaults).\n\n        Return:\n            netting_channel: the (newly opened) netting channel object.\n        \"\"\"\n        # Check, if peer is discoverable\n        registry_address = decode_hex(registry_address_hex)\n        peer_address = decode_hex(peer_address_hex)\n        token_address = decode_hex(token_address_hex)\n        try:\n            self._discovery.get(peer_address)\n        except KeyError:\n            print('Error: peer {} not found in discovery'.format(peer_address_hex))\n            return None\n\n        self._api.channel_open(\n            registry_address,\n            token_address,\n            peer_address,\n            settle_timeout=settle_timeout,\n        )\n\n        return self._api.set_total_channel_deposit(\n            registry_address,\n            token_address,\n            peer_address,\n            total_deposit,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait until a contract is mined", "response": "def wait_for_contract(self, contract_address_hex, timeout=None):\n        \"\"\" Wait until a contract is mined\n\n        Args:\n            contract_address_hex (string): hex encoded address of the contract\n            timeout (int): time to wait for the contract to get mined\n\n        Returns:\n            True if the contract got mined, false otherwise\n        \"\"\"\n        contract_address = decode_hex(contract_address_hex)\n        start_time = time.time()\n        result = self._raiden.chain.client.web3.eth.getCode(\n            to_checksum_address(contract_address),\n        )\n\n        current_time = time.time()\n        while not result:\n            if timeout and start_time + timeout > current_time:\n                return False\n\n            result = self._raiden.chain.client.web3.eth.getCode(\n                to_checksum_address(contract_address),\n            )\n            gevent.sleep(0.5)\n\n            current_time = time.time()\n\n        return len(result) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _filter_from_dict(current: Dict[str, Any]) -> Dict[str, Any]:\n    filter_ = dict()\n\n    for k, v in current.items():\n        if isinstance(v, dict):\n            for sub, v2 in _filter_from_dict(v).items():\n                filter_[f'{k}.{sub}'] = v2\n        else:\n            filter_[k] = v\n\n    return filter_", "response": "Takes in a nested dictionary as a filter and returns a flattened filter dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging the raiden version and timestamp to help with debugging", "response": "def log_run(self):\n        \"\"\" Log timestamp and raiden version to help with debugging \"\"\"\n        version = get_system_spec()['raiden']\n        cursor = self.conn.cursor()\n        cursor.execute('INSERT INTO runs(raiden_version) VALUES (?)', [version])\n        self.maybe_commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave events. Args: state_change_identifier: Id of the state change that generate these events. events: List of Event objects.", "response": "def write_events(self, events):\n        \"\"\" Save events.\n\n        Args:\n            state_change_identifier: Id of the state change that generate these events.\n            events: List of Event objects.\n        \"\"\"\n        with self.write_lock, self.conn:\n            self.conn.executemany(\n                'INSERT INTO state_events('\n                '   identifier, source_statechange_id, log_time, data'\n                ') VALUES(?, ?, ?, ?)',\n                events,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting the state changes from the database.", "response": "def delete_state_changes(self, state_changes_to_delete: List[int]) -> None:\n        \"\"\" Delete state changes.\n\n        Args:\n            state_changes_to_delete: List of ids to delete.\n        \"\"\"\n        with self.write_lock, self.conn:\n            self.conn.executemany(\n                'DELETE FROM state_events WHERE identifier = ?',\n                state_changes_to_delete,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the most recent state change that is closest to the provided state change.", "response": "def get_snapshot_closest_to_state_change(\n            self,\n            state_change_identifier: int,\n    ) -> Tuple[int, Any]:\n        \"\"\" Get snapshots earlier than state_change with provided ID. \"\"\"\n\n        if not (state_change_identifier == 'latest' or isinstance(state_change_identifier, int)):\n            raise ValueError(\"from_identifier must be an integer or 'latest'\")\n\n        cursor = self.conn.cursor()\n        if state_change_identifier == 'latest':\n            cursor.execute(\n                'SELECT identifier FROM state_changes ORDER BY identifier DESC LIMIT 1',\n            )\n            result = cursor.fetchone()\n\n            if result:\n                state_change_identifier = result[0]\n            else:\n                state_change_identifier = 0\n\n        cursor = self.conn.execute(\n            'SELECT statechange_id, data FROM state_snapshot '\n            'WHERE statechange_id <= ? '\n            'ORDER BY identifier DESC LIMIT 1',\n            (state_change_identifier, ),\n        )\n        rows = cursor.fetchall()\n\n        if rows:\n            assert len(rows) == 1, 'LIMIT 1 must return one element'\n            last_applied_state_change_id = rows[0][0]\n            snapshot_state = rows[0][1]\n            result = (last_applied_state_change_id, snapshot_state)\n        else:\n            result = (0, None)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a batch of state changes for the given object.", "response": "def _get_state_changes(\n            self,\n            limit: int = None,\n            offset: int = None,\n            filters: List[Tuple[str, Any]] = None,\n            logical_and: bool = True,\n    ) -> List[StateChangeRecord]:\n        \"\"\" Return a batch of state change records (identifier and data)\n\n        The batch size can be tweaked with the `limit` and `offset` arguments.\n\n        Additionally the returned state changes can be optionally filtered with\n        the `filters` parameter to search for specific data in the state change data.\n        \"\"\"\n        cursor = self._form_and_execute_json_query(\n            query='SELECT identifier, data FROM state_changes ',\n            limit=limit,\n            offset=offset,\n            filters=filters,\n            logical_and=logical_and,\n        )\n        result = [\n            StateChangeRecord(\n                state_change_identifier=row[0],\n                data=row[1],\n            )\n            for row in cursor\n        ]\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbatching query state change records with a given batch size and an optional filter This is a generator function returning each batch to the caller to work with.", "response": "def batch_query_state_changes(\n            self,\n            batch_size: int,\n            filters: List[Tuple[str, Any]] = None,\n            logical_and: bool = True,\n    ) -> Iterator[List[StateChangeRecord]]:\n        \"\"\"Batch query state change records with a given batch size and an optional filter\n\n        This is a generator function returning each batch to the caller to work with.\n        \"\"\"\n        limit = batch_size\n        offset = 0\n        result_length = 1\n\n        while result_length != 0:\n            result = self._get_state_changes(\n                limit=limit,\n                offset=offset,\n                filters=filters,\n                logical_and=logical_and,\n            )\n            result_length = len(result)\n            offset += result_length\n            yield result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_state_changes(self, state_changes_data: List[Tuple[str, int]]) -> None:\n        cursor = self.conn.cursor()\n        cursor.executemany(\n            'UPDATE state_changes SET data=? WHERE identifier=?',\n            state_changes_data,\n        )\n        self.maybe_commit()", "response": "Update the state_changes table with the given data state tuples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_event_records(\n            self,\n            limit: int = None,\n            offset: int = None,\n            filters: List[Tuple[str, Any]] = None,\n            logical_and: bool = True,\n    ) -> List[EventRecord]:\n        \"\"\" Return a batch of event records\n\n        The batch size can be tweaked with the `limit` and `offset` arguments.\n\n        Additionally the returned events can be optionally filtered with\n        the `filters` parameter to search for specific data in the event data.\n        \"\"\"\n        cursor = self._form_and_execute_json_query(\n            query='SELECT identifier, source_statechange_id, data FROM state_events ',\n            limit=limit,\n            offset=offset,\n            filters=filters,\n            logical_and=logical_and,\n        )\n\n        result = [\n            EventRecord(\n                event_identifier=row[0],\n                state_change_identifier=row[1],\n                data=row[2],\n            ) for row in cursor\n        ]\n        return result", "response": "Returns a batch of event records from the state_events table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef batch_query_event_records(\n            self,\n            batch_size: int,\n            filters: List[Tuple[str, Any]] = None,\n            logical_and: bool = True,\n    ) -> Iterator[List[EventRecord]]:\n        \"\"\"Batch query event records with a given batch size and an optional filter\n\n        This is a generator function returning each batch to the caller to work with.\n        \"\"\"\n        limit = batch_size\n        offset = 0\n        result_length = 1\n\n        while result_length != 0:\n            result = self._get_event_records(\n                limit=limit,\n                offset=offset,\n                filters=filters,\n                logical_and=logical_and,\n            )\n            result_length = len(result)\n            offset += result_length\n            yield result", "response": "This method returns an iterator over the event records in a given batch size and an optional filter\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the state_events table with the given list of data event tuples.", "response": "def update_events(self, events_data: List[Tuple[str, int]]) -> None:\n        \"\"\"Given a list of identifier/data event tuples update them in the DB\"\"\"\n        cursor = self.conn.cursor()\n        cursor.executemany(\n            'UPDATE state_events SET data=? WHERE identifier=?',\n            events_data,\n        )\n        self.maybe_commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of snapshot data update them in the DB getTaxID table", "response": "def update_snapshots(self, snapshots_data: List[Tuple[str, int]]):\n        \"\"\"Given a list of snapshot data, update them in the DB\n\n        The snapshots_data should be a list of tuples of snapshots data\n        and identifiers in that order.\n        \"\"\"\n        cursor = self.conn.cursor()\n        cursor.executemany(\n            'UPDATE state_snapshot SET data=? WHERE identifier=?',\n            snapshots_data,\n        )\n        self.maybe_commit()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the events to the log file.", "response": "def write_events(self, state_change_identifier, events, log_time):\n        \"\"\" Save events.\n\n        Args:\n            state_change_identifier: Id of the state change that generate these events.\n            events: List of Event objects.\n        \"\"\"\n        events_data = [\n            (None, state_change_identifier, log_time, self.serializer.serialize(event))\n            for event in events\n        ]\n        return super().write_events(events_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_snapshot_closest_to_state_change(\n            self,\n            state_change_identifier: int,\n    ) -> Tuple[int, Any]:\n        \"\"\" Get snapshots earlier than state_change with provided ID. \"\"\"\n\n        row = super().get_snapshot_closest_to_state_change(state_change_identifier)\n\n        if row[1]:\n            last_applied_state_change_id = row[0]\n            snapshot_state = self.serializer.deserialize(row[1])\n            result = (last_applied_state_change_id, snapshot_state)\n        else:\n            result = (0, None)\n\n        return result", "response": "Get the closest state change to the given state change."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_neighbour_nodes(chain_state: ChainState) -> Set[Address]:\n    addresses = set()\n\n    for payment_network in chain_state.identifiers_to_paymentnetworks.values():\n        for token_network in payment_network.tokenidentifiers_to_tokennetworks.values():\n            channel_states = token_network.channelidentifiers_to_channels.values()\n            for channel_state in channel_states:\n                addresses.add(channel_state.partner_state.address)\n\n    return addresses", "response": "Return the set of all nodes that have a channel open with this one."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the list of token networks registered with the given payment network.", "response": "def get_token_network_identifiers(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n) -> List[TokenNetworkID]:\n    \"\"\" Return the list of token networks registered with the given payment network. \"\"\"\n    payment_network = chain_state.identifiers_to_paymentnetworks.get(payment_network_id)\n\n    if payment_network is not None:\n        return [\n            token_network.address\n            for token_network in payment_network.tokenidentifiers_to_tokennetworks.values()\n        ]\n\n    return list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the list of tokens registered with the given payment network.", "response": "def get_token_identifiers(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n) -> List[TokenAddress]:\n    \"\"\" Return the list of tokens registered with the given payment network. \"\"\"\n    payment_network = chain_state.identifiers_to_paymentnetworks.get(payment_network_id)\n\n    if payment_network is not None:\n        return [\n            token_address\n            for token_address in payment_network.tokenaddresses_to_tokenidentifiers.keys()\n        ]\n\n    return list()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_channelstate_for(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        partner_address: Address,\n) -> Optional[NettingChannelState]:\n    \"\"\" Return the NettingChannelState if it exists, None otherwise. \"\"\"\n    token_network = get_token_network_by_token_address(\n        chain_state,\n        payment_network_id,\n        token_address,\n    )\n\n    channel_state = None\n    if token_network:\n        channels = [\n            token_network.channelidentifiers_to_channels[channel_id]\n            for channel_id in token_network.partneraddresses_to_channelidentifiers[partner_address]\n        ]\n        states = filter_channels_by_status(\n            channels,\n            [CHANNEL_STATE_UNUSABLE],\n        )\n        # If multiple channel states are found, return the last one.\n        if states:\n            channel_state = states[-1]\n\n    return channel_state", "response": "Return the NettingChannelState for the given token and partner."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_channelstate_by_token_network_and_partner(\n        chain_state: ChainState,\n        token_network_id: TokenNetworkID,\n        partner_address: Address,\n) -> Optional[NettingChannelState]:\n    \"\"\" Return the NettingChannelState if it exists, None otherwise. \"\"\"\n    token_network = get_token_network_by_identifier(\n        chain_state,\n        token_network_id,\n    )\n\n    channel_state = None\n    if token_network:\n        channels = [\n            token_network.channelidentifiers_to_channels[channel_id]\n            for channel_id in token_network.partneraddresses_to_channelidentifiers[partner_address]\n        ]\n        states = filter_channels_by_status(\n            channels,\n            [CHANNEL_STATE_UNUSABLE],\n        )\n        if states:\n            channel_state = states[-1]\n\n    return channel_state", "response": "Return the NettingChannelState for the given token network and partner."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the NettingChannelState for the given canonical identifier.", "response": "def get_channelstate_by_canonical_identifier(\n        chain_state: ChainState,\n        canonical_identifier: CanonicalIdentifier,\n) -> Optional[NettingChannelState]:\n    \"\"\" Return the NettingChannelState if it exists, None otherwise. \"\"\"\n    token_network = get_token_network_by_identifier(\n        chain_state,\n        TokenNetworkID(canonical_identifier.token_network_address),\n    )\n\n    channel_state = None\n    if token_network:\n        channel_state = token_network.channelidentifiers_to_channels.get(\n            canonical_identifier.channel_identifier,\n        )\n\n    return channel_state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the state of channels that match the condition in filter_fn.", "response": "def get_channelstate_filter(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        filter_fn: Callable,\n) -> List[NettingChannelState]:\n    \"\"\" Return the state of channels that match the condition in `filter_fn` \"\"\"\n    token_network = get_token_network_by_token_address(\n        chain_state,\n        payment_network_id,\n        token_address,\n    )\n\n    result: List[NettingChannelState] = []\n    if not token_network:\n        return result\n\n    for channel_state in token_network.channelidentifiers_to_channels.values():\n        if filter_fn(channel_state):\n            result.append(channel_state)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the state of open channels in a token network.", "response": "def get_channelstate_open(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n) -> List[NettingChannelState]:\n    \"\"\"Return the state of open channels in a token network.\"\"\"\n    return get_channelstate_filter(\n        chain_state,\n        payment_network_id,\n        token_address,\n        lambda channel_state: channel.get_status(channel_state) == CHANNEL_STATE_OPENED,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the state of closing channels in a token network.", "response": "def get_channelstate_closing(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n) -> List[NettingChannelState]:\n    \"\"\"Return the state of closing channels in a token network.\"\"\"\n    return get_channelstate_filter(\n        chain_state,\n        payment_network_id,\n        token_address,\n        lambda channel_state: channel.get_status(channel_state) == CHANNEL_STATE_CLOSING,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the state of closed channels in a token network.", "response": "def get_channelstate_closed(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n) -> List[NettingChannelState]:\n    \"\"\"Return the state of closed channels in a token network.\"\"\"\n    return get_channelstate_filter(\n        chain_state,\n        payment_network_id,\n        token_address,\n        lambda channel_state: channel.get_status(channel_state) == CHANNEL_STATE_CLOSED,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the state of settling channels in a token network.", "response": "def get_channelstate_settling(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n) -> List[NettingChannelState]:\n    \"\"\"Return the state of settling channels in a token network.\"\"\"\n    return get_channelstate_filter(\n        chain_state,\n        payment_network_id,\n        token_address,\n        lambda channel_state: channel.get_status(channel_state) == CHANNEL_STATE_SETTLING,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_channelstate_settled(\n        chain_state: ChainState,\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n) -> List[NettingChannelState]:\n    \"\"\"Return the state of settled channels in a token network.\"\"\"\n    return get_channelstate_filter(\n        chain_state,\n        payment_network_id,\n        token_address,\n        lambda channel_state: channel.get_status(channel_state) == CHANNEL_STATE_SETTLED,\n    )", "response": "Return the state of settled channels in a token network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef role_from_transfer_task(transfer_task: TransferTask) -> str:\n    if isinstance(transfer_task, InitiatorTask):\n        return 'initiator'\n    if isinstance(transfer_task, MediatorTask):\n        return 'mediator'\n    if isinstance(transfer_task, TargetTask):\n        return 'target'\n\n    raise ValueError('Argument to role_from_transfer_task is not a TransferTask')", "response": "Returns the role and type for the given transfer task."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef secret_from_transfer_task(\n        transfer_task: Optional[TransferTask],\n        secrethash: SecretHash,\n) -> Optional[Secret]:\n    \"\"\"Return the secret for the transfer, None on EMPTY_SECRET.\"\"\"\n    assert isinstance(transfer_task, InitiatorTask)\n\n    transfer_state = transfer_task.manager_state.initiator_transfers[secrethash]\n\n    if transfer_state is None:\n        return None\n\n    return transfer_state.transfer_description.secret", "response": "Return the secret for the transfer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_transfer_role(chain_state: ChainState, secrethash: SecretHash) -> Optional[str]:\n    task = chain_state.payment_mapping.secrethashes_to_task.get(secrethash)\n    if not task:\n        return None\n    return role_from_transfer_task(task)", "response": "Returns initiator mediator or target to signify the role the node has\n    in a transfer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering the list of channels by excluding ones that have the status of the given state.", "response": "def filter_channels_by_status(\n        channel_states: List[NettingChannelState],\n        exclude_states: Optional[List[str]] = None,\n) -> List[NettingChannelState]:\n    \"\"\" Filter the list of channels by excluding ones\n    for which the state exists in `exclude_states`. \"\"\"\n\n    if exclude_states is None:\n        exclude_states = []\n\n    states = []\n    for channel_state in channel_states:\n        if channel.get_status(channel_state) not in exclude_states:\n            states.append(channel_state)\n\n    return states"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetects if the balance proof of a token network has changed.", "response": "def detect_balance_proof_change(\n        old_state: ChainState,\n        current_state: ChainState,\n) -> Iterator[Union[BalanceProofSignedState, BalanceProofUnsignedState]]:\n    \"\"\" Compare two states for any received balance_proofs that are not in `old_state`. \"\"\"\n    if old_state == current_state:\n        return\n    for payment_network_identifier in current_state.identifiers_to_paymentnetworks:\n        try:\n            old_payment_network = old_state.identifiers_to_paymentnetworks.get(\n                payment_network_identifier,\n            )\n        except AttributeError:\n            old_payment_network = None\n\n        current_payment_network = current_state.identifiers_to_paymentnetworks[\n            payment_network_identifier\n        ]\n        if old_payment_network == current_payment_network:\n            continue\n\n        for token_network_identifier in current_payment_network.tokenidentifiers_to_tokennetworks:\n            if old_payment_network:\n                old_token_network = old_payment_network.tokenidentifiers_to_tokennetworks.get(\n                    token_network_identifier,\n                )\n            else:\n                old_token_network = None\n\n            current_token_network = current_payment_network.tokenidentifiers_to_tokennetworks[\n                token_network_identifier\n            ]\n            if old_token_network == current_token_network:\n                continue\n\n            for channel_identifier in current_token_network.channelidentifiers_to_channels:\n                if old_token_network:\n                    old_channel = old_token_network.channelidentifiers_to_channels.get(\n                        channel_identifier,\n                    )\n                else:\n                    old_channel = None\n\n                current_channel = current_token_network.channelidentifiers_to_channels[\n                    channel_identifier\n                ]\n                if current_channel == old_channel:\n                    continue\n\n                else:\n                    partner_state_updated = (\n                        current_channel.partner_state.balance_proof is not None and\n                        (\n                            old_channel is None or\n                            old_channel.partner_state.balance_proof !=\n                            current_channel.partner_state.balance_proof\n                        )\n                    )\n\n                    if partner_state_updated:\n                        assert current_channel.partner_state.balance_proof, MYPY_ANNOTATION\n                        yield current_channel.partner_state.balance_proof\n\n                    our_state_updated = (\n                        current_channel.our_state.balance_proof is not None and\n                        (\n                            old_channel is None or\n                            old_channel.our_state.balance_proof !=\n                            current_channel.our_state.balance_proof\n                        )\n                    )\n\n                    if our_state_updated:\n                        assert current_channel.our_state.balance_proof, MYPY_ANNOTATION\n                        yield current_channel.our_state.balance_proof"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect():\n    upnp = miniupnpc.UPnP()\n    upnp.discoverdelay = 200\n    providers = upnp.discover()\n    if providers > 1:\n        log.debug('multiple upnp providers found', num_providers=providers)\n    elif providers < 1:\n        log.error('no upnp providers found')\n        return None\n\n    try:\n        location = upnp.selectigd()\n        log.debug('connected', upnp=upnp)\n    except Exception as e:\n        log.error('Error when connecting to uPnP provider', exception_info=e)\n        return None\n\n    if not valid_mappable_ipv4(upnp.lanaddr):\n        log.error('could not query your lanaddr', reported=upnp.lanaddr)\n        return None\n    try:  # this can fail if router advertises uPnP incorrectly\n        if not valid_mappable_ipv4(upnp.externalipaddress()):\n            log.error('could not query your externalipaddress', reported=upnp.externalipaddress())\n            return None\n        return upnp, location\n    except Exception:\n        log.error('error when connecting with uPnP provider', location=location)\n        return None", "response": "Try to connect to the router."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_port(upnp, internal_port, external_start_port=None):\n    if external_start_port is None:\n        external_start_port = internal_port\n\n    if upnp is None:\n        return False\n\n    def register(internal, external):\n        # test existing mappings\n        mapping = upnp.getspecificportmapping(external, 'UDP')\n        if mapping is not None:\n            # FIXME: figure out semantics of the last two values\n            lanaddr, internal_mapped, name, _, _ = mapping\n\n            is_valid_mapping = (\n                lanaddr == upnp.lanaddr and\n                name == RAIDEN_IDENTIFICATOR and\n                internal_mapped == internal\n            )\n            is_not_our_mapping = (\n                internal_mapped != internal and\n                name != RAIDEN_IDENTIFICATOR\n            )\n            is_previous_mapping = (\n                internal_mapped != internal and\n                name == RAIDEN_IDENTIFICATOR and\n                lanaddr == upnp.lanaddr\n            )\n            if is_valid_mapping:\n                log.debug(\n                    'keeping pre-existing portmapping',\n                    internal=internal,\n                    external=external,\n                    lanaddr=lanaddr,\n                )\n                return True\n            elif lanaddr != upnp.lanaddr:\n                # don't touch other people's mappings\n                log.debug(\n                    'ignoring existing mapping for other IP',\n                    internal=internal,\n                    external=external,\n                    other_ip=lanaddr,\n                    our_ip=upnp.lanaddr,\n                )\n                return False\n            elif is_not_our_mapping:\n                log.debug(\n                    'ignoring existing mapping for other program',\n                    name=name,\n                )\n                # some other program uses our port\n                return False\n            elif is_previous_mapping:\n                # we ran before on a different internal port\n                log.debug('releasing previous port mapping')\n                upnp.deleteportmapping(external, 'UDP')\n\n        log.debug('trying to create new port mapping', internal=internal, external=external)\n        return upnp.addportmapping(\n            external,\n            'UDP',\n            upnp.lanaddr,\n            internal,\n            RAIDEN_IDENTIFICATOR,\n            '',\n        )\n\n    external_port = external_start_port\n    success = register(internal_port, external_port)\n    while not success and external_port <= MAX_PORT:\n        external_port += 1\n        log.debug('trying', external=external_port)\n        success = register(internal_port, external_port)\n\n    if success:\n        return upnp.externalipaddress(), external_port\n    else:\n        log.error(\n            'could not register a port-mapping',\n            location='FIXME',\n        )\n        return False\n\n    return False", "response": "Open a port for the raiden service through a base port."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef release_port(upnp, external_port):\n    mapping = upnp.getspecificportmapping(external_port, 'UDP')\n\n    if mapping is None:\n        log.error('could not find a port mapping', external=external_port)\n        return False\n    else:\n        log.debug('found existing port mapping', mapping=mapping)\n\n    if upnp.deleteportmapping(external_port, 'UDP'):\n        log.info('successfully released port mapping', external=external_port)\n        return True\n\n    log.warning(\n        'could not release port mapping, check your router for stale mappings',\n    )\n    return False", "response": "Try to release the port mapping for external_port."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates a blocktime estimate based on some past blocks.", "response": "def estimate_blocktime(self, oldest: int = 256) -> float:\n        \"\"\"Calculate a blocktime estimate based on some past blocks.\n        Args:\n            oldest: delta in block numbers to go back.\n        Return:\n            average block time in seconds\n        \"\"\"\n        last_block_number = self.block_number()\n        # around genesis block there is nothing to estimate\n        if last_block_number < 1:\n            return 15\n        # if there are less than `oldest` blocks available, start at block 1\n        if last_block_number < oldest:\n            interval = (last_block_number - 1) or 1\n        else:\n            interval = last_block_number - oldest\n        assert interval > 0\n        last_timestamp = self.get_block_header(last_block_number)['timestamp']\n        first_timestamp = self.get_block_header(last_block_number - interval)['timestamp']\n        delta = last_timestamp - first_timestamp\n        return delta / interval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a proxy to interact with a token.", "response": "def token(self, token_address: Address) -> Token:\n        \"\"\" Return a proxy to interact with a token. \"\"\"\n        if not is_binary_address(token_address):\n            raise ValueError('token_address must be a valid address')\n\n        with self._token_creation_lock:\n            if token_address not in self.address_to_token:\n                self.address_to_token[token_address] = Token(\n                    jsonrpc_client=self.client,\n                    token_address=token_address,\n                    contract_manager=self.contract_manager,\n                )\n\n        return self.address_to_token[token_address]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef discovery(self, discovery_address: Address) -> Discovery:\n        if not is_binary_address(discovery_address):\n            raise ValueError('discovery_address must be a valid address')\n\n        with self._discovery_creation_lock:\n            if discovery_address not in self.address_to_discovery:\n                self.address_to_discovery[discovery_address] = Discovery(\n                    jsonrpc_client=self.client,\n                    discovery_address=discovery_address,\n                    contract_manager=self.contract_manager,\n                )\n\n        return self.address_to_discovery[discovery_address]", "response": "Return a proxy to interact with the discovery."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclass factory, returns a class to wrap a buffer instance and expose the data as fields. The field spec specifies how many bytes should be used for a field and what is the encoding / decoding function.", "response": "def namedbuffer(buffer_name, fields_spec):  # noqa (ignore ciclomatic complexity)\n    \"\"\" Class factory, returns a class to wrap a buffer instance and expose the\n    data as fields.\n\n    The field spec specifies how many bytes should be used for a field and what\n    is the encoding / decoding function.\n    \"\"\"\n    # pylint: disable=protected-access,unused-argument\n\n    if not len(buffer_name):\n        raise ValueError('buffer_name is empty')\n\n    if not len(fields_spec):\n        raise ValueError('fields_spec is empty')\n\n    fields = [\n        field\n        for field in fields_spec\n        if not isinstance(field, Pad)\n    ]\n\n    if any(field.size_bytes < 0 for field in fields):\n        raise ValueError('negative size_bytes')\n\n    if any(len(field.name) < 0 for field in fields):\n        raise ValueError('field missing name')\n\n    names_fields = {\n        field.name: field\n        for field in fields\n    }\n\n    if 'data' in names_fields:\n        raise ValueError('data field shadowing underlying buffer')\n\n    if any(count > 1 for count in Counter(field.name for field in fields).values()):\n        raise ValueError('repeated field name')\n\n    # big endian format\n    fields_format = '>' + ''.join(field.format_string for field in fields_spec)\n    size = sum(field.size_bytes for field in fields_spec)\n    names_slices = compute_slices(fields_spec)\n    sorted_names = sorted(names_fields.keys())\n\n    @staticmethod\n    def get_bytes_from(buffer_, name):\n        slice_ = names_slices[name]\n        return buffer_[slice_]\n\n    def __init__(self, data):\n        if len(data) < size:\n            raise InvalidProtocolMessage(\n                'data buffer has less than the expected size {}'.format(size),\n            )\n\n        object.__setattr__(self, 'data', data)\n\n    # Intentionally exposing only the attributes from the spec, since the idea\n    # is for the instance to expose the underlying buffer as attributes\n    def __getattribute__(self, name):\n        if name in names_slices:\n            slice_ = names_slices[name]\n            field = names_fields[name]\n\n            data = object.__getattribute__(self, 'data')\n            value = data[slice_]\n\n            if field.encoder:\n                value = field.encoder.decode(value)\n\n            return value\n\n        if name == 'data':\n            return object.__getattribute__(self, 'data')\n\n        raise AttributeError\n\n    def __setattr__(self, name, value):\n        if name in names_slices:\n            slice_ = names_slices[name]\n            field = names_fields[name]\n\n            if field.encoder:\n                field.encoder.validate(value)\n                value = field.encoder.encode(value, field.size_bytes)\n\n            length = len(value)\n            if length > field.size_bytes:\n                msg = 'value with length {length} for {attr} is too big'.format(\n                    length=length,\n                    attr=name,\n                )\n                raise ValueError(msg)\n            elif length < field.size_bytes:\n                pad_size = field.size_bytes - length\n                pad_value = b'\\x00' * pad_size\n                value = pad_value + value\n\n            data = object.__getattribute__(self, 'data')\n            if isinstance(value, str):\n                value = value.encode()\n            data[slice_] = value\n        else:\n            super(self.__class__, self).__setattr__(name, value)\n\n    def __repr__(self):\n        return '<{} [...]>'.format(buffer_name)\n\n    def __len__(self):\n        return size\n\n    def __dir__(self):\n        return sorted_names\n\n    attributes = {\n        '__init__': __init__,\n        '__slots__': ('data',),\n        '__getattribute__': __getattribute__,\n        '__setattr__': __setattr__,\n        '__repr__': __repr__,\n        '__len__': __len__,\n        '__dir__': __dir__,\n\n        # These are class attributes hidden from instance, i.e. must be\n        # accessed through the class instance.\n        'fields_spec': fields_spec,\n        'format': fields_format,\n        'size': size,\n        'get_bytes_from': get_bytes_from,\n    }\n\n    return type(buffer_name, (), attributes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logs_blocks_sanity_check(from_block: BlockSpecification, to_block: BlockSpecification) -> None:\n    is_valid_from = isinstance(from_block, int) or isinstance(from_block, str)\n    assert is_valid_from, 'event log from block can be integer or latest,pending, earliest'\n    is_valid_to = isinstance(to_block, int) or isinstance(to_block, str)\n    assert is_valid_to, 'event log to block can be integer or latest,pending, earliest'", "response": "Checks that the from and to blocks passed onto log calls contain only appropriate types."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parity_discover_next_available_nonce(\n        web3: Web3,\n        address: AddressHex,\n) -> Nonce:\n    \"\"\"Returns the next available nonce for `address`.\"\"\"\n    next_nonce_encoded = web3.manager.request_blocking('parity_nextNonce', [address])\n    return Nonce(int(next_nonce_encoded, 16))", "response": "Returns the next available nonce for address."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the next available nonce for address.", "response": "def geth_discover_next_available_nonce(\n        web3: Web3,\n        address: AddressHex,\n) -> Nonce:\n    \"\"\"Returns the next available nonce for `address`.\"\"\"\n\n    # The nonces of the mempool transactions are considered used, and it's\n    # assumed these transactions are different from the ones currently pending\n    # in the client. This is a simplification, otherwise it would be necessary\n    # to filter the local pending transactions based on the mempool.\n    pool = web3.txpool.inspect or {}\n\n    # pool is roughly:\n    #\n    # {'queued': {'account1': {nonce1: ... nonce2: ...}, 'account2': ...}, 'pending': ...}\n    #\n    # Pending refers to the current block and if it contains transactions from\n    # the user, these will be the younger transactions. Because this needs the\n    # largest nonce, queued is checked first.\n\n    address = to_checksum_address(address)\n    queued = pool.get('queued', {}).get(address)\n    if queued:\n        return Nonce(max(int(k) for k in queued.keys()) + 1)\n\n    pending = pool.get('pending', {}).get(address)\n    if pending:\n        return Nonce(max(int(k) for k in pending.keys()) + 1)\n\n    # The first valid nonce is 0, therefore the count is already the next\n    # available nonce\n    return web3.eth.getTransactionCount(address, 'latest')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_address_has_code(\n        client: 'JSONRPCClient',\n        address: Address,\n        contract_name: str = '',\n):\n    \"\"\" Checks that the given address contains code. \"\"\"\n    result = client.web3.eth.getCode(to_checksum_address(address), 'latest')\n\n    if not result:\n        if contract_name:\n            formated_contract_name = '[{}]: '.format(contract_name)\n        else:\n            formated_contract_name = ''\n\n        raise AddressWithoutCode(\n            '{}Address {} does not contain code'.format(\n                formated_contract_name,\n                to_checksum_address(address),\n            ),\n        )", "response": "Checks that the given address contains code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dependencies_order_of_build(target_contract, dependencies_map):\n    if not dependencies_map:\n        return [target_contract]\n\n    if target_contract not in dependencies_map:\n        raise ValueError('no dependencies defined for {}'.format(target_contract))\n\n    order = [target_contract]\n    todo = list(dependencies_map[target_contract])\n\n    while todo:\n        target_contract = todo.pop(0)\n        target_pos = len(order)\n\n        for dependency in dependencies_map[target_contract]:\n            # we need to add the current contract before all its depedencies\n            if dependency in order:\n                target_pos = order.index(dependency)\n            else:\n                todo.append(dependency)\n\n        order.insert(target_pos, target_contract)\n\n    order.reverse()\n    return order", "response": "Return an ordered list of contracts that is sufficient to successfully deploy the target contract."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the value_error is a valid parity call.", "response": "def check_value_error_for_parity(value_error: ValueError, call_type: ParityCallType) -> bool:\n    \"\"\"\n    For parity failing calls and functions do not return None if the transaction\n    will fail but instead throw a ValueError exception.\n\n    This function checks the thrown exception to see if it's the correct one and\n    if yes returns True, if not returns False\n    \"\"\"\n    try:\n        error_data = json.loads(str(value_error).replace(\"'\", '\"'))\n    except json.JSONDecodeError:\n        return False\n\n    if call_type == ParityCallType.ESTIMATE_GAS:\n        code_checks_out = error_data['code'] == -32016\n        message_checks_out = 'The execution failed due to an exception' in error_data['message']\n    elif call_type == ParityCallType.CALL:\n        code_checks_out = error_data['code'] == -32015\n        message_checks_out = 'VM execution error' in error_data['message']\n    else:\n        raise ValueError('Called check_value_error_for_parity() with illegal call type')\n\n    if code_checks_out and message_checks_out:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patched_web3_eth_estimate_gas(self, transaction, block_identifier=None):\n    if 'from' not in transaction and is_checksum_address(self.defaultAccount):\n        transaction = assoc(transaction, 'from', self.defaultAccount)\n\n    if block_identifier is None:\n        params = [transaction]\n    else:\n        params = [transaction, block_identifier]\n\n    try:\n        result = self.web3.manager.request_blocking(\n            'eth_estimateGas',\n            params,\n        )\n    except ValueError as e:\n        if check_value_error_for_parity(e, ParityCallType.ESTIMATE_GAS):\n            result = None\n        else:\n            # else the error is not denoting estimate gas failure and is something else\n            raise e\n\n    return result", "response": "This method is deprecated in web3. py."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nestimate gas for a function", "response": "def estimate_gas_for_function(\n        address,\n        web3,\n        fn_identifier=None,\n        transaction=None,\n        contract_abi=None,\n        fn_abi=None,\n        block_identifier=None,\n        *args,\n        **kwargs,\n):\n    \"\"\"Temporary workaround until next web3.py release (5.X.X)\"\"\"\n    estimate_transaction = prepare_transaction(\n        address,\n        web3,\n        fn_identifier=fn_identifier,\n        contract_abi=contract_abi,\n        fn_abi=fn_abi,\n        transaction=transaction,\n        fn_args=args,\n        fn_kwargs=kwargs,\n    )\n\n    try:\n        gas_estimate = web3.eth.estimateGas(estimate_transaction, block_identifier)\n    except ValueError as e:\n        if check_value_error_for_parity(e, ParityCallType.ESTIMATE_GAS):\n            gas_estimate = None\n        else:\n            # else the error is not denoting estimate gas failure and is something else\n            raise e\n\n    return gas_estimate"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_block(self, block_identifier: BlockSpecification) -> Dict:\n        return self.web3.eth.getBlock(block_identifier)", "response": "Given a block number query the chain to get its corresponding block hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the block hash from the CONFIRMATION_BLOCKS in the past and returns its block hash", "response": "def get_confirmed_blockhash(self):\n        \"\"\" Gets the block CONFIRMATION_BLOCKS in the past and returns its block hash \"\"\"\n        confirmed_block_number = self.web3.eth.blockNumber - self.default_block_num_confirmations\n        if confirmed_block_number < 0:\n            confirmed_block_number = 0\n\n        return self.blockhash_from_blocknumber(confirmed_block_number)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blockhash_from_blocknumber(self, block_number: BlockSpecification) -> BlockHash:\n        block = self.get_block(block_number)\n        return BlockHash(bytes(block['hash']))", "response": "Given a block number query the chain to get its corresponding block hash"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning if the provided block identifier is safe enough to query chain state for.", "response": "def can_query_state_for_block(self, block_identifier: BlockSpecification) -> bool:\n        \"\"\"\n        Returns if the provided block identifier is safe enough to query chain\n        state for. If it's close to the state pruning blocks then state should\n        not be queried.\n        More info: https://github.com/raiden-network/raiden/issues/3566.\n        \"\"\"\n        latest_block_number = self.block_number()\n        preconditions_block = self.web3.eth.getBlock(block_identifier)\n        preconditions_block_number = int(preconditions_block['number'])\n        difference = latest_block_number - preconditions_block_number\n        return difference < constants.NO_STATE_QUERY_AFTER_BLOCKS"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef balance(self, account: Address):\n        return self.web3.eth.getBalance(to_checksum_address(account), 'pending')", "response": "Return the balance of the given account"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the parity transaction pool and searches for a transaction hash by a given nonce.", "response": "def parity_get_pending_transaction_hash_by_nonce(\n            self,\n            address: AddressHex,\n            nonce: Nonce,\n    ) -> Optional[TransactionHash]:\n        \"\"\"Queries the local parity transaction pool and searches for a transaction.\n\n        Checks the local tx pool for a transaction from a particular address and for\n        a given nonce. If it exists it returns the transaction hash.\n        \"\"\"\n        assert self.eth_node is constants.EthClient.PARITY\n        # https://wiki.parity.io/JSONRPC-parity-module.html?q=traceTransaction#parity_alltransactions\n        transactions = self.web3.manager.request_blocking('parity_allTransactions', [])\n        log.debug('RETURNED TRANSACTIONS', transactions=transactions)\n        for tx in transactions:\n            address_match = to_checksum_address(tx['from']) == address\n            if address_match and int(tx['nonce'], 16) == nonce:\n                return tx['hash']\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a proxy for interacting with a smart contract.", "response": "def new_contract_proxy(self, contract_interface, contract_address: Address):\n        \"\"\" Return a proxy for interacting with a smart contract.\n\n        Args:\n            contract_interface: The contract interface as defined by the json.\n            address: The contract's address.\n        \"\"\"\n        return ContractProxy(\n            self,\n            contract=self.new_contract(contract_interface, contract_address),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeploys a solidity contract.", "response": "def deploy_solidity_contract(\n            self,  # pylint: disable=too-many-locals\n            contract_name: str,\n            all_contracts: Dict[str, ABI],\n            libraries: Dict[str, Address] = None,\n            constructor_parameters: Tuple[Any] = None,\n            contract_path: str = None,\n    ):\n        \"\"\"\n        Deploy a solidity contract.\n\n        Args:\n            contract_name: The name of the contract to compile.\n            all_contracts: The json dictionary containing the result of compiling a file.\n            libraries: A list of libraries to use in deployment.\n            constructor_parameters: A tuple of arguments to pass to the constructor.\n            contract_path: If we are dealing with solc >= v0.4.9 then the path\n                           to the contract is a required argument to extract\n                           the contract data from the `all_contracts` dict.\n        \"\"\"\n        if libraries:\n            libraries = dict(libraries)\n        else:\n            libraries = dict()\n\n        ctor_parameters = constructor_parameters or ()\n        all_contracts = copy.deepcopy(all_contracts)\n\n        if contract_name in all_contracts:\n            contract_key = contract_name\n\n        elif contract_path is not None:\n            contract_key = os.path.basename(contract_path) + ':' + contract_name\n\n            if contract_key not in all_contracts:\n                raise ValueError('Unknown contract {}'.format(contract_name))\n        else:\n            raise ValueError(\n                'Unknown contract {} and no contract_path given'.format(contract_name),\n            )\n\n        contract = all_contracts[contract_key]\n        contract_interface = contract['abi']\n        symbols = solidity_unresolved_symbols(contract['bin'])\n\n        if symbols:\n            available_symbols = list(map(solidity_library_symbol, all_contracts.keys()))\n\n            unknown_symbols = set(symbols) - set(available_symbols)\n            if unknown_symbols:\n                msg = 'Cannot deploy contract, known symbols {}, unresolved symbols {}.'.format(\n                    available_symbols,\n                    unknown_symbols,\n                )\n                raise Exception(msg)\n\n            dependencies = deploy_dependencies_symbols(all_contracts)\n            deployment_order = dependencies_order_of_build(contract_key, dependencies)\n\n            deployment_order.pop()  # remove `contract_name` from the list\n\n            log.debug(\n                'Deploying dependencies: {}'.format(str(deployment_order)),\n                node=pex(self.address),\n            )\n\n            for deploy_contract in deployment_order:\n                dependency_contract = all_contracts[deploy_contract]\n\n                hex_bytecode = solidity_resolve_symbols(dependency_contract['bin'], libraries)\n                bytecode = decode_hex(hex_bytecode)\n\n                dependency_contract['bin'] = bytecode\n\n                gas_limit = self.web3.eth.getBlock('latest')['gasLimit'] * 8 // 10\n                transaction_hash = self.send_transaction(\n                    to=Address(b''),\n                    startgas=gas_limit,\n                    data=bytecode,\n                )\n\n                self.poll(transaction_hash)\n                receipt = self.get_transaction_receipt(transaction_hash)\n\n                contract_address = receipt['contractAddress']\n                # remove the hexadecimal prefix 0x from the address\n                contract_address = remove_0x_prefix(contract_address)\n\n                libraries[deploy_contract] = contract_address\n\n                deployed_code = self.web3.eth.getCode(to_checksum_address(contract_address))\n\n                if not deployed_code:\n                    raise RuntimeError('Contract address has no code, check gas usage.')\n\n            hex_bytecode = solidity_resolve_symbols(contract['bin'], libraries)\n            bytecode = decode_hex(hex_bytecode)\n\n            contract['bin'] = bytecode\n\n        if isinstance(contract['bin'], str):\n            contract['bin'] = decode_hex(contract['bin'])\n\n        contract_object = self.web3.eth.contract(abi=contract['abi'], bytecode=contract['bin'])\n        contract_transaction = contract_object.constructor(*ctor_parameters).buildTransaction()\n        transaction_hash = self.send_transaction(\n            to=Address(b''),\n            data=contract_transaction['data'],\n            startgas=self._gas_estimate_correction(contract_transaction['gas']),\n        )\n\n        self.poll(transaction_hash)\n        receipt = self.get_transaction_receipt(transaction_hash)\n        contract_address = receipt['contractAddress']\n\n        deployed_code = self.web3.eth.getCode(to_checksum_address(contract_address))\n\n        if not deployed_code:\n            raise RuntimeError(\n                'Deployment of {} failed. Contract address has no code, check gas usage.'.format(\n                    contract_name,\n                ),\n            )\n\n        return self.new_contract_proxy(contract_interface, contract_address), receipt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npolling for the given transaction_hash.", "response": "def poll(\n            self,\n            transaction_hash: bytes,\n    ):\n        \"\"\" Wait until the `transaction_hash` is applied or rejected.\n\n        Args:\n            transaction_hash: Transaction hash that we are waiting for.\n        \"\"\"\n        if len(transaction_hash) != 32:\n            raise ValueError(\n                'transaction_hash must be a 32 byte hash',\n            )\n\n        transaction_hash = encode_hex(transaction_hash)\n\n        # used to check if the transaction was removed, this could happen\n        # if gas price is too low:\n        #\n        # > Transaction (acbca3d6) below gas price (tx=1 Wei ask=18\n        # > Shannon). All sequential txs from this address(7d0eae79)\n        # > will be ignored\n        #\n        last_result = None\n\n        while True:\n            # Could return None for a short period of time, until the\n            # transaction is added to the pool\n            transaction = self.web3.eth.getTransaction(transaction_hash)\n\n            # if the transaction was added to the pool and then removed\n            if transaction is None and last_result is not None:\n                raise Exception('invalid transaction, check gas price')\n\n            # the transaction was added to the pool and mined\n            if transaction and transaction['blockNumber'] is not None:\n                last_result = transaction\n\n                # this will wait for both APPLIED and REVERTED transactions\n                transaction_block = transaction['blockNumber']\n                confirmation_block = transaction_block + self.default_block_num_confirmations\n\n                block_number = self.block_number()\n\n                if block_number >= confirmation_block:\n                    return transaction\n\n            gevent.sleep(1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new filter in the ethereum node.", "response": "def new_filter(\n            self,\n            contract_address: Address,\n            topics: List[str] = None,\n            from_block: BlockSpecification = 0,\n            to_block: BlockSpecification = 'latest',\n    ) -> StatelessFilter:\n        \"\"\" Create a filter in the ethereum node. \"\"\"\n        logs_blocks_sanity_check(from_block, to_block)\n        return StatelessFilter(\n            self.web3,\n            {\n                'fromBlock': from_block,\n                'toBlock': to_block,\n                'address': to_checksum_address(contract_address),\n                'topics': topics,\n            },\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_filter_events(\n            self,\n            contract_address: Address,\n            topics: List[str] = None,\n            from_block: BlockSpecification = 0,\n            to_block: BlockSpecification = 'latest',\n    ) -> List[Dict]:\n        \"\"\" Get events for the given query. \"\"\"\n        logs_blocks_sanity_check(from_block, to_block)\n        return self.web3.eth.getLogs({\n            'fromBlock': from_block,\n            'toBlock': to_block,\n            'address': to_checksum_address(contract_address),\n            'topics': topics,\n        })", "response": "Get the events for the given contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if our address has enough balance to execute the transaction.", "response": "def check_for_insufficient_eth(\n            self,\n            transaction_name: str,\n            transaction_executed: bool,\n            required_gas: int,\n            block_identifier: BlockSpecification,\n    ):\n        \"\"\" After estimate gas failure checks if our address has enough balance.\n\n        If the account did not have enough ETH balance to execute the,\n        transaction then it raises an `InsufficientFunds` error\n        \"\"\"\n        if transaction_executed:\n            return\n\n        our_address = to_checksum_address(self.address)\n        balance = self.web3.eth.getBalance(our_address, block_identifier)\n        required_balance = required_gas * self.gas_price()\n        if balance < required_balance:\n            msg = f'Failed to execute {transaction_name} due to insufficient ETH'\n            log.critical(msg, required_wei=required_balance, actual_wei=balance)\n            raise InsufficientFunds(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_db_version(db_filename: Path) -> int:\n\n    assert os.path.exists(db_filename)\n\n    # Perform a query directly through SQL rather than using\n    # storage.get_version()\n    # as get_version will return the latest version if it doesn't\n    # find a record in the database.\n    conn = sqlite3.connect(\n        str(db_filename),\n        detect_types=sqlite3.PARSE_DECLTYPES,\n    )\n    cursor = conn.cursor()\n\n    try:\n        cursor.execute('SELECT value FROM settings WHERE name=\"version\";')\n        result = cursor.fetchone()\n    except sqlite3.OperationalError:\n        raise RuntimeError(\n            'Corrupted database. Database does not the settings table.',\n        )\n\n    if not result:\n        raise RuntimeError(\n            'Corrupted database. Settings table does not contain an entry the db version.',\n        )\n\n    return int(result[0])", "response": "Return the version value stored in the db."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_or_deploy_token(runner) -> Tuple[ContractProxy, int]:\n    token_contract = runner.contract_manager.get_contract(CONTRACT_CUSTOM_TOKEN)\n\n    token_config = runner.scenario.get('token', {})\n    if not token_config:\n        token_config = {}\n    address = token_config.get('address')\n    block = token_config.get('block', 0)\n    reuse = token_config.get('reuse', False)\n\n    token_address_file = runner.data_path.joinpath('token.infos')\n    if reuse:\n        if address:\n            raise ScenarioError('Token settings \"address\" and \"reuse\" are mutually exclusive.')\n        if token_address_file.exists():\n            token_data = json.loads(token_address_file.read_text())\n            address = token_data['address']\n            block = token_data['block']\n    if address:\n        check_address_has_code(runner.client, address, 'Token')\n        token_ctr = runner.client.new_contract_proxy(token_contract['abi'], address)\n\n        log.debug(\n            \"Reusing token\",\n            address=to_checksum_address(address),\n            name=token_ctr.contract.functions.name().call(),\n            symbol=token_ctr.contract.functions.symbol().call(),\n        )\n        return token_ctr, block\n\n    token_id = uuid.uuid4()\n    now = datetime.now()\n    name = token_config.get('name', f\"Scenario Test Token {token_id!s} {now:%Y-%m-%dT%H:%M}\")\n    symbol = token_config.get('symbol', f\"T{token_id!s:.3}\")\n    decimals = token_config.get('decimals', 0)\n\n    log.debug(\"Deploying token\", name=name, symbol=symbol, decimals=decimals)\n\n    token_ctr, receipt = runner.client.deploy_solidity_contract(\n        'CustomToken',\n        runner.contract_manager.contracts,\n        constructor_parameters=(0, decimals, name, symbol),\n    )\n    contract_deployment_block = receipt['blockNumber']\n    contract_checksum_address = to_checksum_address(token_ctr.contract_address)\n    if reuse:\n        token_address_file.write_text(json.dumps({\n            'address': contract_checksum_address,\n            'block': contract_deployment_block,\n        }))\n\n    log.info(\n        \"Deployed token\",\n        address=contract_checksum_address,\n        name=name,\n        symbol=symbol,\n    )\n    return token_ctr, contract_deployment_block", "response": "Get or deploy a token contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn contract proxies for the UserDepositContract and associated token", "response": "def get_udc_and_token(runner) -> Tuple[Optional[ContractProxy], Optional[ContractProxy]]:\n    \"\"\" Return contract proxies for the UserDepositContract and associated token \"\"\"\n    from scenario_player.runner import ScenarioRunner\n    assert isinstance(runner, ScenarioRunner)\n\n    udc_config = runner.scenario.services.get('udc', {})\n\n    if not udc_config.get('enable', False):\n        return None, None\n\n    udc_address = udc_config.get('address')\n    if udc_address is None:\n        contracts = get_contracts_deployment_info(\n            chain_id=runner.chain_id,\n            version=DEVELOPMENT_CONTRACT_VERSION,\n        )\n        udc_address = contracts['contracts'][CONTRACT_USER_DEPOSIT]['address']\n    udc_abi = runner.contract_manager.get_contract_abi(CONTRACT_USER_DEPOSIT)\n    udc_proxy = runner.client.new_contract_proxy(udc_abi, udc_address)\n\n    ud_token_address = udc_proxy.contract.functions.token().call()\n    # FIXME: We assume the UD token is a CustomToken (supporting the `mint()` function)\n    custom_token_abi = runner.contract_manager.get_contract_abi(CONTRACT_CUSTOM_TOKEN)\n    ud_token_proxy = runner.client.new_contract_proxy(custom_token_abi, ud_token_address)\n\n    return udc_proxy, ud_token_proxy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mint_token_if_balance_low(\n        token_contract: ContractProxy,\n        target_address: str,\n        min_balance: int,\n        fund_amount: int,\n        gas_limit: int,\n        mint_msg: str,\n        no_action_msg: str = None,\n) -> Optional[TransactionHash]:\n    \"\"\" Check token balance and mint if below minimum \"\"\"\n    balance = token_contract.contract.functions.balanceOf(target_address).call()\n    if balance < min_balance:\n        mint_amount = fund_amount - balance\n        log.debug(mint_msg, address=target_address, amount=mint_amount)\n        return token_contract.transact('mintFor', gas_limit, mint_amount, target_address)\n    else:\n        if no_action_msg:\n            log.debug(no_action_msg, balance=balance)\n\n    return None", "response": "Check token balance and mint if below minimum"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n        if self.pre_start_check():\n            # Some other executor (or process) is running with same config:\n            raise AlreadyRunning(self)\n\n        if self.process is None:\n            command = self.command\n            if not self._shell:\n                command = self.command_parts\n\n            env = os.environ.copy()\n            env[ENV_UUID] = self._uuid\n            popen_kwargs = {\n                'shell': self._shell,\n                'stdin': subprocess.PIPE,\n                'stdout': stdout,\n                'stderr': stderr,\n                'universal_newlines': True,\n                'env': env,\n            }\n            if platform.system() != 'Windows':\n                popen_kwargs['preexec_fn'] = os.setsid\n            self.process = subprocess.Popen(\n                command,\n                **popen_kwargs,\n            )\n\n        self._set_timeout()\n\n        self.wait_for(self.check_subprocess)\n        return self", "response": "Starts the process and waits for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies paste job from `SimpleExecutor.stop()` to add the `timeout` parameter.", "response": "def stop(self, sig=None, timeout=10):\n        \"\"\" Copy paste job from `SimpleExecutor.stop()` to add the `timeout` parameter. \"\"\"\n        if self.process is None:\n            return self\n\n        if sig is None:\n            sig = self._sig_stop\n\n        try:\n            os.killpg(self.process.pid, sig)\n        except OSError as err:\n            if err.errno in IGNORED_ERROR_CODES:\n                pass\n            else:\n                raise\n\n        def process_stopped():\n            \"\"\"Return True only only when self.process is not running.\"\"\"\n            return self.running() is False\n\n        self._set_timeout(timeout)\n        try:\n            self.wait_for(process_stopped)\n        except TimeoutExpired:\n            log.warning('Timeout expired, killing process', process=self)\n            pass\n\n        self._kill_all_kids(sig)\n        self._clear_process()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_and_dispatch(self, state_change):\n\n        with self._lock:\n            timestamp = datetime.utcnow().isoformat(timespec='milliseconds')\n            state_change_id = self.storage.write_state_change(state_change, timestamp)\n            self.state_change_id = state_change_id\n\n            events = self.state_manager.dispatch(state_change)\n\n            self.storage.write_events(state_change_id, events, timestamp)\n\n        return events", "response": "Log and dispatch a state change."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef snapshot(self):\n        with self._lock:\n            current_state = self.state_manager.current_state\n            state_change_id = self.state_change_id\n\n            # otherwise no state change was dispatched\n            if state_change_id:\n                self.storage.write_state_snapshot(state_change_id, current_state)", "response": "Snapshot the application state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle a TokenNetworkCreated event.", "response": "def handle_tokennetwork_new(raiden: 'RaidenService', event: Event):\n    \"\"\" Handles a `TokenNetworkCreated` event. \"\"\"\n    data = event.event_data\n    args = data['args']\n    block_number = data['block_number']\n    token_network_address = args['token_network_address']\n    token_address = typing.TokenAddress(args['token_address'])\n    block_hash = data['block_hash']\n\n    token_network_proxy = raiden.chain.token_network(token_network_address)\n    raiden.blockchain_events.add_token_network_listener(\n        token_network_proxy=token_network_proxy,\n        contract_manager=raiden.contract_manager,\n        from_block=block_number,\n    )\n\n    token_network_state = TokenNetworkState(\n        token_network_address,\n        token_address,\n    )\n\n    transaction_hash = event.event_data['transaction_hash']\n\n    new_token_network = ContractReceiveNewTokenNetwork(\n        transaction_hash=transaction_hash,\n        payment_network_identifier=event.originating_contract,\n        token_network=token_network_state,\n        block_number=block_number,\n        block_hash=block_hash,\n    )\n    raiden.handle_and_track_state_change(new_token_network)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries the blockchain for a given event name within a certain range.", "response": "def query_blockchain_events(\n        web3: Web3,\n        contract_manager: ContractManager,\n        contract_address: Address,\n        contract_name: str,\n        topics: List,\n        from_block: BlockNumber,\n        to_block: BlockNumber,\n) -> List[Dict]:\n    \"\"\"Returns events emmitted by a contract for a given event name, within a certain range.\n\n    Args:\n        web3: A Web3 instance\n        contract_manager: A contract manager\n        contract_address: The address of the contract to be filtered, can be `None`\n        contract_name: The name of the contract\n        topics: The topics to filter for\n        from_block: The block to start search events\n        to_block: The block to stop searching for events\n\n    Returns:\n        All matching events\n    \"\"\"\n    filter_params = {\n        'fromBlock': from_block,\n        'toBlock': to_block,\n        'address': to_checksum_address(contract_address),\n        'topics': topics,\n    }\n\n    events = web3.eth.getLogs(filter_params)\n\n    contract_abi = contract_manager.get_contract_abi(contract_name)\n    return [\n        decode_event(\n            abi=contract_abi,\n            log_=raw_event,\n        )\n        for raw_event in events\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform the snapshot into a version 16 data model.", "response": "def _transform_snapshot(raw_snapshot):\n    \"\"\"\n    Version 16 data model:\n    - The top-level is always a `ChainState` object, this object will always\n      have a `PaymentMappingState`, the attribute `secrethashes_to_task` is a\n      dictionary that may be empty.\n    - `secrethashes_to_task` may have `InitiatorTask`s in it, these objects always\n      have a `manager_state: InitiatorPaymentState`,which always have\n      `initiator: InitiatorTransferState`\n\n    This migration upgrades the objects:\n\n    - `InitiatorPaymentState`, that may be contained in `secrethashes_to_task`.\n      In version 16 these objects had a single `initiator` object,\n      where in version 17 this was changed to a `Dict[SecretHash, 'InitiatorTransferState']`\n    - `InitiatorTransferState` has a new attribute `transfer_state`\n    \"\"\"\n    snapshot = json.loads(raw_snapshot)\n    secrethash_to_task = snapshot['payment_mapping']['secrethashes_to_task']\n    for secrethash, task in secrethash_to_task.items():\n        if task['_type'] != 'raiden.transfer.state.InitiatorTask':\n            continue\n\n        # The transfer is pending as long as the initiator task still exists\n        transfer_secrethash = task['manager_state']['initiator']['transfer']['lock']['secrethash']\n        task['manager_state']['initiator']['transfer_state'] = 'transfer_pending'\n        task['manager_state']['initiator_transfers'] = {\n            transfer_secrethash: task['manager_state']['initiator'],\n        }\n        del task['manager_state']['initiator']\n        secrethash_to_task[secrethash] = task\n    return json.dumps(snapshot, indent=4)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upgrade_v16_to_v17(storage: SQLiteStorage, old_version: int, current_version: int, **kwargs):\n    if old_version == SOURCE_VERSION:\n        _transform_snapshots(storage)\n\n    return TARGET_VERSION", "response": "Upgrade the InitiatorPaymentState to v17."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_db_names(paths: List[str]) -> List[str]:\n    return [\n        db_path\n        for db_path in paths\n        if VERSION_RE.match(os.path.basename(db_path))\n    ]", "response": "Returns a filtered list of paths where every name matches our format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare_contract_versions(\n        proxy: ContractProxy,\n        expected_version: str,\n        contract_name: str,\n        address: Address,\n) -> None:\n    \"\"\"Compare version strings of a contract.\n\n    If not matching raise ContractVersionMismatch. Also may raise AddressWrongContract\n    if the contract contains no code.\"\"\"\n    assert isinstance(expected_version, str)\n    try:\n        deployed_version = proxy.contract.functions.contract_version().call()\n    except BadFunctionCallOutput:\n        raise AddressWrongContract('')\n\n    deployed_version = deployed_version.replace('_', '0')\n    expected_version = expected_version.replace('_', '0')\n\n    deployed = [int(x) for x in deployed_version.split('.')]\n    expected = [int(x) for x in expected_version.split('.')]\n\n    if deployed != expected:\n        raise ContractVersionMismatch(\n            f'Provided {contract_name} contract ({to_normalized_address(address)}) '\n            f'version mismatch. Expected: {expected_version} Got: {deployed_version}',\n        )", "response": "Compare version strings of a contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_onchain_locksroots(\n        chain: 'BlockChainService',\n        canonical_identifier: CanonicalIdentifier,\n        participant1: Address,\n        participant2: Address,\n        block_identifier: BlockSpecification,\n) -> Tuple[Locksroot, Locksroot]:\n    \"\"\"Return the locksroot for `participant1` and `participant2` at `block_identifier`.\"\"\"\n    payment_channel = chain.payment_channel(canonical_identifier=canonical_identifier)\n    token_network = payment_channel.token_network\n\n    # This will not raise RaidenRecoverableError because we are providing the channel_identifier\n    participants_details = token_network.detail_participants(\n        participant1=participant1,\n        participant2=participant2,\n        channel_identifier=canonical_identifier.channel_identifier,\n        block_identifier=block_identifier,\n    )\n\n    our_details = participants_details.our_details\n    our_locksroot = our_details.locksroot\n\n    partner_details = participants_details.partner_details\n    partner_locksroot = partner_details.locksroot\n\n    return our_locksroot, partner_locksroot", "response": "Return the locksroot for participant1 and participant2 at block_identifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if defined URL returns expected status to a request.", "response": "def after_start_check(self):\n        \"\"\" Check if defined URL returns expected status to a <method> request. \"\"\"\n        try:\n            if self.url.scheme == 'http':\n                conn = HTTPConnection(self.host, self.port)\n            elif self.url.scheme == 'https':\n                ssl_context = None\n                if not self.verify_tls:\n                    ssl_context = ssl._create_unverified_context()\n                conn = HTTPSConnection(\n                    self.host,\n                    self.port,\n                    context=ssl_context,\n                )\n            else:\n                raise ValueError(f'Unsupported URL scheme: \"{self.url.scheme}\"')\n\n            self._send_request(conn)\n            response = conn.getresponse()\n            status = str(response.status)\n\n            if not self._validate_response(response):\n                return False\n\n            if status == self.status or self.status_re.match(status):\n                conn.close()\n                return True\n\n        except (HTTPException, socket.timeout, socket.error) as ex:\n            log.debug('Executor process not healthy yet', command=self.command, error=ex)\n            time.sleep(.05)\n            return False\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the process and returns self. process.", "response": "def start(self):\n        \"\"\"\n        Reimplements Executor and SimpleExecutor start to allow setting stdin/stdout/stderr/cwd\n\n        It may break input/output/communicate, but will ensure child output redirects won't\n        break parent process by filling the PIPE.\n        Also, catches ProcessExitedWithError and raise FileNotFoundError if exitcode was 127\n        \"\"\"\n        if self.pre_start_check():\n            # Some other executor (or process) is running with same config:\n            raise AlreadyRunning(self)\n\n        if self.process is None:\n            command = self.command\n            if not self._shell:\n                command = self.command_parts\n\n            if isinstance(self.stdio, (list, tuple)):\n                stdin, stdout, stderr = self.stdio\n            else:\n                stdin = stdout = stderr = self.stdio\n            env = os.environ.copy()\n            env[ENV_UUID] = self._uuid\n            popen_kwargs = {\n                'shell': self._shell,\n                'stdin': stdin,\n                'stdout': stdout,\n                'stderr': stderr,\n                'universal_newlines': True,\n                'env': env,\n                'cwd': self.cwd,\n            }\n            if platform.system() != 'Windows':\n                popen_kwargs['preexec_fn'] = os.setsid\n            self.process = subprocess.Popen(\n                command,\n                **popen_kwargs,\n            )\n\n        self._set_timeout()\n\n        try:\n            self.wait_for(self.check_subprocess)\n        except ProcessExitedWithError as e:\n            if e.exit_code == 127:\n                raise FileNotFoundError(\n                    f'Can not execute {command!r}, check that the executable exists.',\n                ) from e\n            else:\n                output_file_names = {io.name for io in (stdout, stderr) if hasattr(io, 'name')}\n                if output_file_names:\n                    log.warning('Process output file(s)', output_files=output_file_names)\n            raise\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inplace_delete_message_queue(\n        chain_state: ChainState,\n        state_change: Union[ReceiveDelivered, ReceiveProcessed],\n        queueid: QueueIdentifier,\n) -> None:\n    \"\"\" Filter messages from queue, if the queue becomes empty, cleanup the queue itself. \"\"\"\n    queue = chain_state.queueids_to_queues.get(queueid)\n    if not queue:\n        return\n\n    inplace_delete_message(\n        message_queue=queue,\n        state_change=state_change,\n    )\n\n    if len(queue) == 0:\n        del chain_state.queueids_to_queues[queueid]\n    else:\n        chain_state.queueids_to_queues[queueid] = queue", "response": "Inplace delete a message from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inplace_delete_message(\n        message_queue: List[SendMessageEvent],\n        state_change: Union[ReceiveDelivered, ReceiveProcessed],\n) -> None:\n    \"\"\" Check if the message exists in queue with ID `queueid` and exclude if found.\"\"\"\n    for message in list(message_queue):\n        message_found = (\n            message.message_identifier == state_change.message_identifier and\n            message.recipient == state_change.sender\n        )\n        if message_found:\n            message_queue.remove(message)", "response": "Inplace delete a message from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the Delivered message exists in the global queue and delete if found.", "response": "def handle_delivered(\n        chain_state: ChainState,\n        state_change: ReceiveDelivered,\n) -> TransitionResult[ChainState]:\n    \"\"\" Check if the \"Delivered\" message exists in the global queue and delete if found.\"\"\"\n    queueid = QueueIdentifier(state_change.sender, CHANNEL_IDENTIFIER_GLOBAL_QUEUE)\n    inplace_delete_message_queue(chain_state, state_change, queueid)\n    return TransitionResult(chain_state, [])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_transaction_effect_satisfied(\n        chain_state: ChainState,\n        transaction: ContractSendEvent,\n        state_change: StateChange,\n) -> bool:\n    \"\"\" True if the side-effect of `transaction` is satisfied by\n    `state_change`.\n\n    This predicate is used to clear the transaction queue. This should only be\n    done once the expected side effect of a transaction is achieved. This\n    doesn't necessarily mean that the transaction sent by *this* node was\n    mined, but only that *some* transaction which achieves the same side-effect\n    was successfully executed and mined. This distinction is important for\n    restarts and to reduce the number of state changes.\n\n    On restarts: The state of the on-chain channel could have changed while the\n    node was offline. Once the node learns about the change (e.g. the channel\n    was settled), new transactions can be dispatched by Raiden as a side effect for the\n    on-chain *event* (e.g. do the batch unlock with the latest merkle tree),\n    but the dispatched transaction could have been completed by another agent (e.g.\n    the partner node). For these cases, the transaction from a different\n    address which achieves the same side-effect is sufficient, otherwise\n    unnecessary transactions would be sent by the node.\n\n    NOTE: The above is not important for transactions sent as a side-effect for\n    a new *block*. On restart the node first synchronizes its state by querying\n    for new events, only after the off-chain state is up-to-date, a Block state\n    change is dispatched. At this point some transactions are not required\n    anymore and therefore are not dispatched.\n\n    On the number of state changes: Accepting a transaction from another\n    address removes the need for clearing state changes, e.g. when our\n    node's close transaction fails but its partner's close transaction\n    succeeds.\n    \"\"\"\n    # These transactions are not made atomic through the WAL. They are sent\n    # exclusively through the external APIs.\n    #\n    #  - ContractReceiveChannelNew\n    #  - ContractReceiveChannelNewBalance\n    #  - ContractReceiveNewPaymentNetwork\n    #  - ContractReceiveNewTokenNetwork\n    #  - ContractReceiveRouteNew\n    #\n    # Note: Deposits and Withdraws must consider a transaction with a higher\n    # value as sufficient, because the values are monotonically increasing and\n    # the transaction with a lower value will never be executed.\n\n    # Transactions are used to change the on-chain state of a channel. It\n    # doesn't matter if the sender of the transaction is the local node or\n    # another node authorized to perform the operation. So, for the following\n    # transactions, as long as the side-effects are the same, the local\n    # transaction can be removed from the queue.\n    #\n    # - An update transfer can be done by a trusted third party (i.e. monitoring service)\n    # - A close transaction can be sent by our partner\n    # - A settle transaction can be sent by anyone\n    # - A secret reveal can be done by anyone\n\n    # - A lower nonce is not a valid replacement, since that is an older balance\n    #   proof\n    # - A larger raiden state change nonce is impossible.\n    #   That would require the partner node to produce an invalid balance proof,\n    #   and this node to accept the invalid balance proof and sign it\n    is_valid_update_transfer = (\n        isinstance(state_change, ContractReceiveUpdateTransfer) and\n        isinstance(transaction, ContractSendChannelUpdateTransfer) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier and\n        state_change.nonce == transaction.balance_proof.nonce\n    )\n    if is_valid_update_transfer:\n        return True\n\n    # The balance proof data cannot be verified, the local close could have\n    # lost a race against a remote close, and the balance proof data would be\n    # the one provided by this node's partner\n    is_valid_close = (\n        isinstance(state_change, ContractReceiveChannelClosed) and\n        isinstance(transaction, ContractSendChannelClose) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_valid_close:\n        return True\n\n    is_valid_settle = (\n        isinstance(state_change, ContractReceiveChannelSettled) and\n        isinstance(transaction, ContractSendChannelSettle) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_valid_settle:\n        return True\n\n    is_valid_secret_reveal = (\n        isinstance(state_change, ContractReceiveSecretReveal) and\n        isinstance(transaction, ContractSendSecretReveal) and\n        state_change.secret == transaction.secret\n    )\n    if is_valid_secret_reveal:\n        return True\n\n    is_batch_unlock = (\n        isinstance(state_change, ContractReceiveChannelBatchUnlock) and\n        isinstance(transaction, ContractSendChannelBatchUnlock)\n    )\n    if is_batch_unlock:\n        assert isinstance(state_change, ContractReceiveChannelBatchUnlock), MYPY_ANNOTATION\n        assert isinstance(transaction, ContractSendChannelBatchUnlock), MYPY_ANNOTATION\n\n        our_address = chain_state.our_address\n\n        # Don't assume that because we sent the transaction, we are a\n        # participant\n        partner_address = None\n        if state_change.participant == our_address:\n            partner_address = state_change.partner\n        elif state_change.partner == our_address:\n            partner_address = state_change.participant\n\n        # Use the second address as the partner address, but check that a\n        # channel exists for our_address and partner_address\n        if partner_address:\n            channel_state = views.get_channelstate_by_token_network_and_partner(\n                chain_state,\n                TokenNetworkID(state_change.token_network_identifier),\n                partner_address,\n            )\n            # If the channel was cleared, that means that both\n            # sides of the channel were successfully unlocked.\n            # In this case, we clear the batch unlock\n            # transaction from the queue only in case there\n            # were no more locked funds to unlock.\n            if channel_state is None:\n                return True\n\n    return False", "response": "True if the side - effect of transaction is satisfied by state_change."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrue if the transaction is made invalid by state_change.", "response": "def is_transaction_invalidated(transaction, state_change):\n    \"\"\" True if the `transaction` is made invalid by `state_change`.\n\n    Some transactions will fail due to race conditions. The races are:\n\n    - Another transaction which has the same side effect is executed before.\n    - Another transaction which *invalidates* the state of the smart contract\n    required by the local transaction is executed before it.\n\n    The first case is handled by the predicate `is_transaction_effect_satisfied`,\n    where a transaction from a different source which does the same thing is\n    considered. This predicate handles the second scenario.\n\n    A transaction can **only** invalidate another iff both share a valid\n    initial state but a different end state.\n\n    Valid example:\n\n        A close can invalidate a deposit, because both a close and a deposit\n        can be executed from an opened state (same initial state), but a close\n        transaction will transition the channel to a closed state which doesn't\n        allow for deposits (different end state).\n\n    Invalid example:\n\n        A settle transaction cannot invalidate a deposit because a settle is\n        only allowed for the closed state and deposits are only allowed for\n        the open state. In such a case a deposit should never have been sent.\n        The deposit transaction for an invalid state is a bug and not a\n        transaction which was invalidated.\n    \"\"\"\n    # Most transactions cannot be invalidated by others. These are:\n    #\n    # - close transactions\n    # - settle transactions\n    # - batch unlocks\n    #\n    # Deposits and withdraws are invalidated by the close, but these are not\n    # made atomic through the WAL.\n\n    is_our_failed_update_transfer = (\n        isinstance(state_change, ContractReceiveChannelSettled) and\n        isinstance(transaction, ContractSendChannelUpdateTransfer) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_our_failed_update_transfer:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntruing if the transaction is expired.", "response": "def is_transaction_expired(transaction, block_number):\n    \"\"\" True if transaction cannot be mined because it has expired.\n\n    Some transactions are time dependent, e.g. the secret registration must be\n    done before the lock expiration, and the update transfer must be done\n    before the settlement window is over. If the current block is higher than\n    any of these expirations blocks, the transaction is expired and cannot be\n    successfully executed.\n    \"\"\"\n\n    is_update_expired = (\n        isinstance(transaction, ContractSendChannelUpdateTransfer) and\n        transaction.expiration < block_number\n    )\n    if is_update_expired:\n        return True\n\n    is_secret_register_expired = (\n        isinstance(transaction, ContractSendSecretReveal) and\n        transaction.expiration < block_number\n    )\n    if is_secret_register_expired:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deposit(\n            self,\n            beneficiary: Address,\n            total_deposit: TokenAmount,\n            block_identifier: BlockSpecification,\n    ) -> None:\n        \"\"\" Deposit provided amount into the user-deposit contract\n        to the beneficiary's account. \"\"\"\n\n        token_address = self.token_address(block_identifier)\n        token = Token(\n            jsonrpc_client=self.client,\n            token_address=token_address,\n            contract_manager=self.contract_manager,\n        )\n\n        log_details = {\n            'beneficiary': pex(beneficiary),\n            'contract': pex(self.address),\n            'total_deposit': total_deposit,\n        }\n\n        checking_block = self.client.get_checking_block()\n        error_prefix = 'Call to deposit will fail'\n\n        with self.deposit_lock:\n            amount_to_deposit, log_details = self._deposit_preconditions(\n                total_deposit=total_deposit,\n                beneficiary=beneficiary,\n                token=token,\n                block_identifier=block_identifier,\n            )\n            gas_limit = self.proxy.estimate_gas(\n                checking_block,\n                'deposit',\n                to_checksum_address(beneficiary),\n                total_deposit,\n            )\n\n            if gas_limit:\n                error_prefix = 'Call to deposit failed'\n                log.debug('deposit called', **log_details)\n                transaction_hash = self.proxy.transact(\n                    'deposit',\n                    safe_gas_limit(gas_limit),\n                    to_checksum_address(beneficiary),\n                    total_deposit,\n                )\n\n                self.client.poll(transaction_hash)\n                receipt_or_none = check_transaction_threw(self.client, transaction_hash)\n\n            transaction_executed = gas_limit is not None\n            if not transaction_executed or receipt_or_none:\n                if transaction_executed:\n                    block = receipt_or_none['blockNumber']\n                else:\n                    block = checking_block\n\n                self.proxy.jsonrpc_client.check_for_insufficient_eth(\n                    transaction_name='deposit',\n                    transaction_executed=transaction_executed,\n                    required_gas=GAS_REQUIRED_FOR_UDC_DEPOSIT,\n                    block_identifier=block,\n                )\n\n                msg = self._check_why_deposit_failed(\n                    token=token,\n                    amount_to_deposit=amount_to_deposit,\n                    total_deposit=total_deposit,\n                    block_identifier=block,\n                )\n                error_msg = f'{error_prefix}. {msg}'\n                log.critical(error_msg, **log_details)\n                raise RaidenUnrecoverableError(error_msg)\n\n        log.info('deposit successful', **log_details)", "response": "Deposit the amount into the user - deposit contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef effective_balance(self, address: Address, block_identifier: BlockSpecification) -> Balance:\n        fn = getattr(self.proxy.contract.functions, 'effectiveBalance')\n        balance = fn(address).call(block_identifier=block_identifier)\n\n        if balance == b'':\n            raise RuntimeError(f\"Call to 'effectiveBalance' returned nothing\")\n\n        return balance", "response": "The user s balance with planned withdrawals deducted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the integer is in the value range.", "response": "def validate(self, value: int):\n        ''' Validates the integer is in the value range. '''\n        if not isinstance(value, int):\n            raise ValueError('value is not an integer')\n\n        if self.minimum > value or self.maximum < value:\n            msg = (\n                '{} is outside the valide range [{},{}]'\n            ).format(value, self.minimum, self.maximum)\n            raise ValueError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef event_filter_for_payments(\n        event: architecture.Event,\n        token_network_identifier: TokenNetworkID = None,\n        partner_address: Address = None,\n) -> bool:\n    \"\"\"Filters out non payment history related events\n\n    - If no other args are given, all payment related events match\n    - If a token network identifier is given then only payment events for that match\n    - If a partner is also given then if the event is a payment sent event and the\n      target matches it's returned. If it's a payment received and the initiator matches\n      then it's returned.\n    \"\"\"\n    is_matching_event = (\n        isinstance(event, EVENTS_PAYMENT_HISTORY_RELATED) and\n        (\n            token_network_identifier is None or\n            token_network_identifier == event.token_network_identifier\n        )\n    )\n    if not is_matching_event:\n        return False\n\n    sent_and_target_matches = (\n        isinstance(event, (EventPaymentSentFailed, EventPaymentSentSuccess)) and\n        (\n            partner_address is None or\n            event.target == partner_address\n        )\n    )\n    received_and_initiator_matches = (\n        isinstance(event, EventPaymentReceivedSuccess) and\n        (\n            partner_address is None or\n            event.initiator == partner_address\n        )\n    )\n    return sent_and_target_matches or received_and_initiator_matches", "response": "Filters out events related to payments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef token_network_register(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            channel_participant_deposit_limit: TokenAmount,\n            token_network_deposit_limit: TokenAmount,\n            retry_timeout: NetworkTimeout = DEFAULT_RETRY_TIMEOUT,\n    ) -> TokenNetworkAddress:\n        \"\"\"Register the `token_address` in the blockchain. If the address is already\n           registered but the event has not been processed this function will block\n           until the next block to make sure the event is processed.\n\n        Raises:\n            InvalidAddress: If the registry_address or token_address is not a valid address.\n            AlreadyRegisteredTokenAddress: If the token is already registered.\n            TransactionThrew: If the register transaction failed, this may\n                happen because the account has not enough balance to pay for the\n                gas or this register call raced with another transaction and lost.\n        \"\"\"\n\n        if not is_binary_address(registry_address):\n            raise InvalidAddress('registry_address must be a valid address in binary')\n\n        if not is_binary_address(token_address):\n            raise InvalidAddress('token_address must be a valid address in binary')\n\n        if token_address in self.get_tokens_list(registry_address):\n            raise AlreadyRegisteredTokenAddress('Token already registered')\n\n        contracts_version = self.raiden.contract_manager.contracts_version\n\n        registry = self.raiden.chain.token_network_registry(registry_address)\n\n        try:\n            if contracts_version == DEVELOPMENT_CONTRACT_VERSION:\n                return registry.add_token_with_limits(\n                    token_address=token_address,\n                    channel_participant_deposit_limit=channel_participant_deposit_limit,\n                    token_network_deposit_limit=token_network_deposit_limit,\n                )\n            else:\n                return registry.add_token_without_limits(\n                    token_address=token_address,\n                )\n        except RaidenRecoverableError as e:\n            if 'Token already registered' in str(e):\n                raise AlreadyRegisteredTokenAddress('Token already registered')\n            # else\n            raise\n\n        finally:\n            # Assume the transaction failed because the token is already\n            # registered with the smart contract and this node has not yet\n            # polled for the event (otherwise the check above would have\n            # failed).\n            #\n            # To provide a consistent view to the user, wait one block, this\n            # will guarantee that the events have been processed.\n            next_block = self.raiden.get_block_number() + 1\n            waiting.wait_for_block(self.raiden, next_block, retry_timeout)", "response": "Register the token_address in the token network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting the given token network to the given token network.", "response": "def token_network_connect(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            funds: TokenAmount,\n            initial_channel_target: int = 3,\n            joinable_funds_target: float = 0.4,\n    ) -> None:\n        \"\"\" Automatically maintain channels open for the given token network.\n\n        Args:\n            token_address: the ERC20 token network to connect to.\n            funds: the amount of funds that can be used by the ConnectionMananger.\n            initial_channel_target: number of channels to open proactively.\n            joinable_funds_target: fraction of the funds that will be used to join\n                channels opened by other participants.\n        \"\"\"\n        if not is_binary_address(registry_address):\n            raise InvalidAddress('registry_address must be a valid address in binary')\n        if not is_binary_address(token_address):\n            raise InvalidAddress('token_address must be a valid address in binary')\n\n        token_network_identifier = views.get_token_network_identifier_by_token_address(\n            chain_state=views.state_from_raiden(self.raiden),\n            payment_network_id=registry_address,\n            token_address=token_address,\n        )\n\n        connection_manager = self.raiden.connection_manager_for_token_network(\n            token_network_identifier,\n        )\n\n        has_enough_reserve, estimated_required_reserve = has_enough_gas_reserve(\n            raiden=self.raiden,\n            channels_to_open=initial_channel_target,\n        )\n\n        if not has_enough_reserve:\n            raise InsufficientGasReserve((\n                'The account balance is below the estimated amount necessary to '\n                'finish the lifecycles of all active channels. A balance of at '\n                f'least {estimated_required_reserve} wei is required.'\n            ))\n\n        connection_manager.connect(\n            funds=funds,\n            initial_channel_target=initial_channel_target,\n            joinable_funds_target=joinable_funds_target,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef token_network_leave(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n    ) -> List[NettingChannelState]:\n        \"\"\" Close all channels and wait for settlement. \"\"\"\n        if not is_binary_address(registry_address):\n            raise InvalidAddress('registry_address must be a valid address in binary')\n        if not is_binary_address(token_address):\n            raise InvalidAddress('token_address must be a valid address in binary')\n\n        if token_address not in self.get_tokens_list(registry_address):\n            raise UnknownTokenAddress('token_address unknown')\n\n        token_network_identifier = views.get_token_network_identifier_by_token_address(\n            chain_state=views.state_from_raiden(self.raiden),\n            payment_network_id=registry_address,\n            token_address=token_address,\n        )\n\n        connection_manager = self.raiden.connection_manager_for_token_network(\n            token_network_identifier,\n        )\n\n        return connection_manager.leave(registry_address)", "response": "Closes all channels and wait for settlement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef channel_open(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            partner_address: Address,\n            settle_timeout: BlockTimeout = None,\n            retry_timeout: NetworkTimeout = DEFAULT_RETRY_TIMEOUT,\n    ) -> ChannelID:\n        \"\"\" Open a channel with the peer at `partner_address`\n        with the given `token_address`.\n        \"\"\"\n        if settle_timeout is None:\n            settle_timeout = self.raiden.config['settle_timeout']\n\n        if settle_timeout < self.raiden.config['reveal_timeout'] * 2:\n            raise InvalidSettleTimeout(\n                'settle_timeout can not be smaller than double the reveal_timeout',\n            )\n\n        if not is_binary_address(registry_address):\n            raise InvalidAddress('Expected binary address format for registry in channel open')\n\n        if not is_binary_address(token_address):\n            raise InvalidAddress('Expected binary address format for token in channel open')\n\n        if not is_binary_address(partner_address):\n            raise InvalidAddress('Expected binary address format for partner in channel open')\n\n        chain_state = views.state_from_raiden(self.raiden)\n        channel_state = views.get_channelstate_for(\n            chain_state=chain_state,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            partner_address=partner_address,\n        )\n\n        if channel_state:\n            raise DuplicatedChannelError('Channel with given partner address already exists')\n\n        registry = self.raiden.chain.token_network_registry(registry_address)\n        token_network_address = registry.get_token_network(token_address)\n\n        if token_network_address is None:\n            raise TokenNotRegistered(\n                'Token network for token %s does not exist' % to_checksum_address(token_address),\n            )\n\n        token_network = self.raiden.chain.token_network(\n            registry.get_token_network(token_address),\n        )\n\n        with self.raiden.gas_reserve_lock:\n            has_enough_reserve, estimated_required_reserve = has_enough_gas_reserve(\n                self.raiden,\n                channels_to_open=1,\n            )\n\n            if not has_enough_reserve:\n                raise InsufficientGasReserve((\n                    'The account balance is below the estimated amount necessary to '\n                    'finish the lifecycles of all active channels. A balance of at '\n                    f'least {estimated_required_reserve} wei is required.'\n                ))\n\n            try:\n                token_network.new_netting_channel(\n                    partner=partner_address,\n                    settle_timeout=settle_timeout,\n                    given_block_identifier=views.state_from_raiden(self.raiden).block_hash,\n                )\n            except DuplicatedChannelError:\n                log.info('partner opened channel first')\n\n        waiting.wait_for_newchannel(\n            raiden=self.raiden,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            partner_address=partner_address,\n            retry_timeout=retry_timeout,\n        )\n        chain_state = views.state_from_raiden(self.raiden)\n        channel_state = views.get_channelstate_for(\n            chain_state=chain_state,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            partner_address=partner_address,\n        )\n\n        assert channel_state, f'channel {channel_state} is gone'\n\n        return channel_state.identifier", "response": "Open a channel with the given token_address and partner_address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_total_channel_deposit(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            partner_address: Address,\n            total_deposit: TokenAmount,\n            retry_timeout: NetworkTimeout = DEFAULT_RETRY_TIMEOUT,\n    ):\n        \"\"\" Set the `total_deposit` in the channel with the peer at `partner_address` and the\n        given `token_address` in order to be able to do transfers.\n\n        Raises:\n            InvalidAddress: If either token_address or partner_address is not\n                20 bytes long.\n            TransactionThrew: May happen for multiple reasons:\n                - If the token approval fails, e.g. the token may validate if\n                account has enough balance for the allowance.\n                - The deposit failed, e.g. the allowance did not set the token\n                aside for use and the user spent it before deposit was called.\n                - The channel was closed/settled between the allowance call and\n                the deposit call.\n            AddressWithoutCode: The channel was settled during the deposit\n                execution.\n            DepositOverLimit: The total deposit amount is higher than the limit.\n        \"\"\"\n        chain_state = views.state_from_raiden(self.raiden)\n\n        token_addresses = views.get_token_identifiers(\n            chain_state,\n            registry_address,\n        )\n        channel_state = views.get_channelstate_for(\n            chain_state=chain_state,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            partner_address=partner_address,\n        )\n\n        if not is_binary_address(token_address):\n            raise InvalidAddress('Expected binary address format for token in channel deposit')\n\n        if not is_binary_address(partner_address):\n            raise InvalidAddress('Expected binary address format for partner in channel deposit')\n\n        if token_address not in token_addresses:\n            raise UnknownTokenAddress('Unknown token address')\n\n        if channel_state is None:\n            raise InvalidAddress('No channel with partner_address for the given token')\n\n        if self.raiden.config['environment_type'] == Environment.PRODUCTION:\n            per_token_network_deposit_limit = RED_EYES_PER_TOKEN_NETWORK_LIMIT\n        else:\n            per_token_network_deposit_limit = UINT256_MAX\n\n        token = self.raiden.chain.token(token_address)\n        token_network_registry = self.raiden.chain.token_network_registry(registry_address)\n        token_network_address = token_network_registry.get_token_network(token_address)\n        token_network_proxy = self.raiden.chain.token_network(token_network_address)\n        channel_proxy = self.raiden.chain.payment_channel(\n            canonical_identifier=channel_state.canonical_identifier,\n        )\n\n        if total_deposit == 0:\n            raise DepositMismatch('Attempted to deposit with total deposit being 0')\n\n        addendum = total_deposit - channel_state.our_state.contract_balance\n\n        total_network_balance = token.balance_of(registry_address)\n\n        if total_network_balance + addendum > per_token_network_deposit_limit:\n            raise DepositOverLimit(\n                f'The deposit of {addendum} will exceed the '\n                f'token network limit of {per_token_network_deposit_limit}',\n            )\n\n        balance = token.balance_of(self.raiden.address)\n\n        functions = token_network_proxy.proxy.contract.functions\n        deposit_limit = functions.channel_participant_deposit_limit().call()\n\n        if total_deposit > deposit_limit:\n            raise DepositOverLimit(\n                f'The additional deposit of {addendum} will exceed the '\n                f'channel participant limit of {deposit_limit}',\n            )\n\n        # If this check succeeds it does not imply the the `deposit` will\n        # succeed, since the `deposit` transaction may race with another\n        # transaction.\n        if not balance >= addendum:\n            msg = 'Not enough balance to deposit. {} Available={} Needed={}'.format(\n                pex(token_address),\n                balance,\n                addendum,\n            )\n            raise InsufficientFunds(msg)\n\n        # set_total_deposit calls approve\n        # token.approve(netcontract_address, addendum)\n        channel_proxy.set_total_deposit(\n            total_deposit=total_deposit,\n            block_identifier=views.state_from_raiden(self.raiden).block_hash,\n        )\n\n        target_address = self.raiden.address\n        waiting.wait_for_participant_newbalance(\n            raiden=self.raiden,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            partner_address=partner_address,\n            target_address=target_address,\n            target_balance=total_deposit,\n            retry_timeout=retry_timeout,\n        )", "response": "Set the total deposit in the channel with the peer at partner_address."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncloses a channel with the given token_address.", "response": "def channel_close(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            partner_address: Address,\n            retry_timeout: NetworkTimeout = DEFAULT_RETRY_TIMEOUT,\n    ):\n        \"\"\"Close a channel opened with `partner_address` for the given\n        `token_address`.\n\n        Race condition, this can fail if channel was closed externally.\n        \"\"\"\n        self.channel_batch_close(\n            registry_address=registry_address,\n            token_address=token_address,\n            partner_addresses=[partner_address],\n            retry_timeout=retry_timeout,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncloses a channel with the given partner address.", "response": "def channel_batch_close(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            partner_addresses: List[Address],\n            retry_timeout: NetworkTimeout = DEFAULT_RETRY_TIMEOUT,\n    ):\n        \"\"\"Close a channel opened with `partner_address` for the given\n        `token_address`.\n\n        Race condition, this can fail if channel was closed externally.\n        \"\"\"\n\n        if not is_binary_address(token_address):\n            raise InvalidAddress('Expected binary address format for token in channel close')\n\n        if not all(map(is_binary_address, partner_addresses)):\n            raise InvalidAddress('Expected binary address format for partner in channel close')\n\n        valid_tokens = views.get_token_identifiers(\n            chain_state=views.state_from_raiden(self.raiden),\n            payment_network_id=registry_address,\n        )\n        if token_address not in valid_tokens:\n            raise UnknownTokenAddress('Token address is not known.')\n\n        chain_state = views.state_from_raiden(self.raiden)\n        channels_to_close = views.filter_channels_by_partneraddress(\n            chain_state=chain_state,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            partner_addresses=partner_addresses,\n        )\n\n        greenlets: Set[Greenlet] = set()\n        for channel_state in channels_to_close:\n            channel_close = ActionChannelClose(\n                canonical_identifier=channel_state.canonical_identifier,\n            )\n\n            greenlets.update(\n                self.raiden.handle_state_change(channel_close),\n            )\n\n        gevent.joinall(greenlets, raise_error=True)\n\n        channel_ids = [channel_state.identifier for channel_state in channels_to_close]\n\n        waiting.wait_for_close(\n            raiden=self.raiden,\n            payment_network_id=registry_address,\n            token_address=token_address,\n            channel_ids=channel_ids,\n            retry_timeout=retry_timeout,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all channels associated with the optionally given token_address and or partner_address. Optionally a token_address and partner_address must be provided.", "response": "def get_channel_list(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress = None,\n            partner_address: Address = None,\n    ) -> List[NettingChannelState]:\n        \"\"\"Returns a list of channels associated with the optionally given\n           `token_address` and/or `partner_address`.\n\n        Args:\n            token_address: an optionally provided token address\n            partner_address: an optionally provided partner address\n\n        Return:\n            A list containing all channels the node participates. Optionally\n            filtered by a token address and/or partner address.\n\n        Raises:\n            KeyError: An error occurred when the token address is unknown to the node.\n        \"\"\"\n        if registry_address and not is_binary_address(registry_address):\n            raise InvalidAddress('Expected binary address format for registry in get_channel_list')\n\n        if token_address and not is_binary_address(token_address):\n            raise InvalidAddress('Expected binary address format for token in get_channel_list')\n\n        if partner_address:\n            if not is_binary_address(partner_address):\n                raise InvalidAddress(\n                    'Expected binary address format for partner in get_channel_list',\n                )\n            if not token_address:\n                raise UnknownTokenAddress('Provided a partner address but no token address')\n\n        if token_address and partner_address:\n            channel_state = views.get_channelstate_for(\n                chain_state=views.state_from_raiden(self.raiden),\n                payment_network_id=registry_address,\n                token_address=token_address,\n                partner_address=partner_address,\n            )\n\n            if channel_state:\n                result = [channel_state]\n            else:\n                result = []\n\n        elif token_address:\n            result = views.list_channelstate_for_tokennetwork(\n                chain_state=views.state_from_raiden(self.raiden),\n                payment_network_id=registry_address,\n                token_address=token_address,\n            )\n\n        else:\n            result = views.list_all_channelstate(\n                chain_state=views.state_from_raiden(self.raiden),\n            )\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_node_network_state(self, node_address: Address):\n        return views.get_node_network_status(\n            chain_state=views.state_from_raiden(self.raiden),\n            node_address=node_address,\n        )", "response": "Returns the currently network status of node_address."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tokens_list(self, registry_address: PaymentNetworkID):\n        tokens_list = views.get_token_identifiers(\n            chain_state=views.state_from_raiden(self.raiden),\n            payment_network_id=registry_address,\n        )\n        return tokens_list", "response": "Returns a list of tokens the node knows about"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transfer_and_wait(\n            self,\n            registry_address: PaymentNetworkID,\n            token_address: TokenAddress,\n            amount: TokenAmount,\n            target: Address,\n            identifier: PaymentID = None,\n            transfer_timeout: int = None,\n            secret: Secret = None,\n            secret_hash: SecretHash = None,\n    ):\n        \"\"\" Do a transfer with `target` with the given `amount` of `token_address`. \"\"\"\n        # pylint: disable=too-many-arguments\n\n        payment_status = self.transfer_async(\n            registry_address=registry_address,\n            token_address=token_address,\n            amount=amount,\n            target=target,\n            identifier=identifier,\n            secret=secret,\n            secret_hash=secret_hash,\n        )\n        payment_status.payment_done.wait(timeout=transfer_timeout)\n        return payment_status", "response": "Transfer with target with given amount of token_address and wait for the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of blockchain events coresponding to the token_address.", "response": "def get_blockchain_events_token_network(\n            self,\n            token_address: TokenAddress,\n            from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,\n            to_block: BlockSpecification = 'latest',\n    ):\n        \"\"\"Returns a list of blockchain events coresponding to the token_address.\"\"\"\n\n        if not is_binary_address(token_address):\n            raise InvalidAddress(\n                'Expected binary address format for token in get_blockchain_events_token_network',\n            )\n\n        token_network_address = self.raiden.default_registry.get_token_network(\n            token_address,\n        )\n\n        if token_network_address is None:\n            raise UnknownTokenAddress('Token address is not known.')\n\n        returned_events = blockchain_events.get_token_network_events(\n            chain=self.raiden.chain,\n            token_network_address=token_network_address,\n            contract_manager=self.raiden.contract_manager,\n            events=blockchain_events.ALL_EVENTS,\n            from_block=from_block,\n            to_block=to_block,\n        )\n\n        for event in returned_events:\n            if event.get('args'):\n                event['args'] = dict(event['args'])\n\n        returned_events.sort(key=lambda evt: evt.get('block_number'), reverse=True)\n        return returned_events"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_monitoring_request(\n            self,\n            balance_proof: BalanceProofSignedState,\n            reward_amount: TokenAmount,\n    ) -> Optional[RequestMonitoring]:\n        \"\"\" This method can be used to create a `RequestMonitoring` message.\n        It will contain all data necessary for an external monitoring service to\n        - send an updateNonClosingBalanceProof transaction to the TokenNetwork contract,\n        for the `balance_proof` that we received from a channel partner.\n        - claim the `reward_amount` from the UDC.\n        \"\"\"\n        # create RequestMonitoring message from the above + `reward_amount`\n        monitor_request = RequestMonitoring.from_balance_proof_signed_state(\n            balance_proof=balance_proof,\n            reward_amount=reward_amount,\n        )\n        # sign RequestMonitoring and return\n        monitor_request.sign(self.raiden.signer)\n        return monitor_request", "response": "This method creates a RequestMonitoring message from the given balance_proof and reward_amount."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a LockedTransferSignedState from a LockedTransfer message.", "response": "def lockedtransfersigned_from_message(message: 'LockedTransfer') -> 'LockedTransferSignedState':\n    \"\"\" Create LockedTransferSignedState from a LockedTransfer message. \"\"\"\n    balance_proof = balanceproof_from_envelope(message)\n\n    lock = HashTimeLockState(\n        message.lock.amount,\n        message.lock.expiration,\n        message.lock.secrethash,\n    )\n\n    transfer_state = LockedTransferSignedState(\n        message.message_identifier,\n        message.payment_identifier,\n        message.token,\n        balance_proof,\n        lock,\n        message.initiator,\n        message.target,\n    )\n\n    return transfer_state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhaving the common app options as a decorator facilitates reuse.", "response": "def options(func):\n    \"\"\"Having the common app options as a decorator facilitates reuse.\"\"\"\n\n    # Until https://github.com/pallets/click/issues/926 is fixed the options need to be re-defined\n    # for every use\n    options_ = [\n        option(\n            '--datadir',\n            help='Directory for storing raiden data.',\n            default=lambda: os.path.join(os.path.expanduser('~'), '.raiden'),\n            type=click.Path(\n                exists=False,\n                dir_okay=True,\n                file_okay=False,\n                writable=True,\n                resolve_path=True,\n                allow_dash=False,\n            ),\n            show_default=True,\n        ),\n        option(\n            '--config-file',\n            help='Configuration file (TOML)',\n            default=os.path.join('${datadir}', 'config.toml'),\n            type=PathRelativePath(\n                file_okay=True,\n                dir_okay=False,\n                exists=False,\n                readable=True,\n                resolve_path=True,\n            ),\n            show_default=True,\n        ),\n        option(\n            '--keystore-path',\n            help=(\n                'If you have a non-standard path for the ethereum keystore directory'\n                ' provide it using this argument.'\n            ),\n            default=None,\n            type=click.Path(exists=True),\n            show_default=True,\n        ),\n        option(\n            '--address',\n            help=(\n                'The ethereum address you would like raiden to use and for which '\n                'a keystore file exists in your local system.'\n            ),\n            default=None,\n            type=ADDRESS_TYPE,\n            show_default=True,\n        ),\n        option(\n            '--password-file',\n            help='Text file containing the password for the provided account',\n            default=None,\n            type=click.File(lazy=True),\n            show_default=True,\n        ),\n        option(\n            '--tokennetwork-registry-contract-address',\n            help='hex encoded address of the Token Network Registry contract.',\n            type=ADDRESS_TYPE,\n            show_default=True,\n        ),\n        option(\n            '--secret-registry-contract-address',\n            help='hex encoded address of the Secret Registry contract.',\n            type=ADDRESS_TYPE,\n            show_default=True,\n        ),\n        option(\n            '--service-registry-contract-address',\n            help='hex encoded address of the Service Registry contract.',\n            type=ADDRESS_TYPE,\n        ),\n        option(\n            '--endpoint-registry-contract-address',\n            help='hex encoded address of the Endpoint Registry contract.',\n            type=ADDRESS_TYPE,\n            show_default=True,\n        ),\n        option(\n            '--user-deposit-contract-address',\n            help='hex encoded address of the User Deposit contract.',\n            type=ADDRESS_TYPE,\n        ),\n        option(\n            '--console',\n            help='Start the interactive raiden console',\n            is_flag=True,\n        ),\n        option(\n            '--transport',\n            help='Transport system to use. UDP is not recommended',\n            type=click.Choice(['udp', 'matrix']),\n            default='matrix',\n            show_default=True,\n        ),\n        option(\n            '--network-id',\n            help=(\n                'Specify the network name/id of the Ethereum network to run Raiden on.\\n'\n                'Available networks:\\n'\n                '\"mainnet\" - network id: 1\\n'\n                '\"ropsten\" - network id: 3\\n'\n                '\"rinkeby\" - network id: 4\\n'\n                '\"goerli\" - network id: 5\\n'\n                '\"kovan\" - network id: 42\\n'\n                '\"<NETWORK_ID>\": use the given network id directly\\n'\n            ),\n            type=NetworkChoiceType([\n                'mainnet',\n                'ropsten',\n                'rinkeby',\n                'goerli',\n                'kovan',\n                '<NETWORK_ID>',\n            ]),\n            default='mainnet',\n            show_default=True,\n        ),\n        option(\n            '--environment-type',\n            help=(\n                'Specify the environment (production or development).\\n'\n                'The \"production\" setting adds some safety measures and is mainly intended '\n                'for running Raiden on the mainnet.\\n'\n            ),\n            type=EnumChoiceType(Environment),\n            default=Environment.PRODUCTION.value,\n            show_default=True,\n        ),\n        option(\n            '--accept-disclaimer',\n            help='Bypass the experimental software disclaimer prompt',\n            is_flag=True,\n        ),\n        option(\n            '--showconfig',\n            help='Show all configuration values used to control Raiden\\'s behavior',\n            is_flag=True,\n        ),\n        option_group(\n            'Ethereum Node Options',\n            option(\n                '--sync-check/--no-sync-check',\n                help='Checks if the ethereum node is synchronized against etherscan.',\n                default=True,\n                show_default=True,\n            ),\n            option(\n                '--gas-price',\n                help=(\n                    'Set the gas price for ethereum transactions. If not provided '\n                    'the normal gas price startegy is used.\\n'\n                    'Available options:\\n'\n                    '\"fast\" - transactions are usually mined within 60 seconds\\n'\n                    '\"normal\" - transactions are usually mined within 5 minutes\\n'\n                    '<GAS_PRICE> - use given gas price\\n'\n                ),\n                type=GasPriceChoiceType(['normal', 'fast']),\n                default='fast',\n                show_default=True,\n            ),\n            option(\n                '--eth-rpc-endpoint',\n                help=(\n                    '\"host:port\" address of ethereum JSON-RPC server.\\n'\n                    'Also accepts a protocol prefix (http:// or https://) with optional port'\n                ),\n                default='http://127.0.0.1:8545',  # geth default jsonrpc port\n                type=str,\n                show_default=True,\n            ),\n        ),\n        option_group(\n            'Raiden Services Options',\n            option(\n                '--routing-mode',\n                help=(\n                    'Specify the routing mode to be used.\\n'\n                    '\"basic\": use local routing\\n'\n                    '\"pfs\": use the path finding service\\n'\n                ),\n                type=EnumChoiceType(RoutingMode),\n                default=RoutingMode.BASIC.value,\n                show_default=True,\n            ),\n            option(\n                '--pathfinding-service-address',\n                help=(\n                    'URL to the Raiden path finding service to request paths from.\\n'\n                    'Example: https://pfs-ropsten.services-dev.raiden.network\\n'\n                    'Can also be given the \"auto\" value so that raiden chooses a '\n                    'PFS randomly from the service registry contract'\n                ),\n                default='auto',\n                type=str,\n                show_default=True,\n            ),\n            option(\n                '--pathfinding-eth-address',\n                help=(\n                    'Ethereum address to which to pay the fees of the path finding service.\\n'\n                    'If the path finding service is chosen from the service registry contract, '\n                    'this option will be ignored. If the path finding service is configured '\n                    'manually, i. e. \"--pathfinding-service-address\" set to a value other than '\n                    '\"auto\", this argument must be set to a valid EIP55 address.'\n                ),\n                type=str,\n            ),\n            option(\n                '--pathfinding-max-paths',\n                help='Set maximum number of paths to be requested from the path finding service.',\n                default=DEFAULT_PATHFINDING_MAX_PATHS,\n                type=int,\n                show_default=True,\n            ),\n            option(\n                '--pathfinding-max-fee',\n                help='Set max fee per request paid to the path finding service.',\n                default=DEFAULT_PATHFINDING_MAX_FEE,\n                type=int,\n                show_default=True,\n            ),\n            option(\n                '--pathfinding-iou-timeout',\n                help='Number of blocks before a new IOU to the path finding service expires.',\n                default=DEFAULT_PATHFINDING_IOU_TIMEOUT,\n                type=int,\n                show_default=True,\n            ),\n            option(\n                '--enable-monitoring',\n                help='Enable broadcasting of balance proofs to the monitoring services.',\n                is_flag=True,\n            ),\n        ),\n        option_group(\n            'UDP Transport Options',\n            option(\n                '--listen-address',\n                help='\"host:port\" for the raiden service to listen on.',\n                default='0.0.0.0:{}'.format(INITIAL_PORT),\n                type=str,\n                show_default=True,\n            ),\n            option(\n                '--max-unresponsive-time',\n                help=(\n                    'Max time in seconds for which an address can send no packets and '\n                    'still be considered healthy.'\n                ),\n                default=30,\n                type=int,\n                show_default=True,\n            ),\n            option(\n                '--send-ping-time',\n                help=(\n                    'Time in seconds after which if we have received no message from a '\n                    'node we have a connection with, we are going to send a PING message'\n                ),\n                default=60,\n                type=int,\n                show_default=True,\n            ),\n            option(\n                '--nat',\n                help=(\n                    'Manually specify method to use for determining public IP / NAT traversal.\\n'\n                    'Available methods:\\n'\n                    '\"auto\" - Try UPnP, then STUN, fallback to none\\n'\n                    '\"upnp\" - Try UPnP, fallback to none\\n'\n                    '\"stun\" - Try STUN, fallback to none\\n'\n                    '\"none\" - Use the local interface address '\n                    '(this will likely cause connectivity issues)\\n'\n                    '\"ext:<IP>[:<PORT>]\" - manually specify the external IP (and optionally port '\n                    'number)'\n                ),\n                type=NATChoiceType(['auto', 'upnp', 'stun', 'none', 'ext:<IP>[:<PORT>]']),\n                default='auto',\n                show_default=True,\n                option_group='udp_transport',\n            ),\n        ),\n        option_group(\n            'Matrix Transport Options',\n            option(\n                '--matrix-server',\n                help=(\n                    'Matrix homeserver to use for communication.\\n'\n                    'Valid values:\\n'\n                    '\"auto\" - automatically select a suitable homeserver\\n'\n                    'A URL pointing to a Raiden matrix homeserver'\n                ),\n                default='auto',\n                type=MatrixServerType(['auto', '<url>']),\n                show_default=True,\n            ),\n        ),\n        option_group(\n            'Logging Options',\n            option(\n                '--log-config',\n                help='Log level configuration.\\n'\n                     'Format: [<logger-name-1>]:<level>[,<logger-name-2>:level][,...]',\n                type=LOG_LEVEL_CONFIG_TYPE,\n                default=':info',\n                show_default=True,\n            ),\n            option(\n                '--log-file',\n                help='file path for logging to file',\n                default=None,\n                type=str,\n                show_default=True,\n            ),\n            option(\n                '--log-json',\n                help='Output log lines in JSON format',\n                is_flag=True,\n            ),\n            option(\n                '--disable-debug-logfile',\n                help=(\n                    'Disable the debug logfile feature. This is independent of '\n                    'the normal logging setup'\n                ),\n                is_flag=True,\n            ),\n        ),\n        option_group(\n            'RPC Options',\n            option(\n                '--rpc/--no-rpc',\n                help='Start with or without the RPC server.',\n                default=True,\n                show_default=True,\n            ),\n            option(\n                '--rpccorsdomain',\n                help='Comma separated list of domains to accept cross origin requests.',\n                default='http://localhost:*/*',\n                type=str,\n                show_default=True,\n            ),\n            option(\n                '--api-address',\n                help='\"host:port\" for the RPC server to listen on.',\n                default='127.0.0.1:5001',\n                type=str,\n                show_default=True,\n            ),\n            option(\n                '--web-ui/--no-web-ui',\n                help=(\n                    'Start with or without the web interface. Requires --rpc. '\n                    'It will be accessible at http://<api-address>. '\n                ),\n                default=True,\n                show_default=True,\n            ),\n        ),\n        option_group(\n            'Debugging options',\n            option(\n                '--unrecoverable-error-should-crash',\n                help=(\n                    'DO NOT use, unless you know what you are doing. If provided '\n                    'along with a production environment setting then all '\n                    'unrecoverable errors will lead to a crash and not simply get logged.'\n                ),\n                is_flag=True,\n                default=False,\n            ),\n        ),\n        option_group(\n            'Hash Resolver options',\n            option(\n                '--resolver-endpoint',\n                help=(\n                    'URL of the resolver server that is used to resolve '\n                    'a payment hash to a secret. '\n                    'Accepts a protocol prefix (http:// or https://) with optional port'\n                ),\n                default=None,\n                type=str,\n                show_default=True,\n            ),\n        ),\n    ]\n\n    for option_ in reversed(options_):\n        func = option_(func)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef version(short):\n    if short:\n        print(get_system_spec()['raiden'])\n    else:\n        print(json.dumps(\n            get_system_spec(),\n            indent=2,\n        ))", "response": "Print version information and exit."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the latest state change which contains the corresponding balance proof.", "response": "def get_state_change_with_balance_proof_by_balance_hash(\n        storage: sqlite.SQLiteStorage,\n        canonical_identifier: CanonicalIdentifier,\n        balance_hash: BalanceHash,\n        sender: Address,\n) -> sqlite.StateChangeRecord:\n    \"\"\" Returns the state change which contains the corresponding balance\n    proof.\n\n    Use this function to find a balance proof for a call to settle, which only\n    has the blinded balance proof data.\n    \"\"\"\n    return storage.get_latest_state_change_by_data_field({\n        'balance_proof.canonical_identifier.chain_identifier':\n        str(canonical_identifier.chain_identifier),\n        'balance_proof.canonical_identifier.token_network_address': to_checksum_address(\n            canonical_identifier.token_network_address,\n        ),\n        'balance_proof.canonical_identifier.channel_identifier': str(\n            canonical_identifier.channel_identifier,\n        ),\n        'balance_proof.balance_hash': serialize_bytes(balance_hash),\n        'balance_proof.sender': to_checksum_address(sender),\n    })"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the latest state change that contains the corresponding balance proof.", "response": "def get_state_change_with_balance_proof_by_locksroot(\n        storage: sqlite.SQLiteStorage,\n        canonical_identifier: CanonicalIdentifier,\n        locksroot: Locksroot,\n        sender: Address,\n) -> sqlite.StateChangeRecord:\n    \"\"\" Returns the state change which contains the corresponding balance\n    proof.\n\n    Use this function to find a balance proof for a call to unlock, which only\n    happens after settle, so the channel has the unblinded version of the\n    balance proof.\n    \"\"\"\n    return storage.get_latest_state_change_by_data_field({\n        'balance_proof.canonical_identifier.chain_identifier': str(\n            canonical_identifier.chain_identifier,\n        ),\n        'balance_proof.canonical_identifier.token_network_address': to_checksum_address(\n            canonical_identifier.token_network_address,\n        ),\n        'balance_proof.canonical_identifier.channel_identifier': str(\n            canonical_identifier.channel_identifier,\n        ),\n        'balance_proof.locksroot': serialize_bytes(locksroot),\n        'balance_proof.sender': to_checksum_address(sender),\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the latest event which contains the corresponding balance proof.", "response": "def get_event_with_balance_proof_by_balance_hash(\n        storage: sqlite.SQLiteStorage,\n        canonical_identifier: CanonicalIdentifier,\n        balance_hash: BalanceHash,\n) -> sqlite.EventRecord:\n    \"\"\" Returns the event which contains the corresponding balance\n    proof.\n\n    Use this function to find a balance proof for a call to settle, which only\n    has the blinded balance proof data.\n    \"\"\"\n    return storage.get_latest_event_by_data_field({\n        'balance_proof.canonical_identifier.chain_identifier': str(\n            canonical_identifier.chain_identifier,\n        ),\n        'balance_proof.canonical_identifier.token_network_address': to_checksum_address(\n            canonical_identifier.token_network_address,\n        ),\n        'balance_proof.canonical_identifier.channel_identifier': str(\n            canonical_identifier.channel_identifier,\n        ),\n        'balance_proof.balance_hash': serialize_bytes(balance_hash),\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_event_with_balance_proof_by_locksroot(\n        storage: sqlite.SQLiteStorage,\n        canonical_identifier: CanonicalIdentifier,\n        locksroot: Locksroot,\n        recipient: Address,\n) -> sqlite.EventRecord:\n    \"\"\" Returns the event which contains the corresponding balance proof.\n\n    Use this function to find a balance proof for a call to unlock, which only\n    happens after settle, so the channel has the unblinded version of the\n    balance proof.\n    \"\"\"\n    return storage.get_latest_event_by_data_field({\n        'balance_proof.canonical_identifier.chain_identifier': str(\n            canonical_identifier.chain_identifier,\n        ),\n        'balance_proof.canonical_identifier.token_network_address': to_checksum_address(\n            canonical_identifier.token_network_address,\n        ),\n        'balance_proof.canonical_identifier.channel_identifier': str(\n            canonical_identifier.channel_identifier,\n        ),\n        'balance_proof.locksroot': serialize_bytes(locksroot),\n        'recipient': to_checksum_address(recipient),\n    })", "response": "Returns the latest event which contains the corresponding balance proof."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(\n            self,\n            funds: typing.TokenAmount,\n            initial_channel_target: int = 3,\n            joinable_funds_target: float = 0.4,\n    ):\n        \"\"\"Connect to the network.\n\n        Subsequent calls to `connect` are allowed, but will only affect the spendable\n        funds and the connection strategy parameters for the future. `connect` will not\n        close any channels.\n\n        Note: the ConnectionManager does not discriminate manually opened channels from\n        automatically opened ones. If the user manually opened channels, those deposit\n        amounts will affect the funding per channel and the number of new channels opened.\n\n        Args:\n            funds: Target amount of tokens spendable to join the network.\n            initial_channel_target: Target number of channels to open.\n            joinable_funds_target: Amount of funds not initially assigned.\n        \"\"\"\n        token = self.raiden.chain.token(self.token_address)\n        token_balance = token.balance_of(self.raiden.address)\n\n        if token_balance < funds:\n            raise InvalidAmount(\n                f'Insufficient balance for token {pex(self.token_address)}',\n            )\n\n        if funds <= 0:\n            raise InvalidAmount(\n                'The funds to use in the connection need to be a positive integer',\n            )\n\n        if joinable_funds_target < 0 or joinable_funds_target > 1:\n            raise InvalidAmount(\n                f'joinable_funds_target should be between 0 and 1. Given: {joinable_funds_target}',\n            )\n\n        with self.lock:\n            self.funds = funds\n            self.initial_channel_target = initial_channel_target\n            self.joinable_funds_target = joinable_funds_target\n\n            log_open_channels(self.raiden, self.registry_address, self.token_address, funds)\n\n            qty_network_channels = views.count_token_network_channels(\n                views.state_from_raiden(self.raiden),\n                self.registry_address,\n                self.token_address,\n            )\n\n            if not qty_network_channels:\n                log.info(\n                    'Bootstrapping token network.',\n                    node=pex(self.raiden.address),\n                    network_id=pex(self.registry_address),\n                    token_id=pex(self.token_address),\n                )\n                self.api.channel_open(\n                    self.registry_address,\n                    self.token_address,\n                    self.BOOTSTRAP_ADDR,\n                )\n            else:\n                self._open_channels()", "response": "Connect to the token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef leave(self, registry_address):\n        with self.lock:\n            self.initial_channel_target = 0\n\n            channels_to_close = views.get_channelstate_open(\n                chain_state=views.state_from_raiden(self.raiden),\n                payment_network_id=registry_address,\n                token_address=self.token_address,\n            )\n\n            partner_addresses = [\n                channel_state.partner_state.address\n                for channel_state in channels_to_close\n            ]\n            self.api.channel_batch_close(\n                registry_address,\n                self.token_address,\n                partner_addresses,\n            )\n\n            channel_ids = [\n                channel_state.identifier\n                for channel_state in channels_to_close\n            ]\n\n            waiting.wait_for_settle(\n                self.raiden,\n                registry_address,\n                self.token_address,\n                channel_ids,\n                self.raiden.alarm.sleep_time,\n            )\n\n        return channels_to_close", "response": "This method closes all channels and waits for all channels to be settled."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_new_partners(self):\n        open_channels = views.get_channelstate_open(\n            chain_state=views.state_from_raiden(self.raiden),\n            payment_network_id=self.registry_address,\n            token_address=self.token_address,\n        )\n        known = set(channel_state.partner_state.address for channel_state in open_channels)\n        known.add(self.BOOTSTRAP_ADDR)\n        known.add(self.raiden.address)\n\n        participants_addresses = views.get_participants_addresses(\n            views.state_from_raiden(self.raiden),\n            self.registry_address,\n            self.token_address,\n        )\n\n        available = participants_addresses - known\n        available = list(available)\n        shuffle(available)\n        new_partners = available\n\n        log.debug(\n            'Found partners',\n            node=pex(self.raiden.address),\n            number_of_partners=len(available),\n        )\n\n        return new_partners", "response": "Search the token network for potential channel partners."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures a channel exists with partner and is funded in our side.", "response": "def _join_partner(self, partner: Address):\n        \"\"\" Ensure a channel exists with partner and is funded in our side \"\"\"\n        try:\n            self.api.channel_open(\n                self.registry_address,\n                self.token_address,\n                partner,\n            )\n        except DuplicatedChannelError:\n            # If channel already exists (either because partner created it,\n            # or it's nonfunded channel), continue to ensure it's funded\n            pass\n\n        total_deposit = self._initial_funding_per_partner\n        if total_deposit == 0:\n            return\n\n        try:\n            self.api.set_total_channel_deposit(\n                registry_address=self.registry_address,\n                token_address=self.token_address,\n                partner_address=partner,\n                total_deposit=total_deposit,\n            )\n        except InvalidDBData:\n            raise\n        except RECOVERABLE_ERRORS:\n            log.info(\n                'Deposit failed',\n                node=pex(self.raiden.address),\n                partner=pex(partner),\n            )\n        except RaidenUnrecoverableError:\n            should_crash = (\n                self.raiden.config['environment_type'] != Environment.PRODUCTION or\n                self.raiden.config['unrecoverable_error_should_crash']\n            )\n            if should_crash:\n                raise\n\n            log.critical(\n                'Deposit failed',\n                node=pex(self.raiden.address),\n                partner=pex(partner),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen channels until there are enough channels open.", "response": "def _open_channels(self) -> bool:\n        \"\"\" Open channels until there are `self.initial_channel_target`\n        channels open. Do nothing if there are enough channels open already.\n\n        Note:\n            - This method must be called with the lock held.\n        Return:\n            - False if no channels could be opened\n        \"\"\"\n\n        open_channels = views.get_channelstate_open(\n            chain_state=views.state_from_raiden(self.raiden),\n            payment_network_id=self.registry_address,\n            token_address=self.token_address,\n        )\n        open_channels = [\n            channel_state\n            for channel_state in open_channels\n            if channel_state.partner_state.address != self.BOOTSTRAP_ADDR\n        ]\n        funded_channels = [\n            channel_state for channel_state in open_channels\n            if channel_state.our_state.contract_balance >= self._initial_funding_per_partner\n        ]\n        nonfunded_channels = [\n            channel_state for channel_state in open_channels\n            if channel_state not in funded_channels\n        ]\n        possible_new_partners = self._find_new_partners()\n        if possible_new_partners == 0:\n            return False\n\n        # if we already met our target, break\n        if len(funded_channels) >= self.initial_channel_target:\n            return False\n        # if we didn't, but there's no nonfunded channels and no available partners\n        # it means the network is smaller than our target, so we should also break\n        if not nonfunded_channels and possible_new_partners == 0:\n            return False\n\n        n_to_join = self.initial_channel_target - len(funded_channels)\n        nonfunded_partners = [\n            channel_state.partner_state.address\n            for channel_state in nonfunded_channels\n        ]\n        # first, fund nonfunded channels, then open and fund with possible_new_partners,\n        # until initial_channel_target of funded channels is met\n        join_partners = (nonfunded_partners + possible_new_partners)[:n_to_join]\n\n        log.debug(\n            'Spawning greenlets to join partners',\n            node=pex(self.raiden.address),\n            num_greenlets=len(join_partners),\n        )\n\n        greenlets = set(\n            gevent.spawn(self._join_partner, partner)\n            for partner in join_partners\n        )\n        gevent.joinall(greenlets, raise_error=True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _funds_remaining(self) -> int:\n        if self.funds > 0:\n            token = self.raiden.chain.token(self.token_address)\n            token_balance = token.balance_of(self.raiden.address)\n            sum_deposits = views.get_our_capacity_for_token_network(\n                views.state_from_raiden(self.raiden),\n                self.registry_address,\n                self.token_address,\n            )\n\n            return min(self.funds - sum_deposits, token_balance)\n\n        return 0", "response": "Returns the amount of funds that are already deposited."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _address_rxp(self, addr):\n        try:\n            addr = to_checksum_address(addr)\n            rxp = '(?:0x)?' + pex(address_checksum_and_decode(addr)) + f'(?:{addr.lower()[10:]})?'\n            self._extra_keys[pex(address_checksum_and_decode(addr))] = addr.lower()\n            self._extra_keys[addr[2:].lower()] = addr.lower()\n        except ValueError:\n            rxp = addr\n        return rxp", "response": "Create a regex string for addresses that match several representations of the address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_regex(self):\n        rxp = \"|\".join(\n            map(self._address_rxp, self.keys()),\n        )\n        self._regex = re.compile(\n            rxp,\n            re.IGNORECASE,\n        )", "response": "Compile rxp with all keys concatenated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self):\n        return self.rest_api.get_tokens_list(\n            self.rest_api.raiden_api.raiden.default_registry.address,\n        )", "response": "get all token addresses we have channels open for"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_safe_to_wait(\n        lock_expiration: BlockExpiration,\n        reveal_timeout: BlockTimeout,\n        block_number: BlockNumber,\n) -> SuccessOrError:\n    \"\"\" True if waiting is safe, i.e. there are more than enough blocks to safely\n    unlock on chain.\n    \"\"\"\n    # reveal timeout will not ever be larger than the lock_expiration otherwise\n    # the expected block_number is negative\n    assert block_number > 0\n    assert reveal_timeout > 0\n    assert lock_expiration > reveal_timeout\n\n    lock_timeout = lock_expiration - block_number\n\n    # A node may wait for a new balance proof while there are reveal_timeout\n    # blocks left, at that block and onwards it is not safe to wait.\n    if lock_timeout > reveal_timeout:\n        return True, None\n\n    msg = (\n        f'lock timeout is unsafe.'\n        f' timeout must be larger than {reveal_timeout}, but it is {lock_timeout}.'\n        f' expiration: {lock_expiration} block_number: {block_number}'\n    )\n    return False, msg", "response": "True if waiting is safe and the lock_expiration is larger than the reveal_timeout."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if both transfers are for the same mediated transfer.", "response": "def is_send_transfer_almost_equal(\n        send_channel: NettingChannelState,\n        send: LockedTransferUnsignedState,\n        received: LockedTransferSignedState,\n) -> bool:\n    \"\"\" True if both transfers are for the same mediated transfer. \"\"\"\n    # The only thing that may change is the direction of the transfer\n    return (\n        isinstance(send, LockedTransferUnsignedState) and\n        isinstance(received, LockedTransferSignedState) and\n        send.payment_identifier == received.payment_identifier and\n        send.token == received.token and\n        send.lock.amount == received.lock.amount - send_channel.mediation_fee and\n        send.lock.expiration == received.lock.expiration and\n        send.lock.secrethash == received.lock.secrethash and\n        send.initiator == received.initiator and\n        send.target == received.target\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_reachable_routes(\n        routes: List[RouteState],\n        nodeaddresses_to_networkstates: NodeNetworkStateMap,\n) -> List[RouteState]:\n    \"\"\"This function makes sure we use reachable routes only.\"\"\"\n    reachable_routes = []\n\n    for route in routes:\n        node_network_state = nodeaddresses_to_networkstates.get(\n            route.node_address,\n            NODE_NETWORK_UNREACHABLE,\n        )\n\n        if node_network_state == NODE_NETWORK_REACHABLE:\n            reachable_routes.append(route)\n\n    return reachable_routes", "response": "This function makes sure we only use routes that are reachable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the payee channel of a given transfer pair or None if it s not found.", "response": "def get_payee_channel(\n        channelidentifiers_to_channels: ChannelMap,\n        transfer_pair: MediationPairState,\n) -> Optional[NettingChannelState]:\n    \"\"\" Returns the payee channel of a given transfer pair or None if it's not found \"\"\"\n    payee_channel_identifier = transfer_pair.payee_transfer.balance_proof.channel_identifier\n    return channelidentifiers_to_channels.get(payee_channel_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the payer channel of a given transfer pair.", "response": "def get_payer_channel(\n        channelidentifiers_to_channels: ChannelMap,\n        transfer_pair: MediationPairState,\n) -> Optional[NettingChannelState]:\n    \"\"\" Returns the payer channel of a given transfer pair or None if it's not found \"\"\"\n    payer_channel_identifier = transfer_pair.payer_transfer.balance_proof.channel_identifier\n    return channelidentifiers_to_channels.get(payer_channel_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pending_transfer_pairs(\n        transfers_pair: List[MediationPairState],\n) -> List[MediationPairState]:\n    \"\"\" Return the transfer pairs that are not at a final state. \"\"\"\n    pending_pairs = list(\n        pair\n        for pair in transfers_pair\n        if pair.payee_state not in STATE_TRANSFER_FINAL or\n        pair.payer_state not in STATE_TRANSFER_FINAL\n    )\n    return pending_pairs", "response": "Return the list of pending transfer pairs that are not at a final state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_lock_amount_after_fees(\n        lock: HashTimeLockState,\n        payee_channel: NettingChannelState,\n) -> PaymentWithFeeAmount:\n    \"\"\"\n    Return the lock.amount after fees are taken.\n\n    Fees are taken only for the outgoing channel, which is the one with\n    collateral locked from this node.\n    \"\"\"\n    return PaymentWithFeeAmount(lock.amount - payee_channel.mediation_fee)", "response": "Returns the amount after fees are taken for the outgoing channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the mediator task if all the locks have been finalized.", "response": "def clear_if_finalized(\n        iteration: TransitionResult,\n        channelidentifiers_to_channels: ChannelMap,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\"Clear the mediator task if all the locks have been finalized.\n\n    A lock is considered finalized if it has been removed from the merkle tree\n    offchain, either because the transfer was unlocked or expired, or because the\n    channel was settled on chain and therefore the channel is removed.\"\"\"\n    state = cast(MediatorTransferState, iteration.new_state)\n\n    if state is None:\n        return iteration\n\n    # Only clear the task if all channels have the lock cleared.\n    secrethash = state.secrethash\n    for pair in state.transfers_pair:\n        payer_channel = get_payer_channel(channelidentifiers_to_channels, pair)\n        if payer_channel and channel.is_lock_pending(payer_channel.partner_state, secrethash):\n            return iteration\n\n        payee_channel = get_payee_channel(channelidentifiers_to_channels, pair)\n        if payee_channel and channel.is_lock_pending(payee_channel.our_state, secrethash):\n            return iteration\n\n    if state.waiting_transfer:\n        waiting_transfer = state.waiting_transfer.transfer\n        waiting_channel_identifier = waiting_transfer.balance_proof.channel_identifier\n        waiting_channel = channelidentifiers_to_channels.get(waiting_channel_identifier)\n\n        if waiting_channel and channel.is_lock_pending(waiting_channel.partner_state, secrethash):\n            return iteration\n\n    return TransitionResult(None, iteration.events)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef next_channel_from_routes(\n        available_routes: List['RouteState'],\n        channelidentifiers_to_channels: Dict,\n        transfer_amount: PaymentWithFeeAmount,\n        lock_timeout: BlockTimeout,\n) -> Optional[NettingChannelState]:\n    \"\"\" Returns the first route that may be used to mediated the transfer.\n    The routing service can race with local changes, so the recommended routes\n    must be validated.\n    Args:\n        available_routes: Current available routes that may be used, it's\n            assumed that the available_routes list is ordered from best to\n            worst.\n        channelidentifiers_to_channels: Mapping from channel identifier\n            to NettingChannelState.\n        transfer_amount: The amount of tokens that will be transferred\n            through the given route.\n        lock_timeout: Number of blocks until the lock expires, used to filter\n            out channels that have a smaller settlement window.\n    Returns:\n        The next route.\n    \"\"\"\n    for route in available_routes:\n        channel_state = channelidentifiers_to_channels.get(route.channel_identifier)\n\n        if not channel_state:\n            continue\n\n        if is_channel_usable(channel_state, transfer_amount, lock_timeout):\n            return channel_state\n\n    return None", "response": "Returns the next route that can be mediated through the given available_routes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a LockedTransferSignedState and a list of available routes and a new block number tries to proceed with the mediation.", "response": "def forward_transfer_pair(\n        payer_transfer: LockedTransferSignedState,\n        available_routes: List['RouteState'],\n        channelidentifiers_to_channels: Dict,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n) -> Tuple[Optional[MediationPairState], List[Event]]:\n    \"\"\" Given a payer transfer tries a new route to proceed with the mediation.\n\n    Args:\n        payer_transfer: The transfer received from the payer_channel.\n        available_routes: Current available routes that may be used, it's\n            assumed that the routes list is ordered from best to worst.\n        channelidentifiers_to_channels: All the channels available for this\n            transfer.\n        pseudo_random_generator: Number generator to generate a message id.\n        block_number: The current block number.\n    \"\"\"\n    transfer_pair = None\n    mediated_events: List[Event] = list()\n    lock_timeout = BlockTimeout(payer_transfer.lock.expiration - block_number)\n\n    payee_channel = next_channel_from_routes(\n        available_routes=available_routes,\n        channelidentifiers_to_channels=channelidentifiers_to_channels,\n        transfer_amount=payer_transfer.lock.amount,\n        lock_timeout=lock_timeout,\n    )\n\n    if payee_channel:\n        assert payee_channel.settle_timeout >= lock_timeout\n        assert payee_channel.token_address == payer_transfer.token\n\n        message_identifier = message_identifier_from_prng(pseudo_random_generator)\n        lock = payer_transfer.lock\n        lockedtransfer_event = channel.send_lockedtransfer(\n            channel_state=payee_channel,\n            initiator=payer_transfer.initiator,\n            target=payer_transfer.target,\n            amount=get_lock_amount_after_fees(lock, payee_channel),\n            message_identifier=message_identifier,\n            payment_identifier=payer_transfer.payment_identifier,\n            expiration=lock.expiration,\n            secrethash=lock.secrethash,\n        )\n        assert lockedtransfer_event\n\n        transfer_pair = MediationPairState(\n            payer_transfer,\n            payee_channel.partner_state.address,\n            lockedtransfer_event.transfer,\n        )\n\n        mediated_events = [lockedtransfer_event]\n\n    return (\n        transfer_pair,\n        mediated_events,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a transfer to the next mediator pair and returns the corresponding mediation pair and the refund event.", "response": "def backward_transfer_pair(\n        backward_channel: NettingChannelState,\n        payer_transfer: LockedTransferSignedState,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n) -> Tuple[Optional[MediationPairState], List[Event]]:\n    \"\"\" Sends a transfer backwards, allowing the previous hop to try a new\n    route.\n\n    When all the routes available for this node failed, send a transfer\n    backwards with the same amount and secrethash, allowing the previous hop to\n    do a retry.\n\n    Args:\n        backward_channel: The original channel which sent the mediated transfer\n            to this node.\n        payer_transfer: The *latest* payer transfer which is backing the\n            mediation.\n        block_number: The current block number.\n\n    Returns:\n        The mediator pair and the correspoding refund event.\n    \"\"\"\n    transfer_pair = None\n    events: List[Event] = list()\n\n    lock = payer_transfer.lock\n    lock_timeout = BlockTimeout(lock.expiration - block_number)\n\n    # Ensure the refund transfer's lock has a safe expiration, otherwise don't\n    # do anything and wait for the received lock to expire.\n    if is_channel_usable(backward_channel, lock.amount, lock_timeout):\n        message_identifier = message_identifier_from_prng(pseudo_random_generator)\n        refund_transfer = channel.send_refundtransfer(\n            channel_state=backward_channel,\n            initiator=payer_transfer.initiator,\n            target=payer_transfer.target,\n            amount=get_lock_amount_after_fees(lock, backward_channel),\n            message_identifier=message_identifier,\n            payment_identifier=payer_transfer.payment_identifier,\n            expiration=lock.expiration,\n            secrethash=lock.secrethash,\n        )\n\n        transfer_pair = MediationPairState(\n            payer_transfer,\n            backward_channel.partner_state.address,\n            refund_transfer.transfer,\n        )\n\n        events.append(refund_transfer)\n\n    return (transfer_pair, events)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the secret to all mediated transfers.", "response": "def set_offchain_secret(\n        state: MediatorTransferState,\n        channelidentifiers_to_channels: ChannelMap,\n        secret: Secret,\n        secrethash: SecretHash,\n) -> List[Event]:\n    \"\"\" Set the secret to all mediated transfers. \"\"\"\n    state.secret = secret\n\n    for pair in state.transfers_pair:\n        payer_channel = channelidentifiers_to_channels.get(\n            pair.payer_transfer.balance_proof.channel_identifier,\n        )\n        if payer_channel:\n            channel.register_offchain_secret(\n                payer_channel,\n                secret,\n                secrethash,\n            )\n\n        payee_channel = channelidentifiers_to_channels.get(\n            pair.payee_transfer.balance_proof.channel_identifier,\n        )\n        if payee_channel:\n            channel.register_offchain_secret(\n                payee_channel,\n                secret,\n                secrethash,\n            )\n\n    # The secret should never be revealed if `waiting_transfer` is not None.\n    # For this to happen this node must have received a transfer, which it did\n    # *not* mediate, and neverthless the secret was revealed.\n    #\n    # This can only be possible if the initiator reveals the secret without the\n    # target's secret request, or if the node which sent the `waiting_transfer`\n    # has sent another transfer which reached the target (meaning someone along\n    # the path will lose tokens).\n    if state.waiting_transfer:\n        payer_channel = channelidentifiers_to_channels.get(\n            state.waiting_transfer.transfer.balance_proof.channel_identifier,\n        )\n        if payer_channel:\n            channel.register_offchain_secret(\n                payer_channel,\n                secret,\n                secrethash,\n            )\n\n        unexpected_reveal = EventUnexpectedSecretReveal(\n            secrethash=secrethash,\n            reason='The mediator has a waiting transfer.',\n        )\n        return [unexpected_reveal]\n\n    return list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_onchain_secret(\n        state: MediatorTransferState,\n        channelidentifiers_to_channels: ChannelMap,\n        secret: Secret,\n        secrethash: SecretHash,\n        block_number: BlockNumber,\n) -> List[Event]:\n    \"\"\" Set the secret to all mediated transfers.\n\n    The secret should have been learned from the secret registry.\n    \"\"\"\n    state.secret = secret\n\n    for pair in state.transfers_pair:\n        payer_channel = channelidentifiers_to_channels.get(\n            pair.payer_transfer.balance_proof.channel_identifier,\n        )\n        if payer_channel:\n            channel.register_onchain_secret(\n                payer_channel,\n                secret,\n                secrethash,\n                block_number,\n            )\n\n        payee_channel = channelidentifiers_to_channels.get(\n            pair.payee_transfer.balance_proof.channel_identifier,\n        )\n        if payee_channel:\n            channel.register_onchain_secret(\n                channel_state=payee_channel,\n                secret=secret,\n                secrethash=secrethash,\n                secret_reveal_block_number=block_number,\n            )\n\n    # Like the off-chain secret reveal, the secret should never be revealed\n    # on-chain if there is a waiting transfer.\n    if state.waiting_transfer:\n        payer_channel = channelidentifiers_to_channels.get(\n            state.waiting_transfer.transfer.balance_proof.channel_identifier,\n        )\n        if payer_channel:\n            channel.register_onchain_secret(\n                channel_state=payer_channel,\n                secret=secret,\n                secrethash=secrethash,\n                secret_reveal_block_number=block_number,\n            )\n\n        unexpected_reveal = EventUnexpectedSecretReveal(\n            secrethash=secrethash,\n            reason='The mediator has a waiting transfer.',\n        )\n        return [unexpected_reveal]\n\n    return list()", "response": "Sets the secret to all mediated transfers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_offchain_reveal_state(\n        transfers_pair: List[MediationPairState],\n        payee_address: Address,\n) -> None:\n    \"\"\" Set the state of a transfer *sent* to a payee. \"\"\"\n    for pair in transfers_pair:\n        if pair.payee_address == payee_address:\n            pair.payee_state = 'payee_secret_revealed'", "response": "Sets the state of a transfer sent to a payee."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of Event objects that are added to the event list for expired locks.", "response": "def events_for_expired_pairs(\n        channelidentifiers_to_channels: ChannelMap,\n        transfers_pair: List[MediationPairState],\n        waiting_transfer: Optional[WaitingTransferState],\n        block_number: BlockNumber,\n) -> List[Event]:\n    \"\"\" Informational events for expired locks. \"\"\"\n    pending_transfers_pairs = get_pending_transfer_pairs(transfers_pair)\n\n    events: List[Event] = list()\n    for pair in pending_transfers_pairs:\n        payer_balance_proof = pair.payer_transfer.balance_proof\n        payer_channel = channelidentifiers_to_channels.get(payer_balance_proof.channel_identifier)\n        if not payer_channel:\n            continue\n\n        has_payer_transfer_expired = channel.is_transfer_expired(\n            transfer=pair.payer_transfer,\n            affected_channel=payer_channel,\n            block_number=block_number,\n        )\n\n        if has_payer_transfer_expired:\n            # For safety, the correct behavior is:\n            #\n            # - If the payee has been paid, then the payer must pay too.\n            #\n            #   And the corollary:\n            #\n            # - If the payer transfer has expired, then the payee transfer must\n            #   have expired too.\n            #\n            # The problem is that this corollary cannot be asserted. If a user\n            # is running Raiden without a monitoring service, then it may go\n            # offline after having paid a transfer to a payee, but without\n            # getting a balance proof of the payer, and once it comes back\n            # online the transfer may have expired.\n            #\n            # assert pair.payee_state == 'payee_expired'\n\n            pair.payer_state = 'payer_expired'\n            unlock_claim_failed = EventUnlockClaimFailed(\n                pair.payer_transfer.payment_identifier,\n                pair.payer_transfer.lock.secrethash,\n                'lock expired',\n            )\n            events.append(unlock_claim_failed)\n\n    if waiting_transfer and waiting_transfer.state != 'expired':\n        waiting_transfer.state = 'expired'\n        unlock_claim_failed = EventUnlockClaimFailed(\n            waiting_transfer.transfer.payment_identifier,\n            waiting_transfer.transfer.lock.secrethash,\n            'lock expired',\n        )\n        events.append(unlock_claim_failed)\n\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef events_for_secretreveal(\n        transfers_pair: List[MediationPairState],\n        secret: Secret,\n        pseudo_random_generator: random.Random,\n) -> List[Event]:\n    \"\"\" Reveal the secret off-chain.\n\n    The secret is revealed off-chain even if there is a pending transaction to\n    reveal it on-chain, this allows the unlock to happen off-chain, which is\n    faster.\n\n    This node is named N, suppose there is a mediated transfer with two refund\n    transfers, one from B and one from C:\n\n        A-N-B...B-N-C..C-N-D\n\n    Under normal operation N will first learn the secret from D, then reveal to\n    C, wait for C to inform the secret is known before revealing it to B, and\n    again wait for B before revealing the secret to A.\n\n    If B somehow sent a reveal secret before C and D, then the secret will be\n    revealed to A, but not C and D, meaning the secret won't be propagated\n    forward. Even if D sent a reveal secret at about the same time, the secret\n    will only be revealed to B upon confirmation from C.\n\n    If the proof doesn't arrive in time and the lock's expiration is at risk, N\n    won't lose tokens since it knows the secret can go on-chain at any time.\n    \"\"\"\n    events: List[Event] = list()\n    for pair in reversed(transfers_pair):\n        payee_knows_secret = pair.payee_state in STATE_SECRET_KNOWN\n        payer_knows_secret = pair.payer_state in STATE_SECRET_KNOWN\n        is_transfer_pending = pair.payer_state == 'payer_pending'\n\n        should_send_secret = (\n            payee_knows_secret and\n            not payer_knows_secret and\n            is_transfer_pending\n        )\n\n        if should_send_secret:\n            message_identifier = message_identifier_from_prng(pseudo_random_generator)\n            pair.payer_state = 'payer_secret_revealed'\n            payer_transfer = pair.payer_transfer\n            revealsecret = SendSecretReveal(\n                recipient=payer_transfer.balance_proof.sender,\n                channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n                message_identifier=message_identifier,\n                secret=secret,\n            )\n\n            events.append(revealsecret)\n\n    return events", "response": "Reveal the secret off - chain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef events_for_balanceproof(\n        channelidentifiers_to_channels: ChannelMap,\n        transfers_pair: List[MediationPairState],\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n        secret: Secret,\n        secrethash: SecretHash,\n) -> List[Event]:\n    \"\"\" While it's safe do the off-chain unlock. \"\"\"\n\n    events: List[Event] = list()\n    for pair in reversed(transfers_pair):\n        payee_knows_secret = pair.payee_state in STATE_SECRET_KNOWN\n        payee_payed = pair.payee_state in STATE_TRANSFER_PAID\n\n        payee_channel = get_payee_channel(channelidentifiers_to_channels, pair)\n        payee_channel_open = (\n            payee_channel and channel.get_status(payee_channel) == CHANNEL_STATE_OPENED\n        )\n\n        payer_channel = get_payer_channel(channelidentifiers_to_channels, pair)\n\n        # The mediator must not send to the payee a balance proof if the lock\n        # is in the danger zone, because the payer may not do the same and the\n        # on-chain unlock may fail. If the lock is nearing it's expiration\n        # block, then on-chain unlock should be done, and if successful it can\n        # be unlocked off-chain.\n        is_safe_to_send_balanceproof = False\n        if payer_channel:\n            is_safe_to_send_balanceproof, _ = is_safe_to_wait(\n                pair.payer_transfer.lock.expiration,\n                payer_channel.reveal_timeout,\n                block_number,\n            )\n\n        should_send_balanceproof_to_payee = (\n            payee_channel_open and\n            payee_knows_secret and\n            not payee_payed and\n            is_safe_to_send_balanceproof\n        )\n\n        if should_send_balanceproof_to_payee:\n            # At this point we are sure that payee_channel exists due to the\n            # payee_channel_open check above. So let mypy know about this\n            assert payee_channel\n            payee_channel = cast(NettingChannelState, payee_channel)\n            pair.payee_state = 'payee_balance_proof'\n\n            message_identifier = message_identifier_from_prng(pseudo_random_generator)\n            unlock_lock = channel.send_unlock(\n                channel_state=payee_channel,\n                message_identifier=message_identifier,\n                payment_identifier=pair.payee_transfer.payment_identifier,\n                secret=secret,\n                secrethash=secrethash,\n            )\n\n            unlock_success = EventUnlockSuccess(\n                pair.payer_transfer.payment_identifier,\n                pair.payer_transfer.lock.secrethash,\n            )\n            events.append(unlock_lock)\n            events.append(unlock_success)\n\n    return events", "response": "This function returns a list of events that can be sent to the mediator to check if the lock is in the danger zone and if so the mediator can send to the payee."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrevealing the secret on - chain if the lock enters the unsafe region and the secret is not yet on - chain.", "response": "def events_for_onchain_secretreveal_if_dangerzone(\n        channelmap: ChannelMap,\n        secrethash: SecretHash,\n        transfers_pair: List[MediationPairState],\n        block_number: BlockNumber,\n        block_hash: BlockHash,\n) -> List[Event]:\n    \"\"\" Reveal the secret on-chain if the lock enters the unsafe region and the\n    secret is not yet on-chain.\n    \"\"\"\n    events: List[Event] = list()\n\n    all_payer_channels = []\n    for pair in transfers_pair:\n        channel_state = get_payer_channel(channelmap, pair)\n        if channel_state:\n            all_payer_channels.append(channel_state)\n\n    transaction_sent = has_secret_registration_started(\n        all_payer_channels,\n        transfers_pair,\n        secrethash,\n    )\n\n    # Only consider the transfers which have a pair. This means if we have a\n    # waiting transfer and for some reason the node knows the secret, it will\n    # not try to register it. Otherwise it would be possible for an attacker to\n    # reveal the secret late, just to force the node to send an unecessary\n    # transaction.\n\n    for pair in get_pending_transfer_pairs(transfers_pair):\n        payer_channel = get_payer_channel(channelmap, pair)\n        if not payer_channel:\n            continue\n\n        lock = pair.payer_transfer.lock\n\n        safe_to_wait, _ = is_safe_to_wait(\n            lock.expiration,\n            payer_channel.reveal_timeout,\n            block_number,\n        )\n\n        secret_known = channel.is_secret_known(\n            payer_channel.partner_state,\n            pair.payer_transfer.lock.secrethash,\n        )\n\n        if not safe_to_wait and secret_known:\n            pair.payer_state = 'payer_waiting_secret_reveal'\n\n            if not transaction_sent:\n                secret = channel.get_secret(\n                    payer_channel.partner_state,\n                    lock.secrethash,\n                )\n                assert secret, 'the secret should be known at this point'\n\n                reveal_events = secret_registry.events_for_onchain_secretreveal(\n                    channel_state=payer_channel,\n                    secret=secret,\n                    expiration=lock.expiration,\n                    block_hash=block_hash,\n                )\n                events.extend(reveal_events)\n\n                transaction_sent = True\n\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef events_for_onchain_secretreveal_if_closed(\n        channelmap: ChannelMap,\n        transfers_pair: List[MediationPairState],\n        secret: Secret,\n        secrethash: SecretHash,\n        block_hash: BlockHash,\n) -> List[Event]:\n    \"\"\" Register the secret on-chain if the payer channel is already closed and\n    the mediator learned the secret off-chain.\n\n    Balance proofs are not exchanged for closed channels, so there is no reason\n    to wait for the unsafe region to register secret.\n\n    Note:\n\n        If the secret is learned before the channel is closed, then the channel\n        will register the secrets in bulk, not the transfer.\n    \"\"\"\n    events: List[Event] = list()\n\n    all_payer_channels = []\n    for pair in transfers_pair:\n        channel_state = get_payer_channel(channelmap, pair)\n        if channel_state:\n            all_payer_channels.append(channel_state)\n    transaction_sent = has_secret_registration_started(\n        all_payer_channels,\n        transfers_pair,\n        secrethash,\n    )\n\n    # Just like the case for entering the danger zone, this will only consider\n    # the transfers which have a pair.\n\n    for pending_pair in get_pending_transfer_pairs(transfers_pair):\n        payer_channel = get_payer_channel(channelmap, pending_pair)\n        # Don't register the secret on-chain if the channel is open or settled\n        if payer_channel and channel.get_status(payer_channel) == CHANNEL_STATE_CLOSED:\n            pending_pair.payer_state = 'payer_waiting_secret_reveal'\n\n            if not transaction_sent:\n                partner_state = payer_channel.partner_state\n\n                lock = channel.get_lock(partner_state, secrethash)\n\n                # The mediator task lives as long as there are any pending\n                # locks, it may be the case that some of the transfer_pairs got\n                # resolved off-chain, but others didn't. For this reason we\n                # must check if the lock is still part of the channel\n                if lock:\n                    reveal_events = secret_registry.events_for_onchain_secretreveal(\n                        channel_state=payer_channel,\n                        secret=secret,\n                        expiration=lock.expiration,\n                        block_hash=block_hash,\n                    )\n                    events.extend(reveal_events)\n                    transaction_sent = True\n\n    return events", "response": "This function returns a list of events that are triggered when the secret on - chain is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef events_to_remove_expired_locks(\n        mediator_state: MediatorTransferState,\n        channelidentifiers_to_channels: ChannelMap,\n        block_number: BlockNumber,\n        pseudo_random_generator: random.Random,\n) -> List[Event]:\n    \"\"\" Clear the channels which have expired locks.\n\n    This only considers the *sent* transfers, received transfers can only be\n    updated by the partner.\n    \"\"\"\n    events: List[Event] = list()\n\n    for transfer_pair in mediator_state.transfers_pair:\n        balance_proof = transfer_pair.payee_transfer.balance_proof\n        channel_identifier = balance_proof.channel_identifier\n        channel_state = channelidentifiers_to_channels.get(channel_identifier)\n        if not channel_state:\n            continue\n\n        secrethash = mediator_state.secrethash\n        lock: Union[None, LockType] = None\n        if secrethash in channel_state.our_state.secrethashes_to_lockedlocks:\n            assert secrethash not in channel_state.our_state.secrethashes_to_unlockedlocks\n            lock = channel_state.our_state.secrethashes_to_lockedlocks.get(secrethash)\n        elif secrethash in channel_state.our_state.secrethashes_to_unlockedlocks:\n            lock = channel_state.our_state.secrethashes_to_unlockedlocks.get(secrethash)\n\n        if lock:\n            lock_expiration_threshold = channel.get_sender_expiration_threshold(lock)\n            has_lock_expired, _ = channel.is_lock_expired(\n                end_state=channel_state.our_state,\n                lock=lock,\n                block_number=block_number,\n                lock_expiration_threshold=lock_expiration_threshold,\n            )\n\n            is_channel_open = channel.get_status(channel_state) == CHANNEL_STATE_OPENED\n\n            if has_lock_expired and is_channel_open:\n                transfer_pair.payee_state = 'payee_expired'\n                expired_lock_events = channel.events_for_expired_lock(\n                    channel_state=channel_state,\n                    locked_lock=lock,\n                    pseudo_random_generator=pseudo_random_generator,\n                )\n                events.extend(expired_lock_events)\n\n                unlock_failed = EventUnlockFailed(\n                    transfer_pair.payee_transfer.payment_identifier,\n                    transfer_pair.payee_transfer.lock.secrethash,\n                    'lock expired',\n                )\n                events.append(unlock_failed)\n\n    return events", "response": "Remove expired locks from the mediator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef secret_learned(\n        state: MediatorTransferState,\n        channelidentifiers_to_channels: ChannelMap,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n        block_hash: BlockHash,\n        secret: Secret,\n        secrethash: SecretHash,\n        payee_address: Address,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" Unlock the payee lock, reveal the lock to the payer, and if necessary\n    register the secret on-chain.\n    \"\"\"\n    secret_reveal_events = set_offchain_secret(\n        state,\n        channelidentifiers_to_channels,\n        secret,\n        secrethash,\n    )\n\n    set_offchain_reveal_state(\n        state.transfers_pair,\n        payee_address,\n    )\n\n    onchain_secret_reveal = events_for_onchain_secretreveal_if_closed(\n        channelmap=channelidentifiers_to_channels,\n        transfers_pair=state.transfers_pair,\n        secret=secret,\n        secrethash=secrethash,\n        block_hash=block_hash,\n    )\n\n    offchain_secret_reveal = events_for_secretreveal(\n        state.transfers_pair,\n        secret,\n        pseudo_random_generator,\n    )\n\n    balance_proof = events_for_balanceproof(\n        channelidentifiers_to_channels,\n        state.transfers_pair,\n        pseudo_random_generator,\n        block_number,\n        secret,\n        secrethash,\n    )\n\n    events = secret_reveal_events + offchain_secret_reveal + balance_proof + onchain_secret_reveal\n    iteration = TransitionResult(state, events)\n\n    return iteration", "response": "Unlock the payee lock reveal the secret on - chain and register the secret on - chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mediate_transfer(\n        state: MediatorTransferState,\n        possible_routes: List['RouteState'],\n        payer_channel: NettingChannelState,\n        channelidentifiers_to_channels: ChannelMap,\n        nodeaddresses_to_networkstates: NodeNetworkStateMap,\n        pseudo_random_generator: random.Random,\n        payer_transfer: LockedTransferSignedState,\n        block_number: BlockNumber,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" Try a new route or fail back to a refund.\n\n    The mediator can safely try a new route knowing that the tokens from\n    payer_transfer will cover the expenses of the mediation. If there is no\n    route available that may be used at the moment of the call the mediator may\n    send a refund back to the payer, allowing the payer to try a different\n    route.\n    \"\"\"\n    reachable_routes = filter_reachable_routes(\n        possible_routes,\n        nodeaddresses_to_networkstates,\n    )\n    available_routes = filter_used_routes(\n        state.transfers_pair,\n        reachable_routes,\n    )\n\n    assert payer_channel.partner_state.address == payer_transfer.balance_proof.sender\n\n    transfer_pair, mediated_events = forward_transfer_pair(\n        payer_transfer,\n        available_routes,\n        channelidentifiers_to_channels,\n        pseudo_random_generator,\n        block_number,\n    )\n\n    if transfer_pair is None:\n        assert not mediated_events\n\n        if state.transfers_pair:\n            original_pair = state.transfers_pair[0]\n            original_channel = get_payer_channel(\n                channelidentifiers_to_channels,\n                original_pair,\n            )\n        else:\n            original_channel = payer_channel\n\n        if original_channel:\n            transfer_pair, mediated_events = backward_transfer_pair(\n                original_channel,\n                payer_transfer,\n                pseudo_random_generator,\n                block_number,\n            )\n        else:\n            transfer_pair = None\n            mediated_events = list()\n\n    if transfer_pair is None:\n        assert not mediated_events\n        mediated_events = list()\n        state.waiting_transfer = WaitingTransferState(payer_transfer)\n\n    else:\n        # the list must be ordered from high to low expiration, expiration\n        # handling depends on it\n        state.transfers_pair.append(transfer_pair)\n\n    return TransitionResult(state, mediated_events)", "response": "Try a new mediation to a new route."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_refundtransfer(\n        mediator_state: MediatorTransferState,\n        mediator_state_change: ReceiveTransferRefund,\n        channelidentifiers_to_channels: ChannelMap,\n        nodeaddresses_to_networkstates: NodeNetworkStateMap,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" Validate and handle a ReceiveTransferRefund mediator_state change.\n    A node might participate in mediated transfer more than once because of\n    refund transfers, eg. A-B-C-B-D-T, B tried to mediate the transfer through\n    C, which didn't have an available route to proceed and refunds B, at this\n    point B is part of the path again and will try a new partner to proceed\n    with the mediation through D, D finally reaches the target T.\n    In the above scenario B has two pairs of payer and payee transfers:\n        payer:A payee:C from the first SendLockedTransfer\n        payer:C payee:D from the following SendRefundTransfer\n    Args:\n        mediator_state (MediatorTransferState): Current mediator_state.\n        mediator_state_change (ReceiveTransferRefund): The mediator_state change.\n    Returns:\n        TransitionResult: The resulting iteration.\n    \"\"\"\n    events: List[Event] = list()\n    if mediator_state.secret is None:\n        # The last sent transfer is the only one that may be refunded, all the\n        # previous ones are refunded already.\n        transfer_pair = mediator_state.transfers_pair[-1]\n        payee_transfer = transfer_pair.payee_transfer\n        payer_transfer = mediator_state_change.transfer\n        channel_identifier = payer_transfer.balance_proof.channel_identifier\n        payer_channel = channelidentifiers_to_channels.get(channel_identifier)\n        if not payer_channel:\n            return TransitionResult(mediator_state, list())\n\n        is_valid, channel_events, _ = channel.handle_refundtransfer(\n            received_transfer=payee_transfer,\n            channel_state=payer_channel,\n            refund=mediator_state_change,\n        )\n        if not is_valid:\n            return TransitionResult(mediator_state, channel_events)\n\n        iteration = mediate_transfer(\n            mediator_state,\n            mediator_state_change.routes,\n            payer_channel,\n            channelidentifiers_to_channels,\n            nodeaddresses_to_networkstates,\n            pseudo_random_generator,\n            payer_transfer,\n            block_number,\n        )\n\n        events.extend(channel_events)\n        events.extend(iteration.events)\n\n    iteration = TransitionResult(mediator_state, events)\n    return iteration", "response": "Validate and handle a ReceiveTransferRefund mediator_state change."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_offchain_secretreveal(\n        mediator_state: MediatorTransferState,\n        mediator_state_change: ReceiveSecretReveal,\n        channelidentifiers_to_channels: ChannelMap,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n        block_hash: BlockHash,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" Handles the secret reveal and sends SendBalanceProof/RevealSecret if necessary. \"\"\"\n    is_valid_reveal = is_valid_secret_reveal(\n        state_change=mediator_state_change,\n        transfer_secrethash=mediator_state.secrethash,\n        secret=mediator_state_change.secret,\n    )\n    is_secret_unknown = mediator_state.secret is None\n\n    # a SecretReveal should be rejected if the payer transfer\n    # has expired. To check for this, we use the last\n    # transfer pair.\n    transfer_pair = mediator_state.transfers_pair[-1]\n    payer_transfer = transfer_pair.payer_transfer\n    channel_identifier = payer_transfer.balance_proof.channel_identifier\n    payer_channel = channelidentifiers_to_channels.get(channel_identifier)\n    if not payer_channel:\n        return TransitionResult(mediator_state, list())\n\n    has_payer_transfer_expired = channel.is_transfer_expired(\n        transfer=transfer_pair.payer_transfer,\n        affected_channel=payer_channel,\n        block_number=block_number,\n    )\n\n    if is_secret_unknown and is_valid_reveal and not has_payer_transfer_expired:\n        iteration = secret_learned(\n            state=mediator_state,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            pseudo_random_generator=pseudo_random_generator,\n            block_number=block_number,\n            block_hash=block_hash,\n            secret=mediator_state_change.secret,\n            secrethash=mediator_state_change.secrethash,\n            payee_address=mediator_state_change.sender,\n        )\n\n    else:\n        iteration = TransitionResult(mediator_state, list())\n\n    return iteration", "response": "Handles the SecretReveal event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_unlock(\n        mediator_state: MediatorTransferState,\n        state_change: ReceiveUnlock,\n        channelidentifiers_to_channels: ChannelMap,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" Handle a ReceiveUnlock state change. \"\"\"\n    events = list()\n    balance_proof_sender = state_change.balance_proof.sender\n    channel_identifier = state_change.balance_proof.channel_identifier\n\n    for pair in mediator_state.transfers_pair:\n        if pair.payer_transfer.balance_proof.sender == balance_proof_sender:\n            channel_state = channelidentifiers_to_channels.get(channel_identifier)\n\n            if channel_state:\n                is_valid, channel_events, _ = channel.handle_unlock(\n                    channel_state,\n                    state_change,\n                )\n                events.extend(channel_events)\n\n                if is_valid:\n                    unlock = EventUnlockClaimSuccess(\n                        pair.payee_transfer.payment_identifier,\n                        pair.payee_transfer.lock.secrethash,\n                    )\n                    events.append(unlock)\n\n                    send_processed = SendProcessed(\n                        recipient=balance_proof_sender,\n                        channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n                        message_identifier=state_change.message_identifier,\n                    )\n                    events.append(send_processed)\n\n                    pair.payer_state = 'payer_balance_proof'\n\n    iteration = TransitionResult(mediator_state, events)\n    return iteration", "response": "Handle a ReceiveUnlock state change."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef state_transition(\n        mediator_state: Optional[MediatorTransferState],\n        state_change: StateChange,\n        channelidentifiers_to_channels: ChannelMap,\n        nodeaddresses_to_networkstates: NodeNetworkStateMap,\n        pseudo_random_generator: random.Random,\n        block_number: BlockNumber,\n        block_hash: BlockHash,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" State machine for a node mediating a transfer. \"\"\"\n    # pylint: disable=too-many-branches\n    # Notes:\n    # - A user cannot cancel a mediated transfer after it was initiated, she\n    #   may only reject to mediate before hand. This is because the mediator\n    #   doesn't control the secret reveal and needs to wait for the lock\n    #   expiration before safely discarding the transfer.\n\n    iteration = TransitionResult(mediator_state, list())\n\n    if type(state_change) == ActionInitMediator:\n        assert isinstance(state_change, ActionInitMediator), MYPY_ANNOTATION\n        if mediator_state is None:\n            iteration = handle_init(\n                state_change=state_change,\n                channelidentifiers_to_channels=channelidentifiers_to_channels,\n                nodeaddresses_to_networkstates=nodeaddresses_to_networkstates,\n                pseudo_random_generator=pseudo_random_generator,\n                block_number=block_number,\n            )\n\n    elif type(state_change) == Block:\n        assert isinstance(state_change, Block), MYPY_ANNOTATION\n        assert mediator_state, 'Block should be accompanied by a valid mediator state'\n        iteration = handle_block(\n            mediator_state=mediator_state,\n            state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            pseudo_random_generator=pseudo_random_generator,\n        )\n\n    elif type(state_change) == ReceiveTransferRefund:\n        assert isinstance(state_change, ReceiveTransferRefund), MYPY_ANNOTATION\n        msg = 'ReceiveTransferRefund should be accompanied by a valid mediator state'\n        assert mediator_state, msg\n        iteration = handle_refundtransfer(\n            mediator_state=mediator_state,\n            mediator_state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            nodeaddresses_to_networkstates=nodeaddresses_to_networkstates,\n            pseudo_random_generator=pseudo_random_generator,\n            block_number=block_number,\n        )\n\n    elif type(state_change) == ReceiveSecretReveal:\n        assert isinstance(state_change, ReceiveSecretReveal), MYPY_ANNOTATION\n        msg = 'ReceiveSecretReveal should be accompanied by a valid mediator state'\n        assert mediator_state, msg\n        iteration = handle_offchain_secretreveal(\n            mediator_state=mediator_state,\n            mediator_state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            pseudo_random_generator=pseudo_random_generator,\n            block_number=block_number,\n            block_hash=block_hash,\n        )\n\n    elif type(state_change) == ContractReceiveSecretReveal:\n        assert isinstance(state_change, ContractReceiveSecretReveal), MYPY_ANNOTATION\n        msg = 'ContractReceiveSecretReveal should be accompanied by a valid mediator state'\n        assert mediator_state, msg\n        iteration = handle_onchain_secretreveal(\n            mediator_state=mediator_state,\n            onchain_secret_reveal=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            pseudo_random_generator=pseudo_random_generator,\n            block_number=block_number,\n        )\n\n    elif type(state_change) == ReceiveUnlock:\n        assert isinstance(state_change, ReceiveUnlock), MYPY_ANNOTATION\n        assert mediator_state, 'ReceiveUnlock should be accompanied by a valid mediator state'\n        iteration = handle_unlock(\n            mediator_state=mediator_state,\n            state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n        )\n\n    elif type(state_change) == ReceiveLockExpired:\n        assert isinstance(state_change, ReceiveLockExpired), MYPY_ANNOTATION\n        assert mediator_state, 'ReceiveLockExpired should be accompanied by a valid mediator state'\n        iteration = handle_lock_expired(\n            mediator_state=mediator_state,\n            state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            block_number=block_number,\n        )\n    elif type(state_change) == ActionChangeNodeNetworkState:\n        assert isinstance(state_change, ActionChangeNodeNetworkState), MYPY_ANNOTATION\n        msg = 'ActionChangeNodeNetworkState should be accompanied by a valid mediator state'\n        assert mediator_state, msg\n        iteration = handle_node_change_network_state(\n            mediator_state=mediator_state,\n            state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            pseudo_random_generator=pseudo_random_generator,\n            block_number=block_number,\n        )\n\n    # this is the place for paranoia\n    if iteration.new_state is not None:\n        assert isinstance(iteration.new_state, MediatorTransferState)\n        sanity_check(iteration.new_state, channelidentifiers_to_channels)\n\n    return clear_if_finalized(iteration, channelidentifiers_to_channels)", "response": "State machine for a node mediating a transfer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the number of services in the database.", "response": "def service_count(self, block_identifier: BlockSpecification) -> int:\n        \"\"\"Get the number of registered services\"\"\"\n        result = self.proxy.contract.functions.serviceCount().call(\n            block_identifier=block_identifier,\n        )\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the address of a service by index.", "response": "def get_service_address(\n            self,\n            block_identifier: BlockSpecification,\n            index: int,\n    ) -> Optional[AddressHex]:\n        \"\"\"Gets the address of a service by index. If index is out of range return None\"\"\"\n        try:\n            result = self.proxy.contract.functions.service_addresses(index).call(\n                block_identifier=block_identifier,\n            )\n        except web3.exceptions.BadFunctionCallOutput:\n            result = None\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_service_url(\n            self,\n            block_identifier: BlockSpecification,\n            service_hex_address: AddressHex,\n    ) -> Optional[str]:\n        \"\"\"Gets the URL of a service by address. If does not exist return None\"\"\"\n        result = self.proxy.contract.functions.urls(service_hex_address).call(\n            block_identifier=block_identifier,\n        )\n        if result == '':\n            return None\n        return result", "response": "Gets the URL of a service by address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_url(self, url: str):\n        gas_limit = self.proxy.estimate_gas('latest', 'setURL', url)\n        transaction_hash = self.proxy.transact('setURL', gas_limit, url)\n        self.client.poll(transaction_hash)\n        assert not check_transaction_threw(self.client, transaction_hash)", "response": "Sets the url needed to access the service via HTTP for the caller"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recover(\n        data: bytes,\n        signature: Signature,\n        hasher: Callable[[bytes], bytes] = eth_sign_sha3,\n) -> Address:\n    \"\"\" eth_recover address from data hash and signature \"\"\"\n    _hash = hasher(data)\n\n    # ecdsa_recover accepts only standard [0,1] v's so we add support also for [27,28] here\n    # anything else will raise BadSignature\n    if signature[-1] >= 27:  # support (0,1,27,28) v values\n        signature = Signature(signature[:-1] + bytes([signature[-1] - 27]))\n\n    try:\n        sig = keys.Signature(signature_bytes=signature)\n        public_key = keys.ecdsa_recover(message_hash=_hash, signature=sig)\n    except BadSignature as e:\n        raise InvalidSignature from e\n    return public_key.to_canonical_address()", "response": "This function is used to recover an address from a data hash and signature."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsigns data hash with local private key", "response": "def sign(self, data: bytes, v: int = 27) -> Signature:\n        \"\"\" Sign data hash with local private key \"\"\"\n        assert v in (0, 27), 'Raiden is only signing messages with v in (0, 27)'\n        _hash = eth_sign_sha3(data)\n        signature = self.private_key.sign_msg_hash(message_hash=_hash)\n        sig_bytes = signature.to_bytes()\n        # adjust last byte to v\n        return sig_bytes[:-1] + bytes([sig_bytes[-1] + v])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef approve(\n            self,\n            allowed_address: Address,\n            allowance: TokenAmount,\n    ):\n        \"\"\" Aprove `allowed_address` to transfer up to `deposit` amount of token.\n\n        Note:\n\n            For channel deposit please use the channel proxy, since it does\n            additional validations.\n        \"\"\"\n        # Note that given_block_identifier is not used here as there\n        # are no preconditions to check before sending the transaction\n\n        log_details = {\n            'node': pex(self.node_address),\n            'contract': pex(self.address),\n            'allowed_address': pex(allowed_address),\n            'allowance': allowance,\n        }\n\n        checking_block = self.client.get_checking_block()\n        error_prefix = 'Call to approve will fail'\n        gas_limit = self.proxy.estimate_gas(\n            checking_block,\n            'approve',\n            to_checksum_address(allowed_address),\n            allowance,\n        )\n\n        if gas_limit:\n            error_prefix = 'Call to approve failed'\n            log.debug('approve called', **log_details)\n            transaction_hash = self.proxy.transact(\n                'approve',\n                safe_gas_limit(gas_limit),\n                to_checksum_address(allowed_address),\n                allowance,\n            )\n\n            self.client.poll(transaction_hash)\n            receipt_or_none = check_transaction_threw(self.client, transaction_hash)\n\n        transaction_executed = gas_limit is not None\n        if not transaction_executed or receipt_or_none:\n            if transaction_executed:\n                block = receipt_or_none['blockNumber']\n            else:\n                block = checking_block\n\n            self.proxy.jsonrpc_client.check_for_insufficient_eth(\n                transaction_name='approve',\n                transaction_executed=transaction_executed,\n                required_gas=GAS_REQUIRED_FOR_APPROVE,\n                block_identifier=block,\n            )\n\n            msg = self._check_why_approved_failed(allowance, block)\n            error_msg = f'{error_prefix}. {msg}'\n            log.critical(error_msg, **log_details)\n            raise RaidenUnrecoverableError(error_msg)\n\n        log.info('approve successful', **log_details)", "response": "Approve the given allowed_address to transfer up to the given amount of token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the balance of the given address.", "response": "def balance_of(self, address, block_identifier='latest'):\n        \"\"\" Return the balance of `address`. \"\"\"\n        return self.proxy.contract.functions.balanceOf(\n            to_checksum_address(address),\n        ).call(block_identifier=block_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the total supply of the token at the given block identifier.", "response": "def total_supply(self, block_identifier='latest'):\n        \"\"\" Return the total supply of the token at the given block identifier. \"\"\"\n        return self.proxy.contract.functions.totalSupply().call(block_identifier=block_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_if_finalized(\n        iteration: TransitionResult,\n) -> TransitionResult[InitiatorPaymentState]:\n    \"\"\" Clear the initiator payment task if all transfers have been finalized\n    or expired. \"\"\"\n    state = cast(InitiatorPaymentState, iteration.new_state)\n\n    if state is None:\n        return iteration\n\n    if len(state.initiator_transfers) == 0:\n        return TransitionResult(None, iteration.events)\n\n    return iteration", "response": "Clear the initiator payment task if all transfers have been finalized."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels the current route.", "response": "def cancel_current_route(\n        payment_state: InitiatorPaymentState,\n        initiator_state: InitiatorTransferState,\n) -> List[Event]:\n    \"\"\" Cancel current route.\n\n    This allows a new route to be tried.\n    \"\"\"\n    assert can_cancel(initiator_state), 'Cannot cancel a route after the secret is revealed'\n\n    transfer_description = initiator_state.transfer_description\n\n    payment_state.cancelled_channels.append(initiator_state.channel_identifier)\n\n    return events_for_cancel_current_route(transfer_description)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subdispatch_to_all_initiatortransfer(\n        payment_state: InitiatorPaymentState,\n        state_change: StateChange,\n        channelidentifiers_to_channels: ChannelMap,\n        pseudo_random_generator: random.Random,\n) -> TransitionResult[InitiatorPaymentState]:\n    events = list()\n    ''' Copy and iterate over the list of keys because this loop\n    will alter the `initiator_transfers` list and this is not\n    allowed if iterating over the original list.\n    '''\n    for secrethash in list(payment_state.initiator_transfers.keys()):\n        initiator_state = payment_state.initiator_transfers[secrethash]\n        sub_iteration = subdispatch_to_initiatortransfer(\n            payment_state=payment_state,\n            initiator_state=initiator_state,\n            state_change=state_change,\n            channelidentifiers_to_channels=channelidentifiers_to_channels,\n            pseudo_random_generator=pseudo_random_generator,\n        )\n        events.extend(sub_iteration.events)\n    return TransitionResult(payment_state, events)", "response": "This function handles all initiator transfers and returns a list of events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the cancelpayment transition.", "response": "def handle_cancelpayment(\n        payment_state: InitiatorPaymentState,\n        channelidentifiers_to_channels: ChannelMap,\n) -> TransitionResult[InitiatorPaymentState]:\n    \"\"\" Cancel the payment and all related transfers. \"\"\"\n    # Cannot cancel a transfer after the secret is revealed\n    events = list()\n    for initiator_state in payment_state.initiator_transfers.values():\n        channel_identifier = initiator_state.channel_identifier\n        channel_state = channelidentifiers_to_channels.get(channel_identifier)\n\n        if not channel_state:\n            continue\n\n        if can_cancel(initiator_state):\n            transfer_description = initiator_state.transfer_description\n            cancel_events = cancel_current_route(payment_state, initiator_state)\n\n            initiator_state.transfer_state = 'transfer_cancelled'\n\n            cancel = EventPaymentSentFailed(\n                payment_network_identifier=channel_state.payment_network_identifier,\n                token_network_identifier=TokenNetworkID(channel_state.token_network_identifier),\n                identifier=transfer_description.payment_identifier,\n                target=transfer_description.target,\n                reason='user canceled payment',\n            )\n            cancel_events.append(cancel)\n\n            events.extend(cancel_events)\n\n    return TransitionResult(payment_state, events)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_lock_expired(\n        payment_state: InitiatorPaymentState,\n        state_change: ReceiveLockExpired,\n        channelidentifiers_to_channels: ChannelMap,\n        block_number: BlockNumber,\n) -> TransitionResult[InitiatorPaymentState]:\n    \"\"\"Initiator also needs to handle LockExpired messages when refund transfers are involved.\n\n    A -> B -> C\n\n    - A sends locked transfer to B\n    - B attempted to forward to C but has not enough capacity\n    - B sends a refund transfer with the same secrethash back to A\n    - When the lock expires B will also send a LockExpired message to A\n    - A needs to be able to properly process it\n\n    Related issue: https://github.com/raiden-network/raiden/issues/3183\n\"\"\"\n    initiator_state = payment_state.initiator_transfers.get(state_change.secrethash)\n    if not initiator_state:\n        return TransitionResult(payment_state, list())\n\n    channel_identifier = initiator_state.channel_identifier\n    channel_state = channelidentifiers_to_channels.get(channel_identifier)\n\n    if not channel_state:\n        return TransitionResult(payment_state, list())\n\n    secrethash = initiator_state.transfer.lock.secrethash\n    result = channel.handle_receive_lock_expired(\n        channel_state=channel_state,\n        state_change=state_change,\n        block_number=block_number,\n    )\n    assert result.new_state, 'handle_receive_lock_expired should not delete the task'\n\n    if not channel.get_lock(result.new_state.partner_state, secrethash):\n        transfer = initiator_state.transfer\n        unlock_failed = EventUnlockClaimFailed(\n            identifier=transfer.payment_identifier,\n            secrethash=transfer.lock.secrethash,\n            reason='Lock expired',\n        )\n        result.events.append(unlock_failed)\n\n    return TransitionResult(payment_state, result.events)", "response": "This function handles LockExpired messages when refund transfers are involved."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef http_retry_with_backoff_middleware(\n        make_request,\n        web3,  # pylint: disable=unused-argument\n        errors: Tuple = (\n            exceptions.ConnectionError,\n            exceptions.HTTPError,\n            exceptions.Timeout,\n            exceptions.TooManyRedirects,\n        ),\n        retries: int = 10,\n        first_backoff: float = 0.2,\n        backoff_factor: float = 2,\n):\n    \"\"\" Retry requests with exponential backoff\n    Creates middleware that retries failed HTTP requests and exponentially\n    increases the backoff between retries. Meant to replace the default\n    middleware `http_retry_request_middleware` for HTTPProvider.\n    \"\"\"\n    def middleware(method, params):\n        backoff = first_backoff\n        if check_if_retry_on_failure(method):\n            for i in range(retries):\n                try:\n                    return make_request(method, params)\n                except errors:\n                    if i < retries - 1:\n                        gevent.sleep(backoff)\n                        backoff *= backoff_factor\n                        continue\n                    else:\n                        raise\n        else:\n            return make_request(method, params)\n\n    return middleware", "response": "Retry requests with exponential backoff."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_random_service(\n        service_registry: ServiceRegistry,\n        block_identifier: BlockSpecification,\n) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"Selects a random PFS from service_registry.\n\n    Returns a tuple of the chosen services url and eth address.\n    If there are no PFS in the given registry, it returns (None, None).\n    \"\"\"\n    count = service_registry.service_count(block_identifier=block_identifier)\n    if count == 0:\n        return None, None\n    index = random.SystemRandom().randint(0, count - 1)\n    address = service_registry.get_service_address(\n        block_identifier=block_identifier,\n        index=index,\n    )\n    # We are using the same blockhash for both blockchain queries so the address\n    # should exist for this query. Additionally at the moment there is no way for\n    # services to be removed from the registry.\n    assert address, 'address should exist for this index'\n    url = service_registry.get_service_url(\n        block_identifier=block_identifier,\n        service_hex_address=address,\n    )\n    return url, address", "response": "Selects a random PFS from the given service registry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure_pfs_or_exit(\n        pfs_address: Optional[str],\n        pfs_eth_address: Optional[str],\n        routing_mode: RoutingMode,\n        service_registry,\n) -> Optional[PFSConfiguration]:\n    \"\"\"\n    Take in the given pfs_address argument, the service registry and find out a\n    pfs address to use.\n\n    If pfs_address is None then basic routing must have been requested.\n    If pfs_address is provided we use that.\n    If pfs_address is 'auto' then we randomly choose a PFS address from the registry\n\n    Returns a NamedTuple containing url, eth_address and fee (per paths request) of\n    the selected PFS, or None if we use basic routing instead of a PFS.\n    \"\"\"\n    if routing_mode == RoutingMode.BASIC:\n        msg = 'Not using path finding services, falling back to basic routing.'\n        log.info(msg)\n        click.secho(msg)\n        return None\n\n    msg = \"With PFS routing mode we shouldn't get to configure pfs with pfs_address being None\"\n    assert pfs_address, msg\n    if pfs_address == 'auto':\n        assert service_registry, 'Should not get here without a service registry'\n        block_hash = service_registry.client.get_confirmed_blockhash()\n        pfs_address, pfs_eth_address = get_random_service(\n            service_registry=service_registry,\n            block_identifier=block_hash,\n        )\n        if pfs_address is None:\n            click.secho(\n                \"The service registry has no registered path finding service \"\n                \"and we don't use basic routing.\",\n            )\n            sys.exit(1)\n\n    assert pfs_eth_address, \"At this point pfs_eth_address can't be none\"\n    pathfinding_service_info = get_pfs_info(pfs_address)\n    if not pathfinding_service_info:\n        click.secho(\n            f'There is an error with the pathfinding service with address '\n            f'{pfs_address}. Raiden will shut down.',\n        )\n        sys.exit(1)\n    else:\n        msg = configure_pfs_message(\n            info=pathfinding_service_info,\n            url=pfs_address,\n            eth_address=pfs_eth_address,\n        )\n        click.secho(msg)\n        log.info('Using PFS', pfs_info=pathfinding_service_info)\n\n    return PFSConfiguration(\n        url=pfs_address,\n        eth_address=pfs_eth_address,\n        fee=pathfinding_service_info.get('price_info', 0),\n    )", "response": "Configure the path finding PFS."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_paths(\n        service_config: Dict[str, Any],\n        our_address: Address,\n        privkey: bytes,\n        current_block_number: BlockNumber,\n        token_network_address: Union[TokenNetworkAddress, TokenNetworkID],\n        route_from: InitiatorAddress,\n        route_to: TargetAddress,\n        value: PaymentAmount,\n) -> List[Dict[str, Any]]:\n    \"\"\" Query paths from the PFS.\n\n    Send a request to the /paths endpoint of the PFS specified in service_config, and\n    retry in case of a failed request if it makes sense.\n    \"\"\"\n\n    max_paths = service_config['pathfinding_max_paths']\n    url = service_config['pathfinding_service_address']\n    payload = {\n        'from': to_checksum_address(route_from),\n        'to': to_checksum_address(route_to),\n        'value': value,\n        'max_paths': max_paths,\n    }\n    offered_fee = service_config.get('pathfinding_fee', service_config['pathfinding_max_fee'])\n    scrap_existing_iou = False\n\n    for retries in reversed(range(MAX_PATHS_QUERY_ATTEMPTS)):\n        payload['iou'] = create_current_iou(\n            config=service_config,\n            token_network_address=token_network_address,\n            our_address=our_address,\n            privkey=privkey,\n            block_number=current_block_number,\n            offered_fee=offered_fee,\n            scrap_existing_iou=scrap_existing_iou,\n        )\n\n        try:\n            return post_pfs_paths(\n                url=url,\n                token_network_address=token_network_address,\n                payload=payload,\n            )\n        except ServiceRequestIOURejected as error:\n            code = error.error_code\n            if retries == 0 or code in (PFSError.WRONG_IOU_RECIPIENT, PFSError.DEPOSIT_TOO_LOW):\n                raise\n            elif code in (PFSError.IOU_ALREADY_CLAIMED, PFSError.IOU_EXPIRED_TOO_EARLY):\n                scrap_existing_iou = True\n            elif code == PFSError.INSUFFICIENT_SERVICE_PAYMENT:\n                if offered_fee < service_config['pathfinding_max_fee']:\n                    offered_fee = service_config['pathfinding_max_fee']\n                    # TODO: Query the PFS for the fee here instead of using the max fee\n                else:\n                    raise\n            log.info(f'PFS rejected our IOU, reason: {error}. Attempting again.')\n\n    # If we got no results after MAX_PATHS_QUERY_ATTEMPTS return empty list of paths\n    return list()", "response": "Query paths from the PFS."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a single message to a recipient.", "response": "def single_queue_send(\n        transport: 'UDPTransport',\n        recipient: Address,\n        queue: Queue_T,\n        queue_identifier: QueueIdentifier,\n        event_stop: Event,\n        event_healthy: Event,\n        event_unhealthy: Event,\n        message_retries: int,\n        message_retry_timeout: int,\n        message_retry_max_timeout: int,\n):\n    \"\"\" Handles a single message queue for `recipient`.\n\n    Notes:\n    - This task must be the only consumer of queue.\n    - This task can be killed at any time, but the intended usage is to stop it\n      with the event_stop.\n    - If there are many queues for the same recipient, it is the\n      caller's responsibility to not start them together to avoid congestion.\n    - This task assumes the endpoint is never cleared after it's first known.\n      If this assumption changes the code must be updated to handle unknown\n      addresses.\n    \"\"\"\n\n    # A NotifyingQueue is required to implement cancelability, otherwise the\n    # task cannot be stopped while the greenlet waits for an element to be\n    # inserted in the queue.\n    if not isinstance(queue, NotifyingQueue):\n        raise ValueError('queue must be a NotifyingQueue.')\n\n    # Reusing the event, clear must be carefully done\n    data_or_stop = event_first_of(\n        queue,\n        event_stop,\n    )\n\n    # Wait for the endpoint registration or to quit\n    transport.log.debug(\n        'queue: waiting for node to become healthy',\n        queue_identifier=queue_identifier,\n        queue_size=len(queue),\n    )\n\n    event_first_of(\n        event_healthy,\n        event_stop,\n    ).wait()\n\n    transport.log.debug(\n        'queue: processing queue',\n        queue_identifier=queue_identifier,\n        queue_size=len(queue),\n    )\n\n    while True:\n        data_or_stop.wait()\n\n        if event_stop.is_set():\n            transport.log.debug(\n                'queue: stopping',\n                queue_identifier=queue_identifier,\n                queue_size=len(queue),\n            )\n            return\n\n        # The queue is not empty at this point, so this won't raise Empty.\n        # This task being the only consumer is a requirement.\n        (messagedata, message_id) = queue.peek(block=False)\n\n        transport.log.debug(\n            'queue: sending message',\n            recipient=pex(recipient),\n            msgid=message_id,\n            queue_identifier=queue_identifier,\n            queue_size=len(queue),\n        )\n\n        backoff = timeout_exponential_backoff(\n            message_retries,\n            message_retry_timeout,\n            message_retry_max_timeout,\n        )\n\n        acknowledged = retry_with_recovery(\n            transport,\n            messagedata,\n            message_id,\n            recipient,\n            event_stop,\n            event_healthy,\n            event_unhealthy,\n            backoff,\n        )\n\n        if acknowledged:\n            queue.get()\n\n            # Checking the length of the queue does not trigger a\n            # context-switch, so it's safe to assume the length of the queue\n            # won't change under our feet and when a new item will be added the\n            # event will be set again.\n            if not queue:\n                data_or_stop.clear()\n\n                if event_stop.is_set():\n                    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run(self):  # pylint: disable=method-hidden\n        try:\n            self.event_stop.wait()\n        except gevent.GreenletExit:  # killed without exception\n            self.event_stop.set()\n            gevent.killall(self.greenlets)  # kill children\n            raise  # re-raise to keep killed status\n        except Exception:\n            self.stop()  # ensure cleanup and wait on subtasks\n            raise", "response": "Main method for long - running tasks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting a healthcheck task for recipient and returns a HealthEvents with locks to react on its current state.", "response": "def get_health_events(self, recipient):\n        \"\"\" Starts a healthcheck task for `recipient` and returns a\n        HealthEvents with locks to react on its current state.\n        \"\"\"\n        if recipient not in self.addresses_events:\n            self.start_health_check(recipient)\n\n        return self.addresses_events[recipient]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a task for healthchecking recipient.", "response": "def start_health_check(self, recipient):\n        \"\"\" Starts a task for healthchecking `recipient` if there is not\n        one yet.\n\n        It also whitelists the address\n        \"\"\"\n        if recipient not in self.addresses_events:\n            self.whitelist(recipient)  # noop for now, for compatibility\n            ping_nonce = self.nodeaddresses_to_nonces.setdefault(\n                recipient,\n                {'nonce': 0},  # HACK: Allows the task to mutate the object\n            )\n\n            events = healthcheck.HealthEvents(\n                event_healthy=Event(),\n                event_unhealthy=Event(),\n            )\n\n            self.addresses_events[recipient] = events\n\n            greenlet_healthcheck = gevent.spawn(\n                healthcheck.healthcheck,\n                self,\n                recipient,\n                self.event_stop,\n                events.event_healthy,\n                events.event_unhealthy,\n                self.nat_keepalive_retries,\n                self.nat_keepalive_timeout,\n                self.nat_invitation_timeout,\n                ping_nonce,\n            )\n            greenlet_healthcheck.name = f'Healthcheck for {pex(recipient)}'\n            greenlet_healthcheck.link_exception(self.on_error)\n            self.greenlets.append(greenlet_healthcheck)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_queue_for(\n            self,\n            queue_identifier: QueueIdentifier,\n            items: List[QueueItem_T],\n    ) -> NotifyingQueue:\n        \"\"\" Create the queue identified by the queue_identifier\n        and initialize it with `items`.\n        \"\"\"\n        recipient = queue_identifier.recipient\n        queue = self.queueids_to_queues.get(queue_identifier)\n        assert queue is None\n\n        queue = NotifyingQueue(items=items)\n        self.queueids_to_queues[queue_identifier] = queue\n\n        events = self.get_health_events(recipient)\n\n        greenlet_queue = gevent.spawn(\n            single_queue_send,\n            self,\n            recipient,\n            queue,\n            queue_identifier,\n            self.event_stop,\n            events.event_healthy,\n            events.event_unhealthy,\n            self.retries_before_backoff,\n            self.retry_interval,\n            self.retry_interval * 10,\n        )\n\n        if queue_identifier.channel_identifier == CHANNEL_IDENTIFIER_GLOBAL_QUEUE:\n            greenlet_queue.name = f'Queue for {pex(recipient)} - global'\n        else:\n            greenlet_queue.name = (\n                f'Queue for {pex(recipient)} - {queue_identifier.channel_identifier}'\n            )\n\n        greenlet_queue.link_exception(self.on_error)\n        self.greenlets.append(greenlet_queue)\n\n        self.log.debug(\n            'new queue created for',\n            queue_identifier=queue_identifier,\n            items_qty=len(items),\n        )\n\n        return queue", "response": "Create the queue identified by the queue_identifier and initialize it with items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the queue identified by the given queue identifier.", "response": "def get_queue_for(\n            self,\n            queue_identifier: QueueIdentifier,\n    ) -> NotifyingQueue:\n        \"\"\" Return the queue identified by the given queue identifier.\n\n        If the queue doesn't exist it will be instantiated.\n        \"\"\"\n        queue = self.queueids_to_queues.get(queue_identifier)\n\n        if queue is None:\n            items: List[QueueItem_T] = list()\n            queue = self.init_queue_for(queue_identifier, items)\n\n        return queue"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a new message to the recipient.", "response": "def send_async(\n            self,\n            queue_identifier: QueueIdentifier,\n            message: Message,\n    ):\n        \"\"\" Send a new ordered message to recipient.\n\n        Messages that use the same `queue_identifier` are ordered.\n        \"\"\"\n        recipient = queue_identifier.recipient\n        if not is_binary_address(recipient):\n            raise ValueError('Invalid address {}'.format(pex(recipient)))\n\n        # These are not protocol messages, but transport specific messages\n        if isinstance(message, (Delivered, Ping, Pong)):\n            raise ValueError('Do not use send for {} messages'.format(message.__class__.__name__))\n\n        messagedata = message.encode()\n        if len(messagedata) > self.UDP_MAX_MESSAGE_SIZE:\n            raise ValueError(\n                'message size exceeds the maximum {}'.format(self.UDP_MAX_MESSAGE_SIZE),\n            )\n\n        # message identifiers must be unique\n        message_id = message.message_identifier\n\n        # ignore duplicates\n        if message_id not in self.messageids_to_asyncresults:\n            self.messageids_to_asyncresults[message_id] = AsyncResult()\n\n            queue = self.get_queue_for(queue_identifier)\n            queue.put((messagedata, message_id))\n            assert queue.is_set()\n\n            self.log.debug(\n                'Message queued',\n                queue_identifier=queue_identifier,\n                queue_size=len(queue),\n                message=message,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maybe_send(self, recipient: Address, message: Message):\n\n        if not is_binary_address(recipient):\n            raise InvalidAddress('Invalid address {}'.format(pex(recipient)))\n\n        messagedata = message.encode()\n        host_port = self.get_host_port(recipient)\n\n        self.maybe_sendraw(host_port, messagedata)", "response": "Send a message to the recipient if the transport is running."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maybe_sendraw_with_result(\n            self,\n            recipient: Address,\n            messagedata: bytes,\n            message_id: UDPMessageID,\n    ) -> AsyncResult:\n        \"\"\" Send message to recipient if the transport is running.\n\n        Returns:\n            An AsyncResult that will be set once the message is delivered. As\n            long as the message has not been acknowledged with a Delivered\n            message the function will return the same AsyncResult.\n        \"\"\"\n        async_result = self.messageids_to_asyncresults.get(message_id)\n        if async_result is None:\n            async_result = AsyncResult()\n            self.messageids_to_asyncresults[message_id] = async_result\n\n        host_port = self.get_host_port(recipient)\n        self.maybe_sendraw(host_port, messagedata)\n\n        return async_result", "response": "Send a message to recipient and return an AsyncResult that will be set once the message is delivered."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maybe_sendraw(self, host_port: Tuple[int, int], messagedata: bytes):\n\n        # Don't sleep if timeout is zero, otherwise a context-switch is done\n        # and the message is delayed, increasing its latency\n        sleep_timeout = self.throttle_policy.consume(1)\n        if sleep_timeout:\n            gevent.sleep(sleep_timeout)\n\n        # Check the udp socket is still available before trying to send the\n        # message. There must be *no context-switches after this test*.\n        if hasattr(self.server, 'socket'):\n            self.server.sendto(\n                messagedata,\n                host_port,\n            )", "response": "Send a message to the recipient if the transport is running."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling an UDP packet.", "response": "def receive(\n            self,\n            messagedata: bytes,\n            host_port: Tuple[str, int],  # pylint: disable=unused-argument\n    ) -> bool:\n        \"\"\" Handle an UDP packet. \"\"\"\n        # pylint: disable=unidiomatic-typecheck\n\n        if len(messagedata) > self.UDP_MAX_MESSAGE_SIZE:\n            self.log.warning(\n                'Invalid message: Packet larger than maximum size',\n                message=encode_hex(messagedata),\n                length=len(messagedata),\n            )\n            return False\n\n        try:\n            message = decode(messagedata)\n        except InvalidProtocolMessage as e:\n            self.log.warning(\n                'Invalid protocol message',\n                error=str(e),\n                message=encode_hex(messagedata),\n            )\n            return False\n\n        if type(message) == Pong:\n            assert isinstance(message, Pong), MYPY_ANNOTATION\n            self.receive_pong(message)\n        elif type(message) == Ping:\n            assert isinstance(message, Ping), MYPY_ANNOTATION\n            self.receive_ping(message)\n        elif type(message) == Delivered:\n            assert isinstance(message, Delivered), MYPY_ANNOTATION\n            self.receive_delivered(message)\n        elif message is not None:\n            self.receive_message(message)\n        else:\n            self.log.warning(\n                'Invalid message: Unknown cmdid',\n                message=encode_hex(messagedata),\n            )\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive_message(self, message: Message):\n        self.raiden.on_message(message)\n\n        # Sending Delivered after the message is decoded and *processed*\n        # gives a stronger guarantee than what is required from a\n        # transport.\n        #\n        # Alternatives are, from weakest to strongest options:\n        # - Just save it on disk and asynchronously process the messages\n        # - Decode it, save to the WAL, and asynchronously process the\n        #   state change\n        # - Decode it, save to the WAL, and process it (the current\n        #   implementation)\n        delivered_message = Delivered(delivered_message_identifier=message.message_identifier)\n        self.raiden.sign(delivered_message)\n\n        self.maybe_send(\n            message.sender,\n            delivered_message,\n        )", "response": "Handle a Raiden protocol message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive_delivered(self, delivered: Delivered):\n        self.raiden.on_message(delivered)\n\n        message_id = delivered.delivered_message_identifier\n        async_result = self.raiden.transport.messageids_to_asyncresults.get(message_id)\n\n        # clear the async result, otherwise we have a memory leak\n        if async_result is not None:\n            del self.messageids_to_asyncresults[message_id]\n            async_result.set()\n        else:\n            self.log.warn(\n                'Unknown delivered message received',\n                message_id=message_id,\n            )", "response": "Handle a Delivered message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive_ping(self, ping: Ping):\n\n        self.log_healthcheck.debug(\n            'Ping received',\n            message_id=ping.nonce,\n            message=ping,\n            sender=pex(ping.sender),\n        )\n\n        pong = Pong(nonce=ping.nonce)\n        self.raiden.sign(pong)\n\n        try:\n            self.maybe_send(ping.sender, pong)\n        except (InvalidAddress, UnknownAddress) as e:\n            self.log.debug(\"Couldn't send the `Delivered` message\", e=e)", "response": "Handle a Ping message by answering with a Pong."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive_pong(self, pong: Pong):\n\n        message_id = ('ping', pong.nonce, pong.sender)\n        async_result = self.messageids_to_asyncresults.get(message_id)\n\n        if async_result is not None:\n            self.log_healthcheck.debug(\n                'Pong received',\n                sender=pex(pong.sender),\n                message_id=pong.nonce,\n            )\n\n            async_result.set(True)\n\n        else:\n            self.log_healthcheck.warn(\n                'Unknown pong received',\n                message_id=message_id,\n            )", "response": "Handles a Pong message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ping(self, nonce: Nonce) -> bytes:\n        message = Ping(\n            nonce=nonce,\n            current_protocol_version=constants.PROTOCOL_VERSION,\n        )\n        self.raiden.sign(message)\n        return message.encode()", "response": "Returns a Ping message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_config_file(\n        command_function: Union[click.Command, click.Group],\n        cli_params: Dict[str, Any],\n        ctx,\n        config_file_option_name='config_file',\n):\n    \"\"\" Applies all options set in the config file to `cli_params` \"\"\"\n    paramname_to_param = {param.name: param for param in command_function.params}\n    path_params = {\n        param.name\n        for param in command_function.params\n        if isinstance(param.type, (click.Path, click.File))\n    }\n\n    config_file_path = Path(cli_params[config_file_option_name])\n    config_file_values = dict()\n    try:\n        with config_file_path.open() as config_file:\n            config_file_values = load(config_file)\n    except OSError as ex:\n        # Silently ignore if 'file not found' and the config file path is the default\n        config_file_param = paramname_to_param[config_file_option_name]\n        config_file_default_path = Path(\n            config_file_param.type.expand_default(config_file_param.get_default(ctx), cli_params),\n        )\n        default_config_missing = (\n            ex.errno == errno.ENOENT and\n            config_file_path.resolve() == config_file_default_path.resolve()\n        )\n        if default_config_missing:\n            cli_params['config_file'] = None\n        else:\n            click.secho(f\"Error opening config file: {ex}\", fg='red')\n            sys.exit(1)\n    except TomlError as ex:\n        click.secho(f'Error loading config file: {ex}', fg='red')\n        sys.exit(1)\n\n    for config_name, config_value in config_file_values.items():\n        config_name_int = config_name.replace('-', '_')\n\n        if config_name_int not in paramname_to_param:\n            click.secho(\n                f\"Unknown setting '{config_name}' found in config file - ignoring.\",\n                fg='yellow',\n            )\n            continue\n\n        if config_name_int in path_params:\n            # Allow users to use `~` in paths in the config file\n            config_value = os.path.expanduser(config_value)\n\n        if config_name_int == LOG_CONFIG_OPTION_NAME:\n            # Uppercase log level names\n            config_value = {k: v.upper() for k, v in config_value.items()}\n        else:\n            # Pipe config file values through cli converter to ensure correct types\n            # We exclude `log-config` because it already is a dict when loading from toml\n            try:\n                config_value = paramname_to_param[config_name_int].type.convert(\n                    config_value,\n                    paramname_to_param[config_name_int],\n                    ctx,\n                )\n            except click.BadParameter as ex:\n                click.secho(f\"Invalid config file setting '{config_name}': {ex}\", fg='red')\n                sys.exit(1)\n\n        # Use the config file value if the value from the command line is the default\n        if cli_params[config_name_int] == paramname_to_param[config_name_int].get_default(ctx):\n            cli_params[config_name_int] = config_value", "response": "Applies all options set in the config file to cli_params"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_matrix_servers(url: str) -> List[str]:\n    try:\n        response = requests.get(url)\n        if response.status_code != 200:\n            raise requests.RequestException('Response: {response!r}')\n    except requests.RequestException as ex:\n        raise RuntimeError(f'Could not fetch matrix servers list: {url!r} => {ex!r}') from ex\n\n    available_servers = []\n    for line in response.text.splitlines():\n        line = line.strip(string.whitespace + '-')\n        if line.startswith('#') or not line:\n            continue\n        if not line.startswith('http'):\n            line = 'https://' + line  # default schema\n        available_servers.append(line)\n    return available_servers", "response": "Fetch a list of matrix servers from a text url"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmodifies data in - place and replace keys named secret.", "response": "def _redact_secret(\n        data: Union[Dict, List],\n) -> Union[Dict, List]:\n    \"\"\" Modify `data` in-place and replace keys named `secret`. \"\"\"\n\n    if isinstance(data, dict):\n        stack = [data]\n    else:\n        stack = []\n\n    while stack:\n        current = stack.pop()\n\n        if 'secret' in current:\n            current['secret'] = '<redacted>'\n        else:\n            stack.extend(\n                value\n                for value in current.values()\n                if isinstance(value, dict)\n            )\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        assert self.stop_event.ready(), f'Node already started. node:{self!r}'\n        self.stop_event.clear()\n        self.greenlets = list()\n\n        self.ready_to_process_events = False  # set to False because of restarts\n\n        if self.database_dir is not None:\n            self.db_lock.acquire(timeout=0)\n            assert self.db_lock.is_locked, f'Database not locked. node:{self!r}'\n\n        # start the registration early to speed up the start\n        if self.config['transport_type'] == 'udp':\n            endpoint_registration_greenlet = gevent.spawn(\n                self.discovery.register,\n                self.address,\n                self.config['transport']['udp']['external_ip'],\n                self.config['transport']['udp']['external_port'],\n            )\n\n        self.maybe_upgrade_db()\n\n        storage = sqlite.SerializedSQLiteStorage(\n            database_path=self.database_path,\n            serializer=serialize.JSONSerializer(),\n        )\n        storage.update_version()\n        storage.log_run()\n        self.wal = wal.restore_to_state_change(\n            transition_function=node.state_transition,\n            storage=storage,\n            state_change_identifier='latest',\n        )\n\n        if self.wal.state_manager.current_state is None:\n            log.debug(\n                'No recoverable state available, creating inital state.',\n                node=pex(self.address),\n            )\n            # On first run Raiden needs to fetch all events for the payment\n            # network, to reconstruct all token network graphs and find opened\n            # channels\n            last_log_block_number = self.query_start_block\n            last_log_block_hash = self.chain.client.blockhash_from_blocknumber(\n                last_log_block_number,\n            )\n\n            state_change = ActionInitChain(\n                pseudo_random_generator=random.Random(),\n                block_number=last_log_block_number,\n                block_hash=last_log_block_hash,\n                our_address=self.chain.node_address,\n                chain_id=self.chain.network_id,\n            )\n            self.handle_and_track_state_change(state_change)\n\n            payment_network = PaymentNetworkState(\n                self.default_registry.address,\n                [],  # empty list of token network states as it's the node's startup\n            )\n            state_change = ContractReceiveNewPaymentNetwork(\n                transaction_hash=constants.EMPTY_HASH,\n                payment_network=payment_network,\n                block_number=last_log_block_number,\n                block_hash=last_log_block_hash,\n            )\n            self.handle_and_track_state_change(state_change)\n        else:\n            # The `Block` state change is dispatched only after all the events\n            # for that given block have been processed, filters can be safely\n            # installed starting from this position without losing events.\n            last_log_block_number = views.block_number(self.wal.state_manager.current_state)\n            log.debug(\n                'Restored state from WAL',\n                last_restored_block=last_log_block_number,\n                node=pex(self.address),\n            )\n\n            known_networks = views.get_payment_network_identifiers(views.state_from_raiden(self))\n            if known_networks and self.default_registry.address not in known_networks:\n                configured_registry = pex(self.default_registry.address)\n                known_registries = lpex(known_networks)\n                raise RuntimeError(\n                    f'Token network address mismatch.\\n'\n                    f'Raiden is configured to use the smart contract '\n                    f'{configured_registry}, which conflicts with the current known '\n                    f'smart contracts {known_registries}',\n                )\n\n        # Restore the current snapshot group\n        state_change_qty = self.wal.storage.count_state_changes()\n        self.snapshot_group = state_change_qty // SNAPSHOT_STATE_CHANGES_COUNT\n\n        # Install the filters using the latest confirmed from_block value,\n        # otherwise blockchain logs can be lost.\n        self.install_all_blockchain_filters(\n            self.default_registry,\n            self.default_secret_registry,\n            last_log_block_number,\n        )\n\n        # Complete the first_run of the alarm task and synchronize with the\n        # blockchain since the last run.\n        #\n        # Notes about setup order:\n        # - The filters must be polled after the node state has been primed,\n        # otherwise the state changes won't have effect.\n        # - The alarm must complete its first run before the transport is started,\n        #   to reject messages for closed/settled channels.\n        self.alarm.register_callback(self._callback_new_block)\n        self.alarm.first_run(last_log_block_number)\n\n        chain_state = views.state_from_raiden(self)\n\n        self._initialize_payment_statuses(chain_state)\n        self._initialize_transactions_queues(chain_state)\n        self._initialize_messages_queues(chain_state)\n        self._initialize_whitelists(chain_state)\n        self._initialize_monitoring_services_queue(chain_state)\n        self._initialize_ready_to_processed_events()\n\n        if self.config['transport_type'] == 'udp':\n            endpoint_registration_greenlet.get()  # re-raise if exception occurred\n\n        # Start the side-effects:\n        # - React to blockchain events\n        # - React to incoming messages\n        # - Send pending transactions\n        # - Send pending message\n        self.alarm.link_exception(self.on_error)\n        self.transport.link_exception(self.on_error)\n        self._start_transport(chain_state)\n        self._start_alarm_task()\n\n        log.debug('Raiden Service started', node=pex(self.address))\n        super().start()", "response": "Start the node synchronously."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the node and return the result.", "response": "def _run(self, *args, **kwargs):  # pylint: disable=method-hidden\n        \"\"\" Busy-wait on long-lived subtasks/greenlets, re-raise if any error occurs \"\"\"\n        self.greenlet.name = f'RaidenService._run node:{pex(self.address)}'\n        try:\n            self.stop_event.wait()\n        except gevent.GreenletExit:  # killed without exception\n            self.stop_event.set()\n            gevent.killall([self.alarm, self.transport])  # kill children\n            raise  # re-raise to keep killed status\n        except Exception:\n            self.stop()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop(self):\n        if self.stop_event.ready():  # not started\n            return\n\n        # Needs to come before any greenlets joining\n        self.stop_event.set()\n\n        # Filters must be uninstalled after the alarm task has stopped. Since\n        # the events are polled by an alarm task callback, if the filters are\n        # uninstalled before the alarm task is fully stopped the callback\n        # `poll_blockchain_events` will fail.\n        #\n        # We need a timeout to prevent an endless loop from trying to\n        # contact the disconnected client\n        self.transport.stop()\n        self.alarm.stop()\n\n        self.transport.join()\n        self.alarm.join()\n\n        self.blockchain_events.uninstall_all_event_listeners()\n\n        # Close storage DB to release internal DB lock\n        self.wal.storage.conn.close()\n\n        if self.db_lock is not None:\n            self.db_lock.release()\n\n        log.debug('Raiden Service stopped', node=pex(self.address))", "response": "Stop the node gracefully. Raise if any stop - time error occurred on any subtask."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_pending_greenlet(self, greenlet: Greenlet):\n\n        def remove(_):\n            self.greenlets.remove(greenlet)\n\n        self.greenlets.append(greenlet)\n        greenlet.link_exception(self.on_error)\n        greenlet.link_value(remove)", "response": "Adds a greenlet to the list of pending greenlets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _start_transport(self, chain_state: ChainState):\n        assert self.alarm.is_primed(), f'AlarmTask not primed. node:{self!r}'\n        assert self.ready_to_process_events, f'Event procossing disable. node:{self!r}'\n\n        self.transport.start(\n            raiden_service=self,\n            message_handler=self.message_handler,\n            prev_auth_data=chain_state.last_transport_authdata,\n        )\n\n        for neighbour in views.all_neighbour_nodes(chain_state):\n            if neighbour != ConnectionManager.BOOTSTRAP_ADDR:\n                self.start_health_check_for(neighbour)", "response": "Initialize the transport and related facilities."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_and_track_state_change(self, state_change: StateChange):\n        for greenlet in self.handle_state_change(state_change):\n            self.add_pending_greenlet(greenlet)", "response": "Dispatch the state change and track the exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndispatching the state change and return the processing threads.", "response": "def handle_state_change(self, state_change: StateChange) -> List[Greenlet]:\n        \"\"\" Dispatch the state change and return the processing threads.\n\n        Use this for error reporting, failures in the returned greenlets,\n        should be re-raised using `gevent.joinall` with `raise_error=True`.\n        \"\"\"\n        assert self.wal, f'WAL not restored. node:{self!r}'\n        log.debug(\n            'State change',\n            node=pex(self.address),\n            state_change=_redact_secret(serialize.JSONSerializer.serialize(state_change)),\n        )\n\n        old_state = views.state_from_raiden(self)\n\n        raiden_event_list = self.wal.log_and_dispatch(state_change)\n\n        current_state = views.state_from_raiden(self)\n        for changed_balance_proof in views.detect_balance_proof_change(old_state, current_state):\n            update_services_from_balance_proof(self, current_state, changed_balance_proof)\n\n        log.debug(\n            'Raiden events',\n            node=pex(self.address),\n            raiden_events=[\n                _redact_secret(serialize.JSONSerializer.serialize(event))\n                for event in raiden_event_list\n            ],\n        )\n\n        greenlets: List[Greenlet] = list()\n        if self.ready_to_process_events:\n            for raiden_event in raiden_event_list:\n                greenlets.append(\n                    self.handle_event(raiden_event=raiden_event),\n                )\n\n            state_changes_count = self.wal.storage.count_state_changes()\n            new_snapshot_group = (\n                state_changes_count // SNAPSHOT_STATE_CHANGES_COUNT\n            )\n            if new_snapshot_group > self.snapshot_group:\n                log.debug('Storing snapshot', snapshot_id=new_snapshot_group)\n                self.wal.snapshot()\n                self.snapshot_group = new_snapshot_group\n\n        return greenlets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nspawn a new thread to handle a Raiden event.", "response": "def handle_event(self, raiden_event: RaidenEvent) -> Greenlet:\n        \"\"\"Spawn a new thread to handle a Raiden event.\n\n        This will spawn a new greenlet to handle each event, which is\n        important for two reasons:\n\n        - Blockchain transactions can be queued without interfering with each\n          other.\n        - The calling thread is free to do more work. This is specially\n          important for the AlarmTask thread, which will eventually cause the\n          node to send transactions when a given Block is reached (e.g.\n          registering a secret or settling a channel).\n\n        Important:\n\n            This is spawing a new greenlet for /each/ transaction. It's\n            therefore /required/ that there is *NO* order among these.\n        \"\"\"\n        return gevent.spawn(self._handle_event, raiden_event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart health checking for the given node_address.", "response": "def start_health_check_for(self, node_address: Address):\n        \"\"\"Start health checking `node_address`.\n\n        This function is a noop during initialization, because health checking\n        can be started as a side effect of some events (e.g. new channel). For\n        these cases the healthcheck will be started by\n        `start_neighbours_healthcheck`.\n        \"\"\"\n        if self.transport:\n            self.transport.start_health_check(node_address)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _callback_new_block(self, latest_block: Dict):\n        # User facing APIs, which have on-chain side-effects, force polled the\n        # blockchain to update the node's state. This force poll is used to\n        # provide a consistent view to the user, e.g. a channel open call waits\n        # for the transaction to be mined and force polled the event to update\n        # the node's state. This pattern introduced a race with the alarm task\n        # and the task which served the user request, because the events are\n        # returned only once per filter. The lock below is to protect against\n        # these races (introduced by the commit\n        # 3686b3275ff7c0b669a6d5e2b34109c3bdf1921d)\n        with self.event_poll_lock:\n            latest_block_number = latest_block['number']\n\n            # Handle testing with private chains. The block number can be\n            # smaller than confirmation_blocks\n            confirmed_block_number = max(\n                GENESIS_BLOCK_NUMBER,\n                latest_block_number - self.config['blockchain']['confirmation_blocks'],\n            )\n            confirmed_block = self.chain.client.web3.eth.getBlock(confirmed_block_number)\n\n            # These state changes will be procesed with a block_number which is\n            # /larger/ than the ChainState's block_number.\n            for event in self.blockchain_events.poll_blockchain_events(confirmed_block_number):\n                on_blockchain_event(self, event)\n\n            # On restart the Raiden node will re-create the filters with the\n            # ethereum node. These filters will have the from_block set to the\n            # value of the latest Block state change. To avoid missing events\n            # the Block state change is dispatched only after all of the events\n            # have been processed.\n            #\n            # This means on some corner cases a few events may be applied\n            # twice, this will happen if the node crashed and some events have\n            # been processed but the Block state change has not been\n            # dispatched.\n            state_change = Block(\n                block_number=confirmed_block_number,\n                gas_limit=confirmed_block['gasLimit'],\n                block_hash=BlockHash(bytes(confirmed_block['hash'])),\n            )\n\n            # Note: It's important to /not/ block here, because this function\n            # can be called from the alarm task greenlet, which should not\n            # starve.\n            self.handle_and_track_state_change(state_change)", "response": "Called when a new block is detected by the alarm task."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initialize_transactions_queues(self, chain_state: ChainState):\n        assert self.alarm.is_primed(), f'AlarmTask not primed. node:{self!r}'\n\n        pending_transactions = views.get_pending_transactions(chain_state)\n\n        log.debug(\n            'Processing pending transactions',\n            num_pending_transactions=len(pending_transactions),\n            node=pex(self.address),\n        )\n\n        for transaction in pending_transactions:\n            try:\n                self.raiden_event_handler.on_raiden_event(self, transaction)\n            except RaidenRecoverableError as e:\n                log.error(str(e))\n            except InvalidDBData:\n                raise\n            except RaidenUnrecoverableError as e:\n                log_unrecoverable = (\n                    self.config['environment_type'] == Environment.PRODUCTION and\n                    not self.config['unrecoverable_error_should_crash']\n                )\n                if log_unrecoverable:\n                    log.error(str(e))\n                else:\n                    raise", "response": "Initialize the pending transaction queues from the previous run."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes all the message queues with the transport.", "response": "def _initialize_messages_queues(self, chain_state: ChainState):\n        \"\"\"Initialize all the message queues with the transport.\n\n        Note:\n            All messages from the state queues must be pushed to the transport\n            before it's started. This is necessary to avoid a race where the\n            transport processes network messages too quickly, queueing new\n            messages before any of the previous messages, resulting in new\n            messages being out-of-order.\n\n            The Alarm task must be started before this method is called,\n            otherwise queues for channel closed while the node was offline\n            won't be properly cleared. It is not bad but it is suboptimal.\n        \"\"\"\n        assert not self.transport, f'Transport is running. node:{self!r}'\n        assert self.alarm.is_primed(), f'AlarmTask not primed. node:{self!r}'\n\n        events_queues = views.get_all_messagequeues(chain_state)\n\n        for queue_identifier, event_queue in events_queues.items():\n            self.start_health_check_for(queue_identifier.recipient)\n\n            for event in event_queue:\n                message = message_from_sendevent(event)\n                self.sign(message)\n                self.transport.send_async(queue_identifier, message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the monitoring services queue for all current balance proofs.", "response": "def _initialize_monitoring_services_queue(self, chain_state: ChainState):\n        \"\"\"Send the monitoring requests for all current balance proofs.\n\n        Note:\n            The node must always send the *received* balance proof to the\n            monitoring service, *before* sending its own locked transfer\n            forward. If the monitoring service is updated after, then the\n            following can happen:\n\n            For a transfer A-B-C where this node is B\n\n            - B receives T1 from A and processes it\n            - B forwards its T2 to C\n            * B crashes (the monitoring service is not updated)\n\n            For the above scenario, the monitoring service would not have the\n            latest balance proof received by B from A available with the lock\n            for T1, but C would. If the channel B-C is closed and B does not\n            come back online in time, the funds for the lock L1 can be lost.\n\n            During restarts the rationale from above has to be replicated.\n            Because the initialization code *is not* the same as the event\n            handler. This means the balance proof updates must be done prior to\n            the processing of the message queues.\n        \"\"\"\n        msg = (\n            'Transport was started before the monitoring service queue was updated. '\n            'This can lead to safety issue. node:{self!r}'\n        )\n        assert not self.transport, msg\n\n        msg = (\n            'The node state was not yet recovered, cant read balance proofs. node:{self!r}'\n        )\n        assert self.wal, msg\n\n        current_balance_proofs = views.detect_balance_proof_change(\n            old_state=ChainState(\n                pseudo_random_generator=chain_state.pseudo_random_generator,\n                block_number=GENESIS_BLOCK_NUMBER,\n                block_hash=constants.EMPTY_HASH,\n                our_address=chain_state.our_address,\n                chain_id=chain_state.chain_id,\n            ),\n            current_state=chain_state,\n        )\n        for balance_proof in current_balance_proofs:\n            update_services_from_balance_proof(self, chain_state, balance_proof)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the whitelist neighbors and mediated transfers on transport.", "response": "def _initialize_whitelists(self, chain_state: ChainState):\n        \"\"\" Whitelist neighbors and mediated transfer targets on transport \"\"\"\n\n        for neighbour in views.all_neighbour_nodes(chain_state):\n            if neighbour == ConnectionManager.BOOTSTRAP_ADDR:\n                continue\n            self.transport.whitelist(neighbour)\n\n        events_queues = views.get_all_messagequeues(chain_state)\n\n        for event_queue in events_queues.values():\n            for event in event_queue:\n                if isinstance(event, SendLockedTransfer):\n                    transfer = event.transfer\n                    if transfer.initiator == self.address:\n                        self.transport.whitelist(address=transfer.target)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransfer `amount` between this node and `target`. This method will start an asynchronous transfer, the transfer might fail or succeed depending on a couple of factors: - Existence of a path that can be used, through the usage of direct or intermediary channels. - Network speed, making the transfer sufficiently fast so it doesn't expire.", "response": "def mediated_transfer_async(\n            self,\n            token_network_identifier: TokenNetworkID,\n            amount: PaymentAmount,\n            target: TargetAddress,\n            identifier: PaymentID,\n            fee: FeeAmount = MEDIATION_FEE,\n            secret: Secret = None,\n            secret_hash: SecretHash = None,\n    ) -> PaymentStatus:\n        \"\"\" Transfer `amount` between this node and `target`.\n\n        This method will start an asynchronous transfer, the transfer might fail\n        or succeed depending on a couple of factors:\n\n            - Existence of a path that can be used, through the usage of direct\n              or intermediary channels.\n            - Network speed, making the transfer sufficiently fast so it doesn't\n              expire.\n        \"\"\"\n        if secret is None:\n            if secret_hash is None:\n                secret = random_secret()\n            else:\n                secret = EMPTY_SECRET\n\n        payment_status = self.start_mediated_transfer_with_secret(\n            token_network_identifier=token_network_identifier,\n            amount=amount,\n            fee=fee,\n            target=target,\n            identifier=identifier,\n            secret=secret,\n            secret_hash=secret_hash,\n        )\n\n        return payment_status"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_dict_hook(data):\n    type_ = data.get('_type', None)\n    if type_ is not None:\n        klass = _import_type(type_)\n\n        msg = '_type must point to a class with `from_dict` static method'\n        assert hasattr(klass, 'from_dict'), msg\n\n        return klass.from_dict(data)\n    return data", "response": "Decode internal objects encoded using to_dict_hook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_dict_hook(obj):\n    if hasattr(obj, 'to_dict'):\n        result = obj.to_dict()\n        assert isinstance(result, dict), 'to_dict must return a dictionary'\n        result['_type'] = f'{obj.__module__}.{obj.__class__.__name__}'\n        result['_version'] = 0\n        return result\n\n    raise TypeError(\n        f'Object of type {obj.__class__.__name__} is not JSON serializable',\n    )", "response": "Convert internal objects to a serializable representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the transaction was reverted or if it executed properly MF", "response": "def check_transaction_threw(client, transaction_hash: bytes):\n    \"\"\"Check if the transaction threw/reverted or if it executed properly\n       Returns None in case of success and the transaction receipt if the\n       transaction's status indicator is 0x0.\n    \"\"\"\n    receipt = client.get_transaction_receipt(transaction_hash)\n\n    if 'status' not in receipt:\n        raise ValueError(\n            'Transaction receipt does not contain a status field. Upgrade your client',\n        )\n\n    if receipt['status'] == RECEIPT_FAILURE_CODE:\n        return receipt\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rgb_color_picker(obj, min_luminance=None, max_luminance=None):\n    color_value = int.from_bytes(\n        hashlib.md5(str(obj).encode('utf-8')).digest(),\n        'little',\n    ) % 0xffffff\n    color = Color(f'#{color_value:06x}')\n    if min_luminance and color.get_luminance() < min_luminance:\n        color.set_luminance(min_luminance)\n    elif max_luminance and color.get_luminance() > max_luminance:\n        color.set_luminance(max_luminance)\n    return color", "response": "Modified version of colour. RGB_color_picker"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a random 32 byte secret except the 0 secret since it s not accepted in the contracts", "response": "def random_secret() -> Secret:\n    \"\"\" Return a random 32 byte secret except the 0 secret since it's not accepted in the contracts\n    \"\"\"\n    while True:\n        secret = os.urandom(32)\n        if secret != constants.EMPTY_HASH:\n            return Secret(secret)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a string address and turns it into binary.", "response": "def address_checksum_and_decode(addr: str) -> Address:\n    \"\"\" Accepts a string address and turns it into binary.\n\n        Makes sure that the string address provided starts is 0x prefixed and\n        checksummed according to EIP55 specification\n    \"\"\"\n    if not is_0x_prefixed(addr):\n        raise InvalidAddress('Address must be 0x prefixed')\n\n    if not is_checksum_address(addr):\n        raise InvalidAddress('Address must be EIP55 checksummed')\n\n    addr_bytes = decode_hex(addr)\n    assert len(addr_bytes) in (20, 0)\n    return Address(addr_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef privatekey_to_publickey(private_key_bin: bytes) -> bytes:\n    if not ishash(private_key_bin):\n        raise ValueError('private_key_bin format mismatch. maybe hex encoded?')\n    return keys.PrivateKey(private_key_bin).public_key.to_bytes()", "response": "Returns public key in bitcoins bin encoding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncollect information about the system and installation.", "response": "def get_system_spec() -> Dict[str, str]:\n    \"\"\"Collect information about the system and installation.\n    \"\"\"\n    import pkg_resources\n    import platform\n\n    if sys.platform == 'darwin':\n        system_info = 'macOS {} {}'.format(\n            platform.mac_ver()[0],\n            platform.architecture()[0],\n        )\n    else:\n        system_info = '{} {} {}'.format(\n            platform.system(),\n            '_'.join(part for part in platform.architecture() if part),\n            platform.release(),\n        )\n\n    try:\n        version = pkg_resources.require(raiden.__name__)[0].version\n    except (pkg_resources.VersionConflict, pkg_resources.DistributionNotFound):\n        raise RuntimeError(\n            'Cannot detect Raiden version. Did you do python setup.py?  '\n            'Refer to https://raiden-network.readthedocs.io/en/latest/'\n            'overview_and_guide.html#for-developers',\n        )\n\n    system_spec = {\n        'raiden': version,\n        'python_implementation': platform.python_implementation(),\n        'python_version': platform.python_version(),\n        'system': system_info,\n        'architecture': platform.machine(),\n        'distribution': 'bundled' if getattr(sys, 'frozen', False) else 'source',\n    }\n    return system_spec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest for a function and wait for it to return a truth value or to timeout.", "response": "def wait_until(func, wait_for=None, sleep_for=0.5):\n    \"\"\"Test for a function and wait for it to return a truth value or to timeout.\n    Returns the value or None if a timeout is given and the function didn't return\n    inside time timeout\n    Args:\n        func (callable): a function to be evaluated, use lambda if parameters are required\n        wait_for (float, integer, None): the maximum time to wait, or None for an infinite loop\n        sleep_for (float, integer): how much to gevent.sleep between calls\n    Returns:\n        func(): result of func, if truth value, or None\"\"\"\n    res = func()\n\n    if res:\n        return res\n\n    if wait_for:\n        deadline = time.time() + wait_for\n        while not res and time.time() <= deadline:\n            gevent.sleep(sleep_for)\n            res = func()\n\n    else:\n        while not res:\n            gevent.sleep(sleep_for)\n            res = func()\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_in_pairs(arg: Iterable) -> Iterable[Tuple]:\n    # We are using zip_longest with one clever hack:\n    # https://docs.python.org/3/library/itertools.html#itertools.zip_longest\n    # We create an iterator out of the list and then pass the same iterator to\n    # the function two times. Thus the function consumes a different element\n    # from the iterator each time and produces the desired result.\n    iterator = iter(arg)\n    return zip_longest(iterator, iterator)", "response": "Splits given iterable in pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges a dictionary into b.", "response": "def merge_dict(to_update: dict, other_dict: dict):\n    \"\"\" merges b into a \"\"\"\n    for key, value in other_dict.items():\n        has_map = (\n            isinstance(value, collections.Mapping) and\n            isinstance(to_update.get(key, None), collections.Mapping)\n        )\n\n        if has_map:\n            merge_dict(to_update[key], value)\n        else:\n            to_update[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_gas_limit(*estimates: int) -> int:\n    assert None not in estimates, 'if estimateGas returned None it should not reach here'\n    calculated_limit = max(estimates)\n    return int(calculated_limit * constants.GAS_FACTOR)", "response": "Calculates a safe gas limit for a number of gas estimates\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef block_specification_to_number(block: BlockSpecification, web3: Web3) -> BlockNumber:\n    if isinstance(block, str):\n        msg = f\"string block specification can't contain {block}\"\n        assert block in ('latest', 'pending'), msg\n        number = web3.eth.getBlock(block)['number']\n    elif isinstance(block, T_BlockHash):\n        number = web3.eth.getBlock(block)['number']\n    elif isinstance(block, T_BlockNumber):\n        number = block\n    else:\n        if __debug__:\n            raise AssertionError(f'Unknown type {type(block)} given for block specification')\n\n    return BlockNumber(number)", "response": "Converts a block specification to an actual block number"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detail(self, block_identifier: BlockSpecification) -> ChannelDetails:\n        return self.token_network.detail(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )", "response": "Returns the channel details."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the channels settle_timeout.", "response": "def settle_timeout(self) -> int:\n        \"\"\" Returns the channels settle_timeout. \"\"\"\n\n        # There is no way to get the settle timeout after the channel has been closed as\n        # we're saving gas. Therefore get the ChannelOpened event and get the timeout there.\n        filter_args = get_filter_args_for_specific_event_from_channel(\n            token_network_address=self.token_network.address,\n            channel_identifier=self.channel_identifier,\n            event_name=ChannelEvent.OPENED,\n            contract_manager=self.contract_manager,\n        )\n\n        events = self.token_network.proxy.contract.web3.eth.getLogs(filter_args)\n        assert len(events) > 0, 'No matching ChannelOpen event found.'\n\n        # we want the latest event here, there might have been multiple channels\n        event = decode_event(\n            self.contract_manager.get_contract_abi(CONTRACT_TOKEN_NETWORK),\n            events[-1],\n        )\n        return event['args']['settle_timeout']"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns if the channel is opened.", "response": "def opened(self, block_identifier: BlockSpecification) -> bool:\n        \"\"\" Returns if the channel is opened. \"\"\"\n        return self.token_network.channel_is_opened(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the channel is closed.", "response": "def closed(self, block_identifier: BlockSpecification) -> bool:\n        \"\"\" Returns if the channel is closed. \"\"\"\n        return self.token_network.channel_is_closed(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning if the channel is settled.", "response": "def settled(self, block_identifier: BlockSpecification) -> bool:\n        \"\"\" Returns if the channel is settled. \"\"\"\n        return self.token_network.channel_is_settled(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the address of the closer of the channel.", "response": "def closing_address(self, block_identifier: BlockSpecification) -> Optional[Address]:\n        \"\"\" Returns the address of the closer of the channel. \"\"\"\n        return self.token_network.closing_address(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef can_transfer(self, block_identifier: BlockSpecification) -> bool:\n        return self.token_network.can_transfer(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )", "response": "Returns True if the channel is opened and the node has deposit in it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose the channel using the provided balance proof.", "response": "def close(\n            self,\n            nonce: Nonce,\n            balance_hash: BalanceHash,\n            additional_hash: AdditionalHash,\n            signature: Signature,\n            block_identifier: BlockSpecification,\n    ):\n        \"\"\" Closes the channel using the provided balance proof. \"\"\"\n        self.token_network.close(\n            channel_identifier=self.channel_identifier,\n            partner=self.participant2,\n            balance_hash=balance_hash,\n            nonce=nonce,\n            additional_hash=additional_hash,\n            signature=signature,\n            given_block_identifier=block_identifier,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_transfer(\n            self,\n            nonce: Nonce,\n            balance_hash: BalanceHash,\n            additional_hash: AdditionalHash,\n            partner_signature: Signature,\n            signature: Signature,\n            block_identifier: BlockSpecification,\n    ):\n        \"\"\" Updates the channel using the provided balance proof. \"\"\"\n        self.token_network.update_transfer(\n            channel_identifier=self.channel_identifier,\n            partner=self.participant2,\n            balance_hash=balance_hash,\n            nonce=nonce,\n            additional_hash=additional_hash,\n            closing_signature=partner_signature,\n            non_closing_signature=signature,\n            given_block_identifier=block_identifier,\n        )", "response": "Updates the channel with the balance proof."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef events_for_unlock_lock(\n        initiator_state: InitiatorTransferState,\n        channel_state: NettingChannelState,\n        secret: Secret,\n        secrethash: SecretHash,\n        pseudo_random_generator: random.Random,\n) -> List[Event]:\n    \"\"\" Unlocks the lock offchain, and emits the events for the successful payment. \"\"\"\n    # next hop learned the secret, unlock the token locally and send the\n    # lock claim message to next hop\n    transfer_description = initiator_state.transfer_description\n\n    message_identifier = message_identifier_from_prng(pseudo_random_generator)\n    unlock_lock = channel.send_unlock(\n        channel_state=channel_state,\n        message_identifier=message_identifier,\n        payment_identifier=transfer_description.payment_identifier,\n        secret=secret,\n        secrethash=secrethash,\n    )\n\n    payment_sent_success = EventPaymentSentSuccess(\n        payment_network_identifier=channel_state.payment_network_identifier,\n        token_network_identifier=TokenNetworkID(channel_state.token_network_identifier),\n        identifier=transfer_description.payment_identifier,\n        amount=transfer_description.amount,\n        target=transfer_description.target,\n        secret=secret,\n    )\n\n    unlock_success = EventUnlockSuccess(\n        transfer_description.payment_identifier,\n        transfer_description.secrethash,\n    )\n\n    return [unlock_lock, payment_sent_success, unlock_success]", "response": "Unlocks the lock offchain and emits the events for the successful payment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the lock has expired and if it has sends a remove expired lock and emits the failing events.", "response": "def handle_block(\n        initiator_state: InitiatorTransferState,\n        state_change: Block,\n        channel_state: NettingChannelState,\n        pseudo_random_generator: random.Random,\n) -> TransitionResult[InitiatorTransferState]:\n    \"\"\" Checks if the lock has expired, and if it has sends a remove expired\n    lock and emits the failing events.\n    \"\"\"\n    secrethash = initiator_state.transfer.lock.secrethash\n    locked_lock = channel_state.our_state.secrethashes_to_lockedlocks.get(secrethash)\n\n    if not locked_lock:\n        if channel_state.partner_state.secrethashes_to_lockedlocks.get(secrethash):\n            return TransitionResult(initiator_state, list())\n        else:\n            # if lock is not in our or our partner's locked locks then the\n            # task can go\n            return TransitionResult(None, list())\n\n    lock_expiration_threshold = BlockNumber(\n        locked_lock.expiration + DEFAULT_WAIT_BEFORE_LOCK_REMOVAL,\n    )\n    lock_has_expired, _ = channel.is_lock_expired(\n        end_state=channel_state.our_state,\n        lock=locked_lock,\n        block_number=state_change.block_number,\n        lock_expiration_threshold=lock_expiration_threshold,\n    )\n\n    events: List[Event] = list()\n\n    if lock_has_expired:\n        is_channel_open = channel.get_status(channel_state) == CHANNEL_STATE_OPENED\n        if is_channel_open:\n            expired_lock_events = channel.events_for_expired_lock(\n                channel_state=channel_state,\n                locked_lock=locked_lock,\n                pseudo_random_generator=pseudo_random_generator,\n            )\n            events.extend(expired_lock_events)\n\n        if initiator_state.received_secret_request:\n            reason = 'bad secret request message from target'\n        else:\n            reason = 'lock expired'\n\n        transfer_description = initiator_state.transfer_description\n        payment_identifier = transfer_description.payment_identifier\n        # TODO: When we introduce multiple transfers per payment this needs to be\n        #       reconsidered. As we would want to try other routes once a route\n        #       has failed, and a transfer failing does not mean the entire payment\n        #       would have to fail.\n        #       Related issue: https://github.com/raiden-network/raiden/issues/2329\n        payment_failed = EventPaymentSentFailed(\n            payment_network_identifier=transfer_description.payment_network_identifier,\n            token_network_identifier=transfer_description.token_network_identifier,\n            identifier=payment_identifier,\n            target=transfer_description.target,\n            reason=reason,\n        )\n        unlock_failed = EventUnlockFailed(\n            identifier=payment_identifier,\n            secrethash=initiator_state.transfer_description.secrethash,\n            reason=reason,\n        )\n\n        lock_exists = channel.lock_exists_in_either_channel_side(\n            channel_state=channel_state,\n            secrethash=secrethash,\n        )\n\n        return TransitionResult(\n            # If the lock is either in our state or partner state we keep the\n            # task around to wait for the LockExpired messages to sync.\n            # Check https://github.com/raiden-network/raiden/issues/3183\n            initiator_state if lock_exists else None,\n            events + [payment_failed, unlock_failed],\n        )\n    else:\n        return TransitionResult(initiator_state, events)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the next available channel that can be used to start the transfer.", "response": "def next_channel_from_routes(\n        available_routes: List[RouteState],\n        channelidentifiers_to_channels: ChannelMap,\n        transfer_amount: PaymentAmount,\n) -> Optional[NettingChannelState]:\n    \"\"\" Returns the first channel that can be used to start the transfer.\n    The routing service can race with local changes, so the recommended routes\n    must be validated.\n    \"\"\"\n    for route in available_routes:\n        channel_identifier = route.channel_identifier\n        channel_state = channelidentifiers_to_channels.get(channel_identifier)\n\n        if not channel_state:\n            continue\n\n        if channel.get_status(channel_state) != CHANNEL_STATE_OPENED:\n            continue\n\n        pending_transfers = channel.get_number_of_pending_transfers(channel_state.our_state)\n        if pending_transfers >= MAXIMUM_PENDING_TRANSFERS:\n            continue\n\n        distributable = channel.get_distributable(\n            channel_state.our_state,\n            channel_state.partner_state,\n        )\n        if transfer_amount > distributable:\n            continue\n\n        if channel.is_valid_amount(channel_state.our_state, transfer_amount):\n            return channel_state\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_lockedtransfer(\n        transfer_description: TransferDescriptionWithSecretState,\n        channel_state: NettingChannelState,\n        message_identifier: MessageID,\n        block_number: BlockNumber,\n) -> SendLockedTransfer:\n    \"\"\" Create a mediated transfer using channel. \"\"\"\n    assert channel_state.token_network_identifier == transfer_description.token_network_identifier\n\n    lock_expiration = get_initial_lock_expiration(\n        block_number,\n        channel_state.reveal_timeout,\n    )\n\n    # The payment amount and the fee amount must be included in the locked\n    # amount, as a guarantee to the mediator that the fee will be claimable\n    # on-chain.\n    total_amount = PaymentWithFeeAmount(\n        transfer_description.amount + transfer_description.allocated_fee,\n    )\n\n    lockedtransfer_event = channel.send_lockedtransfer(\n        channel_state=channel_state,\n        initiator=transfer_description.initiator,\n        target=transfer_description.target,\n        amount=total_amount,\n        message_identifier=message_identifier,\n        payment_identifier=transfer_description.payment_identifier,\n        expiration=lock_expiration,\n        secrethash=transfer_description.secrethash,\n    )\n    return lockedtransfer_event", "response": "Create a locked transfer using the channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_lock_pending(\n        end_state: NettingChannelEndState,\n        secrethash: SecretHash,\n) -> bool:\n    \"\"\"True if the `secrethash` corresponds to a lock that is pending to be claimed\n    and didn't expire.\n    \"\"\"\n    return (\n        secrethash in end_state.secrethashes_to_lockedlocks or\n        secrethash in end_state.secrethashes_to_unlockedlocks or\n        secrethash in end_state.secrethashes_to_onchain_unlockedlocks\n    )", "response": "True if the secrethash corresponds to a lock that was pending to be claimed by the given end_state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_deposit_confirmed(\n        channel_state: NettingChannelState,\n        block_number: BlockNumber,\n) -> bool:\n    \"\"\"True if the block which mined the deposit transaction has been\n    confirmed.\n    \"\"\"\n    if not channel_state.deposit_transaction_queue:\n        return False\n\n    return is_transaction_confirmed(\n        channel_state.deposit_transaction_queue[0].block_number,\n        block_number,\n    )", "response": "True if the block which mined the deposit transaction has been confirmed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_lock_expired(\n        end_state: NettingChannelEndState,\n        lock: LockType,\n        block_number: BlockNumber,\n        lock_expiration_threshold: BlockNumber,\n) -> SuccessOrError:\n    \"\"\" Determine whether a lock has expired.\n\n    The lock has expired if both:\n\n        - The secret was not registered on-chain in time.\n        - The current block exceeds lock's expiration + confirmation blocks.\n    \"\"\"\n\n    secret_registered_on_chain = lock.secrethash in end_state.secrethashes_to_onchain_unlockedlocks\n    if secret_registered_on_chain:\n        return (False, 'lock has been unlocked on-chain')\n\n    if block_number < lock_expiration_threshold:\n        msg = (\n            f'current block number ({block_number}) is not larger than '\n            f'lock.expiration + confirmation blocks ({lock_expiration_threshold})'\n        )\n        return (False, msg)\n\n    return (True, None)", "response": "Determine whether a lock has expired."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrues if the secrethash is for a lock with a known secret.", "response": "def is_secret_known(\n        end_state: NettingChannelEndState,\n        secrethash: SecretHash,\n) -> bool:\n    \"\"\"True if the `secrethash` is for a lock with a known secret.\"\"\"\n    return (\n        secrethash in end_state.secrethashes_to_unlockedlocks or\n        secrethash in end_state.secrethashes_to_onchain_unlockedlocks\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the secret if the secrethash is for a lock with a known secret.", "response": "def get_secret(\n        end_state: NettingChannelEndState,\n        secrethash: SecretHash,\n) -> Optional[Secret]:\n    \"\"\"Returns `secret` if the `secrethash` is for a lock with a known secret.\"\"\"\n    partial_unlock_proof = end_state.secrethashes_to_unlockedlocks.get(secrethash)\n\n    if partial_unlock_proof is None:\n        partial_unlock_proof = end_state.secrethashes_to_onchain_unlockedlocks.get(secrethash)\n\n    if partial_unlock_proof is not None:\n        return partial_unlock_proof.secret\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the balance proof would overflow onchain operations.", "response": "def is_balance_proof_safe_for_onchain_operations(\n        balance_proof: BalanceProofSignedState,\n) -> bool:\n    \"\"\" Check if the balance proof would overflow onchain. \"\"\"\n    total_amount = balance_proof.transferred_amount + balance_proof.locked_amount\n    return total_amount <= UINT256_MAX"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the balance proof can be used on - chain.", "response": "def is_balance_proof_usable_onchain(\n        received_balance_proof: BalanceProofSignedState,\n        channel_state: NettingChannelState,\n        sender_state: NettingChannelEndState,\n) -> SuccessOrError:\n    \"\"\" Checks the balance proof can be used on-chain.\n\n    For a balance proof to be valid it must be newer than the previous one,\n    i.e. the nonce must increase, the signature must tie the balance proof to\n    the correct channel, and the values must not result in an under/overflow\n    onchain.\n\n    Important: This predicate does not validate all the message fields. The\n    fields locksroot, transferred_amount, and locked_amount **MUST** be\n    validated elsewhere based on the message type.\n    \"\"\"\n    expected_nonce = get_next_nonce(sender_state)\n\n    is_valid_signature_, signature_msg = is_valid_signature(\n        received_balance_proof,\n        sender_state.address,\n    )\n\n    result: SuccessOrError\n\n    # TODO: Accept unlock messages if the node has not yet sent a transaction\n    # with the balance proof to the blockchain, this will save one call to\n    # unlock on-chain for the non-closing party.\n    if get_status(channel_state) != CHANNEL_STATE_OPENED:\n        # The channel must be opened, otherwise if receiver is the closer, the\n        # balance proof cannot be used onchain.\n        msg = f'The channel is already closed.'\n        result = (False, msg)\n\n    elif received_balance_proof.channel_identifier != channel_state.identifier:\n        # Informational message, the channel_identifier **validated by the\n        # signature** must match for the balance_proof to be valid.\n        msg = (\n            f\"channel_identifier does not match. \"\n            f\"expected: {channel_state.identifier} \"\n            f\"got: {received_balance_proof.channel_identifier}.\"\n        )\n        result = (False, msg)\n\n    elif received_balance_proof.token_network_identifier != channel_state.token_network_identifier:\n        # Informational message, the token_network_identifier **validated by\n        # the signature** must match for the balance_proof to be valid.\n        msg = (\n            f\"token_network_identifier does not match. \"\n            f\"expected: {channel_state.token_network_identifier} \"\n            f\"got: {received_balance_proof.token_network_identifier}.\"\n        )\n        result = (False, msg)\n\n    elif received_balance_proof.chain_id != channel_state.chain_id:\n        # Informational message, the chain_id **validated by the signature**\n        # must match for the balance_proof to be valid.\n        msg = (\n            f\"chain_id does not match channel's \"\n            f\"chain_id. expected: {channel_state.chain_id} \"\n            f\"got: {received_balance_proof.chain_id}.\"\n        )\n        result = (False, msg)\n\n    elif not is_balance_proof_safe_for_onchain_operations(received_balance_proof):\n        transferred_amount_after_unlock = (\n            received_balance_proof.transferred_amount +\n            received_balance_proof.locked_amount\n        )\n        msg = (\n            f\"Balance proof total transferred amount would overflow onchain. \"\n            f\"max: {UINT256_MAX} result would be: {transferred_amount_after_unlock}\"\n        )\n\n        result = (False, msg)\n\n    elif received_balance_proof.nonce != expected_nonce:\n        # The nonces must increase sequentially, otherwise there is a\n        # synchronization problem.\n        msg = (\n            f'Nonce did not change sequentially, expected: {expected_nonce} '\n            f'got: {received_balance_proof.nonce}.'\n        )\n\n        result = (False, msg)\n\n    elif not is_valid_signature_:\n        # The signature must be valid, otherwise the balance proof cannot be\n        # used onchain.\n        result = (False, signature_msg)\n\n    else:\n        result = (True, None)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the amount of unlocked and unclaimed locks and on - chain unlocked locks.", "response": "def get_batch_unlock_gain(\n        channel_state: NettingChannelState,\n) -> UnlockGain:\n    \"\"\"Collect amounts for unlocked/unclaimed locks and onchain unlocked locks.\n    Note: this function does not check expiry, so the values make only sense during settlement.\n\n    Returns:\n        gain_from_partner_locks: locks amount received and unlocked on-chain\n        gain_from_our_locks: locks amount which are unlocked or unclaimed\n    \"\"\"\n    gain_from_partner_locks = TokenAmount(sum(\n        unlock.lock.amount\n        for unlock in channel_state.partner_state.secrethashes_to_onchain_unlockedlocks.values()\n    ))\n\n    \"\"\"\n    The current participant will gain from unlocking its own locks when:\n    - The partner never managed to provide the secret to unlock the locked amount.\n    - The partner provided the secret to claim the locked amount but the current\n      participant node never sent out the unlocked balance proof and the partner\n      did not unlock the lock on-chain.\n    \"\"\"\n    our_locked_locks_amount = sum(\n        lock.amount\n        for lock in channel_state.our_state.secrethashes_to_lockedlocks.values()\n    )\n    our_unclaimed_locks_amount = sum(\n        lock.amount for lock in channel_state.our_state.secrethashes_to_unlockedlocks.values()\n    )\n    gain_from_our_locks = TokenAmount(our_locked_locks_amount + our_unclaimed_locks_amount)\n    return UnlockGain(\n        from_partner_locks=gain_from_partner_locks,\n        from_our_locks=gain_from_our_locks,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_distributable(\n        sender: NettingChannelEndState,\n        receiver: NettingChannelEndState,\n) -> TokenAmount:\n    \"\"\"Return the amount of tokens that can be used by the `sender`.\n\n    The returned value is limited to a UINT256, since that is the representation\n    used in the smart contracts and we cannot use a larger value. The limit is\n    enforced on transferred_amount + locked_amount to avoid overflows. This is\n    an additional security check.\n    \"\"\"\n    _, _, transferred_amount, locked_amount = get_current_balanceproof(sender)\n\n    distributable = get_balance(sender, receiver) - get_amount_locked(sender)\n\n    overflow_limit = max(\n        UINT256_MAX - transferred_amount - locked_amount,\n        0,\n    )\n\n    return TokenAmount(min(overflow_limit, distributable))", "response": "Return the amount of tokens that can be used by the sender."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the merkle tree that unlocks the given token.", "response": "def get_batch_unlock(\n        end_state: NettingChannelEndState,\n) -> Optional[MerkleTreeLeaves]:\n    \"\"\" Unlock proof for an entire merkle tree of pending locks\n\n    The unlock proof contains all the merkle tree data, tightly packed, needed by the token\n    network contract to verify the secret expiry and calculate the token amounts to transfer.\n    \"\"\"\n\n    if len(end_state.merkletree.layers[LEAVES]) == 0:  # pylint: disable=len-as-condition\n        return None\n\n    lockhashes_to_locks = dict()\n    lockhashes_to_locks.update({\n        lock.lockhash: lock\n        for secrethash, lock in end_state.secrethashes_to_lockedlocks.items()\n    })\n    lockhashes_to_locks.update({\n        proof.lock.lockhash: proof.lock\n        for secrethash, proof in end_state.secrethashes_to_unlockedlocks.items()\n    })\n    lockhashes_to_locks.update({\n        proof.lock.lockhash: proof.lock\n        for secrethash, proof in end_state.secrethashes_to_onchain_unlockedlocks.items()\n    })\n\n    ordered_locks = [\n        lockhashes_to_locks[LockHash(lockhash)]\n        for lockhash in end_state.merkletree.layers[LEAVES]\n    ]\n\n    # Not sure why the cast is needed here. The error was:\n    # Incompatible return value type\n    # (got \"List[HashTimeLockState]\", expected \"Optional[MerkleTreeLeaves]\")\n    return cast(MerkleTreeLeaves, ordered_locks)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the lock corresponding to secrethash or None if the lock is unknown.", "response": "def get_lock(\n        end_state: NettingChannelEndState,\n        secrethash: SecretHash,\n) -> Optional[HashTimeLockState]:\n    \"\"\"Return the lock correspoding to `secrethash` or None if the lock is\n    unknown.\n    \"\"\"\n    lock = end_state.secrethashes_to_lockedlocks.get(secrethash)\n\n    if not lock:\n        partial_unlock = end_state.secrethashes_to_unlockedlocks.get(secrethash)\n\n        if not partial_unlock:\n            partial_unlock = end_state.secrethashes_to_onchain_unlockedlocks.get(secrethash)\n\n        if partial_unlock:\n            lock = partial_unlock.lock\n\n    assert isinstance(lock, HashTimeLockState) or lock is None\n    return lock"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lock_exists_in_either_channel_side(\n        channel_state: NettingChannelState,\n        secrethash: SecretHash,\n) -> bool:\n    \"\"\"Check if the lock with `secrethash` exists in either our state or the partner's state\"\"\"\n    lock = get_lock(channel_state.our_state, secrethash)\n    if not lock:\n        lock = get_lock(channel_state.partner_state, secrethash)\n    return lock is not None", "response": "Check if the lock with secrethash exists in either our state or the partner s state"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the lock from the index structures.", "response": "def _del_lock(end_state: NettingChannelEndState, secrethash: SecretHash) -> None:\n    \"\"\"Removes the lock from the indexing structures.\n\n    Note:\n        This won't change the merkletree!\n    \"\"\"\n    assert is_lock_pending(end_state, secrethash)\n\n    _del_unclaimed_lock(end_state, secrethash)\n\n    if secrethash in end_state.secrethashes_to_onchain_unlockedlocks:\n        del end_state.secrethashes_to_onchain_unlockedlocks[secrethash]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_merkletree_with(\n        merkletree: MerkleTreeState,\n        lockhash: LockHash,\n) -> Optional[MerkleTreeState]:\n    \"\"\"Register the given lockhash with the existing merkle tree.\"\"\"\n    # Use None to inform the caller the lockshash is already known\n    result = None\n\n    leaves = merkletree.layers[LEAVES]\n    if lockhash not in leaves:\n        leaves = list(leaves)\n        leaves.append(Keccak256(lockhash))\n        result = MerkleTreeState(compute_layers(leaves))\n\n    return result", "response": "Register the given lockhash with the existing merkle tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_offchain_secret(\n        channel_state: NettingChannelState,\n        secret: Secret,\n        secrethash: SecretHash,\n) -> None:\n    \"\"\"This will register the secret and set the lock to the unlocked stated.\n\n    Even though the lock is unlock it is *not* claimed. The capacity will\n    increase once the next balance proof is received.\n    \"\"\"\n    our_state = channel_state.our_state\n    partner_state = channel_state.partner_state\n\n    register_secret_endstate(our_state, secret, secrethash)\n    register_secret_endstate(partner_state, secret, secrethash)", "response": "This will register the secret and set the lock to unlocked stated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_receive_lock_expired(\n        channel_state: NettingChannelState,\n        state_change: ReceiveLockExpired,\n        block_number: BlockNumber,\n) -> TransitionResult[NettingChannelState]:\n    \"\"\"Remove expired locks from channel states.\"\"\"\n    is_valid, msg, merkletree = is_valid_lock_expired(\n        state_change=state_change,\n        channel_state=channel_state,\n        sender_state=channel_state.partner_state,\n        receiver_state=channel_state.our_state,\n        block_number=block_number,\n    )\n\n    events: List[Event] = list()\n    if is_valid:\n        assert merkletree, 'is_valid_lock_expired should return merkletree if valid'\n        channel_state.partner_state.balance_proof = state_change.balance_proof\n        channel_state.partner_state.merkletree = merkletree\n\n        _del_unclaimed_lock(channel_state.partner_state, state_change.secrethash)\n\n        send_processed = SendProcessed(\n            recipient=state_change.balance_proof.sender,\n            channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n            message_identifier=state_change.message_identifier,\n        )\n        events = [send_processed]\n    else:\n        assert msg, 'is_valid_lock_expired should return error msg if not valid'\n        invalid_lock_expired = EventInvalidReceivedLockExpired(\n            secrethash=state_change.secrethash,\n            reason=msg,\n        )\n        events = [invalid_lock_expired]\n\n    return TransitionResult(channel_state, events)", "response": "Remove expired locks from channel states."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_receive_lockedtransfer(\n        channel_state: NettingChannelState,\n        mediated_transfer: LockedTransferSignedState,\n) -> EventsOrError:\n    \"\"\"Register the latest known transfer.\n\n    The receiver needs to use this method to update the container with a\n    _valid_ transfer, otherwise the locksroot will not contain the pending\n    transfer. The receiver needs to ensure that the merkle root has the\n    secrethash included, otherwise it won't be able to claim it.\n    \"\"\"\n    events: List[Event]\n    is_valid, msg, merkletree = is_valid_lockedtransfer(\n        mediated_transfer,\n        channel_state,\n        channel_state.partner_state,\n        channel_state.our_state,\n    )\n\n    if is_valid:\n        assert merkletree, 'is_valid_lock_expired should return merkletree if valid'\n        channel_state.partner_state.balance_proof = mediated_transfer.balance_proof\n        channel_state.partner_state.merkletree = merkletree\n\n        lock = mediated_transfer.lock\n        channel_state.partner_state.secrethashes_to_lockedlocks[lock.secrethash] = lock\n\n        send_processed = SendProcessed(\n            recipient=mediated_transfer.balance_proof.sender,\n            channel_identifier=CHANNEL_IDENTIFIER_GLOBAL_QUEUE,\n            message_identifier=mediated_transfer.message_identifier,\n        )\n        events = [send_processed]\n    else:\n        assert msg, 'is_valid_lock_expired should return error msg if not valid'\n        invalid_locked = EventInvalidReceivedLockedTransfer(\n            payment_identifier=mediated_transfer.payment_identifier,\n            reason=msg,\n        )\n        events = [invalid_locked]\n\n    return is_valid, events, msg", "response": "Handles a received locked transfer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove and returns an item from the queue.", "response": "def get(self, block=True, timeout=None):\n        \"\"\" Removes and returns an item from the queue. \"\"\"\n        value = self._queue.get(block, timeout)\n        if self._queue.empty():\n            self.clear()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying the current queue items.", "response": "def copy(self):\n        \"\"\" Copies the current queue items. \"\"\"\n        copy = self._queue.copy()\n\n        result = list()\n        while not copy.empty():\n            result.append(copy.get_nowait())\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of routes that can be used to make a transfer.", "response": "def get_best_routes_internal(\n        chain_state: ChainState,\n        token_network_id: TokenNetworkID,\n        from_address: InitiatorAddress,\n        to_address: TargetAddress,\n        amount: int,\n        previous_address: Optional[Address],\n) -> List[RouteState]:\n    \"\"\" Returns a list of channels that can be used to make a transfer.\n\n    This will filter out channels that are not open and don't have enough\n    capacity.\n    \"\"\"\n    # TODO: Route ranking.\n    # Rate each route to optimize the fee price/quality of each route and add a\n    # rate from in the range [0.0,1.0].\n\n    available_routes = list()\n\n    token_network = views.get_token_network_by_identifier(\n        chain_state,\n        token_network_id,\n    )\n\n    if not token_network:\n        return list()\n\n    neighbors_heap: List[Neighbour] = list()\n    try:\n        all_neighbors = networkx.all_neighbors(token_network.network_graph.network, from_address)\n    except networkx.NetworkXError:\n        # If `our_address` is not in the graph, no channels opened with the\n        # address\n        return list()\n\n    for partner_address in all_neighbors:\n        # don't send the message backwards\n        if partner_address == previous_address:\n            continue\n\n        channel_state = views.get_channelstate_by_token_network_and_partner(\n            chain_state,\n            token_network_id,\n            partner_address,\n        )\n\n        if not channel_state:\n            continue\n\n        if channel.get_status(channel_state) != CHANNEL_STATE_OPENED:\n            log.info(\n                'Channel is not opened, ignoring',\n                from_address=pex(from_address),\n                partner_address=pex(partner_address),\n                routing_source='Internal Routing',\n            )\n            continue\n\n        nonrefundable = amount > channel.get_distributable(\n            channel_state.partner_state,\n            channel_state.our_state,\n        )\n\n        try:\n            length = networkx.shortest_path_length(\n                token_network.network_graph.network,\n                partner_address,\n                to_address,\n            )\n            neighbour = Neighbour(\n                length=length,\n                nonrefundable=nonrefundable,\n                partner_address=partner_address,\n                channelid=channel_state.identifier,\n            )\n            heappush(neighbors_heap, neighbour)\n        except (networkx.NetworkXNoPath, networkx.NodeNotFound):\n            pass\n\n    if not neighbors_heap:\n        log.warning(\n            'No routes available',\n            from_address=pex(from_address),\n            to_address=pex(to_address),\n        )\n        return list()\n\n    while neighbors_heap:\n        neighbour = heappop(neighbors_heap)\n        route_state = RouteState(\n            node_address=neighbour.partner_address,\n            channel_identifier=neighbour.channelid,\n        )\n        available_routes.append(route_state)\n    return available_routes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_joined_members(self, force_resync=False) -> List[User]:\n        if force_resync:\n            response = self.client.api.get_room_members(self.room_id)\n            for event in response['chunk']:\n                if event['content']['membership'] == 'join':\n                    user_id = event[\"state_key\"]\n                    if user_id not in self._members:\n                        self._mkmembers(\n                            User(\n                                self.client.api,\n                                user_id,\n                                event['content'].get('displayname'),\n                            ),\n                        )\n        return list(self._members.values())", "response": "Get a list of members of this room."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_aliases(self):\n        changed = False\n        try:\n            response = self.client.api.get_room_state(self.room_id)\n        except MatrixRequestError:\n            return False\n        for chunk in response:\n            content = chunk.get('content')\n            if content:\n                if 'aliases' in content:\n                    aliases = content['aliases']\n                    if aliases != self.aliases:\n                        self.aliases = aliases\n                        changed = True\n                if chunk.get('type') == 'm.room.canonical_alias':\n                    canonical_alias = content['alias']\n                    if self.canonical_alias != canonical_alias:\n                        self.canonical_alias = canonical_alias\n                        changed = True\n        if changed and self.aliases and not self.canonical_alias:\n            self.canonical_alias = self.aliases[0]\n        return changed", "response": "Update the aliases of the room."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nkeeps listening for events forever.", "response": "def listen_forever(\n            self,\n            timeout_ms: int = 30000,\n            exception_handler: Callable[[Exception], None] = None,\n            bad_sync_timeout: int = 5,\n    ):\n        \"\"\"\n        Keep listening for events forever.\n        Args:\n            timeout_ms: How long to poll the Home Server for before retrying.\n            exception_handler: Optional exception handler function which can\n                be used to handle exceptions in the caller thread.\n            bad_sync_timeout: Base time to wait after an error before retrying.\n                Will be increased according to exponential backoff.\n        \"\"\"\n        _bad_sync_timeout = bad_sync_timeout\n        self.should_listen = True\n        while self.should_listen:\n            try:\n                # may be killed and raise exception from _handle_thread\n                self._sync(timeout_ms)\n                _bad_sync_timeout = bad_sync_timeout\n            except MatrixRequestError as e:\n                log.warning('A MatrixRequestError occured during sync.')\n                if e.code >= 500:\n                    log.warning(\n                        'Problem occured serverside. Waiting',\n                        wait_for=_bad_sync_timeout,\n                    )\n                    gevent.sleep(_bad_sync_timeout)\n                    _bad_sync_timeout = min(_bad_sync_timeout * 2, self.bad_sync_timeout_limit)\n                else:\n                    raise\n            except MatrixHttpLibError:\n                log.exception('A MatrixHttpLibError occured during sync.')\n                if self.should_listen:\n                    gevent.sleep(_bad_sync_timeout)\n                    _bad_sync_timeout = min(_bad_sync_timeout * 2, self.bad_sync_timeout_limit)\n            except Exception as e:\n                log.exception('Exception thrown during sync')\n                if exception_handler is not None:\n                    exception_handler(e)\n                else:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart a listener thread that will listen for events in the background.", "response": "def start_listener_thread(self, timeout_ms: int = 30000, exception_handler: Callable = None):\n        \"\"\"\n        Start a listener greenlet to listen for events in the background.\n        Args:\n            timeout_ms: How long to poll the Home Server for before retrying.\n            exception_handler: Optional exception handler function which can\n                be used to handle exceptions in the caller thread.\n        \"\"\"\n        assert not self.should_listen and self.sync_thread is None, 'Already running'\n        self.should_listen = True\n        self.sync_thread = gevent.spawn(self.listen_forever, timeout_ms, exception_handler)\n        self.sync_thread.name = f'GMatrixClient.listen_forever user_id:{self.user_id}'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop_listener_thread(self):\n        # when stopping, `kill` will cause the `self.api.sync` call in _sync\n        # to raise a connection error. This flag will ensure it exits gracefully then\n        self.should_listen = False\n        if self.sync_thread:\n            self.sync_thread.kill()\n            self.sync_thread.get()\n        if self._handle_thread is not None:\n            self._handle_thread.get()\n        self.sync_thread = None\n        self._handle_thread = None", "response": "Stops the sync_thread greenlet before joining it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_user_directory(self, term: str) -> List[User]:\n        response = self.api._send(\n            'POST',\n            '/user_directory/search',\n            {\n                'search_term': term,\n            },\n        )\n        try:\n            return [\n                User(self.api, _user['user_id'], _user['display_name'])\n                for _user in response['results']\n            ]\n        except KeyError:\n            return []", "response": "Search user directory for a given term returning a list of users"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef typing(self, room: Room, timeout: int = 5000):\n        path = f'/rooms/{quote(room.room_id)}/typing/{quote(self.user_id)}'\n        return self.api._send('PUT', path, {'typing': True, 'timeout': timeout})", "response": "Send typing event directly to API"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mkroom(self, room_id: str) -> Room:\n        if room_id not in self.rooms:\n            self.rooms[room_id] = Room(self, room_id)\n        room = self.rooms[room_id]\n        if not room.canonical_alias:\n            room.update_aliases()\n        return room", "response": "Creates a new room with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_sync_limit(self, limit: int) -> Optional[int]:\n        try:\n            prev_limit = json.loads(self.sync_filter)['room']['timeline']['limit']\n        except (json.JSONDecodeError, KeyError):\n            prev_limit = None\n        self.sync_filter = json.dumps({'room': {'timeline': {'limit': limit}}})\n        return prev_limit", "response": "Sets the limit per room for sync and returns previous limit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert values that are of type HexBytes to strings.", "response": "def hexbytes_to_str(map_: Dict):\n    \"\"\" Converts values that are of type `HexBytes` to strings. \"\"\"\n    for k, v in map_.items():\n        if isinstance(v, HexBytes):\n            map_[k] = encode_hex(v)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_byte_values(map_: Dict):\n    for k, v in map_.items():\n        if isinstance(v, bytes):\n            map_[k] = encode_hex(v)", "response": "Converts values that are of type bytes to strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize_events_list(old_list):\n    new_list = []\n    for _event in old_list:\n        new_event = dict(_event)\n        if new_event.get('args'):\n            new_event['args'] = dict(new_event['args'])\n            encode_byte_values(new_event['args'])\n        # remove the queue identifier\n        if new_event.get('queue_identifier'):\n            del new_event['queue_identifier']\n        # the events contain HexBytes values, convert those to strings\n        hexbytes_to_str(new_event)\n        # Some of the raiden events contain accounts and as such need to\n        # be exported in hex to the outside world\n        name = new_event['event']\n        if name == 'EventPaymentReceivedSuccess':\n            new_event['initiator'] = to_checksum_address(new_event['initiator'])\n        if name in ('EventPaymentSentSuccess', 'EventPaymentSentFailed'):\n            new_event['target'] = to_checksum_address(new_event['target'])\n        encode_byte_values(new_event)\n        # encode unserializable objects\n        encode_object_to_str(new_event)\n        new_list.append(new_event)\n    return new_list", "response": "Normalizes the list of events to be a list of APIVersion related objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflask errorhandler when an exception was not correctly handled", "response": "def unhandled_exception(self, exception: Exception):\n        \"\"\" Flask.errorhandler when an exception wasn't correctly handled \"\"\"\n        log.critical(\n            'Unhandled exception when processing endpoint request',\n            exc_info=True,\n            node=pex(self.rest_api.raiden_api.address),\n        )\n        self.greenlet.kill(exception)\n        return api_error([str(exception)], HTTPStatus.INTERNAL_SERVER_ERROR)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a dict of all connection managers for a given token network", "response": "def get_connection_managers_info(self, registry_address: typing.PaymentNetworkID):\n        \"\"\"Get a dict whose keys are token addresses and whose values are\n        open channels, funds of last request, sum of deposits and number of channels\"\"\"\n        log.debug(\n            'Getting connection managers info',\n            node=pex(self.raiden_api.address),\n            registry_address=to_checksum_address(registry_address),\n        )\n        connection_managers = dict()\n\n        for token in self.raiden_api.get_tokens_list(registry_address):\n            token_network_identifier = views.get_token_network_identifier_by_token_address(\n                views.state_from_raiden(self.raiden_api.raiden),\n                payment_network_id=registry_address,\n                token_address=token,\n            )\n\n            try:\n                connection_manager = self.raiden_api.raiden.connection_manager_for_token_network(\n                    token_network_identifier,\n                )\n            except InvalidAddress:\n                connection_manager = None\n\n            open_channels = views.get_channelstate_open(\n                chain_state=views.state_from_raiden(self.raiden_api.raiden),\n                payment_network_id=registry_address,\n                token_address=token,\n            )\n            if connection_manager is not None and open_channels:\n                connection_managers[to_checksum_address(connection_manager.token_address)] = {\n                    'funds': connection_manager.funds,\n                    'sum_deposits': views.get_our_capacity_for_token_network(\n                        views.state_from_raiden(self.raiden_api.raiden),\n                        registry_address,\n                        token,\n                    ),\n                    'channels': len(open_channels),\n                }\n\n        return connection_managers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the network id and returns the network id and whether it is a known network.", "response": "def setup_network_id_or_exit(\n        config: Dict[str, Any],\n        given_network_id: int,\n        web3: Web3,\n) -> Tuple[int, bool]:\n    \"\"\"\n    Takes the given network id and checks it against the connected network\n\n    If they don't match, exits the program with an error. If they do adds it\n    to the configuration and then returns it and whether it is a known network\n    \"\"\"\n    node_network_id = int(web3.version.network)  # pylint: disable=no-member\n    known_given_network_id = given_network_id in ID_TO_NETWORKNAME\n    known_node_network_id = node_network_id in ID_TO_NETWORKNAME\n\n    if node_network_id != given_network_id:\n        if known_given_network_id and known_node_network_id:\n            click.secho(\n                f\"The chosen ethereum network '{ID_TO_NETWORKNAME[given_network_id]}' \"\n                f\"differs from the ethereum client '{ID_TO_NETWORKNAME[node_network_id]}'. \"\n                \"Please update your settings.\",\n                fg='red',\n            )\n        else:\n            click.secho(\n                f\"The chosen ethereum network id '{given_network_id}' differs \"\n                f\"from the ethereum client '{node_network_id}'. \"\n                \"Please update your settings.\",\n                fg='red',\n            )\n        sys.exit(1)\n\n    config['chain_id'] = given_network_id\n    return given_network_id, known_node_network_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_environment(config: Dict[str, Any], environment_type: Environment) -> None:\n    # interpret the provided string argument\n    if environment_type == Environment.PRODUCTION:\n        # Safe configuration: restrictions for mainnet apply and matrix rooms have to be private\n        config['transport']['matrix']['private_rooms'] = True\n\n    config['environment_type'] = environment_type\n\n    print(f'Raiden is running in {environment_type.value.lower()} mode')", "response": "Sets the config depending on the environment type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the contracts deployment data depending on the network id and environment type.", "response": "def setup_contracts_or_exit(\n        config: Dict[str, Any],\n        network_id: int,\n) -> Dict[str, Any]:\n    \"\"\"Sets the contract deployment data depending on the network id and environment type\n\n    If an invalid combination of network id and environment type is provided, exits\n    the program with an error\n    \"\"\"\n    environment_type = config['environment_type']\n\n    not_allowed = (  # for now we only disallow mainnet with test configuration\n        network_id == 1 and\n        environment_type == Environment.DEVELOPMENT\n    )\n    if not_allowed:\n        click.secho(\n            f'The chosen network ({ID_TO_NETWORKNAME[network_id]}) is not a testnet, '\n            f'but the \"development\" environment was selected.\\n'\n            f'This is not allowed. Please start again with a safe environment setting '\n            f'(--environment production).',\n            fg='red',\n        )\n        sys.exit(1)\n\n    contracts = dict()\n    contracts_version = environment_type_to_contracts_version(environment_type)\n\n    config['contracts_path'] = contracts_precompiled_path(contracts_version)\n\n    if network_id in ID_TO_NETWORKNAME and ID_TO_NETWORKNAME[network_id] != 'smoketest':\n        try:\n            deployment_data = get_contracts_deployment_info(\n                chain_id=network_id,\n                version=contracts_version,\n            )\n        except ValueError:\n            return contracts, False\n\n        contracts = deployment_data['contracts']\n\n    return contracts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_proxies_or_exit(\n        config: Dict[str, Any],\n        tokennetwork_registry_contract_address: str,\n        secret_registry_contract_address: str,\n        endpoint_registry_contract_address: str,\n        user_deposit_contract_address: str,\n        service_registry_contract_address: str,\n        blockchain_service: BlockChainService,\n        contracts: Dict[str, Any],\n        routing_mode: RoutingMode,\n        pathfinding_service_address: str,\n        pathfinding_eth_address: str,\n) -> Proxies:\n    \"\"\"\n    Initialize and setup the contract proxies.\n\n    Depending on the provided contract addresses via the CLI, the routing mode,\n    the environment type and the network id try to initialize the proxies.\n    Returns the initialized proxies or exits the application with an error if\n    there is a problem.\n\n    Also depending on the given arguments populate config with PFS related settings\n    \"\"\"\n    node_network_id = config['chain_id']\n    environment_type = config['environment_type']\n    contract_addresses_given = (\n        tokennetwork_registry_contract_address is not None and\n        secret_registry_contract_address is not None and\n        endpoint_registry_contract_address is not None\n    )\n\n    if not contract_addresses_given and not bool(contracts):\n        click.secho(\n            f\"There are no known contract addresses for network id '{node_network_id}'. and \"\n            f\"environment type {environment_type}. Please provide them on the command line or \"\n            f\"in the configuration file.\",\n            fg='red',\n        )\n        sys.exit(1)\n\n    try:\n        token_network_registry = blockchain_service.token_network_registry(\n            tokennetwork_registry_contract_address or to_canonical_address(\n                contracts[CONTRACT_TOKEN_NETWORK_REGISTRY]['address'],\n            ),\n        )\n    except ContractVersionMismatch as e:\n        handle_contract_version_mismatch(e)\n    except AddressWithoutCode:\n        handle_contract_no_code('token network registry', tokennetwork_registry_contract_address)\n    except AddressWrongContract:\n        handle_contract_wrong_address(\n            'token network registry',\n            tokennetwork_registry_contract_address,\n        )\n\n    try:\n        secret_registry = blockchain_service.secret_registry(\n            secret_registry_contract_address or to_canonical_address(\n                contracts[CONTRACT_SECRET_REGISTRY]['address'],\n            ),\n        )\n    except ContractVersionMismatch as e:\n        handle_contract_version_mismatch(e)\n    except AddressWithoutCode:\n        handle_contract_no_code('secret registry', secret_registry_contract_address)\n    except AddressWrongContract:\n        handle_contract_wrong_address('secret registry', secret_registry_contract_address)\n\n    # If services contracts are provided via the CLI use them instead\n    if user_deposit_contract_address is not None:\n        contracts[CONTRACT_USER_DEPOSIT] = user_deposit_contract_address\n    if service_registry_contract_address is not None:\n        contracts[CONTRACT_SERVICE_REGISTRY] = (\n            service_registry_contract_address\n        )\n\n    user_deposit = None\n    should_use_user_deposit = (\n        environment_type == Environment.DEVELOPMENT and\n        ID_TO_NETWORKNAME.get(node_network_id) != 'smoketest' and\n        CONTRACT_USER_DEPOSIT in contracts\n    )\n    if should_use_user_deposit:\n        try:\n            user_deposit = blockchain_service.user_deposit(\n                user_deposit_contract_address or to_canonical_address(\n                    contracts[CONTRACT_USER_DEPOSIT]['address'],\n                ),\n            )\n        except ContractVersionMismatch as e:\n            handle_contract_version_mismatch(e)\n        except AddressWithoutCode:\n            handle_contract_no_code('user deposit', user_deposit_contract_address)\n        except AddressWrongContract:\n            handle_contract_wrong_address('user_deposit', user_deposit_contract_address)\n\n    service_registry = None\n    if CONTRACT_SERVICE_REGISTRY in contracts or service_registry_contract_address:\n        try:\n            service_registry = blockchain_service.service_registry(\n                service_registry_contract_address or to_canonical_address(\n                    contracts[CONTRACT_SERVICE_REGISTRY]['address'],\n                ),\n            )\n        except ContractVersionMismatch as e:\n            handle_contract_version_mismatch(e)\n        except AddressWithoutCode:\n            handle_contract_no_code('service registry', service_registry_contract_address)\n        except AddressWrongContract:\n            handle_contract_wrong_address('secret registry', service_registry_contract_address)\n\n    if routing_mode == RoutingMode.PFS:\n        if environment_type == Environment.PRODUCTION:\n            click.secho(\n                'Requested production mode and PFS routing mode. This is not supported',\n                fg='red',\n            )\n            sys.exit(1)\n\n        if not service_registry and not pathfinding_service_address:\n            click.secho(\n                'Requested PFS routing mode but no service registry or no specific pathfinding '\n                ' service address is provided. Please provide it via either the '\n                '--service-registry-contract-address or the --pathfinding-service-address '\n                'argument',\n                fg='red',\n            )\n            sys.exit(1)\n\n        pfs_config = configure_pfs_or_exit(\n            pfs_address=pathfinding_service_address,\n            pfs_eth_address=pathfinding_eth_address,\n            routing_mode=routing_mode,\n            service_registry=service_registry,\n        )\n        msg = 'Eth address of selected pathfinding service is unknown.'\n        assert pfs_config.eth_address is not None, msg\n        config['services']['pathfinding_service_address'] = pfs_config.url\n        config['services']['pathfinding_eth_address'] = pfs_config.eth_address\n        config['services']['pathfinding_fee'] = pfs_config.fee\n    else:\n        config['services']['pathfinding_service_address'] = None\n        config['services']['pathfinding_eth_address'] = None\n\n    proxies = Proxies(\n        token_network_registry=token_network_registry,\n        secret_registry=secret_registry,\n        user_deposit=user_deposit,\n        service_registry=service_registry,\n    )\n    return proxies", "response": "Initialize and setup the proxies for the given token network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndispatching the state change in the current state machine and returns the resulting events.", "response": "def dispatch(self, state_change: StateChange) -> List[Event]:\n        \"\"\" Apply the `state_change` in the current machine and return the\n        resulting events.\n\n        Args:\n            state_change: An object representation of a state\n            change.\n\n        Return:\n            A list of events produced by the state transition.\n            It's the upper layer's responsibility to decided how to handle\n            these events.\n        \"\"\"\n        assert isinstance(state_change, StateChange)\n\n        # the state objects must be treated as immutable, so make a copy of the\n        # current state and pass the copy to the state machine to be modified.\n        next_state = deepcopy(self.current_state)\n\n        # update the current state by applying the change\n        iteration = self.state_transition(\n            next_state,\n            state_change,\n        )\n\n        assert isinstance(iteration, TransitionResult)\n\n        self.current_state = iteration.new_state\n        events = iteration.events\n\n        assert isinstance(self.current_state, (State, type(None)))\n        assert all(isinstance(e, Event) for e in events)\n\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_for_newchannel(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        partner_address: Address,\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until the channel with partner_address is registered.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    channel_state = views.get_channelstate_for(\n        views.state_from_raiden(raiden),\n        payment_network_id,\n        token_address,\n        partner_address,\n    )\n\n    while channel_state is None:\n        gevent.sleep(retry_timeout)\n        channel_state = views.get_channelstate_for(\n            views.state_from_raiden(raiden),\n            payment_network_id,\n            token_address,\n            partner_address,\n        )", "response": "Wait until a new channel with partner_address is registered."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_for_participant_newbalance(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        partner_address: Address,\n        target_address: Address,\n        target_balance: TokenAmount,\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until a given channels balance exceeds the target balance.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    if target_address == raiden.address:\n        balance = lambda channel_state: channel_state.our_state.contract_balance\n    elif target_address == partner_address:\n        balance = lambda channel_state: channel_state.partner_state.contract_balance\n    else:\n        raise ValueError('target_address must be one of the channel participants')\n\n    channel_state = views.get_channelstate_for(\n        views.state_from_raiden(raiden),\n        payment_network_id,\n        token_address,\n        partner_address,\n    )\n\n    while balance(channel_state) < target_balance:\n        gevent.sleep(retry_timeout)\n        channel_state = views.get_channelstate_for(\n            views.state_from_raiden(raiden),\n            payment_network_id,\n            token_address,\n            partner_address,\n        )", "response": "Wait until a given channel balance exceeds the target balance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for_payment_balance(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        partner_address: Address,\n        target_address: Address,\n        target_balance: TokenAmount,\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until a given channel's balance exceeds the target balance.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    def get_balance(end_state):\n        if end_state.balance_proof:\n            return end_state.balance_proof.transferred_amount\n        else:\n            return 0\n\n    if target_address == raiden.address:\n        balance = lambda channel_state: get_balance(channel_state.partner_state)\n    elif target_address == partner_address:\n        balance = lambda channel_state: get_balance(channel_state.our_state)\n    else:\n        raise ValueError('target_address must be one of the channel participants')\n\n    channel_state = views.get_channelstate_for(\n        views.state_from_raiden(raiden),\n        payment_network_id,\n        token_address,\n        partner_address,\n    )\n\n    while balance(channel_state) < target_balance:\n        log.critical('wait', b=balance(channel_state), t=target_balance)\n        gevent.sleep(retry_timeout)\n        channel_state = views.get_channelstate_for(\n            views.state_from_raiden(raiden),\n            payment_network_id,\n            token_address,\n            partner_address,\n        )", "response": "Wait until a given channel s balance exceeds the target balance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits until all channels are in target_states.", "response": "def wait_for_channel_in_states(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        channel_ids: List[ChannelID],\n        retry_timeout: float,\n        target_states: Sequence[str],\n) -> None:\n    \"\"\"Wait until all channels are in `target_states`.\n\n    Raises:\n        ValueError: If the token_address is not registered in the\n            payment_network.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    chain_state = views.state_from_raiden(raiden)\n    token_network = views.get_token_network_by_token_address(\n        chain_state=chain_state,\n        payment_network_id=payment_network_id,\n        token_address=token_address,\n    )\n\n    if token_network is None:\n        raise ValueError(\n            f'The token {token_address} is not registered on the network {payment_network_id}.',\n        )\n\n    token_network_address = token_network.address\n\n    list_cannonical_ids = [\n        CanonicalIdentifier(\n            chain_identifier=chain_state.chain_id,\n            token_network_address=token_network_address,\n            channel_identifier=channel_identifier,\n        )\n        for channel_identifier in channel_ids\n    ]\n\n    while list_cannonical_ids:\n        canonical_id = list_cannonical_ids[-1]\n        chain_state = views.state_from_raiden(raiden)\n\n        channel_state = views.get_channelstate_by_canonical_identifier(\n            chain_state=chain_state,\n            canonical_identifier=canonical_id,\n        )\n\n        channel_is_settled = (\n            channel_state is None or\n            channel.get_status(channel_state) in target_states\n        )\n\n        if channel_is_settled:\n            list_cannonical_ids.pop()\n        else:\n            gevent.sleep(retry_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_for_close(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        channel_ids: List[ChannelID],\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until all channels are closed.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    return wait_for_channel_in_states(\n        raiden=raiden,\n        payment_network_id=payment_network_id,\n        token_address=token_address,\n        channel_ids=channel_ids,\n        retry_timeout=retry_timeout,\n        target_states=CHANNEL_AFTER_CLOSE_STATES,\n    )", "response": "Wait until all channels are closed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwaiting until all channels are settled.", "response": "def wait_for_settle(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        channel_ids: List[ChannelID],\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until all channels are settled.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    return wait_for_channel_in_states(\n        raiden=raiden,\n        payment_network_id=payment_network_id,\n        token_address=token_address,\n        channel_ids=channel_ids,\n        retry_timeout=retry_timeout,\n        target_states=(CHANNEL_STATE_SETTLED,),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_settle_all_channels(\n        raiden: 'RaidenService',\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until all channels are settled.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    chain_state = views.state_from_raiden(raiden)\n\n    id_paymentnetworkstate = chain_state.identifiers_to_paymentnetworks.items()\n    for payment_network_id, payment_network_state in id_paymentnetworkstate:\n\n        id_tokennetworkstate = payment_network_state.tokenidentifiers_to_tokennetworks.items()\n        for token_network_id, token_network_state in id_tokennetworkstate:\n            channel_ids = cast(\n                List[ChannelID], token_network_state.channelidentifiers_to_channels.keys(),\n            )\n\n            wait_for_settle(\n                raiden=raiden,\n                payment_network_id=payment_network_id,\n                token_address=TokenAddress(token_network_id),\n                channel_ids=channel_ids,\n                retry_timeout=retry_timeout,\n            )", "response": "Wait until all channels are settled."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_healthy(\n        raiden: 'RaidenService',\n        node_address: Address,\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until `node_address` becomes healthy.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    network_statuses = views.get_networkstatuses(\n        views.state_from_raiden(raiden),\n    )\n\n    while network_statuses.get(node_address) != NODE_NETWORK_REACHABLE:\n        gevent.sleep(retry_timeout)\n        network_statuses = views.get_networkstatuses(\n            views.state_from_raiden(raiden),\n        )", "response": "Wait until the node_address is healthy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting until a transfer with a specific identifier and amount is seen in the WAL.", "response": "def wait_for_transfer_success(\n        raiden: 'RaidenService',\n        payment_identifier: PaymentID,\n        amount: PaymentAmount,\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until a transfer with a specific identifier and amount\n    is seen in the WAL.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    found = False\n    while not found:\n        state_events = raiden.wal.storage.get_events()\n        for event in state_events:\n            found = (\n                isinstance(event, EventPaymentReceivedSuccess) and\n                event.identifier == payment_identifier and\n                event.amount == amount\n            )\n            if found:\n                break\n\n        gevent.sleep(retry_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef echo_node_alarm_callback(self, block_number):\n        if not self.ready.is_set():\n            self.ready.set()\n        log.debug('echo_node callback', block_number=block_number)\n        if self.stop_signal is not None:\n            return REMOVE_CALLBACK\n        else:\n            self.greenlets.add(gevent.spawn(self.poll_all_received_events))\n            return True", "response": "This is the callback function that is called when the node is ready to be used."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef echo_worker(self):\n        log.debug('echo worker', qsize=self.received_transfers.qsize())\n        while self.stop_signal is None:\n            if self.received_transfers.qsize() > 0:\n                transfer = self.received_transfers.get()\n                if transfer in self.seen_transfers:\n                    log.debug(\n                        'duplicate transfer ignored',\n                        initiator=pex(transfer.initiator),\n                        amount=transfer.amount,\n                        identifier=transfer.identifier,\n                    )\n                else:\n                    self.seen_transfers.append(transfer)\n                    self.greenlets.add(gevent.spawn(self.on_transfer, transfer))\n            else:\n                gevent.sleep(.5)", "response": "The echo_worker method is a thread that runs in a thread that will process all not - yet - seen transfers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        if self.greenlet:\n            raise RuntimeError(f'Greenlet {self.greenlet!r} already started')\n        pristine = (\n            not self.greenlet.dead and\n            tuple(self.greenlet.args) == tuple(self.args) and\n            self.greenlet.kwargs == self.kwargs\n        )\n        if not pristine:\n            self.greenlet = Greenlet(self._run, *self.args, **self.kwargs)\n            self.greenlet.name = f'{self.__class__.__name__}|{self.greenlet.name}'\n        self.greenlet.start()", "response": "Synchronously start the task with the given arguments and keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_error(self, subtask: Greenlet):\n        log.error(\n            'Runnable subtask died!',\n            this=self,\n            running=bool(self),\n            subtask=subtask,\n            exc=subtask.exception,\n        )\n        if not self.greenlet:\n            return\n        self.greenlet.kill(subtask.exception)", "response": "Default callback for substasks link_exception\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join_global_room(client: GMatrixClient, name: str, servers: Sequence[str] = ()) -> Room:\n    our_server_name = urlparse(client.api.base_url).netloc\n    assert our_server_name, 'Invalid client\\'s homeserver url'\n    servers = [our_server_name] + [  # client's own server first\n        urlparse(s).netloc\n        for s in servers\n        if urlparse(s).netloc not in {None, '', our_server_name}\n    ]\n\n    our_server_global_room_alias_full = f'#{name}:{servers[0]}'\n\n    # try joining a global room on any of the available servers, starting with ours\n    for server in servers:\n        global_room_alias_full = f'#{name}:{server}'\n        try:\n            global_room = client.join_room(global_room_alias_full)\n        except MatrixRequestError as ex:\n            if ex.code not in (403, 404, 500):\n                raise\n            log.debug(\n                'Could not join global room',\n                room_alias_full=global_room_alias_full,\n                _exception=ex,\n            )\n        else:\n            if our_server_global_room_alias_full not in global_room.aliases:\n                # we managed to join a global room, but it's not aliased in our server\n                global_room.add_room_alias(our_server_global_room_alias_full)\n                global_room.aliases.append(our_server_global_room_alias_full)\n            break\n    else:\n        log.debug('Could not join any global room, trying to create one')\n        for _ in range(JOIN_RETRIES):\n            try:\n                global_room = client.create_room(name, is_public=True)\n            except MatrixRequestError as ex:\n                if ex.code not in (400, 409):\n                    raise\n                try:\n                    global_room = client.join_room(\n                        our_server_global_room_alias_full,\n                    )\n                except MatrixRequestError as ex:\n                    if ex.code not in (404, 403):\n                        raise\n                else:\n                    break\n            else:\n                break\n        else:\n            raise TransportError('Could neither join nor create a global room')\n\n    return global_room", "response": "Join or create a global public room with given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef login_or_register(\n        client: GMatrixClient,\n        signer: Signer,\n        prev_user_id: str = None,\n        prev_access_token: str = None,\n) -> User:\n    \"\"\"Login to a Raiden matrix server with password and displayname proof-of-keys\n\n    - Username is in the format: 0x<eth_address>(.<suffix>)?, where the suffix is not required,\n    but a deterministic (per-account) random 8-hex string to prevent DoS by other users registering\n    our address\n    - Password is the signature of the server hostname, verified by the server to prevent account\n    creation spam\n    - Displayname currently is the signature of the whole user_id (including homeserver), to be\n    verified by other peers. May include in the future other metadata such as protocol version\n\n    Params:\n        client: GMatrixClient instance configured with desired homeserver\n        signer: raiden.utils.signer.Signer instance for signing password and displayname\n        prev_user_id: (optional) previous persisted client.user_id. Must match signer's account\n        prev_access_token: (optional) previous persistend client.access_token for prev_user_id\n    Returns:\n        Own matrix_client.User\n    \"\"\"\n    server_url = client.api.base_url\n    server_name = urlparse(server_url).netloc\n\n    base_username = to_normalized_address(signer.address)\n    _match_user = re.match(\n        f'^@{re.escape(base_username)}.*:{re.escape(server_name)}$',\n        prev_user_id or '',\n    )\n    if _match_user:  # same user as before\n        log.debug('Trying previous user login', user_id=prev_user_id)\n        client.set_access_token(user_id=prev_user_id, token=prev_access_token)\n\n        try:\n            client.api.get_devices()\n        except MatrixRequestError as ex:\n            log.debug(\n                'Couldn\\'t use previous login credentials, discarding',\n                prev_user_id=prev_user_id,\n                _exception=ex,\n            )\n        else:\n            prev_sync_limit = client.set_sync_limit(0)\n            client._sync()  # initial_sync\n            client.set_sync_limit(prev_sync_limit)\n            log.debug('Success. Valid previous credentials', user_id=prev_user_id)\n            return client.get_user(client.user_id)\n    elif prev_user_id:\n        log.debug(\n            'Different server or account, discarding',\n            prev_user_id=prev_user_id,\n            current_address=base_username,\n            current_server=server_name,\n        )\n\n    # password is signed server address\n    password = encode_hex(signer.sign(server_name.encode()))\n    rand = None\n    # try login and register on first 5 possible accounts\n    for i in range(JOIN_RETRIES):\n        username = base_username\n        if i:\n            if not rand:\n                rand = Random()  # deterministic, random secret for username suffixes\n                # initialize rand for seed (which requires a signature) only if/when needed\n                rand.seed(int.from_bytes(signer.sign(b'seed')[-32:], 'big'))\n            username = f'{username}.{rand.randint(0, 0xffffffff):08x}'\n\n        try:\n            client.login(username, password, sync=False)\n            prev_sync_limit = client.set_sync_limit(0)\n            client._sync()  # when logging, do initial_sync with limit=0\n            client.set_sync_limit(prev_sync_limit)\n            log.debug(\n                'Login',\n                homeserver=server_name,\n                server_url=server_url,\n                username=username,\n            )\n            break\n        except MatrixRequestError as ex:\n            if ex.code != 403:\n                raise\n            log.debug(\n                'Could not login. Trying register',\n                homeserver=server_name,\n                server_url=server_url,\n                username=username,\n            )\n            try:\n                client.register_with_password(username, password)\n                log.debug(\n                    'Register',\n                    homeserver=server_name,\n                    server_url=server_url,\n                    username=username,\n                )\n                break\n            except MatrixRequestError as ex:\n                if ex.code != 400:\n                    raise\n                log.debug('Username taken. Continuing')\n                continue\n    else:\n        raise ValueError('Could not register or login!')\n\n    name = encode_hex(signer.sign(client.user_id.encode()))\n    user = client.get_user(client.user_id)\n    user.set_display_name(name)\n    return user", "response": "Login to a Raiden matrix server with password and displayname proof - of - keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates a userId format and signature on displayName and return its address", "response": "def validate_userid_signature(user: User) -> Optional[Address]:\n    \"\"\" Validate a userId format and signature on displayName, and return its address\"\"\"\n    # display_name should be an address in the USERID_RE format\n    match = USERID_RE.match(user.user_id)\n    if not match:\n        return None\n\n    encoded_address = match.group(1)\n    address: Address = to_canonical_address(encoded_address)\n\n    try:\n        displayname = user.get_display_name()\n        recovered = recover(\n            data=user.user_id.encode(),\n            signature=decode_hex(displayname),\n        )\n        if not (address and recovered and recovered == address):\n            return None\n    except (\n            DecodeError,\n            TypeError,\n            InvalidSignature,\n            MatrixRequestError,\n            json.decoder.JSONDecodeError,\n    ):\n        return None\n    return address"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort_servers_closest(servers: Sequence[str]) -> Sequence[Tuple[str, float]]:\n    if not {urlparse(url).scheme for url in servers}.issubset({'http', 'https'}):\n        raise TransportError('Invalid server urls')\n\n    get_rtt_jobs = set(\n        gevent.spawn(lambda url: (url, get_http_rtt(url)), server_url)\n        for server_url\n        in servers\n    )\n    # these tasks should never raise, returns None on errors\n    gevent.joinall(get_rtt_jobs, raise_error=False)  # block and wait tasks\n    sorted_servers: List[Tuple[str, float]] = sorted(\n        (job.value for job in get_rtt_jobs if job.value[1] is not None),\n        key=itemgetter(1),\n    )\n    log.debug('Matrix homeserver RTT times', rtt_times=sorted_servers)\n    return sorted_servers", "response": "Sorts a list of servers by http round - trip time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a list of possible servers chooses the closest available and creates a GMatrixClient instance for that server.", "response": "def make_client(servers: Sequence[str], *args, **kwargs) -> GMatrixClient:\n    \"\"\"Given a list of possible servers, chooses the closest available and create a GMatrixClient\n\n    Params:\n        servers: list of servers urls, with scheme (http or https)\n        Rest of args and kwargs are forwarded to GMatrixClient constructor\n    Returns:\n        GMatrixClient instance for one of the available servers\n    \"\"\"\n    if len(servers) > 1:\n        sorted_servers = [\n            server_url\n            for (server_url, _) in sort_servers_closest(servers)\n        ]\n        log.info(\n            'Automatically selecting matrix homeserver based on RTT',\n            sorted_servers=sorted_servers,\n        )\n    elif len(servers) == 1:\n        sorted_servers = servers\n    else:\n        raise TransportError('No valid servers list given')\n\n    last_ex = None\n    for server_url in sorted_servers:\n        server_url: str = server_url\n        client = GMatrixClient(server_url, *args, **kwargs)\n        try:\n            client.api._send('GET', '/versions', api_path='/_matrix/client')\n        except MatrixError as ex:\n            log.warning('Selected server not usable', server_url=server_url, _exception=ex)\n            last_ex = ex\n        else:\n            break\n    else:\n        raise TransportError(\n            'Unable to find a reachable Matrix server. Please check your network connectivity.',\n        ) from last_ex\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_room_alias(chain_id: ChainID, *suffixes: str) -> str:\n    network_name = ID_TO_NETWORKNAME.get(chain_id, str(chain_id))\n    return ROOM_NAME_SEPARATOR.join([ROOM_NAME_PREFIX, network_name, *suffixes])", "response": "Given a chain id and any number of suffixes compose and return the canonical room name for that chain."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a user_id for the given address.", "response": "def add_userid_for_address(self, address: Address, user_id: str):\n        \"\"\" Add a ``user_id`` for the given ``address``.\n\n        Implicitly adds the address if it was unknown before.\n        \"\"\"\n        self._address_to_userids[address].add(user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds multiple user_ids for the given address.", "response": "def add_userids_for_address(self, address: Address, user_ids: Iterable[str]):\n        \"\"\" Add multiple ``user_ids`` for the given ``address``.\n\n        Implicitly adds any addresses if they were unknown before.\n        \"\"\"\n        self._address_to_userids[address].update(user_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all known user ids for the given address.", "response": "def get_userids_for_address(self, address: Address) -> Set[str]:\n        \"\"\" Return all known user ids for the given ``address``. \"\"\"\n        if not self.is_address_known(address):\n            return set()\n        return self._address_to_userids[address]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the current presence state of the user_id.", "response": "def get_userid_presence(self, user_id: str) -> UserPresence:\n        \"\"\" Return the current presence state of ``user_id``. \"\"\"\n        return self._userid_to_presence.get(user_id, UserPresence.UNKNOWN)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_address_reachability(self, address: Address) -> AddressReachability:\n        return self._address_to_reachability.get(address, AddressReachability.UNKNOWN)", "response": "Return the current reachability state for the given address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrefresh synthesized user presence state for a given address.", "response": "def refresh_address_presence(self, address):\n        \"\"\"\n        Update synthesized address presence state from cached user presence states.\n\n        Triggers callback (if any) in case the state has changed.\n\n        This method is only provided to cover an edge case in our use of the Matrix protocol and\n        should **not** generally be used.\n        \"\"\"\n        composite_presence = {\n            self._fetch_user_presence(uid)\n            for uid\n            in self._address_to_userids[address]\n        }\n\n        # Iterate over UserPresence in definition order (most to least online) and pick\n        # first matching state\n        new_presence = UserPresence.UNKNOWN\n        for presence in UserPresence.__members__.values():\n            if presence in composite_presence:\n                new_presence = presence\n                break\n\n        new_address_reachability = USER_PRESENCE_TO_ADDRESS_REACHABILITY[new_presence]\n\n        if new_address_reachability == self._address_to_reachability.get(address):\n            # Cached address reachability matches new state, do nothing\n            return\n        log.debug(\n            'Changing address presence state',\n            current_user=self._user_id,\n            address=to_normalized_address(address),\n            prev_state=self._address_to_reachability.get(address),\n            state=new_address_reachability,\n        )\n        self._address_to_reachability[address] = new_address_reachability\n        self._address_reachability_changed_callback(address, new_address_reachability)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _presence_listener(self, event: Dict[str, Any]):\n        if self._stop_event.ready():\n            return\n        user_id = event['sender']\n        if event['type'] != 'm.presence' or user_id == self._user_id:\n            return\n\n        user = self._get_user(user_id)\n        user.displayname = event['content'].get('displayname') or user.displayname\n        address = self._validate_userid_signature(user)\n        if not address:\n            # Malformed address - skip\n            return\n\n        # not a user we've whitelisted, skip\n        if not self.is_address_known(address):\n            return\n        self.add_userid_for_address(address, user_id)\n\n        new_state = UserPresence(event['content']['presence'])\n        if new_state == self._userid_to_presence.get(user_id):\n            # Cached presence state matches, no action required\n            return\n\n        self._userid_to_presence[user_id] = new_state\n        self.refresh_address_presence(address)\n\n        if self._user_presence_changed_callback:\n            self._user_presence_changed_callback(user, new_state)", "response": "Update cached user presence state from Matrix presence events.\n\n        Due to the possibility of nodes using accounts on multiple homeservers a composite\n        address state is synthesised from the cached individual user presence states."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the maximum number of pending transfers and find the maximum number of pending transfers so gas_limit is not exceeded.", "response": "def find_max_pending_transfers(gas_limit):\n    \"\"\"Measure gas consumption of TokenNetwork.unlock() depending on number of\n    pending transfers and find the maximum number of pending transfers so\n    gas_limit is not exceeded.\"\"\"\n\n    tester = ContractTester(generate_keys=2)\n\n    tester.deploy_contract('SecretRegistry')\n\n    tester.deploy_contract(\n        'HumanStandardToken',\n        _initialAmount=100000,\n        _decimalUnits=3,\n        _tokenName='SomeToken',\n        _tokenSymbol='SMT',\n    )\n\n    tester.deploy_contract(\n        'TokenNetwork',\n        _token_address=tester.contract_address('HumanStandardToken'),\n        _secret_registry=tester.contract_address('SecretRegistry'),\n        _chain_id=1,\n        _settlement_timeout_min=100,\n        _settlement_timeout_max=200,\n    )\n\n    tester.call_transaction(\n        'HumanStandardToken',\n        'transfer',\n        _to=tester.accounts[1],\n        _value=10000,\n    )\n\n    receipt = tester.call_transaction(\n        'TokenNetwork',\n        'openChannel',\n        participant1=tester.accounts[0],\n        participant2=tester.accounts[1],\n        settle_timeout=150,\n    )\n\n    channel_identifier = int(encode_hex(receipt['logs'][0]['topics'][1]), 16)\n\n    tester.call_transaction(\n        'HumanStandardToken',\n        'approve',\n        sender=tester.accounts[0],\n        _spender=tester.contract_address('TokenNetwork'),\n        _value=10000,\n    )\n\n    tester.call_transaction(\n        'HumanStandardToken',\n        'approve',\n        sender=tester.accounts[1],\n        _spender=tester.contract_address('TokenNetwork'),\n        _value=5000,\n    )\n\n    tester.call_transaction(\n        'TokenNetwork',\n        'setTotalDeposit',\n        channel_identifier=channel_identifier,\n        participant=tester.accounts[0],\n        total_deposit=5000,\n        partner=tester.accounts[1],\n    )\n\n    tester.call_transaction(\n        'TokenNetwork',\n        'setTotalDeposit',\n        channel_identifier=channel_identifier,\n        participant=tester.accounts[1],\n        total_deposit=2000,\n        partner=tester.accounts[0],\n    )\n\n    print(\"Measuring unlock()'s gas cost for different Merkle tree widths, can take a while...\")\n\n    before_closing = tester.tester.take_snapshot()\n    enough = 0\n    too_much = 1024\n\n    nonce = 10\n    additional_hash = urandom(32)\n    token_network_identifier = tester.contract_address('TokenNetwork')\n\n    while enough + 1 < too_much:\n        tree_size = (enough + too_much) // 2\n        tester.tester.revert_to_snapshot(before_closing)\n\n        pending_transfers_tree = get_pending_transfers_tree(\n            tester.web3,\n            unlockable_amounts=[1] * tree_size,\n        )\n\n        balance_hash = hash_balance_data(3000, 2000, pending_transfers_tree.merkle_root)\n        data_to_sign = pack_balance_proof(\n            nonce=nonce,\n            balance_hash=balance_hash,\n            additional_hash=additional_hash,\n            channel_identifier=channel_identifier,\n            token_network_identifier=token_network_identifier,\n            chain_id=1,\n        )\n        signature = LocalSigner(tester.private_keys[1]).sign(data=data_to_sign)\n\n        tester.call_transaction(\n            'TokenNetwork',\n            'closeChannel',\n            channel_identifier=channel_identifier,\n            partner=tester.accounts[1],\n            balance_hash=balance_hash,\n            nonce=nonce,\n            additional_hash=additional_hash,\n            signature=signature,\n        )\n\n        tester.tester.mine_blocks(160)  # close settlement window\n\n        tester.call_transaction(\n            'TokenNetwork',\n            'settleChannel',\n            channel_identifier=channel_identifier,\n            participant1=tester.accounts[0],\n            participant1_transferred_amount=0,\n            participant1_locked_amount=0,\n            participant1_locksroot=b'\\x00' * 32,\n            participant2=tester.accounts[1],\n            participant2_transferred_amount=3000,\n            participant2_locked_amount=2000,\n            participant2_locksroot=pending_transfers_tree.merkle_root,\n        )\n\n        receipt = tester.call_transaction(\n            'TokenNetwork',\n            'unlock',\n            channel_identifier=channel_identifier,\n            participant=tester.accounts[0],\n            partner=tester.accounts[1],\n            merkle_tree_leaves=pending_transfers_tree.packed_transfers,\n        )\n        gas_used = receipt['gasUsed']\n\n        if gas_used <= gas_limit:\n            enough = tree_size\n            print(f'{tree_size} pending transfers work ({gas_used} gas needed to unlock)')\n        else:\n            too_much = tree_size\n            print(f'{tree_size} pending transfers are too much ({gas_used} gas needed to unlock)')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck the version of the current version of the node.", "response": "def check_version(current_version: str):\n    \"\"\" Check periodically for a new release \"\"\"\n    app_version = parse_version(current_version)\n    while True:\n        try:\n            _do_check_version(app_version)\n        except requests.exceptions.HTTPError as herr:\n            click.secho('Error while checking for version', fg='red')\n            print(herr)\n        except ValueError as verr:\n            click.secho('Error while checking the version', fg='red')\n            print(verr)\n        finally:\n            # repeat the process once every 3h\n            gevent.sleep(CHECK_VERSION_INTERVAL)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_gas_reserve(raiden):\n    while True:\n        has_enough_balance, estimated_required_balance = gas_reserve.has_enough_gas_reserve(\n            raiden,\n            channels_to_open=1,\n        )\n        estimated_required_balance_eth = Web3.fromWei(estimated_required_balance, 'ether')\n\n        if not has_enough_balance:\n            log.info('Missing gas reserve', required_wei=estimated_required_balance)\n            click.secho(\n                (\n                    'WARNING\\n'\n                    \"Your account's balance is below the estimated gas reserve of \"\n                    f'{estimated_required_balance_eth} eth. This may lead to a loss of '\n                    'of funds because your account will be unable to perform on-chain '\n                    'transactions. Please add funds to your account as soon as possible.'\n                ),\n                fg='red',\n            )\n\n        gevent.sleep(CHECK_GAS_RESERVE_INTERVAL)", "response": "Check if gas reserve is needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_rdn_deposits(raiden, user_deposit_proxy: UserDeposit):\n    while True:\n        rei_balance = user_deposit_proxy.effective_balance(raiden.address, \"latest\")\n        rdn_balance = to_rdn(rei_balance)\n        if rei_balance < MIN_REI_THRESHOLD:\n            click.secho(\n                (\n                    f'WARNING\\n'\n                    f'Your account\\'s RDN balance of {rdn_balance} is below the '\n                    f'minimum threshold. Provided that you have either a monitoring '\n                    f'service or a path finding service activated, your node is not going '\n                    f'to be able to pay those services which may lead to denial of service or '\n                    f'loss of funds.'\n                ),\n                fg='red',\n            )\n\n        gevent.sleep(CHECK_RDN_MIN_DEPOSIT_INTERVAL)", "response": "Check for RDN deposits in the user - deposits contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the underlying ethereum client s network id has changed.", "response": "def check_network_id(network_id, web3: Web3):\n    \"\"\" Check periodically if the underlying ethereum client's network id has changed\"\"\"\n    while True:\n        current_id = int(web3.version.network)\n        if network_id != current_id:\n            raise RuntimeError(\n                f'Raiden was running on network with id {network_id} and it detected '\n                f'that the underlying ethereum client network id changed to {current_id}.'\n                f' Changing the underlying blockchain while the Raiden node is running '\n                f'is not supported.',\n            )\n        gevent.sleep(CHECK_NETWORK_ID_INTERVAL)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a new callback. Note: The callback will be executed in the AlarmTask context and for this reason it should not block, otherwise we can miss block changes.", "response": "def register_callback(self, callback):\n        \"\"\" Register a new callback.\n\n        Note:\n            The callback will be executed in the AlarmTask context and for\n            this reason it should not block, otherwise we can miss block\n            changes.\n        \"\"\"\n        if not callable(callback):\n            raise ValueError('callback is not a callable')\n\n        self.callbacks.append(callback)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a callback from the list of callbacks if it exists", "response": "def remove_callback(self, callback):\n        \"\"\"Remove callback from the list of callbacks if it exists\"\"\"\n        if callback in self.callbacks:\n            self.callbacks.remove(callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nblocks call to update the local state if necessary.", "response": "def first_run(self, known_block_number):\n        \"\"\" Blocking call to update the local state, if necessary. \"\"\"\n        assert self.callbacks, 'callbacks not set'\n        latest_block = self.chain.get_block(block_identifier='latest')\n\n        log.debug(\n            'Alarm task first run',\n            known_block_number=known_block_number,\n            latest_block_number=latest_block['number'],\n            latest_gas_limit=latest_block['gasLimit'],\n            latest_block_hash=to_hex(latest_block['hash']),\n        )\n\n        self.known_block_number = known_block_number\n        self.chain_id = self.chain.network_id\n        self._maybe_run_callbacks(latest_block)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the callbacks if there is at least one new block.", "response": "def _maybe_run_callbacks(self, latest_block):\n        \"\"\" Run the callbacks if there is at least one new block.\n\n        The callbacks are executed only if there is a new block, otherwise the\n        filters may try to poll for an inexisting block number and the Ethereum\n        client can return an JSON-RPC error.\n        \"\"\"\n        assert self.known_block_number is not None, 'known_block_number not set'\n\n        latest_block_number = latest_block['number']\n        missed_blocks = latest_block_number - self.known_block_number\n\n        if missed_blocks < 0:\n            log.critical(\n                'Block number decreased',\n                chain_id=self.chain_id,\n                known_block_number=self.known_block_number,\n                old_block_number=latest_block['number'],\n                old_gas_limit=latest_block['gasLimit'],\n                old_block_hash=to_hex(latest_block['hash']),\n            )\n        elif missed_blocks > 0:\n            log_details = dict(\n                known_block_number=self.known_block_number,\n                latest_block_number=latest_block_number,\n                latest_block_hash=to_hex(latest_block['hash']),\n                latest_block_gas_limit=latest_block['gasLimit'],\n            )\n            if missed_blocks > 1:\n                log_details['num_missed_blocks'] = missed_blocks - 1\n\n            log.debug(\n                'Received new block',\n                **log_details,\n            )\n\n            remove = list()\n            for callback in self.callbacks:\n                result = callback(latest_block)\n                if result is REMOVE_CALLBACK:\n                    remove.append(callback)\n\n            for callback in remove:\n                self.callbacks.remove(callback)\n\n            self.known_block_number = latest_block_number"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngo through WAL state changes until a certain balance hash is found.", "response": "def channel_state_until_state_change(\n        raiden,\n        canonical_identifier: CanonicalIdentifier,\n        state_change_identifier: int,\n) -> typing.Optional[NettingChannelState]:\n    \"\"\" Go through WAL state changes until a certain balance hash is found. \"\"\"\n    wal = restore_to_state_change(\n        transition_function=node.state_transition,\n        storage=raiden.wal.storage,\n        state_change_identifier=state_change_identifier,\n    )\n\n    msg = 'There is a state change, therefore the state must not be None'\n    assert wal.state_manager.current_state is not None, msg\n\n    chain_state = wal.state_manager.current_state\n    channel_state = views.get_channelstate_by_canonical_identifier(\n        chain_state=chain_state,\n        canonical_identifier=canonical_identifier,\n    )\n\n    if not channel_state:\n        raise RaidenUnrecoverableError(\n            f\"Channel was not found before state_change {state_change_identifier}\",\n        )\n\n    return channel_state"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes data and pack them into a byte array.", "response": "def pack_data(abi_types, values) -> bytes:\n    \"\"\"Normalize data and pack them into a byte array\"\"\"\n    if len(abi_types) != len(values):\n        raise ValueError(\n            \"Length mismatch between provided abi types and values.  Got \"\n            \"{0} types and {1} values.\".format(len(abi_types), len(values)),\n        )\n\n    normalized_values = map_abi_data([abi_address_to_hex], abi_types, values)\n\n    return decode_hex(''.join(\n        remove_0x_prefix(hex_encode_abi_type(abi_type, value))\n        for abi_type, value\n        in zip(abi_types, normalized_values)\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enqueue(self, queue_identifier: QueueIdentifier, message: Message):\n        assert queue_identifier.recipient == self.receiver\n        with self._lock:\n            already_queued = any(\n                queue_identifier == data.queue_identifier and message == data.message\n                for data in self._message_queue\n            )\n            if already_queued:\n                self.log.warning(\n                    'Message already in queue - ignoring',\n                    receiver=pex(self.receiver),\n                    queue=queue_identifier,\n                    message=message,\n                )\n                return\n            timeout_generator = udp_utils.timeout_exponential_backoff(\n                self.transport._config['retries_before_backoff'],\n                self.transport._config['retry_interval'],\n                self.transport._config['retry_interval'] * 10,\n            )\n            expiration_generator = self._expiration_generator(timeout_generator)\n            self._message_queue.append(_RetryQueue._MessageData(\n                queue_identifier=queue_identifier,\n                message=message,\n                text=json.dumps(message.to_dict()),\n                expiration_generator=expiration_generator,\n            ))\n        self.notify()", "response": "Enqueue a message to be sent and notify main loop"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks and send all pending and queued messages that are not waiting on retry timeout", "response": "def _check_and_send(self):\n        \"\"\"Check and send all pending/queued messages that are not waiting on retry timeout\n\n        After composing the to-be-sent message, also message queue from messages that are not\n        present in the respective SendMessageEvent queue anymore\n        \"\"\"\n        if self.transport._stop_event.ready() or not self.transport.greenlet:\n            self.log.error(\"Can't retry - stopped\")\n            return\n\n        if self.transport._prioritize_global_messages:\n            # During startup global messages have to be sent first\n            self.transport._global_send_queue.join()\n\n        self.log.debug('Retrying message', receiver=to_normalized_address(self.receiver))\n        status = self.transport._address_mgr.get_address_reachability(self.receiver)\n        if status is not AddressReachability.REACHABLE:\n            # if partner is not reachable, return\n            self.log.debug(\n                'Partner not reachable. Skipping.',\n                partner=pex(self.receiver),\n                status=status,\n            )\n            return\n        # sort output by channel_identifier (so global/unordered queue goes first)\n        # inside queue, preserve order in which messages were enqueued\n        ordered_queue = sorted(\n            self._message_queue,\n            key=lambda d: d.queue_identifier.channel_identifier,\n        )\n        message_texts = [\n            data.text\n            for data in ordered_queue\n            # if expired_gen generator yields False, message was sent recently, so skip it\n            if next(data.expiration_generator)\n        ]\n\n        def message_is_in_queue(data: _RetryQueue._MessageData) -> bool:\n            return any(\n                isinstance(data.message, RetrieableMessage) and\n                send_event.message_identifier == data.message.message_identifier\n                for send_event in self.transport._queueids_to_queues[data.queue_identifier]\n            )\n\n        # clean after composing, so any queued messages (e.g. Delivered) are sent at least once\n        for msg_data in self._message_queue[:]:\n            remove = False\n            if isinstance(msg_data.message, (Delivered, Ping, Pong)):\n                # e.g. Delivered, send only once and then clear\n                # TODO: Is this correct? Will a missed Delivered be 'fixed' by the\n                #       later `Processed` message?\n                remove = True\n            elif msg_data.queue_identifier not in self.transport._queueids_to_queues:\n                remove = True\n                self.log.debug(\n                    'Stopping message send retry',\n                    queue=msg_data.queue_identifier,\n                    message=msg_data.message,\n                    reason='Raiden queue is gone',\n                )\n            elif not message_is_in_queue(msg_data):\n                remove = True\n                self.log.debug(\n                    'Stopping message send retry',\n                    queue=msg_data.queue_identifier,\n                    message=msg_data.message,\n                    reason='Message was removed from queue',\n                )\n\n            if remove:\n                self._message_queue.remove(msg_data)\n\n        if message_texts:\n            self.log.debug('Send', receiver=pex(self.receiver), messages=message_texts)\n            self.transport._send_raw(self.receiver, '\\n'.join(message_texts))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop(self):\n        if self._stop_event.ready():\n            return\n        self._stop_event.set()\n        self._global_send_event.set()\n\n        for retrier in self._address_to_retrier.values():\n            if retrier:\n                retrier.notify()\n\n        self._client.set_presence_state(UserPresence.OFFLINE.value)\n\n        self._client.stop_listener_thread()  # stop sync_thread, wait client's greenlets\n        # wait own greenlets, no need to get on them, exceptions should be raised in _run()\n        gevent.wait(self.greenlets + [r.greenlet for r in self._address_to_retrier.values()])\n\n        # Ensure keep-alive http connections are closed\n        self._client.api.session.close()\n\n        self.log.debug('Matrix stopped', config=self._config)\n        del self.log", "response": "Stop the greenlet asynchronously"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _spawn(self, func: Callable, *args, **kwargs) -> gevent.Greenlet:\n\n        def on_success(greenlet):\n            if greenlet in self.greenlets:\n                self.greenlets.remove(greenlet)\n\n        greenlet = gevent.spawn(func, *args, **kwargs)\n        greenlet.link_exception(self.on_error)\n        greenlet.link_value(on_success)\n        self.greenlets.append(greenlet)\n        return greenlet", "response": "Spawn a sub - task and ensures an error on it crashes self / main greenlet"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef whitelist(self, address: Address):\n        self.log.debug('Whitelist', address=to_normalized_address(address))\n        self._address_mgr.add_address(address)", "response": "Whitelist peer address to receive communications from\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_health_check(self, node_address):\n        if self._stop_event.ready():\n            return\n\n        with self._health_lock:\n            if self._address_mgr.is_address_known(node_address):\n                return  # already healthchecked\n\n            node_address_hex = to_normalized_address(node_address)\n            self.log.debug('Healthcheck', peer_address=node_address_hex)\n\n            candidates = [\n                self._get_user(user)\n                for user in self._client.search_user_directory(node_address_hex)\n            ]\n            user_ids = {\n                user.user_id\n                for user in candidates\n                if validate_userid_signature(user) == node_address\n            }\n            self.whitelist(node_address)\n            self._address_mgr.add_userids_for_address(node_address, user_ids)\n\n            # Ensure network state is updated in case we already know about the user presences\n            # representing the target node\n            self._address_mgr.refresh_address_presence(node_address)", "response": "Start healthcheck for a peer node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message to the recipient in the queue_identifier.", "response": "def send_async(\n            self,\n            queue_identifier: QueueIdentifier,\n            message: Message,\n    ):\n        \"\"\"Queue the message for sending to recipient in the queue_identifier\n\n        It may be called before transport is started, to initialize message queues\n        The actual sending is started only when the transport is started\n        \"\"\"\n        # even if transport is not started, can run to enqueue messages to send when it starts\n        receiver_address = queue_identifier.recipient\n\n        if not is_binary_address(receiver_address):\n            raise ValueError('Invalid address {}'.format(pex(receiver_address)))\n\n        # These are not protocol messages, but transport specific messages\n        if isinstance(message, (Delivered, Ping, Pong)):\n            raise ValueError(\n                'Do not use send_async for {} messages'.format(message.__class__.__name__),\n            )\n\n        self.log.debug(\n            'Send async',\n            receiver_address=pex(receiver_address),\n            message=message,\n            queue_identifier=queue_identifier,\n        )\n\n        self._send_with_retry(queue_identifier, message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a message to one of the global rooms.", "response": "def send_global(self, room: str, message: Message) -> None:\n        \"\"\"Sends a message to one of the global rooms\n\n        These rooms aren't being listened on and therefore no reply could be heard, so these\n        messages are sent in a send-and-forget async way.\n        The actual room name is composed from the suffix given as parameter and chain name or id\n        e.g.: raiden_ropsten_discovery\n        Params:\n            room: name suffix as passed in config['global_rooms'] list\n            message: Message instance to be serialized and sent\n        \"\"\"\n        self._global_send_queue.put((room, message))\n        self._global_send_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles an invite event.", "response": "def _handle_invite(self, room_id: _RoomID, state: dict):\n        \"\"\" Join rooms invited by whitelisted partners \"\"\"\n        if self._stop_event.ready():\n            return\n\n        self.log.debug('Got invite', room_id=room_id)\n        invite_events = [\n            event\n            for event in state['events']\n            if event['type'] == 'm.room.member' and\n            event['content'].get('membership') == 'invite' and\n            event['state_key'] == self._user_id\n        ]\n        if not invite_events:\n            self.log.debug('Invite: no invite event found', room_id=room_id)\n            return  # there should always be one and only one invite membership event for us\n        invite_event = invite_events[0]\n        sender = invite_event['sender']\n\n        sender_join_events = [\n            event\n            for event in state['events']\n            if event['type'] == 'm.room.member' and\n            event['content'].get('membership') == 'join' and\n            event['state_key'] == sender\n        ]\n        if not sender_join_events:\n            self.log.debug('Invite: no sender join event', room_id=room_id)\n            return  # there should always be one and only one join membership event for the sender\n        sender_join_event = sender_join_events[0]\n\n        user = self._get_user(sender)\n        user.displayname = sender_join_event['content'].get('displayname') or user.displayname\n        peer_address = validate_userid_signature(user)\n        if not peer_address:\n            self.log.debug(\n                'Got invited to a room by invalid signed user - ignoring',\n                room_id=room_id,\n                user=user,\n            )\n            return\n\n        if not self._address_mgr.is_address_known(peer_address):\n            self.log.debug(\n                'Got invited by a non-whitelisted user - ignoring',\n                room_id=room_id,\n                user=user,\n            )\n            return\n\n        join_rules_events = [\n            event\n            for event in state['events']\n            if event['type'] == 'm.room.join_rules'\n        ]\n\n        # room privacy as seen from the event\n        private_room: bool = False\n        if join_rules_events:\n            join_rules_event = join_rules_events[0]\n            private_room = join_rules_event['content'].get('join_rule') == 'invite'\n\n        # we join room and _set_room_id_for_address despite room privacy and requirements,\n        # _get_room_ids_for_address will take care of returning only matching rooms and\n        # _leave_unused_rooms will clear it in the future, if and when needed\n        room: Room = None\n        last_ex: Optional[Exception] = None\n        retry_interval = 0.1\n        for _ in range(JOIN_RETRIES):\n            try:\n                room = self._client.join_room(room_id)\n            except MatrixRequestError as e:\n                last_ex = e\n                if self._stop_event.wait(retry_interval):\n                    break\n                retry_interval = retry_interval * 2\n            else:\n                break\n        else:\n            assert last_ex is not None\n            raise last_ex  # re-raise if couldn't succeed in retries\n\n        if not room.listeners:\n            room.add_listener(self._handle_message, 'm.room.message')\n\n        # room state may not populated yet, so we populate 'invite_only' from event\n        room.invite_only = private_room\n\n        self._set_room_id_for_address(address=peer_address, room_id=room_id)\n\n        self.log.debug(\n            'Joined from invite',\n            room_id=room_id,\n            aliases=room.aliases,\n            peer=to_checksum_address(peer_address),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles messages sent to listening rooms.", "response": "def _handle_message(self, room, event) -> bool:\n        \"\"\" Handle text messages sent to listening rooms \"\"\"\n        if (\n                event['type'] != 'm.room.message' or\n                event['content']['msgtype'] != 'm.text' or\n                self._stop_event.ready()\n        ):\n            # Ignore non-messages and non-text messages\n            return False\n\n        sender_id = event['sender']\n\n        if sender_id == self._user_id:\n            # Ignore our own messages\n            return False\n\n        user = self._get_user(sender_id)\n        peer_address = validate_userid_signature(user)\n        if not peer_address:\n            self.log.debug(\n                'Message from invalid user displayName signature',\n                peer_user=user.user_id,\n                room=room,\n            )\n            return False\n\n        # don't proceed if user isn't whitelisted (yet)\n        if not self._address_mgr.is_address_known(peer_address):\n            # user not start_health_check'ed\n            self.log.debug(\n                'Message from non-whitelisted peer - ignoring',\n                sender=user,\n                sender_address=pex(peer_address),\n                room=room,\n            )\n            return False\n\n        # rooms we created and invited user, or were invited specifically by them\n        room_ids = self._get_room_ids_for_address(peer_address)\n\n        # TODO: Remove clause after `and` and check if things still don't hang\n        if room.room_id not in room_ids and (self._private_rooms and not room.invite_only):\n            # this should not happen, but is not fatal, as we may not know user yet\n            if self._private_rooms and not room.invite_only:\n                reason = 'required private room, but received message in a public'\n            else:\n                reason = 'unknown room for user'\n            self.log.debug(\n                'Ignoring invalid message',\n                peer_user=user.user_id,\n                peer_address=pex(peer_address),\n                room=room,\n                expected_room_ids=room_ids,\n                reason=reason,\n            )\n            return False\n\n        # TODO: With the condition in the TODO above restored this one won't have an effect, check\n        #       if it can be removed after the above is solved\n        if not room_ids or room.room_id != room_ids[0]:\n            self.log.debug(\n                'Received message triggered new comms room for peer',\n                peer_user=user.user_id,\n                peer_address=pex(peer_address),\n                known_user_rooms=room_ids,\n                room=room,\n            )\n            self._set_room_id_for_address(peer_address, room.room_id)\n\n        is_peer_reachable = self._address_mgr.get_address_reachability(peer_address) is (\n            AddressReachability.REACHABLE\n        )\n        if not is_peer_reachable:\n            self.log.debug('Forcing presence update', peer_address=peer_address, user_id=sender_id)\n            self._address_mgr.force_user_presence(user, UserPresence.ONLINE)\n            self._address_mgr.refresh_address_presence(peer_address)\n\n        data = event['content']['body']\n        if not isinstance(data, str):\n            self.log.warning(\n                'Received message body not a string',\n                peer_user=user.user_id,\n                peer_address=to_checksum_address(peer_address),\n                room=room,\n            )\n            return False\n\n        messages: List[Message] = list()\n\n        if data.startswith('0x'):\n            try:\n                message = message_from_bytes(decode_hex(data))\n                if not message:\n                    raise InvalidProtocolMessage\n            except (DecodeError, AssertionError) as ex:\n                self.log.warning(\n                    \"Can't parse message binary data\",\n                    message_data=data,\n                    peer_address=pex(peer_address),\n                    _exc=ex,\n                )\n                return False\n            except InvalidProtocolMessage as ex:\n                self.log.warning(\n                    'Received message binary data is not a valid message',\n                    message_data=data,\n                    peer_address=pex(peer_address),\n                    _exc=ex,\n                )\n                return False\n            else:\n                messages.append(message)\n\n        else:\n            for line in data.splitlines():\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    message_dict = json.loads(line)\n                    message = message_from_dict(message_dict)\n                except (UnicodeDecodeError, json.JSONDecodeError) as ex:\n                    self.log.warning(\n                        \"Can't parse message data JSON\",\n                        message_data=line,\n                        peer_address=pex(peer_address),\n                        _exc=ex,\n                    )\n                    continue\n                except InvalidProtocolMessage as ex:\n                    self.log.warning(\n                        \"Message data JSON are not a valid message\",\n                        message_data=line,\n                        peer_address=pex(peer_address),\n                        _exc=ex,\n                    )\n                    continue\n                if not isinstance(message, (SignedRetrieableMessage, SignedMessage)):\n                    self.log.warning(\n                        'Received invalid message',\n                        message=message,\n                    )\n                    continue\n                elif message.sender != peer_address:\n                    self.log.warning(\n                        'Message not signed by sender!',\n                        message=message,\n                        signer=message.sender,\n                        peer_address=peer_address,\n                    )\n                    continue\n                messages.append(message)\n\n        if not messages:\n            return False\n\n        self.log.debug(\n            'Incoming messages',\n            messages=messages,\n            sender=pex(peer_address),\n            sender_user=user,\n            room=room,\n        )\n\n        for message in messages:\n            if isinstance(message, Delivered):\n                self._receive_delivered(message)\n            elif isinstance(message, Processed):\n                self._receive_message(message)\n            else:\n                assert isinstance(message, SignedRetrieableMessage)\n                self._receive_message(message)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_retrier(self, receiver: Address) -> _RetryQueue:\n        if receiver not in self._address_to_retrier:\n            retrier = _RetryQueue(transport=self, receiver=receiver)\n            self._address_to_retrier[receiver] = retrier\n            # Always start the _RetryQueue, otherwise `stop` will block forever\n            # waiting for the corresponding gevent.Greenlet to complete. This\n            # has no negative side-effects if the transport has stopped because\n            # the retrier itself checks the transport running state.\n            retrier.start()\n        return self._address_to_retrier[receiver]", "response": "Construct and return a _RetryQueue for receiver."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_private_room(self, invitees: List[User]):\n        return self._client.create_room(\n            None,\n            invitees=[user.user_id for user in invitees],\n            is_public=False,\n        )", "response": "Create an anonymous private room and invite peers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_public_room(self, room_name, invitees: List[User]):\n        room_name_full = f'#{room_name}:{self._server_name}'\n        invitees_uids = [user.user_id for user in invitees]\n\n        for _ in range(JOIN_RETRIES):\n            # try joining room\n            try:\n                room = self._client.join_room(room_name_full)\n            except MatrixRequestError as error:\n                if error.code == 404:\n                    self.log.debug(\n                        f'No room for peer, trying to create',\n                        room_name=room_name_full,\n                        error=error,\n                    )\n                else:\n                    self.log.debug(\n                        f'Error joining room',\n                        room_name=room_name,\n                        error=error.content,\n                        error_code=error.code,\n                    )\n            else:\n                # Invite users to existing room\n                member_ids = {user.user_id for user in room.get_joined_members(force_resync=True)}\n                users_to_invite = set(invitees_uids) - member_ids\n                self.log.debug('Inviting users', room=room, invitee_ids=users_to_invite)\n                for invitee_id in users_to_invite:\n                    room.invite_user(invitee_id)\n                self.log.debug('Room joined successfully', room=room)\n                break\n\n            # if can't, try creating it\n            try:\n                room = self._client.create_room(\n                    room_name,\n                    invitees=invitees_uids,\n                    is_public=True,\n                )\n            except MatrixRequestError as error:\n                if error.code == 409:\n                    msg = (\n                        'Error creating room, '\n                        'seems to have been created by peer meanwhile, retrying.'\n                    )\n                else:\n                    msg = 'Error creating room, retrying.'\n\n                self.log.debug(\n                    msg,\n                    room_name=room_name,\n                    error=error.content,\n                    error_code=error.code,\n                )\n            else:\n                self.log.debug('Room created successfully', room=room, invitees=invitees)\n                break\n        else:\n            # if can't join nor create, create an unnamed one\n            room = self._client.create_room(\n                None,\n                invitees=invitees_uids,\n                is_public=True,\n            )\n            self.log.warning(\n                'Could not create nor join a named room. Successfuly created an unnamed one',\n                room=room,\n                invitees=invitees,\n            )\n\n        return room", "response": "Get a public room and invite peers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses eth_sign compatible hasher to sign matrix data", "response": "def _sign(self, data: bytes) -> bytes:\n        \"\"\" Use eth_sign compatible hasher to sign matrix data \"\"\"\n        assert self._raiden_service is not None\n        return self._raiden_service.signer.sign(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_user(self, user: Union[User, str]) -> User:\n        user_id: str = getattr(user, 'user_id', user)\n        discovery_room = self._global_rooms.get(\n            make_room_alias(self.network_id, DISCOVERY_DEFAULT_ROOM),\n        )\n        if discovery_room and user_id in discovery_room._members:\n            duser = discovery_room._members[user_id]\n            # if handed a User instance with displayname set, update the discovery room cache\n            if getattr(user, 'displayname', None):\n                assert isinstance(user, User)\n                duser.displayname = user.displayname\n            user = duser\n        elif not isinstance(user, User):\n            user = self._client.get_user(user_id)\n        return user", "response": "Creates a User instance from an user_id if none or fetch a cached User\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_room_id_for_address(self, address: Address, room_id: Optional[_RoomID] = None):\n\n        assert not room_id or room_id in self._client.rooms, 'Invalid room_id'\n        address_hex: AddressHex = to_checksum_address(address)\n        # filter_private=False to preserve public rooms on the list, even if we require privacy\n        room_ids = self._get_room_ids_for_address(address, filter_private=False)\n\n        with self._account_data_lock:\n            # no need to deepcopy, we don't modify lists in-place\n            # cast generic Dict[str, Any] to types we expect, to satisfy mypy, runtime no-op\n            _address_to_room_ids = cast(\n                Dict[AddressHex, List[_RoomID]],\n                self._client.account_data.get('network.raiden.rooms', {}).copy(),\n            )\n\n            changed = False\n            if not room_id:  # falsy room_id => clear list\n                changed = address_hex in _address_to_room_ids\n                _address_to_room_ids.pop(address_hex, None)\n            else:\n                # push to front\n                room_ids = [room_id] + [r for r in room_ids if r != room_id]\n                if room_ids != _address_to_room_ids.get(address_hex):\n                    _address_to_room_ids[address_hex] = room_ids\n                    changed = True\n\n            if changed:\n                # dict will be set at the end of _clean_unused_rooms\n                self._leave_unused_rooms(_address_to_room_ids)", "response": "Sets the room_id for the given address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_room_ids_for_address(\n            self,\n            address: Address,\n            filter_private: bool = None,\n    ) -> List[_RoomID]:\n        \"\"\" Uses GMatrixClient.get_account_data to get updated mapping of address->rooms\n\n        It'll filter only existing rooms.\n        If filter_private=True, also filter out public rooms.\n        If filter_private=None, filter according to self._private_rooms\n        \"\"\"\n        address_hex: AddressHex = to_checksum_address(address)\n        with self._account_data_lock:\n            room_ids = self._client.account_data.get(\n                'network.raiden.rooms',\n                {},\n            ).get(address_hex)\n            self.log.debug('matrix get account data', room_ids=room_ids, for_address=address_hex)\n            if not room_ids:  # None or empty\n                room_ids = list()\n            if not isinstance(room_ids, list):  # old version, single room\n                room_ids = [room_ids]\n\n            if filter_private is None:\n                filter_private = self._private_rooms\n            if not filter_private:\n                # existing rooms\n                room_ids = [\n                    room_id\n                    for room_id in room_ids\n                    if room_id in self._client.rooms\n                ]\n            else:\n                # existing and private rooms\n                room_ids = [\n                    room_id\n                    for room_id in room_ids\n                    if room_id in self._client.rooms and self._client.rooms[room_id].invite_only\n                ]\n\n            return room_ids", "response": "Returns a list of room ids for the given address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _leave_unused_rooms(self, _address_to_room_ids: Dict[AddressHex, List[_RoomID]]):\n        _msg = '_leave_unused_rooms called without account data lock'\n        assert self._account_data_lock.locked(), _msg\n\n        # TODO: Remove the next five lines and check if transfers start hanging again\n        self._client.set_account_data(\n            'network.raiden.rooms',  # back from cast in _set_room_id_for_address\n            cast(Dict[str, Any], _address_to_room_ids),\n        )\n        return\n\n        # cache in a set all whitelisted addresses\n        whitelisted_hex_addresses: Set[AddressHex] = {\n            to_checksum_address(address)\n            for address in self._address_mgr.known_addresses\n        }\n\n        keep_rooms: Set[_RoomID] = set()\n\n        for address_hex, room_ids in list(_address_to_room_ids.items()):\n            if not room_ids:  # None or empty\n                room_ids = list()\n            if not isinstance(room_ids, list):  # old version, single room\n                room_ids = [room_ids]\n\n            if address_hex not in whitelisted_hex_addresses:\n                _address_to_room_ids.pop(address_hex)\n                continue\n\n            counters = [0, 0]  # public, private\n            new_room_ids: List[_RoomID] = list()\n\n            # limit to at most 2 public and 2 private rooms, preserving order\n            for room_id in room_ids:\n                if room_id not in self._client.rooms:\n                    continue\n                elif self._client.rooms[room_id].invite_only is None:\n                    new_room_ids.append(room_id)  # not known, postpone cleaning\n                elif counters[self._client.rooms[room_id].invite_only] < 2:\n                    counters[self._client.rooms[room_id].invite_only] += 1\n                    new_room_ids.append(room_id)  # not enough rooms of this type yet\n                else:\n                    continue  # enough rooms, leave and clean\n\n            keep_rooms |= set(new_room_ids)\n            if room_ids != new_room_ids:\n                _address_to_room_ids[address_hex] = new_room_ids\n\n        rooms: List[Tuple[_RoomID, Room]] = list(self._client.rooms.items())\n\n        self.log.debug(\n            'Updated address room mapping',\n            address_to_room_ids=_address_to_room_ids,\n        )\n        self._client.set_account_data('network.raiden.rooms', _address_to_room_ids)\n\n        def leave(room: Room):\n            \"\"\"A race between /leave and /sync may remove the room before\n            del on _client.rooms key. Suppress it, as the end result is the same: no more room\"\"\"\n            try:\n                self.log.debug('Leaving unused room', room=room)\n                return room.leave()\n            except KeyError:\n                return True\n\n        for room_id, room in rooms:\n            if room_id in {groom.room_id for groom in self._global_rooms.values() if groom}:\n                # don't leave global room\n                continue\n            if room_id not in keep_rooms:\n                greenlet = self._spawn(leave, room)\n                greenlet.name = (\n                    f'MatrixTransport.leave '\n                    f'node:{pex(self._raiden_service.address)} '\n                    f'user_id:{self._user_id}'\n                )", "response": "Private method to check if any of the rooms in the given address_to_room_ids are not known."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nodes(self) -> List[str]:\n        if self._scenario_version == 1 and 'range' in self._config:\n            range_config = self._config['range']\n\n            try:\n                start, stop = range_config['first'], range_config['last'] + 1\n            except KeyError:\n                raise MissingNodesConfiguration(\n                    'Setting \"range\" must be a dict containing keys \"first\" and \"last\",'\n                    ' whose values are integers!',\n                )\n\n            try:\n                template = range_config['template']\n            except KeyError:\n                raise MissingNodesConfiguration(\n                    'Must specify \"template\" setting when giving \"range\" setting.',\n                )\n\n            return [template.format(i) for i in range(start, stop)]\n        try:\n            return self._config['list']\n        except KeyError:\n            raise MissingNodesConfiguration('Must specify nodes under \"list\" setting!')", "response": "Return the list of nodes configured in the scenario s yaml."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the scenario s version.", "response": "def version(self) -> int:\n        \"\"\"Return the scenario's version.\n\n        If this is not present, we default to version 1.\n\n        :raises InvalidScenarioVersion:\n            if the supplied version is not present in :var:`SUPPORTED_SCENARIO_VERSIONS`.\n        \"\"\"\n        version = self._config.get('version', 1)\n\n        if version not in SUPPORTED_SCENARIO_VERSIONS:\n            raise InvalidScenarioVersion(f'Unexpected scenario version {version}')\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef protocol(self) -> str:\n        if self.nodes.mode is NodeMode.MANAGED:\n            if 'protocol' in self._config:\n                log.warning('The \"protocol\" setting is not supported in \"managed\" node mode.')\n            return 'http'\n        return self._config.get('protocol', 'http')", "response": "Return the designated protocol of the scenario."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the scenario s task configuration as a tuple.", "response": "def task(self) -> Tuple[str, Any]:\n        \"\"\"Return the scenario's task configuration as a tuple.\n\n        :raises MultipleTaskDefinitions:\n            if there is more than one task config under the 'scenario' key.\n        \"\"\"\n        try:\n            items, = self.configuration.items()\n        except ValueError:\n            raise MultipleTaskDefinitions(\n                'Multiple tasks defined in scenario configuration!',\n            )\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef task_class(self):\n        from scenario_player.tasks.base import get_task_class_for_type\n\n        root_task_type, _ = self.task\n\n        task_class = get_task_class_for_type(root_task_type)\n        return task_class", "response": "Return the Task class type configured for the scenario."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the common shared jobs in the raiden - default and nightly workflows are identical.", "response": "def _check_workflows_align(config):\n    \"\"\"\n    Ensure that the common shared jobs in the `raiden-default` and `nightly` workflows are\n    identical.\n    \"\"\"\n\n    jobs_default = config['workflows']['raiden-default']['jobs']\n    jobs_nightly = config['workflows']['nightly']['jobs']\n\n    if jobs_default == jobs_nightly[:len(jobs_default)]:\n        return True, []\n\n    job_diff = unified_diff(\n        [f\"{line}\\n\" for line in jobs_default],\n        [f\"{line}\\n\" for line in jobs_nightly[:len(jobs_default)]],\n        'raiden-default',\n        'nightly',\n    )\n    message = [\n        _yellow(\n            \"Mismatch in common items of workflow definitions 'raiden-default' and \"\n            \"'nightly':\\n\",\n        ),\n    ]\n    for line in job_diff:\n        if line.startswith('-'):\n            message.append(_red(line))\n        elif line.startswith('+'):\n            message.append(_green(line))\n        else:\n            message.append(line)\n\n    return False, ''.join(message)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_transaction_data(\n            abi: Dict,\n            function_name: str,\n            args: Any = None,\n            kwargs: Any = None,\n    ):\n        \"\"\"Get encoded transaction data\"\"\"\n        args = args or list()\n        fn_abi = find_matching_fn_abi(\n            abi,\n            function_name,\n            args=args,\n            kwargs=kwargs,\n        )\n        return encode_transaction_data(\n            None,\n            function_name,\n            contract_abi=abi,\n            fn_abi=fn_abi,\n            args=args,\n            kwargs=kwargs,\n        )", "response": "Get encoded transaction data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a transaction input", "response": "def decode_transaction_input(self, transaction_hash: bytes) -> Dict:\n        \"\"\"Return inputs of a method call\"\"\"\n        transaction = self.contract.web3.eth.getTransaction(\n            transaction_hash,\n        )\n\n        return self.contract.decode_function_input(\n            transaction['input'],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a gas estimate for the given function with the given arguments or None if the function call will fail due to Insufficient funds or the logic in the called function.", "response": "def estimate_gas(\n            self,\n            block_identifier,\n            function: str,\n            *args,\n            **kwargs,\n    ) -> typing.Optional[int]:\n        \"\"\"Returns a gas estimate for the function with the given arguments or\n        None if the function call will fail due to Insufficient funds or\n        the logic in the called function.\"\"\"\n        fn = getattr(self.contract.functions, function)\n        address = to_checksum_address(self.jsonrpc_client.address)\n        if self.jsonrpc_client.eth_node is constants.EthClient.GETH:\n            # Unfortunately geth does not follow the ethereum JSON-RPC spec and\n            # does not accept a block identifier argument for eth_estimateGas\n            # parity and py-evm (trinity) do.\n            #\n            # Geth only runs estimateGas on the pending block and that's why we\n            # should also enforce parity, py-evm and others to do the same since\n            # we can't customize geth.\n            #\n            # Spec: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_estimategas\n            # Geth Issue: https://github.com/ethereum/go-ethereum/issues/2586\n            # Relevant web3 PR: https://github.com/ethereum/web3.py/pull/1046\n            block_identifier = None\n        try:\n            return fn(*args, **kwargs).estimateGas(\n                transaction={'from': address},\n                block_identifier=block_identifier,\n            )\n        except ValueError as err:\n            action = inspect_client_error(err, self.jsonrpc_client.eth_node)\n            will_fail = action in (\n                ClientErrorInspectResult.INSUFFICIENT_FUNDS,\n                ClientErrorInspectResult.ALWAYS_FAIL,\n            )\n            if will_fail:\n                return None\n\n            raise err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a private cluster of geth nodes.", "response": "def geth_commands(ctx, geth_hosts, datadir):\n    \"\"\"This is helpful to setup a private cluster of geth nodes that won't need discovery\n    (because they can use the content of `static_nodes` as `static-nodes.json`).\n    \"\"\"\n    pretty = ctx.obj['pretty']\n    nodes = []  # pylint: disable=redefined-outer-name\n\n    for i, host in enumerate(geth_hosts):\n        nodes.append(create_node_configuration(host=host, node_key_seed=i))\n\n    for node in nodes:\n        node.pop('unlock')\n        node.pop('rpcport')\n\n    config = {'{host}'.format(**node): ' '.join(to_cmd(node, datadir=datadir)) for node in nodes}\n    config['static_nodes'] = [node['enode'] for node in nodes]\n\n    indent = None\n    if pretty:\n        indent = 2\n\n    print(json.dumps(\n        config,\n        indent=indent,\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _data_to_sign(self) -> bytes:\n        packed = self.packed()\n\n        field = type(packed).fields_spec[-1]\n        assert field.name == 'signature', 'signature is not the last field'\n\n        # this slice must be from the end of the buffer\n        return packed.data[:-field.size_bytes]", "response": "Return the binary data to be signed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsigning the message using signer.", "response": "def sign(self, signer: Signer):\n        \"\"\" Sign message using signer. \"\"\"\n        message_data = self._data_to_sign()\n        self.signature = signer.sign(data=message_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sign(self, signer: Signer) -> Signature:\n        # Important: we don't write the signature to `.signature`\n        data = self._data_to_sign()\n        return signer.sign(data)", "response": "Internal function for the overall sign function of RequestMonitoring."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_dict(self) -> Dict[str, Any]:\n        return {\n            'type': self.__class__.__name__,\n            'channel_identifier': self.channel_identifier,\n            'token_network_address': to_normalized_address(self.token_network_address),\n            'balance_hash': encode_hex(self.balance_hash),\n            'nonce': self.nonce,\n            'additional_hash': encode_hex(self.additional_hash),\n            'signature': encode_hex(self.signature),\n            'chain_id': self.chain_id,\n        }", "response": "Return a dictionary representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary representation of the onchain balance proof.", "response": "def to_dict(self) -> Dict:\n        \"\"\"Message format according to monitoring service spec\"\"\"\n        if not self.non_closing_signature:\n            raise ValueError('onchain_balance_proof needs to be signed')\n        if not self.reward_proof_signature:\n            raise ValueError('monitoring request needs to be signed')\n        return {\n            'type': self.__class__.__name__,\n            'onchain_balance_proof': self.balance_proof.to_dict(),\n            'reward_amount': str(self.reward_amount),\n            'non_closing_signature': encode_hex(self.non_closing_signature),\n            'reward_proof_signature': encode_hex(self.reward_proof_signature),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the binary data to be signed", "response": "def _data_to_sign(self) -> bytes:\n        \"\"\" Return the binary data to be/which was signed \"\"\"\n        packed = pack_reward_proof(\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            reward_amount=self.reward_amount,\n            nonce=self.balance_proof.nonce,\n        )\n        return packed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sign(self, signer: Signer):\n        self.non_closing_signature = self.balance_proof._sign(signer)\n        message_data = self._data_to_sign()\n        self.signature = signer.sign(data=message_data)", "response": "This method signs twice the same object with the given signer"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the request monitoring message is valid.", "response": "def verify_request_monitoring(\n            self,\n            partner_address: Address,\n            requesting_address: Address,\n    ) -> bool:\n        \"\"\" One should only use this method to verify integrity and signatures of a\n        RequestMonitoring message. \"\"\"\n        if not self.non_closing_signature:\n            return False\n\n        balance_proof_data = pack_balance_proof(\n            nonce=self.balance_proof.nonce,\n            balance_hash=self.balance_proof.balance_hash,\n            additional_hash=self.balance_proof.additional_hash,\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n        )\n        blinded_data = pack_balance_proof_update(\n            nonce=self.balance_proof.nonce,\n            balance_hash=self.balance_proof.balance_hash,\n            additional_hash=self.balance_proof.additional_hash,\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            partner_signature=self.balance_proof.signature,\n        )\n        reward_proof_data = pack_reward_proof(\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            reward_amount=self.reward_amount,\n            nonce=self.balance_proof.nonce,\n        )\n        return (\n            recover(balance_proof_data, self.balance_proof.signature) == partner_address and\n            recover(blinded_data, self.non_closing_signature) == requesting_address and\n            recover(reward_proof_data, self.reward_proof_signature) == requesting_address\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the filter params for a specific event of a given channel.", "response": "def get_filter_args_for_specific_event_from_channel(\n        token_network_address: TokenNetworkAddress,\n        channel_identifier: ChannelID,\n        event_name: str,\n        contract_manager: ContractManager,\n        from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,\n        to_block: BlockSpecification = 'latest',\n):\n    \"\"\" Return the filter params for a specific event of a given channel. \"\"\"\n    if not event_name:\n        raise ValueError('Event name must be given')\n\n    event_abi = contract_manager.get_event_abi(CONTRACT_TOKEN_NETWORK, event_name)\n\n    # Here the topics for a specific event are created\n    # The first entry of the topics list is the event name, then the first parameter is encoded,\n    # in the case of a token network, the first parameter is always the channel identifier\n    _, event_filter_params = construct_event_filter_params(\n        event_abi=event_abi,\n        contract_address=to_checksum_address(token_network_address),\n        argument_filters={\n            'channel_identifier': channel_identifier,\n        },\n        fromBlock=from_block,\n        toBlock=to_block,\n    )\n\n    return event_filter_params"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the filter params for all events of a given channel.", "response": "def get_filter_args_for_all_events_from_channel(\n        token_network_address: TokenNetworkAddress,\n        channel_identifier: ChannelID,\n        contract_manager: ContractManager,\n        from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,\n        to_block: BlockSpecification = 'latest',\n) -> Dict:\n    \"\"\" Return the filter params for all events of a given channel. \"\"\"\n\n    event_filter_params = get_filter_args_for_specific_event_from_channel(\n        token_network_address=token_network_address,\n        channel_identifier=channel_identifier,\n        event_name=ChannelEvent.OPENED,\n        contract_manager=contract_manager,\n        from_block=from_block,\n        to_block=to_block,\n    )\n\n    # As we want to get all events for a certain channel we remove the event specific code here\n    # and filter just for the channel identifier\n    # We also have to remove the trailing topics to get all filters\n    event_filter_params['topics'] = [None, event_filter_params['topics'][1]]\n\n    return event_filter_params"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, predicted, target):\n        predicted = predicted.cpu().numpy()\n        target = target.cpu().numpy()\n\n        assert predicted.shape[0] == target.shape[0], \\\n            'number of targets and predicted outputs do not match'\n\n        if np.ndim(predicted) != 1:\n            assert predicted.shape[1] == self.k, \\\n                'number of predictions does not match size of confusion matrix'\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n                'predicted values are not between 1 and k'\n\n        onehot_target = np.ndim(target) != 1\n        if onehot_target:\n            assert target.shape[1] == self.k, \\\n                'Onehot target does not match size of confusion matrix'\n            assert (target >= 0).all() and (target <= 1).all(), \\\n                'in one-hot encoding, target values should be 0 or 1'\n            assert (target.sum(1) == 1).all(), \\\n                'multi-label setting is not supported'\n            target = np.argmax(target, 1)\n        else:\n            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n                'predicted values are not between 0 and k-1'\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.k * target\n        bincount_2d = np.bincount(x.astype(np.int32),\n                                  minlength=self.k ** 2)\n        assert bincount_2d.size == self.k ** 2\n        conf = bincount_2d.reshape((self.k, self.k))\n\n        self.conf += conf", "response": "Adds the predicted and target confusion matrix to the confusion matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef value(self):\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n        else:\n            return self.conf", "response": "Returns the value of the object in the correct format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resample(self, seed=None):\n        if seed is not None:\n            gen = torch.manual_seed(seed)\n        else:\n            gen = torch.default_generator\n\n        if self.replacement:\n            self.perm = torch.LongTensor(len(self)).random_(\n                len(self.dataset), generator=gen)\n        else:\n            self.perm = torch.randperm(\n                len(self.dataset), generator=gen).narrow(0, 0, len(self))", "response": "Resample the dataset.\n\n        Args:\n            seed (int, optional): Seed for resampling. By default no seed is\n            used."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self.scores = torch.FloatTensor(torch.FloatStorage())\n        self.targets = torch.LongTensor(torch.LongStorage())\n        self.weights = torch.FloatTensor(torch.FloatStorage())", "response": "Resets the meter with empty member variables"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new observation to the current object.", "response": "def add(self, output, target, weight=None):\n        \"\"\"Add a new observation\n\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                (eg: a row [0, 1, 0, 1] indicates that the example is\n                associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n\n        \"\"\"\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if weight is not None:\n            if not torch.is_tensor(weight):\n                weight = torch.from_numpy(weight)\n            weight = weight.squeeze()\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert output.dim() == 2, \\\n                'wrong output size (should be 1D or 2D with one column \\\n                per class)'\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert target.dim() == 2, \\\n                'wrong target size (should be 1D or 2D with one column \\\n                per class)'\n        if weight is not None:\n            assert weight.dim() == 1, 'Weight dimension should be 1'\n            assert weight.numel() == target.size(0), \\\n                'Weight dimension 1 should be the same as that of target'\n            assert torch.min(weight) >= 0, 'Weight should be non-negative only'\n        assert torch.equal(target**2, target), \\\n            'targets should be binary (0 or 1)'\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(1), \\\n                'dimensions for output should match previously added examples.'\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            new_weight_size = math.ceil(self.weights.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n            if weight is not None:\n                self.weights.storage().resize_(int(new_weight_size + output.size(0)))\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n\n        if weight is not None:\n            self.weights.resize_(offset + weight.size(0))\n            self.weights.narrow(0, offset, weight.size(0)).copy_(weight)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the model s average precision for each class k Return the average precision for each class k Return 0 if no class k returns 1 if no class k returns 1 if no class k returns 0 if no class k returns 1 if no class k returns 1 if no class k returns 0", "response": "def value(self):\n        \"\"\"Returns the model's average precision for each class\n\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n\n        \"\"\"\n\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        if hasattr(torch, \"arange\"):\n            rg = torch.arange(1, self.scores.size(0) + 1).float()\n        else:\n            rg = torch.range(1, self.scores.size(0)).float()\n        if self.weights.numel() > 0:\n            weight = self.weights.new(self.weights.size())\n            weighted_truth = self.weights.new(self.weights.size())\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n            _, sortind = torch.sort(scores, 0, True)\n            truth = targets[sortind]\n            if self.weights.numel() > 0:\n                weight = self.weights[sortind]\n                weighted_truth = truth.float() * weight\n                rg = weight.cumsum(0)\n\n            # compute true positive sums\n            if self.weights.numel() > 0:\n                tp = weighted_truth.cumsum(0)\n            else:\n                tp = truth.float().cumsum(0)\n\n            # compute precision curve\n            precision = tp.div(rg)\n\n            # compute average precision\n            ap[k] = precision[truth.byte()].sum() / max(float(truth.sum()), 1)\n        return ap"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a backward hook.", "response": "def hook(self, name, state):\n        r\"\"\"Registers a backward hook.\n\n        The hook will be called every time a gradient with respect to the\n        Tensor is computed. The hook should have the following signature::\n\n            hook (grad) -> Tensor or None\n\n        The hook should not modify its argument, but it can optionally return\n        a new gradient which will be used in place of :attr:`grad`.\n        This function returns a handle with a method ``handle.remove()``\n        that removes the hook from the module.\n\n        Example:\n            >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n            >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n            >>> v.backward(torch.tensor([1., 2., 3.]))\n            >>> v.grad\n             2\n             4\n             6\n            [torch.FloatTensor of size (3,)]\n            >>> h.remove()  # removes the hook\n\n        \"\"\"\n        if name in self.hooks:\n            self.hooks[name](state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _viz_prototype(self, vis_fn):\n        ''' Outputs a function which will log the arguments to Visdom in an appropriate way.\n\n            Args:\n                vis_fn: A function, such as self.vis.image\n        '''\n        def _viz_logger(*args, **kwargs):\n            self.win = vis_fn(*args,\n                              win=self.win,\n                              env=self.env,\n                              opts=self.opts,\n                              **kwargs)\n        return _viz_logger", "response": "Returns a function which will log the arguments to Visdom in an appropriate way."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_state(self, state):\n        results = []\n        for field_idx, field in enumerate(self.fields):\n            parent, stat = None, state\n            for f in field:\n                parent, stat = stat, stat[f]\n            results.append(stat)\n        self.log(*results)", "response": "Log the state of the state of the current state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef peek_meter(self):\n        '''Returns a dict of all meters and their values.'''\n        result = {}\n        for key in self.meter.keys():\n            val = self.meter[key].value()\n            val = val[0] if isinstance(val, (list, tuple)) else val\n            result[key] = val\n        return result", "response": "Returns a dict of all meters and their values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the results file with new information.", "response": "def update(self, task_name, result):\n        ''' Update the results file with new information.\n\n        Args:\n            task_name (str): Name of the currently running task. A previously unseen\n                ``task_name`` will create a new entry in both :attr:`tasks`\n                and :attr:`results`.\n            result: This will be appended to the list in :attr:`results` which\n                corresponds to the ``task_name`` in ``task_name``:attr:`tasks`.\n\n        '''\n        with open(self.filepath, 'rb') as f:\n            existing_results = pickle.load(f)\n        if task_name not in self.tasks:\n            self._add_task(task_name)\n            existing_results['tasks'].append(task_name)\n            existing_results['results'].append([])\n        task_name_idx = existing_results['tasks'].index(task_name)\n        results = existing_results['results'][task_name_idx]\n        results.append(result)\n        with open(self.filepath, 'wb') as f:\n            pickle.dump(existing_results, f)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the Issuer of the Logout Response Message", "response": "def get_issuer(self):\n        \"\"\"\n        Gets the Issuer of the Logout Response Message\n        :return: The Issuer\n        :rtype: string\n        \"\"\"\n        issuer = None\n        issuer_nodes = self.__query('/samlp:LogoutResponse/saml:Issuer')\n        if len(issuer_nodes) == 1:\n            issuer = OneLogin_Saml2_XML.element_text(issuer_nodes[0])\n        return issuer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the Status of the current user.", "response": "def get_status(self):\n        \"\"\"\n        Gets the Status\n        :return: The Status\n        :rtype: string\n        \"\"\"\n        entries = self.__query('/samlp:LogoutResponse/samlp:Status/samlp:StatusCode')\n        if len(entries) == 0:\n            return None\n        status = entries[0].attrib['Value']\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the SAML Logout Response is valid.", "response": "def is_valid(self, request_data, request_id=None, raise_exceptions=False):\n        \"\"\"\n        Determines if the SAML LogoutResponse is valid\n        :param request_id: The ID of the LogoutRequest sent by this SP to the IdP\n        :type request_id: string\n\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n\n        :return: Returns if the SAML LogoutResponse is or not valid\n        :rtype: boolean\n        \"\"\"\n        self.__error = None\n        try:\n            idp_data = self.__settings.get_idp_data()\n            idp_entity_id = idp_data['entityId']\n            get_data = request_data['get_data']\n\n            if self.__settings.is_strict():\n                res = OneLogin_Saml2_XML.validate_xml(self.document, 'saml-schema-protocol-2.0.xsd', self.__settings.is_debug_active())\n                if isinstance(res, str):\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd',\n                        OneLogin_Saml2_ValidationError.INVALID_XML_FORMAT\n                    )\n\n                security = self.__settings.get_security_data()\n\n                # Check if the InResponseTo of the Logout Response matches the ID of the Logout Request (requestId) if provided\n                in_response_to = self.document.get('InResponseTo', None)\n                if request_id is not None and in_response_to and in_response_to != request_id:\n                    raise OneLogin_Saml2_ValidationError(\n                        'The InResponseTo of the Logout Response: %s, does not match the ID of the Logout request sent by the SP: %s' % (in_response_to, request_id),\n                        OneLogin_Saml2_ValidationError.WRONG_INRESPONSETO\n                    )\n\n                # Check issuer\n                issuer = self.get_issuer()\n                if issuer is not None and issuer != idp_entity_id:\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid issuer in the Logout Response (expected %(idpEntityId)s, got %(issuer)s)' %\n                        {\n                            'idpEntityId': idp_entity_id,\n                            'issuer': issuer\n                        },\n                        OneLogin_Saml2_ValidationError.WRONG_ISSUER\n                    )\n\n                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)\n\n                # Check destination\n                destination = self.document.get('Destination', None)\n                if destination and current_url not in destination:\n                    raise OneLogin_Saml2_ValidationError(\n                        'The LogoutResponse was received at %s instead of %s' % (current_url, destination),\n                        OneLogin_Saml2_ValidationError.WRONG_DESTINATION\n                    )\n\n                if security['wantMessagesSigned']:\n                    if 'Signature' not in get_data:\n                        raise OneLogin_Saml2_ValidationError(\n                            'The Message of the Logout Response is not signed and the SP require it',\n                            OneLogin_Saml2_ValidationError.NO_SIGNED_MESSAGE\n                        )\n            return True\n        # pylint: disable=R0801\n        except Exception as err:\n            self.__error = str(err)\n            debug = self.__settings.is_debug_active()\n            if debug:\n                print(err)\n            if raise_exceptions:\n                raise\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Logout Response object. :param in_response_to: InResponseTo value for the Logout Response. :type in_response_to: string", "response": "def build(self, in_response_to):\n        \"\"\"\n        Creates a Logout Response object.\n        :param in_response_to: InResponseTo value for the Logout Response.\n        :type in_response_to: string\n        \"\"\"\n        sp_data = self.__settings.get_sp_data()\n        idp_data = self.__settings.get_idp_data()\n\n        uid = OneLogin_Saml2_Utils.generate_unique_id()\n        issue_instant = OneLogin_Saml2_Utils.parse_time_to_SAML(OneLogin_Saml2_Utils.now())\n\n        logout_response = OneLogin_Saml2_Templates.LOGOUT_RESPONSE % \\\n            {\n                'id': uid,\n                'issue_instant': issue_instant,\n                'destination': idp_data['singleLogoutService']['url'],\n                'in_response_to': in_response_to,\n                'entity_id': sp_data['entityId'],\n                'status': \"urn:oasis:names:tc:SAML:2.0:status:Success\"\n            }\n\n        self.__logout_response = logout_response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize an element to an encoded string representation of its XML tree.", "response": "def to_string(xml, **kwargs):\n        \"\"\"\n        Serialize an element to an encoded string representation of its XML tree.\n        :param xml: The root node\n        :type xml: str|bytes|xml.dom.minidom.Document|etree.Element\n        :returns: string representation of xml\n        :rtype: string\n        \"\"\"\n\n        if isinstance(xml, OneLogin_Saml2_XML._text_class):\n            return xml\n\n        if isinstance(xml, OneLogin_Saml2_XML._element_class):\n            OneLogin_Saml2_XML.cleanup_namespaces(xml)\n            return OneLogin_Saml2_XML._unparse_etree(xml, **kwargs)\n\n        raise ValueError(\"unsupported type %r\" % type(xml))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_etree(xml):\n        if isinstance(xml, OneLogin_Saml2_XML._element_class):\n            return xml\n        if isinstance(xml, OneLogin_Saml2_XML._bytes_class):\n            return OneLogin_Saml2_XML._parse_etree(xml, forbid_dtd=True)\n        if isinstance(xml, OneLogin_Saml2_XML._text_class):\n            return OneLogin_Saml2_XML._parse_etree(compat.to_bytes(xml), forbid_dtd=True)\n\n        raise ValueError('unsupported type %r' % type(xml))", "response": "Parses an XML document or fragment from a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate a xml against a schema", "response": "def validate_xml(xml, schema, debug=False):\n        \"\"\"\n        Validates a xml against a schema\n        :param xml: The xml that will be validated\n        :type xml: str|bytes|xml.dom.minidom.Document|etree.Element\n        :param schema: The schema\n        :type schema: string\n        :param debug: If debug is active, the parse-errors will be showed\n        :type debug: bool\n        :returns: Error code or the DomDocument of the xml\n        :rtype: xml.dom.minidom.Document\n        \"\"\"\n\n        assert isinstance(schema, compat.str_type)\n        try:\n            xml = OneLogin_Saml2_XML.to_etree(xml)\n        except Exception as e:\n            if debug:\n                print(e)\n            return 'unloaded_xml'\n\n        schema_file = join(dirname(__file__), 'schemas', schema)\n        with open(schema_file, 'r') as f_schema:\n            xmlschema = OneLogin_Saml2_XML._schema_class(etree.parse(f_schema))\n\n        if not xmlschema.validate(xml):\n            if debug:\n                print('Errors validating the metadata: ')\n                for error in xmlschema.error_log:\n                    print(error.message)\n            return 'invalid_xml'\n        return xml"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(dom, query, context=None, tagid=None):\n        if context is None:\n            source = dom\n        else:\n            source = context\n\n        if tagid is None:\n            return source.xpath(query, namespaces=OneLogin_Saml2_Constants.NSMAP)\n        else:\n            return source.xpath(query, tagid=tagid, namespaces=OneLogin_Saml2_Constants.NSMAP)", "response": "Query the nodes that match the query"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the xmlns : xs namespace intact when etree. cleanup_namespaces is invoked.", "response": "def cleanup_namespaces(tree_or_element, top_nsmap=None, keep_ns_prefixes=None):\n        \"\"\"\n        Keeps the xmlns:xs namespace intact when etree.cleanup_namespaces is invoked.\n        :param tree_or_element: An XML tree or element\n        :type tree_or_element: etree.Element\n        :param top_nsmap: A mapping from namespace prefixes to namespace URIs\n        :type top_nsmap: dict\n        :param keep_ns_prefixes: List of prefixes that should not be removed as part of the cleanup\n        :type keep_ns_prefixes: list\n        :returns: An XML tree or element\n        :rtype: etree.Element\n        \"\"\"\n        all_prefixes_to_keep = [\n            OneLogin_Saml2_Constants.NS_PREFIX_XS\n        ]\n\n        if keep_ns_prefixes:\n            all_prefixes_to_keep = list(set(all_prefixes_to_keep.extend(keep_ns_prefixes)))\n\n        return etree.cleanup_namespaces(tree_or_element, keep_ns_prefixes=all_prefixes_to_keep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the metadata XML from the provided URL and returns a dictionary with extracted data.", "response": "def parse_remote(url, validate_cert=True, entity_id=None, **kwargs):\n        \"\"\"\n        Gets the metadata XML from the provided URL and parse it, returning a dict with extracted data\n        :param url: Url where the XML of the Identity Provider Metadata is published.\n        :type url: string\n\n        :param validate_cert: If the url uses https schema, that flag enables or not the verification of the associated certificate.\n        :type validate_cert: bool\n\n        :param entity_id: Specify the entity_id of the EntityDescriptor that you want to parse a XML\n                          that contains multiple EntityDescriptor.\n        :type entity_id: string\n\n        :returns: settings dict with extracted data\n        :rtype: dict\n        \"\"\"\n        idp_metadata = OneLogin_Saml2_IdPMetadataParser.get_metadata(url, validate_cert)\n        return OneLogin_Saml2_IdPMetadataParser.parse(idp_metadata, entity_id=entity_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_settings(settings, new_metadata_settings):\n        for d in (settings, new_metadata_settings):\n            if not isinstance(d, dict):\n                raise TypeError('Both arguments must be dictionaries.')\n\n        # Guarantee to not modify original data (`settings.copy()` would not\n        # be sufficient, as it's just a shallow copy).\n        result_settings = deepcopy(settings)\n\n        # previously I will take care of cert stuff\n        if 'idp' in new_metadata_settings and 'idp' in result_settings:\n            if new_metadata_settings['idp'].get('x509cert', None) and result_settings['idp'].get('x509certMulti', None):\n                del result_settings['idp']['x509certMulti']\n            if new_metadata_settings['idp'].get('x509certMulti', None) and result_settings['idp'].get('x509cert', None):\n                del result_settings['idp']['x509cert']\n\n        # Merge `new_metadata_settings` into `result_settings`.\n        dict_deep_merge(result_settings, new_metadata_settings)\n        return result_settings", "response": "Merges the current settings with the provided new settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __load_paths(self, base_path=None):\n        if base_path is None:\n            base_path = dirname(dirname(dirname(__file__)))\n        if not base_path.endswith(sep):\n            base_path += sep\n        self.__paths = {\n            'base': base_path,\n            'cert': base_path + 'certs' + sep,\n            'lib': base_path + 'lib' + sep,\n            'extlib': base_path + 'extlib' + sep,\n        }", "response": "Load the paths of the different folders"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the paths of the custom paths if necessary.", "response": "def __update_paths(self, settings):\n        \"\"\"\n        Set custom paths if necessary\n        \"\"\"\n        if not isinstance(settings, dict):\n            return\n\n        if 'custom_base_path' in settings:\n            base_path = settings['custom_base_path']\n            base_path = join(dirname(__file__), base_path)\n            self.__load_paths(base_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __load_settings_from_dict(self, settings):\n        errors = self.check_settings(settings)\n        if len(errors) == 0:\n            self.__errors = []\n            self.__sp = settings['sp']\n            self.__idp = settings.get('idp', {})\n            self.__strict = settings.get('strict', False)\n            self.__debug = settings.get('debug', False)\n            self.__security = settings.get('security', {})\n            self.__contacts = settings.get('contactPerson', {})\n            self.__organization = settings.get('organization', {})\n\n            self.__add_default_values()\n            return True\n\n        self.__errors = errors\n        return False", "response": "Loads the settings info from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the settings from the json file and returns True if the settings info is valid False otherwise", "response": "def __load_settings_from_file(self):\n        \"\"\"\n        Loads settings info from the settings json file\n\n        :returns: True if the settings info is valid\n        :rtype: boolean\n        \"\"\"\n        filename = self.get_base_path() + 'settings.json'\n\n        if not exists(filename):\n            raise OneLogin_Saml2_Error(\n                'Settings file not found: %s',\n                OneLogin_Saml2_Error.SETTINGS_FILE_NOT_FOUND,\n                filename\n            )\n\n        # In the php toolkit instead of being a json file it is a php file and\n        # it is directly included\n        with open(filename, 'r') as json_data:\n            settings = json.loads(json_data.read())\n\n        advanced_filename = self.get_base_path() + 'advanced_settings.json'\n        if exists(advanced_filename):\n            with open(advanced_filename, 'r') as json_data:\n                settings.update(json.loads(json_data.read()))  # Merge settings\n\n        return self.__load_settings_from_dict(settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_settings(self, settings):\n        assert isinstance(settings, dict)\n\n        errors = []\n        if not isinstance(settings, dict) or len(settings) == 0:\n            errors.append('invalid_syntax')\n        else:\n            if not self.__sp_validation_only:\n                errors += self.check_idp_settings(settings)\n            sp_errors = self.check_sp_settings(settings)\n            errors += sp_errors\n\n        return errors", "response": "Checks the settings info."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_idp_settings(self, settings):\n        assert isinstance(settings, dict)\n\n        errors = []\n        if not isinstance(settings, dict) or len(settings) == 0:\n            errors.append('invalid_syntax')\n        else:\n            if not settings.get('idp'):\n                errors.append('idp_not_found')\n            else:\n                idp = settings['idp']\n                if not idp.get('entityId'):\n                    errors.append('idp_entityId_not_found')\n\n                if not idp.get('singleSignOnService', {}).get('url'):\n                    errors.append('idp_sso_not_found')\n                elif not validate_url(idp['singleSignOnService']['url']):\n                    errors.append('idp_sso_url_invalid')\n\n                slo_url = idp.get('singleLogoutService', {}).get('url')\n                if slo_url and not validate_url(slo_url):\n                    errors.append('idp_slo_url_invalid')\n\n                if 'security' in settings:\n                    security = settings['security']\n\n                    exists_x509 = bool(idp.get('x509cert'))\n                    exists_fingerprint = bool(idp.get('certFingerprint'))\n\n                    exists_multix509sign = 'x509certMulti' in idp and \\\n                        'signing' in idp['x509certMulti'] and \\\n                        idp['x509certMulti']['signing']\n                    exists_multix509enc = 'x509certMulti' in idp and \\\n                        'encryption' in idp['x509certMulti'] and \\\n                        idp['x509certMulti']['encryption']\n\n                    want_assert_sign = bool(security.get('wantAssertionsSigned'))\n                    want_mes_signed = bool(security.get('wantMessagesSigned'))\n                    nameid_enc = bool(security.get('nameIdEncrypted'))\n\n                    if (want_assert_sign or want_mes_signed) and \\\n                            not(exists_x509 or exists_fingerprint or exists_multix509sign):\n                        errors.append('idp_cert_or_fingerprint_not_found_and_required')\n                    if nameid_enc and not (exists_x509 or exists_multix509enc):\n                        errors.append('idp_cert_not_found_and_required')\n        return errors", "response": "Checks the IdP settings info."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the x509 certs of the SP exists and are valid.", "response": "def check_sp_certs(self):\n        \"\"\"\n        Checks if the x509 certs of the SP exists and are valid.\n        :returns: If the x509 certs of the SP exists and are valid\n        :rtype: boolean\n        \"\"\"\n        key = self.get_sp_key()\n        cert = self.get_sp_cert()\n        return key is not None and cert is not None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sp_key(self):\n        key = self.__sp.get('privateKey')\n        key_file_name = self.__paths['cert'] + 'sp.key'\n\n        if not key and exists(key_file_name):\n            with open(key_file_name) as f:\n                key = f.read()\n\n        return key or None", "response": "Returns the private key of the SP."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_sp_cert(self):\n        cert = self.__sp.get('x509cert')\n        cert_file_name = self.__paths['cert'] + 'sp.crt'\n\n        if not cert and exists(cert_file_name):\n            with open(cert_file_name) as f:\n                cert = f.read()\n\n        return cert or None", "response": "Returns the x509 public cert of the SP."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sp_metadata(self):\n        metadata = OneLogin_Saml2_Metadata.builder(\n            self.__sp, self.__security['authnRequestsSigned'],\n            self.__security['wantAssertionsSigned'],\n            self.__security['metadataValidUntil'],\n            self.__security['metadataCacheDuration'],\n            self.get_contacts(), self.get_organization()\n        )\n\n        add_encryption = self.__security['wantNameIdEncrypted'] or self.__security['wantAssertionsEncrypted']\n\n        cert_new = self.get_sp_cert_new()\n        metadata = OneLogin_Saml2_Metadata.add_x509_key_descriptors(metadata, cert_new, add_encryption)\n\n        cert = self.get_sp_cert()\n        metadata = OneLogin_Saml2_Metadata.add_x509_key_descriptors(metadata, cert, add_encryption)\n\n        # Sign metadata\n        if 'signMetadata' in self.__security and self.__security['signMetadata'] is not False:\n            if self.__security['signMetadata'] is True:\n                # Use the SP's normal key to sign the metadata:\n                if not cert:\n                    raise OneLogin_Saml2_Error(\n                        'Cannot sign metadata: missing SP public key certificate.',\n                        OneLogin_Saml2_Error.PUBLIC_CERT_FILE_NOT_FOUND\n                    )\n                cert_metadata = cert\n                key_metadata = self.get_sp_key()\n                if not key_metadata:\n                    raise OneLogin_Saml2_Error(\n                        'Cannot sign metadata: missing SP private key.',\n                        OneLogin_Saml2_Error.PRIVATE_KEY_FILE_NOT_FOUND\n                    )\n            else:\n                # Use a custom key to sign the metadata:\n                if ('keyFileName' not in self.__security['signMetadata'] or\n                        'certFileName' not in self.__security['signMetadata']):\n                    raise OneLogin_Saml2_Error(\n                        'Invalid Setting: signMetadata value of the sp is not valid',\n                        OneLogin_Saml2_Error.SETTINGS_INVALID_SYNTAX\n                    )\n                key_file_name = self.__security['signMetadata']['keyFileName']\n                cert_file_name = self.__security['signMetadata']['certFileName']\n                key_metadata_file = self.__paths['cert'] + key_file_name\n                cert_metadata_file = self.__paths['cert'] + cert_file_name\n\n                try:\n                    with open(key_metadata_file, 'r') as f_metadata_key:\n                        key_metadata = f_metadata_key.read()\n                except IOError:\n                    raise OneLogin_Saml2_Error(\n                        'Private key file not readable: %s',\n                        OneLogin_Saml2_Error.PRIVATE_KEY_FILE_NOT_FOUND,\n                        key_metadata_file\n                    )\n\n                try:\n                    with open(cert_metadata_file, 'r') as f_metadata_cert:\n                        cert_metadata = f_metadata_cert.read()\n                except IOError:\n                    raise OneLogin_Saml2_Error(\n                        'Public cert file not readable: %s',\n                        OneLogin_Saml2_Error.PUBLIC_CERT_FILE_NOT_FOUND,\n                        cert_metadata_file\n                    )\n\n            signature_algorithm = self.__security['signatureAlgorithm']\n            digest_algorithm = self.__security['digestAlgorithm']\n\n            metadata = OneLogin_Saml2_Metadata.sign_metadata(metadata, key_metadata, cert_metadata, signature_algorithm, digest_algorithm)\n\n        return metadata", "response": "Gets the SP metadata. The XML representation of the SP is returned."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_metadata(self, xml):\n\n        assert isinstance(xml, compat.text_types)\n\n        if len(xml) == 0:\n            raise Exception('Empty string supplied as input')\n\n        errors = []\n        root = OneLogin_Saml2_XML.validate_xml(xml, 'saml-schema-metadata-2.0.xsd', self.__debug)\n        if isinstance(root, str):\n            errors.append(root)\n        else:\n            if root.tag != '{%s}EntityDescriptor' % OneLogin_Saml2_Constants.NS_MD:\n                errors.append('noEntityDescriptor_xml')\n            else:\n                if (len(root.findall('.//md:SPSSODescriptor', namespaces=OneLogin_Saml2_Constants.NSMAP))) != 1:\n                    errors.append('onlySPSSODescriptor_allowed_xml')\n                else:\n                    valid_until, cache_duration = root.get('validUntil'), root.get('cacheDuration')\n\n                    if valid_until:\n                        valid_until = OneLogin_Saml2_Utils.parse_SAML_to_time(valid_until)\n                    expire_time = OneLogin_Saml2_Utils.get_expire_time(cache_duration, valid_until)\n                    if expire_time is not None and int(time()) > int(expire_time):\n                        errors.append('expired_xml')\n\n        # TODO: Validate Sign\n\n        return errors", "response": "Validates an XML SP Metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_idp_cert_multi(self):\n        if 'x509certMulti' in self.__idp:\n            if 'signing' in self.__idp['x509certMulti']:\n                for idx in range(len(self.__idp['x509certMulti']['signing'])):\n                    self.__idp['x509certMulti']['signing'][idx] = OneLogin_Saml2_Utils.format_cert(self.__idp['x509certMulti']['signing'][idx])\n\n            if 'encryption' in self.__idp['x509certMulti']:\n                for idx in range(len(self.__idp['x509certMulti']['encryption'])):\n                    self.__idp['x509certMulti']['encryption'][idx] = OneLogin_Saml2_Utils.format_cert(self.__idp['x509certMulti']['encryption'][idx])", "response": "Formats the Multple IdP certs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nescapes the non - safe symbols in url", "response": "def escape_url(url, lowercase_urlencoding=False):\n        \"\"\"\n        escape the non-safe symbols in url\n        The encoding used by ADFS 3.0 is not compatible with\n        python's quote_plus (ADFS produces lower case hex numbers and quote_plus produces\n        upper case hex numbers)\n        :param url: the url to escape\n        :type url: str\n\n        :param lowercase_urlencoding: lowercase or no\n        :type lowercase_urlencoding: boolean\n\n        :return: the escaped url\n        :rtype str\n        \"\"\"\n        encoded = quote_plus(url)\n        return re.sub(r\"%[A-F0-9]{2}\", lambda m: m.group(0).lower(), encoded) if lowercase_urlencoding else encoded"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_base64_and_inflate(value, ignore_zip=False):\n        encoded = OneLogin_Saml2_Utils.b64decode(value)\n        try:\n            return zlib.decompress(encoded, -15)\n        except zlib.error:\n            if not ignore_zip:\n                raise\n        return encoded", "response": "base64 decodes and then inflates according to RFC1951\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats a x509 cert into a string.", "response": "def format_cert(cert, heads=True):\n        \"\"\"\n        Returns a x509 cert (adding header & footer if required).\n\n        :param cert: A x509 unformatted cert\n        :type: string\n\n        :param heads: True if we want to include head and footer\n        :type: boolean\n\n        :returns: Formatted cert\n        :rtype: string\n        \"\"\"\n        x509_cert = cert.replace('\\x0D', '')\n        x509_cert = x509_cert.replace('\\r', '')\n        x509_cert = x509_cert.replace('\\n', '')\n        if len(x509_cert) > 0:\n            x509_cert = x509_cert.replace('-----BEGIN CERTIFICATE-----', '')\n            x509_cert = x509_cert.replace('-----END CERTIFICATE-----', '')\n            x509_cert = x509_cert.replace(' ', '')\n\n            if heads:\n                x509_cert = \"-----BEGIN CERTIFICATE-----\\n\" + \"\\n\".join(wrap(x509_cert, 64)) + \"\\n-----END CERTIFICATE-----\\n\"\n\n        return x509_cert"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats a private key for the current user.", "response": "def format_private_key(key, heads=True):\n        \"\"\"\n        Returns a private key (adding header & footer if required).\n\n        :param key A private key\n        :type: string\n\n        :param heads: True if we want to include head and footer\n        :type: boolean\n\n        :returns: Formated private key\n        :rtype: string\n        \"\"\"\n        private_key = key.replace('\\x0D', '')\n        private_key = private_key.replace('\\r', '')\n        private_key = private_key.replace('\\n', '')\n        if len(private_key) > 0:\n            if private_key.find('-----BEGIN PRIVATE KEY-----') != -1:\n                private_key = private_key.replace('-----BEGIN PRIVATE KEY-----', '')\n                private_key = private_key.replace('-----END PRIVATE KEY-----', '')\n                private_key = private_key.replace(' ', '')\n                if heads:\n                    private_key = \"-----BEGIN PRIVATE KEY-----\\n\" + \"\\n\".join(wrap(private_key, 64)) + \"\\n-----END PRIVATE KEY-----\\n\"\n            else:\n                private_key = private_key.replace('-----BEGIN RSA PRIVATE KEY-----', '')\n                private_key = private_key.replace('-----END RSA PRIVATE KEY-----', '')\n                private_key = private_key.replace(' ', '')\n                if heads:\n                    private_key = \"-----BEGIN RSA PRIVATE KEY-----\\n\" + \"\\n\".join(wrap(private_key, 64)) + \"\\n-----END RSA PRIVATE KEY-----\\n\"\n        return private_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the protocol + current host + the port", "response": "def get_self_url_host(request_data):\n        \"\"\"\n        Returns the protocol + the current host + the port (if different than\n        common ports).\n\n        :param request_data: The request as a dict\n        :type: dict\n\n        :return: Url\n        :rtype: string\n        \"\"\"\n        current_host = OneLogin_Saml2_Utils.get_self_host(request_data)\n        port = ''\n        if OneLogin_Saml2_Utils.is_https(request_data):\n            protocol = 'https'\n        else:\n            protocol = 'http'\n\n        if 'server_port' in request_data and request_data['server_port'] is not None:\n            port_number = str(request_data['server_port'])\n            port = ':' + port_number\n\n            if protocol == 'http' and port_number == '80':\n                port = ''\n            elif protocol == 'https' and port_number == '443':\n                port = ''\n\n        return '%s://%s%s' % (protocol, current_host, port)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_https(request_data):\n        is_https = 'https' in request_data and request_data['https'] != 'off'\n        is_https = is_https or ('server_port' in request_data and str(request_data['server_port']) == '443')\n        return is_https", "response": "Checks if https or http.\n        is set to True."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_self_url_no_query(request_data):\n        self_url_host = OneLogin_Saml2_Utils.get_self_url_host(request_data)\n        script_name = request_data['script_name']\n        if script_name:\n            if script_name[0] != '/':\n                script_name = '/' + script_name\n        else:\n            script_name = ''\n        self_url_no_query = self_url_host + script_name\n        if 'path_info' in request_data:\n            self_url_no_query += request_data['path_info']\n\n        return self_url_no_query", "response": "Returns the url of the current host + current view."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_self_routed_url_no_query(request_data):\n        self_url_host = OneLogin_Saml2_Utils.get_self_url_host(request_data)\n        route = ''\n        if 'request_uri' in request_data and request_data['request_uri']:\n            route = request_data['request_uri']\n            if 'query_string' in request_data and request_data['query_string']:\n                route = route.replace(request_data['query_string'], '')\n\n        return self_url_host + route", "response": "Returns the routed URL of the current host + current view."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the URL of the current host + current view + query.", "response": "def get_self_url(request_data):\n        \"\"\"\n        Returns the URL of the current host + current view + query.\n\n        :param request_data: The request as a dict\n        :type: dict\n\n        :return: The url of current host + current view + query\n        :rtype: string\n        \"\"\"\n        self_url_host = OneLogin_Saml2_Utils.get_self_url_host(request_data)\n\n        request_uri = ''\n        if 'request_uri' in request_data:\n            request_uri = request_data['request_uri']\n            if not request_uri.startswith('/'):\n                match = re.search('^https?://[^/]*(/.*)', request_uri)\n                if match is not None:\n                    request_uri = match.groups()[0]\n\n        return self_url_host + request_uri"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_SAML_to_time(timestr):\n        try:\n            data = datetime.strptime(timestr, '%Y-%m-%dT%H:%M:%SZ')\n        except ValueError:\n            data = datetime.strptime(timestr, '%Y-%m-%dT%H:%M:%S.%fZ')\n        return calendar.timegm(data.utctimetuple())", "response": "r Converts a SAML2 timestamp on the form yyyy - mm - ddThh - mm - ss(\\. s+)?Z\n        to a UNIX timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_expire_time(cache_duration=None, valid_until=None):\n        expire_time = None\n\n        if cache_duration is not None:\n            expire_time = OneLogin_Saml2_Utils.parse_duration(cache_duration)\n\n        if valid_until is not None:\n            if isinstance(valid_until, int):\n                valid_until_time = valid_until\n            else:\n                valid_until_time = OneLogin_Saml2_Utils.parse_SAML_to_time(valid_until)\n            if expire_time is None or expire_time > valid_until_time:\n                expire_time = valid_until_time\n\n        if expire_time is not None:\n            return '%d' % expire_time\n        return None", "response": "Returns the expiration time of the cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the fingerprint of a formatted x509 certificate.", "response": "def calculate_x509_fingerprint(x509_cert, alg='sha1'):\n        \"\"\"\n        Calculates the fingerprint of a formatted x509cert.\n\n        :param x509_cert: x509 cert formatted\n        :type: string\n\n        :param alg: The algorithm to build the fingerprint\n        :type: string\n\n        :returns: fingerprint\n        :rtype: string\n        \"\"\"\n        assert isinstance(x509_cert, compat.str_type)\n\n        lines = x509_cert.split('\\n')\n        data = ''\n        inData = False\n\n        for line in lines:\n            # Remove '\\r' from end of line if present.\n            line = line.rstrip()\n            if not inData:\n                if line == '-----BEGIN CERTIFICATE-----':\n                    inData = True\n                elif line == '-----BEGIN PUBLIC KEY-----' or line == '-----BEGIN RSA PRIVATE KEY-----':\n                    # This isn't an X509 certificate.\n                    return None\n            else:\n                if line == '-----END CERTIFICATE-----':\n                    break\n\n                # Append the current line to the certificate data.\n                data += line\n\n        if not data:\n            return None\n\n        decoded_data = base64.b64decode(compat.to_bytes(data))\n\n        if alg == 'sha512':\n            fingerprint = sha512(decoded_data)\n        elif alg == 'sha384':\n            fingerprint = sha384(decoded_data)\n        elif alg == 'sha256':\n            fingerprint = sha256(decoded_data)\n        else:\n            fingerprint = sha1(decoded_data)\n\n        return fingerprint.hexdigest().lower()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a nameID. :param value: fingerprint :type: string :param sp_nq: SP Name Qualifier :type: string :param sp_format: SP Format :type: string :param cert: IdP Public Cert to encrypt the nameID :type: string :param debug: Activate the xmlsec debug :type: bool :returns: DOMElement | XMLSec nameID :rtype: string :param nq: IDP Name Qualifier :type: string", "response": "def generate_name_id(value, sp_nq, sp_format=None, cert=None, debug=False, nq=None):\n        \"\"\"\n        Generates a nameID.\n\n        :param value: fingerprint\n        :type: string\n\n        :param sp_nq: SP Name Qualifier\n        :type: string\n\n        :param sp_format: SP Format\n        :type: string\n\n        :param cert: IdP Public Cert to encrypt the nameID\n        :type: string\n\n        :param debug: Activate the xmlsec debug\n        :type: bool\n\n        :returns: DOMElement | XMLSec nameID\n        :rtype: string\n\n        :param nq: IDP Name Qualifier\n        :type: string\n        \"\"\"\n\n        root = OneLogin_Saml2_XML.make_root(\"{%s}container\" % OneLogin_Saml2_Constants.NS_SAML)\n        name_id = OneLogin_Saml2_XML.make_child(root, '{%s}NameID' % OneLogin_Saml2_Constants.NS_SAML)\n        if sp_nq is not None:\n            name_id.set('SPNameQualifier', sp_nq)\n        if sp_format is not None:\n            name_id.set('Format', sp_format)\n        if nq is not None:\n            name_id.set('NameQualifier', nq)\n        name_id.text = value\n\n        if cert is not None:\n            xmlsec.enable_debug_trace(debug)\n\n            # Load the public cert\n            manager = xmlsec.KeysManager()\n            manager.add_key(xmlsec.Key.from_memory(cert, xmlsec.KeyFormat.CERT_PEM, None))\n\n            # Prepare for encryption\n            enc_data = xmlsec.template.encrypted_data_create(\n                root, xmlsec.Transform.AES128, type=xmlsec.EncryptionType.ELEMENT, ns=\"xenc\")\n\n            xmlsec.template.encrypted_data_ensure_cipher_value(enc_data)\n            key_info = xmlsec.template.encrypted_data_ensure_key_info(enc_data, ns=\"dsig\")\n            enc_key = xmlsec.template.add_encrypted_key(key_info, xmlsec.Transform.RSA_OAEP)\n            xmlsec.template.encrypted_data_ensure_cipher_value(enc_key)\n\n            # Encrypt!\n            enc_ctx = xmlsec.EncryptionContext(manager)\n            enc_ctx.key = xmlsec.Key.generate(xmlsec.KeyData.AES, 128, xmlsec.KeyDataType.SESSION)\n            enc_data = enc_ctx.encrypt_xml(enc_data, name_id)\n            return '<saml:EncryptedID>' + compat.to_string(OneLogin_Saml2_XML.to_string(enc_data)) + '</saml:EncryptedID>'\n        else:\n            return OneLogin_Saml2_XML.extract_tag_text(root, \"saml:NameID\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_sign(xml, key, cert, debug=False, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1, digest_algorithm=OneLogin_Saml2_Constants.SHA1):\n        if xml is None or xml == '':\n            raise Exception('Empty string supplied as input')\n\n        elem = OneLogin_Saml2_XML.to_etree(xml)\n\n        sign_algorithm_transform_map = {\n            OneLogin_Saml2_Constants.DSA_SHA1: xmlsec.Transform.DSA_SHA1,\n            OneLogin_Saml2_Constants.RSA_SHA1: xmlsec.Transform.RSA_SHA1,\n            OneLogin_Saml2_Constants.RSA_SHA256: xmlsec.Transform.RSA_SHA256,\n            OneLogin_Saml2_Constants.RSA_SHA384: xmlsec.Transform.RSA_SHA384,\n            OneLogin_Saml2_Constants.RSA_SHA512: xmlsec.Transform.RSA_SHA512\n        }\n        sign_algorithm_transform = sign_algorithm_transform_map.get(sign_algorithm, xmlsec.Transform.RSA_SHA1)\n\n        signature = xmlsec.template.create(elem, xmlsec.Transform.EXCL_C14N, sign_algorithm_transform, ns='ds')\n\n        issuer = OneLogin_Saml2_XML.query(elem, '//saml:Issuer')\n        if len(issuer) > 0:\n            issuer = issuer[0]\n            issuer.addnext(signature)\n            elem_to_sign = issuer.getparent()\n        else:\n            entity_descriptor = OneLogin_Saml2_XML.query(elem, '//md:EntityDescriptor')\n            if len(entity_descriptor) > 0:\n                elem.insert(0, signature)\n            else:\n                elem[0].insert(0, signature)\n            elem_to_sign = elem\n\n        elem_id = elem_to_sign.get('ID', None)\n        if elem_id is not None:\n            if elem_id:\n                elem_id = '#' + elem_id\n        else:\n            generated_id = generated_id = OneLogin_Saml2_Utils.generate_unique_id()\n            elem_id = '#' + generated_id\n            elem_to_sign.attrib['ID'] = generated_id\n\n        xmlsec.enable_debug_trace(debug)\n        xmlsec.tree.add_ids(elem_to_sign, [\"ID\"])\n\n        digest_algorithm_transform_map = {\n            OneLogin_Saml2_Constants.SHA1: xmlsec.Transform.SHA1,\n            OneLogin_Saml2_Constants.SHA256: xmlsec.Transform.SHA256,\n            OneLogin_Saml2_Constants.SHA384: xmlsec.Transform.SHA384,\n            OneLogin_Saml2_Constants.SHA512: xmlsec.Transform.SHA512\n        }\n        digest_algorithm_transform = digest_algorithm_transform_map.get(digest_algorithm, xmlsec.Transform.SHA1)\n\n        ref = xmlsec.template.add_reference(signature, digest_algorithm_transform, uri=elem_id)\n        xmlsec.template.add_transform(ref, xmlsec.Transform.ENVELOPED)\n        xmlsec.template.add_transform(ref, xmlsec.Transform.EXCL_C14N)\n        key_info = xmlsec.template.ensure_key_info(signature)\n        xmlsec.template.add_x509_data(key_info)\n\n        dsig_ctx = xmlsec.SignatureContext()\n        sign_key = xmlsec.Key.from_memory(key, xmlsec.KeyFormat.PEM, None)\n        sign_key.load_cert_from_memory(cert, xmlsec.KeyFormat.PEM)\n\n        dsig_ctx.key = sign_key\n        dsig_ctx.sign(signature)\n\n        return OneLogin_Saml2_XML.to_string(elem)", "response": "Adds a signature key and senders certificate to an XML element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating a signature of a message or assertion.", "response": "def validate_sign(xml, cert=None, fingerprint=None, fingerprintalg='sha1', validatecert=False, debug=False, xpath=None, multicerts=None):\n        \"\"\"\n        Validates a signature (Message or Assertion).\n\n        :param xml: The element we should validate\n        :type: string | Document\n\n        :param cert: The public cert\n        :type: string\n\n        :param fingerprint: The fingerprint of the public cert\n        :type: string\n\n        :param fingerprintalg: The algorithm used to build the fingerprint\n        :type: string\n\n        :param validatecert: If true, will verify the signature and if the cert is valid.\n        :type: bool\n\n        :param debug: Activate the xmlsec debug\n        :type: bool\n\n        :param xpath: The xpath of the signed element\n        :type: string\n\n        :param multicerts: Multiple public certs\n        :type: list\n\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n        \"\"\"\n        if xml is None or xml == '':\n            raise Exception('Empty string supplied as input')\n\n        elem = OneLogin_Saml2_XML.to_etree(xml)\n        xmlsec.enable_debug_trace(debug)\n        xmlsec.tree.add_ids(elem, [\"ID\"])\n\n        if xpath:\n            signature_nodes = OneLogin_Saml2_XML.query(elem, xpath)\n        else:\n            signature_nodes = OneLogin_Saml2_XML.query(elem, OneLogin_Saml2_Utils.RESPONSE_SIGNATURE_XPATH)\n\n            if len(signature_nodes) == 0:\n                signature_nodes = OneLogin_Saml2_XML.query(elem, OneLogin_Saml2_Utils.ASSERTION_SIGNATURE_XPATH)\n\n        if len(signature_nodes) == 1:\n            signature_node = signature_nodes[0]\n\n            if not multicerts:\n                return OneLogin_Saml2_Utils.validate_node_sign(signature_node, elem, cert, fingerprint, fingerprintalg, validatecert, debug, raise_exceptions=True)\n            else:\n                # If multiple certs are provided, I may ignore cert and\n                # fingerprint provided by the method and just check the\n                # certs multicerts\n                fingerprint = fingerprintalg = None\n                for cert in multicerts:\n                    if OneLogin_Saml2_Utils.validate_node_sign(signature_node, elem, cert, fingerprint, fingerprintalg, validatecert, False, raise_exceptions=False):\n                        return True\n                raise OneLogin_Saml2_ValidationError(\n                    'Signature validation failed. SAML Response rejected.',\n                    OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                )\n        else:\n            raise OneLogin_Saml2_ValidationError(\n                'Expected exactly one signature node; got {}.'.format(len(signature_nodes)),\n                OneLogin_Saml2_ValidationError.WRONG_NUMBER_OF_SIGNATURES\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_metadata_sign(xml, cert=None, fingerprint=None, fingerprintalg='sha1', validatecert=False, debug=False):\n        if xml is None or xml == '':\n            raise Exception('Empty string supplied as input')\n\n        elem = OneLogin_Saml2_XML.to_etree(xml)\n        xmlsec.enable_debug_trace(debug)\n        xmlsec.tree.add_ids(elem, [\"ID\"])\n\n        signature_nodes = OneLogin_Saml2_XML.query(elem, '/md:EntitiesDescriptor/ds:Signature')\n\n        if len(signature_nodes) == 0:\n            signature_nodes += OneLogin_Saml2_XML.query(elem, '/md:EntityDescriptor/ds:Signature')\n\n            if len(signature_nodes) == 0:\n                signature_nodes += OneLogin_Saml2_XML.query(elem, '/md:EntityDescriptor/md:SPSSODescriptor/ds:Signature')\n                signature_nodes += OneLogin_Saml2_XML.query(elem, '/md:EntityDescriptor/md:IDPSSODescriptor/ds:Signature')\n\n        if len(signature_nodes) > 0:\n            for signature_node in signature_nodes:\n                # Raises expection if invalid\n                OneLogin_Saml2_Utils.validate_node_sign(signature_node, elem, cert, fingerprint, fingerprintalg, validatecert, debug, raise_exceptions=True)\n            return True\n        else:\n            raise Exception('Could not validate metadata signature: No signature nodes found.')", "response": "Validates a metadata signature of a EntityDescriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_node_sign(signature_node, elem, cert=None, fingerprint=None, fingerprintalg='sha1', validatecert=False, debug=False):\n        if (cert is None or cert == '') and fingerprint:\n            x509_certificate_nodes = OneLogin_Saml2_XML.query(signature_node, '//ds:Signature/ds:KeyInfo/ds:X509Data/ds:X509Certificate')\n            if len(x509_certificate_nodes) > 0:\n                x509_certificate_node = x509_certificate_nodes[0]\n                x509_cert_value = OneLogin_Saml2_XML.element_text(x509_certificate_node)\n                x509_cert_value_formatted = OneLogin_Saml2_Utils.format_cert(x509_cert_value)\n                x509_fingerprint_value = OneLogin_Saml2_Utils.calculate_x509_fingerprint(x509_cert_value_formatted, fingerprintalg)\n                if fingerprint == x509_fingerprint_value:\n                    cert = x509_cert_value_formatted\n\n        if cert is None or cert == '':\n            raise OneLogin_Saml2_Error(\n                'Could not validate node signature: No certificate provided.',\n                OneLogin_Saml2_Error.CERT_NOT_FOUND\n            )\n\n        # Check if Reference URI is empty\n        reference_elem = OneLogin_Saml2_XML.query(signature_node, '//ds:Reference')\n        if len(reference_elem) > 0:\n            if reference_elem[0].get('URI') == '':\n                reference_elem[0].set('URI', '#%s' % signature_node.getparent().get('ID'))\n\n        if validatecert:\n            manager = xmlsec.KeysManager()\n            manager.load_cert_from_memory(cert, xmlsec.KeyFormat.CERT_PEM, xmlsec.KeyDataType.TRUSTED)\n            dsig_ctx = xmlsec.SignatureContext(manager)\n        else:\n            dsig_ctx = xmlsec.SignatureContext()\n            dsig_ctx.key = xmlsec.Key.from_memory(cert, xmlsec.KeyFormat.CERT_PEM, None)\n\n        dsig_ctx.set_enabled_key_data([xmlsec.KeyData.X509])\n\n        try:\n            dsig_ctx.verify(signature_node)\n        except Exception as err:\n            raise OneLogin_Saml2_ValidationError(\n                'Signature validation failed. SAML Response rejected. %s',\n                OneLogin_Saml2_ValidationError.INVALID_SIGNATURE,\n                str(err)\n            )\n\n        return True", "response": "Validates a signature node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsign a binary message.", "response": "def sign_binary(msg, key, algorithm=xmlsec.Transform.RSA_SHA1, debug=False):\n        \"\"\"\n        Sign binary message\n\n        :param msg: The element we should validate\n        :type: bytes\n\n        :param key: The private key\n        :type: string\n\n        :param debug: Activate the xmlsec debug\n        :type: bool\n\n        :return signed message\n        :rtype str\n        \"\"\"\n\n        if isinstance(msg, str):\n            msg = msg.encode('utf8')\n\n        xmlsec.enable_debug_trace(debug)\n        dsig_ctx = xmlsec.SignatureContext()\n        dsig_ctx.key = xmlsec.Key.from_memory(key, xmlsec.KeyFormat.PEM, None)\n        return dsig_ctx.sign_binary(compat.to_bytes(msg), algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_binary_sign(signed_query, signature, cert=None, algorithm=OneLogin_Saml2_Constants.RSA_SHA1, debug=False):\n        try:\n            xmlsec.enable_debug_trace(debug)\n            dsig_ctx = xmlsec.SignatureContext()\n            dsig_ctx.key = xmlsec.Key.from_memory(cert, xmlsec.KeyFormat.CERT_PEM, None)\n\n            sign_algorithm_transform_map = {\n                OneLogin_Saml2_Constants.DSA_SHA1: xmlsec.Transform.DSA_SHA1,\n                OneLogin_Saml2_Constants.RSA_SHA1: xmlsec.Transform.RSA_SHA1,\n                OneLogin_Saml2_Constants.RSA_SHA256: xmlsec.Transform.RSA_SHA256,\n                OneLogin_Saml2_Constants.RSA_SHA384: xmlsec.Transform.RSA_SHA384,\n                OneLogin_Saml2_Constants.RSA_SHA512: xmlsec.Transform.RSA_SHA512\n            }\n            sign_algorithm_transform = sign_algorithm_transform_map.get(algorithm, xmlsec.Transform.RSA_SHA1)\n\n            dsig_ctx.verify_binary(compat.to_bytes(signed_query),\n                                   sign_algorithm_transform,\n                                   compat.to_bytes(signature))\n            return True\n        except xmlsec.Error as e:\n            if debug:\n                print(e)\n            return False", "response": "Validates the binary data for the current version of the XML - SEC - 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(global_config, **settings):\n    config = Configurator(settings=settings)\n    config.set_session_factory(session_factory)\n    config.include('pyramid_jinja2')\n    config.add_static_view('static', 'static', cache_max_age=3600)\n    config.add_route('index', '/')\n    config.add_route('attrs', '/attrs/')\n    config.add_route('metadata', '/metadata/')\n    config.scan()\n    return config.make_wsgi_app()", "response": "This function returns a Pyramid WSGI application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the strict mode active or disable the nagle locale.", "response": "def set_strict(self, value):\n        \"\"\"\n        Set the strict mode active/disable\n\n        :param value:\n        :type value: bool\n        \"\"\"\n        assert isinstance(value, bool)\n        self.__settings.set_strict(value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_response(self, request_id=None):\n        self.__errors = []\n        self.__error_reason = None\n\n        if 'post_data' in self.__request_data and 'SAMLResponse' in self.__request_data['post_data']:\n            # AuthnResponse -- HTTP_POST Binding\n            response = OneLogin_Saml2_Response(self.__settings, self.__request_data['post_data']['SAMLResponse'])\n            self.__last_response = response.get_xml_document()\n\n            if response.is_valid(self.__request_data, request_id):\n                self.__attributes = response.get_attributes()\n                self.__nameid = response.get_nameid()\n                self.__nameid_format = response.get_nameid_format()\n                self.__session_index = response.get_session_index()\n                self.__session_expiration = response.get_session_not_on_or_after()\n                self.__last_message_id = response.get_id()\n                self.__last_assertion_id = response.get_assertion_id()\n                self.__last_authn_contexts = response.get_authn_contexts()\n                self.__authenticated = True\n                self.__last_assertion_not_on_or_after = response.get_assertion_not_on_or_after()\n\n            else:\n                self.__errors.append('invalid_response')\n                self.__error_reason = response.get_error()\n\n        else:\n            self.__errors.append('invalid_binding')\n            raise OneLogin_Saml2_Error(\n                'SAML Response not found, Only supported HTTP_POST Binding',\n                OneLogin_Saml2_Error.SAML_RESPONSE_NOT_FOUND\n            )", "response": "Processes the SAML Response sent by the IdP."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_slo(self, keep_local_session=False, request_id=None, delete_session_cb=None):\n        self.__errors = []\n        self.__error_reason = None\n\n        get_data = 'get_data' in self.__request_data and self.__request_data['get_data']\n        if get_data and 'SAMLResponse' in get_data:\n            logout_response = OneLogin_Saml2_Logout_Response(self.__settings, get_data['SAMLResponse'])\n            self.__last_response = logout_response.get_xml()\n            if not self.validate_response_signature(get_data):\n                self.__errors.append('invalid_logout_response_signature')\n                self.__errors.append('Signature validation failed. Logout Response rejected')\n            elif not logout_response.is_valid(self.__request_data, request_id):\n                self.__errors.append('invalid_logout_response')\n                self.__error_reason = logout_response.get_error()\n            elif logout_response.get_status() != OneLogin_Saml2_Constants.STATUS_SUCCESS:\n                self.__errors.append('logout_not_success')\n            else:\n                self.__last_message_id = logout_response.id\n                if not keep_local_session:\n                    OneLogin_Saml2_Utils.delete_local_session(delete_session_cb)\n\n        elif get_data and 'SAMLRequest' in get_data:\n            logout_request = OneLogin_Saml2_Logout_Request(self.__settings, get_data['SAMLRequest'])\n            self.__last_request = logout_request.get_xml()\n            if not self.validate_request_signature(get_data):\n                self.__errors.append(\"invalid_logout_request_signature\")\n                self.__errors.append('Signature validation failed. Logout Request rejected')\n            elif not logout_request.is_valid(self.__request_data):\n                self.__errors.append('invalid_logout_request')\n                self.__error_reason = logout_request.get_error()\n            else:\n                if not keep_local_session:\n                    OneLogin_Saml2_Utils.delete_local_session(delete_session_cb)\n\n                in_response_to = logout_request.id\n                self.__last_message_id = logout_request.id\n                response_builder = OneLogin_Saml2_Logout_Response(self.__settings)\n                response_builder.build(in_response_to)\n                self.__last_response = response_builder.get_xml()\n                logout_response = response_builder.get_response()\n\n                parameters = {'SAMLResponse': logout_response}\n                if 'RelayState' in self.__request_data['get_data']:\n                    parameters['RelayState'] = self.__request_data['get_data']['RelayState']\n\n                security = self.__settings.get_security_data()\n                if security['logoutResponseSigned']:\n                    self.add_response_signature(parameters, security['signatureAlgorithm'])\n\n                return self.redirect_to(self.get_slo_url(), parameters)\n        else:\n            self.__errors.append('invalid_binding')\n            raise OneLogin_Saml2_Error(\n                'SAML LogoutRequest/LogoutResponse not found. Only supported HTTP_REDIRECT Binding',\n                OneLogin_Saml2_Error.SAML_LOGOUTMESSAGE_NOT_FOUND\n            )", "response": "Processes the SAML Logout Response and Logout Request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nredirect the user to the specified URL or to the URL that we defined in our SSO Request.", "response": "def redirect_to(self, url=None, parameters={}):\n        \"\"\"\n        Redirects the user to the URL passed by parameter or to the URL that we defined in our SSO Request.\n\n        :param url: The target URL to redirect the user\n        :type url: string\n        :param parameters: Extra parameters to be passed as part of the URL\n        :type parameters: dict\n\n        :returns: Redirection URL\n        \"\"\"\n        if url is None and 'RelayState' in self.__request_data['get_data']:\n            url = self.__request_data['get_data']['RelayState']\n        return OneLogin_Saml2_Utils.redirect(url, parameters, request_data=self.__request_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_attribute(self, name):\n        assert isinstance(name, compat.str_type)\n        return self.__attributes.get(name)", "response": "Returns the requested SAML attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the Signature of the SAML Request.", "response": "def add_request_signature(self, request_data, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1):\n        \"\"\"\n        Builds the Signature of the SAML Request.\n\n        :param request_data: The Request parameters\n        :type request_data: dict\n\n        :param sign_algorithm: Signature algorithm method\n        :type sign_algorithm: string\n        \"\"\"\n        return self.__build_signature(request_data, 'SAMLRequest', sign_algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the Signature of the SAML Response.", "response": "def add_response_signature(self, response_data, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1):\n        \"\"\"\n        Builds the Signature of the SAML Response.\n        :param response_data: The Response parameters\n        :type response_data: dict\n\n        :param sign_algorithm: Signature algorithm method\n        :type sign_algorithm: string\n        \"\"\"\n        return self.__build_signature(response_data, 'SAMLResponse', sign_algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the sign query string for the current session.", "response": "def __build_sign_query(saml_data, relay_state, algorithm, saml_type, lowercase_urlencoding=False):\n        \"\"\"\n        Build sign query\n\n        :param saml_data: The Request data\n        :type saml_data: str\n\n        :param relay_state: The Relay State\n        :type relay_state: str\n\n        :param algorithm: The Signature Algorithm\n        :type algorithm: str\n\n        :param saml_type: The target URL the user should be redirected to\n        :type saml_type: string  SAMLRequest | SAMLResponse\n\n        :param lowercase_urlencoding: lowercase or no\n        :type lowercase_urlencoding: boolean\n        \"\"\"\n        sign_data = ['%s=%s' % (saml_type, OneLogin_Saml2_Utils.escape_url(saml_data, lowercase_urlencoding))]\n        if relay_state is not None:\n            sign_data.append('RelayState=%s' % OneLogin_Saml2_Utils.escape_url(relay_state, lowercase_urlencoding))\n        sign_data.append('SigAlg=%s' % OneLogin_Saml2_Utils.escape_url(algorithm, lowercase_urlencoding))\n        return '&'.join(sign_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __build_signature(self, data, saml_type, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1):\n        assert saml_type in ('SAMLRequest', 'SAMLResponse')\n        key = self.get_settings().get_sp_key()\n\n        if not key:\n            raise OneLogin_Saml2_Error(\n                \"Trying to sign the %s but can't load the SP private key.\" % saml_type,\n                OneLogin_Saml2_Error.PRIVATE_KEY_NOT_FOUND\n            )\n\n        msg = self.__build_sign_query(data[saml_type],\n                                      data.get('RelayState', None),\n                                      sign_algorithm,\n                                      saml_type)\n\n        sign_algorithm_transform_map = {\n            OneLogin_Saml2_Constants.DSA_SHA1: xmlsec.Transform.DSA_SHA1,\n            OneLogin_Saml2_Constants.RSA_SHA1: xmlsec.Transform.RSA_SHA1,\n            OneLogin_Saml2_Constants.RSA_SHA256: xmlsec.Transform.RSA_SHA256,\n            OneLogin_Saml2_Constants.RSA_SHA384: xmlsec.Transform.RSA_SHA384,\n            OneLogin_Saml2_Constants.RSA_SHA512: xmlsec.Transform.RSA_SHA512\n        }\n        sign_algorithm_transform = sign_algorithm_transform_map.get(sign_algorithm, xmlsec.Transform.RSA_SHA1)\n\n        signature = OneLogin_Saml2_Utils.sign_binary(msg, key, sign_algorithm_transform, self.__settings.is_debug_active())\n        data['Signature'] = OneLogin_Saml2_Utils.b64encode(signature)\n        data['SigAlg'] = sign_algorithm", "response": "Builds the Signature for the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __validate_signature(self, data, saml_type, raise_exceptions=False):\n        try:\n            signature = data.get('Signature', None)\n            if signature is None:\n                if self.__settings.is_strict() and self.__settings.get_security_data().get('wantMessagesSigned', False):\n                    raise OneLogin_Saml2_ValidationError(\n                        'The %s is not signed. Rejected.' % saml_type,\n                        OneLogin_Saml2_ValidationError.NO_SIGNED_MESSAGE\n                    )\n                return True\n\n            idp_data = self.get_settings().get_idp_data()\n\n            exists_x509cert = 'x509cert' in idp_data and idp_data['x509cert']\n            exists_multix509sign = 'x509certMulti' in idp_data and \\\n                'signing' in idp_data['x509certMulti'] and \\\n                idp_data['x509certMulti']['signing']\n\n            if not (exists_x509cert or exists_multix509sign):\n                error_msg = 'In order to validate the sign on the %s, the x509cert of the IdP is required' % saml_type\n                self.__errors.append(error_msg)\n                raise OneLogin_Saml2_Error(\n                    error_msg,\n                    OneLogin_Saml2_Error.CERT_NOT_FOUND\n                )\n\n            sign_alg = data.get('SigAlg', OneLogin_Saml2_Constants.RSA_SHA1)\n            if isinstance(sign_alg, bytes):\n                sign_alg = sign_alg.decode('utf8')\n\n            lowercase_urlencoding = False\n            if 'lowercase_urlencoding' in self.__request_data.keys():\n                lowercase_urlencoding = self.__request_data['lowercase_urlencoding']\n\n            signed_query = self.__build_sign_query(data[saml_type],\n                                                   data.get('RelayState', None),\n                                                   sign_alg,\n                                                   saml_type,\n                                                   lowercase_urlencoding\n                                                   )\n\n            if exists_multix509sign:\n                for cert in idp_data['x509certMulti']['signing']:\n                    if OneLogin_Saml2_Utils.validate_binary_sign(signed_query,\n                                                                 OneLogin_Saml2_Utils.b64decode(signature),\n                                                                 cert,\n                                                                 sign_alg):\n                        return True\n                raise OneLogin_Saml2_ValidationError(\n                    'Signature validation failed. %s rejected' % saml_type,\n                    OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                )\n            else:\n                cert = idp_data['x509cert']\n\n                if not OneLogin_Saml2_Utils.validate_binary_sign(signed_query,\n                                                                 OneLogin_Saml2_Utils.b64decode(signature),\n                                                                 cert,\n                                                                 sign_alg,\n                                                                 self.__settings.is_debug_active()):\n                    raise OneLogin_Saml2_ValidationError(\n                        'Signature validation failed. %s rejected' % saml_type,\n                        OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                    )\n            return True\n        except Exception as e:\n            self.__error_reason = str(e)\n            if raise_exceptions:\n                raise e\n            return False", "response": "Validate the Signature of the request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_last_response_xml(self, pretty_print_if_possible=False):\n        response = None\n        if self.__last_response is not None:\n            if isinstance(self.__last_response, compat.str_type):\n                response = self.__last_response\n            else:\n                response = tostring(self.__last_response, encoding='unicode', pretty_print=pretty_print_if_possible)\n        return response", "response": "Retrieves the raw XML of the last SAML response generated or processed XML."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nameid_data(request, key=None):\n        elem = OneLogin_Saml2_XML.to_etree(request)\n        name_id = None\n        encrypted_entries = OneLogin_Saml2_XML.query(elem, '/samlp:LogoutRequest/saml:EncryptedID')\n\n        if len(encrypted_entries) == 1:\n            if key is None:\n                raise OneLogin_Saml2_Error(\n                    'Private Key is required in order to decrypt the NameID, check settings',\n                    OneLogin_Saml2_Error.PRIVATE_KEY_NOT_FOUND\n                )\n\n            encrypted_data_nodes = OneLogin_Saml2_XML.query(elem, '/samlp:LogoutRequest/saml:EncryptedID/xenc:EncryptedData')\n            if len(encrypted_data_nodes) == 1:\n                encrypted_data = encrypted_data_nodes[0]\n                name_id = OneLogin_Saml2_Utils.decrypt_element(encrypted_data, key)\n        else:\n            entries = OneLogin_Saml2_XML.query(elem, '/samlp:LogoutRequest/saml:NameID')\n            if len(entries) == 1:\n                name_id = entries[0]\n\n        if name_id is None:\n            raise OneLogin_Saml2_ValidationError(\n                'NameID not found in the Logout Request',\n                OneLogin_Saml2_ValidationError.NO_NAMEID\n            )\n\n        name_id_data = {\n            'Value': OneLogin_Saml2_XML.element_text(name_id)\n        }\n        for attr in ['Format', 'SPNameQualifier', 'NameQualifier']:\n            if attr in name_id.attrib:\n                name_id_data[attr] = name_id.attrib[attr]\n\n        return name_id_data", "response": "Gets the NameID Data of the Logout Request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the NameID Format of the Logout Request Message", "response": "def get_nameid_format(request, key=None):\n        \"\"\"\n        Gets the NameID Format of the Logout Request Message\n        :param request: Logout Request Message\n        :type request: string|DOMDocument\n        :param key: The SP key\n        :type key: string\n        :return: Name ID Format\n        :rtype: string\n        \"\"\"\n        name_id_format = None\n        name_id_data = OneLogin_Saml2_Logout_Request.get_nameid_data(request, key)\n        if name_id_data and 'Format' in name_id_data.keys():\n            name_id_format = name_id_data['Format']\n        return name_id_format"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_issuer(request):\n\n        elem = OneLogin_Saml2_XML.to_etree(request)\n        issuer = None\n        issuer_nodes = OneLogin_Saml2_XML.query(elem, '/samlp:LogoutRequest/saml:Issuer')\n        if len(issuer_nodes) == 1:\n            issuer = OneLogin_Saml2_XML.element_text(issuer_nodes[0])\n        return issuer", "response": "Gets the Issuer of the Logout Request Message\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the SessionIndexes from the Logout Request.", "response": "def get_session_indexes(request):\n        \"\"\"\n        Gets the SessionIndexes from the Logout Request\n        :param request: Logout Request Message\n        :type request: string|DOMDocument\n        :return: The SessionIndex value\n        :rtype: list\n        \"\"\"\n\n        elem = OneLogin_Saml2_XML.to_etree(request)\n        session_indexes = []\n        session_index_nodes = OneLogin_Saml2_XML.query(elem, '/samlp:LogoutRequest/samlp:SessionIndex')\n        for session_index_node in session_index_nodes:\n            session_indexes.append(OneLogin_Saml2_XML.element_text(session_index_node))\n        return session_indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_valid(self, request_data, raise_exceptions=False):\n        self.__error = None\n        try:\n            root = OneLogin_Saml2_XML.to_etree(self.__logout_request)\n\n            idp_data = self.__settings.get_idp_data()\n            idp_entity_id = idp_data['entityId']\n\n            get_data = ('get_data' in request_data and request_data['get_data']) or dict()\n\n            if self.__settings.is_strict():\n                res = OneLogin_Saml2_XML.validate_xml(root, 'saml-schema-protocol-2.0.xsd', self.__settings.is_debug_active())\n                if isinstance(res, str):\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd',\n                        OneLogin_Saml2_ValidationError.INVALID_XML_FORMAT\n                    )\n\n                security = self.__settings.get_security_data()\n\n                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)\n\n                # Check NotOnOrAfter\n                if root.get('NotOnOrAfter', None):\n                    na = OneLogin_Saml2_Utils.parse_SAML_to_time(root.get('NotOnOrAfter'))\n                    if na <= OneLogin_Saml2_Utils.now():\n                        raise OneLogin_Saml2_ValidationError(\n                            'Could not validate timestamp: expired. Check system clock.)',\n                            OneLogin_Saml2_ValidationError.RESPONSE_EXPIRED\n                        )\n\n                # Check destination\n                if root.get('Destination', None):\n                    destination = root.get('Destination')\n                    if destination != '':\n                        if current_url not in destination:\n                            raise OneLogin_Saml2_ValidationError(\n                                'The LogoutRequest was received at '\n                                '%(currentURL)s instead of %(destination)s' %\n                                {\n                                    'currentURL': current_url,\n                                    'destination': destination,\n                                },\n                                OneLogin_Saml2_ValidationError.WRONG_DESTINATION\n                            )\n\n                # Check issuer\n                issuer = OneLogin_Saml2_Logout_Request.get_issuer(root)\n                if issuer is not None and issuer != idp_entity_id:\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid issuer in the Logout Request (expected %(idpEntityId)s, got %(issuer)s)' %\n                        {\n                            'idpEntityId': idp_entity_id,\n                            'issuer': issuer\n                        },\n                        OneLogin_Saml2_ValidationError.WRONG_ISSUER\n                    )\n\n                if security['wantMessagesSigned']:\n                    if 'Signature' not in get_data:\n                        raise OneLogin_Saml2_ValidationError(\n                            'The Message of the Logout Request is not signed and the SP require it',\n                            OneLogin_Saml2_ValidationError.NO_SIGNED_MESSAGE\n                        )\n\n            return True\n        except Exception as err:\n            # pylint: disable=R0801\n            self.__error = str(err)\n            debug = self.__settings.is_debug_active()\n            if debug:\n                print(err)\n            if raise_exceptions:\n                raise\n            return False", "response": "Checks if the Logout Request received is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the status of the response is Success or not.", "response": "def check_status(self):\n        \"\"\"\n        Check if the status of the response is success or not\n\n        :raises: Exception. If the status is not success\n        \"\"\"\n        status = OneLogin_Saml2_Utils.get_status(self.document)\n        code = status.get('code', None)\n        if code and code != OneLogin_Saml2_Constants.STATUS_SUCCESS:\n            splited_code = code.split(':')\n            printable_code = splited_code.pop()\n            status_exception_msg = 'The status code of the Response was not Success, was %s' % printable_code\n            status_msg = status.get('msg', None)\n            if status_msg:\n                status_exception_msg += ' -> ' + status_msg\n            raise OneLogin_Saml2_ValidationError(\n                status_exception_msg,\n                OneLogin_Saml2_ValidationError.STATUS_CODE_IS_NOT_SUCCESS\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the valid audiences for the SAML Response", "response": "def get_audiences(self):\n        \"\"\"\n        Gets the audiences\n\n        :returns: The valid audiences for the SAML Response\n        :rtype: list\n        \"\"\"\n        audience_nodes = self.__query_assertion('/saml:Conditions/saml:AudienceRestriction/saml:Audience')\n        return [OneLogin_Saml2_XML.element_text(node) for node in audience_nodes if OneLogin_Saml2_XML.element_text(node) is not None]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_authn_contexts(self):\n        authn_context_nodes = self.__query_assertion('/saml:AuthnStatement/saml:AuthnContext/saml:AuthnContextClassRef')\n        return [OneLogin_Saml2_XML.element_text(node) for node in authn_context_nodes]", "response": "Gets the authentication contexts for the SAML Response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the issuers from the message and assertion.", "response": "def get_issuers(self):\n        \"\"\"\n        Gets the issuers (from message and from assertion)\n\n        :returns: The issuers\n        :rtype: list\n        \"\"\"\n        issuers = set()\n\n        message_issuer_nodes = OneLogin_Saml2_XML.query(self.document, '/samlp:Response/saml:Issuer')\n        if len(message_issuer_nodes) > 0:\n            if len(message_issuer_nodes) == 1:\n                issuer_value = OneLogin_Saml2_XML.element_text(message_issuer_nodes[0])\n                if issuer_value:\n                    issuers.add(issuer_value)\n            else:\n                raise OneLogin_Saml2_ValidationError(\n                    'Issuer of the Response is multiple.',\n                    OneLogin_Saml2_ValidationError.ISSUER_MULTIPLE_IN_RESPONSE\n                )\n\n        assertion_issuer_nodes = self.__query_assertion('/saml:Issuer')\n        if len(assertion_issuer_nodes) == 1:\n            issuer_value = OneLogin_Saml2_XML.element_text(assertion_issuer_nodes[0])\n            if issuer_value:\n                issuers.add(issuer_value)\n        else:\n            raise OneLogin_Saml2_ValidationError(\n                'Issuer of the Assertion not found or multiple.',\n                OneLogin_Saml2_ValidationError.ISSUER_NOT_FOUND_IN_ASSERTION\n            )\n\n        return list(set(issuers))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nameid(self):\n        nameid_value = None\n        nameid_data = self.get_nameid_data()\n        if nameid_data and 'Value' in nameid_data.keys():\n            nameid_value = nameid_data['Value']\n        return nameid_value", "response": "Gets the NameID provided by the SAML Response from the IdP\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nameid_format(self):\n        nameid_format = None\n        nameid_data = self.get_nameid_data()\n        if nameid_data and 'Format' in nameid_data.keys():\n            nameid_format = nameid_data['Format']\n        return nameid_format", "response": "Gets the NameID Format provided by the SAML Response from the IdP\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_session_not_on_or_after(self):\n        not_on_or_after = None\n        authn_statement_nodes = self.__query_assertion('/saml:AuthnStatement[@SessionNotOnOrAfter]')\n        if authn_statement_nodes:\n            not_on_or_after = OneLogin_Saml2_Utils.parse_SAML_to_time(authn_statement_nodes[0].get('SessionNotOnOrAfter'))\n        return not_on_or_after", "response": "Gets the SessionNotOnOrAfter value from the AuthnStatement."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the SessionIndex from the AuthnStatement", "response": "def get_session_index(self):\n        \"\"\"\n        Gets the SessionIndex from the AuthnStatement\n        Could be used to be stored in the local session in order\n        to be used in a future Logout Request that the SP could\n        send to the SP, to set what specific session must be deleted\n\n        :returns: The SessionIndex value\n        :rtype: string|None\n        \"\"\"\n        session_index = None\n        authn_statement_nodes = self.__query_assertion('/saml:AuthnStatement[@SessionIndex]')\n        if authn_statement_nodes:\n            session_index = authn_statement_nodes[0].get('SessionIndex')\n        return session_index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_attributes(self):\n        attributes = {}\n        attribute_nodes = self.__query_assertion('/saml:AttributeStatement/saml:Attribute')\n        for attribute_node in attribute_nodes:\n            attr_name = attribute_node.get('Name')\n            if attr_name in attributes.keys():\n                raise OneLogin_Saml2_ValidationError(\n                    'Found an Attribute element with duplicated Name',\n                    OneLogin_Saml2_ValidationError.DUPLICATED_ATTRIBUTE_NAME_FOUND\n                )\n\n            values = []\n            for attr in attribute_node.iterchildren('{%s}AttributeValue' % OneLogin_Saml2_Constants.NSMAP['saml']):\n                attr_text = OneLogin_Saml2_XML.element_text(attr)\n                if attr_text:\n                    attr_text = attr_text.strip()\n                    if attr_text:\n                        values.append(attr_text)\n\n                # Parse any nested NameID children\n                for nameid in attr.iterchildren('{%s}NameID' % OneLogin_Saml2_Constants.NSMAP['saml']):\n                    values.append({\n                        'NameID': {\n                            'Format': nameid.get('Format'),\n                            'NameQualifier': nameid.get('NameQualifier'),\n                            'value': nameid.text\n                        }\n                    })\n            attributes[attr_name] = values\n        return attributes", "response": "Gets the Attributes from the AttributeStatement element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify that the document only contains a single Assertion encrypted or not", "response": "def validate_num_assertions(self):\n        \"\"\"\n        Verifies that the document only contains a single Assertion (encrypted or not)\n\n        :returns: True if only 1 assertion encrypted or not\n        :rtype: bool\n        \"\"\"\n        encrypted_assertion_nodes = OneLogin_Saml2_XML.query(self.document, '//saml:EncryptedAssertion')\n        assertion_nodes = OneLogin_Saml2_XML.query(self.document, '//saml:Assertion')\n\n        valid = len(encrypted_assertion_nodes) + len(assertion_nodes) == 1\n\n        if (self.encrypted):\n            assertion_nodes = OneLogin_Saml2_XML.query(self.decrypted_document, '//saml:Assertion')\n            valid = valid and len(assertion_nodes) == 1\n\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the signed elements are valid and that they are not duplicate.", "response": "def process_signed_elements(self):\n        \"\"\"\n        Verifies the signature nodes:\n         - Checks that are Response or Assertion\n         - Check that IDs and reference URI are unique and consistent.\n\n        :returns: The signed elements tag names\n        :rtype: list\n        \"\"\"\n        sign_nodes = self.__query('//ds:Signature')\n\n        signed_elements = []\n        verified_seis = []\n        verified_ids = []\n        response_tag = '{%s}Response' % OneLogin_Saml2_Constants.NS_SAMLP\n        assertion_tag = '{%s}Assertion' % OneLogin_Saml2_Constants.NS_SAML\n\n        for sign_node in sign_nodes:\n            signed_element = sign_node.getparent().tag\n            if signed_element != response_tag and signed_element != assertion_tag:\n                raise OneLogin_Saml2_ValidationError(\n                    'Invalid Signature Element %s SAML Response rejected' % signed_element,\n                    OneLogin_Saml2_ValidationError.WRONG_SIGNED_ELEMENT\n                )\n\n            if not sign_node.getparent().get('ID'):\n                raise OneLogin_Saml2_ValidationError(\n                    'Signed Element must contain an ID. SAML Response rejected',\n                    OneLogin_Saml2_ValidationError.ID_NOT_FOUND_IN_SIGNED_ELEMENT\n                )\n\n            id_value = sign_node.getparent().get('ID')\n            if id_value in verified_ids:\n                raise OneLogin_Saml2_ValidationError(\n                    'Duplicated ID. SAML Response rejected',\n                    OneLogin_Saml2_ValidationError.DUPLICATED_ID_IN_SIGNED_ELEMENTS\n                )\n            verified_ids.append(id_value)\n\n            # Check that reference URI matches the parent ID and no duplicate References or IDs\n            ref = OneLogin_Saml2_XML.query(sign_node, './/ds:Reference')\n            if ref:\n                ref = ref[0]\n                if ref.get('URI'):\n                    sei = ref.get('URI')[1:]\n\n                    if sei != id_value:\n                        raise OneLogin_Saml2_ValidationError(\n                            'Found an invalid Signed Element. SAML Response rejected',\n                            OneLogin_Saml2_ValidationError.INVALID_SIGNED_ELEMENT\n                        )\n\n                    if sei in verified_seis:\n                        raise OneLogin_Saml2_ValidationError(\n                            'Duplicated Reference URI. SAML Response rejected',\n                            OneLogin_Saml2_ValidationError.DUPLICATED_REFERENCE_IN_SIGNED_ELEMENTS\n                        )\n                    verified_seis.append(sei)\n\n            signed_elements.append(signed_element)\n\n        if signed_elements:\n            if not self.validate_signed_elements(signed_elements, raise_exceptions=True):\n                raise OneLogin_Saml2_ValidationError(\n                    'Found an unexpected Signature Element. SAML Response rejected',\n                    OneLogin_Saml2_ValidationError.UNEXPECTED_SIGNED_ELEMENTS\n                )\n        return signed_elements"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_timestamps(self):\n        conditions_nodes = self.__query_assertion('/saml:Conditions')\n\n        for conditions_node in conditions_nodes:\n            nb_attr = conditions_node.get('NotBefore')\n            nooa_attr = conditions_node.get('NotOnOrAfter')\n            if nb_attr and OneLogin_Saml2_Utils.parse_SAML_to_time(nb_attr) > OneLogin_Saml2_Utils.now() + OneLogin_Saml2_Constants.ALLOWED_CLOCK_DRIFT:\n                raise OneLogin_Saml2_ValidationError(\n                    'Could not validate timestamp: not yet valid. Check system clock.',\n                    OneLogin_Saml2_ValidationError.ASSERTION_TOO_EARLY\n                )\n            if nooa_attr and OneLogin_Saml2_Utils.parse_SAML_to_time(nooa_attr) + OneLogin_Saml2_Constants.ALLOWED_CLOCK_DRIFT <= OneLogin_Saml2_Utils.now():\n                raise OneLogin_Saml2_ValidationError(\n                    'Could not validate timestamp: expired. Check system clock.',\n                    OneLogin_Saml2_ValidationError.ASSERTION_EXPIRED\n                )\n        return True", "response": "Validates that the document is valid according to the Conditions Element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the assertion and the message for the given XPath expression", "response": "def __query_assertion(self, xpath_expr):\n        \"\"\"\n        Extracts nodes that match the query from the Assertion\n\n        :param xpath_expr: Xpath Expresion\n        :type xpath_expr: String\n\n        :returns: The queried nodes\n        :rtype: list\n        \"\"\"\n\n        assertion_expr = '/saml:Assertion'\n        signature_expr = '/ds:Signature/ds:SignedInfo/ds:Reference'\n        signed_assertion_query = '/samlp:Response' + assertion_expr + signature_expr\n        assertion_reference_nodes = self.__query(signed_assertion_query)\n        tagid = None\n\n        if not assertion_reference_nodes:\n            # Check if the message is signed\n            signed_message_query = '/samlp:Response' + signature_expr\n            message_reference_nodes = self.__query(signed_message_query)\n            if message_reference_nodes:\n                message_id = message_reference_nodes[0].get('URI')\n                final_query = \"/samlp:Response[@ID=$tagid]/\"\n                tagid = message_id[1:]\n            else:\n                final_query = \"/samlp:Response\"\n            final_query += assertion_expr\n        else:\n            assertion_id = assertion_reference_nodes[0].get('URI')\n            final_query = '/samlp:Response' + assertion_expr + \"[@ID=$tagid]\"\n            tagid = assertion_id[1:]\n        final_query += xpath_expr\n        return self.__query(final_query, tagid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the response for the given query", "response": "def __query(self, query, tagid=None):\n        \"\"\"\n        Extracts nodes that match the query from the Response\n\n        :param query: Xpath Expresion\n        :type query: String\n\n        :param tagid: Tag ID\n        :type query: String\n\n        :returns: The queried nodes\n        :rtype: list\n        \"\"\"\n        if self.encrypted:\n            document = self.decrypted_document\n        else:\n            document = self.document\n        return OneLogin_Saml2_XML.query(document, query, None, tagid)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_assertion_id(self):\n        if not self.validate_num_assertions():\n            raise OneLogin_Saml2_ValidationError(\n                'SAML Response must contain 1 assertion',\n                OneLogin_Saml2_ValidationError.WRONG_NUMBER_OF_ASSERTIONS\n            )\n        return self.__query_assertion('')[0].get('ID', None)", "response": "Gets the ID of the assertion in the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsigning the metadata with the key and cert provided", "response": "def sign_metadata(metadata, key, cert, sign_algorithm=OneLogin_Saml2_Constants.RSA_SHA1, digest_algorithm=OneLogin_Saml2_Constants.SHA1):\n        \"\"\"\n        Signs the metadata with the key/cert provided\n\n        :param metadata: SAML Metadata XML\n        :type metadata: string\n\n        :param key: x509 key\n        :type key: string\n\n        :param cert: x509 cert\n        :type cert: string\n\n        :returns: Signed Metadata\n        :rtype: string\n\n        :param sign_algorithm: Signature algorithm method\n        :type sign_algorithm: string\n\n        :param digest_algorithm: Digest algorithm method\n        :type digest_algorithm: string\n        \"\"\"\n        return OneLogin_Saml2_Utils.add_sign(metadata, key, cert, False, sign_algorithm, digest_algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_x509_key_descriptors(metadata, cert=None, add_encryption=True):\n        if cert is None or cert == '':\n            return metadata\n        try:\n            root = OneLogin_Saml2_XML.to_etree(metadata)\n        except Exception as e:\n            raise Exception('Error parsing metadata. ' + str(e))\n\n        assert root.tag == '{%s}EntityDescriptor' % OneLogin_Saml2_Constants.NS_MD\n        try:\n            sp_sso_descriptor = next(root.iterfind('.//md:SPSSODescriptor', namespaces=OneLogin_Saml2_Constants.NSMAP))\n        except StopIteration:\n            raise Exception('Malformed metadata.')\n\n        if add_encryption:\n            OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, False)\n        OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, True)\n        return OneLogin_Saml2_XML.to_string(root)", "response": "Adds the x509 key descriptors to the metadata XML."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_config(self, budget):\n\t\t\n\t\tself.logger.debug('start sampling a new configuration.')\n\t\t\n\n\t\tsample = None\n\t\tinfo_dict = {}\n\t\t\n\t\t# If no model is available, sample from prior\n\t\t# also mix in a fraction of random configs\n\t\tif len(self.kde_models.keys()) == 0 or np.random.rand() < self.random_fraction:\n\t\t\tsample =  self.configspace.sample_configuration()\n\t\t\tinfo_dict['model_based_pick'] = False\n\n\t\tbest = np.inf\n\t\tbest_vector = None\n\n\t\tif sample is None:\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\t#sample from largest budget\n\t\t\t\tbudget = max(self.kde_models.keys())\n\n\t\t\t\tl = self.kde_models[budget]['good'].pdf\n\t\t\t\tg = self.kde_models[budget]['bad' ].pdf\n\t\t\t\n\t\t\t\tminimize_me = lambda x: max(1e-32, g(x))/max(l(x),1e-32)\n\t\t\t\t\n\t\t\t\tkde_good = self.kde_models[budget]['good']\n\t\t\t\tkde_bad = self.kde_models[budget]['bad']\n\n\t\t\t\tfor i in range(self.num_samples):\n\t\t\t\t\tidx = np.random.randint(0, len(kde_good.data))\n\t\t\t\t\tdatum = kde_good.data[idx]\n\t\t\t\t\tvector = []\n\t\t\t\t\t\n\t\t\t\t\tfor m,bw,t in zip(datum, kde_good.bw, self.vartypes):\n\t\t\t\t\t\t\n\t\t\t\t\t\tbw = max(bw, self.min_bandwidth)\n\t\t\t\t\t\tif t == 0:\n\t\t\t\t\t\t\tbw = self.bw_factor*bw\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tvector.append(sps.truncnorm.rvs(-m/bw,(1-m)/bw, loc=m, scale=bw))\n\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\tself.logger.warning(\"Truncated Normal failed for:\\ndatum=%s\\nbandwidth=%s\\nfor entry with value %s\"%(datum, kde_good.bw, m))\n\t\t\t\t\t\t\t\tself.logger.warning(\"data in the KDE:\\n%s\"%kde_good.data)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif np.random.rand() < (1-bw):\n\t\t\t\t\t\t\t\tvector.append(int(m))\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tvector.append(np.random.randint(t))\n\t\t\t\t\tval = minimize_me(vector)\n\n\t\t\t\t\tif not np.isfinite(val):\n\t\t\t\t\t\tself.logger.warning('sampled vector: %s has EI value %s'%(vector, val))\n\t\t\t\t\t\tself.logger.warning(\"data in the KDEs:\\n%s\\n%s\"%(kde_good.data, kde_bad.data))\n\t\t\t\t\t\tself.logger.warning(\"bandwidth of the KDEs:\\n%s\\n%s\"%(kde_good.bw, kde_bad.bw))\n\t\t\t\t\t\tself.logger.warning(\"l(x) = %s\"%(l(vector)))\n\t\t\t\t\t\tself.logger.warning(\"g(x) = %s\"%(g(vector)))\n\n\t\t\t\t\t\t# right now, this happens because a KDE does not contain all values for a categorical parameter\n\t\t\t\t\t\t# this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one\n\t\t\t\t\t\t# if the good_kde has a finite value, i.e. there is no config with that value in the bad kde, so it shouldn't be terrible.\n\t\t\t\t\t\tif np.isfinite(l(vector)):\n\t\t\t\t\t\t\tbest_vector = vector\n\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\tif val < best:\n\t\t\t\t\t\tbest = val\n\t\t\t\t\t\tbest_vector = vector\n\n\t\t\t\tif best_vector is None:\n\t\t\t\t\tself.logger.debug(\"Sampling based optimization with %i samples failed -> using random configuration\"%self.num_samples)\n\t\t\t\t\tsample = self.configspace.sample_configuration().get_dictionary()\n\t\t\t\t\tinfo_dict['model_based_pick']  = False\n\t\t\t\telse:\n\t\t\t\t\tself.logger.debug('best_vector: {}, {}, {}, {}'.format(best_vector, best, l(best_vector), g(best_vector)))\n\t\t\t\t\tfor i, hp_value in enumerate(best_vector):\n\t\t\t\t\t\tif isinstance(\n\t\t\t\t\t\t\tself.configspace.get_hyperparameter(\n\t\t\t\t\t\t\t\tself.configspace.get_hyperparameter_by_idx(i)\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tConfigSpace.hyperparameters.CategoricalHyperparameter\n\t\t\t\t\t\t):\n\t\t\t\t\t\t\tbest_vector[i] = int(np.rint(best_vector[i]))\n\t\t\t\t\tsample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n\t\t\t\t\t\n\t\t\t\t\ttry:\n\t\t\t\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\t\t\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\t\t\t\t\t\tconfiguration=sample\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\tinfo_dict['model_based_pick'] = True\n\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tself.logger.warning((\"=\"*50 + \"\\n\")*3 +\\\n\t\t\t\t\t\t\t\t\"Error converting configuration:\\n%s\"%sample+\\\n\t\t\t\t\t\t\t\t\"\\n here is a traceback:\" +\\\n\t\t\t\t\t\t\t\ttraceback.format_exc())\n\t\t\t\t\t\traise(e)\n\n\t\t\texcept:\n\t\t\t\tself.logger.warning(\"Sampling based optimization with %i samples failed\\n %s \\nUsing random configuration\"%(self.num_samples, traceback.format_exc()))\n\t\t\t\tsample = self.configspace.sample_configuration()\n\t\t\t\tinfo_dict['model_based_pick']  = False\n\n\n\t\ttry:\n\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\tconfiguration=sample.get_dictionary()\n\t\t\t).get_dictionary()\n\t\texcept Exception as e:\n\t\t\tself.logger.warning(\"Error (%s) converting configuration: %s -> \"\n\t\t\t\t\t\t\t\t\"using random configuration!\",\n\t\t\t\t\t\t\t\te,\n\t\t\t\t\t\t\t\tsample)\n\t\t\tsample = self.configspace.sample_configuration().get_dictionary()\n\t\tself.logger.debug('done sampling a new configuration.')\n\t\treturn sample, info_dict", "response": "This function is called inside Hyperband to query a new configuration from the Hyperband. It is called inside Hyperband to query a new configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(self, data, weights=None, bw_estimator='scott', efficient_bw_estimation=True, update_bandwidth=True):\n\t\t\n\t\tif self.data is None:\n\t\t\t# overwrite some values in case this is the first fit of the KDE\n\t\t\tefficient_bw_estimation = False\n\t\t\tupdate_bandwidth=True\n\n\t\tself.data = np.asfortranarray(data)\n\t\tfor i,k in enumerate(self.kernels):\n\t\t\t\tself.kernels[i].data = self.data[:,i]\n\n\t\tself.weights = self._normalize_weights(weights)\n\n\t\tif not update_bandwidth:\n\t\t\treturn\n\t\t\n\t\tif not efficient_bw_estimation or bw_estimator == 'scott':\n\t\t\t# inspired by the the statsmodels code\n\t\t\tsigmas = np.std(self.data, ddof=1, axis=0)\n\t\t\tIQRs = np.subtract.reduce(np.percentile(self.data, [75,25], axis=0))\n\t\t\tself.bandwidths = 1.059 * np.minimum(sigmas, IQRs) * np.power(self.data.shape[0], -0.2)\n\t\t\t# crop bandwidths for categorical parameters\n\t\t\tself.bandwidths = np.clip(self.bandwidths , self.min_bandwidth, self.bw_clip)\n\t\t\t\n\t\tif bw_estimator == 'mlcv':\n\t\t\t# optimize bandwidths here\n\t\t\tdef opt_me(bw):\n\t\t\t\tself.bandwidths=bw\n\t\t\t\tself._set_kernel_bandwidths()\n\t\t\t\treturn(self.loo_negloglikelihood())\n\t\t\t\n\t\t\tres = spo.minimize(opt_me, self.bandwidths, bounds=self.bw_bounds, method='SLSQP')\n\t\t\tself.optimizer_result = res\n\t\t\tself.bandwidths[:] = res.x\n\t\tself._set_kernel_bandwidths()", "response": "fit the KDE to the data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the probability density function at all x_test", "response": "def pdf(self, x_test):\n\t\t\"\"\"\n\t\t\tComputes the probability density function at all x_test\n\t\t\"\"\"\n\t\tN,D = self.data.shape\n\t\tx_test = np.asfortranarray(x_test)\n\t\tx_test = x_test.reshape([-1, D])\n\t\t\n\t\tpdfs = self._individual_pdfs(x_test)\n\t\t#import pdb; pdb.set_trace()\n\t\t# combine values based on fully_dimensional!\n\t\tif self.fully_dimensional:\n\t\t\t# first the product of the individual pdfs for each point in the data across dimensions and then the average (factorized kernel)\n\t\t\tpdfs = np.sum(np.prod(pdfs, axis=-1)*self.weights[None, :], axis=-1)\n\t\telse:\n\t\t\t# first the average over the 1d pdfs and the the product over dimensions (TPE like factorization of the pdf)\n\t\t\tpdfs = np.prod(np.sum(pdfs*self.weights[None,:,None], axis=-2), axis=-1)\n\t\treturn(pdfs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the needed types from the configspace for faster retrival later", "response": "def _get_types(self):\n\t\t\"\"\" extracts the needed types from the configspace for faster retrival later\n\t\t\n\t\t\ttype = 0 - numerical (continuous or integer) parameter\n\t\t\ttype >=1 - categorical parameter\n\t\t\t\n\t\t\tTODO: figure out a way to properly handle ordinal parameters\n\t\t\n\t\t\"\"\"\n\t\ttypes = []\n\t\tnum_values = []\n\t\tfor hp in self.configspace.get_hyperparameters():\n\t\t\t#print(hp)\n\t\t\tif isinstance(hp, CS.CategoricalHyperparameter):\n\t\t\t\ttypes.append('U')\n\t\t\t\tnum_values.append(len(hp.choices))\n\t\t\telif isinstance(hp, CS.UniformIntegerHyperparameter):\n\t\t\t\ttypes.append('I')\n\t\t\t\tnum_values.append((hp.upper - hp.lower + 1))\n\t\t\telif isinstance(hp, CS.UniformFloatHyperparameter):\n\t\t\t\ttypes.append('C')\n\t\t\t\tnum_values.append(np.inf)\n\t\t\telif isinstance(hp, CS.OrdinalHyperparameter):\n\t\t\t\ttypes.append('O')\n\t\t\t\tnum_values.append(len(hp.sequence))\n\t\t\telse:\n\t\t\t\traise ValueError('Unsupported Parametertype %s'%type(hp))\n\t\treturn(types, num_values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute(self, config, budget, working_directory, *args, **kwargs):\n\n\t\t# device = torch.device('cpu')\n\t\tmodel = MNISTConvNet(num_conv_layers=config['num_conv_layers'],\n\t\t\t\t\t\t\tnum_filters_1=config['num_filters_1'],\n\t\t\t\t\t\t\tnum_filters_2=config['num_filters_2'] if 'num_filters_2' in config else None,\n\t\t\t\t\t\t\tnum_filters_3=config['num_filters_3'] if 'num_filters_3' in config else None,\n\t\t\t\t\t\t\tdropout_rate=config['dropout_rate'],\n\t\t\t\t\t\t\tnum_fc_units=config['num_fc_units'],\n\t\t\t\t\t\t\tkernel_size=3\n\t\t)\n\n\t\tcriterion = torch.nn.CrossEntropyLoss()\n\t\tif config['optimizer'] == 'Adam':\n\t\t\toptimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n\t\telse:\n\t\t\toptimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'])\n\n\t\tfor epoch in range(int(budget)):\n\t\t\tloss = 0\n\t\t\tmodel.train()\n\t\t\tfor i, (x, y) in enumerate(self.train_loader):\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\toutput = model(x)\n\t\t\t\tloss = F.nll_loss(output, y)\n\t\t\t\tloss.backward()\n\t\t\t\toptimizer.step()\n\n\t\ttrain_accuracy = self.evaluate_accuracy(model, self.train_loader)\n\t\tvalidation_accuracy = self.evaluate_accuracy(model, self.validation_loader)\n\t\ttest_accuracy = self.evaluate_accuracy(model, self.test_loader)\n\n\t\treturn ({\n\t\t\t'loss': 1-validation_accuracy, # remember: HpBandSter always minimizes!\n\t\t\t'info': {\t'test accuracy': test_accuracy,\n\t\t\t\t\t\t'train accuracy': train_accuracy,\n\t\t\t\t\t\t'validation accuracy': validation_accuracy,\n\t\t\t\t\t\t'number of parameters': model.number_of_parameters(),\n\t\t\t\t\t}\n\t\t\t\t\t\t\n\t\t})", "response": "This example is used to compute the MNIST model using a feed forward network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_configspace():\n\t\tcs = CS.ConfigurationSpace()\n\n\t\tlr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value='1e-2', log=True)\n\n\t\t# For demonstration purposes, we add different optimizers as categorical hyperparameters.\n\t\t# To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n\t\t# SGD has a different parameter 'momentum'.\n\t\toptimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n\n\t\tsgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n\n\t\tcs.add_hyperparameters([lr, optimizer, sgd_momentum])\n\n\t\t# The hyperparameter sgd_momentum will be used,if the configuration\n\t\t# contains 'SGD' as optimizer.\n\t\tcond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n\t\tcs.add_condition(cond)\n\n\t\tnum_conv_layers =  CSH.UniformIntegerHyperparameter('num_conv_layers', lower=1, upper=3, default_value=2)\n\t\t\n\t\tnum_filters_1 = CSH.UniformIntegerHyperparameter('num_filters_1', lower=4, upper=64, default_value=16, log=True)\n\t\tnum_filters_2 = CSH.UniformIntegerHyperparameter('num_filters_2', lower=4, upper=64, default_value=16, log=True)\n\t\tnum_filters_3 = CSH.UniformIntegerHyperparameter('num_filters_3', lower=4, upper=64, default_value=16, log=True)\n\n\n\t\tcs.add_hyperparameters([num_conv_layers, num_filters_1, num_filters_2, num_filters_3])\n\t\t\n\t\t# You can also use inequality conditions:\n\t\tcond = CS.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n\t\tcs.add_condition(cond)\n\n\t\tcond = CS.GreaterThanCondition(num_filters_3, num_conv_layers, 2)\n\t\tcs.add_condition(cond)\n\n\n\t\tdropout_rate = CSH.UniformFloatHyperparameter('dropout_rate', lower=0.0, upper=0.9, default_value=0.5, log=False)\n\t\tnum_fc_units = CSH.UniformIntegerHyperparameter('num_fc_units', lower=8, upper=256, default_value=32, log=True)\n\n\t\tcs.add_hyperparameters([dropout_rate, num_fc_units])\n\n\t\treturn cs", "response": "This function builds the ConfigurationsSpace - Object containing the needed hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction to sample a new configuration This function is called inside Hyperband to query a new configuration Parameters: ----------- budget: float the budget for which this configuration is scheduled returns: config should return a valid configuration", "response": "def get_config(self, budget):\n\t\t\"\"\"\n\t\t\tFunction to sample a new configuration\n\n\t\t\tThis function is called inside Hyperband to query a new configuration\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\tbudget: float\n\t\t\t\tthe budget for which this configuration is scheduled\n\n\t\t\treturns: config\n\t\t\t\tshould return a valid configuration\n\n\t\t\"\"\"\n\t\tsample = None\n\t\tinfo_dict = {}\n\t\t\n\t\t# If no model is available, sample from prior\n\t\t# also mix in a fraction of random configs\n\t\tif len(self.kde_models.keys()) == 0 or np.random.rand() < self.random_fraction:\n\t\t\tsample =  self.configspace.sample_configuration()\n\t\t\tinfo_dict['model_based_pick'] = False\n\n\n\t\tif sample is None:\n\t\t\ttry:\n\t\t\t\t#import pdb; pdb.set_trace()\n\t\t\t\tsamples = self.kde_models[budget]['good'].sample(self.num_samples)\n\t\t\t\tei = self.kde_models[budget]['good'].pdf(samples)/self.kde_models[budget]['bad'].pdf(samples)\n\t\t\t\t\n\t\t\t\tbest_idx = np.argmax(ei)\n\t\t\t\tbest_vector = samples[best_idx]\n\n\t\t\t\tsample = ConfigSpace.Configuration(self.configspace, vector=best_vector)\n\n\n\t\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\t\t\t\tconfiguration=sample.get_dictionary()\n\t\t\t\t\t\t\t)\n\t\t\t\tinfo_dict['model_based_pick'] = True\n\n\t\t\texcept Exception as e:\n\t\t\t\tself.logger.warning((\"=\"*50 + \"\\n\")*3 +\\\n\t\t\t\t\t\t\"Error sampling a configuration!\\n\"+\\\n\t\t\t\t\t\t\"Models for budgets: %s\"%(self.kde_models.keys()) +\\\n\t\t\t\t\t\t\"\\n here is a traceback:\" +\\\n\t\t\t\t\t\ttraceback.format_exc())\n\n\t\t\t\tfor b,l in self.losses.items():\n\t\t\t\t\tself.logger.debug(\"budget: {}\\nlosses:{}\".format(b,l))\n\t\t\t\t\n\t\t\t\tsample =  self.configspace.sample_configuration()\n\t\t\t\tinfo_dict['model_based_pick'] = False\n\t\t\t\t\n\t\treturn sample.get_dictionary(), info_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_result(self, job, update_model=True):\n\n\t\tsuper().new_result(job)\n\n\t\tif job.result is None:\n\t\t\t# One could skip crashed results, but we decided \n\t\t\t# assign a +inf loss and count them as bad configurations\n\t\t\tloss = np.inf\n\t\telse:\n\t\t\tloss = job.result[\"loss\"]\n\n\t\tbudget = job.kwargs[\"budget\"]\n\n\t\tif budget not in self.configs.keys():\n\t\t\tself.configs[budget] = []\n\t\t\tself.losses[budget] = []\n\t\n\t\t\t\n\t\tif len(self.configs.keys()) == 1:\n\t\t\tmin_num_points = 6\n\t\telse:\n\t\t\tmin_num_points = self.min_points_in_model\n\n\n\t\t# skip model building if we already have a bigger model\n\t\tif max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n\t\t\treturn\n\n\t\t# We want to get a numerical representation of the configuration in the original space\n\n\t\tconf = ConfigSpace.Configuration(self.configspace, job.kwargs[\"config\"]).get_array().tolist()\n\t\t\n\t\t\t\n\t\t#import pdb; pdb.set_trace()\n\t\t\n\t\t\n\t\tif conf in self.configs[budget]:\n\t\t\ti = self.configs[budget].index(conf)\n\t\t\tself.losses[budget][i].append(loss)\n\t\t\tprint('-'*50)\n\t\t\tprint('ran config %s with loss %f again'%(conf, loss))\n\t\telse:\n\t\t\tself.configs[budget].append(conf)\n\t\t\tself.losses[budget].append([loss])\n\n\t\t\n\t\t# skip model building:\n\t\t#\t\ta) if not enough points are available\n\t\t\n\t\ttmp = np.array([np.mean(r) for r in self.losses[budget]])\n\t\tif np.sum(np.isfinite(tmp)) < min_num_points:\n\t\t\tself.logger.debug(\"Only %i successful run(s) for budget %f available, need more than %s -> can't build model!\"%(np.sum(np.isfinite(tmp)), budget, min_num_points))\n\t\t\treturn\n\n\t\t#\t\tb) during warnm starting when we feed previous results in and only update once\n\t\tif not update_model:\n\t\t\treturn\n\n\n\n\t\tif budget not in self.kde_models.keys():\n\t\t\tself.kde_models[budget] = {\n\t\t\t\t'good': MultivariateKDE(self.configspace, min_bandwidth=self.min_bandwidth, fully_dimensional=self.fully_dimensional),\n\t\t\t\t'bad' : MultivariateKDE(self.configspace, min_bandwidth=self.min_bandwidth, fully_dimensional=self.fully_dimensional)\n\t\t\t}\t\n\n\n\t\t#import pdb; pdb.set_trace()\n\t\tnum_configs = len(self.losses[budget])\n\t\t\n\t\ttrain_configs = np.array(self.configs[budget][-num_configs:])\n\t\ttrain_losses =  np.array(list(map(np.mean, self.losses[budget][-num_configs:])))\n\n\t\tn_good= max(3,(num_configs * self.top_n_percent) // 100)\n\t\tn_bad = num_configs-n_good\n\n\t\t# Refit KDE for the current budget\n\t\tidx = np.argsort(train_losses)\n\n\t\ttrain_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n\t\ttrain_data_bad  = self.impute_conditional_data(train_configs[idx[n_good:n_good+n_bad+1]])\n\n\n\t\tself.kde_models[budget]['bad'].fit(train_data_bad, bw_estimator=self.bw_estimator)\n\t\tself.kde_models[budget]['good'].fit(train_data_good, bw_estimator=self.bw_estimator)\n\t\t\n\t\t\n\t\tif self.bw_estimator in ['mlcv'] and n_good < 3:\n\t\t\tself.kde_models[budget]['good'].bandwidths[:] = self.kde_models[budget]['bad'].bandwidths\n\n\t\t# update probs for the categorical parameters for later sampling\n\t\tself.logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n\\n\\n\\n\\n'%(budget, n_good, n_bad, np.min(train_losses)))", "response": "This method is called by the job object when it has finished and creates a new result object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef realtime_learning_curves(runs):\n\tsr = sorted(runs, key=lambda r: r.budget)\n\tlc = list(filter(lambda t: not t[1] is None, [(r.time_stamps['finished'], r.info['test accuracy']) for r in sr]))\n\treturn([lc,])", "response": "This function returns a list of the training data for each iteration in the run list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _advance_to_next_stage(self, config_ids, losses):\n\t\tranks = np.argsort(np.argsort(losses))\n\t\treturn(ranks < self.num_configs[self.stage])", "response": "This function is called by the next stage to advance to the next config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_next_iteration(self, iteration, iteration_kwargs={}):\n\t\t\n\t\t# number of 'SH rungs'\n\t\ts = self.max_SH_iter - 1 - (iteration%self.max_SH_iter)\n\t\t# number of configurations in that bracket\n\t\tn0 = int(np.floor((self.max_SH_iter)/(s+1)) * self.eta**s)\n\t\tns = [max(int(n0*(self.eta**(-i))), 1) for i in range(s+1)]\n\n\t\treturn(SuccessiveHalving(HPB_iter=iteration, num_configs=ns, budgets=self.budgets[(-s-1):], config_sampler=self.config_generator.get_config, **iteration_kwargs))", "response": "get_next_iteration - Returns the next iteration of the current iteration"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_next_iteration(self, iteration, iteration_kwargs={}):\n\t\t\n\t\tmin_budget = max( self.min_budget, self.config_generator.largest_budget_with_model())\n\t\tmax_budget = self.max_budget\n\t\teta = self.eta\n\n\t\t# precompute some HB stuff\n\t\tmax_SH_iter = -int(np.log(min_budget/max_budget)/np.log(eta)) + 1\n\t\tbudgets = max_budget * np.power(eta, -np.linspace(max_SH_iter-1, 0, max_SH_iter))\n\n\t\t\n\t\t# number of 'SH rungs'\n\t\ts = max_SH_iter - 1\n\t\t# number of configurations in that bracket\n\t\tn0 = int(np.floor((self.max_SH_iter)/(s+1)) * eta**s)\n\t\tns = np.array([max(int(n0*(eta**(-i))), 1) for i in range(s+1)])\n\t\t\n\t\twhile (ns * budgets[-s-1:]).sum() <= self.budget_per_iteration:\n\t\t    n0 += 1\n\t\t    ns = np.array([max(int(n0*(eta**(-i))), 1) for i in range(s+1)])\n\t\t\n\t\tn0 -= 1\n\t\tns = np.array([max(int(n0*(eta**(-i))), 1) for i in range(s+1)])\n\t\t\n\t\tassert (ns * budgets[-s-1:]).sum() <= self.budget_per_iteration, 'Sampled iteration exceeds the budget per iteration!'\n\t\t\n\t\treturn(SuccessiveHalving(HPB_iter=iteration, num_configs=ns, budgets=budgets, config_sampler=self.config_generator.get_config, **iteration_kwargs))", "response": "This function returns the next iteration of the current iteration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to sample a new configuration This function is called inside Hyperband to query a new configuration Parameters: ----------- budget: float the budget for which this configuration is scheduled returns: config should return a valid configuration", "response": "def get_config(self, budget):\n\t\t\"\"\"\n\t\t\tFunction to sample a new configuration\n\n\t\t\tThis function is called inside Hyperband to query a new configuration\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\tbudget: float\n\t\t\t\tthe budget for which this configuration is scheduled\n\n\t\t\treturns: config\n\t\t\t\tshould return a valid configuration\n\n\t\t\"\"\"\n\t\t# No observations available for this budget sample from the prior\n\t\tif len(self.kde_models.keys()) == 0:\n\t\t\treturn self.configspace.sample_configuration().get_dictionary()\n\t\t# If we haven't seen anything with this budget, we sample from the kde trained on the highest budget\n\t\tif budget not in self.kde_models.keys():\n\t\t\tbudget = sorted(self.kde_models.keys())[-1]\n\t\t# TODO: This only works in continuous space and with gaussian kernels\n\t\tkde = self.kde_models[budget]\n\t\tidx = np.random.randint(0, len(self.kde_models[budget].data))\n\n\t\tvector = [sps.truncnorm.rvs(-m/bw,(1-m)/bw, loc=m, scale=bw) for m,bw in zip(self.kde_models[budget].data[idx], kde.bw)]\n\t\t\n\t\tif np.any(np.array(vector)>1) or np.any(np.array(vector)<0):\n\t\t\traise RuntimeError(\"truncated normal sampling problems!\")\n\t\t\n\t\tsample = ConfigSpace.Configuration(self.configspace, vector=vector)\n\t\treturn sample.get_dictionary(), {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction to register finished runs Every time a run has finished, this function should be called to register it with the result logger. If overwritten, make sure to call this method from the base class to ensure proper logging. Parameters: ----------- job_id: dict a dictionary containing all the info about the run job_result: dict contains all the results of the job, i.e. it's a dict with the keys 'loss' and 'info'", "response": "def new_result(self, job):\n\t\t\"\"\"\n\t\t\tfunction to register finished runs\n\n\t\t\tEvery time a run has finished, this function should be called\n\t\t\tto register it with the result logger. If overwritten, make\n\t\t\tsure to call this method from the base class to ensure proper\n\t\t\tlogging.\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\tjob_id: dict\n\t\t\t\ta dictionary containing all the info about the run\n\t\t\tjob_result: dict\n\t\t\t\tcontains all the results of the job, i.e. it's a dict with\n\t\t\t\tthe keys 'loss' and 'info'\n\n\t\t\"\"\"\n\n\t\tsuper(KernelDensityEstimator, self).new_result(job)\n\t\tbudget = job.kwargs[\"budget\"]\n\t\tif budget not in self.configs.keys():\n\t\t\tself.configs[budget] = []\n\t\t\tself.losses[budget] = []\n\n\t\t# We want to get a numerical representation of the configuration in the original space\n\t\tconf = ConfigSpace.Configuration(self.configspace, job.kwargs['config'])\n\t\tself.configs[budget].append(conf.get_array())\n\t\tself.losses[budget].append(job.result['result'][\"loss\"])\n\n\n\t\t# Check if we have enough data points to fit a KDE\n\t\tif len(self.configs[budget]) % self.update_after_n_points == 0:\n\t\t\ttrain_configs, train_losses = [], []\n\t\t\n\t\t\ttrain_configs.extend(self.configs[budget])\n\t\t\ttrain_losses.extend(self.losses[budget])\n\n\t\t\tn = int(self.top_n_percent * len(train_configs) / 100.)\n\t\t\t\n\t\t\tremaining_budgets = list(self.configs.keys())\n\t\t\tremaining_budgets.remove(budget)\n\t\t\tremaining_budgets.sort(reverse=True)\n\n\n\t\t\tfor b in remaining_budgets:\n\t\t\t\tif  n >= self.min_points_in_model: break\n\t\t\t\ttrain_configs.extend(self.configs[b])\n\t\t\t\ttrain_losses.extend(self.losses[b])\n\t\t\t\tn = int(self.top_n_percent * len(train_configs) / 100.)\n\n\t\t\tif  len(train_losses) < self.min_points_in_model:\n\t\t\t\treturn\n\t\t\t\n\t\t\tn = max(self.min_points_in_model, n)\n\t\t\t\n\t\t\t# Refit KDE for the current budget\n\t\t\tidx = np.argsort(train_losses)\n\n\t\t\ttrain_data = (np.array(train_configs)[idx])[:n]\n\t\t\tself.kde_models[budget] = sm.nonparametric.KDEMultivariate(data=train_data,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t var_type=self.var_type,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t bw='cv_ls')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the nameserver credentials in cases where master and workers share a filesystem", "response": "def load_nameserver_credentials(self, working_directory, num_tries=60, interval=1):\n\t\t\"\"\"\n\t\tloads the nameserver credentials in cases where master and workers share a filesystem\n\n\t\tParameters\n\t\t----------\n\t\t\tworking_directory: str\n\t\t\t\tthe working directory for the HPB run (see master)\n\t\t\tnum_tries: int\n\t\t\t\tnumber of attempts to find the file (default 60)\n\t\t\tinterval: float\n\t\t\t\twaiting period between the attempts\n\t\t\"\"\"\n\t\tfn = os.path.join(working_directory, 'HPB_run_%s_pyro.pkl'%self.run_id)\n\t\t\n\t\tfor i in range(num_tries):\n\t\t\ttry:\n\t\t\t\twith open(fn, 'rb') as fh:\n\t\t\t\t\tself.nameserver, self.nameserver_port = pickle.load(fh)\n\t\t\t\treturn\n\t\t\texcept FileNotFoundError:\n\t\t\t\tself.logger.warning('config file %s not found (trail %i/%i)'%(fn, i+1, num_tries))\n\t\t\t\ttime.sleep(interval)\n\t\t\texcept:\n\t\t\t\traise\n\t\traise RuntimeError(\"Could not find the nameserver information, aborting!\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_workers(self, min_n_workers=1):\n\t\n\t\tself.logger.debug('wait_for_workers trying to get the condition')\n\t\twith self.thread_cond:\n\t\t\twhile (self.dispatcher.number_of_workers() < min_n_workers):\n\t\t\t\tself.logger.debug('HBMASTER: only %i worker(s) available, waiting for at least %i.'%(self.dispatcher.number_of_workers(), min_n_workers))\n\t\t\t\tself.thread_cond.wait(1)\n\t\t\t\tself.dispatcher.trigger_discover_worker()\n\t\t\t\t\n\t\tself.logger.debug('Enough workers to start this run!')", "response": "wait_for_workers helper function to hold execution until some workers are available"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, n_iterations=1, min_n_workers=1, iteration_kwargs = {},):\n\n\t\tself.wait_for_workers(min_n_workers)\n\t\t\n\t\titeration_kwargs.update({'result_logger': self.result_logger})\n\n\t\tif self.time_ref is None:\n\t\t\tself.time_ref = time.time()\n\t\t\tself.config['time_ref'] = self.time_ref\n\t\t\n\t\t\tself.logger.info('HBMASTER: starting run at %s'%(str(self.time_ref)))\n\n\t\tself.thread_cond.acquire()\n\t\twhile True:\n\n\t\t\tself._queue_wait()\n\t\t\t\n\t\t\tnext_run = None\n\t\t\t# find a new run to schedule\n\t\t\tfor i in self.active_iterations():\n\t\t\t\tnext_run = self.iterations[i].get_next_run()\n\t\t\t\tif not next_run is None: break\n\n\t\t\tif not next_run is None:\n\t\t\t\tself.logger.debug('HBMASTER: schedule new run for iteration %i'%i)\n\t\t\t\tself._submit_job(*next_run)\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tif n_iterations > 0:\t#we might be able to start the next iteration\n\t\t\t\t\tself.iterations.append(self.get_next_iteration(len(self.iterations), iteration_kwargs))\n\t\t\t\t\tn_iterations -= 1\n\t\t\t\t\tcontinue\n\n\t\t\t# at this point there is no imediate run that can be scheduled,\n\t\t\t# so wait for some job to finish if there are active iterations\n\t\t\tif self.active_iterations():\n\t\t\t\tself.thread_cond.wait()\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\tself.thread_cond.release()\n\t\t\n\t\tfor i in self.warmstart_iteration:\n\t\t\ti.fix_timestamps(self.time_ref)\n\t\t\t\n\t\tws_data = [i.data for i in self.warmstart_iteration]\n\t\t\n\t\treturn Result([copy.deepcopy(i.data) for i in self.iterations] + ws_data, self.config)", "response": "This method starts a new run in the thread pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef job_callback(self, job):\n\t\tself.logger.debug('job_callback for %s started'%str(job.id))\n\t\twith self.thread_cond:\n\t\t\tself.logger.debug('job_callback for %s got condition'%str(job.id))\n\t\t\tself.num_running_jobs -= 1\n\n\t\t\tif not self.result_logger is None:\n\t\t\t\tself.result_logger(job)\n\t\t\tself.iterations[job.id[0]].register_result(job)\n\t\t\tself.config_generator.new_result(job)\n\n\t\t\tif self.num_running_jobs <= self.job_queue_sizes[0]:\n\t\t\t\tself.logger.debug(\"HBMASTER: Trying to run another job!\")\n\t\t\t\tself.thread_cond.notify()\n\n\t\tself.logger.debug('job_callback for %s finished'%str(job.id))", "response": "This method is called when a job is finished and the job is ready to be processed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for the queue to not overflow or underload it", "response": "def _queue_wait(self):\n\t\t\"\"\"\n\t\thelper function to wait for the queue to not overflow/underload it\n\t\t\"\"\"\n\t\t\n\t\tif self.num_running_jobs >= self.job_queue_sizes[1]:\n\t\t\twhile(self.num_running_jobs > self.job_queue_sizes[0]):\n\t\t\t\tself.logger.debug('HBMASTER: running jobs: %i, queue sizes: %s -> wait'%(self.num_running_jobs, str(self.job_queue_sizes)))\n\t\t\t\tself.thread_cond.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _submit_job(self, config_id, config, budget):\n\t\tself.logger.debug('HBMASTER: trying submitting job %s to dispatcher'%str(config_id))\n\t\twith self.thread_cond:\n\t\t\tself.logger.debug('HBMASTER: submitting job %s to dispatcher'%str(config_id))\n\t\t\tself.dispatcher.submit_job(config_id, config=config, budget=budget, working_directory=self.working_directory)\n\t\t\tself.num_running_jobs += 1\n\n\t\t#shouldn't the next line be executed while holding the condition?\n\t\tself.logger.debug(\"HBMASTER: job %s submitted to dispatcher\"%str(config_id))", "response": "submit a new job to the dispatcher"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef active_iterations(self):\n\n\t\tl = list(filter(lambda idx: not self.iterations[idx].is_finished, range(len(self.iterations))))\n\t\treturn(l)", "response": "get_active_iterations function to find active iterations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to train the model on the observed data.", "response": "def fit(self, times, losses, configs=None):\n        \"\"\"\n            function to train the model on the observed data\n\n            Parameters:\n            -----------\n\n            times: list\n                list of numpy arrays of the timesteps for each curve\n            losses: list\n                list of numpy arrays of the loss (the actual learning curve)\n            configs: list or None\n                list of the configurations for each sample. Each element\n                has to be a numpy array. Set to None, if no configuration\n                information is available.\n        \"\"\"\n\n        assert np.all(times > 0) and np.all(times <= self.max_num_epochs)\n\n        train = None\n        targets = None\n\n        for i in range(len(configs)):\n\n            t_idx = times[i] / self.max_num_epochs\n\n            x = np.repeat(np.array(configs[i])[None, :], t_idx.shape[0], axis=0)\n            x = np.concatenate((x, t_idx[:, None]), axis=1)\n\n            # LCNet assumes increasing curves, if we feed in losses here we have to flip the curves\n            lc = [1 - l for l in losses[i]]\n\n            if train is None:\n                train = x\n                targets = lc\n            else:\n                train = np.concatenate((train, x), 0)\n                targets = np.concatenate((targets, lc), 0)\n\n        self.model.train(train, targets)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_unseen(self, times, config):\n\n        assert np.all(times > 0) and np.all(times <= self.max_num_epochs)\n\n        x = np.array(config)[None, :]\n\n        idx = times / self.max_num_epochs\n        x = np.repeat(x, idx.shape[0], axis=0)\n\n        x = np.concatenate((x, idx[:, None]), axis=1)\n\n        mean, var = self.model.predict(x)\n        return 1 - mean, var", "response": "predict the loss of an unseen configuration at the given times"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextending a partially observed curve Parameters: ----------- times: numpy array times where to predict the loss obs_times: numpy array times where the curve has already been observed obs_losses: numpy array corresponding observed losses config: numpy array numerical reperesentation of the config; None if no config information is available Returns: -------- mean and variance prediction at input times", "response": "def extend_partial(self, times, obs_times, obs_losses, config=None):\n        \"\"\"\n            extends a partially observed curve\n\n            Parameters:\n            -----------\n\n            times: numpy array\n                times where to predict the loss\n            obs_times: numpy array\n                times where the curve has already been observed\n            obs_losses: numpy array\n                corresponding observed losses\n            config: numpy array\n                numerical reperesentation of the config; None if no config\n                information is available\n                \n            Returns:\n            --------\n            \n            mean and variance prediction at input times\n                \n                \n        \"\"\"\n        return self.predict_unseen(times, config)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_result(self, job, update_model=True):\n\t\tif not job.exception is None:\n\t\t\tself.logger.warning(\"job {} failed with exception\\n{}\".format(job.id, job.exception))", "response": "Called by the job manager to log a new result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute(self, config, budget, **kwargs):\n\n        res = numpy.clip(config['x'] + numpy.random.randn()/budget, config['x']/2, 1.5*config['x'])\n        time.sleep(self.sleep_interval)\n\n        return({\n                    'loss': float(res),  # this is the a mandatory field to run hyperband\n                    'info': res  # can be used for any user-defined information - also mandatory\n                })", "response": "Simple example for a compute function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(self, sample_indices=None, num_samples=1):\n\t\tif sample_indices is None:\n\t\t\tsample_indices = np.random.choice(self.data.shape[0], size=num_samples)\n\n\t\tsamples = self.data[sample_indices]\n\n\t\tdelta = np.random.normal(size=num_samples)*self.bw\n\t\tsamples += delta\n\t\toob_idx  = np.argwhere(np.logical_or(samples > 1, samples < 0)).flatten()\n\n\t\twhile len(oob_idx) > 0:\n\t\t\tsamples[oob_idx] -= delta[oob_idx]\t\t# revert move\n\t\t\tdelta[oob_idx] = np.random.normal(size=len(oob_idx))*self.bw\n\t\t\tsamples[oob_idx] += delta[oob_idx]\n\t\t\toob_idx = oob_idx[np.argwhere(np.logical_or(samples[oob_idx] > 1, samples[oob_idx] < 0))].flatten()\n\n\t\treturn(samples)", "response": "returns samples according to the KDE\n\t\t\tParameters\n\tParameters\n\tParameters\n\tnum_samples is the number of samples in the training data used as centers for the training data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninverts the differencing of the original and the original entries", "response": "def invert_differencing(self, initial_part, differenced_rest, order=None):\n        \"\"\"\n            function to invert the differencing\n        \"\"\"\n\n        if order is None: order = self.diff_order\n\n        # compute the differenced values of the initial part:\n        starting_points = [ self.apply_differencing(initial_part, order=order)[-1] for order in range(self.diff_order)]\n        \n        actual_predictions = differenced_rest\n        import pdb\n        pdb.set_trace()\n        for s in starting_points[::-1]:\n            actual_predictions = np.cumsum(np.hstack([s, actual_predictions]))[1:]\n\n        return(actual_predictions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(self):\n\t\n\t\tif self.host is None:\n\t\t\tif self.nic_name is None:\n\t\t\t\tself.host = 'localhost'\n\t\t\telse:\n\t\t\t\tself.host = nic_name_to_host(self.nic_name)\n\n\t\turi, self.pyro_ns, _ = Pyro4.naming.startNS(host=self.host, port=self.port)\n\n\t\tself.host, self.port = self.pyro_ns.locationStr.split(':')\n\t\tself.port = int(self.port)\n\t\t\n\t\tthread = threading.Thread(target=self.pyro_ns.requestLoop, name='Pyro4 nameserver started by HpBandSter')\n\t\tthread.start()\n\n\t\tif not self.dir is None:\n\t\t\tos.makedirs(self.dir, exist_ok=True)\n\t\t\tself.conf_fn = os.path.join(self.dir, 'HPB_run_%s_pyro.pkl'%self.run_id)\n\n\t\t\twith open(self.conf_fn, 'wb') as fh:\n\t\t\t\tpickle.dump((self.host, self.port), fh)\n\t\t\n\t\treturn(self.host, self.port)", "response": "starts a Pyro4 nameserver in a separate thread\n\tReturns-----------\n\t the host name and the used port\n\treturn - > tuple of the result of this method"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shutdown(self):\n\t\tif not self.pyro_ns is None:\n\t\t\tself.pyro_ns.shutdown()\n\t\t\tself.pyro_ns = None\n\t\t\n\t\tif not self.conf_fn is None:\n\t\t\tos.remove(self.conf_fn)\n\t\t\tself.conf_fn = None", "response": "shutdown the nameserver and the config file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a SuccessiveHalving iteration with only evaluations on the biggest budgets", "response": "def get_next_iteration(self, iteration, iteration_kwargs={}):\n\t\t\"\"\"\n\t\tReturns a SH iteration with only evaluations on the biggest budget\n\t\t\n\t\tParameters\n\t\t----------\n\t\t\titeration: int\n\t\t\t\tthe index of the iteration to be instantiated\n\n\t\tReturns\n\t\t-------\n\t\t\tSuccessiveHalving: the SuccessiveHalving iteration with the\n\t\t\t\tcorresponding number of configurations\n\t\t\"\"\"\n\t\t\n\t\t\n\t\tbudgets = [self.max_budget]\n\t\tns = [self.budget_per_iteration//self.max_budget]\n\t\t\n\t\treturn(SuccessiveHalving(HPB_iter=iteration, num_configs=ns, budgets=budgets, config_sampler=self.config_generator.get_config, **iteration_kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new configuration to the current iteration", "response": "def add_configuration(self, config = None, config_info={}):\n\t\t\"\"\"\n\t\tfunction to add a new configuration to the current iteration\n\t\t\n\t\tParameters\n\t\t----------\n\t\t\n\t\tconfig : valid configuration\n\t\t\tThe configuration to add. If None, a configuration is sampled from the config_sampler\n\t\tconfig_info: dict\n\t\t\tSome information about the configuration that will be stored in the results\n\t\t\"\"\"\n\t\t\n\t\tif config is None:\n\t\t\tconfig, config_info = self.config_sampler(self.budgets[self.stage])\n\t\t\n\t\tif self.is_finished:\n\t\t\traise RuntimeError(\"This HPBandSter iteration is finished, you can't add more configurations!\")\n\n\t\tif self.actual_num_configs[self.stage] == self.num_configs[self.stage]:\n\t\t\traise RuntimeError(\"Can't add another configuration to stage %i in HPBandSter iteration %i.\"%(self.stage, self.HPB_iter))\n\n\t\tconfig_id = (self.HPB_iter, self.stage, self.actual_num_configs[self.stage])\n\n\t\tself.data[config_id] = Datum(config=config, config_info=config_info, budget = self.budgets[self.stage])\n\n\t\tself.actual_num_configs[self.stage] += 1\n\t\t\n\t\tif not self.result_logger is None:\n\t\t    self.result_logger.new_config(config_id, config, config_info)\n\t\t\n\t\treturn(config_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_result(self, job, skip_sanity_checks=False):\n\n\t\tif self.is_finished:\n\t\t\traise RuntimeError(\"This HB iteration is finished, you can't register more results!\")\n\n\t\tconfig_id = job.id\n\t\tconfig = job.kwargs['config']\n\t\tbudget = job.kwargs['budget']\n\t\ttimestamps = job.timestamps\n\t\tresult = job.result\n\t\texception = job.exception\n\t\t\n\t\td = self.data[config_id]\n\t\t\n\t\tif not skip_sanity_checks:\n\t\t\tassert d.config == config, 'Configurations differ!'\n\t\t\tassert d.status == 'RUNNING', \"Configuration wasn't scheduled for a run.\"\n\t\t\tassert d.budget == budget, 'Budgets differ (%f != %f)!'%(self.data[config_id]['budget'], budget)\n\n\t\td.time_stamps[budget] = timestamps\n\t\td.results[budget] = result\n\n\t\tif (not job.result is None) and np.isfinite(result['loss']):\n\t\t\td.status = 'REVIEW'\n\t\telse:\n\t\t\td.status = 'CRASHED'\n\n\t\td.exceptions[budget] = exception\n\t\tself.num_running -= 1", "response": "register the result of a job"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to return the next configuration and budget to run. This function is called from HB_master, don't call this from your script. It returns None if this run of SH is finished or there are pending jobs that need to finish to progress to the next stage. If there are empty slots to be filled in the current SH stage (which never happens in the original SH version), a new configuration will be sampled and scheduled to run next.", "response": "def get_next_run(self):\n\t\t\"\"\"\n\t\tfunction to return the next configuration and budget to run.\n\n\t\tThis function is called from HB_master, don't call this from\n\t\tyour script.\n\n\t\tIt returns None if this run of SH is finished or there are\n\t\tpending jobs that need to finish to progress to the next stage.\n\n\t\tIf there are empty slots to be filled in the current SH stage\n\t\t(which never happens in the original SH version), a new\n\t\tconfiguration will be sampled and scheduled to run next.\n\t\t\"\"\"\n\n\t\tif self.is_finished:\n\t\t\treturn(None)\n\t\t\n\t\tfor k,v in self.data.items():\n\t\t\tif v.status == 'QUEUED':\n\t\t\t\tassert v.budget == self.budgets[self.stage], 'Configuration budget does not align with current stage!'\n\t\t\t\tv.status = 'RUNNING'\n\t\t\t\tself.num_running += 1\n\t\t\t\treturn(k, v.config, v.budget)\n\n\t\t# check if there are still slots to fill in the current stage and return that\n\t\tif (self.actual_num_configs[self.stage] < self.num_configs[self.stage]):\n\t\t\tself.add_configuration()\n\t\t\treturn(self.get_next_run())\n\n\t\tif self.num_running == 0:\n\t\t\t# at this point a stage is completed\n\t\t\tself.process_results()\n\t\t\treturn(self.get_next_run())\n\n\t\treturn(None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning that is called when a stage is completed and needs to be analyzed befor further computations. The code here implements the original SH algorithms by advancing the k-best (lowest loss) configurations at the current budget. k is defined by the num_configs list (see __init__) and the current stage value. For more advanced methods like resampling after each stage, overload this function only.", "response": "def process_results(self):\n\t\t\"\"\"\n\t\tfunction that is called when a stage is completed and\n\t\tneeds to be analyzed befor further computations.\n\n\t\tThe code here implements the original SH algorithms by\n\t\tadvancing the k-best (lowest loss) configurations at the current\n\t\tbudget. k is defined by the num_configs list (see __init__)\n\t\tand the current stage value.\n\n\t\tFor more advanced methods like resampling after each stage,\n\t\toverload this function only.\n\t\t\"\"\"\n\t\tself.stage += 1\n\t\t\n\t\t# collect all config_ids that need to be compared\n\t\tconfig_ids = list(filter(lambda cid: self.data[cid].status == 'REVIEW', self.data.keys()))\n\n\t\tif (self.stage >= len(self.num_configs)):\n\t\t\tself.finish_up()\n\t\t\treturn\n\n\t\tbudgets = [self.data[cid].budget for cid in config_ids]\n\t\tif len(set(budgets)) > 1:\n\t\t\traise RuntimeError('Not all configurations have the same budget!')\n\t\tbudget = self.budgets[self.stage-1]\n\n\t\tlosses = np.array([self.data[cid].results[budget]['loss'] for cid in config_ids])\n\n\t\tadvance = self._advance_to_next_stage(config_ids, losses)\n\n\t\tfor i, a in enumerate(advance):\n\t\t\tif a:\n\t\t\t\tself.logger.debug('ITERATION: Advancing config %s to next budget %f'%(config_ids[i], self.budgets[self.stage]))\n\n\t\tfor i, cid in enumerate(config_ids):\n\t\t\tif advance[i]:\n\t\t\t\tself.data[cid].status = 'QUEUED'\n\t\t\t\tself.data[cid].budget = self.budgets[self.stage]\n\t\t\t\tself.actual_num_configs[self.stage] += 1\n\t\t\telse:\n\t\t\t\tself.data[cid].status = 'TERMINATED'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fix_timestamps(self, time_ref):\n\t\t\n\t\tfor k,v in self.data.items():\n\t\t\tfor kk, vv in v.time_stamps.items():\n\t\t\t\tfor kkk,vvv in vv.items():\n\t\t\t\t\tself.data[k].time_stamps[kk][kkk] += time_ref", "response": "fix internal time stamps so that the last run ends at time time_ref"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute(self, config, budget, working_directory, *args, **kwargs):\n\n\t\tmodel = Sequential()\n\t\t\n\t\tmodel.add(Conv2D(config['num_filters_1'], kernel_size=(3,3),\n\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t input_shape=self.input_shape))\n\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\t\tif config['num_conv_layers'] > 1:\n\t\t\tmodel.add(Conv2D(config['num_filters_2'], kernel_size=(3, 3),\n\t\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t\t input_shape=self.input_shape))\n\t\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\t\n\t\t\n\t\tif config['num_conv_layers'] > 2:\n\t\t\tmodel.add(Conv2D(config['num_filters_3'], kernel_size=(3, 3),\n\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t input_shape=self.input_shape))\n\t\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\t\t\t\n\t\tmodel.add(Dropout(config['dropout_rate']))\n\t\tmodel.add(Flatten())\n\t\tmodel.add(Dense(config['num_fc_units'], activation='relu'))\n\t\tmodel.add(Dropout(config['dropout_rate']))\n\t\tmodel.add(Dense(self.num_classes, activation='softmax'))\n\n\n\t\tif config['optimizer'] == 'Adam':\n\t\t\toptimizer = keras.optimizers.Adam(lr=config['lr'])\n\t\telse:\n\t\t\toptimizer = keras.optimizers.SGD(lr=config['lr'], momentum=config['sgd_momentum'])\n\n\t\tmodel.compile(loss=keras.losses.categorical_crossentropy,\n\t\t\t\t\t  optimizer=optimizer,\n\t\t\t\t\t  metrics=['accuracy'])\n\n\t\tmodel.fit(self.x_train, self.y_train,\n\t\t\t\t  batch_size=self.batch_size,\n\t\t\t\t  epochs=int(budget),\n\t\t\t\t  verbose=0,\n\t\t\t\t  validation_data=(self.x_test, self.y_test))\n\t\t\n\t\ttrain_score = model.evaluate(self.x_train, self.y_train, verbose=0)\n\t\tval_score = model.evaluate(self.x_validation, self.y_validation, verbose=0)\n\t\ttest_score = model.evaluate(self.x_test, self.y_test, verbose=0)\n\n\t\t#import IPython; IPython.embed()\n\t\treturn ({\n\t\t\t'loss': 1-val_score[1], # remember: HpBandSter always minimizes!\n\t\t\t'info': {\t'test accuracy': test_score[1],\n\t\t\t\t\t\t'train accuracy': train_score[1],\n\t\t\t\t\t\t'validation accuracy': val_score[1],\n\t\t\t\t\t\t'number of parameters': model.count_params(),\n\t\t\t\t\t}\n\t\t\t\t\t\t\n\t\t})", "response": "This example is a simple compute function for a feed forward network using a feed forward network. It is trained on the MNIST dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to sample a new configuration This function is called inside Hyperband to query a new configuration Parameters: ----------- budget: float the budget for which this configuration is scheduled returns: config should return a valid configuration", "response": "def get_config(self, budget):\n        \"\"\"\n            function to sample a new configuration\n\n            This function is called inside Hyperband to query a new configuration\n\n\n            Parameters:\n            -----------\n            budget: float\n                the budget for which this configuration is scheduled\n\n            returns: config\n                should return a valid configuration\n\n        \"\"\"\n        self.lock.acquire()\n        if not self.is_trained:\n            c = self.config_space.sample_configuration().get_array()\n        else:\n            candidates = np.array([self.config_space.sample_configuration().get_array()\n                                   for _ in range(self.n_candidates)])\n\n            # We are only interested on the asymptotic value\n            projected_candidates = np.concatenate((candidates, np.ones([self.n_candidates, 1])), axis=1)\n\n            # Compute the upper confidence bound of the function at the asymptote\n            m, v = self.model.predict(projected_candidates)\n\n            ucb_values = m + self.delta * np.sqrt(v)\n            print(ucb_values)\n            # Sample a configuration based on the ucb values\n            p = np.ones(self.n_candidates) * (ucb_values / np.sum(ucb_values))\n            idx = np.random.choice(self.n_candidates, 1, False, p)\n\n            c = candidates[idx][0]\n\n        config = ConfigSpace.Configuration(self.config_space, vector=c)\n\n        self.lock.release()\n        return config.get_dictionary(), {}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_result(self, job):\n        super().new_result(job)\n\n        conf = ConfigSpace.Configuration(self.config_space, job.kwargs['config']).get_array()\n\n        epochs = len(job.result[\"info\"][\"learning_curve\"])\n        budget = int(job.kwargs[\"budget\"])\n\n        t_idx = np.linspace(budget / epochs, budget, epochs) / self.max_budget\n        x_new = np.repeat(conf[None, :], t_idx.shape[0], axis=0)\n\n        x_new = np.concatenate((x_new, t_idx[:, None]), axis=1)\n\n        # Smooth learning curve\n        lc = smoothing(job.result[\"info\"][\"learning_curve\"])\n\n        # Flip learning curves since LC-Net wants increasing curves\n        lc_new = [1 - y for y in lc]\n\n        if self.train is None:\n            self.train = x_new\n            self.train_targets = lc_new\n        else:\n            self.train = np.append(self.train, x_new, axis=0)\n            self.train_targets = np.append(self.train_targets, lc_new, axis=0)\n\n        if self.counter >= self.n_points:\n\n            self.lock.acquire()\n            y_min = np.min(self.train_targets)\n            y_max = np.max(self.train_targets)\n\n            train_targets = (self.train_targets - y_min) / (y_max - y_min)\n\n            self.model.train(self.train, train_targets)\n            self.is_trained = True\n            self.counter = 0\n            self.lock.release()\n\n        else:\n            self.counter += epochs", "response": "This function is called by the base class to register finished runs with the result logger."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart a Pyro4 nameserver in a daemon thread", "response": "def start_local_nameserver(host=None, port=0, nic_name=None):\n\t\"\"\"\n\t\tstarts a Pyro4 nameserver in a daemon thread\n\t\t\n\t\tParameters:\n\t\t-----------\n\t\t\thost: str\n\t\t\t\tthe hostname to use for the nameserver\n\t\t\tport: int\n\t\t\t\tthe port to be used. Default =0 means a random port\n\t\t\tnic_name: str\n\t\t\t\tname of the network interface to use\n\t\t\n\t\tReturns:\n\t\t--------\n\t\t\ttuple (str, int):\n\t\t\t\tthe host name and the used port\n\t\"\"\"\n\t\n\tif host is None:\n\t\tif nic_name is None:\n\t\t\thost = 'localhost'\n\t\telse:\n\t\t\thost = nic_name_to_host(nic_name)\n\n\turi, ns, _ = Pyro4.naming.startNS(host=host, port=port)\n\thost, port = ns.locationStr.split(':')\n\t\n\t\n\tthread = threading.Thread(target=ns.requestLoop, name='Pyro4 nameserver started by HpBandSter')\n\tthread.daemon=True\n\t\n\tthread.start()\n\treturn(host, int(port))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_HBS_learning_curves(runs):\n\tsr = sorted(runs, key=lambda r: r.budget)\n\tlc = list(filter(lambda t: not t[1] is None, [(r.budget, r.loss) for r in sr]))\n\treturn([lc,])", "response": "function to get the hyperband learning curves from the given list of HB_result. run objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to import logged results and return a HB_result object", "response": "def logged_results_to_HBS_result(directory):\n\t\"\"\"\n\tfunction to import logged 'live-results' and return a HB_result object\n\n\tYou can load live run results with this function and the returned\n\tHB_result object gives you access to the results the same way\n\ta finished run would.\n\t\n\tParameters\n\t----------\n\tdirectory: str\n\t\tthe directory containing the results.json and config.json files\n\n\tReturns\n\t-------\n\thpbandster.core.result.Result: :object:\n\t\tTODO\n\t\n\t\"\"\"\n\tdata = {}\n\ttime_ref = float('inf')\n\tbudget_set = set()\n\t\n\twith open(os.path.join(directory, 'configs.json')) as fh:\n\t\tfor line in fh:\n\t\t\t\n\t\t\tline = json.loads(line)\n\t\t\t\n\t\t\tif len(line) == 3:\n\t\t\t\tconfig_id, config, config_info = line\n\t\t\tif len(line) == 2:\n\t\t\t\tconfig_id, config, = line\n\t\t\t\tconfig_info = 'N/A'\n\n\t\t\tdata[tuple(config_id)] = Datum(config=config, config_info=config_info)\n\n\twith open(os.path.join(directory, 'results.json')) as fh:\n\t\tfor line in fh:\n\t\t\tconfig_id, budget,time_stamps, result, exception = json.loads(line)\n\n\t\t\tid = tuple(config_id)\n\t\t\t\n\t\t\tdata[id].time_stamps[budget] = time_stamps\n\t\t\tdata[id].results[budget] = result\n\t\t\tdata[id].exceptions[budget] = exception\n\n\t\t\tbudget_set.add(budget)\n\t\t\ttime_ref = min(time_ref, time_stamps['submitted'])\n\n\n\t# infer the hyperband configuration from the data\n\tbudget_list = sorted(list(budget_set))\n\t\n\tHB_config = {\n\t\t\t\t\t\t'eta'        : None if len(budget_list) < 2 else budget_list[1]/budget_list[0],\n\t\t\t\t\t\t'min_budget' : min(budget_set),\n\t\t\t\t\t\t'max_budget' : max(budget_set),\n\t\t\t\t\t\t'budgets'    : budget_list,\n\t\t\t\t\t\t'max_SH_iter': len(budget_set),\n\t\t\t\t\t\t'time_ref'   : time_ref\n\t\t\t\t}\n\treturn(Result([data], HB_config))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the config_id of the incumbent.", "response": "def get_incumbent_id(self):\n\t\t\"\"\"\n\t\tFind the config_id of the incumbent.\n\n\t\tThe incumbent here is the configuration with the smallest loss\n\t\tamong all runs on the maximum budget! If no run finishes on the\n\t\tmaximum budget, None is returned!\n\t\t\"\"\"\n\t\ttmp_list = []\n\t\tfor k,v in self.data.items():\n\t\t\ttry:\n\t\t\t\t# only things run for the max budget are considered\n\t\t\t\tres = v.results[self.HB_config['max_budget']]\n\t\t\t\tif not res is None:\n\t\t\t\t\ttmp_list.append((res['loss'], k))\n\t\t\texcept KeyError as e:\n\t\t\t\tpass\n\t\t\texcept:\n\t\t\t\traise\n\n\t\tif len(tmp_list) > 0:\n\t\t\treturn(min(tmp_list)[1])\n\t\treturn(None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the trajectory that is used to calculate the incumbent for the current budget.", "response": "def get_incumbent_trajectory(self, all_budgets=True, bigger_is_better=True, non_decreasing_budget=True):\n\t\t\"\"\"\n\t\tReturns the best configurations over time\n\t\t\n\t\t\n\t\tParameters\n\t\t----------\n\t\t\tall_budgets: bool\n\t\t\t\tIf set to true all runs (even those not with the largest budget) can be the incumbent.\n\t\t\t\tOtherwise, only full budget runs are considered\n\t\t\tbigger_is_better:bool\n\t\t\t\tflag whether an evaluation on a larger budget is always considered better.\n\t\t\t\tIf True, the incumbent might increase for the first evaluations on a bigger budget\n\t\t\tnon_decreasing_budget: bool\n\t\t\t\tflag whether the budget of a new incumbent should be at least as big as the one for\n\t\t\t\tthe current incumbent.\n\t\tReturns\n\t\t-------\n\t\t\tdict:\n\t\t\t\tdictionary with all the config IDs, the times the runs\n\t\t\t\tfinished, their respective budgets, and corresponding losses\n\t\t\"\"\"\n\t\tall_runs = self.get_all_runs(only_largest_budget = not all_budgets)\n\t\t\n\t\tif not all_budgets:\n\t\t\tall_runs = list(filter(lambda r: r.budget==res.HB_config['max_budget'], all_runs))\n\t\t\n\t\tall_runs.sort(key=lambda r: r.time_stamps['finished'])\n\t\t\n\t\treturn_dict = { 'config_ids' : [],\n\t\t\t\t\t\t'times_finished': [],\n\t\t\t\t\t\t'budgets'    : [],\n\t\t\t\t\t\t'losses'     : [],\n\t\t}\n\t\n\t\tcurrent_incumbent = float('inf')\n\t\tincumbent_budget = self.HB_config['min_budget']\n\t\t\n\t\tfor r in all_runs:\n\t\t\tif r.loss is None: continue\n\t\t\t\n\t\t\tnew_incumbent = False\n\t\t\t\n\t\t\tif bigger_is_better and r.budget > incumbent_budget:\n\t\t\t\tnew_incumbent = True\n\t\t\t\n\t\t\t\n\t\t\tif r.loss < current_incumbent:\n\t\t\t\tnew_incumbent = True\n\t\t\t\n\t\t\tif non_decreasing_budget and r.budget < incumbent_budget:\n\t\t\t\tnew_incumbent = False\n\t\t\t\n\t\t\tif new_incumbent:\n\t\t\t\tcurrent_incumbent = r.loss\n\t\t\t\tincumbent_budget  = r.budget\n\t\t\t\t\n\t\t\t\treturn_dict['config_ids'].append(r.config_id)\n\t\t\t\treturn_dict['times_finished'].append(r.time_stamps['finished'])\n\t\t\t\treturn_dict['budgets'].append(r.budget)\n\t\t\t\treturn_dict['losses'].append(r.loss)\n\n\t\tif current_incumbent != r.loss:\n\t\t\tr = all_runs[-1]\n\t\t\n\t\t\treturn_dict['config_ids'].append(return_dict['config_ids'][-1])\n\t\t\treturn_dict['times_finished'].append(r.time_stamps['finished'])\n\t\t\treturn_dict['budgets'].append(return_dict['budgets'][-1])\n\t\t\treturn_dict['losses'].append(return_dict['losses'][-1])\n\n\t\t\t\n\t\treturn (return_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_runs_by_id(self, config_id):\n\t\td = self.data[config_id]\n\n\t\truns = []\n\t\tfor b in d.results.keys():\n\t\t\ttry:\n\t\t\t\terr_logs = d.exceptions.get(b, None)\n\n\t\t\t\tif d.results[b] is None:\n\t\t\t\t\tr = Run(config_id, b, None, None , d.time_stamps[b], err_logs)\n\t\t\t\telse:\n\t\t\t\t\tr = Run(config_id, b, d.results[b]['loss'], d.results[b]['info'] , d.time_stamps[b], err_logs)\n\t\t\t\truns.append(r)\n\t\t\texcept:\n\t\t\t\traise\n\t\truns.sort(key=lambda r: r.budget)\n\t\treturn(runs)", "response": "returns a list of runs for a given config id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts all learning curves from all run configurations Parameters ---------- lc_extractor: callable a function to return a list of learning_curves. defaults to hpbanster.HB_result.extract_HP_learning_curves config_ids: list of valid config ids if only a subset of the config ids is wanted Returns ------- dict a dictionary with the config_ids as keys and the learning curves as values", "response": "def get_learning_curves(self, lc_extractor=extract_HBS_learning_curves, config_ids=None):\n\t\t\"\"\"\n\t\textracts all learning curves from all run configurations\n\n\t\tParameters\n\t\t----------\n\t\t\tlc_extractor: callable\n\t\t\t\ta function to return a list of learning_curves.\n\t\t\t\tdefaults to hpbanster.HB_result.extract_HP_learning_curves\n\t\t\tconfig_ids: list of valid config ids\n\t\t\t\tif only a subset of the config ids is wanted\n\n\t\tReturns\n\t\t-------\n\t\t\tdict\n\t\t\t\ta dictionary with the config_ids as keys and the\n\t\t\t\tlearning curves as values\n\t\t\"\"\"\n\n\t\tconfig_ids = self.data.keys() if config_ids is None else config_ids\n\t\t\n\t\tlc_dict = {}\n\t\t\n\t\tfor id in config_ids:\n\t\t\truns = self.get_runs_by_id(id)\n\t\t\tlc_dict[id] = lc_extractor(runs)\n\t\t\t\n\t\treturn(lc_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all runs performed in the configuration", "response": "def get_all_runs(self, only_largest_budget=False):\n\t\t\"\"\"\n\t\treturns all runs performed\n\n\t\tParameters\n\t\t----------\n\t\t\tonly_largest_budget: boolean\n\t\t\t\tif True, only the largest budget for each configuration\n\t\t\t\tis returned. This makes sense if the runs are continued\n\t\t\t\tacross budgets and the info field contains the information\n\t\t\t\tyou care about. If False, all runs of a configuration\n\t\t\t\tare returned\n\t\t\"\"\"\n\t\tall_runs = []\n\n\t\tfor k in self.data.keys():\n\t\t\truns = self.get_runs_by_id(k)\n\n\t\t\tif len(runs) > 0:\n\t\t\t\tif only_largest_budget:\n\t\t\t\t\tall_runs.append(runs[-1])\n\t\t\t\telse:\n\t\t\t\t\tall_runs.extend(runs)\n\n\t\treturn(all_runs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_id2config_mapping(self):\n\t\tnew_dict = {}\n\t\tfor k, v in self.data.items():\n\t\t\tnew_dict[k] = {}\n\t\t\tnew_dict[k]['config'] = copy.deepcopy(v.config)\n\t\t\ttry:\n\t\t\t\tnew_dict[k]['config_info'] = copy.deepcopy(v.config_info)\n\t\t\texcept:\n\t\t\t\tpass\n\t\treturn(new_dict)", "response": "returns a dict where the keys are the config_ids and the values are the actual configurations\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges the list of results into one dictionary and normalize the time stamps", "response": "def _merge_results(self):\n\t\t\"\"\"\n\t\thidden function to merge the list of results into one\n\t\tdictionary and 'normalize' the time stamps\n\t\t\"\"\"\n\t\tnew_dict = {}\n\t\tfor it in self.data:\n\t\t\tnew_dict.update(it)\n\n\t\tfor k,v in new_dict.items():\n\t\t\tfor kk, vv in v.time_stamps.items():\n\t\t\t\tfor kkk,vvv in vv.items():\n\t\t\t\t\tnew_dict[k].time_stamps[kk][kkk] = vvv - self.HB_config['time_ref']\n\n\t\tself.data = new_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreverse of dumps(), raise BadSignature if signature fails. The serializer is expected to accept a bytestring.", "response": "def loads(s, key=None, salt='django.core.signing', serializer=JSONSerializer, max_age=None):\n    \"\"\"\n    Reverse of dumps(), raise BadSignature if signature fails.\n\n    The serializer is expected to accept a bytestring.\n    \"\"\"\n    # TimestampSigner.unsign() returns str but base64 and zlib compression\n    # operate on bytes.\n    base64d = force_bytes(TimestampSigner(key, salt=salt).unsign(s, max_age=max_age))\n    decompress = False\n    if base64d[:1] == b'.':\n        # It's compressed; uncompress it first\n        base64d = base64d[1:]\n        decompress = True\n    data = b64_decode(base64d)\n    if decompress:\n        data = zlib.decompress(data)\n    return serializer().loads(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve original value and check it wasn't signed more than max_age seconds ago.", "response": "def unsign(self, value, max_age=None):\n        \"\"\"\n        Retrieve original value and check it wasn't signed more\n        than max_age seconds ago.\n        \"\"\"\n        result = super(TimestampSigner, self).unsign(value)\n        value, timestamp = result.rsplit(self.sep, 1)\n        timestamp = baseconv.base62.decode(timestamp)\n        if max_age is not None:\n            if isinstance(max_age, datetime.timedelta):\n                max_age = max_age.total_seconds()\n            # Check timestamp is not older than max_age\n            age = time.time() - timestamp\n            if age > max_age:\n                raise SignatureExpired(\n                    'Signature age %s > %s seconds' % (age, max_age))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the configured broker type", "response": "def get_broker(list_key=Conf.PREFIX):\n    \"\"\"\n    Gets the configured broker type\n    :param list_key: optional queue name\n    :type list_key: str\n    :return:\n    \"\"\"\n    # custom\n    if Conf.BROKER_CLASS:\n        module, func = Conf.BROKER_CLASS.rsplit('.', 1)\n        m = importlib.import_module(module)\n        broker = getattr(m, func)\n        return broker(list_key=list_key)\n    # disque\n    elif Conf.DISQUE_NODES:\n        from django_q.brokers import disque\n        return disque.Disque(list_key=list_key)\n    # Iron MQ\n    elif Conf.IRON_MQ:\n        from django_q.brokers import ironmq\n        return ironmq.IronMQBroker(list_key=list_key)\n    # SQS\n    elif Conf.SQS:\n        from django_q.brokers import aws_sqs\n        return aws_sqs.Sqs(list_key=list_key)\n    # ORM\n    elif Conf.ORM:\n        from django_q.brokers import orm\n        return orm.ORM(list_key=list_key)\n    # Mongo\n    elif Conf.MONGO:\n        from django_q.brokers import mongo\n        return mongo.Mongo(list_key=list_key)\n    # default to redis\n    else:\n        from django_q.brokers import redis_broker\n        return redis_broker.Redis(list_key=list_key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave a cluster statistic to the cache provider", "response": "def set_stat(self, key, value, timeout):\n        \"\"\"\n        Saves a cluster statistic to the cache provider\n        :type key: str\n        :type value: str\n        :type timeout: int\n        \"\"\"\n        if not self.cache:\n            return\n        key_list = self.cache.get(Conf.Q_STAT, [])\n        if key not in key_list:\n            key_list.append(key)\n        self.cache.set(Conf.Q_STAT, key_list)\n        return self.cache.set(key, value, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all cluster stats from the cache provider", "response": "def get_stats(self, pattern):\n        \"\"\"\n        Returns a list of all cluster stats from the cache provider\n        :type pattern: str\n        :return: a list of Stats\n        \"\"\"\n        if not self.cache:\n            return\n        key_list = self.cache.get(Conf.Q_STAT)\n        if not key_list or len(key_list) == 0:\n            return []\n        stats = []\n        for key in key_list:\n            stat = self.cache.get(key)\n            if stat:\n                stats.append(stat)\n            else:\n                key_list.remove(key)\n        self.cache.set(Conf.Q_STAT, key_list)\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the current status for the given cluster_id.", "response": "def get(cluster_id, broker=None):\n        \"\"\"\n        gets the current status for the cluster\n        :param cluster_id: id of the cluster\n        :return: Stat or Status\n        \"\"\"\n        if not broker:\n            broker = get_broker()\n        pack = broker.get_stat(Stat.get_key(cluster_id))\n        if pack:\n            try:\n                return SignedPackage.loads(pack)\n            except BadSignature:\n                return None\n        return Status(cluster_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the status for all currently running clusters with the same prefix and secret key.", "response": "def get_all(broker=None):\n        \"\"\"\n        Get the status for all currently running clusters with the same prefix\n        and secret key.\n        :return: list of type Stat\n        \"\"\"\n        if not broker:\n            broker = get_broker()\n        stats = []\n        packs = broker.get_stats('{}:*'.format(Conf.Q_STAT)) or []\n        for pack in packs:\n            try:\n                stats.append(SignedPackage.loads(pack))\n            except BadSignature:\n                continue\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhumanize a given hexadecimal digest.", "response": "def humanize(self, hexdigest, words=4, separator='-'):\n\n        \"\"\"\n        Humanize a given hexadecimal digest.\n\n        Change the number of words output by specifying `words`. Change the\n        word separator with `separator`.\n\n            >>> digest = '60ad8d0d871b6095808297'\n            >>> HumanHasher().humanize(digest)\n            'sodium-magnesium-nineteen-hydrogen'\n        \"\"\"\n\n        # Gets a list of byte values between 0-255.\n        bytes = [int(x, 16) for x in list(map(''.join, list(zip(hexdigest[::2], hexdigest[1::2]))))]\n        # Compress an arbitrary number of bytes to `words`.\n        compressed = self.compress(bytes, words)\n        # Map the compressed byte values through the word list.\n        return separator.join(self.wordlist[byte] for byte in compressed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompressing a list of byte values to a fixed target length.", "response": "def compress(bytes, target):\n\n        \"\"\"\n        Compress a list of byte values to a fixed target length.\n\n            >>> bytes = [96, 173, 141, 13, 135, 27, 96, 149, 128, 130, 151]\n            >>> HumanHasher.compress(bytes, 4)\n            [205, 128, 156, 96]\n\n        Attempting to compress a smaller number of bytes to a larger number is\n        an error:\n\n            >>> HumanHasher.compress(bytes, 15)  # doctest: +ELLIPSIS\n            Traceback (most recent call last):\n            ...\n            ValueError: Fewer input bytes than requested output\n        \"\"\"\n\n        length = len(bytes)\n        if target > length:\n            raise ValueError(\"Fewer input bytes than requested output\")\n\n        # Split `bytes` into `target` segments.\n        seg_size = length // target\n        segments = [bytes[i * seg_size:(i + 1) * seg_size]\n                    for i in range(target)]\n        # Catch any left-over bytes in the last segment.\n        segments[-1].extend(bytes[target * seg_size:])\n\n        # Use a simple XOR checksum-like function for compression.\n        checksum = lambda bytes: reduce(operator.xor, bytes, 0)\n        checksums = list(map(checksum, segments))\n        return checksums"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef uuid(self, **params):\n\n        \"\"\"\n        Generate a UUID with a human-readable representation.\n\n        Returns `(human_repr, full_digest)`. Accepts the same keyword arguments\n        as :meth:`humanize` (they'll be passed straight through).\n        \"\"\"\n\n        digest = str(uuidlib.uuid4()).replace('-', '')\n        return self.humanize(digest, **params), digest", "response": "Generate a UUID with a human - readable representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueues a task for the cluster.", "response": "def async_task(func, *args, **kwargs):\n    \"\"\"Queue a task for the cluster.\"\"\"\n    keywords = kwargs.copy()\n    opt_keys = (\n    'hook', 'group', 'save', 'sync', 'cached', 'ack_failure', 'iter_count', 'iter_cached', 'chain', 'broker')\n    q_options = keywords.pop('q_options', {})\n    # get an id\n    tag = uuid()\n    # build the task package\n    task = {'id': tag[1],\n            'name': keywords.pop('task_name', None) or q_options.pop('task_name', None) or tag[0],\n            'func': func,\n            'args': args}\n    # push optionals\n    for key in opt_keys:\n        if q_options and key in q_options:\n            task[key] = q_options[key]\n        elif key in keywords:\n            task[key] = keywords.pop(key)\n    # don't serialize the broker\n    broker = task.pop('broker', get_broker())\n    # overrides\n    if 'cached' not in task and Conf.CACHED:\n        task['cached'] = Conf.CACHED\n    if 'sync' not in task and Conf.SYNC:\n        task['sync'] = Conf.SYNC\n    if 'ack_failure' not in task and Conf.ACK_FAILURES:\n        task['ack_failure'] = Conf.ACK_FAILURES\n    # finalize\n    task['kwargs'] = keywords\n    task['started'] = timezone.now()\n    # signal it\n    pre_enqueue.send(sender=\"django_q\", task=task)\n    # sign it\n    pack = SignedPackage.dumps(task)\n    if task.get('sync', False):\n        return _sync(pack)\n    # push it\n    enqueue_id = broker.enqueue(pack)\n    logger.info('Enqueued {}'.format(enqueue_id))\n    logger.debug('Pushed {}'.format(tag))\n    return task['id']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef schedule(func, *args, **kwargs):\n    name = kwargs.pop('name', None)\n    hook = kwargs.pop('hook', None)\n    schedule_type = kwargs.pop('schedule_type', Schedule.ONCE)\n    minutes = kwargs.pop('minutes', None)\n    repeats = kwargs.pop('repeats', -1)\n    next_run = kwargs.pop('next_run', timezone.now())\n\n    # check for name duplicates instead of am unique constraint\n    if name and Schedule.objects.filter(name=name).exists():\n        raise IntegrityError(\"A schedule with the same name already exists.\")\n\n    # create and return the schedule\n    return Schedule.objects.create(name=name,\n                                   func=func,\n                                   hook=hook,\n                                   args=args,\n                                   kwargs=kwargs,\n                                   schedule_type=schedule_type,\n                                   minutes=minutes,\n                                   repeats=repeats,\n                                   next_run=next_run\n                                   )", "response": "Create a schedule.\n\n    :param func: function to schedule.\n    :param args: function arguments.\n    :param name: optional name for the schedule.\n    :param hook: optional result hook function.\n    :type schedule_type: Schedule.TYPE\n    :param repeats: how many times to repeat. 0=never, -1=always.\n    :param next_run: Next scheduled run.\n    :type next_run: datetime.datetime\n    :param kwargs: function keyword arguments.\n    :return: the schedule object.\n    :rtype: Schedule"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef result(task_id, wait=0, cached=Conf.CACHED):\n    if cached:\n        return result_cached(task_id, wait)\n    start = time()\n    while True:\n        r = Task.get_result(task_id)\n        if r:\n            return r\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Return the result of the named task."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef result_cached(task_id, wait=0, broker=None):\n    if not broker:\n        broker = get_broker()\n    start = time()\n    while True:\n        r = broker.cache.get('{}:{}'.format(broker.list_key, task_id))\n        if r:\n            return SignedPackage.loads(r)['result']\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Return the result from the cache backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef result_group(group_id, failures=False, wait=0, count=None, cached=Conf.CACHED):\n    if cached:\n        return result_group_cached(group_id, failures, wait, count)\n    start = time()\n    if count:\n        while True:\n            if count_group(group_id) == count or wait and (time() - start) * 1000 >= wait >= 0:\n                break\n            sleep(0.01)\n    while True:\n        r = Task.get_result_group(group_id, failures)\n        if r:\n            return r\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Return a list of results for a group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of results for a task group from the cache", "response": "def result_group_cached(group_id, failures=False, wait=0, count=None, broker=None):\n    \"\"\"\n    Return a list of results for a task group from the cache backend\n    \"\"\"\n    if not broker:\n        broker = get_broker()\n    start = time()\n    if count:\n        while True:\n            if count_group_cached(group_id) == count or wait and (time() - start) * 1000 >= wait > 0:\n                break\n            sleep(0.01)\n    while True:\n        group_list = broker.cache.get('{}:{}:keys'.format(broker.list_key, group_id))\n        if group_list:\n            result_list = []\n            for task_key in group_list:\n                task = SignedPackage.loads(broker.cache.get(task_key))\n                if task['success'] or failures:\n                    result_list.append(task['result'])\n            return result_list\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch(task_id, wait=0, cached=Conf.CACHED):\n    if cached:\n        return fetch_cached(task_id, wait)\n    start = time()\n    while True:\n        t = Task.get_task(task_id)\n        if t:\n            return t\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Fetch the full task object from the backend"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_cached(task_id, wait=0, broker=None):\n    if not broker:\n        broker = get_broker()\n    start = time()\n    while True:\n        r = broker.cache.get('{}:{}'.format(broker.list_key, task_id))\n        if r:\n            task = SignedPackage.loads(r)\n            t = Task(id=task['id'],\n                     name=task['name'],\n                     func=task['func'],\n                     hook=task.get('hook'),\n                     args=task['args'],\n                     kwargs=task['kwargs'],\n                     started=task['started'],\n                     stopped=task['stopped'],\n                     result=task['result'],\n                     success=task['success'])\n            return t\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Fetch a task from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch_group(group_id, failures=True, wait=0, count=None, cached=Conf.CACHED):\n    if cached:\n        return fetch_group_cached(group_id, failures, wait, count)\n    start = time()\n    if count:\n        while True:\n            if count_group(group_id) == count or wait and (time() - start) * 1000 >= wait >= 0:\n                break\n            sleep(0.01)\n    while True:\n        r = Task.get_task_group(group_id, failures)\n        if r:\n            return r\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Fetch a list of Tasks for a task group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_group_cached(group_id, failures=True, wait=0, count=None, broker=None):\n    if not broker:\n        broker = get_broker()\n    start = time()\n    if count:\n        while True:\n            if count_group_cached(group_id) == count or wait and (time() - start) * 1000 >= wait >= 0:\n                break\n            sleep(0.01)\n    while True:\n        group_list = broker.cache.get('{}:{}:keys'.format(broker.list_key, group_id))\n        if group_list:\n            task_list = []\n            for task_key in group_list:\n                task = SignedPackage.loads(broker.cache.get(task_key))\n                if task['success'] or failures:\n                    t = Task(id=task['id'],\n                             name=task['name'],\n                             func=task['func'],\n                             hook=task.get('hook'),\n                             args=task['args'],\n                             kwargs=task['kwargs'],\n                             started=task['started'],\n                             stopped=task['stopped'],\n                             result=task['result'],\n                             group=task.get('group'),\n                             success=task['success'])\n                    task_list.append(t)\n            return task_list\n        if (time() - start) * 1000 >= wait >= 0:\n            break\n        sleep(0.01)", "response": "Fetch a list of Tasks for a task group in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_group(group_id, failures=False, cached=Conf.CACHED):\n    if cached:\n        return count_group_cached(group_id, failures)\n    return Task.get_group_count(group_id, failures)", "response": "Count the number of tasks in a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count_group_cached(group_id, failures=False, broker=None):\n    if not broker:\n        broker = get_broker()\n    group_list = broker.cache.get('{}:{}:keys'.format(broker.list_key, group_id))\n    if group_list:\n        if not failures:\n            return len(group_list)\n        failure_count = 0\n        for task_key in group_list:\n            task = SignedPackage.loads(broker.cache.get(task_key))\n            if not task['success']:\n                failure_count += 1\n        return failure_count", "response": "Count the number of results in a group in the cache"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_group(group_id, tasks=False, cached=Conf.CACHED):\n    if cached:\n        return delete_group_cached(group_id)\n    return Task.delete_group(group_id, tasks)", "response": "Delete a group.\n\n    :param str group_id: the group id\n    :param bool tasks: If set to True this will also delete the group tasks.\n    Otherwise just the group label is removed.\n    :param bool cached: run this against the cache backend\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_group_cached(group_id, broker=None):\n    if not broker:\n        broker = get_broker()\n    group_key = '{}:{}:keys'.format(broker.list_key, group_id)\n    group_list = broker.cache.get(group_key)\n    broker.cache.delete_many(group_list)\n    broker.cache.delete(group_key)", "response": "Delete a group from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_cached(task_id, broker=None):\n    if not broker:\n        broker = get_broker()\n    return broker.cache.delete('{}:{}'.format(broker.list_key, task_id))", "response": "Delete a task from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef async_iter(func, args_iter, **kwargs):\n    iter_count = len(args_iter)\n    iter_group = uuid()[1]\n    # clean up the kwargs\n    options = kwargs.get('q_options', kwargs)\n    options.pop('hook', None)\n    options['broker'] = options.get('broker', get_broker())\n    options['group'] = iter_group\n    options['iter_count'] = iter_count\n    if options.get('cached', None):\n        options['iter_cached'] = options['cached']\n    options['cached'] = True\n    # save the original arguments\n    broker = options['broker']\n    broker.cache.set('{}:{}:args'.format(broker.list_key, iter_group), SignedPackage.dumps(args_iter))\n    for args in args_iter:\n        if not isinstance(args, tuple):\n            args = (args,)\n        async_task(func, *args, **options)\n    return iter_group", "response": "enqueues a function with iterable arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef async_chain(chain, group=None, cached=Conf.CACHED, sync=Conf.SYNC, broker=None):\n    if not group:\n        group = uuid()[1]\n    args = ()\n    kwargs = {}\n    task = chain.pop(0)\n    if type(task) is not tuple:\n        task = (task,)\n    if len(task) > 1:\n        args = task[1]\n    if len(task) > 2:\n        kwargs = task[2]\n    kwargs['chain'] = chain\n    kwargs['group'] = group\n    kwargs['cached'] = cached\n    kwargs['sync'] = sync\n    kwargs['broker'] = broker or get_broker()\n    async_task(task[0], *args, **kwargs)\n    return group", "response": "Enqueues a chain of tasks\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sync(pack):\n    task_queue = Queue()\n    result_queue = Queue()\n    task = SignedPackage.loads(pack)\n    task_queue.put(task)\n    task_queue.put('STOP')\n    worker(task_queue, result_queue, Value('f', -1))\n    result_queue.put('STOP')\n    monitor(result_queue)\n    task_queue.close()\n    task_queue.join_thread()\n    result_queue.close()\n    result_queue.join_thread()\n    return task['id']", "response": "Simulate a package travelling through the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, *args):\n        self.args.append(args)\n        if self.started:\n            self.started = False\n        return self.length()", "response": "add arguments to the set\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        self.kwargs['cached'] = self.cached\n        self.kwargs['sync'] = self.sync\n        self.kwargs['broker'] = self.broker\n        self.id = async_iter(self.func, self.args, **self.kwargs)\n        self.started = True\n        return self.id", "response": "Start queueing the tasks to the worker cluster\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the full list of results.", "response": "def result(self, wait=0):\n        \"\"\"\n        return the full list of results.\n        :param int wait: how many milliseconds to wait for a result\n        :return: an unsorted list of results\n        \"\"\"\n        if self.started:\n            return result(self.id, wait=wait, cached=self.cached)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the task result objects.", "response": "def fetch(self, wait=0):\n        \"\"\"\n        get the task result objects.\n        :param int wait: how many milliseconds to wait for a result\n        :return: an unsorted list of task objects\n        \"\"\"\n        if self.started:\n            return fetch(self.id, wait=wait, cached=self.cached)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append(self, func, *args, **kwargs):\n        self.chain.append((func, args, kwargs))\n        # remove existing results\n        if self.started:\n            delete_group(self.group)\n            self.started = False\n        return self.length()", "response": "add a task to the chain"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\n        self.group = async_chain(chain=self.chain[:], group=self.group, cached=self.cached, sync=self.sync,\n                                 broker=self.broker)\n        self.started = True\n        return self.group", "response": "Start queueing the chain to the worker cluster\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef result(self, wait=0):\n        if self.started:\n            return result_group(self.group, wait=wait, count=self.length(), cached=self.cached)", "response": "return the full list of results from the chain when it finishes. blocks until wait milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the task result objects from the chain when it finishes", "response": "def fetch(self, failures=True, wait=0):\n        \"\"\"\n        get the task result objects from the chain when it finishes. blocks until timeout.\n        :param failures: include failed tasks\n        :param int wait: how many milliseconds to wait for a result\n        :return: an unsorted list of task objects\n        \"\"\"\n        if self.started:\n            return fetch_group(self.group, failures=failures, wait=wait, count=self.length(), cached=self.cached)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the index of the currently executing chain element", "response": "def current(self):\n        \"\"\"\n        get the index of the currently executing chain element\n        :return int: current chain index\n        \"\"\"\n        if not self.started:\n            return None\n        return count_group(self.group, cached=self.cached)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retry_failed(FailAdmin, request, queryset):\n    for task in queryset:\n        async_task(task.func, *task.args or (), hook=task.hook, **task.kwargs or {})\n        task.delete()", "response": "Submit selected tasks back to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_readonly_fields(self, request, obj=None):\n        return list(self.readonly_fields) + [field.name for field in obj._meta.fields]", "response": "Set all fields readonly."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npush tasks from the broker to the task queue.", "response": "def pusher(task_queue, event, broker=None):\n    \"\"\"\n    Pulls tasks of the broker and puts them in the task queue\n    :type task_queue: multiprocessing.Queue\n    :type event: multiprocessing.Event\n    \"\"\"\n    if not broker:\n        broker = get_broker()\n    logger.info(_('{} pushing tasks at {}').format(current_process().name, current_process().pid))\n    while True:\n        try:\n            task_set = broker.dequeue()\n        except Exception as e:\n            logger.error(e, traceback.format_exc())\n            # broker probably crashed. Let the sentinel handle it.\n            sleep(10)\n            break\n        if task_set:\n            for task in task_set:\n                ack_id = task[0]\n                # unpack the task\n                try:\n                    task = SignedPackage.loads(task[1])\n                except (TypeError, BadSignature) as e:\n                    logger.error(e, traceback.format_exc())\n                    broker.fail(ack_id)\n                    continue\n                task['ack_id'] = ack_id\n                task_queue.put(task)\n            logger.debug(_('queueing from {}').format(broker.list_key))\n        if event.is_set():\n            break\n    logger.info(_(\"{} stopped pushing tasks\").format(current_process().name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef monitor(result_queue, broker=None):\n    if not broker:\n        broker = get_broker()\n    name = current_process().name\n    logger.info(_(\"{} monitoring at {}\").format(name, current_process().pid))\n    for task in iter(result_queue.get, 'STOP'):\n        # save the result\n        if task.get('cached', False):\n            save_cached(task, broker)\n        else:\n            save_task(task, broker)\n        # acknowledge result\n        ack_id = task.pop('ack_id', False)\n        if ack_id and (task['success'] or task.get('ack_failure', False)):\n            broker.acknowledge(ack_id)\n        # log the result\n        if task['success']:\n            # log success\n            logger.info(_(\"Processed [{}]\").format(task['name']))\n        else:\n            # log failure\n            logger.error(_(\"Failed [{}] - {}\").format(task['name'], task['result']))\n    logger.info(_(\"{} stopped monitoring results\").format(name))", "response": "Takes a result queue and stores them to the result_queue and logs the result to the logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef worker(task_queue, result_queue, timer, timeout=Conf.TIMEOUT):\n    name = current_process().name\n    logger.info(_('{} ready for work at {}').format(name, current_process().pid))\n    task_count = 0\n    if timeout is None:\n        timeout = -1\n    # Start reading the task queue\n    for task in iter(task_queue.get, 'STOP'):\n        result = None\n        timer.value = -1  # Idle\n        task_count += 1\n        # Get the function from the task\n        logger.info(_('{} processing [{}]').format(name, task['name']))\n        f = task['func']\n        # if it's not an instance try to get it from the string\n        if not callable(task['func']):\n            try:\n                module, func = f.rsplit('.', 1)\n                m = importlib.import_module(module)\n                f = getattr(m, func)\n            except (ValueError, ImportError, AttributeError) as e:\n                result = (e, False)\n                if error_reporter:\n                    error_reporter.report()\n        # We're still going\n        if not result:\n            db.close_old_connections()\n            timer_value = task['kwargs'].pop('timeout', timeout)\n            # signal execution\n            pre_execute.send(sender=\"django_q\", func=f, task=task)\n            # execute the payload\n            timer.value = timer_value  # Busy\n            try:\n                res = f(*task['args'], **task['kwargs'])\n                result = (res, True)\n            except Exception as e:\n                result = ('{} : {}'.format(e, traceback.format_exc()), False)\n                if error_reporter:\n                    error_reporter.report()\n        with timer.get_lock():\n            # Process result\n            task['result'] = result[0]\n            task['success'] = result[1]\n            task['stopped'] = timezone.now()\n            result_queue.put(task)\n            timer.value = -1  # Idle\n            # Recycle\n            if task_count == Conf.RECYCLE:\n                timer.value = -2  # Recycled\n                break\n    logger.info(_('{} stopped doing work').format(name))", "response": "This function is the worker function that processes tasks from the task queue and executes them and puts the result back in the result queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_task(task, broker):\n    # SAVE LIMIT < 0 : Don't save success\n    if not task.get('save', Conf.SAVE_LIMIT >= 0) and task['success']:\n        return\n    # enqueues next in a chain\n    if task.get('chain', None):\n        django_q.tasks.async_chain(task['chain'], group=task['group'], cached=task['cached'], sync=task['sync'], broker=broker)\n    # SAVE LIMIT > 0: Prune database, SAVE_LIMIT 0: No pruning\n    db.close_old_connections()\n    try:\n        if task['success'] and 0 < Conf.SAVE_LIMIT <= Success.objects.count():\n            Success.objects.last().delete()\n        # check if this task has previous results\n        if Task.objects.filter(id=task['id'], name=task['name']).exists():\n            existing_task = Task.objects.get(id=task['id'], name=task['name'])\n            # only update the result if it hasn't succeeded yet\n            if not existing_task.success:\n                existing_task.stopped = task['stopped']\n                existing_task.result = task['result']\n                existing_task.success = task['success']\n                existing_task.save()\n        else:\n            Task.objects.create(id=task['id'],\n                                name=task['name'],\n                                func=task['func'],\n                                hook=task.get('hook'),\n                                args=task['args'],\n                                kwargs=task['kwargs'],\n                                started=task['started'],\n                                stopped=task['stopped'],\n                                result=task['result'],\n                                group=task.get('group'),\n                                success=task['success']\n                                )\n    except Exception as e:\n        logger.error(e)", "response": "Saves the task to Django or the cache\n SARL"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scheduler(broker=None):\n    if not broker:\n        broker = get_broker()\n    db.close_old_connections()\n    try:\n        for s in Schedule.objects.exclude(repeats=0).filter(next_run__lt=timezone.now()):\n            args = ()\n            kwargs = {}\n            # get args, kwargs and hook\n            if s.kwargs:\n                try:\n                    # eval should be safe here because dict()\n                    kwargs = eval('dict({})'.format(s.kwargs))\n                except SyntaxError:\n                    kwargs = {}\n            if s.args:\n                args = ast.literal_eval(s.args)\n                # single value won't eval to tuple, so:\n                if type(args) != tuple:\n                    args = (args,)\n            q_options = kwargs.get('q_options', {})\n            if s.hook:\n                q_options['hook'] = s.hook\n            # set up the next run time\n            if not s.schedule_type == s.ONCE:\n                next_run = arrow.get(s.next_run)\n                while True:\n                    if s.schedule_type == s.MINUTES:\n                        next_run = next_run.replace(minutes=+(s.minutes or 1))\n                    elif s.schedule_type == s.HOURLY:\n                        next_run = next_run.replace(hours=+1)\n                    elif s.schedule_type == s.DAILY:\n                        next_run = next_run.replace(days=+1)\n                    elif s.schedule_type == s.WEEKLY:\n                        next_run = next_run.replace(weeks=+1)\n                    elif s.schedule_type == s.MONTHLY:\n                        next_run = next_run.replace(months=+1)\n                    elif s.schedule_type == s.QUARTERLY:\n                        next_run = next_run.replace(months=+3)\n                    elif s.schedule_type == s.YEARLY:\n                        next_run = next_run.replace(years=+1)\n                    if Conf.CATCH_UP or next_run > arrow.utcnow():\n                        break\n                s.next_run = next_run.datetime\n                s.repeats += -1\n            # send it to the cluster\n            q_options['broker'] = broker\n            q_options['group'] = q_options.get('group', s.name or s.id)\n            kwargs['q_options'] = q_options\n            s.task = django_q.tasks.async_task(s.func, *args, **kwargs)\n            # log it\n            if not s.task:\n                logger.error(\n                    _('{} failed to create a task from schedule [{}]').format(current_process().name,\n                                                                              s.name or s.id))\n            else:\n                logger.info(\n                    _('{} created a task from schedule [{}]').format(current_process().name, s.name or s.id))\n            # default behavior is to delete a ONCE schedule\n            if s.schedule_type == s.ONCE:\n                if s.repeats < 0:\n                    s.delete()\n                    continue\n                # but not if it has a positive repeats\n                s.repeats = 0\n            # save the schedule\n            s.save()\n    except Exception as e:\n        logger.error(e)", "response": "Creates a task from a schedule at the scheduled time and schedules next run"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_cpu_affinity(n, process_ids, actual=not Conf.TESTING):\n    # check if we have the psutil module\n    if not psutil:\n        logger.warning('Skipping cpu affinity because psutil was not found.')\n        return\n    # check if the platform supports cpu_affinity\n    if actual and not hasattr(psutil.Process(process_ids[0]), 'cpu_affinity'):\n        logger.warning('Faking cpu affinity because it is not supported on this platform')\n        actual = False\n    # get the available processors\n    cpu_list = list(range(psutil.cpu_count()))\n    # affinities of 0 or gte cpu_count, equals to no affinity\n    if not n or n >= len(cpu_list):\n        return\n    # spread the workers over the available processors.\n    index = 0\n    for pid in process_ids:\n        affinity = []\n        for k in range(n):\n            if index == len(cpu_list):\n                index = 0\n            affinity.append(cpu_list[index])\n            index += 1\n        if psutil.pid_exists(pid):\n            p = psutil.Process(pid)\n            if actual:\n                p.cpu_affinity(affinity)\n            logger.info(_('{} will use cpu {}').format(pid, affinity))", "response": "Sets the cpu affinity for the processes in process_ids."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nspawning a process and return it.", "response": "def spawn_process(self, target, *args):\n        \"\"\"\n        :type target: function or class\n        \"\"\"\n        p = Process(target=target, args=args)\n        p.daemon = True\n        if target == worker:\n            p.daemon = Conf.DAEMONIZE_WORKERS\n            p.timer = args[2]\n            self.pool.append(p)\n        p.start()\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reincarnate(self, process):\n        db.connections.close_all()  # Close any old connections\n        if process == self.monitor:\n            self.monitor = self.spawn_monitor()\n            logger.error(_(\"reincarnated monitor {} after sudden death\").format(process.name))\n        elif process == self.pusher:\n            self.pusher = self.spawn_pusher()\n            logger.error(_(\"reincarnated pusher {} after sudden death\").format(process.name))\n        else:\n            self.pool.remove(process)\n            self.spawn_worker()\n            if process.timer.value == 0:\n                # only need to terminate on timeout, otherwise we risk destabilizing the queues\n                process.terminate()\n                logger.warn(_(\"reincarnated worker {} after timeout\").format(process.name))\n            elif int(process.timer.value) == -2:\n                logger.info(_(\"recycled worker {}\").format(process.name))\n            else:\n                logger.error(_(\"reincarnated worker {} after death\").format(process.name))\n\n        self.reincarnations += 1", "response": "reincarnate the process with the related items"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_service_connection(self, service, service_command=None, create=True, timeout_ms=None):\n\n        connection = self._service_connections.get(service, None)\n\n        if connection:\n            return connection\n\n        if not connection and not create:\n            return None\n\n        if service_command:\n            destination_str = b'%s:%s' % (service, service_command)\n        else:\n            destination_str = service\n\n        connection = self.protocol_handler.Open(\n            self._handle, destination=destination_str, timeout_ms=timeout_ms)\n\n        self._service_connections.update({service: connection})\n\n        return connection", "response": "Get the AdbConnection for a specific service."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the device.", "response": "def _Connect(self, banner=None, **kwargs):\n        \"\"\"Connect to the device.\n\n        Args:\n          banner: See protocol_handler.Connect.\n          **kwargs: See protocol_handler.Connect and adb_commands.ConnectDevice for kwargs.\n               Includes handle, rsa_keys, and auth_timeout_ms.\n        Returns:\n          An instance of this class if the device connected successfully.\n        \"\"\"\n\n        if not banner:\n            banner = socket.gethostname().encode()\n\n        conn_str = self.protocol_handler.Connect(self._handle, banner=banner, **kwargs)\n\n        # Remove banner and colons after device state (state::banner)\n        parts = conn_str.split(b'::')\n        self._device_state = parts[0]\n\n        # Break out the build prop info\n        self.build_props = str(parts[1].split(b';'))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Install(self, apk_path, destination_dir='', replace_existing=True,\n                grant_permissions=False, timeout_ms=None, transfer_progress_callback=None):\n        \"\"\"Install an apk to the device.\n\n        Doesn't support verifier file, instead allows destination directory to be\n        overridden.\n\n        Args:\n          apk_path: Local path to apk to install.\n          destination_dir: Optional destination directory. Use /system/app/ for\n            persistent applications.\n          replace_existing: whether to replace existing application\n          grant_permissions: If True, grant all permissions to the app specified in its manifest\n          timeout_ms: Expected timeout for pushing and installing.\n          transfer_progress_callback: callback method that accepts filename, bytes_written and total_bytes of APK transfer\n\n        Returns:\n          The pm install output.\n        \"\"\"\n        if not destination_dir:\n            destination_dir = '/data/local/tmp/'\n        basename = os.path.basename(apk_path)\n        destination_path = posixpath.join(destination_dir, basename)\n        self.Push(apk_path, destination_path, timeout_ms=timeout_ms, progress_callback=transfer_progress_callback)\n\n        cmd = ['pm install']\n        if grant_permissions:\n            cmd.append('-g')\n        if replace_existing:\n            cmd.append('-r')\n        cmd.append('\"{}\"'.format(destination_path))\n\n        ret = self.Shell(' '.join(cmd), timeout_ms=timeout_ms)\n\n        # Remove the apk\n        rm_cmd = ['rm', destination_path]\n        rmret = self.Shell(' '.join(rm_cmd), timeout_ms=timeout_ms)\n\n        return ret", "response": "Install an apk to the device."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Uninstall(self, package_name, keep_data=False, timeout_ms=None):\n        cmd = ['pm uninstall']\n        if keep_data:\n            cmd.append('-k')\n        cmd.append('\"%s\"' % package_name)\n\n        return self.Shell(' '.join(cmd), timeout_ms=timeout_ms)", "response": "Removes a target package from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes a file or directory to the device.", "response": "def Push(self, source_file, device_filename, mtime='0', timeout_ms=None, progress_callback=None, st_mode=None):\n        \"\"\"Push a file or directory to the device.\n\n        Args:\n          source_file: Either a filename, a directory or file-like object to push to\n                       the device.\n          device_filename: Destination on the device to write to.\n          mtime: Optional, modification time to set on the file.\n          timeout_ms: Expected timeout for any part of the push.\n          st_mode: stat mode for filename\n          progress_callback: callback method that accepts filename, bytes_written and total_bytes,\n                             total_bytes will be -1 for file-like objects\n        \"\"\"\n\n        if isinstance(source_file, str):\n            if os.path.isdir(source_file):\n                self.Shell(\"mkdir \" + device_filename)\n                for f in os.listdir(source_file):\n                    self.Push(os.path.join(source_file, f), device_filename + '/' + f,\n                              progress_callback=progress_callback)\n                return\n            source_file = open(source_file, \"rb\")\n\n        with source_file:\n            connection = self.protocol_handler.Open(\n                self._handle, destination=b'sync:', timeout_ms=timeout_ms)\n            kwargs={}\n            if st_mode is not None:\n                kwargs['st_mode'] = st_mode\n            self.filesync_handler.Push(connection, source_file, device_filename,\n                                       mtime=int(mtime), progress_callback=progress_callback, **kwargs)\n        connection.Close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Pull(self, device_filename, dest_file=None, timeout_ms=None, progress_callback=None):\n        if not dest_file:\n            dest_file = io.BytesIO()\n        elif isinstance(dest_file, str):\n            dest_file = open(dest_file, 'wb')\n        elif isinstance(dest_file, file):\n            pass\n        else:\n            raise ValueError(\"destfile is of unknown type\")\n\n        conn = self.protocol_handler.Open(\n            self._handle, destination=b'sync:', timeout_ms=timeout_ms)\n\n        self.filesync_handler.Pull(conn, device_filename, dest_file, progress_callback)\n\n        conn.Close()\n        if isinstance(dest_file, io.BytesIO):\n            return dest_file.getvalue()\n        else:\n            dest_file.close()\n            if hasattr(dest_file, 'name'):\n                return os.path.exists(dest_file.name)\n            # We don't know what the path is, so we just assume it exists.\n            return True", "response": "Pull a file from the device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a file s stat() information.", "response": "def Stat(self, device_filename):\n        \"\"\"Get a file's stat() information.\"\"\"\n        connection = self.protocol_handler.Open(self._handle, destination=b'sync:')\n        mode, size, mtime = self.filesync_handler.Stat(\n            connection, device_filename)\n        connection.Close()\n        return mode, size, mtime"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef List(self, device_path):\n        connection = self.protocol_handler.Open(self._handle, destination=b'sync:')\n        listing = self.filesync_handler.List(connection, device_path)\n        connection.Close()\n        return listing", "response": "Return a directory listing of the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Shell(self, command, timeout_ms=None):\n        return self.protocol_handler.Command(\n            self._handle, service=b'shell', command=command,\n            timeout_ms=timeout_ms)", "response": "Runs a command on the device returning the output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a command on the target and yields each line of output.", "response": "def StreamingShell(self, command, timeout_ms=None):\n        \"\"\"Run command on the device, yielding each line of output.\n\n        Args:\n          command: Command to run on the target.\n          timeout_ms: Maximum time to allow the command to run.\n\n        Yields:\n          The responses from the shell command.\n        \"\"\"\n        return self.protocol_handler.StreamingCommand(\n            self._handle, service=b'shell', command=command,\n            timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef InteractiveShell(self, cmd=None, strip_cmd=True, delim=None, strip_delim=True):\n        conn = self._get_service_connection(b'shell:')\n\n        return self.protocol_handler.InteractiveShellCommand(\n            conn, cmd=cmd, strip_cmd=strip_cmd,\n            delim=delim, strip_delim=strip_delim)", "response": "Get stdout from the currently open interactive shell and optionally run a commandon the device returning all output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef InterfaceMatcher(clazz, subclass, protocol):\n    interface = (clazz, subclass, protocol)\n\n    def Matcher(device):\n        for setting in device.iterSettings():\n            if GetInterface(setting) == interface:\n                return setting\n\n    return Matcher", "response": "Returns a matcher that returns the setting with the given interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens the USB device for this setting and claims the interface.", "response": "def Open(self):\n        \"\"\"Opens the USB device for this setting, and claims the interface.\"\"\"\n        # Make sure we close any previous handle open to this usb device.\n        port_path = tuple(self.port_path)\n        with self._HANDLE_CACHE_LOCK:\n            old_handle = self._HANDLE_CACHE.get(port_path)\n            if old_handle is not None:\n                old_handle.Close()\n\n        self._read_endpoint = None\n        self._write_endpoint = None\n\n        for endpoint in self._setting.iterEndpoints():\n            address = endpoint.getAddress()\n            if address & libusb1.USB_ENDPOINT_DIR_MASK:\n                self._read_endpoint = address\n                self._max_read_packet_len = endpoint.getMaxPacketSize()\n            else:\n                self._write_endpoint = address\n\n        assert self._read_endpoint is not None\n        assert self._write_endpoint is not None\n\n        handle = self._device.open()\n        iface_number = self._setting.getNumber()\n        try:\n            if (platform.system() != 'Windows'\n                    and handle.kernelDriverActive(iface_number)):\n                handle.detachKernelDriver(iface_number)\n        except libusb1.USBError as e:\n            if e.value == libusb1.LIBUSB_ERROR_NOT_FOUND:\n                _LOG.warning('Kernel driver not found for interface: %s.', iface_number)\n            else:\n                raise\n        handle.claimInterface(iface_number)\n        self._handle = handle\n        self._interface_number = iface_number\n\n        with self._HANDLE_CACHE_LOCK:\n            self._HANDLE_CACHE[port_path] = self\n        # When this object is deleted, make sure it's closed.\n        weakref.ref(self, self.Close)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef PortPathMatcher(cls, port_path):\n        if isinstance(port_path, str):\n            # Convert from sysfs path to port_path.\n            port_path = [int(part) for part in SYSFS_PORT_SPLIT_RE.split(port_path)]\n        return lambda device: device.port_path == port_path", "response": "Returns a function that returns a device matcher for the given port path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the first device that matches according to the keyword args.", "response": "def Find(cls, setting_matcher, port_path=None, serial=None, timeout_ms=None):\n        \"\"\"Gets the first device that matches according to the keyword args.\"\"\"\n        if port_path:\n            device_matcher = cls.PortPathMatcher(port_path)\n            usb_info = port_path\n        elif serial:\n            device_matcher = cls.SerialMatcher(serial)\n            usb_info = serial\n        else:\n            device_matcher = None\n            usb_info = 'first'\n        return cls.FindFirst(setting_matcher, device_matcher,\n                             usb_info=usb_info, timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef FindFirst(cls, setting_matcher, device_matcher=None, **kwargs):\n        try:\n            return next(cls.FindDevices(\n                setting_matcher, device_matcher=device_matcher, **kwargs))\n        except StopIteration:\n            raise usb_exceptions.DeviceNotFoundError(\n                'No device available, or it is in the wrong configuration.')", "response": "Find and return the first matching device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding and yield the devices that match.", "response": "def FindDevices(cls, setting_matcher, device_matcher=None,\n                    usb_info='', timeout_ms=None):\n        \"\"\"Find and yield the devices that match.\n\n        Args:\n          setting_matcher: Function that returns the setting to use given a\n            usb1.USBDevice, or None if the device doesn't have a valid setting.\n          device_matcher: Function that returns True if the given UsbHandle is\n            valid. None to match any device.\n          usb_info: Info string describing device(s).\n          timeout_ms: Default timeout of commands in milliseconds.\n\n        Yields:\n          UsbHandle instances\n        \"\"\"\n        ctx = usb1.USBContext()\n        for device in ctx.getDeviceList(skip_on_error=True):\n            setting = setting_matcher(device)\n            if setting is None:\n                continue\n\n            handle = cls(device, setting, usb_info=usb_info, timeout_ms=timeout_ms)\n            if device_matcher is None or device_matcher(handle):\n                yield handle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a packet and expect an Ack.", "response": "def Write(self, data):\n        \"\"\"Write a packet and expect an Ack.\"\"\"\n        self._Send(b'WRTE', arg0=self.local_id, arg1=self.remote_id, data=data)\n        # Expect an ack in response.\n        cmd, okay_data = self.ReadUntil(b'OKAY')\n        if cmd != b'OKAY':\n            if cmd == b'FAIL':\n                raise usb_exceptions.AdbCommandFailureException(\n                    'Command failed.', okay_data)\n            raise InvalidCommandError(\n                'Expected an OKAY in response to a WRITE, got %s (%s)',\n                cmd, okay_data)\n        return len(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a packet acknowledge any write packets.", "response": "def ReadUntil(self, *expected_cmds):\n        \"\"\"Read a packet, Ack any write packets.\"\"\"\n        cmd, remote_id, local_id, data = AdbMessage.Read(\n            self.usb, expected_cmds, self.timeout_ms)\n        if local_id != 0 and self.local_id != local_id:\n            raise InterleavedDataError(\"We don't support multiple streams...\")\n        if remote_id != 0 and self.remote_id != remote_id:\n            raise InvalidResponseError(\n                'Incorrect remote id, expected %s got %s' % (\n                    self.remote_id, remote_id))\n        # Ack write packets.\n        if cmd == b'WRTE':\n            self.Okay()\n        return cmd, data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ReadUntilClose(self):\n        while True:\n            cmd, data = self.ReadUntil(b'CLSE', b'WRTE')\n            if cmd == b'CLSE':\n                self._Send(b'CLSE', arg0=self.local_id, arg1=self.remote_id)\n                break\n            if cmd != b'WRTE':\n                if cmd == b'FAIL':\n                    raise usb_exceptions.AdbCommandFailureException(\n                        'Command failed.', data)\n                raise InvalidCommandError('Expected a WRITE or a CLOSE, got %s (%s)',\n                                          cmd, data)\n            yield data", "response": "Yields packets until a Close packet is received."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Pack(self):\n        return struct.pack(self.format, self.command, self.arg0, self.arg1,\n                           len(self.data), self.checksum, self.magic)", "response": "Returns this message in an over - the - wire format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Send(self, usb, timeout_ms=None):\n        usb.BulkWrite(self.Pack(), timeout_ms)\n        usb.BulkWrite(self.data, timeout_ms)", "response": "Send this message over USB."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreceiving a response from the device.", "response": "def Read(cls, usb, expected_cmds, timeout_ms=None, total_timeout_ms=None):\n        \"\"\"Receive a response from the device.\"\"\"\n        total_timeout_ms = usb.Timeout(total_timeout_ms)\n        start = time.time()\n        while True:\n            msg = usb.BulkRead(24, timeout_ms)\n            cmd, arg0, arg1, data_length, data_checksum = cls.Unpack(msg)\n            command = cls.constants.get(cmd)\n            if not command:\n                raise InvalidCommandError(\n                    'Unknown command: %x' % cmd, cmd, (arg0, arg1))\n            if command in expected_cmds:\n                break\n\n            if time.time() - start > total_timeout_ms:\n                raise InvalidCommandError(\n                    'Never got one of the expected responses (%s)' % expected_cmds,\n                    cmd, (timeout_ms, total_timeout_ms))\n\n        if data_length > 0:\n            data = bytearray()\n            while data_length > 0:\n                temp = usb.BulkRead(data_length, timeout_ms)\n                if len(temp) != data_length:\n                    print(\n                        \"Data_length {} does not match actual number of bytes read: {}\".format(data_length, len(temp)))\n                data += temp\n\n                data_length -= len(temp)\n\n            actual_checksum = cls.CalculateChecksum(data)\n            if actual_checksum != data_checksum:\n                raise InvalidChecksumError(\n                    'Received checksum %s != %s', (actual_checksum, data_checksum))\n        else:\n            data = b''\n        return command, arg0, arg1, bytes(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestablishing a new connection to the USB device.", "response": "def Connect(cls, usb, banner=b'notadb', rsa_keys=None, auth_timeout_ms=100):\n        \"\"\"Establish a new connection to the device.\n\n        Args:\n          usb: A USBHandle with BulkRead and BulkWrite methods.\n          banner: A string to send as a host identifier.\n          rsa_keys: List of AuthSigner subclass instances to be used for\n              authentication. The device can either accept one of these via the Sign\n              method, or we will send the result of GetPublicKey from the first one\n              if the device doesn't accept any of them.\n          auth_timeout_ms: Timeout to wait for when sending a new public key. This\n              is only relevant when we send a new public key. The device shows a\n              dialog and this timeout is how long to wait for that dialog. If used\n              in automation, this should be low to catch such a case as a failure\n              quickly; while in interactive settings it should be high to allow\n              users to accept the dialog. We default to automation here, so it's low\n              by default.\n\n        Returns:\n          The device's reported banner. Always starts with the state (device,\n              recovery, or sideload), sometimes includes information after a : with\n              various product information.\n\n        Raises:\n          usb_exceptions.DeviceAuthError: When the device expects authentication,\n              but we weren't given any valid keys.\n          InvalidResponseError: When the device does authentication in an\n              unexpected way.\n        \"\"\"\n        # In py3, convert unicode to bytes. In py2, convert str to bytes.\n        # It's later joined into a byte string, so in py2, this ends up kind of being a no-op.\n        if isinstance(banner, str):\n            banner = bytearray(banner, 'utf-8')\n\n        msg = cls(\n            command=b'CNXN', arg0=VERSION, arg1=MAX_ADB_DATA,\n            data=b'host::%s\\0' % banner)\n        msg.Send(usb)\n        cmd, arg0, arg1, banner = cls.Read(usb, [b'CNXN', b'AUTH'])\n        if cmd == b'AUTH':\n            if not rsa_keys:\n                raise usb_exceptions.DeviceAuthError(\n                    'Device authentication required, no keys available.')\n            # Loop through our keys, signing the last 'banner' or token.\n            for rsa_key in rsa_keys:\n                if arg0 != AUTH_TOKEN:\n                    raise InvalidResponseError(\n                        'Unknown AUTH response: %s %s %s' % (arg0, arg1, banner))\n\n                # Do not mangle the banner property here by converting it to a string\n                signed_token = rsa_key.Sign(banner)\n                msg = cls(\n                    command=b'AUTH', arg0=AUTH_SIGNATURE, arg1=0, data=signed_token)\n                msg.Send(usb)\n                cmd, arg0, unused_arg1, banner = cls.Read(usb, [b'CNXN', b'AUTH'])\n                if cmd == b'CNXN':\n                    return banner\n            # None of the keys worked, so send a public key.\n            msg = cls(\n                command=b'AUTH', arg0=AUTH_RSAPUBLICKEY, arg1=0,\n                data=rsa_keys[0].GetPublicKey() + b'\\0')\n            msg.Send(usb)\n            try:\n                cmd, arg0, unused_arg1, banner = cls.Read(\n                    usb, [b'CNXN'], timeout_ms=auth_timeout_ms)\n            except usb_exceptions.ReadFailedError as e:\n                if e.usb_error.value == -7:  # Timeout.\n                    raise usb_exceptions.DeviceAuthError(\n                        'Accept auth key on device, then retry.')\n                raise\n            # This didn't time-out, so we got a CNXN response.\n            return banner\n        return banner"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Open(cls, usb, destination, timeout_ms=None):\n        local_id = 1\n        msg = cls(\n            command=b'OPEN', arg0=local_id, arg1=0,\n            data=destination + b'\\0')\n        msg.Send(usb, timeout_ms)\n        cmd, remote_id, their_local_id, _ = cls.Read(usb, [b'CLSE', b'OKAY'],\n                                                     timeout_ms=timeout_ms)\n        if local_id != their_local_id:\n            raise InvalidResponseError(\n                'Expected the local_id to be {}, got {}'.format(local_id, their_local_id))\n        if cmd == b'CLSE':\n            # Some devices seem to be sending CLSE once more after a request, this *should* handle it\n            cmd, remote_id, their_local_id, _ = cls.Read(usb, [b'CLSE', b'OKAY'],\n                                                         timeout_ms=timeout_ms)\n            # Device doesn't support this service.\n            if cmd == b'CLSE':\n                return None\n        if cmd != b'OKAY':\n            raise InvalidCommandError('Expected a ready response, got {}'.format(cmd),\n                                      cmd, (remote_id, their_local_id))\n        return _AdbConnection(usb, local_id, remote_id, timeout_ms)", "response": "Opens a new connection to the USB device via an OPEN message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Command(cls, usb, service, command='', timeout_ms=None):\n        return ''.join(cls.StreamingCommand(usb, service, command, timeout_ms))", "response": "Send a command to a service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef StreamingCommand(cls, usb, service, command='', timeout_ms=None):\n        if not isinstance(command, bytes):\n            command = command.encode('utf8')\n        connection = cls.Open(\n            usb, destination=b'%s:%s' % (service, command),\n            timeout_ms=timeout_ms)\n        for data in connection.ReadUntilClose():\n            yield data.decode('utf8')", "response": "A method that returns a list of responses from a single command."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a command to the current InteractiveShell.", "response": "def InteractiveShellCommand(cls, conn, cmd=None, strip_cmd=True, delim=None, strip_delim=True, clean_stdout=True):\n        \"\"\"Retrieves stdout of the current InteractiveShell and sends a shell command if provided\n        TODO: Should we turn this into a yield based function so we can stream all output?\n\n        Args:\n          conn: Instance of AdbConnection\n          cmd: Optional. Command to run on the target.\n          strip_cmd: Optional (default True). Strip command name from stdout.\n          delim: Optional. Delimiter to look for in the output to know when to stop expecting more output\n          (usually the shell prompt)\n          strip_delim: Optional (default True): Strip the provided delimiter from the output\n          clean_stdout: Cleanup the stdout stream of any backspaces and the characters that were deleted by the backspace\n        Returns:\n          The stdout from the shell command.\n        \"\"\"\n\n        if delim is not None and not isinstance(delim, bytes):\n            delim = delim.encode('utf-8')\n\n        # Delimiter may be shell@hammerhead:/ $\n        # The user or directory could change, making the delimiter somthing like root@hammerhead:/data/local/tmp $\n        # Handle a partial delimiter to search on and clean up\n        if delim:\n            user_pos = delim.find(b'@')\n            dir_pos = delim.rfind(b':/')\n            if user_pos != -1 and dir_pos != -1:\n                partial_delim = delim[user_pos:dir_pos + 1]  # e.g. @hammerhead:\n            else:\n                partial_delim = delim\n        else:\n            partial_delim = None\n\n        stdout = ''\n        stdout_stream = BytesIO()\n        original_cmd = ''\n\n        try:\n\n            if cmd:\n                original_cmd = str(cmd)\n                cmd += '\\r'  # Required. Send a carriage return right after the cmd\n                cmd = cmd.encode('utf8')\n\n                # Send the cmd raw\n                bytes_written = conn.Write(cmd)\n\n                if delim:\n                    # Expect multiple WRTE cmds until the delim (usually terminal prompt) is detected\n\n                    data = b''\n                    while partial_delim not in data:\n                        cmd, data = conn.ReadUntil(b'WRTE')\n                        stdout_stream.write(data)\n\n                else:\n                    # Otherwise, expect only a single WRTE\n                    cmd, data = conn.ReadUntil(b'WRTE')\n\n                    # WRTE cmd from device will follow with stdout data\n                    stdout_stream.write(data)\n\n            else:\n\n                # No cmd provided means we should just expect a single line from the terminal. Use this sparingly\n                cmd, data = conn.ReadUntil(b'WRTE')\n                if cmd == b'WRTE':\n                    # WRTE cmd from device will follow with stdout data\n                    stdout_stream.write(data)\n                else:\n                    print(\"Unhandled cmd: {}\".format(cmd))\n\n            cleaned_stdout_stream = BytesIO()\n            if clean_stdout:\n                stdout_bytes = stdout_stream.getvalue()\n\n                bsruns = {}  # Backspace runs tracking\n                next_start_pos = 0\n                last_run_pos, last_run_len = find_backspace_runs(stdout_bytes, next_start_pos)\n\n                if last_run_pos != -1 and last_run_len != 0:\n                    bsruns.update({last_run_pos: last_run_len})\n                    cleaned_stdout_stream.write(stdout_bytes[next_start_pos:(last_run_pos - last_run_len)])\n                    next_start_pos += last_run_pos + last_run_len\n\n                while last_run_pos != -1:\n                    last_run_pos, last_run_len = find_backspace_runs(stdout_bytes[next_start_pos:], next_start_pos)\n\n                    if last_run_pos != -1:\n                        bsruns.update({last_run_pos: last_run_len})\n                        cleaned_stdout_stream.write(stdout_bytes[next_start_pos:(last_run_pos - last_run_len)])\n                        next_start_pos += last_run_pos + last_run_len\n\n                cleaned_stdout_stream.write(stdout_bytes[next_start_pos:])\n\n            else:\n                cleaned_stdout_stream.write(stdout_stream.getvalue())\n\n            stdout = cleaned_stdout_stream.getvalue()\n\n            # Strip original cmd that will come back in stdout\n            if original_cmd and strip_cmd:\n                findstr = original_cmd.encode('utf-8') + b'\\r\\r\\n'\n                pos = stdout.find(findstr)\n                while pos >= 0:\n                    stdout = stdout.replace(findstr, b'')\n                    pos = stdout.find(findstr)\n\n                if b'\\r\\r\\n' in stdout:\n                    stdout = stdout.split(b'\\r\\r\\n')[1]\n\n            # Strip delim if requested\n            # TODO: Handling stripping partial delims here - not a deal breaker the way we're handling it now\n            if delim and strip_delim:\n                stdout = stdout.replace(delim, b'')\n\n            stdout = stdout.rstrip()\n\n        except Exception as e:\n            print(\"InteractiveShell exception (most likely timeout): {}\".format(e))\n\n        return stdout"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a command to the device.", "response": "def SendCommand(self, command, arg=None):\n        \"\"\"Sends a command to the device.\n\n        Args:\n          command: The command to send.\n          arg: Optional argument to the command.\n        \"\"\"\n        if arg is not None:\n            if not isinstance(arg, bytes):\n                arg = arg.encode('utf8')\n            command = b'%s:%s' % (command, arg)\n\n        self._Write(io.BytesIO(command), len(command))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef HandleSimpleResponses(\n            self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n        \"\"\"Accepts normal responses from the device.\n\n        Args:\n          timeout_ms: Timeout in milliseconds to wait for each response.\n          info_cb: Optional callback for text sent from the bootloader.\n\n        Returns:\n          OKAY packet's message.\n        \"\"\"\n        return self._AcceptResponses(b'OKAY', info_cb, timeout_ms=timeout_ms)", "response": "Handles simple responses from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the protocol for sending data to the device.", "response": "def HandleDataSending(self, source_file, source_len,\n                          info_cb=DEFAULT_MESSAGE_CALLBACK,\n                          progress_callback=None, timeout_ms=None):\n        \"\"\"Handles the protocol for sending data to the device.\n\n        Args:\n          source_file: File-object to read from for the device.\n          source_len: Amount of data, in bytes, to send to the device.\n          info_cb: Optional callback for text sent from the bootloader.\n          progress_callback: Callback that takes the current and the total progress\n            of the current file.\n          timeout_ms: Timeout in milliseconds to wait for each response.\n\n        Raises:\n          FastbootTransferError: When fastboot can't handle this amount of data.\n          FastbootStateMismatch: Fastboot responded with the wrong packet type.\n          FastbootRemoteFailure: Fastboot reported failure.\n          FastbootInvalidResponse: Fastboot responded with an unknown packet type.\n\n        Returns:\n          OKAY packet's message.\n        \"\"\"\n        accepted_size = self._AcceptResponses(\n            b'DATA', info_cb, timeout_ms=timeout_ms)\n\n        accepted_size = binascii.unhexlify(accepted_size[:8])\n        accepted_size, = struct.unpack(b'>I', accepted_size)\n        if accepted_size != source_len:\n            raise FastbootTransferError(\n                'Device refused to download %s bytes of data (accepts %s bytes)',\n                source_len, accepted_size)\n        self._Write(source_file, accepted_size, progress_callback)\n        return self._AcceptResponses(b'OKAY', info_cb, timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _AcceptResponses(self, expected_header, info_cb, timeout_ms=None):\n        while True:\n            response = self.usb.BulkRead(64, timeout_ms=timeout_ms)\n            header = bytes(response[:4])\n            remaining = bytes(response[4:])\n\n            if header == b'INFO':\n                info_cb(FastbootMessage(remaining, header))\n            elif header in self.FINAL_HEADERS:\n                if header != expected_header:\n                    raise FastbootStateMismatch(\n                        'Expected %s, got %s', expected_header, header)\n                if header == b'OKAY':\n                    info_cb(FastbootMessage(remaining, header))\n                return remaining\n            elif header == b'FAIL':\n                info_cb(FastbootMessage(remaining, header))\n                raise FastbootRemoteFailure('FAIL: %s', remaining)\n            else:\n                raise FastbootInvalidResponse(\n                    'Got unknown header %s and response %s', header, remaining)", "response": "Accept responses until the expected header or a FAIL."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends the data to the device tracking progress with the callback.", "response": "def _Write(self, data, length, progress_callback=None):\n        \"\"\"Sends the data to the device, tracking progress with the callback.\"\"\"\n        if progress_callback:\n            progress = self._HandleProgress(length, progress_callback)\n            next(progress)\n        while length:\n            tmp = data.read(self.chunk_kb * 1024)\n            length -= len(tmp)\n            self.usb.BulkWrite(tmp)\n\n            if progress_callback and progress:\n                progress.send(len(tmp))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ConnectDevice(self, port_path=None, serial=None, default_timeout_ms=None, chunk_kb=1024, **kwargs):\n\n        if 'handle' in kwargs:\n            self._handle = kwargs['handle']\n\n        else:\n            self._handle = common.UsbHandle.FindAndOpen(\n                DeviceIsAvailable, port_path=port_path, serial=serial,\n                timeout_ms=default_timeout_ms)\n\n        self._protocol = FastbootProtocol(self._handle, chunk_kb)\n\n        return self", "response": "Convenience function to get an adb device from usb path or serial number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef FlashFromFile(self, partition, source_file, source_len=0,\n                      info_cb=DEFAULT_MESSAGE_CALLBACK, progress_callback=None):\n        \"\"\"Flashes a partition from the file on disk.\n\n        Args:\n          partition: Partition name to flash to.\n          source_file: Filename to download to the device.\n          source_len: Optional length of source_file, uses os.stat if not provided.\n          info_cb: See Download.\n          progress_callback: See Download.\n\n        Returns:\n          Download and flash responses, normally nothing.\n        \"\"\"\n        if source_len == 0:\n            # Fall back to stat.\n            source_len = os.stat(source_file).st_size\n        download_response = self.Download(\n            source_file, source_len=source_len, info_cb=info_cb,\n            progress_callback=progress_callback)\n        flash_response = self.Flash(partition, info_cb=info_cb)\n        return download_response + flash_response", "response": "Download and flash a partition from a file on disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a file to the device.", "response": "def Download(self, source_file, source_len=0,\n                 info_cb=DEFAULT_MESSAGE_CALLBACK, progress_callback=None):\n        \"\"\"Downloads a file to the device.\n\n        Args:\n          source_file: A filename or file-like object to download to the device.\n          source_len: Optional length of source_file. If source_file is a file-like\n              object and source_len is not provided, source_file is read into\n              memory.\n          info_cb: Optional callback accepting FastbootMessage for text sent from\n              the bootloader.\n          progress_callback: Optional callback called with the percent of the\n              source_file downloaded. Note, this doesn't include progress of the\n              actual flashing.\n\n        Returns:\n          Response to a download request, normally nothing.\n        \"\"\"\n        if isinstance(source_file, str):\n            source_len = os.stat(source_file).st_size\n            source_file = open(source_file)\n\n        with source_file:\n            if source_len == 0:\n                # Fall back to storing it all in memory :(\n                data = source_file.read()\n                source_file = io.BytesIO(data.encode('utf8'))\n                source_len = len(data)\n\n            self._protocol.SendCommand(b'download', b'%08x' % source_len)\n            return self._protocol.HandleDataSending(\n                source_file, source_len, info_cb, progress_callback=progress_callback)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nflashing the last downloaded file to the given partition.", "response": "def Flash(self, partition, timeout_ms=0, info_cb=DEFAULT_MESSAGE_CALLBACK):\n        \"\"\"Flashes the last downloaded file to the given partition.\n\n        Args:\n          partition: Partition to overwrite with the new image.\n          timeout_ms: Optional timeout in milliseconds to wait for it to finish.\n          info_cb: See Download. Usually no messages.\n\n        Returns:\n          Response to a download request, normally nothing.\n        \"\"\"\n        return self._SimpleCommand(b'flash', arg=partition, info_cb=info_cb,\n                                   timeout_ms=timeout_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Erase(self, partition, timeout_ms=None):\n        self._SimpleCommand(b'erase', arg=partition, timeout_ms=timeout_ms)", "response": "Erases the given partition."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of a given variable according to the current bootloader.", "response": "def Getvar(self, var, info_cb=DEFAULT_MESSAGE_CALLBACK):\n        \"\"\"Returns the given variable's definition.\n\n        Args:\n          var: A variable the bootloader tracks. Use 'all' to get them all.\n          info_cb: See Download. Usually no messages.\n\n        Returns:\n          Value of var according to the current bootloader.\n        \"\"\"\n        return self._SimpleCommand(b'getvar', arg=var, info_cb=info_cb)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Oem(self, command, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n        if not isinstance(command, bytes):\n            command = command.encode('utf8')\n        return self._SimpleCommand(\n            b'oem %s' % command, timeout_ms=timeout_ms, info_cb=info_cb)", "response": "Executes an OEM command on the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a docstring documenting arguments into a dict.", "response": "def _DocToArgs(doc):\n    \"\"\"Converts a docstring documenting arguments into a dict.\"\"\"\n    m = None\n    offset = None\n    in_arg = False\n    out = {}\n    for l in doc.splitlines():\n        if l.strip() == 'Args:':\n            in_arg = True\n        elif in_arg:\n            if not l.strip():\n                break\n            if offset is None:\n                offset = len(l) - len(l.lstrip())\n            l = l[offset:]\n            if l[0] == ' ' and m:\n                out[m.group(1)] += ' ' + l.lstrip()\n            else:\n                m = re.match(r'^([a-z_]+): (.+)$', l.strip())\n                out[m.group(1)] = m.group(2)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef MakeSubparser(subparsers, parents, method, arguments=None):\n    name = ('-'.join(re.split(r'([A-Z][a-z]+)', method.__name__)[1:-1:2])).lower()\n    help = method.__doc__.splitlines()[0]\n    subparser = subparsers.add_parser(\n        name=name, description=help, help=help.rstrip('.'), parents=parents)\n    subparser.set_defaults(method=method, positional=[])\n    argspec = inspect.getargspec(method)\n\n    # Figure out positionals and default argument, if any. Explicitly includes\n    # arguments that default to '' but excludes arguments that default to None.\n    offset = len(argspec.args) - len(argspec.defaults or []) - 1\n    positional = []\n    for i in range(1, len(argspec.args)):\n        if i > offset and argspec.defaults[i - offset - 1] is None:\n            break\n        positional.append(argspec.args[i])\n    defaults = [None] * offset + list(argspec.defaults or [])\n\n    # Add all arguments so they append to args.positional.\n    args_help = _DocToArgs(method.__doc__)\n    for name, default in zip(positional, defaults):\n        if not isinstance(default, (None.__class__, str)):\n            continue\n        subparser.add_argument(\n            name, help=(arguments or {}).get(name, args_help.get(name)),\n            default=default, nargs='?' if default is not None else None,\n            action=PositionalArg)\n    if argspec.varargs:\n        subparser.add_argument(\n            argspec.varargs, nargs=argparse.REMAINDER,\n            help=(arguments or {}).get(argspec.varargs, args_help.get(argspec.varargs)))\n    return subparser", "response": "Creates an argparse subparser to create a subcommand to adb."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _RunMethod(dev, args, extra):\n    logging.info('%s(%s)', args.method.__name__, ', '.join(args.positional))\n    result = args.method(dev, *args.positional, **extra)\n    if result is not None:\n        if isinstance(result, io.StringIO):\n            sys.stdout.write(result.getvalue())\n        elif isinstance(result, (list, types.GeneratorType)):\n            r = ''\n            for r in result:\n                r = str(r)\n                sys.stdout.write(r)\n            if not r.endswith('\\n'):\n                sys.stdout.write('\\n')\n        else:\n            result = str(result)\n            sys.stdout.write(result)\n            if not result.endswith('\\n'):\n                sys.stdout.write('\\n')\n    return 0", "response": "Runs a method registered via MakeSubparser."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a common CLI interface for this usb path and protocol.", "response": "def StartCli(args, adb_commands, extra=None, **device_kwargs):\n    \"\"\"Starts a common CLI interface for this usb path and protocol.\"\"\"\n    try:\n        dev = adb_commands()\n        dev.ConnectDevice(port_path=args.port_path, serial=args.serial, default_timeout_ms=args.timeout_ms,\n                          **device_kwargs)\n    except usb_exceptions.DeviceNotFoundError as e:\n        print('No device found: {}'.format(e), file=sys.stderr)\n        return 1\n    except usb_exceptions.CommonUsbError as e:\n        print('Could not connect to device: {}'.format(e), file=sys.stderr)\n        return 1\n    try:\n        return _RunMethod(dev, args, extra or {})\n    except Exception as e:  # pylint: disable=broad-except\n        sys.stdout.write(str(e))\n        return 1\n    finally:\n        dev.Close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npull a file from the device into the file - like dest_file.", "response": "def Pull(cls, connection, filename, dest_file, progress_callback):\n        \"\"\"Pull a file from the device into the file-like dest_file.\"\"\"\n        if progress_callback:\n            total_bytes = cls.Stat(connection, filename)[1]\n            progress = cls._HandleProgress(lambda current: progress_callback(filename, current, total_bytes))\n            next(progress)\n\n        cnxn = FileSyncConnection(connection, b'<2I')\n        try:\n            cnxn.Send(b'RECV', filename)\n            for cmd_id, _, data in cnxn.ReadUntil((b'DATA',), b'DONE'):\n                if cmd_id == b'DONE':\n                    break\n                dest_file.write(data)\n                if progress_callback:\n                    progress.send(len(data))\n        except usb_exceptions.CommonUsbError as e:\n            raise PullFailedError('Unable to pull file %s due to: %s' % (filename, e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npushes a file - like object to the device.", "response": "def Push(cls, connection, datafile, filename,\n             st_mode=DEFAULT_PUSH_MODE, mtime=0, progress_callback=None):\n        \"\"\"Push a file-like object to the device.\n\n        Args:\n          connection: ADB connection\n          datafile: File-like object for reading from\n          filename: Filename to push to\n          st_mode: stat mode for filename\n          mtime: modification time\n          progress_callback: callback method that accepts filename, bytes_written and total_bytes\n\n        Raises:\n          PushFailedError: Raised on push failure.\n        \"\"\"\n\n        fileinfo = ('{},{}'.format(filename, int(st_mode))).encode('utf-8')\n\n        cnxn = FileSyncConnection(connection, b'<2I')\n        cnxn.Send(b'SEND', fileinfo)\n\n        if progress_callback:\n            total_bytes = os.fstat(datafile.fileno()).st_size if isinstance(datafile, file) else -1\n            progress = cls._HandleProgress(lambda current: progress_callback(filename, current, total_bytes))\n            next(progress)\n\n        while True:\n            data = datafile.read(MAX_PUSH_DATA)\n            if data:\n                cnxn.Send(b'DATA', data)\n\n                if progress_callback:\n                    progress.send(len(data))\n            else:\n                break\n\n        if mtime == 0:\n            mtime = int(time.time())\n        # DONE doesn't send data, but it hides the last bit of data in the size\n        # field.\n        cnxn.Send(b'DONE', size=mtime)\n        for cmd_id, _, data in cnxn.ReadUntil((), b'OKAY', b'FAIL'):\n            if cmd_id == b'OKAY':\n                return\n            raise PushFailedError(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Send(self, command_id, data=b'', size=0):\n        if data:\n            if not isinstance(data, bytes):\n                data = data.encode('utf8')\n            size = len(data)\n\n        if not self._CanAddToSendBuffer(len(data)):\n            self._Flush()\n        buf = struct.pack(b'<2I', self.id_to_wire[command_id], size) + data\n        self.send_buffer[self.send_idx:self.send_idx + len(buf)] = buf\n        self.send_idx += len(buf)", "response": "Send a file - sync command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread ADB messages and return FileSync packets.", "response": "def Read(self, expected_ids, read_data=True):\n        \"\"\"Read ADB messages and return FileSync packets.\"\"\"\n        if self.send_idx:\n            self._Flush()\n\n        # Read one filesync packet off the recv buffer.\n        header_data = self._ReadBuffered(self.recv_header_len)\n        header = struct.unpack(self.recv_header_format, header_data)\n        # Header is (ID, ...).\n        command_id = self.wire_to_id[header[0]]\n\n        if command_id not in expected_ids:\n            if command_id == b'FAIL':\n                reason = ''\n                if self.recv_buffer:\n                    reason = self.recv_buffer.decode('utf-8', errors='ignore')\n                raise usb_exceptions.AdbCommandFailureException('Command failed: {}'.format(reason))\n            raise adb_protocol.InvalidResponseError(\n                'Expected one of %s, got %s' % (expected_ids, command_id))\n\n        if not read_data:\n            return command_id, header[1:]\n\n        # Header is (ID, ..., size).\n        size = header[-1]\n        data = self._ReadBuffered(size)\n        return command_id, header[1:-1], data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading until one of the specified IDs are found.", "response": "def ReadUntil(self, expected_ids, *finish_ids):\n        \"\"\"Useful wrapper around Read.\"\"\"\n        while True:\n            cmd_id, header, data = self.Read(expected_ids + finish_ids)\n            yield cmd_id, header, data\n            if cmd_id in finish_ids:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Devices(args):\n    for d in adb_commands.AdbCommands.Devices():\n        if args.output_port_path:\n            print('%s\\tdevice\\t%s' % (\n                d.serial_number, ','.join(str(p) for p in d.port_path)))\n        else:\n            print('%s\\tdevice' % d.serial_number)\n    return 0", "response": "Lists the available devices."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints a directory listing.", "response": "def List(device, device_path):\n    \"\"\"Prints a directory listing.\n\n    Args:\n      device_path: Directory to list.\n    \"\"\"\n    files = device.List(device_path)\n    files.sort(key=lambda x: x.filename)\n    maxname = max(len(f.filename) for f in files)\n    maxsize = max(len(str(f.size)) for f in files)\n    for f in files:\n        mode = (\n                ('d' if stat.S_ISDIR(f.mode) else '-') +\n                ('r' if f.mode & stat.S_IRUSR else '-') +\n                ('w' if f.mode & stat.S_IWUSR else '-') +\n                ('x' if f.mode & stat.S_IXUSR else '-') +\n                ('r' if f.mode & stat.S_IRGRP else '-') +\n                ('w' if f.mode & stat.S_IWGRP else '-') +\n                ('x' if f.mode & stat.S_IXGRP else '-') +\n                ('r' if f.mode & stat.S_IROTH else '-') +\n                ('w' if f.mode & stat.S_IWOTH else '-') +\n                ('x' if f.mode & stat.S_IXOTH else '-'))\n        t = time.gmtime(f.mtime)\n        yield '%s %*d %04d-%02d-%02d %02d:%02d:%02d %-*s\\n' % (\n            mode, maxsize, f.size,\n            t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec,\n            maxname, f.filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun a command on the device and prints the stdout.", "response": "def Shell(device, *command):\n    \"\"\"Runs a command on the device and prints the stdout.\n\n    Args:\n      command: Command to run on the target.\n    \"\"\"\n    if command:\n        return device.StreamingShell(' '.join(command))\n    else:\n        # Retrieve the initial terminal prompt to use as a delimiter for future reads\n        terminal_prompt = device.InteractiveShell()\n        print(terminal_prompt.decode('utf-8'))\n\n        # Accept user input in a loop and write that into the interactive shells stdin, then print output\n        while True:\n            cmd = input('> ')\n            if not cmd:\n                continue\n            elif cmd == 'exit':\n                break\n            else:\n                stdout = device.InteractiveShell(cmd, strip_cmd=True, delim=terminal_prompt, strip_delim=True)\n                if stdout:\n                    if isinstance(stdout, bytes):\n                        stdout = stdout.decode('utf-8')\n                        print(stdout)\n\n        device.Close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_rsa_private_key(pem):\n    # ADB uses private RSA keys in pkcs#8 format. 'rsa' library doesn't support\n    # them natively. Do some ASN unwrapping to extract naked RSA key\n    # (in der-encoded form). See https://www.ietf.org/rfc/rfc2313.txt.\n    # Also http://superuser.com/a/606266.\n    try:\n        der = rsa.pem.load_pem(pem, 'PRIVATE KEY')\n        keyinfo, _ = decoder.decode(der)\n        if keyinfo[1][0] != univ.ObjectIdentifier(\n                '1.2.840.113549.1.1.1'):  # pragma: no cover\n            raise ValueError('Not a DER-encoded OpenSSL private RSA key')\n        private_key_der = keyinfo[2].asOctets()\n    except IndexError:  # pragma: no cover\n        raise ValueError('Not a DER-encoded OpenSSL private RSA key')\n    return rsa.PrivateKey.load_pkcs1(private_key_der, format='DER')", "response": "Load an RSA private key from PEM encoded PKCS#8 private key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the active markup class based on the current file name and the current active markup class.", "response": "def updateActiveMarkupClass(self):\n\t\t'''\n\t\tUpdate the active markup class based on the default class and\n\t\tthe current filename. If the active markup class changes, the\n\t\thighlighter is rerun on the input text, the markup object of\n\t\tthis tab is replaced with one of the new class and the\n\t\tactiveMarkupChanged signal is emitted.\n\t\t'''\n\t\tpreviousMarkupClass = self.activeMarkupClass\n\n\t\tself.activeMarkupClass = find_markup_class_by_name(globalSettings.defaultMarkup)\n\n\t\tif self._fileName:\n\t\t\tmarkupClass = get_markup_for_file_name(\n\t\t\t\tself._fileName, return_class=True)\n\t\t\tif markupClass:\n\t\t\t\tself.activeMarkupClass = markupClass\n\n\t\tif self.activeMarkupClass != previousMarkupClass:\n\t\t\tself.highlighter.docType = self.activeMarkupClass.name if self.activeMarkupClass else None\n\t\t\tself.highlighter.rehighlight()\n\n\t\t\tself.activeMarkupChanged.emit()\n\t\t\tself.triggerPreviewUpdate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetect the content encoding of a file.", "response": "def detectFileEncoding(self, fileName):\n\t\t'''\n\t\tDetect content encoding of specific file.\n\n\t\tIt will return None if it can't determine the encoding.\n\t\t'''\n\t\ttry:\n\t\t\timport chardet\n\t\texcept ImportError:\n\t\t\treturn\n\n\t\twith open(fileName, 'rb') as inputFile:\n\t\t\traw = inputFile.read(2048)\n\n\t\tresult = chardet.detect(raw)\n\t\tif result['confidence'] > 0.9:\n\t\t\tif result['encoding'].lower() == 'ascii':\n\t\t\t\t# UTF-8 files can be falsely detected as ASCII files if they\n\t\t\t\t# don't contain non-ASCII characters in first 2048 bytes.\n\t\t\t\t# We map ASCII to UTF-8 to avoid such situations.\n\t\t\t\treturn 'utf-8'\n\t\t\treturn result['encoding']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef openSourceFile(self, fileToOpen):\n\t\tif self.fileName:\n\t\t\tcurrentExt = splitext(self.fileName)[1]\n\t\t\tbasename, ext = splitext(fileToOpen)\n\t\t\tif ext in ('.html', '') and exists(basename + currentExt):\n\t\t\t\tself.p.openFileWrapper(basename + currentExt)\n\t\t\t\treturn basename + currentExt\n\t\tif exists(fileToOpen) and get_markup_for_file_name(fileToOpen, return_class=True):\n\t\t\tself.p.openFileWrapper(fileToOpen)\n\t\t\treturn fileToOpen", "response": "Finds and opens the source file for link target fileToOpen."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extendMarkdown(self, md):\n        md.preprocessors.register(PosMapMarkPreprocessor(md), 'posmap_mark', 50)\n        md.preprocessors.register(PosMapCleanPreprocessor(md), 'posmap_clean', 5)\n        md.parser.blockprocessors.register(PosMapBlockProcessor(md.parser), 'posmap', 150)\n\n        # Monkey patch CodeHilite constructor to remove the posmap markers from\n        # text before highlighting it\n        orig_codehilite_init = CodeHilite.__init__\n\n        def new_codehilite_init(self, src=None, *args, **kwargs):\n            src = POSMAP_MARKER_RE.sub('', src)\n            orig_codehilite_init(self, src=src, *args, **kwargs)\n        CodeHilite.__init__ = new_codehilite_init\n\n        # Same for PyMdown Extensions if it is available\n        if Highlight is not None:\n            orig_highlight_highlight = Highlight.highlight\n\n            def new_highlight_highlight(self, src, *args, **kwargs):\n                src = POSMAP_MARKER_RE.sub('', src)\n                return orig_highlight_highlight(self, src, *args, **kwargs)\n            Highlight.highlight = new_highlight_highlight", "response": "Extend the Markdown parser with the available ArcGIS tags."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tabFileNameChanged(self, tab):\n\t\t'''\n\t\tPerform all UI state changes that need to be done when the\n\t\tfilename of the current tab has changed.\n\t\t'''\n\t\tif tab == self.currentTab:\n\t\t\tif tab.fileName:\n\t\t\t\tself.setWindowTitle(\"\")\n\t\t\t\tif globalSettings.windowTitleFullPath:\n\t\t\t\t\tself.setWindowTitle(tab.fileName + '[*]')\n\t\t\t\tself.setWindowFilePath(tab.fileName)\n\t\t\t\tself.tabWidget.setTabText(self.ind, tab.getBaseName())\n\t\t\t\tself.tabWidget.setTabToolTip(self.ind, tab.fileName)\n\t\t\t\tQDir.setCurrent(QFileInfo(tab.fileName).dir().path())\n\t\t\telse:\n\t\t\t\tself.setWindowFilePath('')\n\t\t\t\tself.setWindowTitle(self.tr('New document') + '[*]')\n\n\t\t\tcanReload = bool(tab.fileName) and not self.autoSaveActive(tab)\n\t\t\tself.actionSetEncoding.setEnabled(canReload)\n\t\t\tself.actionReload.setEnabled(canReload)", "response": "This method is called when the tab s filename has changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tabActiveMarkupChanged(self, tab):\n\t\t'''\n\t\tPerform all UI state changes that need to be done when the\n\t\tactive markup class of the current tab has changed.\n\t\t'''\n\t\tif tab == self.currentTab:\n\t\t\tmarkupClass = tab.getActiveMarkupClass()\n\t\t\tdtMarkdown = (markupClass == markups.MarkdownMarkup)\n\t\t\tdtMkdOrReST = dtMarkdown or (markupClass == markups.ReStructuredTextMarkup)\n\t\t\tself.formattingBox.setEnabled(dtMarkdown)\n\t\t\tself.symbolBox.setEnabled(dtMarkdown)\n\t\t\tself.actionUnderline.setEnabled(dtMarkdown)\n\t\t\tself.actionBold.setEnabled(dtMkdOrReST)\n\t\t\tself.actionItalic.setEnabled(dtMkdOrReST)", "response": "This method is called when the active markup class of the current tab has changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tabModificationStateChanged(self, tab):\n\t\t'''\n\t\tPerform all UI state changes that need to be done when the\n\t\tmodification state of the current tab has changed.\n\t\t'''\n\t\tif tab == self.currentTab:\n\t\t\tchanged = tab.editBox.document().isModified()\n\t\t\tif self.autoSaveActive(tab):\n\t\t\t\tchanged = False\n\t\t\tself.actionSave.setEnabled(changed)\n\t\t\tself.setWindowModified(changed)", "response": "This method is called when the modification state of the current tab has changed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getPageSizeByName(self, pageSizeName):\n\t\tpageSize = None\n\n\t\tlowerCaseNames = {pageSize.lower(): pageSize for pageSize in\n\t\t                  self.availablePageSizes()}\n\t\tif pageSizeName.lower() in lowerCaseNames:\n\t\t\tpageSize = getattr(QPagedPaintDevice, lowerCaseNames[pageSizeName.lower()])\n\n\t\treturn pageSize", "response": "Returns a validated PageSize instance corresponding to the given pageSizeName. Returns None if the given pageSizeName is not a valid PageSize."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists available page sizes.", "response": "def availablePageSizes(self):\n\t\t\"\"\" List available page sizes. \"\"\"\n\n\t\tsizes = [x for x in dir(QPagedPaintDevice)\n\t\t         if type(getattr(QPagedPaintDevice, x)) == QPagedPaintDevice.PageSize]\n\t\treturn sizes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the projection of the data set X to the lens of the data set X.", "response": "def project(\n        self,\n        X,\n        projection=\"sum\",\n        scaler=preprocessing.MinMaxScaler(),\n        distance_matrix=None,\n    ):\n        \"\"\"Creates the projection/lens from a dataset. Input the data set. Specify a projection/lens type. Output the projected data/lens.\n\n        Parameters\n        ----------\n\n        X : Numpy Array\n            The data to fit a projection/lens to.\n\n        projection :\n            Projection parameter is either a string, a Scikit-learn class with fit_transform, like manifold.TSNE(), or a list of dimension indices. A string from [\"sum\", \"mean\", \"median\", \"max\", \"min\", \"std\", \"dist_mean\", \"l2norm\", \"knn_distance_n\"]. If using knn_distance_n write the number of desired neighbors in place of n: knn_distance_5 for summed distances to 5 nearest neighbors. Default = \"sum\".\n\n        scaler : Scikit-Learn API compatible scaler.\n            Scaler of the data applied after mapping. Use None for no scaling. Default = preprocessing.MinMaxScaler() if None, do no scaling, else apply scaling to the projection. Default: Min-Max scaling\n\n        distance_matrix : Either str or None\n            If not None, then any of [\"braycurtis\", \"canberra\", \"chebyshev\", \"cityblock\", \"correlation\", \"cosine\", \"dice\", \"euclidean\", \"hamming\", \"jaccard\", \"kulsinski\", \"mahalanobis\", \"matching\", \"minkowski\", \"rogerstanimoto\", \"russellrao\", \"seuclidean\", \"sokalmichener\", \"sokalsneath\", \"sqeuclidean\", \"yule\"]. \n            If False do nothing, else create a squared distance matrix with the chosen metric, before applying the projection.\n\n        Returns\n        -------\n        lens : Numpy Array\n            projected data.\n\n        Examples\n        --------\n        >>> # Project by taking the first dimension and third dimension\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     projection=[0,2]\n        >>> )\n\n        >>> # Project by taking the sum of row values\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     projection=\"sum\"\n        >>> )\n\n        >>> # Do not scale the projection (default is minmax-scaling)\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     scaler=None\n        >>> )\n\n        >>> # Project by standard-scaled summed distance to 5 nearest neighbors\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     projection=\"knn_distance_5\",\n        >>>     scaler=sklearn.preprocessing.StandardScaler()\n        >>> )\n\n        >>> # Project by first two PCA components\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     projection=sklearn.decomposition.PCA()\n        >>> )\n\n        >>> # Project by first three UMAP components\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     projection=umap.UMAP(n_components=3)\n        >>> )\n\n        >>> # Project by L2-norm on squared Pearson distance matrix\n        >>> X_projected = mapper.project(\n        >>>     X_inverse,\n        >>>     projection=\"l2norm\",\n        >>>     distance_matrix=\"pearson\"\n        >>> )\n\n        >>> # Mix and match different projections\n        >>> X_projected = np.c_[\n        >>>     mapper.project(X_inverse, projection=sklearn.decomposition.PCA()),\n        >>>     mapper.project(X_inverse, projection=\"knn_distance_5\")\n        >>> ]\n\n        >>> # Stack / chain projections. You could do this manually, \n        >>> # or pipeline with `.fit_transform()`. Works the same as `.project()`,\n        >>> # but accepts lists. f(raw text) -> f(tfidf) -> f(isomap 100d) -> f(umap 2d)\n        >>> projected_X = mapper.fit_transform(\n        >>>     X,\n        >>>     projections=[TfidfVectorizer(analyzer=\"char\",\n        >>>                                  ngram_range=(1,6),\n        >>>                                  max_df=0.93,\n        >>>                                  min_df=0.03),\n        >>>                  manifold.Isomap(n_components=100,\n        >>>                                  n_jobs=-1),\n        >>>                  umap.UMAP(n_components=2,\n        >>>                            random_state=1)],\n        >>>     scalers=[None,\n        >>>              None,\n        >>>              preprocessing.MinMaxScaler()],\n        >>>     distance_matrices=[False,\n        >>>                        False,\n        >>>                        False])\n        \"\"\"\n\n        # Sae original values off so they can be referenced by later functions in the pipeline\n        self.inverse = X\n        self.scaler = scaler\n        self.projection = str(projection)\n        self.distance_matrix = distance_matrix\n\n        if self.verbose > 0:\n            print(\"..Projecting on data shaped %s\" % (str(X.shape)))\n\n        # If distance_matrix is a scipy.spatial.pdist string, we create a square distance matrix\n        # from the vectors, before applying a projection.\n        if self.distance_matrix in [\n            \"braycurtis\",\n            \"canberra\",\n            \"chebyshev\",\n            \"cityblock\",\n            \"correlation\",\n            \"cosine\",\n            \"dice\",\n            \"euclidean\",\n            \"hamming\",\n            \"jaccard\",\n            \"kulsinski\",\n            \"mahalanobis\",\n            \"matching\",\n            \"minkowski\",\n            \"rogerstanimoto\",\n            \"russellrao\",\n            \"seuclidean\",\n            \"sokalmichener\",\n            \"sokalsneath\",\n            \"sqeuclidean\",\n            \"yule\",\n        ]:\n            X = distance.squareform(distance.pdist(X, metric=distance_matrix))\n            if self.verbose > 0:\n                print(\n                    \"Created distance matrix, shape: %s, with distance metric `%s`\"\n                    % (X.shape, distance_matrix)\n                )\n\n        # Detect if projection is a class (for scikit-learn)\n        try:\n            p = projection.get_params()  # fail quickly\n            reducer = projection\n            if self.verbose > 0:\n                try:\n                    projection.set_params(**{\"verbose\": self.verbose})\n                except:\n                    pass\n                print(\"\\n..Projecting data using: \\n\\t%s\\n\" % str(projection))\n            X = reducer.fit_transform(X)\n        except:\n            pass\n\n        # What is this used for?\n        if isinstance(projection, tuple):\n            X = self._process_projection_tuple(projection)\n\n        # Detect if projection is a string (for standard functions)\n        # TODO: test each one of these projections\n        if isinstance(projection, str):\n            if self.verbose > 0:\n                print(\"\\n..Projecting data using: %s\" % (projection))\n\n            def dist_mean(X, axis=1):\n                X_mean = np.mean(X, axis=0)\n                X = np.sum(np.sqrt((X - X_mean) ** 2), axis=1)\n                return X\n\n            projection_funcs = {\n                \"sum\": np.sum,\n                \"mean\": np.mean,\n                \"median\": np.median,\n                \"max\": np.max,\n                \"min\": np.min,\n                \"std\": np.std,\n                \"l2norm\": np.linalg.norm,\n                \"dist_mean\": dist_mean,\n            }\n\n            if projection in projection_funcs.keys():\n                X = projection_funcs[projection](X, axis=1).reshape((X.shape[0], 1))\n\n            if \"knn_distance_\" in projection:\n                n_neighbors = int(projection.split(\"_\")[2])\n                if (\n                    self.distance_matrix\n                ):  # We use the distance matrix for finding neighbors\n                    X = np.sum(np.sort(X, axis=1)[:, :n_neighbors], axis=1).reshape(\n                        (X.shape[0], 1)\n                    )\n                else:\n                    from sklearn import neighbors\n\n                    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors)\n                    nn.fit(X)\n                    X = np.sum(\n                        nn.kneighbors(X, n_neighbors=n_neighbors, return_distance=True)[\n                            0\n                        ],\n                        axis=1,\n                    ).reshape((X.shape[0], 1))\n\n        # Detect if projection is a list (with dimension indices)\n        if isinstance(projection, list):\n            if self.verbose > 0:\n                print(\"\\n..Projecting data using: %s\" % (str(projection)))\n            X = X[:, np.array(projection)]\n\n        # If projection produced sparse output, turn into a dense array\n        if issparse(X):\n            X = X.toarray()\n            if self.verbose > 0:\n                print(\"\\n..Created projection shaped %s\" % (str(X.shape)))\n\n        # Scaling\n        if scaler is not None:\n            if self.verbose > 0:\n                print(\"\\n..Scaling with: %s\\n\" % str(scaler))\n            X = scaler.fit_transform(X)\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit_transform(\n        self,\n        X,\n        projection=\"sum\",\n        scaler=preprocessing.MinMaxScaler(),\n        distance_matrix=False,\n    ):\n        \"\"\"Same as .project() but accepts lists for arguments so you can chain.\n\n        \"\"\"\n\n        projections = projection\n        scalers = scaler\n        distance_matrices = distance_matrix\n\n        # Turn single projection arguments into a pipeline\n        if isinstance(projection, list) and isinstance(projection[0], int):\n            projections = [projection]\n\n        if not isinstance(projection, list):\n            projections = [projection]\n\n        # Turn single scaler arguments into a pipeline\n        if not isinstance(scaler, list):\n            scalers = [scaler]\n\n        # Turn single distance matrix arguments into a pipeline\n        if not isinstance(distance_matrix, list):\n            distance_matrices = [distance_matrix]\n\n        # set defaults to first list item, if not (correctly) set by the user\n        if len(scalers) != len(projections):\n            scalers = [scalers[0]] * len(projections)\n\n        if len(distance_matrices) != len(projections):\n            distance_matrices = [distance_matrices[0]] * len(projections)\n\n        if self.verbose > 0:\n            print(\"..Composing projection pipeline of length %s:\" % (len(projections)))\n            print(\"\\tProjections: %s\" % (\"\\n\\t\\t\".join(map(str, projections))))\n            print(\"\\tDistance matrices: %s\" % (\"\\n\".join(map(str, distance_matrices))))\n            print(\"\\tScalers: %s\" % (\"\\n\".join(map(str, scalers))))\n\n        # Pipeline Stack the projection functions\n        lens = X\n        for projection, scaler, distance_matrix in zip(\n            projections, scalers, distance_matrices\n        ):\n            lens = self.project(\n                lens,\n                projection=projection,\n                scaler=scaler,\n                distance_matrix=distance_matrix,\n            )\n\n        return lens", "response": "Fit the user s data to the new data and return the new data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map(\n        self,\n        lens,\n        X=None,\n        clusterer=cluster.DBSCAN(eps=0.5, min_samples=3),\n        cover=Cover(n_cubes=10, perc_overlap=0.1),\n        nerve=GraphNerve(),\n        precomputed=False,\n        remove_duplicate_nodes=False,\n        # These arguments are all deprecated\n        overlap_perc=None,\n        nr_cubes=None\n    ):\n        \"\"\"Apply Mapper algorithm on this projection and build a simplicial complex. Returns a dictionary with nodes and links.\n\n        Parameters\n        ----------\n        lens: Numpy Array\n            Lower dimensional representation of data. In general will be output of `fit_transform`.\n\n        X: Numpy Array\n            Original data or data to run clustering on. If `None`, then use `lens` as default. X can be a SciPy sparse matrix.\n\n        clusterer: Default: DBSCAN\n            Scikit-learn API compatible clustering algorithm. Must provide `fit` and `predict`.\n\n        cover: kmapper.Cover\n            Cover scheme for lens. Instance of kmapper.cover providing methods `fit` and `transform`.\n\n        nerve: kmapper.Nerve\n            Nerve builder implementing `__call__(nodes)` API\n\n        precomputed : Boolean\n            Tell Mapper whether the data that you are clustering on is a precomputed distance matrix. If set to\n            `True`, the assumption is that you are also telling your `clusterer` that `metric='precomputed'` (which\n            is an argument for DBSCAN among others), which \n            will then cause the clusterer to expect a square distance matrix for each hypercube. `precomputed=True` will give a square matrix\n            to the clusterer to fit on for each hypercube.\n            \n        remove_duplicate_nodes: Boolean\n            Removes duplicate nodes before edges are determined. A node is considered to be duplicate\n            if it has exactly the same set of points as another node.\n\n        nr_cubes: Int\n            \n            .. deprecated:: 1.1.6\n\n                define Cover explicitly in future versions\n\n            The number of intervals/hypercubes to create. Default = 10.\n            \n        overlap_perc: Float\n            .. deprecated:: 1.1.6\n\n                define Cover explicitly in future versions    \n\n            The percentage of overlap \"between\" the intervals/hypercubes. Default = 0.1. \n            \n\n\n        Returns\n        =======\n        simplicial_complex : dict\n            A dictionary with \"nodes\", \"links\" and \"meta\" information.\n\n        Examples\n        ========\n\n        >>> # Default mapping.\n        >>> graph = mapper.map(X_projected, X_inverse)\n\n        >>> # Apply clustering on the projection instead of on inverse X\n        >>> graph = mapper.map(X_projected)\n\n        >>> # Use 20 cubes/intervals per projection dimension, with a 50% overlap\n        >>> graph = mapper.map(X_projected, X_inverse, \n        >>>                    cover=kmapper.Cover(n_cubes=20, perc_overlap=0.5))\n\n        >>> # Use multiple different cubes/intervals per projection dimension, \n        >>> # And vary the overlap\n        >>> graph = mapper.map(X_projected, X_inverse,\n        >>>                    cover=km.Cover(n_cubes=[10,20,5],\n        >>>                                         perc_overlap=[0.1,0.2,0.5]))\n\n        >>> # Use KMeans with 2 clusters\n        >>> graph = mapper.map(X_projected, X_inverse,\n        >>>     clusterer=sklearn.cluster.KMeans(2))\n\n        >>> # Use DBSCAN with \"cosine\"-distance\n        >>> graph = mapper.map(X_projected, X_inverse,\n        >>>     clusterer=sklearn.cluster.DBSCAN(metric=\"cosine\"))\n\n        >>> # Use HDBSCAN as the clusterer\n        >>> graph = mapper.map(X_projected, X_inverse,\n        >>>     clusterer=hdbscan.HDBSCAN())\n\n        >>> # Parametrize the nerve of the covering\n        >>> graph = mapper.map(X_projected, X_inverse,\n        >>>     nerve=km.GraphNerve(min_intersection=3))\n\n\n        \"\"\"\n\n        start = datetime.now()\n\n        nodes = defaultdict(list)\n        meta = defaultdict(list)\n        graph = {}\n\n        # If inverse image is not provided, we use the projection as the inverse image (suffer projection loss)\n        if X is None:\n            X = lens\n\n        # Deprecation warnings\n        if nr_cubes is not None or overlap_perc is not None:\n            warnings.warn(\n                \"Deprecation Warning: Please supply km.Cover object. Explicitly passing in n_cubes/nr_cubes and overlap_perc will be deprecated in future releases. \",\n                DeprecationWarning,\n            )\n\n\n        # If user supplied nr_cubes, overlap_perc, or coverer, opt for those\n        # TODO: remove this conditional after release in 1.2\n        if nr_cubes is not None or overlap_perc is not None:\n            n_cubes = nr_cubes if nr_cubes else 10\n            overlap_perc = overlap_perc if overlap_perc else 0.1\n            self.cover = Cover(n_cubes=n_cubes, perc_overlap=overlap_perc)\n        else:\n            self.cover = cover\n\n        if self.verbose > 0:\n            print(\n                \"Mapping on data shaped %s using lens shaped %s\\n\"\n                % (str(X.shape), str(lens.shape))\n            )\n\n        # Prefix'ing the data with an ID column\n        ids = np.array([x for x in range(lens.shape[0])])\n        lens = np.c_[ids, lens]\n        if issparse(X):\n            X = hstack([ids[np.newaxis].T, X], format='csr')\n        else:\n            X = np.c_[ids, X]\n\n        # Cover scheme defines a list of elements\n        bins = self.cover.fit(lens)\n\n        # Algo's like K-Means, have a set number of clusters. We need this number\n        # to adjust for the minimal number of samples inside an interval before\n        # we consider clustering or skipping it.\n        cluster_params = clusterer.get_params()\n\n        min_cluster_samples = cluster_params.get(\n            \"n_clusters\",\n            cluster_params.get(\n                \"min_cluster_size\", cluster_params.get(\"min_samples\", 1)\n            ),\n        )\n\n        if self.verbose > 1:\n            print(\n                \"Minimal points in hypercube before clustering: %d\"\n                % (min_cluster_samples)\n            )\n\n        # Subdivide the projected data X in intervals/hypercubes with overlap\n        if self.verbose > 0:\n            bins = list(bins)  # extract list from generator\n            total_bins = len(bins)\n            print(\"Creating %s hypercubes.\" % total_bins)\n\n        for i, hypercube in enumerate(self.cover.transform(lens)):\n\n            # If at least min_cluster_samples samples inside the hypercube\n            if hypercube.shape[0] >= min_cluster_samples:\n                # Cluster the data point(s) in the cube, skipping the id-column\n                # Note that we apply clustering on the inverse image (original data samples) that fall inside the cube.\n                ids = [int(nn) for nn in hypercube[:, 0]]\n                X_cube = X[ids]\n\n                fit_data = X_cube[:, 1:]\n                if precomputed:\n                    fit_data = fit_data[:, ids]\n\n                cluster_predictions = clusterer.fit_predict(fit_data)\n\n                if self.verbose > 1:\n                    print(\n                        \"   > Found %s clusters in hypercube %s.\"\n                        % (\n                            np.unique(\n                                cluster_predictions[cluster_predictions > -1]\n                            ).shape[0], i\n                        )\n                    )\n        \n                for pred in np.unique(cluster_predictions):\n                    # if not predicted as noise                       \n                    if pred != -1 and not np.isnan(pred):  \n                        cluster_id = \"cube{}_cluster{}\".format(i, int(pred))\n                        \n                        nodes[cluster_id] = hypercube[:, 0][cluster_predictions == pred].astype(int).tolist()\n            elif self.verbose > 1:\n                print(\"Cube_%s is empty.\\n\" % (i))\n\n        if remove_duplicate_nodes:\n            nodes = self._remove_duplicate_nodes(nodes)\n\n        links, simplices = nerve.compute(nodes)\n\n        graph[\"nodes\"] = nodes\n        graph[\"links\"] = links\n        graph[\"simplices\"] = simplices\n        graph[\"meta_data\"] = {\n            \"projection\": self.projection if self.projection else \"custom\",\n            \"n_cubes\": self.cover.n_cubes,\n            \"perc_overlap\": self.cover.perc_overlap,\n            \"clusterer\": str(clusterer),\n            \"scaler\": str(self.scaler),\n        }\n        graph[\"meta_nodes\"] = meta\n\n        if self.verbose > 0:\n            self._summary(graph, str(datetime.now() - start))\n\n        return graph", "response": "Apply Mapper algorithm on this data and build a simplicial complex."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a visualization of the simplicial complex mapper output.", "response": "def visualize(\n        self,\n        graph,\n        color_function=None,\n        custom_tooltips=None,\n        custom_meta=None,\n        path_html=\"mapper_visualization_output.html\",\n        title=\"Kepler Mapper\",\n        save_file=True,\n        X=None,\n        X_names=None,\n        lens=None,\n        lens_names=None,\n        show_tooltips=True,\n        nbins=10,\n    ):\n        \"\"\"Generate a visualization of the simplicial complex mapper output. Turns the complex dictionary into a HTML/D3.js visualization\n\n        Parameters\n        ----------\n        graph : dict\n            Simplicial complex output from the `map` method.\n\n        color_function : list or 1d array\n            A 1d vector with length equal to number of data points used to build Mapper. Each value should correspond to a value for each data point and color of node is computed as the average value for members in a node.\n\n        path_html : String\n            file name for outputing the resulting html.\n\n        custom_meta: dict\n            Render (key, value) in the Mapper Summary pane. \n\n        custom_tooltip: list or array like\n            Value to display for each entry in the node. The cluster data pane will display entry for all values in the node. Default is index of data.\n\n        save_file: bool, default is True\n            Save file to `path_html`.\n\n        X: numpy arraylike\n            If supplied, compute statistics information about the original data source with respect to each node.\n\n        X_names: list of strings\n            Names of each variable in `X` to be displayed. If None, then display names by index.\n\n        lens: numpy arraylike\n            If supplied, compute statistics of each node based on the projection/lens\n\n        lens_name: list of strings\n            Names of each variable in `lens` to be displayed. In None, then display names by index.\n\n        show_tooltips: bool, default is True.\n            If false, completely disable tooltips. This is useful when using output in space-tight pages or will display node data in custom ways.\n\n        nbins: int, default is 10\n            Number of bins shown in histogram of tooltip color distributions.\n\n        Returns\n        --------\n        html: string\n            Returns the same html that is normally output to `path_html`. Complete graph and data ready for viewing.\n\n        Examples\n        ---------\n\n        >>> # Basic creation of a `.html` file at `kepler-mapper-output.html`\n        >>> html = mapper.visualize(graph, path_html=\"kepler-mapper-output.html\")\n\n        >>> # Jupyter Notebook support\n        >>> from kmapper import jupyter\n        >>> html = mapper.visualize(graph, path_html=\"kepler-mapper-output.html\")\n        >>> jupyter.display(path_html=\"kepler-mapper-output.html\")\n\n        >>> # Customizing the output text\n        >>> html = mapper.visualize(\n        >>>     graph, \n        >>>     path_html=\"kepler-mapper-output.html\",\n        >>>     title=\"Fashion MNIST with UMAP\",\n        >>>     custom_meta={\"Description\":\"A short description.\",\n        >>>                  \"Cluster\": \"HBSCAN()\"}\n        >>> )\n\n        >>> # Custom coloring function based on your 1d lens\n        >>> html = mapper.visualize(\n        >>>     graph,\n        >>>     color_function=lens\n        >>> )\n\n        >>> # Custom coloring function based on the first variable\n        >>> cf = mapper.project(X, projection=[0])\n        >>> html = mapper.visualize(\n        >>>     graph,\n        >>>     color_function=cf\n        >>> )\n\n        >>> # Customizing the tooltips with binary target variables\n        >>> X, y = split_data(df)\n        >>> html = mapper.visualize(\n        >>>     graph, \n        >>>     path_html=\"kepler-mapper-output.html\",\n        >>>     title=\"Fashion MNIST with UMAP\",\n        >>>     custom_tooltips=y\n        >>> )\n\n        >>> # Customizing the tooltips with html-strings: locally stored images of an image dataset\n        >>> html = mapper.visualize(\n        >>>     graph, \n        >>>     path_html=\"kepler-mapper-output.html\",\n        >>>     title=\"Fashion MNIST with UMAP\",\n        >>>     custom_tooltips=np.array(\n        >>>             [\"<img src='img/%s.jpg'>\"%i for i in range(inverse_X.shape[0])]\n        >>>     )\n        >>> )\n\n        \"\"\"\n\n        # TODO:\n        #   - Make color functions more intuitive. How do they even work?\n        #   - Allow multiple color functions that can be toggled on and off.\n\n        if not len(graph[\"nodes\"]) > 0:\n            raise Exception(\n                \"Visualize requires a mapper with more than 0 nodes. \\nIt is possible that the constructed mapper could have been constructed with bad parameters. This can occasionally happens when using the default clustering algorithm. Try changing `eps` or `min_samples` in the DBSCAN clustering algorithm.\"\n            )\n\n        # Find the module absolute path and locate templates\n        module_root = os.path.join(os.path.dirname(__file__), \"templates\")\n        env = Environment(loader=FileSystemLoader(module_root))\n        # Color function is a vector of colors?\n        color_function = init_color_function(graph, color_function)\n\n        if X_names is None:\n            X_names = []\n\n        if lens_names is None:\n            lens_names = []\n\n        mapper_data = format_mapper_data(\n            graph,\n            color_function,\n            X,\n            X_names,\n            lens,\n            lens_names,\n            custom_tooltips,\n            env,\n            nbins,\n        )\n\n        colorscale = colorscale_default\n\n        histogram = graph_data_distribution(graph, color_function, colorscale)\n\n        mapper_summary = format_meta(graph, custom_meta)\n\n        # Find the absolute module path and the static files\n        js_path = os.path.join(os.path.dirname(__file__), \"static\", \"kmapper.js\")\n        with open(js_path, \"r\") as f:\n            js_text = f.read()\n\n        css_path = os.path.join(os.path.dirname(__file__), \"static\", \"style.css\")\n        with open(css_path, \"r\") as f:\n            css_text = f.read()\n\n        # Render the Jinja template, filling fields as appropriate\n        template = env.get_template(\"base.html\").render(\n            title=title,\n            mapper_summary=mapper_summary,\n            histogram=histogram,\n            dist_label=\"Node\",\n            mapper_data=mapper_data,\n            colorscale=colorscale,\n            js_text=js_text,\n            css_text=css_text,\n            show_tooltips=True,\n        )\n\n        if save_file:\n            with open(path_html, \"wb\") as outfile:\n                if self.verbose > 0:\n                    print(\"Wrote visualization to: %s\" % (path_html))\n                outfile.write(template.encode(\"utf-8\"))\n\n        return template"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the original data of each cluster member for a given cluster ID.", "response": "def data_from_cluster_id(self, cluster_id, graph, data):\n        \"\"\"Returns the original data of each cluster member for a given cluster ID\n\n        Parameters\n        ----------\n        cluster_id : String\n            ID of the cluster.\n        graph : dict\n            The resulting dictionary after applying map()\n        data : Numpy Array\n            Original dataset. Accepts both 1-D and 2-D array.\n\n        Returns\n        -------\n        entries:\n            rows of cluster member data as Numpy array.\n\n        \"\"\"\n        if cluster_id in graph[\"nodes\"]:\n            cluster_members = graph[\"nodes\"][cluster_id]\n            cluster_members_data = data[cluster_members]\n            return cluster_members_data\n        else:\n            return np.array([])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display(path_html=\"mapper_visualization_output.html\"):\n\n    iframe = (\n        \"<iframe src=\" + path_html + ' width=100%% height=800 frameBorder=\"0\"></iframe>'\n    )\n    IPython.core.display.display(IPython.core.display.HTML(iframe))", "response": "Displays a Jupyter Notebook output cell."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _colors_to_rgb(colorscale):\n    if colorscale[0][1][0] == \"#\":\n        plotly_colors = np.array(colorscale)[:, 1].tolist()\n        for k, hexcode in enumerate(plotly_colors):\n            hexcode = hexcode.lstrip(\"#\")\n            hex_len = len(hexcode)\n            step = hex_len // 3\n            colorscale[k][1] = \"rgb\" + str(\n                tuple(int(hexcode[j : j + step], 16) for j in range(0, hex_len, step))\n            )\n\n    return colorscale", "response": "Convert a color scale to rgb."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap a value val in vmin vmax to the corresponding color in returns the rgb color code of that color in the colorscale", "response": "def _map_val2color(val, vmin, vmax, colorscale=None):\n    \"\"\" Maps a value val in [vmin, vmax] to the corresponding color in\n        the colorscale\n        returns the rgb color code of that color\n    \"\"\"\n    colorscale = colorscale or colorscale_default\n\n    if vmin >= vmax:\n        raise ValueError(\"vmin should be < vmax\")\n\n    scale = list(map(float, np.array(colorscale)[:, 0]))\n    colors = np.array(colorscale)[:, 1]\n\n    colors_01 = (\n        np.array(list(map(literal_eval, [color[3:] for color in colors]))) / 255.0\n    )\n\n    v = (val - vmin) / float((vmax - vmin))  # val is mapped to v in[0,1]\n\n    idx = 0\n    # sequential search for the two   consecutive indices idx, idx+1 such that\n    # v belongs to the interval  [scale[idx], scale[idx+1]\n    while v > scale[idx + 1]:\n        idx += 1\n    left_scale_val = scale[idx]\n    right_scale_val = scale[idx + 1]\n    vv = (v - left_scale_val) / (right_scale_val - left_scale_val)\n\n    # get the triplet of three values in [0,1] that represent the rgb color\n    # corresponding to val\n    val_color01 = colors_01[idx] + vv * (colors_01[idx + 1] - colors_01[idx])\n    val_color_0255 = list(map(np.uint8, 255 * val_color01))\n\n    return \"rgb\" + str(tuple(val_color_0255))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a histogram of data based on values of color_function", "response": "def build_histogram(data, colorscale=None, nbins=10):\n    \"\"\" Build histogram of data based on values of color_function\n    \"\"\"\n\n    if colorscale is None:\n        colorscale = colorscale_default\n\n    # TODO: we should weave this method of handling colors into the normal build_histogram and combine both functions\n    colorscale = _colors_to_rgb(colorscale)\n\n    h_min, h_max = 0, 1\n    hist, bin_edges = np.histogram(data, range=(h_min, h_max), bins=nbins)\n    bin_mids = np.mean(np.array(list(zip(bin_edges, bin_edges[1:]))), axis=1)\n\n    histogram = []\n    max_bucket_value = max(hist)\n    sum_bucket_value = sum(hist)\n    for bar, mid in zip(hist, bin_mids):\n        height = np.floor(((bar / max_bucket_value) * 100) + 0.5)\n        perc = round((bar / sum_bucket_value) * 100.0, 1)\n        color = _map_val2color(mid, 0.0, 1.0, colorscale)\n\n        histogram.append({\"height\": height, \"perc\": perc, \"color\": color})\n\n    return histogram"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_matplotlib(g, ax=None, fig=None):\n    import networkx as nx\n    import matplotlib.pyplot as plt\n\n    fig = fig if fig else plt.figure()\n    ax = ax if ax else plt.gca()\n\n    if not isinstance(g, nx.Graph):\n        from .adapter import to_networkx\n\n        g = to_networkx(g)\n\n    # Determine a fine size for nodes\n    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    width, height = bbox.width, bbox.height\n    area = width * height * fig.dpi\n\n    n_nodes = len(g.nodes)\n\n    # size of node should be related to area and number of nodes -- heuristic\n    node_size = np.pi * area / n_nodes\n    node_r = np.sqrt(node_size / np.pi)\n    node_edge = node_r / 3\n\n    pos = nx.spring_layout(g)\n\n    nodes = nx.draw_networkx_nodes(g, node_size=node_size, pos=pos)\n    edges = nx.draw_networkx_edges(g, pos=pos)\n    nodes.set_edgecolor(\"w\")\n    nodes.set_linewidth(node_edge)\n\n    plt.axis(\"square\")\n    plt.axis(\"off\")\n\n    return nodes", "response": "Draw the graph using NetworkX drawing functionality."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Plotly figure from a complex graph.", "response": "def plotlyviz(\r\n    scomplex,\r\n    colorscale=None,\r\n    title=\"Kepler Mapper\",\r\n    graph_layout=\"kk\",\r\n    color_function=None,\r\n    color_function_name=None,\r\n    dashboard=False,\r\n    graph_data=False,\r\n    factor_size=3,\r\n    edge_linewidth=1.5,\r\n    node_linecolor=\"rgb(200,200,200)\",\r\n    width=600,\r\n    height=500,\r\n    bgcolor=\"rgba(240, 240, 240, 0.95)\",\r\n    left=10,\r\n    bottom=35,\r\n    summary_height=300,\r\n    summary_width=600,\r\n    summary_left=20,\r\n    summary_right=20,\r\n    hist_left=25,\r\n    hist_right=25,\r\n    member_textbox_width=800,\r\n    filename=None,\r\n):\r\n    \"\"\"\r\n        Visualizations and dashboards for kmapper graphs using Plotly. This method is suitable for use in Jupyter notebooks.\r\n\r\n\r\n        The generated FigureWidget can be updated (by performing a restyle or relayout). For example, let us add a title \r\n        to the colorbar (the name of the color function, if any),\r\n        and set the title font size. To perform these updates faster, Plotly 3.+ provides a context manager that batches up all data and layout updates:\r\n\r\n        To display more info on the generated kmapper-graph, define two more FigureWidget(s):  \r\n        the global node distribution figure, and a dummy figure\r\n        that displays info on the  algorithms involved in getting the graph from data, as well as  sklearn  class instances.\r\n\r\n        A FigureWidget has event listeners for hovering, clicking or selecting. Using the first one for `fw_graph`\r\n        we   define, via the function `hovering_widgets()`, widgets that display the node distribution, when the node is hovered over, and two textboxes for the cluster size and the member ids/labels of the hovered node members.\r\n\r\n\r\n\r\n        Parameters\r\n        -----------\r\n\r\n        scomplex: dict\r\n            Simplicial complex is the output from the KeplerMapper `map` method.\r\n    \r\n        title: str\r\n            Title of output graphic\r\n\r\n        graph_layout: igraph layout; \r\n            recommended 'kk' (kamada-kawai) or 'fr' (fruchterman-reingold)\r\n        \r\n        colorscale: \r\n             Plotly colorscale(colormap) to color graph nodes\r\n\r\n        dashboard: bool, default is False\r\n            If true, display complete dashboard of node information\r\n\r\n        graph_data: bool, default is False\r\n            If true, display graph metadata\r\n\r\n        factor_size: double, default is 3\r\n            a factor for the node size\r\n\r\n        edge_linewidth : double, default is 1.5\r\n        node_linecolor: color str, default is \"rgb(200,200,200)\"\r\n        width: int, default is 600,\r\n        height: int, default is 500,\r\n        bgcolor: color str, default is \"rgba(240, 240, 240, 0.95)\",\r\n        left: int, default is 10,\r\n        bottom: int, default is 35,\r\n        summary_height: int, default is 300,\r\n        summary_width: int, default is 600, \r\n        summary_left: int, default is 20,\r\n        summary_right: int, default is 20,\r\n        hist_left: int, default is 25,\r\n        hist_right: int, default is 25,\r\n        member_textbox_width: int, default is 800, \r\n        filename: str, default is None\r\n            if filename is given, the graphic will be saved to that file.\r\n\r\n\r\n        Returns\r\n        ---------\r\n        result: plotly.FigureWidget\r\n            A FigureWidget that can be shown or editted. See the Plotly Demo notebook for examples of use.\r\n\r\n    \"\"\"\r\n\r\n    if not colorscale:\r\n        colorscale = default_colorscale\r\n\r\n    kmgraph, mapper_summary, n_color_distribution = get_mapper_graph(\r\n        scomplex,\r\n        colorscale=colorscale,\r\n        color_function=color_function,\r\n        color_function_name=color_function_name,\r\n    )\r\n\r\n    annotation = get_kmgraph_meta(mapper_summary)\r\n\r\n    plgraph_data = plotly_graph(\r\n        kmgraph,\r\n        graph_layout=graph_layout,\r\n        colorscale=colorscale,\r\n        factor_size=factor_size,\r\n        edge_linewidth=edge_linewidth,\r\n        node_linecolor=node_linecolor,\r\n    )\r\n\r\n    layout = plot_layout(\r\n        title=title,\r\n        width=width,\r\n        height=height,\r\n        annotation_text=annotation,\r\n        bgcolor=bgcolor,\r\n        left=left,\r\n        bottom=bottom,\r\n    )\r\n    result = go.FigureWidget(data=plgraph_data, layout=layout)\r\n\r\n    if color_function_name:\r\n        with result.batch_update():\r\n            result.data[1].marker.colorbar.title = color_function_name\r\n            result.data[1].marker.colorbar.titlefont.size = 10\r\n\r\n    if dashboard or graph_data:\r\n        fw_hist = node_hist_fig(n_color_distribution, left=hist_left, right=hist_right)\r\n        fw_summary = summary_fig(\r\n            mapper_summary,\r\n            width=summary_width,\r\n            height=summary_height,\r\n            left=summary_left,\r\n            right=summary_right,\r\n        )\r\n\r\n        fw_graph = result\r\n        result = hovering_widgets(\r\n            kmgraph, fw_graph, member_textbox_width=member_textbox_width\r\n        )\r\n\r\n        if graph_data:\r\n            result = ipw.VBox([fw_graph, ipw.HBox([fw_summary, fw_hist])])\r\n\r\n    if filename:\r\n        pio.write_image(result, filename)\r\n\r\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mapper_graph(\r\n    simplicial_complex,\r\n    color_function=None,\r\n    color_function_name=None,\r\n    colorscale=None,\r\n    custom_tooltips=None,\r\n    custom_meta=None,\r\n    X=None,\r\n    X_names=None,\r\n    lens=None,\r\n    lens_names=None,\r\n):\r\n    \"\"\"Generate data for mapper graph visualization and annotation.\r\n\r\n    Parameters\r\n    ----------\r\n    simplicial_complex : dict\r\n        Simplicial complex is the output from the KeplerMapper `map` method.\r\n    \r\n    Returns\r\n    -------\r\n    the graph dictionary in a json representation, the mapper summary\r\n    and the node_distribution\r\n\r\n    Example\r\n    -------\r\n\r\n    >>> kmgraph,  mapper_summary, n_distribution = get_mapper_graph(simplicial_complex)\r\n\r\n    \"\"\"\r\n\r\n    if not colorscale:\r\n        colorscale = default_colorscale\r\n\r\n    if not len(simplicial_complex[\"nodes\"]) > 0:\r\n        raise Exception(\r\n            \"A mapper graph should have more than 0 nodes. This might be because your clustering algorithm might be too sensitive and be classifying all points as noise.\"\r\n        )\r\n\r\n    color_function = init_color_function(simplicial_complex, color_function)\r\n\r\n    if X_names is None:\r\n        X_names = []\r\n\r\n    if lens_names is None:\r\n        lens_names = []\r\n\r\n    json_graph = scomplex_to_graph(\r\n        simplicial_complex,\r\n        color_function,\r\n        X,\r\n        X_names,\r\n        lens,\r\n        lens_names,\r\n        custom_tooltips,\r\n        colorscale=colorscale,\r\n    )\r\n    colorf_distribution = graph_data_distribution(\r\n        simplicial_complex, color_function, colorscale\r\n    )\r\n    mapper_summary = format_meta(\r\n        simplicial_complex,\r\n        color_function_name=color_function_name,\r\n        custom_meta=custom_meta,\r\n    )\r\n\r\n    return json_graph, mapper_summary, colorf_distribution", "response": "Returns a dictionary that represents a single graph in a json representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plotly_graph(\r\n    kmgraph,\r\n    graph_layout=\"kk\",\r\n    colorscale=None,\r\n    showscale=True,\r\n    factor_size=3,\r\n    edge_linecolor=\"rgb(180,180,180)\",\r\n    edge_linewidth=1.5,\r\n    node_linecolor=\"rgb(255,255,255)\",\r\n    node_linewidth=1.0,\r\n):\r\n    \"\"\"Generate Plotly data structures that represent the mapper graph\r\n\r\n    Parameters\r\n    ----------\r\n    kmgraph: dict representing the mapper graph,\r\n             returned by the function get_mapper_graph()\r\n    graph_layout: igraph layout; recommended 'kk' (kamada-kawai)\r\n                  or 'fr' (fruchterman-reingold)\r\n    colorscale: a Plotly colorscale(colormap) to color graph nodes\r\n    showscale: boolean to display or not the colorbar\r\n    factor_size: a factor for the node size\r\n\r\n    Returns\r\n    -------\r\n    The plotly traces (dicts) representing the graph edges and nodes\r\n    \"\"\"\r\n\r\n    if not colorscale:\r\n        colorscale = default_colorscale\r\n\r\n\r\n    # define an igraph.Graph instance of n_nodes\r\n    n_nodes = len(kmgraph[\"nodes\"])\r\n    if n_nodes == 0:\r\n        raise ValueError(\"Your graph has 0 nodes\")\r\n    G = ig.Graph(n=n_nodes)\r\n    links = [(e[\"source\"], e[\"target\"]) for e in kmgraph[\"links\"]]\r\n    G.add_edges(links)\r\n    layt = G.layout(graph_layout)\r\n\r\n    hover_text = [node[\"name\"] for node in kmgraph[\"nodes\"]]\r\n    color_vals = [node[\"color\"] for node in kmgraph[\"nodes\"]]\r\n    node_size = np.array(\r\n        [factor_size * node[\"size\"] for node in kmgraph[\"nodes\"]], dtype=np.int\r\n    )\r\n    Xn, Yn, Xe, Ye = _get_plotly_data(links, layt)\r\n\r\n    edge_trace = dict(\r\n        type=\"scatter\",\r\n        x=Xe,\r\n        y=Ye,\r\n        mode=\"lines\",\r\n        line=dict(color=edge_linecolor, width=edge_linewidth),\r\n        hoverinfo=\"none\",\r\n    )\r\n\r\n    node_trace = dict(\r\n        type=\"scatter\",\r\n        x=Xn,\r\n        y=Yn,\r\n        mode=\"markers\",\r\n        marker=dict(\r\n            size=node_size.tolist(),\r\n            color=color_vals,\r\n            opacity=1.0,\r\n            colorscale=colorscale,\r\n            showscale=showscale,\r\n            line=dict(color=node_linecolor, width=node_linewidth),\r\n            colorbar=dict(thickness=20, ticklen=4, x=1.01, tickfont=dict(size=10)),\r\n        ),\r\n        text=hover_text,\r\n        hoverinfo=\"text\",\r\n    )\r\n\r\n    return [edge_trace, node_trace]", "response": "Returns a new graph that is a plotly representation of the mapper graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract info from mapper summary to be displayed below the graph plot", "response": "def get_kmgraph_meta(mapper_summary):\r\n    \"\"\" Extract info from mapper summary to be displayed below the graph plot\r\n    \"\"\"\r\n    d = mapper_summary[\"custom_meta\"]\r\n    meta = (\r\n        \"<b>N_cubes:</b> \"\r\n        + str(d[\"n_cubes\"])\r\n        + \" <b>Perc_overlap:</b> \"\r\n        + str(d[\"perc_overlap\"])\r\n    )\r\n    meta += (\r\n        \"<br><b>Nodes:</b> \"\r\n        + str(mapper_summary[\"n_nodes\"])\r\n        + \" <b>Edges:</b> \"\r\n        + str(mapper_summary[\"n_edges\"])\r\n        + \" <b>Total samples:</b> \"\r\n        + str(mapper_summary[\"n_total\"])\r\n        + \" <b>Unique_samples:</b> \"\r\n        + str(mapper_summary[\"n_unique\"])\r\n    )\r\n\r\n    return meta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_layout(\r\n    title=\"TDA KMapper\",\r\n    width=600,\r\n    height=600,\r\n    bgcolor=\"rgba(255, 255, 255, 1)\",\r\n    annotation_text=None,\r\n    annotation_x=0,\r\n    annotation_y=-0.01,\r\n    top=100,\r\n    left=60,\r\n    right=60,\r\n    bottom=60,\r\n):\r\n    \"\"\"Set the plotly layout\r\n\r\n    Parameters\r\n    ----------\r\n    width, height: integers \r\n        setting  width and height of plot window\r\n    bgcolor: string, \r\n        rgba or hex color code for the background color\r\n    annotation_text: string\r\n        meta data to be displayed\r\n    annotation_x & annotation_y:\r\n        The coordinates of the point where we insert the annotation; the negative sign for y coord points output that annotation is inserted below the plot\r\n    \"\"\"\r\n    pl_layout = dict(\r\n        title=title,\r\n        font=dict(size=12),\r\n        showlegend=False,\r\n        autosize=False,\r\n        width=width,\r\n        height=height,\r\n        xaxis=dict(visible=False),\r\n        yaxis=dict(visible=False),\r\n        hovermode=\"closest\",\r\n        plot_bgcolor=bgcolor,\r\n        margin=dict(t=top, b=bottom, l=left, r=right),\r\n    )\r\n\r\n    if annotation_text is None:\r\n        return pl_layout\r\n    else:\r\n        annotations = [\r\n            dict(\r\n                showarrow=False,\r\n                text=annotation_text,\r\n                xref=\"paper\",\r\n                yref=\"paper\",\r\n                x=annotation_x,\r\n                y=annotation_y,\r\n                align=\"left\",\r\n                xanchor=\"left\",\r\n                yanchor=\"top\",\r\n                font=dict(size=12),\r\n            )\r\n        ]\r\n        pl_layout.update(annotations=annotations)\r\n        return pl_layout", "response": "Set the plotly layout of a TDA KMapper."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine the plotly figure representing the node histogram of the graph.", "response": "def node_hist_fig(\r\n    node_color_distribution,\r\n    title=\"Graph Node Distribution\",\r\n    width=400,\r\n    height=300,\r\n    top=60,\r\n    left=25,\r\n    bottom=60,\r\n    right=25,\r\n    bgcolor=\"rgb(240,240,240)\",\r\n    y_gridcolor=\"white\",\r\n):\r\n    \"\"\"Define the plotly plot representing the node histogram\r\n    \r\n    Parameters\r\n    ----------\r\n    node_color_distribution: list of dicts describing the build_histogram\r\n    width, height: integers -  width and height of the histogram FigureWidget\r\n    left, top, right, bottom: ints; number of pixels around the FigureWidget\r\n    bgcolor: rgb of hex color code for the figure background color\r\n    y_gridcolor: rgb of hex color code for the yaxis y_gridcolor\r\n\r\n    Returns\r\n    -------\r\n    FigureWidget object representing the histogram of the graph nodes\r\n    \"\"\"\r\n\r\n    text = [\r\n        \"{perc}%\".format(**locals())\r\n        for perc in [d[\"perc\"] for d in node_color_distribution]\r\n    ]\r\n\r\n    pl_hist = go.Bar(\r\n        y=[d[\"height\"] for d in node_color_distribution],\r\n        marker=dict(color=[d[\"color\"] for d in node_color_distribution]),\r\n        text=text,\r\n        hoverinfo=\"y+text\",\r\n    )\r\n\r\n    hist_layout = dict(\r\n        title=title,\r\n        width=width,\r\n        height=height,\r\n        font=dict(size=12),\r\n        xaxis=dict(showline=True, zeroline=False, showgrid=False, showticklabels=False),\r\n        yaxis=dict(showline=False, gridcolor=y_gridcolor, tickfont=dict(size=10)),\r\n        bargap=0.01,\r\n        margin=dict(l=left, r=right, b=bottom, t=top),\r\n        hovermode=\"x\",\r\n        plot_bgcolor=bgcolor,\r\n    )\r\n\r\n    return go.FigureWidget(data=[pl_hist], layout=hist_layout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summary_fig(\r\n    mapper_summary,\r\n    width=600,\r\n    height=500,\r\n    top=60,\r\n    left=20,\r\n    bottom=60,\r\n    right=20,\r\n    bgcolor=\"rgb(240,240,240)\",\r\n):\r\n    \"\"\"Define a dummy figure that displays info on the algorithms and\r\n       sklearn class instances or methods used\r\n\r\n       Returns a FigureWidget object representing the figure\r\n    \"\"\"\r\n    text = _text_mapper_summary(mapper_summary)\r\n\r\n    data = [\r\n        dict(\r\n            type=\"scatter\",\r\n            x=[0, width],\r\n            y=[height, 0],\r\n            mode=\"text\",\r\n            text=[text, \"\"],\r\n            textposition=\"bottom right\",\r\n            hoverinfo=\"none\",\r\n        )\r\n    ]\r\n\r\n    layout = dict(\r\n        title=\"Algorithms and scikit-learn objects/methods\",\r\n        width=width,\r\n        height=height,\r\n        font=dict(size=12),\r\n        xaxis=dict(visible=False),\r\n        yaxis=dict(visible=False, range=[0, height + 5]),\r\n        margin=dict(t=top, b=bottom, l=left, r=right),\r\n        plot_bgcolor=bgcolor,\r\n    )\r\n\r\n    return go.FigureWidget(data=data, layout=layout)", "response": "Define a dummy figure that displays info on the algorithms and sklearn class instances or methods used\r\nonedata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute(self, nodes):\n\n        result = defaultdict(list)\n\n        # Create links when clusters from different hypercubes have members with the same sample id.\n        candidates = itertools.combinations(nodes.keys(), 2)\n        for candidate in candidates:\n            # if there are non-unique members in the union\n            if (\n                len(set(nodes[candidate[0]]).intersection(nodes[candidate[1]]))\n                >= self.min_intersection\n            ):\n                result[candidate[0]].append(candidate[1])\n\n        edges = [[x, end] for x in result for end in result[x]]\n        simplices = [[n] for n in nodes] + edges\n        return result, simplices", "response": "Compute edges of overlapping clusters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits a cover on the data.", "response": "def fit(self, data):\n        \"\"\" Fit a cover on the data. This method constructs centers and radii in each dimension given the `perc_overlap` and `n_cube`.\n\n        Parameters\n        ============\n\n        data: array-like\n            Data to apply the cover to. Warning: First column must be an index column.\n\n        Returns\n        ========\n\n        centers: list of arrays\n            A list of centers for each cube\n\n        \"\"\"\n\n\n        # TODO: support indexing into any columns\n        di = np.array(range(1, data.shape[1]))\n        indexless_data = data[:, di]\n        n_dims = indexless_data.shape[1]\n\n        # support different values along each dimension\n\n        ## -- is a list, needs to be array\n        ## -- is a singleton, needs repeating\n        if isinstance(self.n_cubes, Iterable):\n            n_cubes = np.array(self.n_cubes)\n            assert (\n                len(n_cubes) == n_dims\n            ), \"Custom cubes in each dimension must match number of dimensions\"\n        else:\n            n_cubes = np.repeat(self.n_cubes, n_dims)\n\n        if isinstance(self.perc_overlap, Iterable):\n            perc_overlap = np.array(self.perc_overlap)\n            assert (\n                len(perc_overlap) == n_dims\n            ), \"Custom cubes in each dimension must match number of dimensions\"\n        else:\n            perc_overlap = np.repeat(self.perc_overlap, n_dims)\n\n        assert all(0.0 <= p <= 1.0 for p in perc_overlap), (\n            \"Each overlap percentage must be between 0.0 and 1.0., not %s\"\n            % perc_overlap\n        )\n\n        bounds = self._compute_bounds(indexless_data)\n        ranges = bounds[1] - bounds[0]\n\n        # (n-1)/n |range|\n        inner_range = ((n_cubes - 1) / n_cubes) * ranges\n        inset = (ranges - inner_range) / 2\n\n        # |range| / (2n ( 1 - p))\n        radius = ranges / (2 * (n_cubes) * (1 - perc_overlap))\n\n        # centers are fixed w.r.t perc_overlap\n        zip_items = list(bounds)  # work around 2.7,3.4 weird behavior\n        zip_items.extend([n_cubes, inset])\n        centers_per_dimension = [\n            np.linspace(b + r, c - r, num=n) for b, c, n, r in zip(*zip_items)\n        ]\n        centers = [np.array(c) for c in product(*centers_per_dimension)]\n\n        self.centers_ = centers\n        self.radius_ = radius\n        self.inset_ = inset\n        self.inner_range_ = inner_range\n        self.bounds_ = bounds\n        self.di_ = di\n\n        if self.verbose > 0:\n            print(\n                \" - Cover - centers: %s\\ninner_range: %s\\nradius: %s\"\n                % (self.centers_, self.inner_range_, self.radius_)\n            )\n\n        return centers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming data into a single hypercube centered at center.", "response": "def transform_single(self, data, center, i=0):\n        \"\"\" Compute entries of `data` in hypercube centered at `center`\n\n        Parameters\n        ===========\n\n        data: array-like\n            Data to find in entries in cube. Warning: first column must be index column.\n        center: array-like\n            Center points for the cube. Cube is found as all data in `[center-self.radius_, center+self.radius_]`\n        i: int, default 0\n            Optional counter to aid in verbose debugging.\n        \"\"\"\n\n        lowerbounds, upperbounds = center - self.radius_, center + self.radius_\n\n        # Slice the hypercube\n        entries = (data[:, self.di_] >= lowerbounds) & (\n            data[:, self.di_] <= upperbounds\n        )\n        hypercube = data[np.invert(np.any(entries == False, axis=1))]\n\n        if self.verbose > 1:\n            print(\n                \"There are %s points in cube %s/%s\"\n                % (hypercube.shape[0], i + 1, len(self.centers_))\n            )\n\n        return hypercube"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a list of data into a list of hypercubes.", "response": "def transform(self, data, centers=None):\n        \"\"\" Find entries of all hypercubes. If `centers=None`, then use `self.centers_` as computed in `self.fit`.\n            \n            Empty hypercubes are removed from the result\n\n        Parameters\n        ===========\n\n        data: array-like\n            Data to find in entries in cube. Warning: first column must be index column.\n        centers: list of array-like\n            Center points for all cubes as returned by `self.fit`. Default is to use `self.centers_`.\n\n        Returns\n        =========\n        hypercubes: list of array-like\n            list of entries in each hypercobe in `data`.\n\n        \"\"\"\n\n        centers = centers or self.centers_\n        hypercubes = [\n            self.transform_single(data, cube, i) for i, cube in enumerate(centers)\n        ]\n\n        # Clean out any empty cubes (common in high dimensions)\n        hypercubes = [cube for cube in hypercubes if len(cube)]\n        return hypercubes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a Mapper 1 - complex graph to a networkx graph.", "response": "def to_networkx(graph):\n    \"\"\" Convert a Mapper 1-complex to a networkx graph.\n\n    Parameters\n    -----------\n\n    graph: dictionary, graph object returned from `kmapper.map`\n\n    Returns\n    --------\n\n    g: graph as networkx.Graph() object\n\n    \"\"\"\n\n    # import here so networkx is not always required.\n    import networkx as nx\n\n    nodes = graph[\"nodes\"].keys()\n    edges = [[start, end] for start, ends in graph[\"links\"].items() for end in ends]\n\n    g = nx.Graph()\n    g.add_nodes_from(nodes)\n    nx.set_node_attributes(g, dict(graph[\"nodes\"]), \"membership\")\n\n    g.add_edges_from(edges)\n\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nspelling a word into a list of words.", "response": "def spell(word: str, engine: str = \"pn\") -> List[str]:\n    \"\"\"\n    :param str word: word to check spelling\n    :param str engine:\n        * pn - Peter Norvig's algorithm (default)\n    :return: list of words\n    \"\"\"\n\n    return DEFAULT_SPELL_CHECKER.spell(word)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_corpus(filename: str) -> frozenset:\n    lines = []\n    with open(os.path.join(corpus_path(), filename), \"r\", encoding=\"utf-8-sig\") as fh:\n        lines = fh.read().splitlines()\n\n    return frozenset(lines)", "response": "Read a file and return a frozenset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_corpus_path(name: str) -> [str, None]:\n    db = TinyDB(corpus_db_path())\n    temp = Query()\n\n    if len(db.search(temp.name == name)) > 0:\n        path = get_full_data_path(db.search(temp.name == name)[0][\"file\"])\n        db.close()\n\n        if not os.path.exists(path):\n            download(name)\n\n        return path\n\n    return None", "response": "Get path to the corpus."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(name: str, force: bool = False):\n    db = TinyDB(corpus_db_path())\n    temp = Query()\n    data = requests.get(corpus_db_url())\n    data_json = data.json()\n    if name in list(data_json.keys()):\n        temp_name = data_json[name]\n        print(\"Download:\", name)\n\n        if not db.search(temp.name == name):\n            print(name + \" \" + temp_name[\"version\"])\n            _download(temp_name[\"download\"], temp_name[\"file_name\"])\n            db.insert(\n                {\n                    \"name\": name,\n                    \"version\": temp_name[\"version\"],\n                    \"file\": temp_name[\"file_name\"],\n                }\n            )\n        else:\n            if not db.search(\n                temp.name == name and temp.version == temp_name[\"version\"]\n            ):\n                print(\"Alert: New version is ready to be updated.\")\n                print(\n                    \"from \"\n                    + name\n                    + \" \"\n                    + db.search(temp.name == name)[0][\"version\"]\n                    + \" update to \"\n                    + name\n                    + \" \"\n                    + temp_name[\"version\"]\n                )\n                yes_no = \"y\"\n                if not force:\n                    yes_no = str(input(\"yes or no (y / n) : \")).lower()\n                if \"y\" == yes_no:\n                    _download(temp_name[\"download\"], temp_name[\"file_name\"])\n                    db.update({\"version\": temp_name[\"version\"]}, temp.name == name)\n            else:\n                print(\"Redownload\")\n                print(\n                    \"from \"\n                    + name\n                    + \" \"\n                    + db.search(temp.name == name)[0][\"version\"]\n                    + \" update to \"\n                    + name\n                    + \" \"\n                    + temp_name[\"version\"]\n                )\n                yes_no = \"y\"\n                if not force:\n                    yes_no = str(input(\"yes or no (y / n) : \")).lower()\n                if \"y\" == yes_no:\n                    _download(temp_name[\"download\"], temp_name[\"file_name\"])\n                    db.update({\"version\": temp_name[\"version\"]}, temp.name == name)\n    db.close()", "response": "Download a single language from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(name: str) -> bool:\n    db = TinyDB(corpus_db_path())\n    temp = Query()\n    data = db.search(temp.name == name)\n\n    if len(data) > 0:\n        path = get_corpus_path(name)\n        os.remove(path)\n        db.remove(temp.name == name)\n        return True\n\n    return False", "response": "Remove a single corpus from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collate(data: Iterable, reverse: bool = False) -> List[str]:\n    return sorted(data, key=_thkey, reverse=reverse)", "response": "collate a list of strings in alphabetically according to Thai rules\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef segment(text: str) -> str:\n\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    if re.search(r\"[\u0e40\u0e41]\" + _C + r\"[\" + \"\".join(_UV) + r\"]\" + r\"\\w\", text):\n        search = re.findall(r\"[\u0e40\u0e41]\" + _C + r\"[\" + \"\".join(_UV) + r\"]\" + r\"\\w\", text)\n        for i in search:\n            text = re.sub(i, \"/\" + i + \"/\", text)\n\n    if re.search(_C + r\"[\" + \"\".join(_UV1) + r\"]\" + _C + _C + r\"\u0e38\" + r\"\u0e4c\", text):\n        search = re.findall(\n            _C + r\"[\" + \"\".join(_UV1) + r\"]\" + _C + _C + r\"\u0e38\" + r\"\u0e4c\", text\n        )\n        for i in search:\n            text = re.sub(i, \"//\" + i + \"/\", text)\n\n    if re.search(_C + _UV2 + _C, text):\n        search = re.findall(_C + _UV2 + _C, text)\n        for i in search:\n            text = re.sub(i, \"/\" + i + \"/\", text)\n    re.sub(\"//\", \"/\", text)\n\n    if re.search(\"\u0e40\" + _C + \"\u0e32\" + \"\u0e30\", text):\n        search = re.findall(\"\u0e40\" + _C + \"\u0e32\" + \"\u0e30\", text)\n        for i in search:\n            text = re.sub(i, \"/\" + i + \"/\", text)\n\n    if re.search(\"\u0e40\" + r\"\\w\\w\" + \"\u0e32\" + \"\u0e30\", text):\n        search = re.findall(\"\u0e40\" + r\"\\w\\w\" + \"\u0e32\" + \"\u0e30\", text)\n        for i in search:\n            text = re.sub(i, \"/\" + i + \"/\", text)\n    text = re.sub(\"//\", \"/\", text)\n\n    if re.search(_C + \"[\" + \"\".join(_UV1) + \"]\" + _C + _C + \"\u0e4c\", text):\n        search = re.findall(_C + \"[\" + \"\".join(_UV1) + \"]\" + _C + _C + \"\u0e4c\", text)\n        for i in search:\n            text = re.sub(i, \"/\" + i + \"/\", text)\n\n    if re.search(\"/\" + _C + \"\".join([\"\u0e38\", \"\u0e4c\"]) + \"/\", text):\n        # \u0e41\u0e01\u0e49\u0e44\u0e02\u0e43\u0e19\u0e01\u0e23\u0e13\u0e35 \u0e1e\u0e31\u0e19/\u0e18\u0e38\u0e4c\n        search = re.findall(\"/\" + _C + \"\".join([\"\u0e38\", \"\u0e4c\"]) + \"/\", text)\n        for i in search:\n            ii = re.sub(\"/\", \"\", i)\n            text = re.sub(i, ii + \"/\", text)\n\n    text = re.sub(\"//\", \"/\", text)\n\n    return text.split(\"/\")", "response": "segment text into Enhanced Thai Character Cluster"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef summarize(\n    text: str, n: int, engine: str = \"frequency\", tokenizer: str = \"newmm\"\n) -> List[str]:\n    \"\"\"\n    Thai text summarization\n\n    :param str text: text to be summarized\n    :param int n: number of sentences to be included in the summary\n    :param str engine: text summarization engine\n    :param str tokenizer: word tokenizer\n    :return List[str] summary: list of selected sentences\n    \"\"\"\n    sents = []\n\n    if engine == \"frequency\":\n        sents = FrequencySummarizer().summarize(text, n, tokenizer)\n    else:  # if engine not found, return first n sentences\n        sents = sent_tokenize(text)[:n]\n\n    return sents", "response": "Summarize the text n sentences in the given n - th thai tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edges(word: str, lang: str = \"th\"):\n\n    obj = requests.get(f\"http://api.conceptnet.io/c/{lang}/{word}\").json()\n    return obj[\"edges\"]", "response": "Get the edges of a word in a language"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert Thai digits to Arabic digits such as 1 2 3.", "response": "def thai_digit_to_arabic_digit(text: str) -> str:\n    \"\"\"\n    :param str text: Text with Thai digits such as '\u0e51', '\u0e52', '\u0e53'\n    :return: Text with Thai digits being converted to Arabic digits such as '1', '2', '3'\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    newtext = []\n    for ch in text:\n        if ch in _thai_arabic:\n            newtext.append(_thai_arabic[ch])\n        else:\n            newtext.append(ch)\n\n    return \"\".join(newtext)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting Arabic digits to Thai digits such as 1 2 3 etc.", "response": "def arabic_digit_to_thai_digit(text: str) -> str:\n    \"\"\"\n    :param str text: Text with Arabic digits such as '1', '2', '3'\n    :return: Text with Arabic digits being converted to Thai digits such as '\u0e51', '\u0e52', '\u0e53'\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    newtext = []\n    for ch in text:\n        if ch in _arabic_thai:\n            newtext.append(_arabic_thai[ch])\n        else:\n            newtext.append(ch)\n\n    return \"\".join(newtext)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a digit to a Thai - Arabic string.", "response": "def digit_to_text(text: str) -> str:\n    \"\"\"\n    :param str text: Text with digits such as '1', '2', '\u0e53', '\u0e54'\n    :return: Text with digits being spelled out in Thai\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    newtext = []\n    for ch in text:\n        if ch in _thai_arabic:\n            ch = _thai_arabic[ch]\n\n        if ch in _digit_spell:\n            newtext.append(_digit_spell[ch])\n        else:\n            newtext.append(ch)\n\n    return \"\".join(newtext)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef word_freq(word: str, domain: str = \"all\") -> int:\n    listdomain = {\n        \"all\": \"\",\n        \"imaginative\": \"1\",\n        \"natural-pure-science\": \"2\",\n        \"applied-science\": \"3\",\n        \"social-science\": \"4\",\n        \"world-affairs-history\": \"5\",\n        \"commerce-finance\": \"6\",\n        \"arts\": \"7\",\n        \"belief-thought\": \"8\",\n        \"leisure\": \"9\",\n        \"others\": \"0\",\n    }\n    url = \"http://www.arts.chula.ac.th/~ling/TNCII/corp.php\"\n    # New URL is http://www.arts.chula.ac.th/~ling/tnc3/\n    data = {\"genre[]\": \"\", \"domain[]\": listdomain[domain], \"sortby\": \"perc\", \"p\": word}\n\n    r = requests.post(url, data=data)\n\n    pat = re.compile(r'TOTAL</font>(?s).*?#ffffff\">(.*?)</font>')\n    match = pat.search(r.text)\n\n    n = 0\n    if match:\n        n = int(match.group(1).strip())\n\n    return n", "response": "Get the frequency of a word by domain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef word_freqs() -> List[Tuple[str, int]]:\n    lines = list(get_corpus(_FILENAME))\n    listword = []\n    for line in lines:\n        listindata = line.split(\"\\t\")\n        listword.append((listindata[0], int(listindata[1])))\n\n    return listword", "response": "Get word frequency from Thai National Corpus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eng_to_thai(text: str) -> str:\n    return \"\".join(\n        [EN_TH_KEYB_PAIRS[ch] if (ch in EN_TH_KEYB_PAIRS) else ch for ch in text]\n    )", "response": "Converts text in one language that is incorrectly - typed with a keyboard layout in another language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef thai_to_eng(text: str) -> str:\n    return \"\".join(\n        [TH_EN_KEYB_PAIRS[ch] if (ch in TH_EN_KEYB_PAIRS) else ch for ch in text]\n    )", "response": "Converts Thai text to English language."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bahttext(number: float) -> str:\n    ret = \"\"\n\n    if number is None:\n        pass\n    elif number == 0:\n        ret = \"\u0e28\u0e39\u0e19\u0e22\u0e4c\u0e1a\u0e32\u0e17\u0e16\u0e49\u0e27\u0e19\"\n    else:\n        num_int, num_dec = \"{:.2f}\".format(number).split(\".\")\n        num_int = int(num_int)\n        num_dec = int(num_dec)\n\n        baht = num_to_thaiword(num_int)\n        if baht:\n            ret = \"\".join([ret, baht, \"\u0e1a\u0e32\u0e17\"])\n\n        satang = num_to_thaiword(num_dec)\n        if satang and satang != \"\u0e28\u0e39\u0e19\u0e22\u0e4c\":\n            ret = \"\".join([ret, satang, \"\u0e2a\u0e15\u0e32\u0e07\u0e04\u0e4c\"])\n        else:\n            ret = \"\".join([ret, \"\u0e16\u0e49\u0e27\u0e19\"])\n\n    return ret", "response": "Converts a number to Thai text and adds a suffix of Baht currency."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef num_to_thaiword(number: int) -> str:\n    ret = \"\"\n\n    if number is None:\n        pass\n    elif number == 0:\n        ret = \"\u0e28\u0e39\u0e19\u0e22\u0e4c\"\n    else:\n        _POS_CALL = [\"\u0e41\u0e2a\u0e19\", \"\u0e2b\u0e21\u0e37\u0e48\u0e19\", \"\u0e1e\u0e31\u0e19\", \"\u0e23\u0e49\u0e2d\u0e22\", \"\u0e2a\u0e34\u0e1a\", \"\"]\n        _NUM_CALL = [\n            \"\",\n            \"\u0e2b\u0e19\u0e36\u0e48\u0e07\",\n            \"\u0e2a\u0e2d\u0e07\",\n            \"\u0e2a\u0e32\u0e21\",\n            \"\u0e2a\u0e35\u0e48\",\n            \"\u0e2b\u0e49\u0e32\",\n            \"\u0e2b\u0e01\",\n            \"\u0e40\u0e08\u0e47\u0e14\",\n            \"\u0e41\u0e1b\u0e14\",\n            \"\u0e40\u0e01\u0e49\u0e32\",\n        ]\n\n        if number > 1000000:\n            ret += num_to_thaiword(int(number / 1000000)) + \"\u0e25\u0e49\u0e32\u0e19\"\n            number = int(math.fmod(number, 1000000))\n        divider = 100000\n\n        pos = 0\n        while number > 0:\n            d = int(number / divider)\n\n            if (divider == 10) and (d == 2):\n                ret += \"\u0e22\u0e35\u0e48\"\n            elif (divider == 10) and (d == 1):\n                ret += \"\"\n            elif (divider == 1) and (d == 1) and (ret != \"\"):\n                ret += \"\u0e40\u0e2d\u0e47\u0e14\"\n            else:\n                ret += _NUM_CALL[d]\n\n            if d:\n                ret += _POS_CALL[pos]\n            else:\n                ret += \"\"\n\n            number = number % divider\n            divider = divider / 10\n            pos += 1\n\n    return ret", "response": "returns a string that indicates the full amount in word form"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a Thai number spellout word to a number value.", "response": "def thaiword_to_num(word: str) -> int:\n    \"\"\"\n    Converts a Thai number spellout word to actual number value\n\n    :param str word: a Thai number spellout\n    :return: number\n    \"\"\"\n    if not word:\n        return None\n\n    tokens = []\n    if isinstance(word, str):\n        tokens = _TOKENIZER.word_tokenize(word)\n    elif isinstance(word, Iterable):\n        for w in word:\n            tokens.extend(_TOKENIZER.word_tokenize(w))\n\n    res = []\n    for tok in tokens:\n        if tok in _THAIWORD_NUMS_UNITS:\n            res.append(tok)\n        else:\n            m = _NU_PAT.fullmatch(tok)\n            if m:\n                res.extend([t for t in m.groups() if t])  # \u0e15\u0e31\u0e14 None \u0e17\u0e34\u0e49\u0e07\n            else:\n                pass  # should not be here\n\n    return _thaiword_to_num(res)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npart of Speech tagging function.", "response": "def pos_tag(\n    words: List[str], engine: str = \"perceptron\", corpus: str = \"orchid\"\n) -> List[Tuple[str, str]]:\n    \"\"\"\n    Part of Speech tagging function.\n\n    :param list words: a list of tokenized words\n    :param str engine:\n        * unigram - unigram tagger\n        * perceptron - perceptron tagger (default)\n        * artagger - RDR POS tagger\n    :param str corpus:\n        * orchid - annotated Thai academic articles (default)\n        * orchid_ud - annotated Thai academic articles using Universal Dependencies Tags\n        * pud - Parallel Universal Dependencies (PUD) treebanks\n    :return: returns a list of labels regarding which part of speech it is\n    \"\"\"\n    _corpus = corpus\n    _tag = []\n    if corpus == \"orchid_ud\":\n        corpus = \"orchid\"\n    if not words:\n        return []\n\n    if engine == \"perceptron\":\n        from .perceptron import tag as tag_\n    elif engine == \"artagger\":\n        tag_ = _artagger_tag\n    else:  # default, use \"unigram\" (\"old\") engine\n        from .unigram import tag as tag_\n    _tag = tag_(words, corpus=corpus)\n\n    if _corpus == \"orchid_ud\":\n        _tag = _orchid_to_ud(_tag)\n\n    return _tag"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pos_tag_sents(\n    sentences: List[List[str]], engine: str = \"perceptron\", corpus: str = \"orchid\"\n) -> List[List[Tuple[str, str]]]:\n    \"\"\"\n    Part of Speech tagging Sentence function.\n\n    :param list sentences: a list of lists of tokenized words\n    :param str engine:\n        * unigram - unigram tagger\n        * perceptron - perceptron tagger (default)\n        * artagger - RDR POS tagger\n    :param str corpus:\n        * orchid - annotated Thai academic articles (default)\n        * orchid_ud - annotated Thai academic articles using Universal Dependencies Tags\n        * pud - Parallel Universal Dependencies (PUD) treebanks\n    :return: returns a list of labels regarding which part of speech it is\n    \"\"\"\n    if not sentences:\n        return []\n\n    return [pos_tag(sent, engine=engine, corpus=corpus) for sent in sentences]", "response": "Part of Speech tagging Sentence function."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a word is an authentic Thai word", "response": "def thaicheck(word: str) -> bool:\n    \"\"\"\n    Check if a word is an \"authentic Thai word\"\n\n    :param str word: word\n    :return: True or False\n    \"\"\"\n    pattern = re.compile(r\"[\u0e01-\u0e2c\u0e2e]\", re.U)  # \u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30\n    res = re.findall(pattern, word)  # \u0e14\u0e36\u0e07\u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30\u0e17\u0e31\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e2d\u0e2d\u0e01\u0e21\u0e32\n\n    if res == []:\n        return False\n\n    if _check1(res[len(res) - 1]) or len(res) == 1:\n        if _check2(word):\n            word2 = list(word)\n            i = 0\n            thai = True\n            if word in [\n                \"\u0e06\u0e48\u0e32\",\n                \"\u0e40\u0e06\u0e35\u0e48\u0e22\u0e19\",\n                \"\u0e28\u0e36\u0e01\",\n                \"\u0e28\u0e2d\u0e01\",\n                \"\u0e40\u0e28\u0e34\u0e01\",\n                \"\u0e40\u0e28\u0e23\u0e49\u0e32\",\n                \"\u0e18\",\n                \"\u0e13\",\n                \"\u0e2f\u0e1e\u0e13\u0e2f\",\n                \"\u0e43\u0e2b\u0e0d\u0e48\",\n                \"\u0e2b\u0e0d\u0e49\u0e32\",\n                \"\u0e04\u0e27\u0e32\u0e22\",\n                \"\u0e04\u0e27\u0e32\u0e21\",\n                \"\u0e01\u0e23\u0e34\u0e48\u0e07\u0e40\u0e01\u0e23\u0e07\",\n                \"\u0e1c\u0e25\u0e34\",\n            ]:  # \u0e02\u0e49\u0e2d\u0e22\u0e01\u0e40\u0e27\u0e49\u0e19 \u0e04\u0e33\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e44\u0e17\u0e22\u0e41\u0e17\u0e49\n                return True\n\n            while i < len(word2) and thai:\n                thai = _check3(word2[i])\n                if not thai:\n                    return False\n                i += 1\n            return True\n\n        return False\n\n    if word in [\"\u0e01\u0e30\", \"\u0e01\u0e23\u0e30\", \"\u0e1b\u0e30\", \"\u0e1b\u0e23\u0e30\"]:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _romanize(word: str) -> str:\n    if not isinstance(word, str) or not word:\n        return \"\"\n\n    word = _replace_vowels(_normalize(word))\n    res = _RE_CONSONANT.findall(word)\n\n    # 2-character word, all consonants\n    if len(word) == 2 and len(res) == 2:\n        word = list(word)\n        word.insert(1, \"o\")\n        word = \"\".join(word)\n\n    word = _replace_consonants(word, res)\n\n    return word", "response": "Romanize a Thai word."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders Thai words in the Latin alphabet or \"romanization\", using the Royal Thai General System of Transcription (RTGS), which is the official system published by the Royal Institute of Thailand. \u0e16\u0e2d\u0e14\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e25\u0e30\u0e15\u0e34\u0e19 :param str text: Thai text to be romanized :return: A string of Thai words rendered in the Latin alphabet.", "response": "def romanize(text: str) -> str:\n    \"\"\"\n    Rendering Thai words in the Latin alphabet or \"romanization\",\n    using the Royal Thai General System of Transcription (RTGS),\n    which is the official system published by the Royal Institute of Thailand.\n    \u0e16\u0e2d\u0e14\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e25\u0e30\u0e15\u0e34\u0e19\n    :param str text: Thai text to be romanized\n    :return: A string of Thai words rendered in the Latin alphabet.\n    \"\"\"\n    words = word_tokenize(text)\n    romanized_words = [_romanize(word) for word in words]\n\n    return \"\".join(romanized_words)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning full path where PyThaiNLP keeps its ( downloaded ) data", "response": "def get_pythainlp_data_path() -> str:\n    \"\"\"\n    Return full path where PyThaiNLP keeps its (downloaded) data\n    \"\"\"\n    path = os.path.join(os.path.expanduser(\"~\"), PYTHAINLP_DATA_DIR)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef segment(text: str, custom_dict: Trie = None) -> List[str]:\n    if not text or not isinstance(text, str):\n        return []\n\n    if not custom_dict:\n        custom_dict = DEFAULT_DICT_TRIE\n\n    return LongestMatchTokenizer(custom_dict).tokenize(text)", "response": "Longest matching of text."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders Thai words in the Latin alphabet or \"romanization\", using the Royal Thai General System of Transcription (RTGS), which is the official system published by the Royal Institute of Thailand. \u0e16\u0e2d\u0e14\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e25\u0e30\u0e15\u0e34\u0e19 :param str text: Thai text to be romanized :param str engine: 'royin' (default) or 'thai2rom'. 'royin' uses the Royal Thai General System of Transcription issued by Royal Institute of Thailand. 'thai2rom' is deep learning Thai romanization (require keras). :return: A string of Thai words rendered in the Latin alphabet.", "response": "def romanize(text: str, engine: str = \"royin\") -> str:\n    \"\"\"\n    Rendering Thai words in the Latin alphabet or \"romanization\",\n    using the Royal Thai General System of Transcription (RTGS),\n    which is the official system published by the Royal Institute of Thailand.\n    \u0e16\u0e2d\u0e14\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e25\u0e30\u0e15\u0e34\u0e19\n    :param str text: Thai text to be romanized\n    :param str engine: 'royin' (default) or 'thai2rom'. 'royin' uses the Royal Thai General System of Transcription issued by Royal Institute of Thailand. 'thai2rom' is deep learning Thai romanization (require keras).\n    :return: A string of Thai words rendered in the Latin alphabet.\n    \"\"\"\n\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    if engine == \"thai2rom\":\n        from .thai2rom import romanize\n    else:  # use default engine \"royin\"\n        from .royin import romanize\n\n    return romanize(text)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transliterate(text: str, engine: str = \"ipa\") -> str:\n\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    if engine == \"icu\" or engine == \"pyicu\":\n        from .pyicu import transliterate\n    else:\n        from .ipa import transliterate\n\n    return transliterate(text)", "response": "Transliterate Thai text into a new language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkeeping only Thai words with at least min_freq frequency and has length between min_len and max_len characters Return True if all words are in the dict False otherwise", "response": "def _keep(\n    word_freq: int,\n    min_freq: int,\n    min_len: int,\n    max_len: int,\n    dict_filter: Callable[[str], bool],\n):\n    \"\"\"\n    Keep only Thai words with at least min_freq frequency\n    and has length between min_len and max_len characters\n    \"\"\"\n    if not word_freq or word_freq[1] < min_freq:\n        return False\n\n    word = word_freq[0]\n    if not word or len(word) < min_len or len(word) > max_len or word[0] == \".\":\n        return False\n\n    return dict_filter(word)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _edits1(word: str) -> Set[str]:\n    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    deletes = [L + R[1:] for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n    replaces = [L + c + R[1:] for L, R in splits if R for c in thai_letters]\n    inserts = [L + c + R for L, R in splits for c in thai_letters]\n\n    return set(deletes + transposes + replaces + inserts)", "response": "Return a set of words with edit distance of 1 from the input word\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a set of words with edit distance of 2 from the input word", "response": "def _edits2(word: str) -> Set[str]:\n    \"\"\"\n    Return a set of words with edit distance of 2 from the input word\n    \"\"\"\n    return set(e2 for e1 in _edits1(word) for e2 in _edits1(e1))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef known(self, words: List[str]) -> List[str]:\n        return list(w for w in words if w in self.__WORDS)", "response": "Returns a list of given words that are found in the spelling dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the probability of an input word according to the spelling dictionary.", "response": "def prob(self, word: str) -> float:\n        \"\"\"\n        Return probability of an input word, according to the spelling dictionary\n\n        :param str word: A word to check its probability of occurrence\n        \"\"\"\n        return self.__WORDS[word] / self.__WORDS_TOTAL"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spell(self, word: str) -> List[str]:\n        if not word:\n            return \"\"\n\n        candidates = (\n            self.known([word])\n            or self.known(_edits1(word))\n            or self.known(_edits2(word))\n            or [word]\n        )\n        candidates.sort(key=self.freq, reverse=True)\n\n        return candidates", "response": "Return a list of possible words according to edit distance of 1 and 2 and return the list of possible words sorted by frequency of word occurrance in the spelling dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntagging a list of words into a list of tuples.", "response": "def tag(words: List[str], corpus: str) -> List[Tuple[str, str]]:\n    \"\"\"\n    \u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''list'' \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''list'' \u0e40\u0e0a\u0e48\u0e19 [('\u0e04\u0e33', '\u0e0a\u0e19\u0e34\u0e14\u0e04\u0e33'), ('\u0e04\u0e33', '\u0e0a\u0e19\u0e34\u0e14\u0e04\u0e33'), ...]\n    \"\"\"\n    if not words:\n        return []\n\n    if corpus == \"orchid\":\n        tagger = nltk.tag.UnigramTagger(model=_orchid_tagger())\n        i = 0\n        while i < len(words):\n            if words[i] == \" \":\n                words[i] = \"<space>\"\n            elif words[i] == \"+\":\n                words[i] = \"<plus>\"\n            elif words[i] == \"-\":\n                words[i] = \"<minus>\"\n            elif words[i] == \"=\":\n                words[i] = \"<equal>\"\n            elif words[i] == \",\":\n                words[i] = \"<comma>\"\n            elif words[i] == \"$\":\n                words[i] = \"<dollar>\"\n            elif words[i] == \".\":\n                words[i] = \"<full_stop>\"\n            elif words[i] == \"(\":\n                words[i] = \"<left_parenthesis>\"\n            elif words[i] == \")\":\n                words[i] = \"<right_parenthesis>\"\n            elif words[i] == '\"':\n                words[i] = \"<quotation>\"\n            elif words[i] == \"@\":\n                words[i] = \"<at_mark>\"\n            elif words[i] == \"&\":\n                words[i] = \"<ampersand>\"\n            elif words[i] == \"{\":\n                words[i] = \"<left_curly_bracket>\"\n            elif words[i] == \"^\":\n                words[i] = \"<circumflex_accent>\"\n            elif words[i] == \"?\":\n                words[i] = \"<question_mark>\"\n            elif words[i] == \"<\":\n                words[i] = \"<less_than>\"\n            elif words[i] == \">\":\n                words[i] = \"<greater_than>\"\n            elif words[i] == \"=\":\n                words[i] = \"<equal>\"\n            elif words[i] == \"!\":\n                words[i] = \"<exclamation>\"\n            elif words[i] == \"\u2019\":\n                words[i] = \"<apostrophe>\"\n            elif words[i] == \":\":\n                words[i] = \"<colon>\"\n            elif words[i] == \"*\":\n                words[i] = \"<asterisk>\"\n            elif words[i] == \";\":\n                words[i] = \"<semi_colon>\"\n            elif words[i] == \"/\":\n                words[i] = \"<slash>\"\n            i += 1\n        t = tagger.tag(words)\n        temp = []\n        i = 0\n        while i < len(t):\n            word = t[i][0]\n            tag = t[i][1]\n            if word == \"<space>\":\n                word = \" \"\n            elif word == \"<plus>\":\n                word = \"+\"\n            elif word == \"<minus>\":\n                word = \"-\"\n            elif word == \"<equal>\":\n                word = \"=\"\n            elif word == \"<comma>\":\n                word = \",\"\n            elif word == \"<dollar>\":\n                word = \"$\"\n            elif word == \"<full_stop>\":\n                word = \".\"\n            elif word == \"<left_parenthesis>\":\n                word = \"(\"\n            elif word == \"<right_parenthesis>\":\n                word = \")\"\n            elif word == \"<quotation>\":\n                word = '\"'\n            elif word == \"<at_mark>\":\n                word = \"@\"\n            elif word == \"<ampersand>\":\n                word = \"&\"\n            elif word == \"<left_curly_bracket>\":\n                word = \"{\"\n            elif word == \"<circumflex_accent>\":\n                word = \"^\"\n            elif word == \"<question_mark>\":\n                word = \"?\"\n            elif word == \"<less_than>\":\n                word = \"<\"\n            elif word == \"<greater_than>\":\n                word = \">\"\n            elif word == \"<equal>\":\n                word = \"=\"\n            elif word == \"<exclamation>\":\n                word = \"!\"\n            elif word == \"<apostrophe>\":\n                word = \"\u2019\"\n            elif word == \"<colon>\":\n                word = \":\"\n            elif word == \"<asterisk>\":\n                word = \"*\"\n            elif word == \"<semi_colon>\":\n                word = \";\"\n            elif word == \"<slash>\":\n                word = \"/\"\n            temp.append((word, tag))\n            i += 1\n        t = temp\n    else:\n        tagger = _pud_tagger()\n        t = tagger.tag(words)\n\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tcc(text: str) -> str:\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    p = 0\n    while p < len(text):\n        m = PAT_TCC.match(text[p:])\n        if m:\n            n = m.span()[1]\n        else:\n            n = 1\n        yield text[p : p + n]\n        p += n", "response": "TCC generator generates Thai Character Clusters\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tcc_pos(text: str) -> Set[int]:\n    if not text or not isinstance(text, str):\n        return set()\n\n    p_set = set()\n    p = 0\n    for w in tcc(text):\n        p += len(w)\n        p_set.add(p)\n\n    return p_set", "response": "returns a set of the positions of the character clusters in the text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef word_tokenize(\n    text: str,\n    custom_dict: Trie = None,\n    engine: str = \"newmm\",\n    keep_whitespace: bool = True,\n) -> List[str]:\n    \"\"\"\n    :param str text: text to be tokenized\n    :param str engine: tokenizer to be used\n    :param dict custom_dict: a dictionary trie\n    :param bool keep_whitespace: True to keep whitespaces, a common mark for end of phrase in Thai\n    :return: list of words\n\n    **Options for engine**\n        * newmm (default) - dictionary-based, Maximum Matching + Thai Character Cluster\n        * longest - dictionary-based, Longest Matching\n        * deepcut - wrapper for deepcut, language-model-based https://github.com/rkcosmos/deepcut\n        * icu - wrapper for ICU (International Components for Unicode, using PyICU), dictionary-based\n        * ulmfit - for thai2fit\n        * a custom_dict can be provided for newmm, longest, and deepcut\n\n    **Example**\n        >>> from pythainlp.tokenize import word_tokenize\n        >>> text = \"\u0e42\u0e2d\u0e40\u0e04\u0e1a\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e23\u0e31\u0e01\u0e20\u0e32\u0e29\u0e32\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14\"\n        >>> word_tokenize(text, engine=\"newmm\")\n        ['\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\n        >>> word_tokenize(text, engine=\"icu\")\n        ['\u0e42\u0e2d', '\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01', '\u0e40\u0e23\u0e32', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19', '\u0e40\u0e01\u0e34\u0e14']\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return []\n\n    segments = []\n    if engine == \"newmm\" or engine == \"onecut\":\n        from .newmm import segment\n\n        segments = segment(text, custom_dict)\n    elif engine == \"longest\":\n        from .longest import segment\n\n        segments = segment(text, custom_dict)\n    elif engine == \"mm\" or engine == \"multi_cut\":\n        from .multi_cut import segment\n\n        segments = segment(text, custom_dict)\n    elif engine == \"deepcut\":  # deepcut can optionally use dictionary\n        from .deepcut import segment\n\n        if custom_dict:\n            custom_dict = list(custom_dict)\n            segments = segment(text, custom_dict)\n        else:\n            segments = segment(text)\n    elif engine == \"ulmfit\":  # ulmfit has its own specific dictionary\n        from .newmm import segment\n\n        segments = segment(text, custom_dict=FROZEN_DICT_TRIE)\n    elif engine == \"icu\":\n        from .pyicu import segment\n\n        segments = segment(text)\n    else:  # default, use \"newmm\" engine\n        from .newmm import segment\n\n        segments = segment(text, custom_dict)\n\n    if not keep_whitespace:\n        segments = [token.strip(\" \") for token in segments if token.strip(\" \")]\n\n    return segments", "response": "Tokenize a text into a list of words."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_word_tokenize(\n    text: str,\n    custom_dict: Trie = DEFAULT_DICT_TRIE,\n    engine: str = \"newmm\",\n    keep_whitespace: bool = True,\n) -> List[str]:\n    \"\"\"\n    :meth: DEPRECATED: Please use `word_tokenize()` with a `custom_dict` argument instead\n    :param str text: text to be tokenized\n    :param dict custom_dict: a dictionary trie, or an iterable of words, or a string of dictionary path\n    :param str engine: choose between different options of engine to token (newmm [default], mm, longest, and deepcut)\n    :param bool keep_whitespace: True to keep whitespaces, a common mark for end of phrase in Thai\n    :return: list of words\n    \"\"\"\n    print(\n        \"Deprecated. Use word_tokenize() with a custom_dict argument instead.\",\n        file=sys.stderr,\n    )\n    return word_tokenize(\n        text=text,\n        custom_dict=custom_dict,\n        engine=engine,\n        keep_whitespace=keep_whitespace,\n    )", "response": "This function uses word_tokenize to tokenize a string of text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subword_tokenize(text: str, engine: str = \"tcc\") -> List[str]:\n    if not text or not isinstance(text, str):\n        return []\n\n    if engine == \"etcc\":\n        from .etcc import segment\n    else:  # default\n        from .tcc import segment\n\n    return segment(text)", "response": "Tokenize a text into a list of subwords."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a dict trie which will be used for word_tokenize function.", "response": "def dict_trie(dict_source: Union[str, Iterable[str], Trie]) -> Trie:\n    \"\"\"\n    Create a dict trie which will be used for word_tokenize() function.\n    For more information on the trie data structure,\n    see: https://marisa-trie.readthedocs.io/en/latest/index.html\n\n    :param string/list dict_source: a list of vocaburaries or a path to source file\n    :return: a trie created from a dictionary input\n    \"\"\"\n    trie = None\n\n    if isinstance(dict_source, Trie):\n        trie = dict_source\n    elif isinstance(dict_source, str):\n        # Receive a file path of the dict to read\n        with open(dict_source, \"r\", encoding=\"utf8\") as f:\n            _vocabs = f.read().splitlines()\n            trie = Trie(_vocabs)\n    elif isinstance(dict_source, Iterable):\n        # Note: Trie and str are both Iterable, Iterable check should be here\n        # Received a sequence type object of vocabs\n        trie = Trie(dict_source)\n    else:\n        raise TypeError(\n            \"Type of dict_source must be marisa_trie.Trie, or Iterable[str], or str (path to source file)\"\n        )\n\n    return trie"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef metasound(text: str, length: int = 4) -> str:\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    # keep only consonants and thanthakhat\n    chars = []\n    for ch in text:\n        if ch in _CONS_THANTHAKHAT:\n            chars.append(ch)\n\n    # remove karan (thanthakhat and a consonant before it)\n    i = 0\n    while i < len(chars):\n        if chars[i] == _THANTHAKHAT:\n            if i > 0:\n                chars[i - 1] = \" \"\n            chars[i] = \" \"\n        i += 1\n\n    # retain first consonant, encode the rest\n    chars = chars[:length]\n    i = 1\n    while i < len(chars):\n        if chars[i] in _C1:\n            chars[i] = \"1\"\n        elif chars[i] in _C2:\n            chars[i] = \"2\"\n        elif chars[i] in _C3:\n            chars[i] = \"3\"\n        elif chars[i] in _C4:\n            chars[i] = \"4\"\n        elif chars[i] in _C5:\n            chars[i] = \"5\"\n        elif chars[i] in _C6:\n            chars[i] = \"6\"\n        elif chars[i] in _C7:\n            chars[i] = \"7\"\n        elif chars[i] in _C8:\n            chars[i] = \"8\"\n        else:\n            chars[i] = \"0\"\n        i += 1\n\n    while len(chars) < length:\n        chars.append(\"0\")\n\n    return \"\".join(chars)", "response": "Return the MetaSound for the given text."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntags the list of words with the Thailand provinces in text.", "response": "def tag_provinces(tokens: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"\n    Recognize Thailand provinces in text\n\n    Input is a list of words\n    Return a list of tuples\n\n    Example::\n     >>> text = ['\u0e2b\u0e19\u0e2d\u0e07\u0e04\u0e32\u0e22', '\u0e19\u0e48\u0e32\u0e2d\u0e22\u0e39\u0e48']\n     >>> tag_provinces(text)\n     [('\u0e2b\u0e19\u0e2d\u0e07\u0e04\u0e32\u0e22', 'B-LOCATION'), ('\u0e19\u0e48\u0e32\u0e2d\u0e22\u0e39\u0e48', 'O')]\n    \"\"\"\n    province_list = provinces()\n\n    output = []\n    for token in tokens:\n        if token in province_list:\n            output.append((token, \"B-LOCATION\"))\n        else:\n            output.append((token, \"O\"))\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ner(\n        self, text: str, pos: bool = True\n    ) -> Union[List[Tuple[str, str]], List[Tuple[str, str, str]]]:\n        \"\"\"\n        Get named-entities in text\n\n        :param string text: Thai text\n        :param boolean pos: get Part-Of-Speech tag (True) or get not (False)\n\n        :return: list of strings with name labels (and part-of-speech tags)\n\n        **Example**::\n            >>> from pythainlp.tag.named_entity import ThaiNameTagger\n            >>> ner = ThaiNameTagger()\n            >>> ner.get_ner(\"\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48 15 \u0e01.\u0e22. 61 \u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e23\u0e30\u0e1a\u0e1a\u0e40\u0e27\u0e25\u0e32 14:49 \u0e19.\")\n            [('\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48', 'NOUN', 'O'), (' ', 'PUNCT', 'O'), ('15', 'NUM', 'B-DATE'),\n            (' ', 'PUNCT', 'I-DATE'), ('\u0e01.\u0e22.', 'NOUN', 'I-DATE'),\n            (' ', 'PUNCT', 'I-DATE'), ('61', 'NUM', 'I-DATE'),\n            (' ', 'PUNCT', 'O'), ('\u0e17\u0e14\u0e2a\u0e2d\u0e1a', 'VERB', 'O'),\n            ('\u0e23\u0e30\u0e1a\u0e1a', 'NOUN', 'O'), ('\u0e40\u0e27\u0e25\u0e32', 'NOUN', 'O'), (' ', 'PUNCT', 'O'),\n            ('14', 'NOUN', 'B-TIME'), (':', 'PUNCT', 'I-TIME'), ('49', 'NUM', 'I-TIME'),\n            (' ', 'PUNCT', 'I-TIME'), ('\u0e19.', 'NOUN', 'I-TIME')]\n            >>> ner.get_ner(\"\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48 15 \u0e01.\u0e22. 61 \u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e23\u0e30\u0e1a\u0e1a\u0e40\u0e27\u0e25\u0e32 14:49 \u0e19.\", pos=False)\n            [('\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48', 'O'), (' ', 'O'), ('15', 'B-DATE'), (' ', 'I-DATE'),\n            ('\u0e01.\u0e22.', 'I-DATE'), (' ', 'I-DATE'), ('61', 'I-DATE'), (' ', 'O'),\n            ('\u0e17\u0e14\u0e2a\u0e2d\u0e1a', 'O'), ('\u0e23\u0e30\u0e1a\u0e1a', 'O'), ('\u0e40\u0e27\u0e25\u0e32', 'O'), (' ', 'O'), ('14', 'B-TIME'),\n            (':', 'I-TIME'), ('49', 'I-TIME'), (' ', 'I-TIME'), ('\u0e19.', 'I-TIME')]\n        \"\"\"\n        self.__tokens = word_tokenize(text, engine=_WORD_TOKENIZER)\n        self.__pos_tags = pos_tag(\n            self.__tokens, engine=\"perceptron\", corpus=\"orchid_ud\"\n        )\n        self.__x_test = self.__extract_features(self.__pos_tags)\n        self.__y = self.crf.predict_single(self.__x_test)\n\n        if pos:\n            return [\n                (self.__pos_tags[i][0], self.__pos_tags[i][1], data)\n                for i, data in enumerate(self.__y)\n            ]\n\n        return [(self.__pos_tags[i][0], data) for i, data in enumerate(self.__y)]", "response": "Get named - entities in text and return a list of tuples with name labels and part - of - speech tags."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deletetone(text: str) -> str:\n    chars = [ch for ch in text if ch not in thai_tonemarks]\n    return \"\".join(chars)", "response": "Removes tonemarks from a thai text"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the soundex code for the given text.", "response": "def soundex(text: str, engine: str = \"udom83\") -> str:\n    \"\"\"\n    Thai Soundex\n\n    :param string text: word\n    :param str engine: soundex engine\n    :Parameters for engine:\n        * udom83 (default)\n        * lk82\n        * metasound\n    :return: soundex code\n    \"\"\"\n    if engine == \"lk82\":\n        _soundex = lk82\n    elif engine == \"metasound\":\n        _soundex = metasound\n    else:  # default, use \"udom83\"\n        _soundex = udom83\n\n    return _soundex(text)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef segment(text: str, custom_dict: Trie = None) -> List[str]:\n    if not text or not isinstance(text, str):\n        return []\n\n    return list(_multicut(text, custom_dict=custom_dict))", "response": "A helper function to segment a text into a list of words."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_all_segment(text: str, custom_dict: Trie = None) -> List[str]:\n    if not text or not isinstance(text, str):\n        return []\n\n    ww = list(_multicut(text, custom_dict=custom_dict))\n\n    return list(_combine(ww))", "response": "Get all possible segment variations of a given text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string representation of the given format character.", "response": "def _thai_strftime(datetime: datetime.datetime, fmt_char: str) -> str:\n    \"\"\"\n    Conversion support for thai_strftime()\n    \"\"\"\n    str_ = \"\"\n    if fmt_char == \"A\":\n        # National representation of the full weekday name\n        str_ = thai_full_weekdays[datetime.weekday()]\n    elif fmt_char == \"a\":\n        # National representation of the abbreviated weekday\n        str_ = thai_abbr_weekdays[datetime.weekday()]\n    elif fmt_char == \"B\":\n        # National representation of the full month name\n        str_ = thai_full_months[datetime.month - 1]\n    elif fmt_char == \"b\":\n        # National representation of the abbreviated month name\n        str_ = thai_abbr_months[datetime.month - 1]\n    elif fmt_char == \"C\":\n        # Thai Buddhist century (AD+543)/100 + 1 as decimal number;\n        str_ = str(int((datetime.year + 543) / 100) + 1)\n    elif fmt_char == \"c\":\n        # Locale\u2019s appropriate date and time representation\n        # Wed  6 Oct 01:40:00 1976\n        # \u0e1e   6 \u0e15.\u0e04. 01:40:00 2519  <-- left-aligned weekday, right-aligned day\n        str_ = \"{:<2} {:>2} {} {} {}\".format(\n            thai_abbr_weekdays[datetime.weekday()],\n            datetime.day,\n            thai_abbr_months[datetime.month - 1],\n            datetime.strftime(\"%H:%M:%S\"),\n            datetime.year + 543,\n        )\n    elif fmt_char == \"D\":\n        # Equivalent to ``%m/%d/%y''\n        str_ = \"{}/{}\".format(datetime.strftime(\"%m/%d\"), str(datetime.year + 543)[-2:])\n    elif fmt_char == \"F\":\n        # Equivalent to ``%Y-%m-%d''\n        str_ = \"{}-{}\".format(str(datetime.year + 543), datetime.strftime(\"%m-%d\"))\n    elif fmt_char == \"G\":\n        # ISO 8601 year with century representing the year that contains the greater part of the ISO week (%V). Monday as the first day of the week.\n        str_ = str(int(datetime.strftime(\"%G\")) + 543)\n    elif fmt_char == \"g\":\n        # Same year as in ``%G'', but as a decimal number without century (00-99).\n        str_ = str(int(datetime.strftime(\"%G\")) + 543)[-2:]\n    elif fmt_char == \"v\":\n        # BSD extension, ' 6-Oct-1976'\n        str_ = \"{:>2}-{}-{}\".format(\n            datetime.day, thai_abbr_months[datetime.month - 1], datetime.year + 543\n        )\n    elif fmt_char == \"X\":\n        # Locale\u2019s appropriate time representation.\n        str_ = datetime.strftime(\"%H:%M:%S\")\n    elif fmt_char == \"x\":\n        # Locale\u2019s appropriate date representation.\n        str_ = \"{}/{}/{}\".format(\n            _padding(datetime.day), _padding(datetime.month), datetime.year + 543\n        )\n    elif fmt_char == \"Y\":\n        # Year with century\n        str_ = str(datetime.year + 543)\n    elif fmt_char == \"y\":\n        # Year without century\n        str_ = str(datetime.year + 543)[2:4]\n    elif fmt_char == \"+\":\n        # National representation of the date and time (the format is similar to that produced by date(1))\n        # Wed  6 Oct 1976 01:40:00\n        str_ = \"{:<2} {:>2} {} {} {}\".format(\n            thai_abbr_weekdays[datetime.weekday()],\n            datetime.day,\n            thai_abbr_months[datetime.month - 1],\n            datetime.year + 543,\n            datetime.strftime(\"%H:%M:%S\"),\n        )\n    else:\n        # No known localization available, use Python's default\n        str_ = datetime.strftime(f\"%{fmt_char}\")\n\n    return str_"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef thai_strftime(\n    datetime: datetime.datetime, fmt: str, thaidigit: bool = False\n) -> str:\n    \"\"\"\n    Thai date and time string formatter\n    Formatting directives similar to datetime.strftime()\n\n    Will use Thai names and Thai Buddhist Era for these directives:\n    - %a abbreviated weekday name\n    - %A full weekday name\n    - %b abbreviated month name\n    - %B full month name\n    - %y year without century\n    - %Y year with century\n    - %c date and time representation\n    - %v short date representation (undocumented)\n\n    Other directives will be passed to datetime.strftime()\n\n    Note 1:\n    The Thai Buddhist Era (BE) year is simply converted from AD by adding 543.\n    This is certainly not accurate for years before 1941 AD,\n    due to the change in Thai New Year's Day.\n\n    Note 2:\n    This meant to be an interrim solution, since Python standard's locale module\n    (which relied on C's strftime()) does not support \"th\" or \"th_TH\" locale yet.\n    If supported, we can just locale.setlocale(locale.LC_TIME, \"th_TH\") and\n    then use native datetime.strftime().\n\n    Note 3:\n    We trying to make this platform-independent and support extentions as many as possible,\n    See these links for strftime() extensions in POSIX, BSD, and GNU libc:\n    - Python https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior\n    - C http://www.cplusplus.com/reference/ctime/strftime/\n    - GNU https://metacpan.org/pod/POSIX::strftime::GNU\n    - Linux https://linux.die.net/man/3/strftime\n    - OpenBSD https://man.openbsd.org/strftime.3\n    - FreeBSD https://www.unix.com/man-page/FreeBSD/3/strftime/\n    - macOS https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/strftime.3.html\n    - PHP https://secure.php.net/manual/en/function.strftime.php\n    - JavaScript's implementation https://github.com/samsonjs/strftime\n    - strftime() quick reference http://www.strftime.net/\n\n    :return: Date and time spelled out in text, with month in Thai name and year in Thai Buddhist era. The year is simply converted from AD by adding 543 (will not accurate for years before 1941 AD, due to change in Thai New Year's Day).\n    \"\"\"\n    thaidate_parts = []\n\n    i = 0\n    fmt_len = len(fmt)\n    while i < fmt_len:\n        str_ = \"\"\n        if fmt[i] == \"%\":\n            j = i + 1\n            if j < fmt_len:\n                fmt_char = fmt[j]\n                if fmt_char in _NEED_L10N:  # requires localization?\n                    str_ = _thai_strftime(datetime, fmt_char)\n                elif fmt_char in _EXTENSIONS:\n\n                    if fmt_char == \"-\":\n                        # GNU libc extension, no padding\n                        k = j + 1\n                        if k < fmt_len:\n                            fmt_char_nopad = fmt[k]\n                            if (\n                                fmt_char_nopad in _NEED_L10N\n                            ):  # check if requires localization\n                                str_ = _thai_strftime(datetime, fmt_char_nopad)\n                            else:\n                                str_ = datetime.strftime(f\"%-{fmt_char_nopad}\")\n                            i = i + 1  # consume char after \"-\"\n                        else:\n                            str_ = \"-\"  # \"-\" at the end of string has no meaning\n                    elif fmt_char == \"_\":\n                        # GNU libc extension, explicitly specify space (\" \") for padding\n                        # Not implemented yet\n                        pass\n                    elif fmt_char == \"0\":\n                        # GNU libc extension, explicitly specify zero (\"0\") for padding\n                        # Not implemented yet\n                        pass\n                    elif fmt_char == \"E\":\n                        # POSIX extension, uses the locale's alternative representation\n                        # Not implemented yet\n                        pass\n                    elif fmt_char == \"O\":\n                        # POSIX extension, uses the locale's alternative numeric symbols\n                        # Not implemented yet\n                        pass\n\n                elif fmt_char:\n                    # the rest of directives, just pass to Python's standard strftime()\n                    str_ = datetime.strftime(f\"%{fmt_char}\")\n\n                i = i + 1  # consume char after \"%\"\n            else:\n                str_ = \"%\"\n        else:\n            str_ = fmt[i]\n\n        thaidate_parts.append(str_)\n        i = i + 1\n\n    thaidate_text = \"\".join(thaidate_parts)\n\n    if thaidigit:\n        thaidate_text = thaidate_text.translate(_HA_TH_DIGITS)\n\n    return thaidate_text", "response": "A naive strftime function that uses Thai Buddhist Era and the given format string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert reign year to ad", "response": "def reign_year_to_ad(reign_year: int, reign: int) -> int:\n    \"\"\"\n    Reign year of Chakri dynasty, Thailand\n    \"\"\"\n    if int(reign) == 10:\n        ad = int(reign_year) + 2015\n    elif int(reign) == 9:\n        ad = int(reign_year) + 1945\n    elif int(reign) == 8:\n        ad = int(reign_year) + 1928\n    elif int(reign) == 7:\n        ad = int(reign_year) + 1924\n    return ad"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntagging a list of words into a list of tuples.", "response": "def tag(words: List[str], corpus: str = \"pud\") -> List[Tuple[str, str]]:\n    \"\"\"\n    \u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''list'' \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''list'' \u0e40\u0e0a\u0e48\u0e19 [('\u0e04\u0e33', '\u0e0a\u0e19\u0e34\u0e14\u0e04\u0e33'), ('\u0e04\u0e33', '\u0e0a\u0e19\u0e34\u0e14\u0e04\u0e33'), ...]\n    \"\"\"\n    if not words:\n        return []\n\n    if corpus == \"orchid\":\n        tagger = _ORCHID_TAGGER\n        i = 0\n        while i < len(words):\n            if words[i] == \" \":\n                words[i] = \"<space>\"\n            elif words[i] == \"+\":\n                words[i] = \"<plus>\"\n            elif words[i] == \"-\":\n                words[i] = \"<minus>\"\n            elif words[i] == \"=\":\n                words[i] = \"<equal>\"\n            elif words[i] == \",\":\n                words[i] = \"<comma>\"\n            elif words[i] == \"$\":\n                words[i] = \"<dollar>\"\n            elif words[i] == \".\":\n                words[i] = \"<full_stop>\"\n            elif words[i] == \"(\":\n                words[i] = \"<left_parenthesis>\"\n            elif words[i] == \")\":\n                words[i] = \"<right_parenthesis>\"\n            elif words[i] == '\"':\n                words[i] = \"<quotation>\"\n            elif words[i] == \"@\":\n                words[i] = \"<at_mark>\"\n            elif words[i] == \"&\":\n                words[i] = \"<ampersand>\"\n            elif words[i] == \"{\":\n                words[i] = \"<left_curly_bracket>\"\n            elif words[i] == \"^\":\n                words[i] = \"<circumflex_accent>\"\n            elif words[i] == \"?\":\n                words[i] = \"<question_mark>\"\n            elif words[i] == \"<\":\n                words[i] = \"<less_than>\"\n            elif words[i] == \">\":\n                words[i] = \"<greater_than>\"\n            elif words[i] == \"=\":\n                words[i] = \"<equal>\"\n            elif words[i] == \"!\":\n                words[i] = \"<exclamation>\"\n            elif words[i] == \"\u2019\":\n                words[i] = \"<apostrophe>\"\n            elif words[i] == \":\":\n                words[i] = \"<colon>\"\n            elif words[i] == \"*\":\n                words[i] = \"<asterisk>\"\n            elif words[i] == \";\":\n                words[i] = \"<semi_colon>\"\n            elif words[i] == \"/\":\n                words[i] = \"<slash>\"\n            i += 1\n        t2 = tagger.tag(words)\n        t = []\n        i = 0\n        while i < len(t2):\n            word = t2[i][0]\n            tag = t2[i][1]\n            if word == \"<space>\":\n                word = \" \"\n            elif word == \"<plus>\":\n                word = \"+\"\n            elif word == \"<minus>\":\n                word = \"-\"\n            elif word == \"<equal>\":\n                word = \"=\"\n            elif word == \"<comma>\":\n                word = \",\"\n            elif word == \"<dollar>\":\n                word = \"$\"\n            elif word == \"<full_stop>\":\n                word = \".\"\n            elif word == \"<left_parenthesis>\":\n                word = \"(\"\n            elif word == \"<right_parenthesis>\":\n                word = \")\"\n            elif word == \"<quotation>\":\n                word = '\"'\n            elif word == \"<at_mark>\":\n                word = \"@\"\n            elif word == \"<ampersand>\":\n                word = \"&\"\n            elif word == \"<left_curly_bracket>\":\n                word = \"{\"\n            elif word == \"<circumflex_accent>\":\n                word = \"^\"\n            elif word == \"<question_mark>\":\n                word = \"?\"\n            elif word == \"<less_than>\":\n                word = \"<\"\n            elif word == \"<greater_than>\":\n                word = \">\"\n            elif word == \"<equal>\":\n                word = \"=\"\n            elif word == \"<exclamation>\":\n                word = \"!\"\n            elif word == \"<apostrophe>\":\n                word = \"\u2019\"\n            elif word == \"<colon>\":\n                word = \":\"\n            elif word == \"<asterisk>\":\n                word = \"*\"\n            elif word == \"<semi_colon>\":\n                word = \";\"\n            elif word == \"<slash>\":\n                word = \"/\"\n            t.append((word, tag))\n            i += 1\n    else:  # default, use \"pud\" as a corpus\n        tagger = _PUD_TAGGER\n        t = tagger.tag(words)\n\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef most_similar_cosmul(positive: List[str], negative: List[str]):\n\n    return _MODEL.most_similar_cosmul(positive=positive, negative=negative)", "response": "Word arithmetic operations\n    If a word is not in the vocabulary, KeyError will be raised.\n\n    :param list positive: a list of words to add\n    :param list negative: a list of words to substract\n\n    :return: the cosine similarity between the two word vectors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef similarity(word1: str, word2: str) -> float:\n    return _MODEL.similarity(word1, word2)", "response": "Get cosine similarity between two words."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a vector of words from a text.", "response": "def sentence_vectorizer(text: str, use_mean: bool = True):\n    \"\"\"\n    Get sentence vector from text\n    If a word is not in the vocabulary, KeyError will be raised.\n\n    :param string text: text input\n    :param boolean use_mean: if `True` use mean of all word vectors else use summation\n\n    :return: sentence vector of given input text\n    \"\"\"\n    words = word_tokenize(text, engine=\"ulmfit\")\n    vec = np.zeros((1, WV_DIM))\n\n    for word in words:\n        if word == \" \":\n            word = \"xxspace\"\n        elif word == \"\\n\":\n            word = \"xxeol\"\n\n        if word in _MODEL.wv.index2word:\n            vec += _MODEL.wv.word_vec(word)\n        else:\n            pass\n\n    if use_mean:\n        vec /= len(words)\n\n    return vec"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a character is Thai", "response": "def isthaichar(ch: str) -> bool:\n    \"\"\"\n    Check if a character is Thai\n    \u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e44\u0e17\u0e22\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\n\n    :param str ch: input character\n    :return: True or False\n    \"\"\"\n    ch_val = ord(ch)\n    if ch_val >= 3584 and ch_val <= 3711:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if all characters in word are Thai or not", "response": "def isthai(word: str, ignore_chars: str = \".\") -> bool:\n    \"\"\"\n    Check if all character is Thai\n    \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e17\u0e35\u0e48\u0e21\u0e35\u0e41\u0e15\u0e48\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e44\u0e17\u0e22\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\n\n    :param str word: input text\n    :param str ignore_chars: characters to be ignored (i.e. will be considered as Thai)\n    :return: True or False\n    \"\"\"\n    if not ignore_chars:\n        ignore_chars = \"\"\n\n    for ch in word:\n        if ch not in ignore_chars and not isthaichar(ch):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncounting the number of Thai characters in the input text.", "response": "def countthai(text: str, ignore_chars: str = _DEFAULT_IGNORE_CHARS) -> float:\n    \"\"\"\n    :param str text: input text\n    :return: float, proportion of characters in the text that is Thai character\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return 0\n\n    if not ignore_chars:\n        ignore_chars = \"\"\n\n    num_thai = 0\n    num_ignore = 0\n\n    for ch in text:\n        if ch in ignore_chars:\n            num_ignore += 1\n        elif isthaichar(ch):\n            num_thai += 1\n\n    num_count = len(text) - num_ignore\n\n    return (num_thai / num_count) * 100"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting words by frequency.", "response": "def rank(words: List[str], exclude_stopwords: bool = False) -> Counter:\n    \"\"\"\n    Sort words by frequency\n\n    :param list words: a list of words\n    :param bool exclude_stopwords: exclude stopwords\n    :return: Counter\n    \"\"\"\n    if not words:\n        return None\n\n    if exclude_stopwords:\n        words = [word for word in words if word not in _STOPWORDS]\n\n    return Counter(words)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_keyword(word_list: List[str], min_len: int = 3) -> Dict[str, int]:\n    word_list = rank(word_list, exclude_stopwords=True)\n\n    return {k: v for k, v in word_list.items() if v >= min_len}", "response": "Finds the first min_len keywords in the list of words."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_path(fname: str) -> str:\n    path = get_corpus_path(fname)\n    if not path:\n        download(fname)\n        path = get_corpus_path(fname)\n    return path", "response": "meth download get path of file from pythainlp - corpus\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces repetitions at the character level in text after the repetition", "response": "def replace_rep_after(text: str) -> str:\n    \"Replace repetitions at the character level in `text` after the repetition\"\n\n    def _replace_rep(m):\n        c, cc = m.groups()\n        return f\"{c}{TK_REP}{len(cc)+1}\"\n\n    re_rep = re.compile(r\"(\\S)(\\1{2,})\")\n\n    return re_rep.sub(_replace_rep, text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rm_brackets(text: str) -> str:\n    \"Remove all empty brackets from `t`.\"\n    new_line = re.sub(r\"\\(\\)\", \"\", text)\n    new_line = re.sub(r\"\\{\\}\", \"\", new_line)\n    new_line = re.sub(r\"\\[\\]\", \"\", new_line)\n\n    return new_line", "response": "Remove all empty brackets from t."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget document vector using fastai language model and data", "response": "def document_vector(text: str, learn, data, agg: str = \"mean\"):\n    \"\"\"\n    :meth: `document_vector` get document vector using fastai language model and data bunch\n    :param str text: text to extract embeddings\n    :param learn: fastai language model learner\n    :param data: fastai data bunch\n    :param agg: how to aggregate embeddings\n    :return: `numpy.array` of document vector sized 400 based on the encoder of the model\n    \"\"\"\n\n    s = _tokenizer.tokenizer(text)\n    t = torch.tensor(data.vocab.numericalize(s), requires_grad=False).to(device)\n    m = learn.model[0].encoder.to(device)\n    res = m(t).cpu().detach().numpy()\n    if agg == \"mean\":\n        res = res.mean(0)\n    elif agg == \"sum\":\n        res = res.sum(0)\n    else:\n        raise ValueError(\"Aggregate by mean or sum\")\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_source_conn(self, source_conn_id, source_bucket_name=None):\n        self.source_conn = BaseHook.get_hook(source_conn_id)\n        self.source_conn_id = source_conn_id\n\n        # Workaround for getting hook in case of s3 connection\n        # This is needed because get_hook silently returns None for s3 connections\n        # See https://issues.apache.org/jira/browse/AIRFLOW-2316 for more info\n        connection = BaseHook._get_connection_from_env(source_conn_id)\n        self.log.info(connection.extra_dejson)\n        if connection.conn_type == 's3':\n            self.log.info(\"Setting up s3 connection {0}\".format(source_conn_id))\n            self.source_conn = S3Hook(aws_conn_id=source_conn_id)\n            # End Workaround\n            if source_bucket_name is None:\n                raise AttributeError(\"Missing source bucket for s3 connection\")\n            self.source_bucket_name = source_bucket_name\n\n        if not isinstance(self.source_conn, DbApiHook) and not isinstance(self.source_conn, S3Hook):\n            raise AttributeError(\n                \"Only s3_csv, local and sql connection types are allowed, not {0}\".format(type(self.source_conn)))", "response": "Setup source connection based on source_conn_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_dest_conn(self, dest_conn_id, results_bucket_name, results_dest_name):\n        conn = BaseHook._get_connection_from_env(dest_conn_id)\n        if conn.conn_type != 's3':\n            raise AttributeError(\n                \"Only s3 is allowed as a results destination, not {0}\".format(conn.conn_type))\n\n        self.dest_conn = S3Hook(aws_conn_id=dest_conn_id)\n        self.dest_conn_id = dest_conn_id\n\n        if results_bucket_name is None or results_dest_name is None:\n            raise AttributeError(\"Specify bucket name and key name to store results\")\n\n        self.results_bucket_name = results_bucket_name\n        self.results_dest_name = results_dest_name", "response": "Setup the destination connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload dataframe based on specified connection", "response": "def _get_dataframe(self):\n        \"\"\"\n        Load dataframe based on specified connection\n        :return:\n        \"\"\"\n        if self.source_conn is None:  # Use local file\n            return ge.read_csv(self.dataset_name, **self.dataset_params)\n\n        if isinstance(self.source_conn, S3Hook):\n            hook = ExpectationS3CsvHook(aws_conn_id=self.source_conn_id)\n\n            return hook.get_ge_df(self.dataset_name, self.source_bucket_name, **self.dataset_params)\n\n        if isinstance(self.source_conn, DbApiHook):\n            hook = ExpectationMySQLHook(mysql_conn_id=self.source_conn_id)\n\n            return hook.get_ge_df(self.dataset_name, **self.dataset_params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading expectation config based on operator parameters.", "response": "def _load_json(self):\n        \"\"\"\n        Load expectation config based on operator parameters. If provided expectations_json is a file the config will\n        be loaded from this file. Otherwise we'll try to load the config as a string.\n        :return:\n        \"\"\"\n        if os.path.isfile(self.expectations_json):\n            self.log.info(\"Loading expectation config from file {file}\".format(file=self.expectations_json))\n            return json.load(open(self.expectations_json))\n        else:\n            self.log.info(\"Loading expectation config from string\")\n            return json.loads(self.expectations_json)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_validation_statistics(validation_results):\n    # calc stats\n    successful_expectations = sum(exp[\"success\"] for exp in validation_results)\n    evaluated_expectations = len(validation_results)\n    unsuccessful_expectations = evaluated_expectations - successful_expectations\n    success = successful_expectations == evaluated_expectations\n    try:\n        success_percent = successful_expectations / evaluated_expectations * 100\n    except ZeroDivisionError:\n        success_percent = float(\"nan\")\n\n    return ValidationStatistics(\n        successful_expectations=successful_expectations,\n        evaluated_expectations=evaluated_expectations,\n        unsuccessful_expectations=unsuccessful_expectations,\n        success=success,\n        success_percent=success_percent,\n    )", "response": "Calculate summary statistics for the validation results and\n    return ValidationStatistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmanages configuration and running of expectation objects. Expectation builds and saves a new expectation configuration to the DataAsset object. It is the core decorator \\ used by great expectations to manage expectation configurations. Args: method_arg_names (List) : An ordered list of the arguments used by the method implementing the expectation \\ (typically the result of inspection). Positional arguments are explicitly mapped to \\ keyword arguments when the expectation is run. Notes: Intermediate decorators that call the core @expectation decorator will most likely need to pass their \\ decorated methods' signature up to the expectation decorator. For example, the MetaPandasDataset \\ column_map_expectation decorator relies on the DataAsset expectation decorator, but will pass through the \\ signature from the implementing method. @expectation intercepts and takes action based on the following parameters: * include_config (boolean or None) : \\ If True, then include the generated expectation config as part of the result object. \\ For more detail, see :ref:`include_config`. * catch_exceptions (boolean or None) : \\ If True, then catch exceptions and include them as part of the result object. \\ For more detail, see :ref:`catch_exceptions`. * result_format (str or None) : \\ Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`. For more detail, see :ref:`result_format <result_format>`. * meta (dict or None): \\ A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\ For more detail, see :ref:`meta`.", "response": "def expectation(cls, method_arg_names):\n        \"\"\"Manages configuration and running of expectation objects.\n\n        Expectation builds and saves a new expectation configuration to the DataAsset object. It is the core decorator \\\n        used by great expectations to manage expectation configurations.\n\n        Args:\n            method_arg_names (List) : An ordered list of the arguments used by the method implementing the expectation \\\n                (typically the result of inspection). Positional arguments are explicitly mapped to \\\n                keyword arguments when the expectation is run.\n\n        Notes:\n            Intermediate decorators that call the core @expectation decorator will most likely need to pass their \\\n            decorated methods' signature up to the expectation decorator. For example, the MetaPandasDataset \\\n            column_map_expectation decorator relies on the DataAsset expectation decorator, but will pass through the \\\n            signature from the implementing method.\n\n            @expectation intercepts and takes action based on the following parameters:\n                * include_config (boolean or None) : \\\n                    If True, then include the generated expectation config as part of the result object. \\\n                    For more detail, see :ref:`include_config`.\n                * catch_exceptions (boolean or None) : \\\n                    If True, then catch exceptions and include them as part of the result object. \\\n                    For more detail, see :ref:`catch_exceptions`.\n                * result_format (str or None) : \\\n                    Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                    For more detail, see :ref:`result_format <result_format>`.\n                * meta (dict or None): \\\n                    A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                    For more detail, see :ref:`meta`.\n        \"\"\"\n        def outer_wrapper(func):\n            @wraps(func)\n            def wrapper(self, *args, **kwargs):\n\n                # Get the name of the method\n                method_name = func.__name__\n\n                # Combine all arguments into a single new \"kwargs\"\n                all_args = dict(zip(method_arg_names, args))\n                all_args.update(kwargs)\n\n                # Unpack display parameters; remove them from all_args if appropriate\n                if \"include_config\" in kwargs:\n                    include_config = kwargs[\"include_config\"]\n                    del all_args[\"include_config\"]\n                else:\n                    include_config = self.default_expectation_args[\"include_config\"]\n\n                if \"catch_exceptions\" in kwargs:\n                    catch_exceptions = kwargs[\"catch_exceptions\"]\n                    del all_args[\"catch_exceptions\"]\n                else:\n                    catch_exceptions = self.default_expectation_args[\"catch_exceptions\"]\n\n                if \"result_format\" in kwargs:\n                    result_format = kwargs[\"result_format\"]\n                else:\n                    result_format = self.default_expectation_args[\"result_format\"]\n\n                # Extract the meta object for use as a top-level expectation_config holder\n                if \"meta\" in kwargs:\n                    meta = kwargs[\"meta\"]\n                    del all_args[\"meta\"]\n                else:\n                    meta = None\n\n                # Get the signature of the inner wrapper:\n                if PY3:\n                    argspec = inspect.getfullargspec(func)[0][1:]\n                else:\n                    argspec = inspect.getargspec(func)[0][1:]\n\n                if \"result_format\" in argspec:\n                    all_args[\"result_format\"] = result_format\n                else:\n                    if \"result_format\" in all_args:\n                        del all_args[\"result_format\"]\n\n                all_args = recursively_convert_to_json_serializable(all_args)\n\n                # Patch in PARAMETER args, and remove locally-supplied arguments\n                # This will become the stored config\n                expectation_args = copy.deepcopy(all_args)\n\n                if \"evaluation_parameters\" in self._expectations_config:\n                    evaluation_args = self._build_evaluation_parameters(expectation_args,\n                                                                        self._expectations_config[\"evaluation_parameters\"])  # This will be passed to the evaluation\n                else:\n                    evaluation_args = self._build_evaluation_parameters(\n                        expectation_args, None)\n\n                # Construct the expectation_config object\n                expectation_config = DotDict({\n                    \"expectation_type\": method_name,\n                    \"kwargs\": expectation_args\n                })\n\n                # Add meta to our expectation_config\n                if meta is not None:\n                    expectation_config[\"meta\"] = meta\n\n                raised_exception = False\n                exception_traceback = None\n                exception_message = None\n\n                # Finally, execute the expectation method itself\n                try:\n                    return_obj = func(self, **evaluation_args)\n\n                except Exception as err:\n                    if catch_exceptions:\n                        raised_exception = True\n                        exception_traceback = traceback.format_exc()\n                        exception_message = str(err)\n\n                        return_obj = {\n                            \"success\": False\n                        }\n\n                    else:\n                        raise(err)\n\n                # Append the expectation to the config.\n                self._append_expectation(expectation_config)\n\n                if include_config:\n                    return_obj[\"expectation_config\"] = copy.deepcopy(\n                        expectation_config)\n\n                if catch_exceptions:\n                    return_obj[\"exception_info\"] = {\n                        \"raised_exception\": raised_exception,\n                        \"exception_message\": exception_message,\n                        \"exception_traceback\": exception_traceback\n                    }\n\n                # Add a \"success\" object to the config\n                expectation_config[\"success_on_last_run\"] = return_obj[\"success\"]\n\n                # Add meta to return object\n                if meta is not None:\n                    return_obj['meta'] = meta\n\n\n                return_obj = recursively_convert_to_json_serializable(\n                    return_obj)\n                return return_obj\n\n            return wrapper\n\n        return outer_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _initialize_expectations(self, config=None, data_asset_name=None):\n        if config != None:\n            #!!! Should validate the incoming config with jsonschema here\n\n            # Copy the original so that we don't overwrite it by accident\n            # Pandas incorrectly interprets this as an attempt to create a column and throws up a warning. Suppress it\n            # since we are subclassing.\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=UserWarning)\n                self._expectations_config = DotDict(copy.deepcopy(config))\n                if data_asset_name is not None:\n                    self._expectations_config[\"data_asset_name\"] = data_asset_name\n\n        else:\n            # Pandas incorrectly interprets this as an attempt to create a column and throws up a warning. Suppress it\n            # since we are subclassing.\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=UserWarning)\n                self._expectations_config = DotDict({\n                    \"data_asset_name\": data_asset_name,\n                    \"data_asset_type\": self.__class__.__name__,\n                    \"meta\": {\n                        \"great_expectations.__version__\": __version__\n                    },\n                    \"expectations\": []\n                })\n\n        # Pandas incorrectly interprets this as an attempt to create a column and throws up a warning. Suppress it\n        # since we are subclassing.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            self.default_expectation_args = {\n                \"include_config\": False,\n                \"catch_exceptions\": False,\n                \"result_format\": 'BASIC',\n            }", "response": "Initializes the _expectations_config attribute with a specified expectation config."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _append_expectation(self, expectation_config):\n        expectation_type = expectation_config['expectation_type']\n\n        # Test to ensure the new expectation is serializable.\n        # FIXME: If it's not, are we sure we want to raise an error?\n        # FIXME: Should we allow users to override the error?\n        # FIXME: Should we try to convert the object using something like recursively_convert_to_json_serializable?\n        json.dumps(expectation_config)\n\n        # Drop existing expectations with the same expectation_type.\n        # For column_expectations, _append_expectation should only replace expectations\n        # where the expectation_type AND the column match\n        #!!! This is good default behavior, but\n        #!!!    it needs to be documented, and\n        #!!!    we need to provide syntax to override it.\n\n        if 'column' in expectation_config['kwargs']:\n            column = expectation_config['kwargs']['column']\n\n            self._expectations_config.expectations = [f for f in filter(\n                lambda exp: (exp['expectation_type'] != expectation_type) or (\n                    'column' in exp['kwargs'] and exp['kwargs']['column'] != column),\n                self._expectations_config.expectations\n            )]\n        else:\n            self._expectations_config.expectations = [f for f in filter(\n                lambda exp: exp['expectation_type'] != expectation_type,\n                self._expectations_config.expectations\n            )]\n\n        self._expectations_config.expectations.append(expectation_config)", "response": "Appends an expectation to the internal list of expectations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _copy_and_clean_up_expectation(self,\n                                       expectation,\n                                       discard_result_format_kwargs=True,\n                                       discard_include_configs_kwargs=True,\n                                       discard_catch_exceptions_kwargs=True,\n                                       ):\n        \"\"\"Returns copy of `expectation` without `success_on_last_run` and other specified key-value pairs removed\n\n          Returns a copy of specified expectation will not have `success_on_last_run` key-value. The other key-value \\\n          pairs will be removed by default but will remain in the copy if specified.\n\n          Args:\n              expectation (json): \\\n                  The expectation to copy and clean.\n              discard_result_format_kwargs (boolean): \\\n                  if True, will remove the kwarg `output_format` key-value pair from the copied expectation.\n              discard_include_configs_kwargs (boolean):\n                  if True, will remove the kwarg `include_configs` key-value pair from the copied expectation.\n              discard_catch_exceptions_kwargs (boolean):\n                  if True, will remove the kwarg `catch_exceptions` key-value pair from the copied expectation.\n\n          Returns:\n              A copy of the provided expectation with `success_on_last_run` and other specified key-value pairs removed\n        \"\"\"\n        new_expectation = copy.deepcopy(expectation)\n\n        if \"success_on_last_run\" in new_expectation:\n            del new_expectation[\"success_on_last_run\"]\n\n        if discard_result_format_kwargs:\n            if \"result_format\" in new_expectation[\"kwargs\"]:\n                del new_expectation[\"kwargs\"][\"result_format\"]\n                # discards[\"result_format\"] += 1\n\n        if discard_include_configs_kwargs:\n            if \"include_configs\" in new_expectation[\"kwargs\"]:\n                del new_expectation[\"kwargs\"][\"include_configs\"]\n                # discards[\"include_configs\"] += 1\n\n        if discard_catch_exceptions_kwargs:\n            if \"catch_exceptions\" in new_expectation[\"kwargs\"]:\n                del new_expectation[\"kwargs\"][\"catch_exceptions\"]\n                # discards[\"catch_exceptions\"] += 1\n\n        return new_expectation", "response": "Returns a copy of the provided expectation with the same keys and with the same values removed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _copy_and_clean_up_expectations_from_indexes(\n        self,\n        match_indexes,\n        discard_result_format_kwargs=True,\n        discard_include_configs_kwargs=True,\n        discard_catch_exceptions_kwargs=True,\n    ):\n        \"\"\"Copies and cleans all expectations provided by their index in DataAsset._expectations_config.expectations.\n\n           Applies the _copy_and_clean_up_expectation method to multiple expectations, provided by their index in \\\n           `DataAsset,_expectations_config.expectations`. Returns a list of the copied and cleaned expectations.\n\n           Args:\n               match_indexes (List): \\\n                   Index numbers of the expectations from `expectation_config.expectations` to be copied and cleaned.\n               discard_result_format_kwargs (boolean): \\\n                   if True, will remove the kwarg `output_format` key-value pair from the copied expectation.\n               discard_include_configs_kwargs (boolean):\n                   if True, will remove the kwarg `include_configs` key-value pair from the copied expectation.\n               discard_catch_exceptions_kwargs (boolean):\n                   if True, will remove the kwarg `catch_exceptions` key-value pair from the copied expectation.\n\n           Returns:\n               A list of the copied expectations with `success_on_last_run` and other specified \\\n               key-value pairs removed.\n\n           See also:\n               _copy_and_clean_expectation\n        \"\"\"\n        rval = []\n        for i in match_indexes:\n            rval.append(\n                self._copy_and_clean_up_expectation(\n                    self._expectations_config.expectations[i],\n                    discard_result_format_kwargs,\n                    discard_include_configs_kwargs,\n                    discard_catch_exceptions_kwargs,\n                )\n            )\n\n        return rval", "response": "Copies and cleans up all expectations provided by the specified indexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_expectation_indexes(self,\n                                 expectation_type=None,\n                                 column=None,\n                                 expectation_kwargs=None\n                                 ):\n        \"\"\"Find matching expectations within _expectation_config.\n        Args:\n            expectation_type=None                : The name of the expectation type to be matched.\n            column=None                          : The name of the column to be matched.\n            expectation_kwargs=None              : A dictionary of kwargs to match against.\n\n        Returns:\n            A list of indexes for matching expectation objects.\n            If there are no matches, the list will be empty.\n        \"\"\"\n        if expectation_kwargs == None:\n            expectation_kwargs = {}\n\n        if \"column\" in expectation_kwargs and column != None and column != expectation_kwargs[\"column\"]:\n            raise ValueError(\"Conflicting column names in remove_expectation: %s and %s\" % (\n                column, expectation_kwargs[\"column\"]))\n\n        if column != None:\n            expectation_kwargs[\"column\"] = column\n\n        match_indexes = []\n        for i, exp in enumerate(self._expectations_config.expectations):\n            if expectation_type == None or (expectation_type == exp['expectation_type']):\n                # if column == None or ('column' not in exp['kwargs']) or (exp['kwargs']['column'] == column) or (exp['kwargs']['column']==:\n                match = True\n\n                for k, v in expectation_kwargs.items():\n                    if k in exp['kwargs'] and exp['kwargs'][k] == v:\n                        continue\n                    else:\n                        match = False\n\n                if match:\n                    match_indexes.append(i)\n\n        return match_indexes", "response": "Find matching expectations within _expectations_config."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind matching expectations within _expectation_config.", "response": "def find_expectations(self,\n                          expectation_type=None,\n                          column=None,\n                          expectation_kwargs=None,\n                          discard_result_format_kwargs=True,\n                          discard_include_configs_kwargs=True,\n                          discard_catch_exceptions_kwargs=True,\n                          ):\n        \"\"\"Find matching expectations within _expectation_config.\n        Args:\n            expectation_type=None                : The name of the expectation type to be matched.\n            column=None                          : The name of the column to be matched.\n            expectation_kwargs=None              : A dictionary of kwargs to match against.\n            discard_result_format_kwargs=True    : In returned expectation object(s), suppress the `result_format` parameter.\n            discard_include_configs_kwargs=True  : In returned expectation object(s), suppress the `include_configs` parameter.\n            discard_catch_exceptions_kwargs=True : In returned expectation object(s), suppress the `catch_exceptions` parameter.\n\n        Returns:\n            A list of matching expectation objects.\n            If there are no matches, the list will be empty.\n        \"\"\"\n\n        match_indexes = self.find_expectation_indexes(\n            expectation_type,\n            column,\n            expectation_kwargs,\n        )\n\n        return self._copy_and_clean_up_expectations_from_indexes(\n            match_indexes,\n            discard_result_format_kwargs,\n            discard_include_configs_kwargs,\n            discard_catch_exceptions_kwargs,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an expectation from the _expectations_config.", "response": "def remove_expectation(self,\n                           expectation_type=None,\n                           column=None,\n                           expectation_kwargs=None,\n                           remove_multiple_matches=False,\n                           dry_run=False,\n                           ):\n        \"\"\"Remove matching expectation(s) from _expectation_config.\n        Args:\n            expectation_type=None                : The name of the expectation type to be matched.\n            column=None                          : The name of the column to be matched.\n            expectation_kwargs=None              : A dictionary of kwargs to match against.\n            remove_multiple_matches=False        : Match multiple expectations\n            dry_run=False                        : Return a list of matching expectations without removing\n\n        Returns:\n            None, unless dry_run=True.\n            If dry_run=True and remove_multiple_matches=False then return the expectation that *would be* removed.\n            If dry_run=True and remove_multiple_matches=True then return a list of expectations that *would be* removed.\n\n        Note:\n            If remove_expectation doesn't find any matches, it raises a ValueError.\n            If remove_expectation finds more than one matches and remove_multiple_matches!=True, it raises a ValueError.\n            If dry_run=True, then `remove_expectation` acts as a thin layer to find_expectations, with the default values for discard_result_format_kwargs, discard_include_configs_kwargs, and discard_catch_exceptions_kwargs\n        \"\"\"\n\n        match_indexes = self.find_expectation_indexes(\n            expectation_type,\n            column,\n            expectation_kwargs,\n        )\n\n        if len(match_indexes) == 0:\n            raise ValueError('No matching expectation found.')\n\n        elif len(match_indexes) > 1:\n            if not remove_multiple_matches:\n                raise ValueError(\n                    'Multiple expectations matched arguments. No expectations removed.')\n            else:\n\n                if not dry_run:\n                    self._expectations_config.expectations = [i for j, i in enumerate(\n                        self._expectations_config.expectations) if j not in match_indexes]\n                else:\n                    return self._copy_and_clean_up_expectations_from_indexes(match_indexes)\n\n        else:  # Exactly one match\n            expectation = self._copy_and_clean_up_expectation(\n                self._expectations_config.expectations[match_indexes[0]]\n            )\n\n            if not dry_run:\n                del self._expectations_config.expectations[match_indexes[0]]\n\n            else:\n                if remove_multiple_matches:\n                    return [expectation]\n                else:\n                    return expectation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the _expectation_config as a JSON object and perform some cleaning along the way.", "response": "def get_expectations_config(self,\n                                discard_failed_expectations=True,\n                                discard_result_format_kwargs=True,\n                                discard_include_configs_kwargs=True,\n                                discard_catch_exceptions_kwargs=True,\n                                suppress_warnings=False\n                                ):\n        \"\"\"Returns _expectation_config as a JSON object, and perform some cleaning along the way.\n\n        Args:\n            discard_failed_expectations (boolean): \\\n                Only include expectations with success_on_last_run=True in the exported config.  Defaults to `True`.\n            discard_result_format_kwargs (boolean): \\\n                In returned expectation objects, suppress the `result_format` parameter. Defaults to `True`.\n            discard_include_configs_kwargs (boolean): \\\n                In returned expectation objects, suppress the `include_configs` parameter. Defaults to `True`.\n            discard_catch_exceptions_kwargs (boolean): \\\n                In returned expectation objects, suppress the `catch_exceptions` parameter.  Defaults to `True`.\n\n        Returns:\n            An expectation config.\n\n        Note:\n            get_expectations_config does not affect the underlying config at all. The returned config is a copy of _expectations_config, not the original object.\n        \"\"\"\n        config = dict(self._expectations_config)\n        config = copy.deepcopy(config)\n        expectations = config[\"expectations\"]\n\n        discards = defaultdict(int)\n\n        if discard_failed_expectations:\n            new_expectations = []\n\n            for expectation in expectations:\n                # Note: This is conservative logic.\n                # Instead of retaining expectations IFF success==True, it discard expectations IFF success==False.\n                # In cases where expectation[\"success\"] is missing or None, expectations are *retained*.\n                # Such a case could occur if expectations were loaded from a config file and never run.\n                if \"success_on_last_run\" in expectation and expectation[\"success_on_last_run\"] == False:\n                    discards[\"failed_expectations\"] += 1\n                else:\n                    new_expectations.append(expectation)\n\n            expectations = new_expectations\n\n        for expectation in expectations:\n            # FIXME: Factor this out into a new function. The logic is duplicated in remove_expectation, which calls _copy_and_clean_up_expectation\n            if \"success_on_last_run\" in expectation:\n                del expectation[\"success_on_last_run\"]\n\n            if discard_result_format_kwargs:\n                if \"result_format\" in expectation[\"kwargs\"]:\n                    del expectation[\"kwargs\"][\"result_format\"]\n                    discards[\"result_format\"] += 1\n\n            if discard_include_configs_kwargs:\n                if \"include_configs\" in expectation[\"kwargs\"]:\n                    del expectation[\"kwargs\"][\"include_configs\"]\n                    discards[\"include_configs\"] += 1\n\n            if discard_catch_exceptions_kwargs:\n                if \"catch_exceptions\" in expectation[\"kwargs\"]:\n                    del expectation[\"kwargs\"][\"catch_exceptions\"]\n                    discards[\"catch_exceptions\"] += 1\n\n        if not suppress_warnings:\n            \"\"\"\nWARNING: get_expectations_config discarded\n    12 failing expectations\n    44 result_format kwargs\n     0 include_config kwargs\n     1 catch_exceptions kwargs\nIf you wish to change this behavior, please set discard_failed_expectations, discard_result_format_kwargs, discard_include_configs_kwargs, and discard_catch_exceptions_kwargs appropirately.\n            \"\"\"\n            if any([discard_failed_expectations, discard_result_format_kwargs, discard_include_configs_kwargs, discard_catch_exceptions_kwargs]):\n                print(\"WARNING: get_expectations_config discarded\")\n                if discard_failed_expectations:\n                    print(\"\\t%d failing expectations\" %\n                          discards[\"failed_expectations\"])\n                if discard_result_format_kwargs:\n                    print(\"\\t%d result_format kwargs\" %\n                          discards[\"result_format\"])\n                if discard_include_configs_kwargs:\n                    print(\"\\t%d include_configs kwargs\" %\n                          discards[\"include_configs\"])\n                if discard_catch_exceptions_kwargs:\n                    print(\"\\t%d catch_exceptions kwargs\" %\n                          discards[\"catch_exceptions\"])\n                print(\"If you wish to change this behavior, please set discard_failed_expectations, discard_result_format_kwargs, discard_include_configs_kwargs, and discard_catch_exceptions_kwargs appropirately.\")\n\n        config[\"expectations\"] = expectations\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the expectations config to a JSON file.", "response": "def save_expectations_config(\n        self,\n        filepath=None,\n        discard_failed_expectations=True,\n        discard_result_format_kwargs=True,\n        discard_include_configs_kwargs=True,\n        discard_catch_exceptions_kwargs=True,\n        suppress_warnings=False\n    ):\n        \"\"\"Writes ``_expectation_config`` to a JSON file.\n\n           Writes the DataAsset's expectation config to the specified JSON ``filepath``. Failing expectations \\\n           can be excluded from the JSON expectations config with ``discard_failed_expectations``. The kwarg key-value \\\n           pairs :ref:`result_format`, :ref:`include_config`, and :ref:`catch_exceptions` are optionally excluded from the JSON \\\n           expectations config.\n\n           Args:\n               filepath (string): \\\n                   The location and name to write the JSON config file to.\n               discard_failed_expectations (boolean): \\\n                   If True, excludes expectations that do not return ``success = True``. \\\n                   If False, all expectations are written to the JSON config file.\n               discard_result_format_kwargs (boolean): \\\n                   If True, the :ref:`result_format` attribute for each expectation is not written to the JSON config file. \\\n               discard_include_configs_kwargs (boolean): \\\n                   If True, the :ref:`include_config` attribute for each expectation is not written to the JSON config file.\\\n               discard_catch_exceptions_kwargs (boolean): \\\n                   If True, the :ref:`catch_exceptions` attribute for each expectation is not written to the JSON config \\\n                   file.\n               suppress_warnings (boolean): \\\n                  It True, all warnings raised by Great Expectations, as a result of dropped expectations, are \\\n                  suppressed.\n\n        \"\"\"\n        if filepath == None:\n            # FIXME: Fetch the proper filepath from the project config\n            pass\n\n        expectations_config = self.get_expectations_config(\n            discard_failed_expectations,\n            discard_result_format_kwargs,\n            discard_include_configs_kwargs,\n            discard_catch_exceptions_kwargs,\n            suppress_warnings\n        )\n        expectation_config_str = json.dumps(expectations_config, indent=2)\n        open(filepath, 'w').write(expectation_config_str)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a JSON - formatted report describing the outcome of all expectations.", "response": "def validate(self, expectations_config=None, evaluation_parameters=None, catch_exceptions=True, result_format=None, only_return_failures=False):\n        \"\"\"Generates a JSON-formatted report describing the outcome of all expectations.\n\n            Use the default expectations_config=None to validate the expectations config associated with the DataAsset.\n\n            Args:\n                expectations_config (json or None): \\\n                    If None, uses the expectations config generated with the DataAsset during the current session. \\\n                    If a JSON file, validates those expectations.\n                evaluation_parameters (dict or None): \\\n                    If None, uses the evaluation_paramters from the expectations_config provided or as part of the data_asset.\n                    If a dict, uses the evaluation parameters in the dictionary.\n                catch_exceptions (boolean): \\\n                    If True, exceptions raised by tests will not end validation and will be described in the returned report.\n                result_format (string or None): \\\n                    If None, uses the default value ('BASIC' or as specified). \\\n                    If string, the returned expectation output follows the specified format ('BOOLEAN_ONLY','BASIC', etc.).\n                include_config (boolean): \\\n                    If True, the returned results include the config information associated with each expectation, if \\\n                    it exists.\n                only_return_failures (boolean): \\\n                    If True, expectation results are only returned when ``success = False`` \\\n\n            Returns:\n                A JSON-formatted dictionary containing a list of the validation results. \\\n                An example of the returned format::\n\n                {\n                  \"results\": [\n                    {\n                      \"unexpected_list\": [unexpected_value_1, unexpected_value_2],\n                      \"expectation_type\": \"expect_*\",\n                      \"kwargs\": {\n                        \"column\": \"Column_Name\",\n                        \"output_format\": \"SUMMARY\"\n                      },\n                      \"success\": true,\n                      \"raised_exception: false.\n                      \"exception_traceback\": null\n                    },\n                    {\n                      ... (Second expectation results)\n                    },\n                    ... (More expectations results)\n                  ],\n                  \"success\": true,\n                  \"statistics\": {\n                    \"evaluated_expectations\": n,\n                    \"successful_expectations\": m,\n                    \"unsuccessful_expectations\": n - m,\n                    \"success_percent\": m / n\n                  }\n                }\n\n           Notes:\n               If the configuration object was built with a different version of great expectations then the current environment. \\\n               If no version was found in the configuration file.\n\n           Raises:\n               AttributeError - if 'catch_exceptions'=None and an expectation throws an AttributeError\n        \"\"\"\n\n        results = []\n\n        if expectations_config is None:\n            expectations_config = self.get_expectations_config(\n                discard_failed_expectations=False,\n                discard_result_format_kwargs=False,\n                discard_include_configs_kwargs=False,\n                discard_catch_exceptions_kwargs=False,\n            )\n        elif isinstance(expectations_config, string_types):\n            expectations_config = json.load(open(expectations_config, 'r'))\n\n        if evaluation_parameters is None:\n            # Use evaluation parameters from the (maybe provided) config\n            if \"evaluation_parameters\" in expectations_config:\n                evaluation_parameters = expectations_config[\"evaluation_parameters\"]\n\n        # Warn if our version is different from the version in the configuration\n        try:\n            if expectations_config['meta']['great_expectations.__version__'] != __version__:\n                warnings.warn(\n                    \"WARNING: This configuration object was built using version %s of great_expectations, but is currently being valided by version %s.\" % (expectations_config['meta']['great_expectations.__version__'], __version__))\n        except KeyError:\n            warnings.warn(\n                \"WARNING: No great_expectations version found in configuration object.\")\n\n        for expectation in expectations_config['expectations']:\n\n            try:\n                expectation_method = getattr(\n                    self, expectation['expectation_type'])\n\n                if result_format is not None:\n                    expectation['kwargs'].update({'result_format': result_format})\n\n                # Counting the number of unexpected values can be expensive when there is a large\n                # number of np.nan values.\n                # This only happens on expect_column_values_to_not_be_null expectations.\n                # Since there is no reason to look for most common unexpected values in this case,\n                # we will instruct the result formatting method to skip this step.\n\n                if expectation['expectation_type'] in ['expect_column_values_to_not_be_null',\n                                                       'expect_column_values_to_be_null']:\n                    expectation['kwargs']['result_format'] = parse_result_format(expectation['kwargs']['result_format'])\n                    expectation['kwargs']['result_format']['partial_unexpected_count'] = 0\n\n                # A missing parameter should raise a KeyError\n                evaluation_args = self._build_evaluation_parameters(\n                    expectation['kwargs'], evaluation_parameters)\n\n                result = expectation_method(\n                    catch_exceptions=catch_exceptions,\n                    **evaluation_args\n                )\n\n            except Exception as err:\n                if catch_exceptions:\n                    raised_exception = True\n                    exception_traceback = traceback.format_exc()\n\n                    result = {\n                        \"success\": False,\n                        \"exception_info\": {\n                            \"raised_exception\": raised_exception,\n                            \"exception_traceback\": exception_traceback,\n                            \"exception_message\": str(err)\n                        }\n                    }\n\n                else:\n                    raise(err)\n\n            # if include_config:\n            result[\"expectation_config\"] = copy.deepcopy(expectation)\n\n            # Add an empty exception_info object if no exception was caught\n            if catch_exceptions and ('exception_info' not in result):\n                result[\"exception_info\"] = {\n                    \"raised_exception\": False,\n                    \"exception_traceback\": None,\n                    \"exception_message\": None\n                }\n\n            results.append(result)\n\n        statistics = _calc_validation_statistics(results)\n\n        if only_return_failures:\n            abbrev_results = []\n            for exp in results:\n                if exp[\"success\"] == False:\n                    abbrev_results.append(exp)\n            results = abbrev_results\n\n        result = {\n            \"results\": results,\n            \"success\": statistics.success,\n            \"statistics\": {\n                \"evaluated_expectations\": statistics.evaluated_expectations,\n                \"successful_expectations\": statistics.successful_expectations,\n                \"unsuccessful_expectations\": statistics.unsuccessful_expectations,\n                \"success_percent\": statistics.success_percent,\n            }\n        }\n\n        if evaluation_parameters is not None:\n            result.update({\"evaluation_parameters\": evaluation_parameters})\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting an evaluation parameter value that has been stored in meta.", "response": "def get_evaluation_parameter(self, parameter_name, default_value=None):\n        \"\"\"Get an evaluation parameter value that has been stored in meta.\n\n        Args:\n            parameter_name (string): The name of the parameter to store.\n            default_value (any): The default value to be returned if the parameter is not found.\n\n        Returns:\n            The current value of the evaluation parameter.\n        \"\"\"\n        if \"evaluation_parameters\" in self._expectations_config and \\\n                parameter_name in self._expectations_config['evaluation_parameters']:\n            return self._expectations_config['evaluation_parameters'][parameter_name]\n        else:\n            return default_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_evaluation_parameter(self, parameter_name, parameter_value):\n\n        if 'evaluation_parameters' not in self._expectations_config:\n            self._expectations_config['evaluation_parameters'] = {}\n\n        self._expectations_config['evaluation_parameters'].update(\n            {parameter_name: parameter_value})", "response": "Set the value of the parameter name in the data_asset evaluation_parameters object and used to evaluate the parameterized expectations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_evaluation_parameters(self, expectation_args, evaluation_parameters):\n\n        evaluation_args = copy.deepcopy(expectation_args)\n\n        # Iterate over arguments, and replace $PARAMETER-defined args with their\n        # specified parameters.\n        for key, value in evaluation_args.items():\n            if isinstance(value, dict) and '$PARAMETER' in value:\n                # First, check to see whether an argument was supplied at runtime\n                # If it was, use that one, but remove it from the stored config\n                if \"$PARAMETER.\" + value[\"$PARAMETER\"] in value:\n                    evaluation_args[key] = evaluation_args[key][\"$PARAMETER.\" +\n                                                                value[\"$PARAMETER\"]]\n                    del expectation_args[key][\"$PARAMETER.\" +\n                                              value[\"$PARAMETER\"]]\n                elif evaluation_parameters is not None and value[\"$PARAMETER\"] in evaluation_parameters:\n                    evaluation_args[key] = evaluation_parameters[value['$PARAMETER']]\n                else:\n                    raise KeyError(\n                        \"No value found for $PARAMETER \" + value[\"$PARAMETER\"])\n\n        return evaluation_args", "response": "Build a dictionary of parameters to evaluate using the provided evaluation_paramters and mutate expectation_args by removing any parameter values passed in as temporary values during exploratory work."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates success and percent_success for column_map_expectations", "response": "def _calc_map_expectation_success(self, success_count, nonnull_count, mostly):\n        \"\"\"Calculate success and percent_success for column_map_expectations\n\n        Args:\n            success_count (int): \\\n                The number of successful values in the column\n            nonnull_count (int): \\\n                The number of nonnull values in the column\n            mostly (float or None): \\\n                A value between 0 and 1 (or None), indicating the percentage of successes required to pass the expectation as a whole\\\n                If mostly=None, then all values must succeed in order for the expectation as a whole to succeed.\n\n        Returns:\n            success (boolean), percent_success (float)\n        \"\"\"\n\n        if nonnull_count > 0:\n            # percent_success = float(success_count)/nonnull_count\n            percent_success = success_count / nonnull_count\n\n            if mostly != None:\n                success = bool(percent_success >= mostly)\n\n            else:\n                success = bool(nonnull_count-success_count == 0)\n\n        else:\n            success = True\n            percent_success = None\n\n        return success, percent_success"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(parsed_args):\n    parsed_args = vars(parsed_args)\n    data_set = parsed_args['dataset']\n    expectations_config_file = parsed_args['expectations_config_file']\n\n    expectations_config = json.load(open(expectations_config_file))\n\n    if parsed_args[\"evaluation_parameters\"] is not None:\n        evaluation_parameters = json.load(\n            open(parsed_args[\"evaluation_parameters\"]))\n    else:\n        evaluation_parameters = None\n\n    # Use a custom dataasset module and class if provided. Otherwise infer from the config.\n    if parsed_args[\"custom_dataset_module\"]:\n        sys.path.insert(0, os.path.dirname(\n            parsed_args[\"custom_dataset_module\"]))\n        module_name = os.path.basename(\n            parsed_args[\"custom_dataset_module\"]).split('.')[0]\n        custom_module = __import__(module_name)\n        dataset_class = getattr(\n            custom_module, parsed_args[\"custom_dataset_class\"])\n    elif \"data_asset_type\" in expectations_config:\n        if expectations_config[\"data_asset_type\"] == \"Dataset\" or expectations_config[\"data_asset_type\"] == \"PandasDataset\":\n            dataset_class = PandasDataset\n        elif expectations_config[\"data_asset_type\"].endswith(\"Dataset\"):\n            logger.info(\"Using PandasDataset to validate dataset of type %s.\" % expectations_config[\"data_asset_type\"])\n            dataset_class = PandasDataset\n        elif expectations_config[\"data_asset_type\"] == \"FileDataAsset\":\n            dataset_class = FileDataAsset\n        else:\n            logger.critical(\"Unrecognized data_asset_type %s. You may need to specifcy custom_dataset_module and custom_dataset_class.\" % expectations_config[\"data_asset_type\"])\n            return -1\n    else:\n        dataset_class = PandasDataset\n\n    if issubclass(dataset_class, Dataset):\n        da = read_csv(data_set, expectations_config=expectations_config,\n                    dataset_class=dataset_class)\n    else:\n        da = dataset_class(data_set, config=expectations_config)\n\n    result = da.validate(\n        evaluation_parameters=evaluation_parameters,\n        result_format=parsed_args[\"result_format\"],\n        catch_exceptions=parsed_args[\"catch_exceptions\"],\n        only_return_failures=parsed_args[\"only_return_failures\"],\n    )\n\n    print(json.dumps(result, indent=2))\n    return result['statistics']['unsuccessful_expectations']", "response": "Read a dataset file and validate it using a config saved in another file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntesting whether a given object is a valid categorical partition object.", "response": "def is_valid_categorical_partition_object(partition_object):\n    \"\"\"Tests whether a given object is a valid categorical partition object.\n    :param partition_object: The partition_object to evaluate\n    :return: Boolean\n    \"\"\"\n    if partition_object is None or (\"weights\" not in partition_object) or (\"values\" not in partition_object):\n        return False\n    # Expect the same number of values as weights; weights should sum to one\n    if len(partition_object['values']) == len(partition_object['weights']) and \\\n            np.allclose(np.sum(partition_object['weights']), 1):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_valid_continuous_partition_object(partition_object):\n    if (partition_object is None) or (\"weights\" not in partition_object) or (\"bins\" not in partition_object):\n        return False\n    if(\"tail_weights\" in partition_object):\n        if (len(partition_object[\"tail_weights\"])!=2):\n            return False\n        comb_weights=partition_object[\"tail_weights\"]+partition_object[\"weights\"]\n    else:\n        comb_weights=partition_object[\"weights\"]\n    # Expect one more bin edge than weight; all bin edges should be monotonically increasing; weights should sum to one\n    if (len(partition_object['bins']) == (len(partition_object['weights']) + 1)) and \\\n            np.all(np.diff(partition_object['bins']) > 0) and \\\n            np.allclose(np.sum(comb_weights), 1):\n        return True\n    return False", "response": "Tests whether a given object is a valid continuous partition object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_distribution_parameters(data, distribution, params=None):\n\n    if params is None:\n        params = dict()\n    elif not isinstance(params, dict):\n        raise TypeError(\n            \"params must be a dictionary object, see great_expectations documentation\")\n\n    if 'mean' not in params.keys():\n        params['mean'] = data.mean()\n\n    if 'std_dev' not in params.keys():\n        params['std_dev'] = data.std()\n\n    if distribution == \"beta\":\n        # scipy cdf(x, a, b, loc=0, scale=1)\n        if 'alpha' not in params.keys():\n            # from https://stats.stackexchange.com/questions/12232/calculating-the-parameters-of-a-beta-distribution-using-the-mean-and-variance\n            params['alpha'] = (params['mean'] ** 2) * (\n                ((1 - params['mean']) / params['std_dev'] ** 2) - (1 / params['mean']))\n        if 'beta' not in params.keys():\n            params['beta'] = params['alpha'] * ((1 / params['mean']) - 1)\n\n    elif distribution == 'gamma':\n        # scipy cdf(x, a, loc=0, scale=1)\n        if 'alpha' not in params.keys():\n            # Using https://en.wikipedia.org/wiki/Gamma_distribution\n            params['alpha'] = (params['mean'] / params.get('scale', 1))\n\n    # elif distribution == 'poisson':\n    #    if 'lambda' not in params.keys():\n    #       params['lambda'] = params['mean']\n\n    elif distribution == 'uniform':\n        # scipy cdf(x, loc=0, scale=1)\n        if 'min' not in params.keys():\n            if 'loc' in params.keys():\n                params['min'] = params['loc']\n            else:\n                params['min'] = min(data)\n        if 'max' not in params.keys():\n            if 'scale' in params.keys():\n                params['max'] = params['scale']\n            else:\n                params['max'] = max(data) - params['min']\n\n    elif distribution == 'chi2':\n        # scipy cdf(x, df, loc=0, scale=1)\n        if 'df' not in params.keys():\n            # from https://en.wikipedia.org/wiki/Chi-squared_distribution\n            params['df'] = params['mean']\n\n    #  Expon only uses loc and scale, use default\n    # elif distribution == 'expon':\n        # scipy cdf(x, loc=0, scale=1)\n    #    if 'lambda' in params.keys():\n            # Lambda is optional\n    #        params['scale'] = 1 / params['lambda']\n    elif distribution is not 'norm':\n        raise AttributeError(\n            \"Unsupported distribution type. Please refer to Great Expectations Documentation\")\n\n    params['loc'] = params.get('loc', 0)\n    params['scale'] = params.get('scale', 1)\n\n    return params", "response": "Convenience method for determining the shape parameters of a given distribution from a list - like object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _scipy_distribution_positional_args_from_dict(distribution, params):\n\n    params['loc'] = params.get('loc', 0)\n    if 'scale' not in params:\n        params['scale'] = 1\n\n    if distribution == 'norm':\n        return params['mean'], params['std_dev']\n    elif distribution == 'beta':\n        return params['alpha'], params['beta'], params['loc'], params['scale']\n    elif distribution == 'gamma':\n        return params['alpha'], params['loc'], params['scale']\n    # elif distribution == 'poisson':\n    #    return params['lambda'], params['loc']\n    elif distribution == 'uniform':\n        return params['min'], params['max']\n    elif distribution == 'chi2':\n        return params['df'], params['loc'], params['scale']\n    elif distribution == 'expon':\n        return params['loc'], params['scale']", "response": "Helper function that returns the positional arguments for a scipy distribution using a dict of parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that necessary parameters for a given distribution are present and that all parameters are sensical.", "response": "def validate_distribution_parameters(distribution, params):\n    \"\"\"Ensures that necessary parameters for a distribution are present and that all parameters are sensical.\n\n       If parameters necessary to construct a distribution are missing or invalid, this function raises ValueError\\\n       with an informative description. Note that 'loc' and 'scale' are optional arguments, and that 'scale'\\\n       must be positive.\n\n       Args:\n           distribution (string): \\\n               The scipy distribution name, e.g. normal distribution is 'norm'.\n           params (dict or list): \\\n               The distribution shape parameters in a named dictionary or positional list form following the scipy \\\n               cdf argument scheme.\n\n               params={'mean': 40, 'std_dev': 5} or params=[40, 5]\n\n       Exceptions:\n           ValueError: \\\n               With an informative description, usually when necessary parameters are omitted or are invalid.\n\n    \"\"\"\n\n    norm_msg = \"norm distributions require 0 parameters and optionally 'mean', 'std_dev'.\"\n    beta_msg = \"beta distributions require 2 positive parameters 'alpha', 'beta' and optionally 'loc', 'scale'.\"\n    gamma_msg = \"gamma distributions require 1 positive parameter 'alpha' and optionally 'loc','scale'.\"\n    # poisson_msg = \"poisson distributions require 1 positive parameter 'lambda' and optionally 'loc'.\"\n    uniform_msg = \"uniform distributions require 0 parameters and optionally 'loc', 'scale'.\"\n    chi2_msg = \"chi2 distributions require 1 positive parameter 'df' and optionally 'loc', 'scale'.\"\n    expon_msg = \"expon distributions require 0 parameters and optionally 'loc', 'scale'.\"\n\n    if (distribution not in ['norm', 'beta', 'gamma', 'poisson', 'uniform', 'chi2', 'expon']):\n        raise AttributeError(\n            \"Unsupported  distribution provided: %s\" % distribution)\n\n    if isinstance(params, dict):\n        # `params` is a dictionary\n        if params.get(\"std_dev\", 1) <= 0 or params.get('scale', 1) <= 0:\n            raise ValueError(\"std_dev and scale must be positive.\")\n\n        # alpha and beta are required and positive\n        if distribution == 'beta' and (params.get('alpha', -1) <= 0 or params.get('beta', -1) <= 0):\n            raise ValueError(\"Invalid parameters: %s\" % beta_msg)\n\n        # alpha is required and positive\n        elif distribution == 'gamma' and params.get('alpha', -1) <= 0:\n            raise ValueError(\"Invalid parameters: %s\" % gamma_msg)\n\n        # lambda is a required and positive\n        # elif distribution == 'poisson' and params.get('lambda', -1) <= 0:\n        #    raise ValueError(\"Invalid parameters: %s\" %poisson_msg)\n\n        # df is necessary and required to be positve\n        elif distribution == 'chi2' and params.get('df', -1) <= 0:\n            raise ValueError(\"Invalid parameters: %s:\" % chi2_msg)\n\n    elif isinstance(params, tuple) or isinstance(params, list):\n        scale = None\n\n        # `params` is a tuple or a list\n        if distribution == 'beta':\n            if len(params) < 2:\n                raise ValueError(\"Missing required parameters: %s\" % beta_msg)\n            if params[0] <= 0 or params[1] <= 0:\n                raise ValueError(\"Invalid parameters: %s\" % beta_msg)\n            if len(params) == 4:\n                scale = params[3]\n            elif len(params) > 4:\n                raise ValueError(\"Too many parameters provided: %s\" % beta_msg)\n\n        elif distribution == 'norm':\n            if len(params) > 2:\n                raise ValueError(\"Too many parameters provided: %s\" % norm_msg)\n            if len(params) == 2:\n                scale = params[1]\n\n        elif distribution == 'gamma':\n            if len(params) < 1:\n                raise ValueError(\"Missing required parameters: %s\" % gamma_msg)\n            if len(params) == 3:\n                scale = params[2]\n            if len(params) > 3:\n                raise ValueError(\n                    \"Too many parameters provided: %s\" % gamma_msg)\n            elif params[0] <= 0:\n                raise ValueError(\"Invalid parameters: %s\" % gamma_msg)\n\n        # elif distribution == 'poisson':\n        #    if len(params) < 1:\n        #        raise ValueError(\"Missing required parameters: %s\" %poisson_msg)\n        #   if len(params) > 2:\n        #        raise ValueError(\"Too many parameters provided: %s\" %poisson_msg)\n        #    elif params[0] <= 0:\n        #        raise ValueError(\"Invalid parameters: %s\" %poisson_msg)\n\n        elif distribution == 'uniform':\n            if len(params) == 2:\n                scale = params[1]\n            if len(params) > 2:\n                raise ValueError(\n                    \"Too many arguments provided: %s\" % uniform_msg)\n\n        elif distribution == 'chi2':\n            if len(params) < 1:\n                raise ValueError(\"Missing required parameters: %s\" % chi2_msg)\n            elif len(params) == 3:\n                scale = params[2]\n            elif len(params) > 3:\n                raise ValueError(\"Too many arguments provided: %s\" % chi2_msg)\n            if params[0] <= 0:\n                raise ValueError(\"Invalid parameters: %s\" % chi2_msg)\n\n        elif distribution == 'expon':\n\n            if len(params) == 2:\n                scale = params[1]\n            if len(params) > 2:\n                raise ValueError(\"Too many arguments provided: %s\" % expon_msg)\n\n        if scale is not None and scale <= 0:\n            raise ValueError(\"std_dev and scale must be positive.\")\n\n    else:\n        raise ValueError(\n            \"params must be a dict or list, or use ge.dataset.util.infer_distribution_parameters(data, distribution)\")\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an identical expectation for each of the given columns with the specified arguments if any.", "response": "def create_multiple_expectations(df, columns, expectation_type, *args, **kwargs):\n    \"\"\"Creates an identical expectation for each of the given columns with the specified arguments, if any.\n\n    Args:\n        df (great_expectations.dataset): A great expectations dataset object.\n        columns (list): A list of column names represented as strings.\n        expectation_type (string): The expectation type.\n\n    Raises:\n        KeyError if the provided column does not exist.\n        AttributeError if the provided expectation type does not exist or df is not a valid great expectations dataset.\n\n    Returns:\n        A list of expectation results.\n\n\n    \"\"\"\n    expectation = getattr(df, expectation_type)\n    results = list()\n\n    for column in columns:\n        results.append(expectation(column, *args,  **kwargs))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef columns_exist(inspect_dataset):\n\n    # Attempt to get column names. For pandas, columns is just a list of strings\n    if not hasattr(inspect_dataset, \"columns\"):\n        warnings.warn(\n            \"No columns list found in dataset; no autoinspection performed.\")\n        return\n    elif isinstance(inspect_dataset.columns[0], string_types):\n        columns = inspect_dataset.columns\n    elif isinstance(inspect_dataset.columns[0], dict) and \"name\" in inspect_dataset.columns[0]:\n        columns = [col['name'] for col in inspect_dataset.columns]\n    else:\n        raise AutoInspectError(\n            \"Unable to determine column names for this dataset.\")\n\n    create_multiple_expectations(\n        inspect_dataset, columns, \"expect_column_to_exist\")", "response": "This function will take a dataset and add expectations that each column present exists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef column_aggregate_expectation(cls, func):\n        if PY3:\n            argspec = inspect.getfullargspec(func)[0][1:]\n        else:\n            argspec = inspect.getargspec(func)[0][1:]\n\n        @cls.expectation(argspec)\n        @wraps(func)\n        def inner_wrapper(self, column, result_format=None, *args, **kwargs):\n\n            if result_format is None:\n                result_format = self.default_expectation_args[\"result_format\"]\n\n            result_format = parse_result_format(result_format)\n\n            evaluation_result = func(self, column, *args, **kwargs)\n\n            if 'success' not in evaluation_result:\n                raise ValueError(\n                    \"Column aggregate expectation failed to return required information: success\")\n\n            if 'result' not in evaluation_result:\n                raise ValueError(\n                    \"Column aggregate expectation failed to return required information: result\")\n\n            if 'observed_value' not in evaluation_result['result']:\n                raise ValueError(\n                    \"Column aggregate expectation failed to return required information: result.observed_value\")\n\n            return_obj = {\n                'success': bool(evaluation_result['success'])\n            }\n\n            if result_format['result_format'] == 'BOOLEAN_ONLY':\n                return return_obj\n\n            # Use the element and null count information from a column_aggregate_expectation if it is needed\n            # it anyway to avoid an extra trip to the database\n\n            if 'element_count' not in evaluation_result and 'null_count' not in evaluation_result:\n                count_query = sa.select([\n                    sa.func.count().label('element_count'),\n                    sa.func.sum(\n                        sa.case([(sa.column(column) == None, 1)], else_=0)\n                    ).label('null_count'),\n                ]).select_from(self._table)\n\n                count_results = dict(\n                    self.engine.execute(count_query).fetchone())\n\n                # Handle case of empty table gracefully:\n                if \"element_count\" not in count_results or count_results[\"element_count\"] is None:\n                    count_results[\"element_count\"] = 0\n                if \"null_count\" not in count_results or count_results[\"null_count\"] is None:\n                    count_results[\"null_count\"] = 0\n\n                return_obj['result'] = {\n                    'observed_value': evaluation_result['result']['observed_value'],\n                    \"element_count\": count_results['element_count'],\n                    \"missing_count\": count_results['null_count'],\n                    \"missing_percent\": count_results['null_count'] / count_results['element_count'] if count_results['element_count'] > 0 else None\n                }\n            else:\n                return_obj['result'] = {\n                    'observed_value': evaluation_result['result']['observed_value'],\n                    \"element_count\": evaluation_result[\"element_count\"],\n                    \"missing_count\": evaluation_result[\"null_count\"],\n                    \"missing_percent\": evaluation_result['null_count'] / evaluation_result['element_count'] if evaluation_result['element_count'] > 0 else None\n                }\n\n            if result_format['result_format'] == 'BASIC':\n                return return_obj\n\n            if 'details' in evaluation_result['result']:\n                return_obj['result']['details'] = evaluation_result['result']['details']\n\n            if result_format['result_format'] in [\"SUMMARY\", \"COMPLETE\"]:\n                return return_obj\n\n            raise ValueError(\"Unknown result_format %s.\" %\n                             (result_format['result_format'],))\n\n        return inner_wrapper", "response": "Decorator for column - aggregate expectations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_temporary_table(self, table_name, custom_sql):\n        stmt = \"CREATE TEMPORARY TABLE {table_name} AS {custom_sql}\".format(\n            table_name=table_name, custom_sql=custom_sql)\n        self.engine.execute(stmt)", "response": "Create a temporary table based on sql query."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef column_reflection_fallback(self):\n        sql = sa.select([sa.text(\"*\")]).select_from(self._table)\n        col_names = self.engine.execute(sql).keys()\n        col_dict = [{'name': col_name} for col_name in col_names]\n        return col_dict", "response": "Fallback to getting column names from the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expect_table_columns_to_match_ordered_list(self, column_list,\n                                                   result_format=None, include_config=False, catch_exceptions=None, meta=None):\n        \"\"\"\n        Checks if observed columns are in the expected order. The expectations will fail if columns are out of expected\n        order, columns are missing, or additional columns are present. On failure, details are provided on the location\n        of the unexpected column(s).\n        \"\"\"\n        observed_columns = [col['name'] for col in self.columns]\n\n        if observed_columns == list(column_list):\n            return {\n                \"success\": True\n            }\n        else:\n            # In the case of differing column lengths between the defined expectation and the observed column set, the\n            # max is determined to generate the column_index.\n            number_of_columns = max(len(column_list), len(observed_columns))\n            column_index = range(number_of_columns)\n\n            # Create a list of the mismatched details\n            compared_lists = list(zip_longest(column_index, list(column_list), observed_columns))\n            mismatched = [{\"Expected Column Position\": i,\n                           \"Expected\": k,\n                           \"Found\": v} for i, k, v in compared_lists if k != v]\n            return {\n                \"success\": False,\n                \"details\": {\"mismatched\": mismatched}\n            }", "response": "Checks if the observed columns of the resource table are in the expected order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_result_format(result_format):\n    if isinstance(result_format, string_types):\n        result_format = {\n            'result_format': result_format,\n            'partial_unexpected_count': 20\n        }\n    else:\n        if 'partial_unexpected_count' not in result_format:\n            result_format['partial_unexpected_count'] = 20\n\n    return result_format", "response": "This is a simple helper utility that can be used to parse a string result_format into a dict format used\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _convert_to_dataset_class(df, dataset_class, expectations_config=None, autoinspect_func=None):\n    if expectations_config is not None:\n        # Cast the dataframe into the new class, and manually initialize expectations according to the provided configuration\n        df.__class__ = dataset_class\n        df._initialize_expectations(expectations_config)\n    else:\n        # Instantiate the new Dataset with default expectations\n        try:\n            df = dataset_class(df, autoinspect_func=autoinspect_func)\n        except:\n            raise NotImplementedError(\n                \"read_csv requires a Dataset class that can be instantiated from a Pandas DataFrame\")\n\n    return df", "response": "Convert a Pandas dataframe to a great_expectations dataset with optional expectations_config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a file using Pandas read_excel and return a great_expectations dataset.", "response": "def read_excel(\n    filename,\n    dataset_class=dataset.pandas_dataset.PandasDataset,\n    expectations_config=None,\n    autoinspect_func=None,\n    *args, **kwargs\n):\n    \"\"\"Read a file using Pandas read_excel and return a great_expectations dataset.\n\n    Args:\n        filename (string): path to file to read\n        dataset_class (Dataset class): class to which to convert resulting Pandas df\n        expectations_config (string): path to great_expectations config file\n\n    Returns:\n        great_expectations dataset or ordered dict of great_expectations datasets,\n        if multiple worksheets are imported\n    \"\"\"\n    df = pd.read_excel(filename, *args, **kwargs)\n    if isinstance(df, dict):\n        for key in df:\n            df[key] = _convert_to_dataset_class(\n                df[key], dataset_class, expectations_config, autoinspect_func)\n    else:\n        df = _convert_to_dataset_class(\n            df, dataset_class, expectations_config, autoinspect_func)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a Pandas data frame and return a great_expectations dataset.", "response": "def from_pandas(pandas_df, \n                dataset_class=dataset.pandas_dataset.PandasDataset,\n                expectations_config=None, \n                autoinspect_func=None\n):\n    \"\"\"Read a Pandas data frame and return a great_expectations dataset.\n\n    Args:\n        pandas_df (Pandas df): Pandas data frame\n        dataset_class (Dataset class) = dataset.pandas_dataset.PandasDataset:\n            class to which to convert resulting Pandas df\n        expectations_config (string) = None: path to great_expectations config file\n        autoinspect_func (function) = None: The autoinspection function that should \n            be run on the dataset to establish baseline expectations.\n\n    Returns:\n        great_expectations dataset\n    \"\"\"\n    return _convert_to_dataset_class(\n        pandas_df,\n        dataset_class,\n        expectations_config,\n        autoinspect_func\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs an expectation using file lines map semantics. The file_lines_map_expectations decorator handles boilerplate issues surrounding the common pattern of evaluating truthiness of some condition on an line by line basis in a file. Args: func (function): \\ The function implementing an expectation that will be applied line by line across a file. The function should take a file and return information about how many lines met expectations. Notes: Users can specify skip value k that will cause the expectation function to disregard the first k lines of the file. file_lines_map_expectation will add a kwarg _lines to the called function with the nonnull lines \\ to process. null_lines_regex defines a regex used to skip lines, but can be overridden See also: :func:`expect_file_line_regex_match_count_to_be_between <great_expectations.data_asset.base.DataAsset.expect_file_line_regex_match_count_to_be_between>` \\ for an example of a file_lines_map_expectation", "response": "def file_lines_map_expectation(cls, func):\n        \"\"\"Constructs an expectation using file lines map semantics.\n        The file_lines_map_expectations decorator handles boilerplate issues\n        surrounding the common pattern of evaluating truthiness of some\n        condition on an line by line basis in a file.\n\n        Args:\n            func (function): \\\n                The function implementing an expectation that will be applied\n                line by line across a file. The function should take a file\n                and return information about how many lines met expectations.\n\n        Notes:\n            Users can specify skip value k that will cause the expectation\n            function to disregard the first k lines of the file.\n\n            file_lines_map_expectation will add a kwarg _lines to the called function with the nonnull lines \\\n            to process.\n\n            null_lines_regex defines a regex used to skip lines, but can be overridden\n\n        See also:\n            :func:`expect_file_line_regex_match_count_to_be_between\n            <great_expectations.data_asset.base.DataAsset.expect_file_line_regex_match_count_to_be_between>` \\\n            for an example of a file_lines_map_expectation\n        \"\"\"\n        if PY3:\n            argspec = inspect.getfullargspec(func)[0][1:]\n        else:\n            argspec = inspect.getargspec(func)[0][1:]\n\n        @cls.expectation(argspec)\n        @wraps(func)\n        def inner_wrapper(self, skip=None, mostly=None, null_lines_regex=r\"^\\s*$\", result_format=None, *args, **kwargs):\n            try:\n                f = open(self._path, \"r\")\n            except:\n                raise\n\n            if result_format is None:\n                result_format = self.default_expectation_args[\"result_format\"]\n\n            result_format = parse_result_format(result_format)\n            lines = f.readlines() #Read in file lines\n\n            #Skip k initial lines designated by the user\n            if skip is not None and skip <= len(lines):\n                try:\n                    assert float(skip).is_integer()\n                    assert float(skip) >= 0\n                except:\n                    raise ValueError(\"skip must be a positive integer\")\n\n                for i in range(1, skip+1):\n                    lines.pop(0)\n\n            if lines:\n                if null_lines_regex is not None:\n                    null_lines = re.compile(null_lines_regex) #Ignore lines that are empty or have only white space (\"null values\" in the line-map context)\n                    boolean_mapped_null_lines = np.array(\n                        [bool(null_lines.match(line)) for line in lines])\n                else:\n                    boolean_mapped_null_lines = np.zeros(len(lines), dtype=bool)\n                element_count = int(len(lines))\n                if element_count > sum(boolean_mapped_null_lines):\n                    nonnull_lines = list(compress(lines, np.invert(boolean_mapped_null_lines)))\n                    nonnull_count = int((boolean_mapped_null_lines == False).sum())\n                    boolean_mapped_success_lines = np.array(\n                        func(self, _lines=nonnull_lines, *args, **kwargs))\n                    success_count = np.count_nonzero(boolean_mapped_success_lines)\n                    unexpected_list = list(compress(nonnull_lines, \\\n                        np.invert(boolean_mapped_success_lines)))\n                    nonnull_lines_index = range(0, len(nonnull_lines)+1)\n                    unexpected_index_list = list(compress(nonnull_lines_index,\n                                                          np.invert(boolean_mapped_success_lines)))\n                    success, percent_success = self._calc_map_expectation_success(\n                        success_count, nonnull_count, mostly)\n                    return_obj = self._format_map_output(\n                        result_format, success,\n                        element_count, nonnull_count,\n                        len(unexpected_list),\n                        unexpected_list, unexpected_index_list\n                    )\n                else:\n                    return_obj = self._format_map_output(\n                        result_format=result_format, success=None,\n                        element_count=element_count, nonnull_count=0,\n                        unexpected_count=0,\n                        unexpected_list=[], unexpected_index_list=[]\n                    )\n            else:\n                return_obj = self._format_map_output(result_format=result_format,\n                                                     success=None,\n                                                     element_count=0,\n                                                     nonnull_count=0,\n                                                     unexpected_count=0,\n                                                     unexpected_list=[],\n                                                     unexpected_index_list=[])\n            f.close()\n            return return_obj\n\n        inner_wrapper.__name__ = func.__name__\n        inner_wrapper.__doc__ = func.__doc__\n\n        return inner_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expect_file_line_regex_match_count_to_be_between(self,\n                                                         regex,\n                                                         expected_min_count=0,\n                                                         expected_max_count=None,\n                                                         skip=None,\n                                                         mostly=None,\n                                                         null_lines_regex=r\"^\\s*$\",\n                                                         result_format=None,\n                                                         include_config=False,\n                                                         catch_exceptions=None,\n                                                         meta=None,\n                                                         _lines=None):\n        \"\"\"\n        Expect the number of times a regular expression appears on each line of\n        a file to be between a maximum and minimum value.\n\n        Args: \n            regex: \\\n                A string that can be compiled as valid regular expression to match\n\n            expected_min_count (None or nonnegative integer): \\\n                Specifies the minimum number of times regex is expected to appear\n                on each line of the file\n\n            expected_max_count (None or nonnegative integer): \\\n               Specifies the maximum number of times regex is expected to appear\n               on each line of the file\n        Keyword Args:\n            skip (None or nonnegative integer): \\\n                Integer specifying the first lines in the file the method should\n                skip before assessing expectations\n\n            mostly (None or number between 0 and 1): \\\n                Specifies an acceptable error for expectations. If the percentage\n                of unexpected lines is less than mostly, the method still returns\n                true even if all lines don't match the expectation criteria.\n\n            null_lines_regex (valid regular expression or None): \\\n                If not none, a regex to skip lines as null. Defaults to empty or whitespace-only lines.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`,\n                or `SUMMARY`. For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the\n                result object. For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the\n                result object. For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be\n                included in the output without modification. For more detail,\n                see :ref:`meta`.\n            _lines (list): \\\n                The lines over which to operate (provided by the file_lines_map_expectation decorator)\n\n        Returns:\n\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to\n            :ref:`result_format <result_format>` and :ref:`include_config`,\n            :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        try:\n            comp_regex = re.compile(regex)\n        except:\n            raise ValueError(\"Must enter valid regular expression for regex\")\n\n        if expected_min_count != None:\n            try:\n                assert float(expected_min_count).is_integer()\n                assert float(expected_min_count) >= 0\n            except:\n                raise ValueError(\"expected_min_count must be a non-negative \\\n                                 integer or None\")\n\n        if expected_max_count != None:\n            try:\n                assert float(expected_max_count).is_integer()\n                assert float(expected_max_count) >= 0\n            except:\n                raise ValueError(\"expected_max_count must be a non-negative \\\n                                 integer or None\")\n\n        if expected_max_count != None and expected_min_count != None:\n            try:\n                assert expected_max_count >= expected_min_count\n            except:\n                raise ValueError(\"expected_max_count must be greater than or \\\n                                 equal to expected_min_count\")\n\n        if expected_max_count != None and expected_min_count != None:\n            truth_list = [True if(len(comp_regex.findall(line)) >= expected_min_count and \\\n                                len(comp_regex.findall(line)) <= expected_max_count) else False \\\n                                for line in _lines]\n\n        elif expected_max_count != None:\n            truth_list = [True if(len(comp_regex.findall(line)) <= expected_max_count) else False \\\n                                for line in _lines]\n\n        elif expected_min_count != None:\n              truth_list = [True if(len(comp_regex.findall(line)) >= expected_min_count) else False \\\n                                for line in _lines]\n        else:\n            truth_list = [True for line in _lines]\n\n        return truth_list", "response": "This method checks the number of times a regular expression appears on each line of the file and expect the number of times that the file line appears to be between a minimum and a maximum value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expect_file_line_regex_match_count_to_equal(self, regex, \n                                                    expected_count=0, \n                                                    skip=None,\n                                                    mostly=None, \n                                                    nonnull_lines_regex=r\"^\\s*$\",\n                                                    result_format=None,\n                                                    include_config=False,\n                                                    catch_exceptions=None,\n                                                    meta=None,\n                                                    _lines=None):\n\n        \"\"\"\n        Expect the number of times a regular expression appears on each line of\n        a file to be between a maximum and minimum value.\n\n        Args: \n            regex: \\\n                A string that can be compiled as valid regular expression to match\n\n            expected_count (None or nonnegative integer): \\\n                Specifies the number of times regex is expected to appear on each\n                line of the file\n        Keyword Args:\n            skip (None or nonnegative integer): \\\n                Integer specifying the first lines in the file the method should\n                skip before assessing expectations\n\n            mostly (None or number between 0 and 1): \\\n                Specifies an acceptable error for expectations. If the percentage\n                of unexpected lines is less than mostly, the method still returns\n                true even if all lines don't match the expectation criteria.\n\n            null_lines_regex (valid regular expression or None): \\\n                If not none, a regex to skip lines as null. Defaults to empty or whitespace-only lines.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`,\n                or `SUMMARY`. For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the\n                result object. For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the\n                result object. For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be\n                included in the output without modification. For more detail,\n                see :ref:`meta`.\n            _lines (list): \\\n                The lines over which to operate (provided by the file_lines_map_expectation decorator)\n\n        Returns:\n\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to\n            :ref:`result_format <result_format>` and :ref:`include_config`,\n            :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        try:\n            comp_regex = re.compile(regex)\n        except:\n            raise ValueError(\"Must enter valid regular expression for regex\")\n\n        try:\n            assert float(expected_count).is_integer()\n            assert float(expected_count) >= 0\n\n        except:\n            raise ValueError(\"expected_count must be a non-negative integer\")\n\n        return [True if(len(comp_regex.findall(line)) == expected_count) else False \\\n                                for line in _lines]", "response": "This method checks that the number of lines in a file line matches the expected number of lines."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_file_hash_to_equal(self, value, hash_alg='md5', result_format=None,\n                                  include_config=False, catch_exceptions=None,\n                                  meta=None):\n\n        \"\"\"\n        Expect computed file hash to equal some given value.\n\n        Args:\n            value: A string to compare with the computed hash value\n\n        Keyword Args:\n            hash_alg (string):  Indicates the hash algorithm to use\n\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`,\n                or `SUMMARY`. For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the\n                result object. For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be\n                included in the output without modification. For more detail,\n                see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n        Exact fields vary depending on the values passed to :ref:`result_format\n        <result_format>` and :ref:`include_config`, :ref:`catch_exceptions`,\n        and :ref:`meta`.\n        \"\"\"\n        success = False\n        try:\n            hash = hashlib.new(hash_alg)\n\n        # Limit file reads to 64 KB chunks at a time\n            BLOCKSIZE = 65536\n            try:\n                with open(self._path, 'rb') as file:\n                    file_buffer = file.read(BLOCKSIZE)\n                    while file_buffer:\n                        hash.update(file_buffer)\n                        file_buffer = file.read(BLOCKSIZE)\n                    success = hash.hexdigest() == value\n            except IOError:\n                raise\n        except ValueError:\n            raise\n        return {\"success\":success}", "response": "Expects the computed file hash to equal some given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpects file size to be between a user specified maxsize and minsize.", "response": "def expect_file_size_to_be_between(self, minsize=0, maxsize=None, result_format=None,\n                                       include_config=False, catch_exceptions=None,\n                                       meta=None):\n\n        \"\"\"\n        Expect file size to be between a user specified maxsize and minsize.\n\n        Args:\n            minsize(integer): minimum expected file size\n            maxsize(integer): maximum expected file size\n\n        Keyword Args:\n\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be\n                included in the output without modification. For more detail,\n                see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n        Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n        :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n        \"\"\"\n\n        try:\n            size = os.path.getsize(self._path)\n        except OSError:\n            raise\n\n        # We want string or float or int versions of numbers, but\n        # they must be representable as clean integers.\n        try:\n            if not float(minsize).is_integer():\n                raise ValueError('minsize must be an integer')\n            minsize = int(float(minsize))\n\n            if maxsize is not None and not float(maxsize).is_integer():\n                raise ValueError('maxsize must be an integer')\n            elif maxsize is not None:\n                maxsize = int(float(maxsize))\n        except TypeError:\n            raise\n\n        if minsize < 0:\n            raise ValueError('minsize must be greater than or equal to 0')\n\n        if maxsize is not None and maxsize < 0:\n            raise ValueError('maxsize must be greater than or equal to 0')\n\n        if maxsize is not None and minsize > maxsize:\n            raise ValueError('maxsize must be greater than or equal to minsize')\n\n        if maxsize is None and size >= minsize:\n            success = True\n        elif (size >= minsize) and (size <= maxsize):\n            success = True\n        else: \n            success = False\n\n        return {\n            \"success\": success,\n            \"details\": {\n                \"filesize\": size\n            }\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks to see if a file specified by the user actually exists in the object.", "response": "def expect_file_to_exist(self, filepath=None, result_format=None, include_config=False,\n                             catch_exceptions=None, meta=None):\n\n        \"\"\"\n        Checks to see if a file specified by the user actually exists\n\n        Args:\n            filepath (str or None): \\\n                The filepath to evalutate. If none, will check the currently-configured path object\n                of this FileDataAsset.\n\n        Keyword Args:\n\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be\n                included in the output without modification. For more detail,\n                see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n        Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n        :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n        \"\"\"\n\n        if filepath is not None and os.path.isfile(filepath):\n            success = True\n        elif self._path is not None and os.path.isfile(self._path):\n            success = True\n        else:\n            success = False\n\n        return {\"success\":success}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expect_file_to_have_valid_table_header(self, regex, skip=None,\n                                               result_format=None,\n                                               include_config=False,\n                                               catch_exceptions=None, meta=None):\n        \"\"\"\n        Checks to see if a file has a line with unique delimited values,\n        such a line may be used as a table header.\n\n        Keyword Args:\n            skip (nonnegative integer): \\\n                Integer specifying the first lines in the file the method\n                should skip before assessing expectations\n\n            regex (string):\n                A string that can be compiled as valid regular expression.\n                Used to specify the elements of the table header (the column headers)\n\n            result_format (str or None):\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be\n                included in the output without modification. For more detail,\n                see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n        Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n        :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n        \"\"\"\n\n        try:\n            comp_regex = re.compile(regex)\n        except:\n            raise ValueError(\"Must enter valid regular expression for regex\")\n\n        success = False\n\n        try:\n            with open(self._path, 'r') as f:\n                lines = f.readlines() #Read in file lines\n\n        except IOError:\n            raise\n\n            #Skip k initial lines designated by the user\n        if skip is not None and skip <= len(lines):\n            try:\n                assert float(skip).is_integer()\n                assert float(skip) >= 0\n            except:\n                raise ValueError(\"skip must be a positive integer\")\n\n            lines = lines[skip:]\n\n        header_line = lines[0].strip()\n        header_names = comp_regex.split(header_line)\n        if len(set(header_names)) == len(header_names):\n            success = True\n\n        return {\"success\":success}", "response": "Checks to see if a file has a valid table header."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expect_file_to_be_valid_json(self, schema=None, result_format=None,\n                                     include_config=False, catch_exceptions=None,\n                                     meta=None):\n\n        \"\"\"\n        schema : string\n            optional JSON schema file on which JSON data file is validated against\n\n        result_format (str or None):\n            Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n            For more detail, see :ref:`result_format <result_format>`.\n\n        include_config (boolean):\n            If True, then include the expectation config as part of the result object. \\\n            For more detail, see :ref:`include_config`.\n\n        catch_exceptions (boolean or None):\n            If True, then catch exceptions and include them as part of the result object. \\\n            For more detail, see :ref:`catch_exceptions`.\n\n        meta (dict or None):\n            A JSON-serializable dictionary (nesting allowed) that will\n            be included in the output without modification. \\\n\n        For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n        Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n        :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n        \"\"\"\n        success = False\n        if schema is None:\n            try:\n                with open(self._path, 'r') as f:\n                    json.load(f)\n                success = True\n            except ValueError:\n                success = False\n        else:\n            try:\n                with open(schema, 'r') as s:\n                    schema_data = s.read()\n                sdata = json.loads(schema_data)\n                with open(self._path, 'r') as f:\n                    json_data = f.read()\n                jdata = json.loads(json_data)\n                jsonschema.validate(jdata, sdata)\n                success = True\n            except jsonschema.ValidationError:\n                success = False\n            except jsonschema.SchemaError:\n                raise\n            except:\n                raise\n        return {\"success\":success}", "response": "Expects the file to be JSON - serializable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_data_context(context_type, options, *args, **kwargs):\n    if context_type == \"SqlAlchemy\":\n        return SqlAlchemyDataContext(options, *args, **kwargs)\n    elif context_type == \"PandasCSV\":\n        return PandasCSVDataContext(options, *args, **kwargs)\n    else:\n        raise ValueError(\"Unknown data context.\")", "response": "Return a new DataContext object which exposes options to list datasets and get a dataset from a node in the node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride data_asset_type with Dataset", "response": "def _initialize_expectations(self, config=None, data_asset_name=None):\n        \"\"\"Override data_asset_type with \"Dataset\"\n        \"\"\"\n        super(Dataset, self)._initialize_expectations(config=config, data_asset_name=data_asset_name)\n        self._expectations_config[\"data_asset_type\"] = \"Dataset\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpecting the specified column to exist.", "response": "def expect_column_to_exist(\n        self, column, column_index=None, result_format=None, include_config=False,\n        catch_exceptions=None, meta=None\n    ):\n        \"\"\"Expect the specified column to exist.\n\n        expect_column_to_exist is a :func:`expectation <great_expectations.data_asset.dataset.Dataset.expectation>`, not a \\\n        `column_map_expectation` or `column_aggregate_expectation`.\n\n        Args:\n            column (str): \\\n                The column name.\n\n        Other Parameters:\n            column_index (int or None): \\\n                If not None, checks the order of the columns. The expectation will fail if the \\\n                column is not in location column_index (zero-indexed).\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expect_table_row_count_to_be_between(self,\n                                             min_value=0,\n                                             max_value=None,\n                                             result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                             ):\n        \"\"\"Expect the number of rows to be between two values.\n\n        expect_table_row_count_to_be_between is a :func:`expectation <great_expectations.data_asset.dataset.Dataset.expectation>`, \\\n        not a `column_map_expectation` or `column_aggregate_expectation`.\n\n        Keyword Args:\n            min_value (int or None): \\\n                The minimum number of rows, inclusive.\n            max_value (int or None): \\\n                The maximum number of rows, inclusive.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            * min_value and max_value are both inclusive.\n            * If min_value is None, then max_value is treated as an upper bound, and the number of acceptable rows has no minimum.\n            * If max_value is None, then min_value is treated as a lower bound, and the number of acceptable rows has no maximum.\n\n        See Also:\n            expect_table_row_count_to_equal\n        \"\"\"\n        raise NotImplementedError", "response": "Expect the number of rows to be between two values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexpect each column entry to be of a specified data type.", "response": "def expect_column_values_to_be_of_type(\n        self,\n        column,\n        type_,\n        mostly=None,\n        result_format=None, include_config=False, catch_exceptions=None, meta=None\n    ):\n        \"\"\"Expect each column entry to be a specified data type.\n\n        expect_column_values_to_be_of_type is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            type\\_ (str): \\\n                A string representing the data type that each column should have as entries.\n                For example, \"double integer\" refers to an integer with double precision.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Warning:\n            expect_column_values_to_be_of_type is slated for major changes in future versions of great_expectations.\n\n            As of v0.3, great_expectations is exclusively based on pandas, which handles typing in its own peculiar way.\n            Future versions of great_expectations will allow for Datasets in SQL, spark, etc.\n            When we make that change, we expect some breaking changes in parts of the codebase that are based strongly on pandas notions of typing.\n\n        See also:\n            expect_column_values_to_be_in_type_list\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expect_column_values_to_be_in_type_list(\n        self,\n        column,\n        type_list,\n        mostly=None,\n        result_format=None, include_config=False, catch_exceptions=None, meta=None\n    ):\n        \"\"\"Expect each column entry to match a list of specified data types.\n\n        expect_column_values_to_be_in_type_list is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            type_list (list of str): \\\n                A list of strings representing the data type that each column should have as entries.\n                For example, \"double integer\" refers to an integer with double precision.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Warning:\n            expect_column_values_to_be_in_type_list is slated for major changes in future versions of great_expectations.\n\n            As of v0.3, great_expectations is exclusively based on pandas, which handles typing in its own peculiar way.\n            Future versions of great_expectations will allow for Datasets in SQL, spark, etc.\n            When we make that change, we expect some breaking changes in parts of the codebase that are based strongly on pandas notions of typing.\n\n        See also:\n            expect_column_values_to_be_of_type\n        \"\"\"\n        raise NotImplementedError", "response": "Expect each column entry to be in a list of data types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpecting each column value to be in a given set.", "response": "def expect_column_values_to_be_in_set(self,\n                                          column,\n                                          value_set,\n                                          mostly=None,\n                                          parse_strings_as_datetimes=None,\n                                          result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                          ):\n        \"\"\"Expect each column value to be in a given set.\n\n        For example:\n        ::\n\n            # my_df.my_col = [1,2,2,3,3,3]\n            >>> my_df.expect_column_values_to_be_in_set(\n                \"my_col\",\n                [2,3]\n            )\n            {\n              \"success\": false\n              \"result\": {\n                \"unexpected_count\": 1\n                \"unexpected_percent\": 0.16666666666666666,\n                \"unexpected_percent_nonmissing\": 0.16666666666666666,\n                \"partial_unexpected_list\": [\n                  1\n                ],\n              },\n            }\n\n        expect_column_values_to_be_in_set is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n\n        Args:\n            column (str): \\\n                The column name.\n            value_set (set-like): \\\n                A set of objects used for comparison.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n            parse_strings_as_datetimes (boolean or None) : If True values provided in value_set will be parsed as \\\n                datetimes before making comparisons.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_values_to_not_be_in_set\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expect_column_values_to_be_between(self,\n                                           column,\n                                           min_value=None,\n                                           max_value=None,\n                                           allow_cross_type_comparisons=None,\n                                           parse_strings_as_datetimes=None,\n                                           output_strftime_format=None,\n                                           mostly=None,\n                                           result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                           ):\n        \"\"\"Expect column entries to be between a minimum value and a maximum value (inclusive).\n\n        expect_column_values_to_be_between is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            min_value (comparable type or None): The minimum value for a column entry.\n            max_value (comparable type or None): The maximum value for a column entry.\n\n        Keyword Args:\n            allow_cross_type_comparisons (boolean or None) : If True, allow comparisons between types (e.g. integer and\\\n                string). Otherwise, attempting such comparisons will raise an exception.\n            parse_strings_as_datetimes (boolean or None) : If True, parse min_value, max_value, and all non-null column\\\n                values to datetimes before making comparisons.\n            output_strftime_format (str or None): \\\n                A valid strfime format for datetime output. Only used if parse_strings_as_datetimes=True.\n\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            * min_value and max_value are both inclusive.\n            * If min_value is None, then max_value is treated as an upper bound, and the number of acceptable rows has no minimum.\n            * If max_value is None, then min_value is treated as a lower bound, and the number of acceptable rows has no maximum.\n\n        See Also:\n            expect_column_value_lengths_to_be_between\n\n        \"\"\"\n        raise NotImplementedError", "response": "Expect column entries to be between a minimum and a maximum value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpect column values to be decreasing.", "response": "def expect_column_values_to_be_decreasing(self,\n                                              column,\n                                              strictly=None,\n                                              parse_strings_as_datetimes=None,\n                                              mostly=None,\n                                              result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                              ):\n        \"\"\"Expect column values to be decreasing.\n\n        By default, this expectation only works for numeric or datetime data.\n        When `parse_strings_as_datetimes=True`, it can also parse strings to datetimes.\n\n        If `strictly=True`, then this expectation is only satisfied if each consecutive value\n        is strictly decreasing--equal values are treated as failures.\n\n        expect_column_values_to_be_decreasing is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n\n        Keyword Args:\n            strictly (Boolean or None): \\\n                If True, values must be strictly greater than previous values\n            parse_strings_as_datetimes (boolean or None) : \\\n                If True, all non-null column values to datetimes before making comparisons\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_values_to_be_increasing\n\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexpecting column entries to be strings with length between a minimum value and a maximum value (inclusive). This expectation only works for string-type values. Invoking it on ints or floats will raise a TypeError. expect_column_value_lengths_to_be_between is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`. Args: column (str): \\ The column name. Keyword Args: min_value (int or None): \\ The minimum value for a column entry length. max_value (int or None): \\ The maximum value for a column entry length. mostly (None or a float between 0 and 1): \\ Return `\"success\": True` if at least mostly percent of values match the expectation. \\ For more detail, see :ref:`mostly`. Other Parameters: result_format (str or None): \\ Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`. For more detail, see :ref:`result_format <result_format>`. include_config (boolean): \\ If True, then include the expectation config as part of the result object. \\ For more detail, see :ref:`include_config`. catch_exceptions (boolean or None): \\ If True, then catch exceptions and include them as part of the result object. \\ For more detail, see :ref:`catch_exceptions`. meta (dict or None): \\ A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\ For more detail, see :ref:`meta`. Returns: A JSON-serializable expectation result object. Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`. Notes: * min_value and max_value are both inclusive. * If min_value is None, then max_value is treated as an upper bound, and the number of acceptable rows has no minimum. * If max_value is None, then min_value is treated as a lower bound, and the number of acceptable rows has no maximum. See Also: expect_column_value_lengths_to_equal", "response": "def expect_column_value_lengths_to_be_between(self,\n                                                  column,\n                                                  min_value=None,\n                                                  max_value=None,\n                                                  mostly=None,\n                                                  result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                  ):\n        \"\"\"Expect column entries to be strings with length between a minimum value and a maximum value (inclusive).\n\n        This expectation only works for string-type values. Invoking it on ints or floats will raise a TypeError.\n\n        expect_column_value_lengths_to_be_between is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n\n        Keyword Args:\n            min_value (int or None): \\\n                The minimum value for a column entry length.\n            max_value (int or None): \\\n                The maximum value for a column entry length.\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            * min_value and max_value are both inclusive.\n            * If min_value is None, then max_value is treated as an upper bound, and the number of acceptable rows has no minimum.\n            * If max_value is None, then min_value is treated as a lower bound, and the number of acceptable rows has no maximum.\n\n        See Also:\n            expect_column_value_lengths_to_equal\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpects column entries to be strings with length equal to the provided value.", "response": "def expect_column_value_lengths_to_equal(self,\n                                             column,\n                                             value,\n                                             mostly=None,\n                                             result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                             ):\n        \"\"\"Expect column entries to be strings with length equal to the provided value.\n\n        This expectation only works for string-type values. Invoking it on ints or floats will raise a TypeError.\n\n        expect_column_values_to_be_between is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            value (int or None): \\\n                The expected value for a column entry length.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_value_lengths_to_be_between\n        \"\"\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_column_values_to_match_regex(self,\n                                            column,\n                                            regex,\n                                            mostly=None,\n                                            result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                            ):\n        \"\"\"Expect column entries to be strings that match a given regular expression. Valid matches can be found \\\n        anywhere in the string, for example \"[at]+\" will identify the following strings as expected: \"cat\", \"hat\", \\\n        \"aa\", \"a\", and \"t\", and the following strings as unexpected: \"fish\", \"dog\".\n\n        expect_column_values_to_match_regex is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            regex (str): \\\n                The regular expression the column entries should match.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_values_to_not_match_regex\n            expect_column_values_to_match_regex_list\n        \"\"\"\n        raise NotImplementedError", "response": "Expect column entries to be strings that match a given regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexpect the column entries to be strings that can be matched to any of the regular expressions in the list of regular expressions.", "response": "def expect_column_values_to_match_regex_list(self,\n                                                 column,\n                                                 regex_list,\n                                                 match_on=\"any\",\n                                                 mostly=None,\n                                                 result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                 ):\n        \"\"\"Expect the column entries to be strings that can be matched to either any of or all of a list of regular expressions.\n        Matches can be anywhere in the string.\n\n        expect_column_values_to_match_regex_list is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            regex_list (list): \\\n                The list of regular expressions which the column entries should match\n\n        Keyword Args:\n            match_on= (string): \\\n                \"any\" or \"all\".\n                Use \"any\" if the value should match at least one regular expression in the list.\n                Use \"all\" if it should match each regular expression in the list.\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_values_to_match_regex\n            expect_column_values_to_not_match_regex\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpect the column entries to be strings that do not match any of the regular expressions in the list regex_list.", "response": "def expect_column_values_to_not_match_regex_list(self, column, regex_list,\n                                                     mostly=None,\n                                                     result_format=None, include_config=False, catch_exceptions=None, meta=None):\n        \"\"\"Expect the column entries to be strings that do not match any of a list of regular expressions. Matches can \\\n        be anywhere in the string.\n\n\n        expect_column_values_to_not_match_regex_list is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            regex_list (list): \\\n                The list of regular expressions which the column entries should not match\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_values_to_match_regex_list\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expect_column_values_to_match_strftime_format(self,\n                                                      column,\n                                                      strftime_format,\n                                                      mostly=None,\n                                                      result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                      ):\n        \"\"\"Expect column entries to be strings representing a date or time with a given format.\n\n        expect_column_values_to_match_strftime_format is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            strftime_format (str): \\\n                A strftime format string to use for matching\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        raise NotImplementedError", "response": "Expect column entries to be strings representing a date or time with a given strftime format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpecting column entries to be parseable using dateutil.", "response": "def expect_column_values_to_be_dateutil_parseable(self,\n                                                      column,\n                                                      mostly=None,\n                                                      result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                      ):\n        \"\"\"Expect column entries to be parseable using dateutil.\n\n        expect_column_values_to_be_dateutil_parseable is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_column_values_to_match_json_schema(self,\n                                                  column,\n                                                  json_schema,\n                                                  mostly=None,\n                                                  result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                  ):\n        \"\"\"Expect column entries to be JSON objects matching a given JSON schema.\n\n        expect_column_values_to_match_json_schema is a :func:`column_map_expectation <great_expectations.data_asset.dataset.Dataset.column_map_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n\n        Keyword Args:\n            mostly (None or a float between 0 and 1): \\\n                Return `\"success\": True` if at least mostly percent of values match the expectation. \\\n                For more detail, see :ref:`mostly`.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        See Also:\n            expect_column_values_to_be_json_parseable\n\n            The JSON-schema docs at: http://json-schema.org/\n        \"\"\"\n        raise NotImplementedError", "response": "Expect column entries to be JSON objects matching a given JSON schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_column_mean_to_be_between(self,\n                                         column,\n                                         min_value=None,\n                                         max_value=None,\n                                         result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                         ):\n        \"\"\"Expect the column mean to be between a minimum value and a maximum value (inclusive).\n\n        expect_column_mean_to_be_between is a :func:`column_aggregate_expectation <great_expectations.data_asset.dataset.Dataset.column_aggregate_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            min_value (float or None): \\\n                The minimum value for the column mean.\n            max_value (float or None): \\\n                The maximum value for the column mean.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            These fields in the result object are customized for this expectation:\n            ::\n\n                {\n                    \"observed_value\": (float) The true mean for the column\n                }\n\n            * min_value and max_value are both inclusive.\n            * If min_value is None, then max_value is treated as an upper bound.\n            * If max_value is None, then min_value is treated as a lower bound.\n\n        See Also:\n            expect_column_median_to_be_between\n            expect_column_stdev_to_be_between\n        \"\"\"\n        raise NotImplementedError", "response": "Expect the column mean to be between a minimum and a maximum value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexpects the most common value to be within the designated set.", "response": "def expect_column_most_common_value_to_be_in_set(self,\n                                                     column,\n                                                     value_set,\n                                                     ties_okay=None,\n                                                     result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                     ):\n        \"\"\"Expect the most common value to be within the designated value set\n\n        expect_column_most_common_value_to_be_in_set is a :func:`column_aggregate_expectation <great_expectations.data_asset.dataset.Dataset.column_aggregate_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name\n            value_set (set-like): \\\n                A list of potential values to match\n\n        Keyword Args:\n            ties_okay (boolean or None): \\\n                If True, then the expectation will still succeed if values outside the designated set are as common (but not more common) than designated values\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            These fields in the result object are customized for this expectation:\n            ::\n\n                {\n                    \"observed_value\": (list) The most common values in the column\n                }\n\n            `observed_value` contains a list of the most common values.\n            Often, this will just be a single element. But if there's a tie for most common among multiple values,\n            `observed_value` will contain a single copy of each most common value.\n\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpecting the column to sum to be between min and max value.", "response": "def expect_column_min_to_be_between(self,\n                                        column,\n                                        min_value=None,\n                                        max_value=None,\n                                        parse_strings_as_datetimes=None,\n                                        output_strftime_format=None,\n                                        result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                        ):\n        \"\"\"Expect the column to sum to be between an min and max value\n\n        expect_column_min_to_be_between is a :func:`column_aggregate_expectation <great_expectations.data_asset.dataset.Dataset.column_aggregate_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name\n            min_value (comparable type or None): \\\n                The minimum number of unique values allowed.\n            max_value (comparable type or None): \\\n                The maximum number of unique values allowed.\n\n        Keyword Args:\n            parse_strings_as_datetimes (Boolean or None): \\\n                If True, parse min_value, max_values, and all non-null column values to datetimes before making comparisons.\n            output_strftime_format (str or None): \\\n                A valid strfime format for datetime output. Only used if parse_strings_as_datetimes=True.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            These fields in the result object are customized for this expectation:\n            ::\n\n                {\n                    \"observed_value\": (list) The actual column min\n                }\n\n\n            * min_value and max_value are both inclusive.\n            * If min_value is None, then max_value is treated as an upper bound\n            * If max_value is None, then min_value is treated as a lower bound\n\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expect_column_kl_divergence_to_be_less_than(self,\n                                                    column,\n                                                    partition_object=None,\n                                                    threshold=None,\n                                                    tail_weight_holdout=0,\n                                                    internal_weight_holdout=0,\n                                                    result_format=None, include_config=False, catch_exceptions=None, meta=None):\n        \"\"\"Expect the Kulback-Leibler (KL) divergence (relative entropy) of the specified column with respect to the \\\n        partition object to be lower than the provided threshold.\n\n        KL divergence compares two distributions. The higher the divergence value (relative entropy), the larger the \\\n        difference between the two distributions. A relative entropy of zero indicates that the data are \\\n        distributed identically, `when binned according to the provided partition`.\n\n        In many practical contexts, choosing a value between 0.5 and 1 will provide a useful test.\n\n        This expectation works on both categorical and continuous partitions. See notes below for details.\n\n        expect_column_kl_divergence_to_be_less_than is a :func:`column_aggregate_expectation <great_expectations.data_asset.dataset.Dataset.column_aggregate_expectation>`.\n\n        Args:\n            column (str): \\\n                The column name.\n            partition_object (dict): \\\n                The expected partition object (see :ref:`partition_object`).\n            threshold (float): \\\n                The maximum KL divergence to for which to return `success=True`. If KL divergence is larger than the\\\n                provided threshold, the test will return `success=False`.\n\n        Keyword Args:\n            internal_weight_holdout (float between 0 and 1 or None): \\\n                The amount of weight to split uniformly among zero-weighted partition bins. internal_weight_holdout \\\n                provides a mechanims to make the test less strict by assigning positive weights to values observed in \\\n                the data for which the partition explicitly expected zero weight. With no internal_weight_holdout, \\\n                any value observed in such a region will cause KL divergence to rise to +Infinity.\\\n                Defaults to 0.\n            tail_weight_holdout (float between 0 and 1 or None): \\\n                The amount of weight to add to the tails of the histogram. Tail weight holdout is split evenly between\\\n                (-Infinity, min(partition_object['bins'])) and (max(partition_object['bins']), +Infinity). \\\n                tail_weight_holdout provides a mechanism to make the test less strict by assigning positive weights to \\\n                values observed in the data that are not present in the partition. With no tail_weight_holdout, \\\n                any value observed outside the provided partition_object will cause KL divergence to rise to +Infinity.\\\n                Defaults to 0.\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        Notes:\n            These fields in the result object are customized for this expectation:\n            ::\n\n                {\n                  \"observed_value\": (float) The true KL divergence (relative entropy) or None if the value is calculated \\\n                  as infinity, -infinity, or NaN\n                  \"details\": {\n                    \"observed_partition\": (dict) The partition observed in the data\n                    \"expected_partition\": (dict) The partition against which the data were compared,\n                                            after applying specified weight holdouts.\n                  }\n                }\n\n            If the partition_object is categorical, this expectation will expect the values in column to also be \\\n            categorical.\n\n                * If the column includes values that are not present in the partition, the tail_weight_holdout will be \\\n                equally split among those values, providing a mechanism to weaken the strictness of the expectation \\\n                (otherwise, relative entropy would immediately go to infinity).\n                * If the partition includes values that are not present in the column, the test will simply include \\\n                zero weight for that value.\n\n            If the partition_object is continuous, this expectation will discretize the values in the column according \\\n            to the bins specified in the partition_object, and apply the test to the resulting distribution.\n\n                * The internal_weight_holdout and tail_weight_holdout parameters provide a mechanism to weaken the \\\n                expectation, since an expected weight of zero would drive relative entropy to be infinite if any data \\\n                are observed in that interval.\n                * If internal_weight_holdout is specified, that value will be distributed equally among any intervals \\\n                with weight zero in the partition_object.\n                * If tail_weight_holdout is specified, that value will be appended to the tails of the bins \\\n                ((-Infinity, min(bins)) and (max(bins), Infinity).\n\n          If relative entropy/kl divergence goes to infinity for any of the reasons mentioned above, the observed value\\\n          will be set to None. This is because inf, -inf, Nan, are not json serializable and cause some json parsers to\\\n          crash when encountered. The python None token will be serialized to null in json. \n\n        See also:\n            expect_column_chisquare_test_p_value_to_be_greater_than\n            expect_column_bootstrapped_ks_test_p_value_to_be_greater_than\n\n        \"\"\"\n        raise NotImplementedError", "response": "Expect the KL divergence of the specified column to be less than the provided threshold."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpect the values in column A to be the same as column B.", "response": "def expect_column_pair_values_to_be_equal(self,\n                                              column_A,\n                                              column_B,\n                                              ignore_row_if=\"both_values_are_missing\",\n                                              result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                              ):\n        \"\"\"\n        Expect the values in column A to be the same as column B.\n\n        Args:\n            column_A (str): The first column name\n            column_B (str): The second column name\n\n        Keyword Args:\n            ignore_row_if (str): \"both_values_are_missing\", \"either_value_is_missing\", \"neither\"\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expect_column_pair_values_A_to_be_greater_than_B(self,\n                                                         column_A,\n                                                         column_B,\n                                                         or_equal=None,\n                                                         parse_strings_as_datetimes=None,\n                                                         allow_cross_type_comparisons=None,\n                                                         ignore_row_if=\"both_values_are_missing\",\n                                                         result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                                         ):\n        \"\"\"\n        Expect values in column A to be greater than column B.\n\n        Args:\n            column_A (str): The first column name\n            column_B (str): The second column name\n            or_equal (boolean or None): If True, then values can be equal, not strictly greater\n\n        Keyword Args:\n            allow_cross_type_comparisons (boolean or None) : If True, allow comparisons between types (e.g. integer and\\\n                string). Otherwise, attempting such comparisons will raise an exception.\n\n        Keyword Args:\n            ignore_row_if (str): \"both_values_are_missing\", \"either_value_is_missing\", \"neither\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        raise NotImplementedError", "response": "Expects values in column A to be greater than column B."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expect_column_pair_values_to_be_in_set(self,\n                                               column_A,\n                                               column_B,\n                                               value_pairs_set,\n                                               ignore_row_if=\"both_values_are_missing\",\n                                               result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                               ):\n        \"\"\"\n        Expect paired values from columns A and B to belong to a set of valid pairs.\n\n        Args:\n            column_A (str): The first column name\n            column_B (str): The second column name\n            value_pairs_set (list of tuples): All the valid pairs to be matched\n\n        Keyword Args:\n            ignore_row_if (str): \"both_values_are_missing\", \"either_value_is_missing\", \"never\"\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        raise NotImplementedError", "response": "Expects paired values from columns A and B to belong to a set of valid pairs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpecting the values for each column in column_list to be unique across the columns listed in column_list.", "response": "def expect_multicolumn_values_to_be_unique(self,\n                                              column_list,\n                                              ignore_row_if=\"all_values_are_missing\",\n                                              result_format=None, include_config=False, catch_exceptions=None, meta=None\n                                              ):\n        \"\"\"\n        Expect the values for each row to be unique across the columns listed.\n\n        Args:\n            column_list (tuple or list): The first column name\n\n        Keyword Args:\n            ignore_row_if (str): \"all_values_are_missing\", \"any_value_is_missing\", \"never\"\n\n        Other Parameters:\n            result_format (str or None): \\\n                Which output mode to use: `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n                For more detail, see :ref:`result_format <result_format>`.\n            include_config (boolean): \\\n                If True, then include the expectation config as part of the result object. \\\n                For more detail, see :ref:`include_config`.\n            catch_exceptions (boolean or None): \\\n                If True, then catch exceptions and include them as part of the result object. \\\n                For more detail, see :ref:`catch_exceptions`.\n            meta (dict or None): \\\n                A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification. \\\n                For more detail, see :ref:`meta`.\n\n        Returns:\n            A JSON-serializable expectation result object.\n\n            Exact fields vary depending on the values passed to :ref:`result_format <result_format>` and\n            :ref:`include_config`, :ref:`catch_exceptions`, and :ref:`meta`.\n\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef column_map_expectation(cls, func):\n        if PY3:\n            argspec = inspect.getfullargspec(func)[0][1:]\n        else:\n            argspec = inspect.getargspec(func)[0][1:]\n\n        @cls.expectation(argspec)\n        @wraps(func)\n        def inner_wrapper(self, column, mostly=None, result_format=None, *args, **kwargs):\n\n            if result_format is None:\n                result_format = self.default_expectation_args[\"result_format\"]\n\n            result_format = parse_result_format(result_format)\n\n            # FIXME temporary fix for missing/ignored value\n            ignore_values = [None, np.nan]\n            if func.__name__ in ['expect_column_values_to_not_be_null', 'expect_column_values_to_be_null']:\n                ignore_values = []\n                result_format['partial_unexpected_count'] = 0  # Optimization to avoid meaningless computation for these expectations\n\n            series = self[column]\n\n            # FIXME rename to mapped_ignore_values?\n            if len(ignore_values) == 0:\n                boolean_mapped_null_values = np.array(\n                    [False for value in series])\n            else:\n                boolean_mapped_null_values = np.array([True if (value in ignore_values) or (pd.isnull(value)) else False\n                                                       for value in series])\n\n            element_count = int(len(series))\n\n            # FIXME rename nonnull to non_ignored?\n            nonnull_values = series[boolean_mapped_null_values == False]\n            nonnull_count = int((boolean_mapped_null_values == False).sum())\n\n            boolean_mapped_success_values = func(\n                self, nonnull_values, *args, **kwargs)\n            success_count = np.count_nonzero(boolean_mapped_success_values)\n\n            unexpected_list = list(\n                nonnull_values[boolean_mapped_success_values == False])\n            unexpected_index_list = list(\n                nonnull_values[boolean_mapped_success_values == False].index)\n\n            success, percent_success = self._calc_map_expectation_success(\n                success_count, nonnull_count, mostly)\n\n            return_obj = self._format_map_output(\n                result_format, success,\n                element_count, nonnull_count,\n                len(unexpected_list),\n                unexpected_list, unexpected_index_list\n            )\n\n            # FIXME Temp fix for result format\n            if func.__name__ in ['expect_column_values_to_not_be_null', 'expect_column_values_to_be_null']:\n                del return_obj['result']['unexpected_percent_nonmissing']\n                try:\n                    del return_obj['result']['partial_unexpected_counts']\n                except KeyError:\n                    pass\n\n            return return_obj\n\n        inner_wrapper.__name__ = func.__name__\n        inner_wrapper.__doc__ = func.__doc__\n\n        return inner_wrapper", "response": "Decorator for column - map expectations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multicolumn_map_expectation(cls, func):\n        if PY3:\n            argspec = inspect.getfullargspec(func)[0][1:]\n        else:\n            argspec = inspect.getargspec(func)[0][1:]\n\n        @cls.expectation(argspec)\n        @wraps(func)\n        def inner_wrapper(self, column_list, mostly=None, ignore_row_if=\"all_values_are_missing\",\n                          result_format=None, *args, **kwargs):\n\n            if result_format is None:\n                result_format = self.default_expectation_args[\"result_format\"]\n\n            test_df = self[column_list]\n\n            if ignore_row_if == \"all_values_are_missing\":\n                boolean_mapped_skip_values = test_df.isnull().all(axis=1)\n            elif ignore_row_if == \"any_value_is_missing\":\n                boolean_mapped_skip_values = test_df.isnull().any(axis=1)\n            elif ignore_row_if == \"never\":\n                boolean_mapped_skip_values = pd.Series([False] * len(test_df))\n            else:\n                raise ValueError(\n                    \"Unknown value of ignore_row_if: %s\", (ignore_row_if,))\n\n            boolean_mapped_success_values = func(\n                self, test_df[boolean_mapped_skip_values == False], *args, **kwargs)\n            success_count = boolean_mapped_success_values.sum()\n            nonnull_count = (~boolean_mapped_skip_values).sum()\n            element_count = len(test_df)\n\n            unexpected_list = test_df[(boolean_mapped_skip_values == False) & (boolean_mapped_success_values == False)]\n            unexpected_index_list = list(unexpected_list.index)\n\n            success, percent_success = self._calc_map_expectation_success(\n                success_count, nonnull_count, mostly)\n\n            return_obj = self._format_map_output(\n                result_format, success,\n                element_count, nonnull_count,\n                len(unexpected_list),\n                unexpected_list.to_dict(orient='records'), unexpected_index_list\n            )\n\n            return return_obj\n\n        inner_wrapper.__name__ = func.__name__\n        inner_wrapper.__doc__ = func.__doc__\n        return inner_wrapper", "response": "The multicolumn_map_expectation decorator handles boilerplate issues surrounding the common pattern of\n        evaluating truthiness of some condition on a per row basis across a set of columns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct an expectation using column-aggregate semantics. The MetaPandasDataset implementation replaces the \"column\" parameter supplied by the user with a pandas Series object containing the actual column from the relevant pandas dataframe. This simplifies the implementing expectation logic while preserving the standard Dataset signature and expected behavior. See :func:`column_aggregate_expectation <great_expectations.data_asset.dataset.Dataset.column_aggregate_expectation>` \\ for full documentation of this function.", "response": "def column_aggregate_expectation(cls, func):\n        \"\"\"Constructs an expectation using column-aggregate semantics.\n\n        The MetaPandasDataset implementation replaces the \"column\" parameter supplied by the user with a pandas\n        Series object containing the actual column from the relevant pandas dataframe. This simplifies the implementing\n        expectation logic while preserving the standard Dataset signature and expected behavior.\n\n        See :func:`column_aggregate_expectation <great_expectations.data_asset.dataset.Dataset.column_aggregate_expectation>` \\\n        for full documentation of this function.\n        \"\"\"\n        if PY3:\n            argspec = inspect.getfullargspec(func)[0][1:]\n        else:\n            argspec = inspect.getargspec(func)[0][1:]\n\n        @cls.expectation(argspec)\n        @wraps(func)\n        def inner_wrapper(self, column, result_format=None, *args, **kwargs):\n\n            if result_format is None:\n                result_format = self.default_expectation_args[\"result_format\"]\n\n            series = self[column]\n            null_indexes = series.isnull()\n\n            element_count = int(len(series))\n            nonnull_values = series[null_indexes == False]\n            # Simplify this expression because the old version fails under pandas 0.21 (but only that version)\n            # nonnull_count = int((null_indexes == False).sum())\n            nonnull_count = len(nonnull_values)\n            null_count = element_count - nonnull_count\n\n            evaluation_result = func(self, nonnull_values, *args, **kwargs)\n\n            if 'success' not in evaluation_result:\n                raise ValueError(\n                    \"Column aggregate expectation failed to return required information: success\")\n\n            if ('result' not in evaluation_result) or ('observed_value' not in evaluation_result['result']):\n                raise ValueError(\n                    \"Column aggregate expectation failed to return required information: observed_value\")\n\n            # Retain support for string-only output formats:\n            result_format = parse_result_format(result_format)\n\n            return_obj = {\n                'success': bool(evaluation_result['success'])\n            }\n\n            if result_format['result_format'] == 'BOOLEAN_ONLY':\n                return return_obj\n\n            return_obj['result'] = {\n                'observed_value': evaluation_result['result']['observed_value'],\n                \"element_count\": element_count,\n                \"missing_count\": null_count,\n                \"missing_percent\": null_count * 1.0 / element_count if element_count > 0 else None\n            }\n\n            if result_format['result_format'] == 'BASIC':\n                return return_obj\n\n            if 'details' in evaluation_result['result']:\n                return_obj['result']['details'] = evaluation_result['result']['details']\n\n            if result_format['result_format'] in [\"SUMMARY\", \"COMPLETE\"]:\n                return return_obj\n\n            raise ValueError(\"Unknown result_format %s.\" %\n                             (result_format['result_format'],))\n\n        return inner_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dispatch(command_class, argv=sys.argv, input_file=sys.stdin, output_file=sys.stdout, module_name=None):\n    assert issubclass(command_class, SearchCommand)\n\n    if module_name is None or module_name == '__main__':\n        command_class().process(argv, input_file, output_file)", "response": "Instantiates and executes a search command class"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef options(self):\n        if self._options is None:\n            self._options = Option.View(self)\n        return self._options", "response": "Returns the options specified as argument to this command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the search results info object for this command invocation.", "response": "def search_results_info(self):\n        \"\"\" Returns the search results info for this command invocation.\n\n        The search results info object is created from the search results info file associated with the command\n        invocation.\n\n        :return: Search results info:const:`None`, if the search results info file associated with the command\n        invocation is inaccessible.\n        :rtype: SearchResultsInfo or NoneType\n\n        \"\"\"\n        if self._search_results_info is not None:\n            return self._search_results_info\n\n        if self._protocol_version == 1:\n            try:\n                path = self._input_header['infoPath']\n            except KeyError:\n                return None\n        else:\n            assert self._protocol_version == 2\n\n            try:\n                dispatch_dir = self._metadata.searchinfo.dispatch_dir\n            except AttributeError:\n                return None\n\n            path = os.path.join(dispatch_dir, 'info.csv')\n\n        try:\n            with io.open(path, 'r') as f:\n                reader = csv.reader(f, dialect=CsvDialect)\n                fields = next(reader)\n                values = next(reader)\n        except IOError as error:\n            if error.errno == 2:\n                self.logger.error('Search results info file {} does not exist.'.format(json_encode_string(path)))\n                return\n            raise\n\n        def convert_field(field):\n            return (field[1:] if field[0] == '_' else field).replace('.', '_')\n\n        decode = MetadataDecoder().decode\n\n        def convert_value(value):\n            try:\n                return decode(value) if len(value) > 0 else value\n            except ValueError:\n                return value\n\n        info = ObjectView(dict(imap(lambda f_v: (convert_field(f_v[0]), convert_value(f_v[1])), izip(fields, values))))\n\n        try:\n            count_map = info.countMap\n        except AttributeError:\n            pass\n        else:\n            count_map = count_map.split(';')\n            n = len(count_map)\n            info.countMap = dict(izip(islice(count_map, 0, n, 2), islice(count_map, 1, n, 2)))\n\n        try:\n            msg_type = info.msgType\n            msg_text = info.msg\n        except AttributeError:\n            pass\n        else:\n            messages = ifilter(lambda t_m: t_m[0] or t_m[1], izip(msg_type.split('\\n'), msg_text.split('\\n')))\n            info.msg = [Message(message) for message in messages]\n            del info.msgType\n\n        try:\n            info.vix_families = ElementTree.fromstring(info.vix_families)\n        except AttributeError:\n            pass\n\n        self._search_results_info = info\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef service(self):\n        if self._service is not None:\n            return self._service\n\n        metadata = self._metadata\n\n        if metadata is None:\n            return None\n\n        try:\n            searchinfo = self._metadata.searchinfo\n        except AttributeError:\n            return None\n\n        splunkd_uri = searchinfo.splunkd_uri\n\n        if splunkd_uri is None:\n            return None\n\n        uri = urlsplit(splunkd_uri, allow_fragments=False)\n\n        self._service = Service(\n            scheme=uri.scheme, host=uri.hostname, port=uri.port, app=searchinfo.app, token=searchinfo.session_key)\n\n        return self._service", "response": "Returns a service object for this command invocation or None if it is not yet set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process(self, argv=sys.argv, ifile=sys.stdin, ofile=sys.stdout):\n        if len(argv) > 1:\n            self._process_protocol_v1(argv, ifile, ofile)\n        else:\n            self._process_protocol_v2(argv, ifile, ofile)", "response": "Process data.\n\n        :param argv: Command line arguments.\n        :type argv: list or tuple\n\n        :param ifile: Input data file.\n        :type ifile: file\n\n        :param ofile: Output data file.\n        :type ofile: file\n\n        :return: :const:`None`\n        :rtype: NoneType"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_protocol_v2(self, argv, ifile, ofile):\n        debug = environment.splunklib_logger.debug\n        class_name = self.__class__.__name__\n\n        debug('%s.process started under protocol_version=2', class_name)\n        self._protocol_version = 2\n\n        # Read search command metadata from splunkd\n        # noinspection PyBroadException\n        try:\n            debug('Reading metadata')\n            metadata, body = self._read_chunk(ifile)\n\n            action = getattr(metadata, 'action', None)\n\n            if action != 'getinfo':\n                raise RuntimeError('Expected getinfo action, not {}'.format(action))\n\n            if len(body) > 0:\n                raise RuntimeError('Did not expect data for getinfo action')\n\n            self._metadata = deepcopy(metadata)\n\n            searchinfo = self._metadata.searchinfo\n\n            searchinfo.earliest_time = float(searchinfo.earliest_time)\n            searchinfo.latest_time = float(searchinfo.latest_time)\n            searchinfo.search = unquote(searchinfo.search)\n\n            self._map_input_header()\n\n            debug('  metadata=%r, input_header=%r', self._metadata, self._input_header)\n\n            try:\n                tempfile.tempdir = self._metadata.searchinfo.dispatch_dir\n            except AttributeError:\n                raise RuntimeError('%s.metadata.searchinfo.dispatch_dir is undefined'.format(class_name))\n\n            debug('  tempfile.tempdir=%r', tempfile.tempdir)\n        except:\n            self._record_writer = RecordWriterV2(ofile)\n            self._report_unexpected_error()\n            self.finish()\n            exit(1)\n\n        # Write search command configuration for consumption by splunkd\n        # noinspection PyBroadException\n        try:\n            self._record_writer = RecordWriterV2(ofile, getattr(self._metadata.searchinfo, 'maxresultrows', None))\n            self.fieldnames = []\n            self.options.reset()\n\n            args = self.metadata.searchinfo.args\n            error_count = 0\n\n            debug('Parsing arguments')\n\n            if args and type(args) == list:\n                for arg in args:\n                    result = arg.split('=', 1)\n                    if len(result) == 1:\n                        self.fieldnames.append(str(result[0]))\n                    else:\n                        name, value = result\n                        name = str(name)\n                        try:\n                            option = self.options[name]\n                        except KeyError:\n                            self.write_error('Unrecognized option: {}={}'.format(name, value))\n                            error_count += 1\n                            continue\n                        try:\n                            option.value = value\n                        except ValueError:\n                            self.write_error('Illegal value: {}={}'.format(name, value))\n                            error_count += 1\n                            continue\n\n            missing = self.options.get_missing()\n\n            if missing is not None:\n                if len(missing) == 1:\n                    self.write_error('A value for \"{}\" is required'.format(missing[0]))\n                else:\n                    self.write_error('Values for these required options are missing: {}'.format(', '.join(missing)))\n                error_count += 1\n\n            if error_count > 0:\n                exit(1)\n\n            debug('  command: %s', six.text_type(self))\n\n            debug('Preparing for execution')\n            self.prepare()\n\n            if self.record:\n\n                ifile, ofile = self._prepare_recording(argv, ifile, ofile)\n                self._record_writer.ofile = ofile\n\n                # Record the metadata that initiated this command after removing the record option from args/raw_args\n\n                info = self._metadata.searchinfo\n\n                for attr in 'args', 'raw_args':\n                    setattr(info, attr, [arg for arg in getattr(info, attr) if not arg.startswith('record=')])\n\n                metadata = MetadataEncoder().encode(self._metadata)\n                ifile.record('chunked 1.0,', six.text_type(len(metadata)), ',0\\n', metadata)\n\n            if self.show_configuration:\n                self.write_info(self.name + ' command configuration: ' + str(self._configuration))\n\n            debug('  command configuration: %s', self._configuration)\n\n        except SystemExit:\n            self._record_writer.write_metadata(self._configuration)\n            self.finish()\n            raise\n        except:\n            self._record_writer.write_metadata(self._configuration)\n            self._report_unexpected_error()\n            self.finish()\n            exit(1)\n\n        self._record_writer.write_metadata(self._configuration)\n\n        # Execute search command on data passing through the pipeline\n        # noinspection PyBroadException\n        try:\n            debug('Executing under protocol_version=2')\n            self._records = self._records_protocol_v2\n            self._metadata.action = 'execute'\n            self._execute(ifile, None)\n        except SystemExit:\n            self.finish()\n            raise\n        except:\n            self._report_unexpected_error()\n            self.finish()\n            exit(1)\n\n        debug('%s.process completed', class_name)", "response": "Process the protocol version 2 of the I2C protocol."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _execute(self, ifile, process):\n        self._record_writer.write_records(process(self._records(ifile)))\n        self.finish()", "response": "Default processing loop\n\n        :param ifile: Input file object.\n        :type ifile: file\n\n        :param process: Bound method to call in processing loop.\n        :type process: instancemethod\n\n        :return: :const:`None`.\n        :rtype: NoneType"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a slice of the given dictionary value containing only the specified keys.", "response": "def dslice(value, *args):\n    \"\"\"Returns a 'slice' of the given dictionary value containing only the\n       requested keys. The keys can be requested in a variety of ways, as an\n       arg list of keys, as a list of keys, or as a dict whose key(s) represent\n       the source keys and whose corresponding values represent the resulting \n       key(s) (enabling key rename), or any combination of the above.\"\"\" \n    result = {}\n    for arg in args:\n        if isinstance(arg, dict):\n            for k, v in six.iteritems(arg):\n                if k in value: \n                    result[v] = value[k]\n        elif isinstance(arg, list):\n            for k in arg:\n                if k in value: \n                    result[k] = value[k]\n        else:\n            if arg in value: \n                result[arg] = value[arg]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(argv, rules=None, config=None, **kwargs):\n    parser_ = parser(rules, **kwargs)\n    if config is not None: parser_.loadrc(config)\n    return parser_.parse(argv).result", "response": "Parse the given arg vector with the default Splunk command rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads at most n characters from this stream.", "response": "def read(self, n=None):\n        \"\"\"Read at most *n* characters from this stream.\n\n        If *n* is ``None``, return all available characters.\n        \"\"\"\n        response = b\"\"\n        while len(self.streams) > 0 and (n is None or n > 0):\n            txt = self.streams[0].read(n)\n            response += txt\n            if n is not None:\n                n -= len(txt)\n            if n is None or n > 0:\n                del self.streams[0]\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread at most n characters from this stream.", "response": "def read(self, n=None):\n        \"\"\"Read at most *n* characters from this stream.\n\n        If *n* is ``None``, return all available characters.\n        \"\"\"\n        response = b\"\"\n        while n is None or n > 0:\n            c = self.stream.read(1)\n            if c == b\"\":\n                break\n            elif c == b\"<\":\n                c += self.stream.read(1)\n                if c == b\"<?\":\n                    while True:\n                        q = self.stream.read(1)\n                        if q == b\">\":\n                            break\n                else:\n                    response += c\n                    if n is not None:\n                        n -= len(c)\n            else:\n                response += c\n                if n is not None:\n                    n -= 1\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the results and messages out of the stream.", "response": "def _parse_results(self, stream):\n        \"\"\"Parse results and messages out of *stream*.\"\"\"\n        result = None\n        values = None\n        try:\n            for event, elem in et.iterparse(stream, events=('start', 'end')):\n                if elem.tag == 'results' and event == 'start':\n                    # The wrapper element is a <results preview=\"0|1\">. We\n                    # don't care about it except to tell is whether these\n                    # are preview results, or the final results from the\n                    # search.\n                    is_preview = elem.attrib['preview'] == '1'\n                    self.is_preview = is_preview\n                if elem.tag == 'result':\n                    if event == 'start':\n                        result = OrderedDict()\n                    elif event == 'end':\n                        yield result\n                        result = None\n                        elem.clear()\n\n                elif elem.tag == 'field' and result is not None:\n                    # We need the 'result is not None' check because\n                    # 'field' is also the element name in the <meta>\n                    # header that gives field order, which is not what we\n                    # want at all.\n                    if event == 'start':\n                        values = []\n                    elif event == 'end':\n                        field_name = elem.attrib['k']\n                        if len(values) == 1:\n                            result[field_name] = values[0]\n                        else:\n                            result[field_name] = values\n                        # Calling .clear() is necessary to let the\n                        # element be garbage collected. Otherwise\n                        # arbitrarily large results sets will use\n                        # arbitrarily large memory intead of\n                        # streaming.\n                        elem.clear()\n\n                elif elem.tag in ('text', 'v') and event == 'end':\n                    try:\n                        text = \"\".join(elem.itertext())\n                    except AttributeError:\n                        # Assume we're running in Python < 2.7, before itertext() was added\n                        # So we'll define it here\n\n                        def __itertext(self):\n                          tag = self.tag\n                          if not isinstance(tag, six.string_types) and tag is not None:\n                              return\n                          if self.text:\n                              yield self.text\n                          for e in self:\n                              for s in __itertext(e):\n                                  yield s\n                              if e.tail:\n                                  yield e.tail\n\n                        text = \"\".join(__itertext(elem))\n                    values.append(text)\n                    elem.clear()\n\n                elif elem.tag == 'msg':\n                    if event == 'start':\n                        msg_type = elem.attrib['type']\n                    elif event == 'end':\n                        text = elem.text if elem.text is not None else \"\"\n                        yield Message(msg_type, text)\n                        elem.clear()\n        except SyntaxError as pe:\n            # This is here to handle the same incorrect return from\n            # splunk that is described in __init__.\n            if 'no element found' in pe.msg:\n                return\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmdline(argv, rules=None, config=None, **kwargs):\n    parser = Parser(rules, **kwargs)\n    if config is not None: parser.loadrc(config)\n    return parser.parse(argv).result", "response": "Simplified cmdopts interface that does not default any parsing rules\n       and that does not allow compounding calls to the parser."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(self, rules):\n        # Initialize the option parser\n        for dest in rules.keys():\n            rule = rules[dest]\n\n            # Assign defaults ourselves here, instead of in the option parser\n            # itself in order to allow for multiple calls to parse (dont want\n            # subsequent calls to override previous values with default vals).\n            if 'default' in rule:\n                self.result['kwargs'][dest] = rule['default']\n\n            flags = rule['flags']\n            kwargs = { 'action': rule.get('action', \"store\") }\n            # NOTE: Don't provision the parser with defaults here, per above.\n            for key in ['callback', 'help', 'metavar', 'type']:\n                if key in rule: kwargs[key] = rule[key]\n            self.add_option(*flags, dest=dest, **kwargs)\n\n            # Remember the dest vars that we see, so that we can merge results\n            self.dests.add(dest)", "response": "Initialize the parser with the given command rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the given filepath if it exists otherwise ignore.", "response": "def loadif(self, filepath):\n        \"\"\"Load the given filepath if it exists, otherwise ignore.\"\"\"\n        if path.isfile(filepath): self.load(filepath)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, argv):\n        kwargs, args = self.parse_args(argv)\n        self.result['args'] += args\n        # Annoying that parse_args doesn't just return a dict\n        for dest in self.dests:\n            value = getattr(kwargs, dest)\n            if value is not None:\n                self.result['kwargs'][dest] = value\n        return self", "response": "Parse the given argument vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfeeds the named index in a specific manner.", "response": "def feed_index(service, opts):\n    \"\"\"Feed the named index in a specific manner.\"\"\"\n\n    indexname = opts.args[0]\n    itype = opts.kwargs['ingest']\n\n\n    # get index handle\n    try:\n        index = service.indexes[indexname]\n    except KeyError:\n        print(\"Index %s not found\" % indexname)\n        return\n\n    if itype in [\"stream\", \"submit\"]:\n        stream = index.attach()\n    else:\n        # create a tcp input if one doesn't exist\n        input_host = opts.kwargs.get(\"inputhost\", SPLUNK_HOST)\n        input_port = int(opts.kwargs.get(\"inputport\", SPLUNK_PORT))\n        input_name = \"tcp:%s\" % (input_port)\n        if input_name not in service.inputs.list():\n            service.inputs.create(\"tcp\", input_port, index=indexname)\n        # connect to socket\n        ingest = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        ingest.connect((input_host, input_port))\n\n    count = 0\n    lastevent = \"\"\n    try:\n        for i in range(0, 10):\n            for j in range(0, 5000):\n                lastevent = \"%s: event bunch %d, number %d\\n\" % \\\n                             (datetime.datetime.now().isoformat(), i, j)\n\n                if itype == \"stream\":\n                    stream.write(lastevent + \"\\n\")\n                elif itype == \"submit\":\n                    index.submit(lastevent + \"\\n\")\n                else:\n                    ingest.send(lastevent + \"\\n\")\n\n                count = count + 1\n            \n            print(\"submitted %d events, sleeping 1 second\" % count)\n            time.sleep(1)\n    except KeyboardInterrupt:\n        print(\"^C detected, last event written:\")\n        print(lastevent)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting loop for the execute command.", "response": "def _execute(self, ifile, process):\n        \"\"\" Execution loop\n\n        :param ifile: Input file object. Unused.\n        :type ifile: file\n\n        :return: `None`.\n\n        \"\"\"\n        if self._protocol_version == 2:\n            result = self._read_chunk(ifile)\n\n            if not result:\n                return\n\n            metadata, body = result\n            action = getattr(metadata, 'action', None)\n\n            if action != 'execute':\n                raise RuntimeError('Expected execute action, not {}'.format(action))\n\n        self._record_writer.write_records(self.generate())\n        self.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xml_compare(expected, found):\n\n    # if comparing the same ET object\n    if expected == found:\n        return True\n\n    # compare element attributes, ignoring order\n    if set(expected.items()) != set(found.items()):\n        return False\n\n    # check for equal number of children\n    expected_children = list(expected)\n    found_children = list(found)\n    if len(expected_children) != len(found_children):\n        return False\n\n    # compare children\n    if not all([xml_compare(a, b) for a, b in zip(expected_children, found_children)]):\n        return False\n\n    # compare elements, if there is no text node, return True\n    if (expected.text is None or expected.text.strip() == \"\") \\\n        and (found.text is None or found.text.strip() == \"\"):\n        return True\n    else:\n        return expected.tag == found.tag and expected.text == found.text \\\n            and expected.attrib == found.attrib", "response": "Checks equality of two XML objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef output(stream):\n    while True:\n        content = stream.read(1024)\n        if len(content) == 0: break\n        sys.stdout.write(content)", "response": "Write the contents of the given stream to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, argv):\n        opts = cmdline(argv, FLAGS_CREATE)\n        if len(opts.args) != 1:\n            error(\"Command requires a search expression\", 2)\n        query = opts.args[0]\n        job = self.service.jobs.create(opts.args[0], **opts.kwargs)\n        print(job.sid)", "response": "Create a search job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef events(self, argv):\n        opts = cmdline(argv, FLAGS_EVENTS)\n        self.foreach(opts.args, lambda job: \n            output(job.events(**opts.kwargs)))", "response": "Retrieve events for the specified search jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the function to each job specified in the argument vector.", "response": "def foreach(self, argv, func):\n        \"\"\"Apply the function to each job specified in the argument vector.\"\"\"\n        if len(argv) == 0:\n            error(\"Command requires a search specifier.\", 2)\n        for item in argv:\n            job = self.lookup(item)\n            if job is None:\n                error(\"Search job '%s' does not exist\" % item, 2)\n            func(job)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all current search jobs if no jobs specified otherwise list the properties of the specified jobs.", "response": "def list(self, argv):\n        \"\"\"List all current search jobs if no jobs specified, otherwise\n           list the properties of the specified jobs.\"\"\"\n\n        def read(job):\n            for key in sorted(job.content.keys()):\n                # Ignore some fields that make the output hard to read and\n                # that are available via other commands.\n                if key in [\"performance\"]: continue\n                print(\"%s: %s\" % (key, job.content[key]))\n\n        if len(argv) == 0:\n            index = 0\n            for job in self.service.jobs:\n                print(\"@%d : %s\" % (index, job.sid))\n                index += 1\n            return\n\n        self.foreach(argv, read)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the preview for the specified search jobs.", "response": "def preview(self, argv):\n        \"\"\"Retrieve the preview for the specified search jobs.\"\"\"\n        opts = cmdline(argv, FLAGS_RESULTS)\n        self.foreach(opts.args, lambda job: \n            output(job.preview(**opts.kwargs)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sid(self, spec):\n        if spec.startswith('@'):\n            index = int(spec[1:])\n            jobs = self.service.jobs.list()\n            if index < len(jobs):\n                return jobs[index].sid\n        return spec", "response": "Convert the given search specifier into a search - id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndispatching the given command.", "response": "def run(self, argv):\n        \"\"\"Dispatch the given command.\"\"\"\n        command = argv[0]\n        handlers = { \n            'cancel': self.cancel,\n            'create': self.create,\n            'events': self.events,\n            'finalize': self.finalize,\n            'list': self.list,\n            'pause': self.pause,\n            'preview': self.preview,\n            'results': self.results,\n            'searchlog': self.searchlog,\n            'summary': self.summary,\n            'perf': self.perf,\n            'timeline': self.timeline,\n            'touch': self.touch,\n            'unpause': self.unpause,\n        }\n        handler = handlers.get(command, None)\n        if handler is None:\n            error(\"Unrecognized command: %s\" % command, 2)\n        handler(argv[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef searchlog(self, argv):\n        opts = cmdline(argv, FLAGS_SEARCHLOG)\n        self.foreach(opts.args, lambda job: \n            output(job.searchlog(**opts.kwargs)))", "response": "Retrieve the searchlog for the specified search jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten(value, prefix=None):\n\n    def issimple(value): # foldr(True, or, value)?\n        for item in value:\n            if isinstance(item, dict) or isinstance(item, list):\n                return False\n        return True\n\n    if isinstance(value, six.text_type):\n        return value.encode(\"utf8\")\n\n    if isinstance(value, list):\n        if issimple(value): return value\n        offset = 0\n        result = {}\n        prefix = \"%d\" if prefix is None else \"%s_%%d\" % prefix\n        for item in value:\n            k = prefix % offset\n            v = flatten(item, k)\n            if not isinstance(v, dict): v = {k:v}\n            result.update(v)\n            offset += 1\n        return result\n\n    if isinstance(value, dict):\n        result = {}\n        prefix = \"%s\" if prefix is None else \"%s_%%s\" % prefix\n        for k, v in six.iteritems(value):\n            k = prefix % str(k)\n            v = flatten(v, k)\n            if not isinstance(v, dict): v = {k:v}\n            result.update(v)\n        return result\n\n    return value", "response": "Takes an arbitrary JSON object and flattens it into a dict\n       with values consisting of either simple types or lists of simple types or lists of simple types."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(argv):\n    usage = 'usage: %prog --help for options'\n    opts = utils.parse(argv, RULES, \".splunkrc\", usage=usage)\n\n    context = binding.connect(**opts.kwargs)\n    operation = None\n\n    # splunk.binding.debug = True # for verbose information (helpful for debugging)\n\n    # Extract from command line and build into variable args\n    kwargs = {}\n    for key in RULES.keys():\n        if key in opts.kwargs:\n            if key == \"operation\":\n                operation = opts.kwargs[key]\n            else:\n                kwargs[key] = opts.kwargs[key]\n\n    # no operation? if name present, default to list, otherwise list-all\n    if not operation:\n        if 'name' in kwargs:\n            operation = 'list'\n        else:\n            operation = 'list-all'\n    \n    # pre-sanitize\n    if (operation != \"list\" and operation != \"create\" \n                            and operation != \"delete\"\n                            and operation != \"list-all\"):\n        print(\"operation %s not one of list-all, list, create, delete\" % operation)\n        sys.exit(0)\n\n    if 'name' not in kwargs and operation != \"list-all\":\n        print(\"operation requires a name\")\n        sys.exit(0)\n\n    # remove arg 'name' from passing through to operation builder, except on create\n    if operation != \"create\" and operation != \"list-all\":\n        name = kwargs['name']\n        kwargs.pop('name')\n\n    # perform operation on saved search created with args from cli\n    if operation == \"list-all\":\n        result = context.get(\"saved/searches\",  **kwargs)\n    elif operation == \"list\":\n        result = context.get(\"saved/searches/%s\" % name, **kwargs)\n    elif operation == \"create\":\n        result = context.post(\"saved/searches\", **kwargs)\n    else:\n        result = context.delete(\"saved/searches/%s\" % name, **kwargs)\n    print(\"HTTP STATUS: %d\" % result.status)\n    xml_data = result.body.read().decode('utf-8')\n    sys.stdout.write(xml_data)", "response": "main entry point for the main function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an HTTP request handler configured with the given ca_file.", "response": "def handler(ca_file=None):\n    \"\"\"Returns an HTTP request handler configured with the given ca_file.\"\"\"\n\n    def request(url, message, **kwargs):\n        scheme, host, port, path = spliturl(url)\n\n        if scheme != \"https\": \n            ValueError(\"unsupported scheme: %s\" % scheme)\n\n        connection = HTTPSConnection(host, port, ca_file)\n        try:\n            body = message.get('body', \"\")\n            headers = dict(message.get('headers', []))\n            connection.request(message['method'], path, body, headers)\n            response = connection.getresponse()\n        finally:\n            connection.close()\n\n        return {\n            'status': response.status,\n            'reason': response.reason,\n            'headers': response.getheaders(),\n            'body': BytesIO(response.read())\n        }\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(self, argv):\n\n        def read(index):\n            print(index.name)\n            for key in sorted(index.content.keys()): \n                value = index.content[key]\n                print(\"    %s: %s\" % (key, value))\n\n        if len(argv) == 0:\n            for index in self.service.indexes:\n                count = index['totalEventCount']\n                print(\"%s (%s)\" % (index.name, count))\n        else:\n            self.foreach(argv, read)", "response": "List available indexes if no names provided otherwise list the\n           properties of the named indexes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, argv):\n        command = argv[0]\n        handlers = { \n            'clean': self.clean,\n            'create': self.create,\n            'disable': self.disable,\n            'enable': self.enable,\n            'list': self.list,\n            'update': self.update,\n        }\n        handler = handlers.get(command, None)\n        if handler is None:\n            error(\"Unrecognized command: %s\" % command, 2)\n        handler(argv[1:])", "response": "Dispatch the given command & args."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef foreach(self, argv, func):\n        opts = cmdline(argv)\n        if len(opts.args) == 0:\n            error(\"Command requires an index name\", 2)\n        for name in opts.args:\n            if name not in self.service.indexes:\n                error(\"Index '%s' does not exist\" % name, 2)\n            index = self.service.indexes[name]\n            func(index)", "response": "Apply the function to each index named in the argument vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, argv):\n\n        if len(argv) == 0: \n            error(\"Command requires an index name\", 2)\n        name = argv[0]\n\n        if name not in self.service.indexes:\n            error(\"Index '%s' does not exist\" % name, 2)\n        index = self.service.indexes[name]\n\n        # Read index metadata and construct command line parser rules that \n        # correspond to each editable field.\n\n        # Request editable fields\n        fields = self.service.indexes.itemmeta().fields.optional\n\n        # Build parser rules\n        rules = dict([(field, {'flags': [\"--%s\" % field]}) for field in fields])\n\n        # Parse the argument vector\n        opts = cmdline(argv, rules)\n\n        # Execute the edit request\n        index.update(**opts.kwargs)", "response": "Update an index according to the given argument vector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_to_document(self, parent):\n        arg = ET.SubElement(parent, \"arg\")\n        arg.set(\"name\", self.name)\n\n        if self.title is not None:\n            ET.SubElement(arg, \"title\").text = self.title\n\n        if self.description is not None:\n            ET.SubElement(arg, \"description\").text = self.description\n\n        if self.validation is not None:\n            ET.SubElement(arg, \"validation\").text = self.validation\n\n        # add all other subelements to this Argument, represented by (tag, text)\n        subelements = [\n            (\"data_type\", self.data_type),\n            (\"required_on_edit\", self.required_on_edit),\n            (\"required_on_create\", self.required_on_create)\n        ]\n\n        for name, value in subelements:\n            ET.SubElement(arg, name).text = str(value).lower()\n\n        return arg", "response": "Adds an argument to this ElementTree document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the start and end of any valid event in the event buffer", "response": "def get_csv_next_event_start(location, event_buffer):\n    \"\"\" determin the event start and end of *any* valid event \"\"\"\n\n    start = -1\n    end = -1\n\n    event_start = event_buffer.find(\"\\n\", location + 1)\n    event_end = event_buffer.find('\"\\n', event_start + 1)\n\n    while (event_end > 0):\n        parts = event_buffer[event_start:event_end].split(\",\")\n        # test parts 0 and 1 of CSV. Format should be time.qqq, anything\n        # else is not time stamp to keep moving.\n        try:\n            int(parts[0].replace('\\n',\"\"))\n            timestamp = parts[1].replace('\"', \"\")\n            timeparts = timestamp.split('.')\n            int(timeparts[0])\n            int(timeparts[1])\n            return (event_start, event_end)\n        except:\n            event_start = event_buffer.find(\"\\n\", event_end + 2)\n            event_end = event_buffer.find('\"\\n', event_start + 1)\n\n    return (start, end)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the start of an event that is different from the Taxonomy", "response": "def get_csv_event_start(event_buffer):\n    \"\"\" get the event start of an event that is different (in time)from the\n        adjoining event, in CSV format \"\"\"\n\n    (start, end) = get_csv_next_event_start(0, event_buffer)\n    if start < 0:\n        return (-1, -1, \"\")\n\n    print(event_buffer[start:end])\n\n    tstart = event_buffer.find(\",\", start)\n    tend = event_buffer.find(\",\", tstart+1)\n    print(event_buffer[tstart:tend])\n    last_time = event_buffer[tstart+1:tend].replace('\"',\"\")\n\n    while end > 0:\n        (start, end) = get_csv_next_event_start(start, event_buffer)\n        if end < 0:\n            return (-1, -1, \"\")\n        tstart = event_buffer.find(\",\", start)\n        tend = event_buffer.find(\",\", tstart+1)\n        this_time = event_buffer[tstart+1:tend].replace('\"',\"\")\n        if this_time != last_time:\n            return (start, end + 1, last_time)\n\n    return (-1, -1, \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the start of an event that is different from the MimeType adjoining event in XML format", "response": "def get_xml_event_start(event_buffer):\n    \"\"\" get the event start of an event that is different (in time)from the\n        adjoining event, in XML format \"\"\"\n\n    result_pattern = \"<result offset='\"\n    time_key_pattern = \"<field k='_time'>\"\n    time_start_pattern = \"<value><text>\"\n    time_end_pattern = \"<\"\n    event_end_pattern = \"</result>\"\n\n    event_start = event_buffer.find(result_pattern)\n    event_end = event_buffer.find(event_end_pattern, event_start) + \\\n                len(event_end_pattern)\n    if event_end < 0:\n        return (-1, -1, \"\")\n    time_key_start = event_buffer.find(time_key_pattern, event_start)\n    time_start = event_buffer.find(time_start_pattern, time_key_start) + \\\n                 len(time_start_pattern)\n    time_end = event_buffer.find(time_end_pattern, time_start + 1)\n    last_time = event_buffer[time_start:time_end]\n\n    # wallk through events until time changes\n    event_start = event_end\n    while event_end > 0:\n        event_start = event_buffer.find(result_pattern, event_start + 1)\n        event_end = event_buffer.find(event_end_pattern, event_start) + \\\n                    len(event_end_pattern)\n        if event_end < 0:\n            return (-1, -1, \"\")\n        time_key_start = event_buffer.find(time_key_pattern, event_start)\n        time_start = event_buffer.find(time_start_pattern, time_key_start)\n        time_end = event_buffer.find(time_end_pattern, time_start)\n        this_time = event_buffer[time_start:time_end]\n        if this_time != last_time:\n            return (event_start, event_end, last_time)\n        event_start = event_end\n\n    return (-1, -1, \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_json_event_start(event_buffer):\n\n    event_start_pattern = '{\"_cd\":\"'\n    time_key_pattern = '\"_time\":\"'\n    time_end_pattern = '\"'\n    event_end_pattern = '\"},\\n'\n    event_end_pattern2 = '\"}[]' # old json output format bug\n\n    event_start = event_buffer.find(event_start_pattern)\n    event_end = event_buffer.find(event_end_pattern, event_start) + \\\n                len(event_end_pattern)\n    if event_end < 0:\n        event_end = event_buffer.find(event_end_pattern2, event_start) + \\\n                    len(event_end_pattern2)\n    if (event_end < 0):\n        return (-1, -1, \"\")\n\n    time_start = event_buffer.find(time_key_pattern, event_start) + \\\n                 len(time_key_pattern)\n    time_end = event_buffer.find(time_end_pattern, time_start + 1)\n    last_time = event_buffer[time_start:time_end]\n\n    event_start = event_end\n    while event_end > 0:\n        event_start = event_buffer.find(event_start_pattern, event_start + 1)\n        event_end = event_buffer.find(event_end_pattern, event_start) + \\\n                    len(event_end_pattern)\n        if event_end < 0:\n            event_end = event_buffer.find(event_end_pattern2, event_start) + \\\n                        len(event_end_pattern2)\n        if (event_end < 0):\n            return (-1, -1, \"\")\n        time_start = event_buffer.find(time_key_pattern, event_start) + \\\n                     len(time_key_pattern)\n        time_end = event_buffer.find(time_end_pattern, time_start + 1)\n        this_time = event_buffer[time_start:time_end]\n        if this_time != last_time:\n            return (event_start-2, event_end, last_time)\n        event_start = event_end\n\n    return (-1, -1, \"\")", "response": "get the start of an event that is different from the\n        adjoining event in XML format"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndispatch event start method based on event format", "response": "def get_event_start(event_buffer, event_format):\n    \"\"\" dispatch event start method based on event format type \"\"\"\n\n    if event_format == \"csv\":\n        return get_csv_event_start(event_buffer)\n    elif event_format == \"xml\":\n        return get_xml_event_start(event_buffer)\n    else:\n        return get_json_event_start(event_buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recover(options):\n\n    event_format = options.kwargs['omode']\n\n    buffer_size = 64*1024\n    fpd = open(options.kwargs['output'], \"r+\")\n    fpd.seek(0, 2) # seek to end\n    fptr = max(fpd.tell() - buffer_size, 0)\n    fptr_eof = 0\n\n    while (fptr > 0):\n        fpd.seek(fptr)\n        event_buffer = fpd.read(buffer_size)\n        (event_start, next_event_start, last_time) = \\\n            get_event_start(event_buffer, event_format)\n        if (event_start != -1):\n            fptr_eof = event_start + fptr\n            break\n        fptr = fptr - buffer_size\n\n    if fptr < 0:\n        # didn't find a valid event, so start over\n        fptr_eof = 0\n        last_time = 0 \n\n    # truncate file here\n    fpd.truncate(fptr_eof)\n    fpd.seek(fptr_eof)\n    fpd.write(\"\\n\")\n    fpd.close()\n\n    return last_time", "response": "This function is used to recover from an existing export run. We do this by finding the last time change between events and restarts from there."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export(options, service):\n\n    start = options.kwargs['start']\n    end = options.kwargs['end']\n    fixtail = options.kwargs['fixtail']\n    once = True\n\n    squery = options.kwargs['search']\n    squery = squery + \" index=%s\" % options.kwargs['index']\n    if (start != \"\"):\n        squery = squery + \" earliest_time=%s\" % start\n    if (end != \"\"):\n        squery = squery + \" latest_time=%s\" % end\n\n    success = False\n\n    while not success:\n        # issue query to splunkd\n        # count=0 overrides the maximum number of events\n        # returned (normally 50K) regardless of what the .conf\n        # file for splunkd says. \n        result = service.get('search/jobs/export', \n                             search=squery, \n                             output_mode=options.kwargs['omode'],\n                             timeout=60,\n                             earliest_time=\"0.000\",\n                             time_format=\"%s.%Q\",\n                             count=0)\n\n        if result.status != 200:\n            print(\"warning: export job failed: %d, sleep/retry\" % result.status)\n            time.sleep(60)\n        else:\n            success = True\n\n    # write export file \n    while True:\n        if fixtail and once:\n            cleanup_tail(options)\n            once = False\n        content = result.body.read()\n        if len(content) == 0: break\n        options.kwargs['fd'].write(content)\n        options.kwargs['fd'].write(\"\\n\")\n\n    options.kwargs['fd'].flush()", "response": "main method for exporting any number of indexes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an XML representation of this object.", "response": "def to_xml(self):\n        \"\"\"Creates an ``ET.Element`` representing self, then returns it.\n\n        :returns root, an ``ET.Element`` representing this scheme.\n        \"\"\"\n        root = ET.Element(\"scheme\")\n\n        ET.SubElement(root, \"title\").text = self.title\n\n        # add a description subelement if it's defined\n        if self.description is not None:\n            ET.SubElement(root, \"description\").text = self.description\n\n        # add all other subelements to this Scheme, represented by (tag, text)\n        subelements = [\n            (\"use_external_validation\", self.use_external_validation),\n            (\"use_single_instance\", self.use_single_instance),\n            (\"streaming_mode\", self.streaming_mode)\n        ]\n        for name, value in subelements:\n            ET.SubElement(root, name).text = str(value).lower()\n\n        endpoint = ET.SubElement(root, \"endpoint\")\n\n        args = ET.SubElement(endpoint, \"args\")\n\n        # add arguments as subelements to the <args> element\n        for arg in self.arguments:\n            arg.add_to_document(args)\n\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_xml_document(self, document):\n        self._out.write(ET.tostring(document))\n        self._out.flush()", "response": "Writes an XML document to the output stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n\n    usage = \"usage: %prog [options] <command> [<args>]\"\n\n    argv = sys.argv[1:]\n\n    command = None\n    commands = ['create', 'delete', 'list']\n\n    # parse args, connect and setup \n    opts = parse(argv, {}, \".splunkrc\", usage=usage)\n    service = connect(**opts.kwargs)\n    program = Program(service)\n\n    if len(opts.args) == 0:\n        # no args means list\n        command = \"list\"\n    elif opts.args[0] in commands:\n        # args and the first in our list of commands, extract \n        # command and remove from regular args\n        command = opts.args[0]\n        opts.args.remove(command)\n    else:\n        # first one not in our list, default to list\n        command = \"list\"\n\n    program.run(command, opts)", "response": "Main function for the\n    command line interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a conf stanza.", "response": "def create(self, opts):\n        \"\"\"Create a conf stanza.\"\"\"\n\n        argv = opts.args\n        count = len(argv)\n\n        # unflagged arguments are conf, stanza, key. In this order\n        # however, we must have a conf and stanza.\n        cpres = True if count > 0 else False\n        spres = True if count > 1 else False\n        kpres = True if count > 2 else False \n\n        if kpres:\n            kvpair = argv[2].split(\"=\")\n            if len(kvpair) != 2:\n                error(\"Creating a k/v pair requires key and value\", 2)\n            else:\n                key, value = kvpair\n\n        if not cpres and not spres:\n            error(\"Conf name and stanza name is required for create\", 2)\n\n        name = argv[0]\n        stan = argv[1]\n        conf = self.service.confs[name]\n\n        if not kpres:\n            # create stanza\n            conf.create(stan)\n            return \n\n        # create key/value pair under existing stanza\n        stanza = conf[stan]\n        stanza.submit({key: value})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, opts):\n\n        argv = opts.args\n        count = len(argv)\n\n        # unflagged arguments are conf, stanza, key. In this order\n        # however, we must have a conf and stanza.\n        cpres = True if count > 0 else False\n        spres = True if count > 1 else False\n        kpres = True if count > 2 else False \n        \n        if not cpres:\n            error(\"Conf name is required for delete\", 2)\n\n        if not cpres and not spres:\n            error(\"Conf name and stanza name is required for delete\", 2)\n\n        if kpres:\n            error(\"Cannot delete individual keys from a stanza\", 2)\n\n        name = argv[0]\n        stan = argv[1]\n        conf = self.service.confs[name]\n        conf.delete(stan)", "response": "Delete a conf stanza."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, opts):\n\n        argv = opts.args\n        count = len(argv)\n\n        # unflagged arguments are conf, stanza, key. In this order\n        # but all are optional\n        cpres = True if count > 0 else False\n        spres = True if count > 1 else False\n        kpres = True if count > 2 else False \n        \n        if not cpres:\n            # List out the available confs\n            for conf in self.service.confs: \n                print(conf.name)\n        else:\n            # Print out detail on the requested conf\n            # check for optional stanza, or key requested (or all)\n            name = argv[0]\n            conf = self.service.confs[name]\n            \n            for stanza in conf:\n                if (spres and argv[1] == stanza.name) or not spres:\n                    print(\"[%s]\" % stanza.name)\n                    for key, value in six.iteritems(stanza.content):\n                        if (kpres and argv[2] == key) or not kpres:\n                            print(\"%s = %s\" % (key, value))\n                print()", "response": "List all confs or if a conf is given all the stanzas in it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, command, opts):\n        handlers = { \n            'create': self.create,\n            'delete': self.delete,\n            'list': self.list\n        }\n        handler = handlers.get(command, None)\n        if handler is None:\n            error(\"Unrecognized command: %s\" % command, 2)\n        handler(opts)", "response": "Dispatch the given command & args."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tob(data, enc='utf8'):\n    return data.encode(enc) if isinstance(data, six.text_type) else bytes(data)", "response": "Convert anything to bytes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_auth(header):\n    try:\n        method, data = header.split(None, 1)\n        if method.lower() == 'basic':\n            name, pwd = base64.b64decode(data).split(':', 1)\n            return name, pwd\n    except (KeyError, ValueError, TypeError):\n        return None", "response": "Parse RFC 2617 HTTP authentication header string and return user and pass tuple or None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompare two strings in a cryptographically save way : Runtime is not affected by length of common prefix.", "response": "def _lscmp(a, b):\n    ''' Compares two strings in a cryptographically save way:\n        Runtime is not affected by length of common prefix. '''\n    return not sum(0 if x==y else 1 for x, y in zip(a, b)) and len(a) == len(b)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef yieldroutes(func):\n    import inspect # Expensive module. Only import if necessary.\n    path = '/' + func.__name__.replace('__','/').lstrip('/')\n    spec = inspect.getargspec(func)\n    argc = len(spec[0]) - len(spec[3] or [])\n    path += ('/:%s' * argc) % tuple(spec[0][:argc])\n    yield path\n    for arg in spec[0][argc:]:\n        path += '/:%s' % arg\n        yield path", "response": "Yields the routes that match the signature of the function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef path_shift(script_name, path_info, shift=1):\n    ''' Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.\n\n        :return: The modified paths.\n        :param script_name: The SCRIPT_NAME path.\n        :param script_name: The PATH_INFO path.\n        :param shift: The number of path fragments to shift. May be negative to\n          change the shift direction. (default: 1)\n    '''\n    if shift == 0: return script_name, path_info\n    pathlist = path_info.strip('/').split('/')\n    scriptlist = script_name.strip('/').split('/')\n    if pathlist and pathlist[0] == '': pathlist = []\n    if scriptlist and scriptlist[0] == '': scriptlist = []\n    if shift > 0 and shift <= len(pathlist):\n        moved = pathlist[:shift]\n        scriptlist = scriptlist + moved\n        pathlist = pathlist[shift:]\n    elif shift < 0 and shift >= -len(scriptlist):\n        moved = scriptlist[shift:]\n        pathlist = moved + pathlist\n        scriptlist = scriptlist[:shift]\n    else:\n        empty = 'SCRIPT_NAME' if shift < 0 else 'PATH_INFO'\n        raise AssertionError(\"Cannot shift. Nothing left from %s\" % empty)\n    new_script_name = '/' + '/'.join(scriptlist)\n    new_path_info = '/' + '/'.join(pathlist)\n    if path_info.endswith('/') and pathlist: new_path_info += '/'\n    return new_script_name, new_path_info", "response": "Shifts the path_info from SCRIPT_NAME to SCRIPT_NAME and vice versa."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating and manipulates keyword arguments by user defined callables. Handles ValueError and missing arguments by raising HTTPError(403).", "response": "def validate(**vkargs):\n    \"\"\"\n    Validates and manipulates keyword arguments by user defined callables.\n    Handles ValueError and missing arguments by raising HTTPError(403).\n    \"\"\"\n    def decorator(func):\n        def wrapper(**kargs):\n            for key, value in six.iteritems(vkargs):\n                if key not in kargs:\n                    abort(403, 'Missing parameter: %s' % key)\n                try:\n                    kargs[key] = value(kargs[key])\n                except ValueError:\n                    abort(403, 'Wrong parameter format for: %s' % key)\n            return func(**kargs)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_default_app_wrapper(name):\n    ''' Return a callable that relays calls to the current default app. '''\n    @functools.wraps(getattr(Bottle, name))\n    def wrapper(*a, **ka):\n        return getattr(app(), name)(*a, **ka)\n    return wrapper", "response": "Return a callable that relays calls to the current default app."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load(target, **vars):\n    module, target = target.split(\":\", 1) if ':' in target else (target, None)\n    if module not in sys.modules:\n        __import__(module)\n    if not target:\n        return sys.modules[module]\n    if target.isalnum():\n        return getattr(sys.modules[module], target)\n    package_name = module.split('.')[0]\n    vars[package_name] = sys.modules[package_name]\n    return eval('%s.%s' % (module, target), vars)", "response": "Load something from a module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a bottle application based on a target string and return the bottle application object.", "response": "def load_app(target):\n    \"\"\" Load a bottle application based on a target string and return the\n        application object.\n\n        If the target is an import path (e.g. package.module), the application\n        stack is used to isolate the routes defined in that module.\n        If the target contains a colon (e.g. package.module:myapp) the\n        module variable specified after the colon is returned instead.\n    \"\"\"\n    tmp = app.push() # Create a new \"default application\"\n    rv = _load(target) # Import the target module\n    app.remove(tmp) # Remove the temporary added default application\n    return rv if isinstance(rv, Bottle) else tmp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart a server instance.", "response": "def run(app=None, server='wsgiref', host='127.0.0.1', port=8080,\n        interval=1, reloader=False, quiet=False, **kargs):\n    \"\"\" Start a server instance. This method blocks until the server terminates.\n\n        :param app: WSGI application or target string supported by\n               :func:`load_app`. (default: :func:`default_app`)\n        :param server: Server adapter to use. See :data:`server_names` keys\n               for valid names or pass a :class:`ServerAdapter` subclass.\n               (default: `wsgiref`)\n        :param host: Server address to bind to. Pass ``0.0.0.0`` to listens on\n               all interfaces including the external one. (default: 127.0.0.1)\n        :param port: Server port to bind to. Values below 1024 require root\n               privileges. (default: 8080)\n        :param reloader: Start auto-reloading server? (default: False)\n        :param interval: Auto-reloader interval in seconds (default: 1)\n        :param quiet: Suppress output to stdout and stderr? (default: False)\n        :param options: Options passed to the server adapter.\n     \"\"\"\n    app = app or default_app()\n    if isinstance(app, six.string_types):\n        app = load_app(app)\n    if isinstance(server, six.string_types):\n        server = server_names.get(server)\n    if isinstance(server, type):\n        server = server(host=host, port=port, **kargs)\n    if not isinstance(server, ServerAdapter):\n        raise RuntimeError(\"Server must be a subclass of ServerAdapter\")\n    server.quiet = server.quiet or quiet\n    if not server.quiet and not os.environ.get('BOTTLE_CHILD'):\n        print(\"Bottle server starting up (using %s)...\" % repr(server))\n        print(\"Listening on http://%s:%d/\" % (server.host, server.port))\n        print(\"Use Ctrl-C to quit.\")\n        print()\n    try:\n        if reloader:\n            interval = min(interval, 1)\n            if os.environ.get('BOTTLE_CHILD'):\n                _reloader_child(server, app, interval)\n            else:\n                _reloader_observer(server, app, interval)\n        else:\n            server.run(app)\n    except KeyboardInterrupt:\n        pass\n    if not server.quiet and not os.environ.get('BOTTLE_CHILD'):\n        print(\"Shutting down...\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reloader_child(server, app, interval):\n    ''' Start the server and check for modified files in a background thread.\n        As soon as an update is detected, KeyboardInterrupt is thrown in\n        the main thread to exit the server loop. The process exists with status\n        code 3 to request a reload by the observer process. If the lockfile\n        is not modified in 2*interval second or missing, we assume that the\n        observer process died and exit with status code 1 or 2.\n    '''\n    lockfile = os.environ.get('BOTTLE_LOCKFILE')\n    bgcheck = FileCheckerThread(lockfile, interval)\n    try:\n        bgcheck.start()\n        server.run(app)\n    except KeyboardInterrupt:\n        pass\n    bgcheck.status, status = 5, bgcheck.status\n    bgcheck.join() # bgcheck.status == 5 --> silent exit\n    if status: sys.exit(status)", "response": "Start the server and check for modified files in a background thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reloader_observer(server, app, interval):\n    ''' Start a child process with identical commandline arguments and restart\n        it as long as it exists with status code 3. Also create a lockfile and\n        touch it (update mtime) every interval seconds.\n    '''\n    fd, lockfile = tempfile.mkstemp(prefix='bottle-reloader.', suffix='.lock')\n    os.close(fd) # We only need this file to exist. We never write to it\n    try:\n        while os.path.exists(lockfile):\n            args = [sys.executable] + sys.argv\n            environ = os.environ.copy()\n            environ['BOTTLE_CHILD'] = 'true'\n            environ['BOTTLE_LOCKFILE'] = lockfile\n            p = subprocess.Popen(args, env=environ)\n            while p.poll() is None: # Busy wait...\n                os.utime(lockfile, None) # I am alive!\n                time.sleep(interval)\n            if p.poll() != 3:\n                if os.path.exists(lockfile): os.unlink(lockfile)\n                sys.exit(p.poll())\n            elif not server.quiet:\n                print(\"Reloading server...\")\n    except KeyboardInterrupt:\n        pass\n    if os.path.exists(lockfile): os.unlink(lockfile)", "response": "Start a child process with identical commandline arguments and restart\nTaxonomy it as long as it exists with status code 3."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string iterator that returns a string for the rendered template.", "response": "def template(*args, **kwargs):\n    '''\n    Get a rendered template as a string iterator.\n    You can use a name, a filename or a template string as first parameter.\n    Template rendering arguments can be passed as dictionaries\n    or directly (as keyword arguments).\n    '''\n    tpl = args[0] if args else None\n    template_adapter = kwargs.pop('template_adapter', SimpleTemplate)\n    if tpl not in TEMPLATES or DEBUG:\n        settings = kwargs.pop('template_settings', {})\n        lookup = kwargs.pop('template_lookup', TEMPLATE_PATH)\n        if isinstance(tpl, template_adapter):\n            TEMPLATES[tpl] = tpl\n            if settings: TEMPLATES[tpl].prepare(**settings)\n        elif \"\\n\" in tpl or \"{\" in tpl or \"%\" in tpl or '$' in tpl:\n            TEMPLATES[tpl] = template_adapter(source=tpl, lookup=lookup, **settings)\n        else:\n            TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup, **settings)\n    if not TEMPLATES[tpl]:\n        abort(500, 'Template (%s) not found' % tpl)\n    for dictarg in args[1:]: kwargs.update(dictarg)\n    return TEMPLATES[tpl].render(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new route or replace the target for an existing route.", "response": "def add(self, rule, method, target, name=None, static=False):\n        ''' Add a new route or replace the target for an existing route. '''\n        if static:\n            depr(\"Use a backslash to escape ':' in routes.\") # 0.9\n            rule = rule.replace(':','\\\\:')\n\n        if rule in self.routes:\n            self.routes[rule][method.upper()] = target\n        else:\n            self.routes[rule] = {method.upper(): target}\n            self.rules.append(rule)\n            if self.static or self.dynamic: # Clear precompiler cache.\n                self.static, self.dynamic = {}, {}\n        if name:\n            self.named[name] = (rule, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self, _name, *anon, **args):\n        ''' Return a string that matches a named route. Use keyword arguments\n            to fill out named wildcards. Remaining arguments are appended as a\n            query string. Raises RouteBuildError or KeyError.'''\n        if _name not in self.named:\n            raise RouteBuildError(\"No route with that name.\", _name)\n        rule, pairs = self.named[_name]\n        if not pairs:\n            token = self.syntax.split(rule)\n            parts = [p.replace('\\\\:',':') for p in token[::3]]\n            names = token[1::3]\n            if len(parts) > len(names): names.append(None)\n            pairs = list(zip(parts, names))\n            self.named[_name] = (rule, pairs)\n        try:\n            anon = list(anon)\n            url = [s if k is None\n                   else s+str(args.pop(k)) if k else s+str(anon.pop())\n                   for s, k in pairs]\n        except IndexError:\n            msg = \"Not enough arguments to fill out anonymous wildcards.\"\n            raise RouteBuildError(msg)\n        except KeyError as e:\n            raise RouteBuildError(*e.args)\n\n        if args: url += ['?', urlencode(args)]\n        return ''.join(url)", "response": "Build a URL that matches a named route."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmatches the given environ.", "response": "def match(self, environ):\n        ''' Return a (target, url_agrs) tuple or raise HTTPError(404/405). '''\n        targets, urlargs = self._match_path(environ)\n        if not targets:\n            raise HTTPError(404, \"Not found: \" + repr(environ['PATH_INFO']))\n        method = environ['REQUEST_METHOD'].upper()\n        if method in targets:\n            return targets[method], urlargs\n        if method == 'HEAD' and 'GET' in targets:\n            return targets['GET'], urlargs\n        if 'ANY' in targets:\n            return targets['ANY'], urlargs\n        allowed = [verb for verb in targets if verb != 'ANY']\n        if 'GET' in allowed and 'HEAD' not in allowed:\n            allowed.append('HEAD')\n        raise HTTPError(405, \"Method not allowed.\",\n                        header=[('Allow',\",\".join(allowed))])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compile(self):\n        ''' Prepare static and dynamic search structures. '''\n        self.static = {}\n        self.dynamic = []\n        def fpat_sub(m):\n            return m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:'\n        for rule in self.rules:\n            target = self.routes[rule]\n            if not self.syntax.search(rule):\n                self.static[rule.replace('\\\\:',':')] = target\n                continue\n            gpat = self._compile_pattern(rule)\n            fpat = re.sub(r'(\\\\*)(\\(\\?P<[^>]*>|\\((?!\\?))', fpat_sub, gpat.pattern)\n            gpat = gpat if gpat.groupindex else None\n            try:\n                combined = '%s|(%s)' % (self.dynamic[-1][0].pattern, fpat)\n                self.dynamic[-1] = (re.compile(combined), self.dynamic[-1][1])\n                self.dynamic[-1][1].append((gpat, target))\n            except (AssertionError, IndexError) as e: # AssertionError: Too many groups\n                self.dynamic.append((re.compile('(^%s$)'%fpat),\n                                    [(gpat, target)]))\n            except re.error as e:\n                raise RouteSyntaxError(\"Could not add Route: %s (%s)\" % (rule, e))", "response": "Compile the static and dynamic search structures."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compile_pattern(self, rule):\n        ''' Return a regular expression with named groups for each wildcard. '''\n        out = ''\n        for i, part in enumerate(self.syntax.split(rule)):\n            if i%3 == 0:   out += re.escape(part.replace('\\\\:',':'))\n            elif i%3 == 1: out += '(?P<%s>' % part if part else '(?:'\n            else:          out += '%s)' % (part or '[^/]+')\n        return re.compile('^%s$'%out)", "response": "Return a regular expression with named groups for each wildcard."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mount(self, app, prefix, **options):\n        ''' Mount an application to a specific URL prefix. The prefix is added\n            to SCIPT_PATH and removed from PATH_INFO before the sub-application\n            is called.\n\n            :param app: an instance of :class:`Bottle`.\n            :param prefix: path prefix used as a mount-point.\n\n            All other parameters are passed to the underlying :meth:`route` call.\n        '''\n        if not isinstance(app, Bottle):\n            raise TypeError('Only Bottle instances are supported for now.')\n        prefix = '/'.join([_f for _f in prefix.split('/') if _f])\n        if not prefix:\n            raise TypeError('Empty prefix. Perhaps you want a merge()?')\n        for other in self.mounts:\n            if other.startswith(prefix):\n                raise TypeError('Conflict with existing mount: %s' % other)\n        path_depth = prefix.count('/') + 1\n        options.setdefault('method', 'ANY')\n        options.setdefault('skip', True)\n        self.mounts[prefix] = app\n        @self.route('/%s/:#.*#' % prefix, **options)\n        def mountpoint():\n            request.path_shift(path_depth)\n            return app._handle(request.environ)", "response": "Mount an application to a specific URL prefix."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall a plugin to the list of plugins and resets the application s cache.", "response": "def install(self, plugin):\n        ''' Add a plugin to the list of plugins and prepare it for beeing\n            applied to all routes of this application. A plugin may be a simple\n            decorator or an object that implements the :class:`Plugin` API.\n        '''\n        if hasattr(plugin, 'setup'): plugin.setup(self)\n        if not callable(plugin) and not hasattr(plugin, 'apply'):\n            raise TypeError(\"Plugins must be callable or implement .apply()\")\n        self.plugins.append(plugin)\n        self.reset()\n        return plugin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting all routes and clear all the caches.", "response": "def reset(self, id=None):\n        ''' Reset all routes (force plugins to be re-applied) and clear all\n            caches. If an ID is given, only that specific route is affected. '''\n        if id is None: self.ccache.clear()\n        else: self.ccache.pop(id, None)\n        if DEBUG:\n            for route in self.routes:\n                if route['id'] not in self.ccache:\n                    self.ccache[route['id']] = self._build_callback(route)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the application and all installed plugins.", "response": "def close(self):\n        ''' Close the application and all installed plugins. '''\n        for plugin in self.plugins:\n            if hasattr(plugin, 'close'): plugin.close()\n        self.stopped = True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_callback(self, config):\n        ''' Apply plugins to a route and return a new callable. '''\n        wrapped = config['callback']\n        plugins = self.plugins + config['apply']\n        skip    = config['skip']\n        try:\n            for plugin in reversed(plugins):\n                if True in skip: break\n                if plugin in skip or type(plugin) in skip: continue\n                if getattr(plugin, 'name', True) in skip: continue\n                if hasattr(plugin, 'apply'):\n                    wrapped = plugin.apply(wrapped, config)\n                else:\n                    wrapped = plugin(wrapped)\n                if not wrapped: break\n                functools.update_wrapper(wrapped, config['callback'])\n            return wrapped\n        except RouteReset: # A plugin may have changed the config dict inplace.\n            return self._build_callback(config)", "response": "Apply plugins to a route and return a new callable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_url(self, routename, **kargs):\n        scriptname = request.environ.get('SCRIPT_NAME', '').strip('/') + '/'\n        location = self.router.build(routename, **kargs).lstrip('/')\n        return urljoin(urljoin('/', scriptname), location)", "response": "Return a URL that matches a named route"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef route(self, path=None, method='GET', callback=None, name=None,\n              apply=None, skip=None, **config):\n        \"\"\" A decorator to bind a function to a request URL. Example::\n\n                @app.route('/hello/:name')\n                def hello(name):\n                    return 'Hello %s' % name\n\n            The ``:name`` part is a wildcard. See :class:`Router` for syntax\n            details.\n\n            :param path: Request path or a list of paths to listen to. If no\n              path is specified, it is automatically generated from the\n              signature of the function.\n            :param method: HTTP method (`GET`, `POST`, `PUT`, ...) or a list of\n              methods to listen to. (default: `GET`)\n            :param callback: An optional shortcut to avoid the decorator\n              syntax. ``route(..., callback=func)`` equals ``route(...)(func)``\n            :param name: The name for this route. (default: None)\n            :param apply: A decorator or plugin or a list of plugins. These are\n              applied to the route callback in addition to installed plugins.\n            :param skip: A list of plugins, plugin classes or names. Matching\n              plugins are not installed to this route. ``True`` skips all.\n\n            Any additional keyword arguments are stored as route-specific\n            configuration and passed to plugins (see :meth:`Plugin.apply`).\n        \"\"\"\n        if callable(path): path, callback = None, path\n\n        plugins = makelist(apply)\n        skiplist = makelist(skip)\n        if 'decorate' in config:\n            depr(\"The 'decorate' parameter was renamed to 'apply'\") # 0.9\n            plugins += makelist(config.pop('decorate'))\n        if config.pop('no_hooks', False):\n            depr(\"The no_hooks parameter is no longer used. Add 'hooks' to the\"\\\n                 \" list of skipped plugins instead.\") # 0.9\n            skiplist.append('hooks')\n        static = config.get('static', False) # depr 0.9\n\n        def decorator(callback):\n            for rule in makelist(path) or yieldroutes(callback):\n                for verb in makelist(method):\n                    verb = verb.upper()\n                    cfg = dict(rule=rule, method=verb, callback=callback,\n                               name=name, app=self, config=config,\n                               apply=plugins, skip=skiplist)\n                    self.routes.append(cfg)\n                    cfg['id'] = self.routes.index(cfg)\n                    self.router.add(rule, verb, cfg['id'], name=name, static=static)\n                    if DEBUG: self.ccache[cfg['id']] = self._build_callback(cfg)\n            return callback\n\n        return decorator(callback) if callback else decorator", "response": "A route decorator that binds a function to a request URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, path=None, method='GET', **options):\n        return self.route(path, method, **options)", "response": "Equals : meth : route."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nequalling : meth. route with a POST method parameter.", "response": "def post(self, path=None, method='POST', **options):\n        \"\"\" Equals :meth:`route` with a ``POST`` method parameter. \"\"\"\n        return self.route(path, method, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nequal : meth. route with a PUT method parameter.", "response": "def put(self, path=None, method='PUT', **options):\n        \"\"\" Equals :meth:`route` with a ``PUT`` method parameter. \"\"\"\n        return self.route(path, method, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, path=None, method='DELETE', **options):\n        return self.route(path, method, **options)", "response": "Equals : meth. route with a DELETE method parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef error(self, code=500):\n        def wrapper(handler):\n            self.error_handler[int(code)] = handler\n            return handler\n        return wrapper", "response": "Decorator to register an output handler for an HTTP error code"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hook(self, name):\n        def wrapper(func):\n            self.hooks.add(name, func)\n            return func\n        return wrapper", "response": "Returns a decorator that attaches a callback to a hook."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle(self, path, method='GET'):\n        depr(\"This method will change semantics in 0.10. Try to avoid it.\")\n        if isinstance(path, dict):\n            return self._handle(path)\n        return self._handle({'PATH_INFO': path, 'REQUEST_METHOD': method.upper()})", "response": "(deprecated) Execute the first matching route callback and return\n            the result. :exc:`HTTPResponse` exceptions are catched and returned.\n            If :attr:`Bottle.catchall` is true, other exceptions are catched as\n            well and returned as :exc:`HTTPError` instances (500)."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbinds a new WSGI environment.", "response": "def bind(self, environ):\n        \"\"\" Bind a new WSGI environment.\n\n            This is done automatically for the global `bottle.request`\n            instance on every request.\n        \"\"\"\n        self.environ = environ\n        # These attributes are used anyway, so it is ok to compute them here\n        self.path = '/' + environ.get('PATH_INFO', '/').lstrip('/')\n        self.method = environ.get('REQUEST_METHOD', 'GET').upper()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshifting path fragments from PATH_INFO to SCRIPT_NAME and vice versa.", "response": "def path_shift(self, shift=1):\n        ''' Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.\n\n           :param shift: The number of path fragments to shift. May be negative\n                         to change the shift direction. (default: 1)\n        '''\n        script_name = self.environ.get('SCRIPT_NAME','/')\n        self['SCRIPT_NAME'], self.path = path_shift(script_name, self.path, shift)\n        self['PATH_INFO'] = self.path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tuple that can be used to reconstruct the full URL as requested by the client.", "response": "def urlparts(self):\n        ''' Return a :class:`urlparse.SplitResult` tuple that can be used\n            to reconstruct the full URL as requested by the client.\n            The tuple contains: (scheme, host, path, query_string, fragment).\n            The fragment is always empty because it is not visible to the server.\n        '''\n        env = self.environ\n        http = env.get('wsgi.url_scheme', 'http')\n        host = env.get('HTTP_X_FORWARDED_HOST') or env.get('HTTP_HOST')\n        if not host:\n            # HTTP 1.1 requires a Host-header. This is for HTTP/1.0 clients.\n            host = env.get('SERVER_NAME', '127.0.0.1')\n            port = env.get('SERVER_PORT')\n            if port and port != ('80' if http == 'http' else '443'):\n                host += ':' + port\n        spath = self.environ.get('SCRIPT_NAME','').rstrip('/') + '/'\n        rpath = self.path.lstrip('/')\n        path = urlquote(urljoin(spath, rpath))\n        return UrlSplitResult(http, host, path, env.get('QUERY_STRING'), '')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GET(self):\n        data = parse_qs(self.query_string, keep_blank_values=True)\n        get = self.environ['bottle.get'] = MultiDict()\n        for key, values in six.iteritems(data):\n            for value in values:\n                get[key] = value\n        return get", "response": "The QUERY_STRING parsed into an instance of MultiDict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a MultiDict containing the POST form values parsed into an instance of MultiDict.", "response": "def forms(self):\n        \"\"\" POST form values parsed into an instance of :class:`MultiDict`.\n\n            This property contains form values parsed from an `url-encoded`\n            or `multipart/form-data` encoded POST request bidy. The values are\n            native strings.\n        \"\"\"\n        forms = MultiDict()\n        for name, item in self.POST.iterallitems():\n            if not hasattr(item, 'filename'):\n                forms[name] = item\n        return forms"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef files(self):\n        files = MultiDict()\n        for name, item in self.POST.iterallitems():\n            if hasattr(item, 'filename'):\n                files[name] = item\n        return files", "response": "A property that returns a MultiDict containing the file uploads parsed into an instance of MultiDict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of cookies parsed into a dictionary. Signed cookies are NOT decoded automatically.", "response": "def COOKIES(self):\n        \"\"\" Cookies parsed into a dictionary. Signed cookies are NOT decoded\n            automatically. See :meth:`get_cookie` for details.\n        \"\"\"\n        raw_dict = SimpleCookie(self.headers.get('Cookie',''))\n        cookies = {}\n        for cookie in six.itervalues(raw_dict):\n            cookies[cookie.key] = cookie.value\n        return cookies"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cookie(self, key, secret=None):\n        value = self.COOKIES.get(key)\n        if secret and value:\n            dec = cookie_decode(value, secret) # (key, value) tuple or None\n            return dec[1] if dec and dec[0] == key else None\n        return value or None", "response": "Get the content of a cookie."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bind(self):\n        self._COOKIES = None\n        self.status = 200\n        self.headers = HeaderDict()\n        self.content_type = 'text/html; charset=UTF-8'", "response": "Binds the response object to its factory defaults."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a wsgi conform list of header value pairs.", "response": "def wsgiheader(self):\n        ''' Returns a wsgi conform list of header/value pairs. '''\n        for c in self.COOKIES.values():\n            if c.OutputString() not in self.headers.getall('Set-Cookie'):\n                self.headers.append('Set-Cookie', c.OutputString())\n        # rfc2616 section 10.2.3, 10.3.5\n        if self.status in (204, 304) and 'content-type' in self.headers:\n            del self.headers['content-type']\n        if self.status == 304:\n            for h in ('allow', 'content-encoding', 'content-language',\n                      'content-length', 'content-md5', 'content-range',\n                      'content-type', 'last-modified'): # + c-location, expires?\n                if h in self.headers:\n                     del self.headers[h]\n        return list(self.headers.iterallitems())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_cookie(self, key, value, secret=None, **kargs):\n        ''' Add a cookie or overwrite an old one. If the `secret` parameter is\n            set, create a `Signed Cookie` (described below).\n\n            :param key: the name of the cookie.\n            :param value: the value of the cookie.\n            :param secret: required for signed cookies. (default: None)\n            :param max_age: maximum age in seconds. (default: None)\n            :param expires: a datetime object or UNIX timestamp. (defaut: None)\n            :param domain: the domain that is allowed to read the cookie.\n              (default: current domain)\n            :param path: limits the cookie to a given path (default: /)\n\n            If neither `expires` nor `max_age` are set (default), the cookie\n            lasts only as long as the browser is not closed.\n\n            Signed cookies may store any pickle-able object and are\n            cryptographically signed to prevent manipulation. Keep in mind that\n            cookies are limited to 4kb in most browsers.\n\n            Warning: Signed cookies are not encrypted (the client can still see\n            the content) and not copy-protected (the client can restore an old\n            cookie). The main intention is to make pickling and unpickling\n            save, not to store secret information at client side.\n        '''\n        if secret:\n            value = touni(cookie_encode((key, value), secret))\n        elif not isinstance(value, six.string_types):\n            raise TypeError('Secret missing for non-string Cookie.')\n\n        self.COOKIES[key] = value\n        for k, v in six.iteritems(kargs):\n            self.COOKIES[key][k.replace('_', '-')] = v", "response": "Add a cookie or overwrite an existing one."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a cookie. Be sure to use the same `domain` and `path` parameters as used to create the cookie.", "response": "def delete_cookie(self, key, **kwargs):\n        ''' Delete a cookie. Be sure to use the same `domain` and `path`\n            parameters as used to create the cookie. '''\n        kwargs['max_age'] = -1\n        kwargs['expires'] = 0\n        self.set_cookie(key, '', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattach a callback to a hook.", "response": "def add(self, name, func):\n        ''' Attach a callback to a hook. '''\n        if name not in self.hooks:\n            raise ValueError(\"Unknown hook name %s\" % name)\n        was_empty = self._empty()\n        self.hooks[name].append(func)\n        if self.app and was_empty and not self._empty(): self.app.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate header field name to CGI environ key.", "response": "def _ekey(self, key):\n        ''' Translate header field name to CGI/WSGI environ key. '''\n        key = key.replace('-','_').upper()\n        if key in self.cgikeys:\n            return key\n        return 'HTTP_' + key"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef raw(self, key, default=None):\n        ''' Return the header value as is (may be bytes or unicode). '''\n        return self.environ.get(self._ekey(key), default)", "response": "Return the header value as is."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npush a new Bottle instance to the stack.", "response": "def push(self, value=None):\n        \"\"\" Add a new :class:`Bottle` instance to the stack \"\"\"\n        if not isinstance(value, Bottle):\n            value = Bottle()\n        self.append(value)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove comments from python code.", "response": "def split_comment(cls, code):\n        \"\"\" Removes comments (#...) from python code. \"\"\"\n        if '#' not in code: return code\n        #: Remove comments only (leave quoted strings as they are)\n        subf = lambda m: '' if m.group(0)[0]=='#' else m.group(0)\n        return re.sub(cls.re_pytokens, subf, code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef service(self):\n        if self._service is not None:\n            return self._service\n\n        if self._input_definition is None:\n            return None\n\n        splunkd_uri = self._input_definition.metadata[\"server_uri\"]\n        session_key = self._input_definition.metadata[\"session_key\"]\n\n        splunkd = urlsplit(splunkd_uri, allow_fragments=False)\n\n        self._service = Service(\n            scheme=splunkd.scheme,\n            host=splunkd.hostname,\n            port=splunkd.port,\n            token=session_key,\n        )\n\n        return self._service", "response": "Returns a service object for this script invocation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_input(self, validation_definition):\n        # Get the parameters from the ValidationDefinition object,\n        # then typecast the values as floats\n        minimum = float(validation_definition.parameters[\"min\"])\n        maximum = float(validation_definition.parameters[\"max\"])\n\n        if minimum >= maximum:\n            raise ValueError(\"min must be less than max; found min=%f, max=%f\" % minimum, maximum)", "response": "This method is used to validate the input for modular input."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stream_events(self, inputs, ew):\n        # Go through each input for this modular input\n        for input_name, input_item in six.iteritems(inputs.inputs):\n            # Get the values, cast them as floats\n            minimum = float(input_item[\"min\"])\n            maximum = float(input_item[\"max\"])\n\n            # Create an Event object, and set its data fields\n            event = Event()\n            event.stanza = input_name\n            event.data = \"number=\\\"%s\\\"\" % str(random.uniform(minimum, maximum))\n\n            # Tell the EventWriter to write this event\n            ew.write_event(event)", "response": "This function handles all the action that splunk calls this modular input\n        without arguments streams XML describing the inputs to stdin and waits\n        for XML on stdout describing events."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites an XML representation of self an Event object to the given stream.", "response": "def write_to(self, stream):\n        \"\"\"Write an XML representation of self, an ``Event`` object, to the given stream.\n\n        The ``Event`` object will only be written if its data field is defined,\n        otherwise a ``ValueError`` is raised.\n\n        :param stream: stream to write XML to.\n        \"\"\"\n        if self.data is None:\n            raise ValueError(\"Events must have at least the data field set to be written to XML.\")\n\n        event = ET.Element(\"event\")\n        if self.stanza is not None:\n            event.set(\"stanza\", self.stanza)\n        event.set(\"unbroken\", str(int(self.unbroken)))\n\n        # if a time isn't set, let Splunk guess by not creating a <time> element\n        if self.time is not None:\n            ET.SubElement(event, \"time\").text = str(self.time)\n\n        # add all other subelements to this Event, represented by (tag, text)\n        subelements = [\n            (\"source\", self.source),\n            (\"sourcetype\", self.sourceType),\n            (\"index\", self.index),\n            (\"host\", self.host),\n            (\"data\", self.data)\n        ]\n        for node, value in subelements:\n            if value is not None:\n                ET.SubElement(event, node).text = value\n\n        if self.done:\n            ET.SubElement(event, \"done\")\n\n        stream.write(ET.tostring(event))\n        stream.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string that is the trailing of the given template.", "response": "def _trailing(template, *targets):\n    \"\"\"Substring of *template* following all *targets*.\n\n    **Example**::\n\n        template = \"this is a test of the bunnies.\"\n        _trailing(template, \"is\", \"est\", \"the\") == \" bunnies\"\n\n    Each target is matched successively in the string, and the string\n    remaining after the last target is returned. If one of the targets\n    fails to match, a ValueError is raised.\n\n    :param template: Template to extract a trailing string from.\n    :type template: ``string``\n    :param targets: Strings to successively match in *template*.\n    :type targets: list of ``string``s\n    :return: Trailing string after all targets are matched.\n    :rtype: ``string``\n    :raises ValueError: Raised when one of the targets does not match.\n    \"\"\"\n    s = template\n    for t in targets:\n        n = s.find(t)\n        if n == -1:\n            raise ValueError(\"Target \" + t + \" not found in template.\")\n        s = s[n + len(t):]\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the list of system capabilities.", "response": "def capabilities(self):\n        \"\"\"Returns the list of system capabilities.\n\n        :return: A ``list`` of capabilities.\n        \"\"\"\n        response = self.get(PATH_CAPABILITIES)\n        return _load_atom(response, MATCH_ENTRY_CONTENT).capabilities"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the collection of the modular input kinds on this Splunk instance.", "response": "def modular_input_kinds(self):\n        \"\"\"Returns the collection of the modular input kinds on this Splunk instance.\n\n        :return: A :class:`ReadOnlyCollection` of :class:`ModularInputKind` entities.\n        \"\"\"\n        if self.splunk_version >= (5,):\n            return ReadOnlyCollection(self, PATH_MODULAR_INPUTS, item=ModularInputKind)\n        else:\n            raise IllegalOperationException(\"Modular inputs are not supported before Splunk version 5.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restart_required(self):\n        response = self.get(\"messages\").body.read()\n        messages = data.load(response)['feed']\n        if 'entry' not in messages:\n            result = False\n        else:\n            if isinstance(messages['entry'], dict):\n                titles = [messages['entry']['title']]\n            else:\n                titles = [x['title'] for x in messages['entry']]\n            result = 'restart_required' in titles\n        return result", "response": "Indicates whether splunkd is in a state that requires a restart."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef splunk_version(self):\n        if self._splunk_version is None:\n            self._splunk_version = tuple([int(p) for p in self.info['version'].split('.')])\n        return self._splunk_version", "response": "Returns the version of the splunkd instance this object is attached\n        to."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, path_segment=\"\", owner=None, app=None, sharing=None, **query):\n        # self.path to the Endpoint is relative in the SDK, so passing\n        # owner, app, sharing, etc. along will produce the correct\n        # namespace in the final request.\n        if path_segment.startswith('/'):\n            path = path_segment\n        else:\n            path = self.service._abspath(self.path + path_segment, owner=owner,\n                                         app=app, sharing=sharing)\n        # ^-- This was \"%s%s\" % (self.path, path_segment).\n        # That doesn't work, because self.path may be UrlEncoded.\n        return self.service.get(path,\n                                owner=owner, app=app, sharing=sharing,\n                                **query)", "response": "Performs a GET operation on the path segment relative to this endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a method and return the content Record from the returned XML.", "response": "def _run_action(self, path_segment, **kwargs):\n        \"\"\"Run a method and return the content Record from the returned XML.\n\n        A method is a relative path from an Entity that is not itself\n        an Entity. _run_action assumes that the returned XML is an\n        Atom field containing one Entry, and the contents of Entry is\n        what should be the return value. This is right in enough cases\n        to make this method useful.\n        \"\"\"\n        response = self.get(path_segment, **kwargs)\n        data = self._load_atom_entry(response)\n        rec = _parse_atom_entry(data)\n        return rec.content"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _proper_namespace(self, owner=None, app=None, sharing=None):\n        if owner is None and app is None and sharing is None: # No namespace provided\n            if self._state is not None and 'access' in self._state:\n                return (self._state.access.owner,\n                        self._state.access.app,\n                        self._state.access.sharing)\n            else:\n                return (self.service.namespace['owner'],\n                        self.service.namespace['app'],\n                        self.service.namespace['sharing'])\n        else:\n            return (owner,app,sharing)", "response": "Produce a namespace sans wildcards for use in entity requests."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef refresh(self, state=None):\n        if state is not None:\n            self._state = state\n        else:\n            self._state = self.read(self.get())\n        return self", "response": "Refreshes the state of this entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disable(self):\n        self.post(\"disable\")\n        if self.service.restart_required:\n            self.service.restart(120)\n        return self", "response": "Disables the entity at this endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _entity_path(self, state):\n        # This has been factored out so that it can be easily\n        # overloaded by Configurations, which has to switch its\n        # entities' endpoints from its own properties/ to configs/.\n        raw_path = urllib.parse.unquote(state.links.alternate)\n        if 'servicesNS/' in raw_path:\n            return _trailing(raw_path, 'servicesNS/', '/', '/')\n        elif 'services/' in raw_path:\n            return _trailing(raw_path, 'services/')\n        else:\n            return raw_path", "response": "Calculate the path to an entity to be returned."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_list(self, response):\n        # Some subclasses of Collection have to override this because\n        # splunkd returns something that doesn't match\n        # <feed><entry></entry><feed>.\n        entries = _load_atom_entries(response)\n        if entries is None: return []\n        entities = []\n        for entry in entries:\n            state = _parse_atom_entry(entry)\n            entity = self.item(\n                self.service,\n                self._entity_path(state),\n                state=state)\n            entities.append(entity)\n\n        return entities", "response": "Converts a response to a list of entities."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the metadata for the members of the collection.", "response": "def itemmeta(self):\n        \"\"\"Returns metadata for members of the collection.\n\n        Makes a single roundtrip to the server, plus two more at most if\n        the ``autologin`` field of :func:`connect` is set to ``True``.\n\n        :return: A :class:`splunklib.data.Record` object containing the metadata.\n\n        **Example**::\n\n            import splunklib.client as client\n            import pprint\n            s = client.connect(...)\n            pprint.pprint(s.apps.itemmeta())\n            {'access': {'app': 'search',\n                                    'can_change_perms': '1',\n                                    'can_list': '1',\n                                    'can_share_app': '1',\n                                    'can_share_global': '1',\n                                    'can_share_user': '1',\n                                    'can_write': '1',\n                                    'modifiable': '1',\n                                    'owner': 'admin',\n                                    'perms': {'read': ['*'], 'write': ['admin']},\n                                    'removable': '0',\n                                    'sharing': 'user'},\n             'fields': {'optional': ['author',\n                                        'configured',\n                                        'description',\n                                        'label',\n                                        'manageable',\n                                        'template',\n                                        'visible'],\n                                        'required': ['name'], 'wildcard': []}}\n        \"\"\"\n        response = self.get(\"_new\")\n        content = _load_atom(response, MATCH_ENTRY_CONTENT)\n        return _parse_atom_metadata(content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, count=None, **kwargs):\n        # response = self.get(count=count, **kwargs)\n        # return self._load_list(response)\n        return list(self.iter(count=count, **kwargs))", "response": "Retrieves a list of entities in this collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting an entity from the collection.", "response": "def delete(self, name, **params):\n        \"\"\"Deletes a specified entity from the collection.\n\n        :param name: The name of the entity to delete.\n        :type name: ``string``\n        :return: The collection.\n        :rtype: ``self``\n\n        This method is implemented for consistency with the REST API's DELETE\n        method.\n\n        If there is no *name* entity on the server, a ``KeyError`` is\n        thrown. This function always makes a roundtrip to the server.\n\n        **Example**::\n\n            import splunklib.client as client\n            c = client.connect(...)\n            saved_searches = c.saved_searches\n            saved_searches.create('my_saved_search',\n                                  'search * | head 1')\n            assert 'my_saved_search' in saved_searches\n            saved_searches.delete('my_saved_search')\n            assert 'my_saved_search' not in saved_searches\n        \"\"\"\n        name = UrlEncoded(name, encode_slash=True)\n        if 'namespace' in params:\n            namespace = params.pop('namespace')\n            params['owner'] = namespace.owner\n            params['app'] = namespace.app\n            params['sharing'] = namespace.sharing\n        try:\n            self.service.delete(_path(self.path, name), **params)\n        except HTTPError as he:\n            # An HTTPError with status code 404 means that the entity\n            # has already been deleted, and we reraise it as a\n            # KeyError.\n            if he.status == 404:\n                raise KeyError(\"No such entity %s\" % name)\n            else:\n                raise\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a GET request to the server on the collection.", "response": "def get(self, name=\"\", owner=None, app=None, sharing=None, **query):\n        \"\"\"Performs a GET request to the server on the collection.\n\n        If *owner*, *app*, and *sharing* are omitted, this method takes a\n        default namespace from the :class:`Service` object for this :class:`Endpoint`.\n        All other keyword arguments are included in the URL as query parameters.\n\n        :raises AuthenticationError: Raised when the ``Service`` is not logged in.\n        :raises HTTPError: Raised when an error in the request occurs.\n        :param path_segment: A path segment relative to this endpoint.\n        :type path_segment: ``string``\n        :param owner: The owner context of the namespace (optional).\n        :type owner: ``string``\n        :param app: The app context of the namespace (optional).\n        :type app: ``string``\n        :param sharing: The sharing mode for the namespace (optional).\n        :type sharing: \"global\", \"system\", \"app\", or \"user\"\n        :param query: All other keyword arguments, which are used as query\n            parameters.\n        :type query: ``string``\n        :return: The response from the server.\n        :rtype: ``dict`` with keys ``body``, ``headers``, ``reason``,\n                and ``status``\n\n        Example:\n\n        import splunklib.client\n            s = client.service(...)\n            saved_searches = s.saved_searches\n            saved_searches.get(\"my/saved/search\") == \\\\\n                {'body': ...a response reader object...,\n                 'headers': [('content-length', '26208'),\n                             ('expires', 'Fri, 30 Oct 1998 00:00:00 GMT'),\n                             ('server', 'Splunkd'),\n                             ('connection', 'close'),\n                             ('cache-control', 'no-store, max-age=0, must-revalidate, no-cache'),\n                             ('date', 'Fri, 11 May 2012 16:30:35 GMT'),\n                             ('content-type', 'text/xml; charset=utf-8')],\n                 'reason': 'OK',\n                 'status': 200}\n            saved_searches.get('nonexistant/search') # raises HTTPError\n            s.logout()\n            saved_searches.get() # raises AuthenticationError\n\n        \"\"\"\n        name = UrlEncoded(name, encode_slash=True)\n        return super(Collection, self).get(name, owner, app, sharing, **query)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubmits the given stanza as a dictionary of key - value pairs.", "response": "def submit(self, stanza):\n        \"\"\"Adds keys to the current configuration stanza as a\n        dictionary of key-value pairs.\n\n        :param stanza: A dictionary of key-value pairs for the stanza.\n        :type stanza: ``dict``\n        :return: The :class:`Stanza` object.\n        \"\"\"\n        body = _encode(**stanza)\n        self.service.post(self.path, body=body)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, username, realm=None):\n        if realm is None:\n            # This case makes the username optional, so\n            # the full name can be passed in as realm.\n            # Assume it's already encoded.\n            name = username\n        else:\n            # Encode each component separately\n            name = UrlEncoded(realm, encode_slash=True) + \":\" + UrlEncoded(username, encode_slash=True)\n\n        # Append the : expected at the end of the name\n        if name[-1] is not \":\":\n            name = name + \":\"\n        return Collection.delete(self, name)", "response": "Delete a storage password by username and realm."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a given index.", "response": "def delete(self, name):\n        \"\"\" Deletes a given index.\n\n        **Note**: This method is only supported in Splunk 5.0 and later.\n\n        :param name: The name of the index to delete.\n        :type name: ``string``\n        \"\"\"\n        if self.service.splunk_version >= (5,):\n            Collection.delete(self, name)\n        else:\n            raise IllegalOperationException(\"Deleting indexes via the REST API is \"\n                                            \"not supported before Splunk version 5.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attach(self, host=None, source=None, sourcetype=None):\n        args = { 'index': self.name }\n        if host is not None: args['host'] = host\n        if source is not None: args['source'] = source\n        if sourcetype is not None: args['sourcetype'] = sourcetype\n        path = UrlEncoded(PATH_RECEIVERS_STREAM + \"?\" + urllib.parse.urlencode(args), skip_encode=True)\n\n        cookie_or_auth_header = \"Authorization: Splunk %s\\r\\n\" % \\\n                                (self.service.token if self.service.token is _NoAuthenticationToken\n                                 else self.service.token.replace(\"Splunk \", \"\"))\n\n        # If we have cookie(s), use them instead of \"Authorization: ...\"\n        if self.service.has_cookies():\n            cookie_or_auth_header = \"Cookie: %s\\r\\n\" % _make_cookie_header(self.service.get_cookies().items())\n\n        # Since we need to stream to the index connection, we have to keep\n        # the connection open and use the Splunk extension headers to note\n        # the input mode\n        sock = self.service.connect()\n        headers = [(\"POST %s HTTP/1.1\\r\\n\" % str(self.service._abspath(path))).encode('utf-8'),\n                   (\"Host: %s:%s\\r\\n\" % (self.service.host, int(self.service.port))).encode('utf-8'),\n                   b\"Accept-Encoding: identity\\r\\n\",\n                   cookie_or_auth_header.encode('utf-8'),\n                   b\"X-Splunk-Input-Mode: Streaming\\r\\n\",\n                   b\"\\r\\n\"]\n\n        for h in headers:\n            sock.write(h)\n        return sock", "response": "Opens a writable socket for writing events to the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens a raw socket in a with block to write data to Splunk.", "response": "def attached_socket(self, *args, **kwargs):\n        \"\"\"Opens a raw socket in a ``with`` block to write data to Splunk.\n\n        The arguments are identical to those for :meth:`attach`. The socket is\n        automatically closed at the end of the ``with`` block, even if an\n        exception is raised in the block.\n\n        :param host: The host value for events written to the stream.\n        :type host: ``string``\n        :param source: The source value for events written to the stream.\n        :type source: ``string``\n        :param sourcetype: The sourcetype value for events written to the\n            stream.\n        :type sourcetype: ``string``\n\n        :returns: Nothing.\n\n        **Example**::\n\n            import splunklib.client as client\n            s = client.connect(...)\n            index = s.indexes['some_index']\n            with index.attached_socket(sourcetype='test') as sock:\n                sock.send('Test event\\\\r\\\\n')\n\n        \"\"\"\n        try:\n            sock = self.attach(*args, **kwargs)\n            yield sock\n        finally:\n            sock.shutdown(socket.SHUT_RDWR)\n            sock.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete the contents of the index. This method blocks until the index is empty, because it needs to restore values at the end of the operation. :param timeout: The time-out period for the operation, in seconds (the default is 60). :type timeout: ``integer`` :return: The :class:`Index`.", "response": "def clean(self, timeout=60):\n        \"\"\"Deletes the contents of the index.\n\n        This method blocks until the index is empty, because it needs to restore\n        values at the end of the operation.\n\n        :param timeout: The time-out period for the operation, in seconds (the\n            default is 60).\n        :type timeout: ``integer``\n\n        :return: The :class:`Index`.\n        \"\"\"\n        self.refresh()\n\n        tds = self['maxTotalDataSizeMB']\n        ftp = self['frozenTimePeriodInSecs']\n        was_disabled_initially = self.disabled\n        try:\n            if (not was_disabled_initially and \\\n                self.service.splunk_version < (5,)):\n                # Need to disable the index first on Splunk 4.x,\n                # but it doesn't work to disable it on 5.0.\n                self.disable()\n            self.update(maxTotalDataSizeMB=1, frozenTimePeriodInSecs=1)\n            self.roll_hot_buckets()\n\n            # Wait until event count goes to 0.\n            start = datetime.now()\n            diff = timedelta(seconds=timeout)\n            while self.content.totalEventCount != '0' and datetime.now() < start+diff:\n                sleep(1)\n                self.refresh()\n\n            if self.content.totalEventCount != '0':\n                raise OperationError(\"Cleaning index %s took longer than %s seconds; timing out.\" % (self.name, timeout))\n        finally:\n            # Restore original values\n            self.update(maxTotalDataSizeMB=tds, frozenTimePeriodInSecs=ftp)\n            if (not was_disabled_initially and \\\n                self.service.splunk_version < (5,)):\n                # Re-enable the index if it was originally enabled and we messed with it.\n                self.enable()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubmit a single event to the index using HTTP POST.", "response": "def submit(self, event, host=None, source=None, sourcetype=None):\n        \"\"\"Submits a single event to the index using ``HTTP POST``.\n\n        :param event: The event to submit.\n        :type event: ``string``\n        :param `host`: The host value of the event.\n        :type host: ``string``\n        :param `source`: The source value of the event.\n        :type source: ``string``\n        :param `sourcetype`: The sourcetype value of the event.\n        :type sourcetype: ``string``\n\n        :return: The :class:`Index`.\n        \"\"\"\n        args = { 'index': self.name }\n        if host is not None: args['host'] = host\n        if source is not None: args['source'] = source\n        if sourcetype is not None: args['sourcetype'] = sourcetype\n\n        # The reason we use service.request directly rather than POST\n        # is that we are not sending a POST request encoded using\n        # x-www-form-urlencoded (as we do not have a key=value body),\n        # because we aren't really sending a \"form\".\n        self.service.post(PATH_RECEIVERS_SIMPLE, body=event, **args)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload a file for immediate indexing.", "response": "def upload(self, filename, **kwargs):\n        \"\"\"Uploads a file for immediate indexing.\n\n        **Note**: The file must be locally accessible from the server.\n\n        :param filename: The name of the file to upload. The file can be a\n            plain, compressed, or archived file.\n        :type filename: ``string``\n        :param kwargs: Additional arguments (optional). For more about the\n            available parameters, see `Index parameters <http://dev.splunk.com/view/SP-CAAAEE6#indexparams>`_ on Splunk Developer Portal.\n        :type kwargs: ``dict``\n\n        :return: The :class:`Index`.\n        \"\"\"\n        kwargs['index'] = self.name\n        path = 'data/inputs/oneshot'\n        self.service.post(path, name=filename, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the server with any changes you ve made to the current input object.", "response": "def update(self, **kwargs):\n        \"\"\"Updates the server with any changes you've made to the current input\n        along with any additional arguments you specify.\n\n        :param kwargs: Additional arguments (optional). For more about the\n            available parameters, see `Input parameters <http://dev.splunk.com/view/SP-CAAAEE6#inputparams>`_ on Splunk Developer Portal.\n        :type kwargs: ``dict``\n\n        :return: The input this method was called on.\n        :rtype: class:`Input`\n        \"\"\"\n        # UDP and TCP inputs require special handling due to their restrictToHost\n        # field. For all other inputs kinds, we can dispatch to the superclass method.\n        if self.kind not in ['tcp', 'splunktcp', 'tcp/raw', 'tcp/cooked', 'udp']:\n            return super(Input, self).update(**kwargs)\n        else:\n            # The behavior of restrictToHost is inconsistent across input kinds and versions of Splunk.\n            # In Splunk 4.x, the name of the entity is only the port, independent of the value of\n            # restrictToHost. In Splunk 5.0 this changed so the name will be of the form <restrictToHost>:<port>.\n            # In 5.0 and 5.0.1, if you don't supply the restrictToHost value on every update, it will\n            # remove the host restriction from the input. As of 5.0.2 you simply can't change restrictToHost\n            # on an existing input.\n\n            # The logic to handle all these cases:\n            # - Throw an exception if the user tries to set restrictToHost on an existing input\n            #   for *any* version of Splunk.\n            # - Set the existing restrictToHost value on the update args internally so we don't\n            #   cause it to change in Splunk 5.0 and 5.0.1.\n            to_update = kwargs.copy()\n\n            if 'restrictToHost' in kwargs:\n                raise IllegalOperationException(\"Cannot set restrictToHost on an existing input with the SDK.\")\n            elif 'restrictToHost' in self._state.content and self.kind != 'udp':\n                to_update['restrictToHost'] = self._state.content['restrictToHost']\n\n            # Do the actual update operation.\n            return super(Input, self).update(**to_update)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, name, kind, **kwargs):\n        kindpath = self.kindpath(kind)\n        self.post(kindpath, name=name, **kwargs)\n\n        # If we created an input with restrictToHost set, then\n        # its path will be <restrictToHost>:<name>, not just <name>,\n        # and we have to adjust accordingly.\n\n        # Url encodes the name of the entity.\n        name = UrlEncoded(name, encode_slash=True)\n        path = _path(\n            self.path + kindpath,\n            '%s:%s' % (kwargs['restrictToHost'], name) \\\n                if 'restrictToHost' in kwargs else name\n                )\n        return Input(self.service, path, kind)", "response": "Creates an input of a specific kind in this collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, name, kind=None):\n        if kind is None:\n            self.service.delete(self[name].path)\n        else:\n            self.service.delete(self[name, kind].path)\n        return self", "response": "Removes an input from the collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a relative path to the resources for a given kind.", "response": "def kindpath(self, kind):\n        \"\"\"Returns a path to the resources for a given input kind.\n\n        :param `kind`: The kind of input:\n\n            - \"ad\": Active Directory\n\n            - \"monitor\": Files and directories\n\n            - \"registry\": Windows Registry\n\n            - \"script\": Scripts\n\n            - \"splunktcp\": TCP, processed\n\n            - \"tcp\": TCP, unprocessed\n\n            - \"udp\": UDP\n\n            - \"win-event-log-collections\": Windows event log\n\n            - \"win-perfmon\": Performance monitoring\n\n            - \"win-wmi-collections\": WMI\n\n        :type kind: ``string``\n\n        :return: The relative endpoint path.\n        :rtype: ``string``\n        \"\"\"\n        if kind == 'tcp':\n            return UrlEncoded('tcp/raw', skip_encode=True)\n        elif kind == 'splunktcp':\n            return UrlEncoded('tcp/cooked', skip_encode=True)\n        else:\n            return UrlEncoded(kind, skip_encode=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of inputs that are in the Inputs collection.", "response": "def list(self, *kinds, **kwargs):\n        \"\"\"Returns a list of inputs that are in the :class:`Inputs` collection.\n        You can also filter by one or more input kinds.\n\n        This function iterates over all possible inputs, regardless of any arguments you\n        specify. Because the :class:`Inputs` collection is the union of all the inputs of each\n        kind, this method implements parameters such as \"count\", \"search\", and so\n        on at the Python level once all the data has been fetched. The exception\n        is when you specify a single input kind, and then this method makes a single request\n        with the usual semantics for parameters.\n\n        :param kinds: The input kinds to return (optional).\n\n            - \"ad\": Active Directory\n\n            - \"monitor\": Files and directories\n\n            - \"registry\": Windows Registry\n\n            - \"script\": Scripts\n\n            - \"splunktcp\": TCP, processed\n\n            - \"tcp\": TCP, unprocessed\n\n            - \"udp\": UDP\n\n            - \"win-event-log-collections\": Windows event log\n\n            - \"win-perfmon\": Performance monitoring\n\n            - \"win-wmi-collections\": WMI\n\n        :type kinds: ``string``\n        :param kwargs: Additional arguments (optional):\n\n            - \"count\" (``integer``): The maximum number of items to return.\n\n            - \"offset\" (``integer``): The offset of the first item to return.\n\n            - \"search\" (``string``): The search query to filter responses.\n\n            - \"sort_dir\" (``string``): The direction to sort returned items:\n              \"asc\" or \"desc\".\n\n            - \"sort_key\" (``string``): The field to use for sorting (optional).\n\n            - \"sort_mode\" (``string``): The collating sequence for sorting\n              returned items: \"auto\", \"alpha\", \"alpha_case\", or \"num\".\n\n        :type kwargs: ``dict``\n\n        :return: A list of input kinds.\n        :rtype: ``list``\n        \"\"\"\n        if len(kinds) == 0:\n            kinds = self.kinds\n        if len(kinds) == 1:\n            kind = kinds[0]\n            logging.debug(\"Inputs.list taking short circuit branch for single kind.\")\n            path = self.kindpath(kind)\n            logging.debug(\"Path for inputs: %s\", path)\n            try:\n                path = UrlEncoded(path, skip_encode=True)\n                response = self.get(path, **kwargs)\n            except HTTPError as he:\n                if he.status == 404: # No inputs of this kind\n                    return []\n            entities = []\n            entries = _load_atom_entries(response)\n            if entries is None:\n                return [] # No inputs in a collection comes back with no feed or entry in the XML\n            for entry in entries:\n                state = _parse_atom_entry(entry)\n                # Unquote the URL, since all URL encoded in the SDK\n                # should be of type UrlEncoded, and all str should not\n                # be URL encoded.\n                path = urllib.parse.unquote(state.links.alternate)\n                entity = Input(self.service, path, kind, state=state)\n                entities.append(entity)\n            return entities\n\n        search = kwargs.get('search', '*')\n\n        entities = []\n        for kind in kinds:\n            response = None\n            try:\n                kind = UrlEncoded(kind, skip_encode=True)\n                response = self.get(self.kindpath(kind), search=search)\n            except HTTPError as e:\n                if e.status == 404:\n                    continue # No inputs of this kind\n                else:\n                    raise\n\n            entries = _load_atom_entries(response)\n            if entries is None: continue # No inputs to process\n            for entry in entries:\n                state = _parse_atom_entry(entry)\n                # Unquote the URL, since all URL encoded in the SDK\n                # should be of type UrlEncoded, and all str should not\n                # be URL encoded.\n                path = urllib.parse.unquote(state.links.alternate)\n                entity = Input(self.service, path, kind, state=state)\n                entities.append(entity)\n        if 'offset' in kwargs:\n            entities = entities[kwargs['offset']:]\n        if 'count' in kwargs:\n            entities = entities[:kwargs['count']]\n        if kwargs.get('sort_mode', None) == 'alpha':\n            sort_field = kwargs.get('sort_field', 'name')\n            if sort_field == 'name':\n                f = lambda x: x.name.lower()\n            else:\n                f = lambda x: x[sort_field].lower()\n            entities = sorted(entities, key=f)\n        if kwargs.get('sort_mode', None) == 'alpha_case':\n            sort_field = kwargs.get('sort_field', 'name')\n            if sort_field == 'name':\n                f = lambda x: x.name\n            else:\n                f = lambda x: x[sort_field]\n            entities = sorted(entities, key=f)\n        if kwargs.get('sort_dir', 'asc') == 'desc':\n            entities = list(reversed(entities))\n        return entities"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the current search and deletes the results cache.", "response": "def cancel(self):\n        \"\"\"Stops the current search and deletes the results cache.\n\n        :return: The :class:`Job`.\n        \"\"\"\n        try:\n            self.post(\"control\", action=\"cancel\")\n        except HTTPError as he:\n            if he.status == 404:\n                # The job has already been cancelled, so\n                # cancelling it twice is a nop.\n                pass\n            else:\n                raise\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a streaming handle to this job s events.", "response": "def events(self, **kwargs):\n        \"\"\"Returns a streaming handle to this job's events.\n\n        :param kwargs: Additional parameters (optional). For a list of valid\n            parameters, see `GET search/jobs/{search_id}/events\n            <http://docs.splunk.com/Documentation/Splunk/latest/RESTAPI/RESTsearch#GET_search.2Fjobs.2F.7Bsearch_id.7D.2Fevents>`_\n            in the REST API documentation.\n        :type kwargs: ``dict``\n\n        :return: The ``InputStream`` IO handle to this job's events.\n        \"\"\"\n        kwargs['segmentation'] = kwargs.get('segmentation', 'none')\n        return self.get(\"events\", **kwargs).body"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nindicates whether the job is done running.", "response": "def is_done(self):\n        \"\"\"Indicates whether this job finished running.\n\n        :return: ``True`` if the job is done, ``False`` if not.\n        :rtype: ``boolean``\n        \"\"\"\n        if not self.is_ready():\n            return False\n        done = (self._state.content['isDone'] == '1')\n        return done"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_ready(self):\n        response = self.get()\n        if response.status == 204:\n            return False\n        self._state = self.read(response)\n        ready = self._state.content['dispatchState'] not in ['QUEUED', 'PARSING']\n        return ready", "response": "Indicates whether the job is ready for querying."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef results(self, **query_params):\n        query_params['segmentation'] = query_params.get('segmentation', 'none')\n        return self.get(\"results\", **query_params).body", "response": "Returns a streaming handle to this job s search results."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a streaming handle to this job s preview search results.", "response": "def preview(self, **query_params):\n        \"\"\"Returns a streaming handle to this job's preview search results.\n\n        Unlike :class:`splunklib.results.ResultsReader`, which requires a job to\n        be finished to\n        return any results, the ``preview`` method returns any results that have\n        been generated so far, whether the job is running or not. The\n        returned search results are the raw data from the server. Pass\n        the handle returned to :class:`splunklib.results.ResultsReader` to get a\n        nice, Pythonic iterator over objects, as in::\n\n            import splunklib.client as client\n            import splunklib.results as results\n            service = client.connect(...)\n            job = service.jobs.create(\"search * | head 5\")\n            rr = results.ResultsReader(job.preview())\n            for result in rr:\n                if isinstance(result, results.Message):\n                    # Diagnostic messages may be returned in the results\n                    print '%s: %s' % (result.type, result.message)\n                elif isinstance(result, dict):\n                    # Normal events are returned as dicts\n                    print result\n            if rr.is_preview:\n                print \"Preview of a running search job.\"\n            else:\n                print \"Job is finished. Results are final.\"\n\n        This method makes one roundtrip to the server, plus at most\n        two more if\n        the ``autologin`` field of :func:`connect` is set to ``True``.\n\n        :param query_params: Additional parameters (optional). For a list of valid\n            parameters, see `GET search/jobs/{search_id}/results_preview\n            <http://docs.splunk.com/Documentation/Splunk/latest/RESTAPI/RESTsearch#GET_search.2Fjobs.2F.7Bsearch_id.7D.2Fresults_preview>`_\n            in the REST API documentation.\n        :type query_params: ``dict``\n\n        :return: The ``InputStream`` IO handle to this job's preview results.\n        \"\"\"\n        query_params['segmentation'] = query_params.get('segmentation', 'none')\n        return self.get(\"results_preview\", **query_params).body"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, query, **kwargs):\n        if kwargs.get(\"exec_mode\", None) == \"oneshot\":\n            raise TypeError(\"Cannot specify exec_mode=oneshot; use the oneshot method instead.\")\n        response = self.post(search=query, **kwargs)\n        sid = _load_sid(response)\n        return Job(self.service, sid)", "response": "Creates a search using a search query and any additional parameters\n            is provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export(self, query, **params):\n        if \"exec_mode\" in params:\n            raise TypeError(\"Cannot specify an exec_mode to export.\")\n        params['segmentation'] = params.get('segmentation', 'none')\n        return self.post(path_segment=\"export\",\n                         search=query,\n                         **params).body", "response": "Runs a search and immediately starts streaming preview events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef oneshot(self, query, **params):\n        if \"exec_mode\" in params:\n            raise TypeError(\"Cannot specify an exec_mode to oneshot.\")\n        params['segmentation'] = params.get('segmentation', 'none')\n        return self.post(search=query,\n                         exec_mode=\"oneshot\",\n                         **params).body", "response": "Run a oneshot search and return a stream of XML fragments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the saved search and returns the resulting search job.", "response": "def dispatch(self, **kwargs):\n        \"\"\"Runs the saved search and returns the resulting search job.\n\n        :param `kwargs`: Additional dispatch arguments (optional). For details,\n                         see the `POST saved/searches/{name}/dispatch\n                         <http://docs.splunk.com/Documentation/Splunk/latest/RESTAPI/RESTsearch#POST_saved.2Fsearches.2F.7Bname.7D.2Fdispatch>`_\n                         endpoint in the REST API documentation.\n        :type kwargs: ``dict``\n        :return: The :class:`Job`.\n        \"\"\"\n        response = self.post(\"dispatch\", **kwargs)\n        sid = _load_sid(response)\n        return Job(self.service, sid)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fired_alerts(self):\n        if self['is_scheduled'] == '0':\n            raise IllegalOperationException('Unscheduled saved searches have no alerts.')\n        c = Collection(\n            self.service,\n            self.service._abspath(PATH_FIRED_ALERTS + self.name,\n                                  owner=self._state.access.owner,\n                                  app=self._state.access.app,\n                                  sharing=self._state.access.sharing),\n            item=AlertGroup)\n        return c", "response": "Returns the collection of fired alert groups corresponding to this saved search."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef history(self):\n        response = self.get(\"history\")\n        entries = _load_atom_entries(response)\n        if entries is None: return []\n        jobs = []\n        for entry in entries:\n            job = Job(self.service, entry.title)\n            jobs.append(job)\n        return jobs", "response": "Returns a list of search jobs corresponding to this saved search."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the server with any changes you ve made to the current saved .", "response": "def update(self, search=None, **kwargs):\n        \"\"\"Updates the server with any changes you've made to the current saved\n        search along with any additional arguments you specify.\n\n        :param `search`: The search query (optional).\n        :type search: ``string``\n        :param `kwargs`: Additional arguments (optional). For a list of available\n            parameters, see `Saved search parameters\n            <http://dev.splunk.com/view/SP-CAAAEE5#savedsearchparams>`_\n            on Splunk Developer Portal.\n        :type kwargs: ``dict``\n\n        :return: The :class:`SavedSearch`.\n        \"\"\"\n        # Updates to a saved search *require* that the search string be\n        # passed, so we pass the current search string if a value wasn't\n        # provided by the caller.\n        if search is None: search = self.content.search\n        Entity.update(self, search=search, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of times when this search is scheduled to run.", "response": "def scheduled_times(self, earliest_time='now', latest_time='+1h'):\n        \"\"\"Returns the times when this search is scheduled to run.\n\n        By default this method returns the times in the next hour. For different\n        time ranges, set *earliest_time* and *latest_time*. For example,\n        for all times in the last day use \"earliest_time=-1d\" and\n        \"latest_time=now\".\n\n        :param earliest_time: The earliest time.\n        :type earliest_time: ``string``\n        :param latest_time: The latest time.\n        :type latest_time: ``string``\n\n        :return: The list of search times.\n        \"\"\"\n        response = self.get(\"scheduled_times\",\n                            earliest_time=earliest_time,\n                            latest_time=latest_time)\n        data = self._load_atom_entry(response)\n        rec = _parse_atom_entry(data)\n        times = [datetime.fromtimestamp(int(t))\n                 for t in rec.content.scheduled_times]\n        return times"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef suppressed(self):\n        r = self._run_action(\"suppress\")\n        if r.suppressed == \"1\":\n            return int(r.expiration)\n        else:\n            return 0", "response": "Returns the number of seconds that this search is blocked from running\n        ( possibly 0."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a saved search.", "response": "def create(self, name, search, **kwargs):\n        \"\"\" Creates a saved search.\n\n        :param name: The name for the saved search.\n        :type name: ``string``\n        :param search: The search query.\n        :type search: ``string``\n        :param kwargs: Additional arguments (optional). For a list of available\n            parameters, see `Saved search parameters\n            <http://dev.splunk.com/view/SP-CAAAEE5#savedsearchparams>`_\n            on Splunk Developer Portal.\n        :type kwargs: ``dict``\n        :return: The :class:`SavedSearches` collection.\n        \"\"\"\n        return Collection.create(self, name, search=search, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef role_entities(self):\n        return [self.service.roles[name] for name in self.content.roles]", "response": "Returns a list of roles assigned to this user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngrants additional capabilities to thisRole.", "response": "def grant(self, *capabilities_to_grant):\n        \"\"\"Grants additional capabilities to this role.\n\n        :param capabilities_to_grant: Zero or more capabilities to grant this\n            role. For a list of capabilities, see\n            `Capabilities <http://dev.splunk.com/view/SP-CAAAEJ6#capabilities>`_\n            on Splunk Developer Portal.\n        :type capabilities_to_grant: ``string`` or ``list``\n        :return: The :class:`Role`.\n\n        **Example**::\n\n            service = client.connect(...)\n            role = service.roles['somerole']\n            role.grant('change_own_password', 'search')\n        \"\"\"\n        possible_capabilities = self.service.capabilities\n        for capability in capabilities_to_grant:\n            if capability not in possible_capabilities:\n                raise NoSuchCapability(capability)\n        new_capabilities = self['capabilities'] + list(capabilities_to_grant)\n        self.post(capabilities=new_capabilities)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrevokes zero or more capabilities from thisRole.", "response": "def revoke(self, *capabilities_to_revoke):\n        \"\"\"Revokes zero or more capabilities from this role.\n\n        :param capabilities_to_revoke: Zero or more capabilities to grant this\n            role. For a list of capabilities, see\n            `Capabilities <http://dev.splunk.com/view/SP-CAAAEJ6#capabilities>`_\n            on Splunk Developer Portal.\n        :type capabilities_to_revoke: ``string`` or ``list``\n\n        :return: The :class:`Role`.\n\n        **Example**::\n\n            service = client.connect(...)\n            role = service.roles['somerole']\n            role.revoke('change_own_password', 'search')\n        \"\"\"\n        possible_capabilities = self.service.capabilities\n        for capability in capabilities_to_revoke:\n            if capability not in possible_capabilities:\n                raise NoSuchCapability(capability)\n        old_capabilities = self['capabilities']\n        new_capabilities = []\n        for c in old_capabilities:\n            if c not in capabilities_to_revoke:\n                new_capabilities.append(c)\n        if new_capabilities == []:\n            new_capabilities = '' # Empty lists don't get passed in the body, so we have to force an empty argument.\n        self.post(capabilities=new_capabilities)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, name, **params):\n        if not isinstance(name, six.string_types):\n            raise ValueError(\"Invalid role name: %s\" % str(name))\n        name = name.lower()\n        self.post(name=name, **params)\n        # splunkd doesn't return the user in the POST response body,\n        # so we have to make a second round trip to fetch it.\n        response = self.get(name)\n        entry = _load_atom(response, XNAME_ENTRY).entry\n        state = _parse_atom_entry(entry)\n        entity = self.item(\n            self.service,\n            urllib.parse.unquote(state.links.alternate),\n            state=state)\n        return entity", "response": "Creates a new role."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new KV Store Collection.", "response": "def create(self, name, indexes = {}, fields = {}, **kwargs):\n        \"\"\"Creates a KV Store Collection.\n\n        :param name: name of collection to create\n        :type name: ``string``\n        :param indexes: dictionary of index definitions\n        :type indexes: ``dict``\n        :param fields: dictionary of field definitions\n        :type fields: ``dict``\n        :param kwargs: a dictionary of additional parameters specifying indexes and field definitions\n        :type kwargs: ``dict``\n\n        :return: Result of POST request\n        \"\"\"\n        for k, v in six.iteritems(indexes):\n            if isinstance(v, dict):\n                v = json.dumps(v)\n            kwargs['index.' + k] = v\n        for k, v in six.iteritems(fields):\n            kwargs['field.' + k] = v\n        return self.post(name=name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the definition of a KV Store index.", "response": "def update_index(self, name, value):\n        \"\"\"Changes the definition of a KV Store index.\n\n        :param name: name of index to change\n        :type name: ``string``\n        :param value: new index definition\n        :type value: ``dict`` or ``string``\n\n        :return: Result of POST request\n        \"\"\"\n        kwargs = {}\n        kwargs['index.' + name] = value if isinstance(value, basestring) else json.dumps(value)\n        return self.post(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the definition of a KV Store field.", "response": "def update_field(self, name, value):\n        \"\"\"Changes the definition of a KV Store field.\n\n        :param name: name of field to change\n        :type name: ``string``\n        :param value: new field definition\n        :type value: ``string``\n\n        :return: Result of POST request\n        \"\"\"\n        kwargs = {}\n        kwargs['field.' + name] = value\n        return self.post(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery the cache for the specified set of keys.", "response": "def query(self, **query):\n        \"\"\"\n        Gets the results of query, with optional parameters sort, limit, skip, and fields.\n\n        :param query: Optional parameters. Valid options are sort, limit, skip, and fields\n        :type query: ``dict``\n\n        :return: Array of documents retrieved by query.\n        :rtype: ``array``\n        \"\"\"\n        return json.loads(self._get('', **query).body.read().decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns object with _id = id. :param id: Value for ID. If not a string will be coerced to string. :type id: ``string`` :return: Document with id :rtype: ``dict``", "response": "def query_by_id(self, id):\n        \"\"\"\n        Returns object with _id = id.\n\n        :param id: Value for ID. If not a string will be coerced to string.\n        :type id: ``string``\n\n        :return: Document with id\n        :rtype: ``dict``\n        \"\"\"\n        return json.loads(self._get(UrlEncoded(str(id))).body.read().decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert(self, data):\n        return json.loads(self._post('', headers=KVStoreCollectionData.JSON_HEADER, body=data).body.read().decode('utf-8'))", "response": "Inserts a new item into this collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, id, data):\n        return json.loads(self._post(UrlEncoded(str(id)), headers=KVStoreCollectionData.JSON_HEADER, body=data).body.read().decode('utf-8'))", "response": "Updates the _id of a document with data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning array of results from queries dbqueries. :param dbqueries: Array of individual queries as dictionaries :type dbqueries: ``array`` of ``dict`` :return: Results of each query :rtype: ``array`` of ``array``", "response": "def batch_find(self, *dbqueries):\n        \"\"\"\n        Returns array of results from queries dbqueries.\n\n        :param dbqueries: Array of individual queries as dictionaries\n        :type dbqueries: ``array`` of ``dict``\n\n        :return: Results of each query\n        :rtype: ``array`` of ``array``\n        \"\"\"\n        if len(dbqueries) < 1:\n            raise Exception('Must have at least one query.')\n\n        data = json.dumps(dbqueries)\n\n        return json.loads(self._post('batch_find', headers=KVStoreCollectionData.JSON_HEADER, body=data).body.read().decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts or updates every document specified in documents.", "response": "def batch_save(self, *documents):\n        \"\"\"\n        Inserts or updates every document specified in documents.\n\n        :param documents: Array of documents to save as dictionaries\n        :type documents: ``array`` of ``dict``\n\n        :return: Results of update operation as overall stats\n        :rtype: ``dict``\n        \"\"\"\n        if len(documents) < 1:\n            raise Exception('Must have at least one document.')\n\n        data = json.dumps(documents)\n\n        return json.loads(self._post('batch_save', headers=KVStoreCollectionData.JSON_HEADER, body=data).body.read().decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python_files(files):\n        python_files = []\n        for file_name in files:\n            if file_name.endswith(\".py\"):\n                python_files.append(file_name)\n\n        return python_files", "response": "Utility function to get. py files from a list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_cookies(cookie_str, dictionary):\n    parsed_cookie = six.moves.http_cookies.SimpleCookie(cookie_str)\n    for cookie in parsed_cookie.values():\n        dictionary[cookie.key] = cookie.coded_value", "response": "Parses any key - value pairs of cookies in a string and updates the dictionary with any key - value pairs found."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _authority(scheme=DEFAULT_SCHEME, host=DEFAULT_HOST, port=DEFAULT_PORT):\n    if ':' in host:\n        # IPv6 addresses must be enclosed in [ ] in order to be well\n        # formed.\n        host = '[' + host + ']'\n    return UrlEncoded(\"%s://%s:%s\" % (scheme, host, port), skip_encode=True)", "response": "Construct a URL authority from the given scheme host and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef namespace(sharing=None, owner=None, app=None, **kwargs):\n    if sharing in [\"system\"]:\n        return record({'sharing': sharing, 'owner': \"nobody\", 'app': \"system\" })\n    if sharing in [\"global\", \"app\"]:\n        return record({'sharing': sharing, 'owner': \"nobody\", 'app': app})\n    if sharing in [\"user\", None]:\n        return record({'sharing': sharing, 'owner': owner, 'app': app})\n    raise ValueError(\"Invalid value for argument: 'sharing'\")", "response": "Creates a Splunk namespace for the given Splunk resources."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _auth_headers(self):\n        if self.has_cookies():\n            return [(\"Cookie\", _make_cookie_header(list(self.get_cookies().items())))]\n        elif self.basic and (self.username and self.password):\n            token = 'Basic %s' % b64encode((\"%s:%s\" % (self.username, self.password)).encode('utf-8')).decode('ascii')\n            return [(\"Authorization\", token)]\n        elif self.token is _NoAuthenticationToken:\n            return []\n        else:\n            # Ensure the token is properly formatted\n            if self.token.startswith('Splunk '):\n                token = self.token\n            else:\n                token = 'Splunk %s' % self.token\n            return [(\"Authorization\", token)]", "response": "Returns a list of headers required to authenticate a Splunk request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        if self.scheme == \"https\":\n            sock = ssl.wrap_socket(sock)\n        sock.connect((socket.gethostbyname(self.host), self.port))\n        return sock", "response": "Returns an open connection to the Splunk instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, path_segment, method=\"GET\", headers=None, body=\"\",\n                owner=None, app=None, sharing=None):\n        \"\"\"Issues an arbitrary HTTP request to the REST path segment.\n\n        This method is named to match ``httplib.request``. This function\n        makes a single round trip to the server.\n\n        If *owner*, *app*, and *sharing* are omitted, this method uses the\n        default :class:`Context` namespace. All other keyword arguments are\n        included in the URL as query parameters.\n\n        :raises AuthenticationError: Raised when the ``Context`` object is not\n             logged in.\n        :raises HTTPError: Raised when an error occurred in a GET operation from\n             *path_segment*.\n        :param path_segment: A REST path segment.\n        :type path_segment: ``string``\n        :param method: The HTTP method to use (optional).\n        :type method: ``string``\n        :param headers: List of extra HTTP headers to send (optional).\n        :type headers: ``list`` of 2-tuples.\n        :param body: Content of the HTTP request (optional).\n        :type body: ``string``\n        :param owner: The owner context of the namespace (optional).\n        :type owner: ``string``\n        :param app: The app context of the namespace (optional).\n        :type app: ``string``\n        :param sharing: The sharing mode of the namespace (optional).\n        :type sharing: ``string``\n        :param query: All other keyword arguments, which are used as query\n            parameters.\n        :type query: ``string``\n        :return: The response from the server.\n        :rtype: ``dict`` with keys ``body``, ``headers``, ``reason``,\n                and ``status``\n\n        **Example**::\n\n            c = binding.connect(...)\n            c.request('saved/searches', method='GET') == \\\\\n                {'body': ...a response reader object...,\n                 'headers': [('content-length', '46722'),\n                             ('expires', 'Fri, 30 Oct 1998 00:00:00 GMT'),\n                             ('server', 'Splunkd'),\n                             ('connection', 'close'),\n                             ('cache-control', 'no-store, max-age=0, must-revalidate, no-cache'),\n                             ('date', 'Fri, 11 May 2012 17:24:19 GMT'),\n                             ('content-type', 'text/xml; charset=utf-8')],\n                 'reason': 'OK',\n                 'status': 200}\n            c.request('nonexistant/path', method='GET') # raises HTTPError\n            c.logout()\n            c.get('apps/local') # raises AuthenticationError\n        \"\"\"\n        if headers is None:\n            headers = []\n\n        path = self.authority \\\n            + self._abspath(path_segment, owner=owner,\n                            app=app, sharing=sharing)\n        all_headers = headers + self.additional_headers + self._auth_headers\n        logging.debug(\"%s request to %s (headers: %s, body: %s)\",\n                      method, path, str(all_headers), repr(body))\n        response = self.http.request(path,\n                                     {'method': method,\n                                     'headers': all_headers,\n                                     'body': body})\n        return response", "response": "Issues an arbitrary HTTP request to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef login(self):\n\n        if self.has_cookies() and \\\n                (not self.username and not self.password):\n            # If we were passed session cookie(s), but no username or\n            # password, then login is a nop, since we're automatically\n            # logged in.\n            return\n\n        if self.token is not _NoAuthenticationToken and \\\n                (not self.username and not self.password):\n            # If we were passed a session token, but no username or\n            # password, then login is a nop, since we're automatically\n            # logged in.\n            return\n\n        if self.basic and (self.username and self.password):\n            # Basic auth mode requested, so this method is a nop as long\n            # as credentials were passed in.\n            return\n\n        # Only try to get a token and updated cookie if username & password are specified\n        try:\n            response = self.http.post(\n                self.authority + self._abspath(\"/services/auth/login\"),\n                username=self.username,\n                password=self.password,\n                headers=self.additional_headers,\n                cookie=\"1\") # In Splunk 6.2+, passing \"cookie=1\" will return the \"set-cookie\" header\n\n            body = response.body.read()\n            session = XML(body).findtext(\"./sessionKey\")\n            self.token = \"Splunk %s\" % session\n            return self\n        except HTTPError as he:\n            if he.status == 401:\n                raise AuthenticationError(\"Login failed.\", he)\n            else:\n                raise", "response": "Logs into the Splunk instance referred to by the Context object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _abspath(self, path_segment,\n                owner=None, app=None, sharing=None):\n        \"\"\"Qualifies *path_segment* into an absolute path for a URL.\n\n        If *path_segment* is already absolute, returns it unchanged.\n        If *path_segment* is relative, then qualifies it with either\n        the provided namespace arguments or the ``Context``'s default\n        namespace. Any forbidden characters in *path_segment* are URL\n        encoded. This function has no network activity.\n\n        Named to be consistent with RFC2396_.\n\n        .. _RFC2396: http://www.ietf.org/rfc/rfc2396.txt\n\n        :param path_segment: A relative or absolute URL path segment.\n        :type path_segment: ``string``\n        :param owner, app, sharing: Components of a namespace (defaults\n                                    to the ``Context``'s namespace if all\n                                    three are omitted)\n        :type owner, app, sharing: ``string``\n        :return: A ``UrlEncoded`` (a subclass of ``str``).\n        :rtype: ``string``\n\n        **Example**::\n\n            import splunklib.binding as binding\n            c = binding.connect(owner='boris', app='search', sharing='user')\n            c._abspath('/a/b/c') == '/a/b/c'\n            c._abspath('/a/b c/d') == '/a/b%20c/d'\n            c._abspath('apps/local/search') == \\\n                '/servicesNS/boris/search/apps/local/search'\n            c._abspath('apps/local/search', sharing='system') == \\\n                '/servicesNS/nobody/system/apps/local/search'\n            url = c.authority + c._abspath('apps/local/sharing')\n        \"\"\"\n        skip_encode = isinstance(path_segment, UrlEncoded)\n        # If path_segment is absolute, escape all forbidden characters\n        # in it and return it.\n        if path_segment.startswith('/'):\n            return UrlEncoded(path_segment, skip_encode=skip_encode)\n\n        # path_segment is relative, so we need a namespace to build an\n        # absolute path.\n        if owner or app or sharing:\n            ns = namespace(owner=owner, app=app, sharing=sharing)\n        else:\n            ns = self.namespace\n\n        # If no app or owner are specified, then use the /services\n        # endpoint. Otherwise, use /servicesNS with the specified\n        # namespace. If only one of app and owner is specified, use\n        # '-' for the other.\n        if ns.app is None and ns.owner is None:\n            return UrlEncoded(\"/services/%s\" % path_segment, skip_encode=skip_encode)\n\n        oname = \"nobody\" if ns.owner is None else ns.owner\n        aname = \"system\" if ns.app is None else ns.app\n        path = UrlEncoded(\"/servicesNS/%s/%s/%s\" % (oname, aname, path_segment),\n                          skip_encode=skip_encode)\n        return path", "response": "Qualifies path_segment into an absolute path for a URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, url, headers=None, **kwargs):\n        if headers is None: headers = []\n        if kwargs:\n            # url is already a UrlEncoded. We have to manually declare\n            # the query to be encoded or it will get automatically URL\n            # encoded by being appended to url.\n            url = url + UrlEncoded('?' + _encode(**kwargs), skip_encode=True)\n        message = {\n            'method': \"DELETE\",\n            'headers': headers,\n        }\n        return self.request(url, message)", "response": "Sends a DELETE request to a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, url, headers=None, **kwargs):\n        if headers is None: headers = []\n        if kwargs:\n            # url is already a UrlEncoded. We have to manually declare\n            # the query to be encoded or it will get automatically URL\n            # encoded by being appended to url.\n            url = url + UrlEncoded('?' + _encode(**kwargs), skip_encode=True)\n        return self.request(url, { 'method': \"GET\", 'headers': headers })", "response": "Sends a GET request to a URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nissues an HTTP request to a URL.", "response": "def request(self, url, message, **kwargs):\n        \"\"\"Issues an HTTP request to a URL.\n\n        :param url: The URL.\n        :type url: ``string``\n        :param message: A dictionary with the format as described in\n            :class:`HttpLib`.\n        :type message: ``dict``\n        :param kwargs: Additional keyword arguments (optional). These arguments\n            are passed unchanged to the handler.\n        :type kwargs: ``dict``\n        :returns: A dictionary describing the response (see :class:`HttpLib` for\n            its structure).\n        :rtype: ``dict``\n        \"\"\"\n        response = self.handler(url, message, **kwargs)\n        response = record(response)\n        if 400 <= response.status:\n            raise HTTPError(response)\n\n        # Update the cookie with any HTTP request\n        # Initially, assume list of 2-tuples\n        key_value_tuples = response.headers\n        # If response.headers is a dict, get the key-value pairs as 2-tuples\n        # this is the case when using urllib2\n        if isinstance(response.headers, dict):\n            key_value_tuples = list(response.headers.items())\n        for key, value in key_value_tuples:\n            if key.lower() == \"set-cookie\":\n                _parse_cookies(value, self._cookies)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef peek(self, size):\n        c = self.read(size)\n        self._buffer = self._buffer + c\n        return c", "response": "Nondestructively retrieves a given number of characters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a given number of characters from the response.", "response": "def read(self, size = None):\n        \"\"\"Reads a given number of characters from the response.\n\n        :param size: The number of characters to read, or \"None\" to read the\n            entire response.\n        :type size: ``integer`` or \"None\"\n\n        \"\"\"\n        r = self._buffer\n        self._buffer = b''\n        if size is not None:\n            size -= len(r)\n        r = r + self._response.read(size)\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef readinto(self, byte_array):\n        max_size = len(byte_array)\n        data = self.read(max_size)\n        bytes_read = len(data)\n        byte_array[:bytes_read] = data\n        return bytes_read", "response": "Reads the data into a byte array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a stream containing XML and returns a ValidationDefinition object.", "response": "def parse(stream):\n        \"\"\"Creates a ``ValidationDefinition`` from a provided stream containing XML.\n\n        The XML typically will look like this:\n\n        ``<items>``\n        ``   <server_host>myHost</server_host>``\n        ``     <server_uri>https://127.0.0.1:8089</server_uri>``\n        ``     <session_key>123102983109283019283</session_key>``\n        ``     <checkpoint_dir>/opt/splunk/var/lib/splunk/modinputs</checkpoint_dir>``\n        ``     <item name=\"myScheme\">``\n        ``       <param name=\"param1\">value1</param>``\n        ``       <param_list name=\"param2\">``\n        ``         <value>value2</value>``\n        ``         <value>value3</value>``\n        ``         <value>value4</value>``\n        ``       </param_list>``\n        ``     </item>``\n        ``</items>``\n\n        :param stream: ``Stream`` containing XML to parse.\n        :return definition: A ``ValidationDefinition`` object.\n\n        \"\"\"\n\n        definition = ValidationDefinition()\n\n        # parse XML from the stream, then get the root node\n        root = ET.parse(stream).getroot()\n\n        for node in root:\n            # lone item node\n            if node.tag == \"item\":\n                # name from item node\n                definition.metadata[\"name\"] = node.get(\"name\")\n                definition.parameters = parse_xml_data(node, \"\")\n            else:\n                # Store anything else in metadata\n                definition.metadata[node.tag] = node.text\n\n        return definition"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef output_results(results, mvdelim = '\\n', output = sys.stdout):\n\n    # We collect all the unique field names, as well as \n    # convert all multivalue keys to the right form\n    fields = set()\n    for result in results:    \n        for key in result.keys():\n            if(isinstance(result[key], list)):\n                result['__mv_' + key] = encode_mv(result[key])\n                result[key] = mvdelim.join(result[key])\n        fields.update(list(result.keys()))\n\n    # convert the fields into a list and create a CSV writer\n    # to output to stdout\n    fields = sorted(list(fields))\n\n    writer = csv.DictWriter(output, fields)\n\n    # Write out the fields, and then the actual results\n    writer.writerow(dict(list(zip(fields, fields))))\n    writer.writerows(results)", "response": "Given a list of dictionaries each representing\n    a single result and an optional list of fields output those results to stdout for consumption by the Splunk pipeline"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_input(buf, has_header = True):\n\n    # Use stdin if there is no supplied buffer\n    if buf is None:\n        buf = sys.stdin\n\n    # Attempt to read a header if necessary\n    header = {}\n    if has_header:\n        # Until we get a blank line, read \"attr:val\" lines, \n        # setting the values in 'header'\n        last_attr = None\n        while True:\n            line = buf.readline()\n\n            # remove lastcharacter (which is a newline)\n            line = line[:-1] \n\n            # When we encounter a newline, we are done with the header\n            if len(line) == 0:\n                break\n\n            colon = line.find(':')\n\n            # If we can't find a colon, then it might be that we are\n            # on a new line, and it belongs to the previous attribute\n            if colon < 0:\n                if last_attr:\n                    header[last_attr] = header[last_attr] + '\\n' + urllib.parse.unquote(line)\n                else:\n                    continue\n\n            # extract it and set value in settings\n            last_attr = attr = line[:colon]\n            val  = urllib.parse.unquote(line[colon+1:])\n            header[attr] = val\n\n    return buf, header", "response": "Read the input from the given buffer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode multivalues into a string.", "response": "def encode_mv(vals):\n    \"\"\"For multivalues, values are wrapped in '$' and separated using ';'\n    Literal '$' values are represented with '$$'\"\"\"\n    s = \"\"\n    for val in vals:\n        val = val.replace('$', '$$')\n        if len(s) > 0:\n            s += ';'\n        s += '$' + val + '$'\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the input of the modular input.", "response": "def validate_input(self, validation_definition):\n        \"\"\"In this example we are using external validation to verify that the Github\n        repository exists. If validate_input does not raise an Exception, the input\n        is assumed to be valid. Otherwise it prints the exception as an error message\n        when telling splunkd that the configuration is invalid.\n\n        When using external validation, after splunkd calls the modular input with\n        --scheme to get a scheme, it calls it again with --validate-arguments for\n        each instance of the modular input in its configuration files, feeding XML\n        on stdin to the modular input to do validation. It is called the same way\n        whenever a modular input's configuration is edited.\n\n        :param validation_definition: a ValidationDefinition object\n        \"\"\"\n        # Get the values of the parameters, and construct a URL for the Github API\n        owner = validation_definition.parameters[\"owner\"]\n        repo_name = validation_definition.parameters[\"repo_name\"]\n        repo_url = \"https://api.github.com/repos/%s/%s\" % (owner, repo_name)\n\n        # Read the response from the Github API, then parse the JSON data into an object\n        response = urllib2.urlopen(repo_url).read()\n        jsondata = json.loads(response)\n\n        # If there is only 1 field in the jsondata object,some kind or error occurred\n        # with the Github API.\n        # Typically, this will happen with an invalid repository.\n        if len(jsondata) == 1:\n            raise ValueError(\"The Github repository was not found.\")\n\n        # If the API response seems normal, validate the fork count\n        # If there's something wrong with getting fork_count, raise a ValueError\n        try:\n            fork_count = int(jsondata[\"forks_count\"])\n        except ValueError as ve:\n            raise ValueError(\"Invalid fork count: %s\", ve.message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(cls, command, argv):\n        debug = environment.splunklib_logger.debug\n        command_class = type(command).__name__\n\n        # Prepare\n\n        debug('Parsing %s command line: %r', command_class, argv)\n        command.fieldnames = None\n        command.options.reset()\n        argv = ' '.join(argv)\n\n        command_args = cls._arguments_re.match(argv)\n\n        if command_args is None:\n            raise SyntaxError('Syntax error: {}'.format(argv))\n\n        # Parse options\n\n        for option in cls._options_re.finditer(command_args.group('options')):\n            name, value = option.group('name'), option.group('value')\n            if name not in command.options:\n                raise ValueError(\n                    'Unrecognized {} command option: {}={}'.format(command.name, name, json_encode_string(value)))\n            command.options[name].value = cls.unquote(value)\n\n        missing = command.options.get_missing()\n\n        if missing is not None:\n            if len(missing) > 1:\n                raise ValueError(\n                    'Values for these {} command options are required: {}'.format(command.name, ', '.join(missing)))\n            raise ValueError('A value for {} command option {} is required'.format(command.name, missing[0]))\n\n        # Parse field names\n\n        fieldnames = command_args.group('fieldnames')\n\n        if fieldnames is None:\n            command.fieldnames = []\n        else:\n            command.fieldnames = [cls.unquote(value.group(0)) for value in cls._fieldnames_re.finditer(fieldnames)]\n\n        debug('  %s: %s', command_class, command)", "response": "Parses the command line and returns a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unquote(cls, string):\n        if len(string) == 0:\n            return ''\n\n        if string[0] == '\"':\n            if len(string) == 1 or string[-1] != '\"':\n                raise SyntaxError('Poorly formed string literal: ' + string)\n            string = string[1:-1]\n\n        if len(string) == 0:\n            return ''\n\n        def replace(match):\n            value = match.group(0)\n            if value == '\"\"':\n                return '\"'\n            if len(value) < 2:\n                raise SyntaxError('Poorly formed string literal: ' + string)\n            return value[1]\n\n        result = re.sub(cls._escaped_character_re, replace, string)\n        return result", "response": "Unquotes a Splunk string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, ifile):\n        name, value = None, None\n\n        for line in ifile:\n            if line == '\\n':\n                break\n            item = line.split(':', 1)\n            if len(item) == 2:\n                # start of a new item\n                if name is not None:\n                    self[name] = value[:-1]  # value sans trailing newline\n                name, value = item[0], urllib.parse.unquote(item[1])\n            elif name is not None:\n                # continuation of the current item\n                value += urllib.parse.unquote(line)\n\n        if name is not None: self[name] = value[:-1] if value[-1] == '\\n' else value", "response": "Reads an input file into the internal dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting an external search command.", "response": "def execute(path, argv=None, environ=None, command_class=ExternalSearchCommand):\n    \"\"\"\n    :param path:\n    :type path: basestring\n    :param argv:\n    :type: argv: list, tuple, or None\n    :param environ:\n    :type environ: dict\n    :param command_class: External search command class to instantiate and execute.\n    :type command_class: type\n    :return:\n    :rtype: None\n    \"\"\"\n    assert issubclass(command_class, ExternalSearchCommand)\n    command_class(path, argv, environ).execute()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(stream):\n        definition = InputDefinition()\n\n        # parse XML from the stream, then get the root node\n        root = ET.parse(stream).getroot()\n\n        for node in root:\n            if node.tag == \"configuration\":\n                # get config for each stanza\n                definition.inputs = parse_xml_data(node, \"stanza\")\n            else:\n                definition.metadata[node.tag] = node.text\n\n        return definition", "response": "Parse a stream containing XML into an InputDefinition object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configure_logging(logger_name, filename=None):\n    if filename is None:\n        if logger_name is None:\n            probing_paths = [path.join('local', 'logging.conf'), path.join('default', 'logging.conf')]\n        else:\n            probing_paths = [\n                path.join('local', logger_name + '.logging.conf'),\n                path.join('default', logger_name + '.logging.conf'),\n                path.join('local', 'logging.conf'),\n                path.join('default', 'logging.conf')]\n        for relative_path in probing_paths:\n            configuration_file = path.join(app_root, relative_path)\n            if path.exists(configuration_file):\n                filename = configuration_file\n                break\n    elif not path.isabs(filename):\n        found = False\n        for conf in 'local', 'default':\n            configuration_file = path.join(app_root, conf, filename)\n            if path.exists(configuration_file):\n                filename = configuration_file\n                found = True\n                break\n        if not found:\n            raise ValueError('Logging configuration file \"{}\" not found in local or default directory'.format(filename))\n    elif not path.exists(filename):\n        raise ValueError('Logging configuration file \"{}\" not found'.format(filename))\n\n    if filename is not None:\n        global _current_logging_configuration_file\n        filename = path.realpath(filename)\n\n        if filename != _current_logging_configuration_file:\n            working_directory = getcwd()\n            chdir(app_root)\n            try:\n                fileConfig(filename, {'SPLUNK_HOME': splunk_home})\n            finally:\n                chdir(working_directory)\n            _current_logging_configuration_file = filename\n\n    if len(root.handlers) == 0:\n        root.addHandler(StreamHandler())\n\n    return None if logger_name is None else getLogger(logger_name), filename", "response": "Configure logging and return the named logger and the location of the logging configuration file loaded."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an unstash index if one is available.", "response": "def unstash_index(self, sync=False, branch=None):\n        \"\"\"Returns an unstash index if one is available.\"\"\"\n\n        stash_list = self.git_exec(['stash', 'list'], no_verbose=True)\n\n        if branch is None:\n            branch = self.get_current_branch_name()\n\n        for stash in stash_list.splitlines():\n\n            verb = 'syncing' if sync else 'switching'\n\n            if (\n                (('Legit' in stash) and\n                 ('On {0}:'.format(branch) in stash) and\n                 (verb in stash)\n                 ) or\n                (('GitHub' in stash) and\n                 ('On {0}:'.format(branch) in stash) and\n                 (verb in stash)\n                 )\n            ):\n                return stash[7]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unstash_it(self, sync=False):\n        if self.stash_index is not None:\n            return self.git_exec(\n                ['stash', 'pop', 'stash@{{{0}}}'.format(self.stash_index)])", "response": "Unstashes changes from current branch for branch sync"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npull the current branch and merges it into the current branch", "response": "def smart_pull(self):\n        \"\"\"\n        'git log --merges origin/master..master'\n        \"\"\"\n        branch = self.get_current_branch_name()\n\n        self.git_exec(['fetch', self.remote.name])\n\n        return self.smart_merge('{0}/{1}'.format(self.remote.name, branch),\n                                self.smart_merge_enabled())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck out given branch.", "response": "def checkout_branch(self, branch):\n        \"\"\"Checks out given branch.\"\"\"\n\n        _, stdout, stderr = self.git_exec(\n            ['checkout', branch],\n            with_extended_output=True)\n        return '\\n'.join([stderr, stdout])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking last commit not exist", "response": "def undo(self, hard=False):\n        \"\"\"Makes last commit not exist\"\"\"\n\n        if not self.fake:\n            return self.repo.git.reset('HEAD^', working_tree=hard)\n        else:\n            click.echo(crayons.red('Faked! >>> git reset {}{}'\n                                   .format('--hard ' if hard else '', 'HEAD^')))\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_branches(self, local=True, remote_branches=True):\n\n        if not self.repo.remotes:\n            remote_branches = False\n\n        branches = []\n\n        if remote_branches:\n\n            # Remote refs.\n            try:\n                for b in self.remote.refs:\n                    name = '/'.join(b.name.split('/')[1:])\n\n                    if name not in legit_settings.forbidden_branches:\n                        branches.append(Branch(name, is_published=True))\n            except (IndexError, AssertionError):\n                pass\n\n        if local:\n\n            # Local refs.\n            for b in [h.name for h in self.repo.heads]:\n\n                if (not remote_branches) or (b not in [br.name for br in branches]):\n                    if b not in legit_settings.forbidden_branches:\n                        branches.append(Branch(b, is_published=False))\n\n        return sorted(branches, key=attrgetter('name'))", "response": "Returns a list of local and remote branches."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nemitting header message executes a callable and echoes the return strings.", "response": "def status_log(func, message, *args, **kwargs):\n    \"\"\"Emits header message, executes a callable, and echoes the return strings.\"\"\"\n\n    click.echo(message)\n    log = func(*args, **kwargs)\n\n    if log:\n        out = []\n\n        for line in log.split('\\n'):\n            if not line.startswith('#'):\n                out.append(line)\n        click.echo(black('\\n'.join(out)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verbose_echo(str, verbose=False, fake=False):\n    verbose = fake or verbose\n\n    if verbose:\n        color = crayons.green\n        prefix = ''\n        if fake:\n            color = crayons.red\n            prefix = 'Faked!'\n        click.echo(color('{} >>> {}'.format(prefix, str)))", "response": "Selectively output str with special formatting if fake is True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(ctx, verbose, fake, install, uninstall, config):\n    # Create a repo object and remember it as as the context object.  From\n    # this point onwards other commands can refer to it by using the\n    # @pass_scm decorator.\n    ctx.obj = SCMRepo()\n    ctx.obj.fake = fake\n    ctx.obj.verbose = fake or verbose\n\n    if install:\n        do_install(ctx, verbose, fake)\n        ctx.exit()\n    elif uninstall:\n        do_uninstall(ctx, verbose, fake)\n        ctx.exit()\n    elif config:\n        do_edit_settings(fake)\n        ctx.exit()\n    else:\n        if ctx.invoked_subcommand is None:\n            # Display help to user if no commands were passed.\n            click.echo(format_help(ctx.get_help()))", "response": "legit command line interface"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nswitching from one branch to another safely stashing and restoring local changes.", "response": "def switch(scm, to_branch, verbose, fake):\n    \"\"\"Switches from one branch to another, safely stashing and restoring local changes.\n    \"\"\"\n    scm.fake = fake\n    scm.verbose = fake or verbose\n\n    scm.repo_check()\n\n    if to_branch is None:\n        scm.display_available_branches()\n        raise click.BadArgumentUsage('Please specify a branch to switch to')\n\n    scm.stash_log()\n    status_log(scm.checkout_branch, 'Switching to {0}.'.format(\n        crayons.yellow(to_branch)), to_branch)\n    scm.unstash_log()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sync(ctx, scm, to_branch, verbose, fake):\n    scm.fake = fake\n    scm.verbose = fake or verbose\n\n    scm.repo_check(require_remote=True)\n\n    if to_branch:\n        # Optional branch specifier.\n        branch = scm.fuzzy_match_branch(to_branch)\n        if branch:\n            is_external = True\n            original_branch = scm.get_current_branch_name()\n        else:\n            raise click.BadArgumentUsage(\n                \"Branch {0} does not exist. Use an existing branch.\"\n                .format(crayons.yellow(branch)))\n    else:\n        # Sync current branch.\n        branch = scm.get_current_branch_name()\n        is_external = False\n\n    if branch in scm.get_branch_names(local=False):\n        if is_external:\n            ctx.invoke(switch, to_branch=branch, verbose=verbose, fake=fake)\n        scm.stash_log(sync=True)\n        status_log(scm.smart_pull, 'Pulling commits from the server.')\n        status_log(scm.push, 'Pushing commits to the server.', branch)\n        scm.unstash_log(sync=True)\n        if is_external:\n            ctx.invoke(switch, to_branch=original_branch, verbose=verbose, fake=fake)\n    else:\n        raise click.BadArgumentUsage(\n            \"Branch {0} is not published. Publish before syncing.\"\n            .format(crayons.yellow(branch)))", "response": "Syncs the current branch with the remote server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npushing an unpublished branch to a remote repository.", "response": "def publish(scm, to_branch, verbose, fake):\n    \"\"\"Pushes an unpublished branch to a remote repository.\"\"\"\n    scm.fake = fake\n    scm.verbose = fake or verbose\n\n    scm.repo_check(require_remote=True)\n    branch = scm.fuzzy_match_branch(to_branch)\n\n    if not branch:\n        branch = scm.get_current_branch_name()\n        scm.display_available_branches()\n        if to_branch is None:\n            click.echo(\"Using current branch {0}\".format(crayons.yellow(branch)))\n        else:\n            click.echo(\n                \"Branch {0} not found, using current branch {1}\"\n                .format(crayons.red(to_branch), crayons.yellow(branch)))\n\n    branch_names = scm.get_branch_names(local=False)\n\n    if branch in branch_names:\n        raise click.BadArgumentUsage(\n            \"Branch {0} is already published. Use a branch that is not published.\"\n            .format(crayons.yellow(branch)))\n\n    status_log(scm.publish_branch, 'Publishing {0}.'.format(\n        crayons.yellow(branch)), branch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unpublish(scm, published_branch, verbose, fake):\n    scm.fake = fake\n    scm.verbose = fake or verbose\n\n    scm.repo_check(require_remote=True)\n    branch = scm.fuzzy_match_branch(published_branch)\n\n    if not branch:\n        scm.display_available_branches()\n        raise click.BadArgumentUsage('Please specify a branch to unpublish')\n\n    branch_names = scm.get_branch_names(local=False)\n\n    if branch not in branch_names:\n        raise click.BadArgumentUsage(\n            \"Branch {0} is not published. Use a branch that is published.\"\n            .format(crayons.yellow(branch)))\n\n    status_log(scm.unpublish_branch, 'Unpublishing {0}.'.format(\n        crayons.yellow(branch)), branch)", "response": "Removes a published branch from the remote repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the last commit from history.", "response": "def undo(scm, verbose, fake, hard):\n    \"\"\"Removes the last commit from history.\"\"\"\n    scm.fake = fake\n    scm.verbose = fake or verbose\n\n    scm.repo_check()\n\n    status_log(scm.undo, 'Last commit removed from history.', hard)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninstalls legit git aliases.", "response": "def do_install(ctx, verbose, fake):\n    \"\"\"Installs legit git aliases.\"\"\"\n    click.echo('The following git aliases will be installed:\\n')\n    aliases = cli.list_commands(ctx)\n    output_aliases(aliases)\n\n    if click.confirm('\\n{}Install aliases above?'.format('FAKE ' if fake else ''), default=fake):\n        for alias in aliases:\n            cmd = '!legit ' + alias\n            system_command = 'git config --global --replace-all alias.{0} \"{1}\"'.format(alias, cmd)\n            verbose_echo(system_command, verbose, fake)\n            if not fake:\n                os.system(system_command)\n        if not fake:\n            click.echo(\"\\nAliases installed.\")\n    else:\n        click.echo(\"\\nAliases will not be installed.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_edit_settings(fake):\n\n    path = resources.user.open('config.ini').name\n\n    click.echo('Legit Settings:\\n')\n\n    for (option, _, description) in legit_settings.config_defaults:\n        click.echo(columns([crayons.yellow(option), 25], [description, None]))\n    click.echo(\"\")  # separate settings info from os output\n\n    if fake:\n        click.echo(crayons.red('Faked! >>> edit {}'.format(path)))\n    else:\n        click.edit(path)", "response": "Opens legit settings in editor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding for showing commands in particular order", "response": "def list_commands(self, ctx):\n        \"\"\"Override for showing commands in particular order\"\"\"\n        commands = super(LegitGroup, self).list_commands(ctx)\n        return [cmd for cmd in order_manually(commands)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_command(self, ctx, cmd_name):\n        rv = click.Group.get_command(self, ctx, cmd_name)\n        if rv is not None:\n            return rv\n        cmd_name = self.command_aliases.get(cmd_name, \"\")\n        return click.Group.get_command(self, ctx, cmd_name)", "response": "Override to handle command aliases"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a ( compressed ) PERMAS dato or post file.", "response": "def read(filename):\n    \"\"\"Reads a (compressed) PERMAS dato or post file.\n    \"\"\"\n    # The format is specified at\n    # <http://www.intes.de>.\n\n    cells = {}\n    meshio_to_permas_type = {\n        \"vertex\": (1, \"PLOT1\"),\n        \"line\": (2, \"PLOTL2\"),\n        \"triangle\": (3, \"TRIA3\"),\n        \"quad\": (4, \"QUAD4\"),\n        \"tetra\": (4, \"TET4\"),\n        \"hexahedron\": (8, \"HEXE8\"),\n        \"wedge\": (6, \"PENTA6\"),\n        \"pyramid\": (5, \"PYRA5\"),\n    }\n\n    opener = gzip.open if filename.endswith(\".gz\") else open\n\n    with opener(filename, \"rb\") as f:\n        while True:\n            line = f.readline().decode(\"utf-8\")\n            if not line or re.search(\"\\\\$END STRUCTURE\", line):\n                break\n            for meshio_type, permas_ele in meshio_to_permas_type.items():\n                num_nodes = permas_ele[0]\n                permas_type = permas_ele[1]\n\n                if re.search(\"\\\\$ELEMENT TYPE = {}\".format(permas_type), line):\n                    while True:\n                        line = f.readline().decode(\"utf-8\")\n                        if not line or line.startswith(\"!\"):\n                            break\n                        data = numpy.array(line.split(), dtype=int)\n                        if meshio_type in cells:\n                            cells[meshio_type].append(data[-num_nodes:])\n                        else:\n                            cells[meshio_type] = [data[-num_nodes:]]\n\n            if re.search(\"\\\\$COOR\", line):\n                points = []\n                while True:\n                    line = f.readline().decode(\"utf-8\")\n                    if not line or line.startswith(\"!\"):\n                        break\n                    for r in numpy.array(line.split(), dtype=float)[1:]:\n                        points.append(r)\n\n    points = numpy.array(points)\n    points = numpy.reshape(points, newshape=(len(points) // 3, 3))\n    for key in cells:\n        # Subtract one to account for the fact that python indices\n        # are 0-based.\n        cells[key] = numpy.array(cells[key], dtype=int) - 1\n\n    return Mesh(points, cells)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(filename, mesh):\n    if mesh.points.shape[1] == 2:\n        logging.warning(\n            \"PERMAS requires 3D points, but 2D points given. \"\n            \"Appending 0 third component.\"\n        )\n        mesh.points = numpy.column_stack(\n            [mesh.points[:, 0], mesh.points[:, 1], numpy.zeros(mesh.points.shape[0])]\n        )\n\n    opener = gzip.open if filename.endswith(\".gz\") else open\n\n    with opener(filename, \"wb\") as fh:\n        fh.write(\n            \"\\n\".join(\n                [\n                    \"!\",\n                    \"! File written by meshio version {}\".format(__version__),\n                    \"! Further information available at {}\".format(__website__),\n                    \"!\",\n                    \"$ENTER COMPONENT NAME = DFLT_COMP DOFTYPE = DISP MATH\",\n                    \"    $SITUATION NAME = REAL_MODES\",\n                    \"        DFLT_COMP SYSTEM = NSV CONSTRAINTS = SPCVAR_1 ! LOADING = LOADVAR_1\",\n                    \"    $END SITUATION\" \"!\",\n                    \"!\",\n                    \"    $STRUCTURE\",\n                    \"!\" \"\\n\",\n                ]\n            ).encode(\"utf-8\")\n        )\n\n        # Write nodes\n        fh.write(\"        $COOR NSET = ALL_NODES\\n\".encode(\"utf-8\"))\n        for k, x in enumerate(mesh.points):\n            fh.write(\n                \"        {:8d} {:+.15f} {:+.15f} {:+.15f}\\n\".format(\n                    k + 1, x[0], x[1], x[2]\n                ).encode(\"utf-8\")\n            )\n\n        meshio_to_permas_type = {\n            \"vertex\": (1, \"PLOT1\"),\n            \"line\": (2, \"PLOTL2\"),\n            \"triangle\": (3, \"TRIA3\"),\n            \"quad\": (4, \"QUAD4\"),\n            \"tetra\": (4, \"TET4\"),\n            \"hexahedron\": (8, \"HEXE8\"),\n            \"wedge\": (6, \"PENTA6\"),\n            \"pyramid\": (5, \"PYRA5\"),\n        }\n        #\n        # Avoid non-unique element numbers in case of multiple element types by\n        # num_ele !!!\n        #\n        num_ele = 0\n\n        for meshio_type, cell in mesh.cells.items():\n            numcells, num_local_nodes = cell.shape\n            permas_type = meshio_to_permas_type[meshio_type]\n            fh.write(\"!\\n\".encode(\"utf-8\"))\n            fh.write(\n                \"        $ELEMENT TYPE = {} ESET = {}\\n\".format(\n                    permas_type[1], permas_type[1]\n                ).encode(\"utf-8\")\n            )\n            for k, c in enumerate(cell):\n                form = \"        {:8d} \" + \" \".join(num_local_nodes * [\"{:8d}\"]) + \"\\n\"\n                fh.write(form.format(k + num_ele + 1, *(c + 1)).encode(\"utf-8\"))\n            num_ele += numcells\n\n        fh.write(\"!\\n\".encode(\"utf-8\"))\n        fh.write(\"    $END STRUCTURE\\n\".encode(\"utf-8\"))\n        fh.write(\"!\\n\".encode(\"utf-8\"))\n        elem_3D = [\"HEXE8\", \"TET4\", \"PENTA6\", \"PYRA5\"]\n        elem_2D = [\"TRIA3\", \"QUAD4\"]\n        elem_1D = [\"PLOT1\", \"PLOTL2\"]\n        fh.write(\"    $SYSTEM NAME = NSV\\n\".encode(\"utf-8\"))\n        fh.write(\"!\\n\".encode(\"utf-8\"))\n        fh.write(\"        $ELPROP\\n\".encode(\"utf-8\"))\n        for meshio_type, cell in mesh.cells.items():\n            permas_type = meshio_to_permas_type[meshio_type]\n            if permas_type[1] in elem_3D:\n                fh.write(\n                    \"            {} MATERIAL = DUMMY_MATERIAL\\n\".format(\n                        permas_type[1]\n                    ).encode(\"utf-8\")\n                )\n            elif permas_type[1] in elem_2D:\n                fh.write(\n                    (\n                        12 * \" \"\n                        + \"{} GEODAT = GD_{} MATERIAL = DUMMY_MATERIAL\\n\".format(\n                            permas_type[1], permas_type[1]\n                        )\n                    ).encode(\"utf-8\")\n                )\n            else:\n                assert permas_type[1] in elem_1D\n        fh.write(\"!\\n\".encode(\"utf-8\"))\n        fh.write(\"        $GEODAT SHELL  CONT = THICK  NODES = ALL\\n\".encode(\"utf-8\"))\n        for meshio_type, cell in mesh.cells.items():\n            permas_type = meshio_to_permas_type[meshio_type]\n            if permas_type[1] in elem_2D:\n                fh.write(\n                    (12 * \" \" + \"GD_{} 1.0\\n\".format(permas_type[1])).encode(\"utf-8\")\n                )\n        fh.write(\n            \"\"\"!\n!\n    $END SYSTEM\n!\n    $CONSTRAINTS NAME = SPCVAR_1\n    $END CONSTRAINTS\n!\n    $LOADING NAME = LOADVAR_1\n    $END LOADING\n!\n$EXIT COMPONENT\n!\n$ENTER MATERIAL\n!\n    $MATERIAL NAME = DUMMY_MATERIAL TYPE = ISO\n!\n        $ELASTIC  GENERAL  INPUT = DATA\n            2.0E+05 0.3\n!\n        $DENSITY  GENERAL  INPUT = DATA\n            7.8E-09\n!\n        $THERMEXP  GENERAL  INPUT = DATA\n            1.2E-05\n!\n    $END MATERIAL\n!\n$EXIT MATERIAL\n!\n$FIN\n\"\"\".encode(\n                \"utf-8\"\n            )\n        )\n    return", "response": "Writes a PERMAS file to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_families(fm_group, tags):\n    for set_id, name in tags.items():\n        family = fm_group.create_group(_family_name(set_id, name))\n        family.attrs.create(\"NUM\", set_id)\n        group = family.create_group(\"GRO\")\n        group.attrs.create(\"NBR\", len(name))  # number of subsets\n        dataset = group.create_dataset(\"NOM\", (len(name),), dtype=\"80int8\")\n        for i in range(len(name)):\n            name_80 = name[i] + \"\\x00\" * (80 - len(name[i]))  # make name 80 characters\n            dataset[i] = [ord(x) for x in name_80]", "response": "Write point and cell tag information under FAS group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(filename):\n    import h5py\n\n    f = h5py.File(filename, \"r\")\n    dset = f[\"tstt\"]\n\n    points = dset[\"nodes\"][\"coordinates\"][()]\n    # read point data\n    point_data = {}\n    if \"tags\" in dset[\"nodes\"]:\n        for name, dataset in dset[\"nodes\"][\"tags\"].items():\n            point_data[name] = dataset[()]\n\n    # # Assert that the GLOBAL_IDs are contiguous.\n    # point_gids = dset['nodes']['tags']['GLOBAL_ID'][()]\n    # point_start_gid = dset['nodes']['coordinates'].attrs['start_id']\n    # point_end_gid = point_start_gid + len(point_gids) - 1\n    # assert all(point_gids == range(point_start_gid, point_end_gid + 1))\n\n    h5m_to_meshio_type = {\"Edge2\": \"line\", \"Tri3\": \"triangle\", \"Tet4\": \"tetra\"}\n    cells = {}\n    cell_data = {}\n    for h5m_type, data in dset[\"elements\"].items():\n        meshio_type = h5m_to_meshio_type[h5m_type]\n        conn = data[\"connectivity\"]\n        # Note that the indices are off by 1 in h5m.\n        cells[meshio_type] = conn[()] - 1\n\n        # TODO bring cell data back\n        # if 'tags' in data:\n        #     for name, dataset in data['tags'].items():\n        #         cell_data[name] = dataset[()]\n\n    # The `sets` in H5M are special in that they represent a segration of data\n    # in the current file, particularly by a load balancer (Metis, Zoltan,\n    # etc.). This segregation has no equivalent in other data types, but is\n    # certainly worthwhile visualizing.\n    # Hence, we will translate the sets into cell data with the prefix \"set::\"\n    # here.\n    field_data = {}\n    # TODO deal with sets\n    # if 'sets' in dset and 'contents' in dset['sets']:\n    #     # read sets\n    #     sets_contents = dset['sets']['contents'][()]\n    #     sets_list = dset['sets']['list'][()]\n    #     sets_tags = dset['sets']['tags']\n\n    #     cell_start_gid = conn.attrs['start_id']\n    #     cell_gids = cell_start_gid + elems['tags']['GLOBAL_ID'][()]\n    #     cell_end_gid = cell_start_gid + len(cell_gids) - 1\n    #     assert all(cell_gids == range(cell_start_gid, cell_end_gid + 1))\n\n    #     # create the sets\n    #     for key, value in sets_tags.items():\n    #         mod_key = 'set::' + key\n    #         cell_data[mod_key] = numpy.empty(len(cells), dtype=int)\n    #         end = 0\n    #         for k, row in enumerate(sets_list):\n    #             bits = _int_to_bool_list(row[3])\n    #             # is_owner = bits[0]\n    #             # is_unique = bits[1]\n    #             # is_ordered = bits[2]\n    #             is_range_compressed = bits[3]\n    #             if is_range_compressed:\n    #                 start_gids = sets_contents[end:row[0]+1:2]\n    #                 lengths = sets_contents[end+1:row[0]+1:2]\n    #                 for start_gid, length in zip(start_gids, lengths):\n    #                     end_gid = start_gid + length - 1\n    #                     if start_gid >= cell_start_gid and \\\n    #                             end_gid <= cell_end_gid:\n    #                         i0 = start_gid - cell_start_gid\n    #                         i1 = end_gid - cell_start_gid + 1\n    #                         cell_data[mod_key][i0:i1] = value[k]\n    #                     else:\n    #                         # TODO deal with point data\n    #                         raise RuntimeError('')\n    #             else:\n    #                 gids = sets_contents[end:row[0]+1]\n    #                 cell_data[mod_key][gids - cell_start_gid] = value[k]\n\n    #             end = row[0] + 1\n\n    return Mesh(\n        points, cells, point_data=point_data, cell_data=cell_data, field_data=field_data\n    )", "response": "Reads the H5M file and returns a dictionary of the data types and data types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(filename, mesh, add_global_ids=True):\n    import h5py\n\n    f = h5py.File(filename, \"w\")\n\n    tstt = f.create_group(\"tstt\")\n\n    # The base index for h5m is 1.\n    global_id = 1\n\n    # add nodes\n    nodes = tstt.create_group(\"nodes\")\n    coords = nodes.create_dataset(\"coordinates\", data=mesh.points)\n    coords.attrs.create(\"start_id\", global_id)\n    global_id += len(mesh.points)\n\n    # Global tags\n    tstt_tags = tstt.create_group(\"tags\")\n\n    # The GLOBAL_ID associated with a point is used to identify points if\n    # distributed across several processes. mbpart automatically adds them,\n    # too.\n    # Copy to pd to avoid changing point_data. The items are not deep-copied.\n    pd = mesh.point_data.copy()\n    if \"GLOBAL_ID\" not in pd and add_global_ids:\n        pd[\"GLOBAL_ID\"] = numpy.arange(1, len(mesh.points) + 1)\n\n    # add point data\n    if pd:\n        tags = nodes.create_group(\"tags\")\n    for key, data in pd.items():\n        if len(data.shape) == 1:\n            dtype = data.dtype\n            tags.create_dataset(key, data=data)\n        else:\n            # H5M doesn't accept n-x-k arrays as data; it wants an n-x-1\n            # array with k-tuples as entries.\n            n, k = data.shape\n            dtype = numpy.dtype((data.dtype, (k,)))\n            dset = tags.create_dataset(key, (n,), dtype=dtype)\n            dset[:] = data\n\n        # Create entry in global tags\n        g = tstt_tags.create_group(key)\n        g[\"type\"] = dtype\n        # Add a class tag:\n        # From\n        # <http://lists.mcs.anl.gov/pipermail/moab-dev/2015/007104.html>:\n        # ```\n        # /* Was dense tag data in mesh database */\n        #  define mhdf_DENSE_TYPE   2\n        # /** \\brief Was sparse tag data in mesh database */\n        # #define mhdf_SPARSE_TYPE  1\n        # /** \\brief Was bit-field tag data in mesh database */\n        # #define mhdf_BIT_TYPE     0\n        # /** \\brief Unused */\n        # #define mhdf_MESH_TYPE    3\n        #\n        g.attrs[\"class\"] = 2\n\n    # add elements\n    elements = tstt.create_group(\"elements\")\n\n    elem_dt = h5py.special_dtype(\n        enum=(\n            \"i\",\n            {\n                \"Edge\": 1,\n                \"Tri\": 2,\n                \"Quad\": 3,\n                \"Polygon\": 4,\n                \"Tet\": 5,\n                \"Pyramid\": 6,\n                \"Prism\": 7,\n                \"Knife\": 8,\n                \"Hex\": 9,\n                \"Polyhedron\": 10,\n            },\n        )\n    )\n\n    tstt[\"elemtypes\"] = elem_dt\n\n    tstt.create_dataset(\n        \"history\",\n        data=[\n            __name__.encode(\"utf-8\"),\n            __about__.__version__.encode(\"utf-8\"),\n            str(datetime.now()).encode(\"utf-8\"),\n        ],\n    )\n\n    # number of nodes to h5m name, element type\n    meshio_to_h5m_type = {\n        \"line\": {\"name\": \"Edge2\", \"type\": 1},\n        \"triangle\": {\"name\": \"Tri3\", \"type\": 2},\n        \"tetra\": {\"name\": \"Tet4\", \"type\": 5},\n    }\n    for key, data in mesh.cells.items():\n        if key not in meshio_to_h5m_type:\n            logging.warning(\"Unsupported H5M element type '%s'. Skipping.\", key)\n            continue\n        this_type = meshio_to_h5m_type[key]\n        elem_group = elements.create_group(this_type[\"name\"])\n        elem_group.attrs.create(\"element_type\", this_type[\"type\"], dtype=elem_dt)\n        # h5m node indices are 1-based\n        conn = elem_group.create_dataset(\"connectivity\", data=(data + 1))\n        conn.attrs.create(\"start_id\", global_id)\n        global_id += len(data)\n\n    # add cell data\n    if mesh.cell_data:\n        tags = elem_group.create_group(\"tags\")\n        for key, value in mesh.cell_data.items():\n            tags.create_dataset(key, data=value)\n\n    # add empty set -- MOAB wants this\n    sets = tstt.create_group(\"sets\")\n    sets.create_group(\"tags\")\n\n    # set max_id\n    tstt.attrs.create(\"max_id\", global_id, dtype=\"u8\")\n    return", "response": "Writes a MAB file to a H5M file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_header(f):\n\n    # http://gmsh.info/doc/texinfo/gmsh.html#MSH-file-format\n\n    line = f.readline().decode(\"utf-8\")\n    # Split the line\n    # 4.1 0 8\n    # into its components.\n    str_list = list(filter(None, line.split()))\n    fmt_version = str_list[0]\n    assert str_list[1] in [\"0\", \"1\"]\n    is_ascii = str_list[1] == \"0\"\n    data_size = int(str_list[2])\n    if not is_ascii:\n        # The next line is the integer 1 in bytes. Useful for checking\n        # endianness. Just assert that we get 1 here.\n        one = f.read(struct.calcsize(\"i\"))\n        assert struct.unpack(\"i\", one)[0] == 1\n        line = f.readline().decode(\"utf-8\")\n        assert line == \"\\n\"\n    line = f.readline().decode(\"utf-8\")\n    assert line.strip() == \"$EndMeshFormat\"\n    return fmt_version, data_size, is_ascii", "response": "Read the header of the mesh format block."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a Gmsh msh file.", "response": "def write(filename, mesh, fmt_version, write_binary=True):\n    \"\"\"Writes a Gmsh msh file.\n    \"\"\"\n    try:\n        writer = _writers[fmt_version]\n    except KeyError:\n        try:\n            writer = _writers[fmt_version.split(\".\")[0]]\n        except KeyError:\n            raise ValueError(\n                \"Need mesh format in {} (got {})\".format(\n                    sorted(_writers.keys()), fmt_version\n                )\n            )\n\n    writer.write(filename, mesh, write_binary=write_binary)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a Kratos file.", "response": "def write(filename, mesh, write_binary=False):\n    \"\"\"Writes mdpa files, cf.\n    <https://github.com/KratosMultiphysics/Kratos/wiki/Input-data>.\n    \"\"\"\n    assert not write_binary\n    if mesh.points.shape[1] == 2:\n        logging.warning(\n            \"mdpa requires 3D points, but 2D points given. \"\n            \"Appending 0 third component.\"\n        )\n        mesh.points = numpy.column_stack(\n            [mesh.points[:, 0], mesh.points[:, 1], numpy.zeros(mesh.points.shape[0])]\n        )\n\n    # Kratos cells are mostly ordered like VTK, with a few exceptions:\n    cells = mesh.cells.copy()\n    if \"hexahedron20\" in cells:\n        cells[\"hexahedron20\"] = cells[\"hexahedron20\"][\n            :, [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 10, 9, 16, 17, 18, 19, 12, 15, 14, 13]\n        ]\n    if \"hexahedron27\" in cells:\n        cells[\"hexahedron27\"] = cells[\"hexahedron27\"][\n            :,\n            [\n                0,\n                1,\n                2,\n                3,\n                4,\n                5,\n                6,\n                7,\n                8,\n                11,\n                10,\n                9,\n                16,\n                17,\n                18,\n                19,\n                12,\n                15,\n                14,\n                13,\n                22,\n                24,\n                21,\n                23,\n                20,\n                25,\n                26,\n            ],\n        ]\n\n    with open(filename, \"wb\") as fh:\n        # Write some additional info\n        fh.write((\"Begin ModelPartData\\n\").encode(\"utf-8\"))\n        fh.write((\"//  VARIABLE_NAME value\\n\").encode(\"utf-8\"))\n        fh.write((\"End ModelPartData\\n\\n\").encode(\"utf-8\"))\n        fh.write((\"Begin Properties 0\\n\").encode(\"utf-8\"))\n        fh.write((\"End Properties\\n\\n\").encode(\"utf-8\"))\n\n        # Split the cell data: gmsh:physical and gmsh:geometrical are tags, the\n        # rest is actual cell data.\n        tag_data = {}\n        other_data = {}\n        for cell_type, a in mesh.cell_data.items():\n            tag_data[cell_type] = {}\n            other_data[cell_type] = {}\n            for key, data in a.items():\n                if key in [\"gmsh:physical\", \"gmsh:geometrical\"]:\n                    tag_data[cell_type][key] = data.astype(numpy.int32)\n                else:\n                    other_data[cell_type][key] = data\n\n        # We identity which dimension are we\n        dimension = 2\n        for cell_type in cells.keys():\n            name_elem = _meshio_to_mdpa_type[cell_type]\n            if local_dimension_types[name_elem] == 3:\n                dimension = 3\n                break\n\n        # We identify the entities\n        _write_nodes(fh, mesh.points, write_binary)\n        _write_elements_and_conditions(fh, cells, tag_data, write_binary, dimension)\n        for name, dat in mesh.point_data.items():\n            _write_data(fh, \"NodalData\", name, dat, write_binary)\n        cell_data_raw = raw_from_cell_data(other_data)\n        for (\n            name,\n            dat,\n        ) in (\n            cell_data_raw.items()\n        ):  # NOTE: We will assume always when writing that the components are elements (for now)\n            _write_data(fh, \"ElementalData\", name, dat, write_binary)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a Gmsh file.", "response": "def write(filename, mesh, write_binary=True):\n    \"\"\"Writes msh files, cf.\n    <http://gmsh.info//doc/texinfo/gmsh.html#MSH-ASCII-file-format>.\n    \"\"\"\n    if mesh.points.shape[1] == 2:\n        logging.warning(\n            \"msh2 requires 3D points, but 2D points given. \"\n            \"Appending 0 third component.\"\n        )\n        mesh.points = numpy.column_stack(\n            [mesh.points[:, 0], mesh.points[:, 1], numpy.zeros(mesh.points.shape[0])]\n        )\n\n    if write_binary:\n        for key, value in mesh.cells.items():\n            if value.dtype != c_int:\n                logging.warning(\n                    \"Binary Gmsh needs 32-bit integers (got %s). Converting.\",\n                    value.dtype,\n                )\n                mesh.cells[key] = numpy.array(value, dtype=c_int)\n\n    # Gmsh cells are mostly ordered like VTK, with a few exceptions:\n    cells = mesh.cells.copy()\n    if \"tetra10\" in cells:\n        cells[\"tetra10\"] = cells[\"tetra10\"][:, [0, 1, 2, 3, 4, 5, 6, 7, 9, 8]]\n    if \"hexahedron20\" in cells:\n        cells[\"hexahedron20\"] = cells[\"hexahedron20\"][\n            :, [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 13, 9, 16, 18, 19, 17, 10, 12, 14, 15]\n        ]\n\n    with open(filename, \"wb\") as fh:\n        mode_idx = 1 if write_binary else 0\n        size_of_double = 8\n        fh.write(\n            (\"$MeshFormat\\n2.2 {} {}\\n\".format(mode_idx, size_of_double)).encode(\n                \"utf-8\"\n            )\n        )\n        if write_binary:\n            fh.write(struct.pack(\"i\", 1))\n            fh.write(\"\\n\".encode(\"utf-8\"))\n        fh.write(\"$EndMeshFormat\\n\".encode(\"utf-8\"))\n\n        if mesh.field_data:\n            _write_physical_names(fh, mesh.field_data)\n\n        # Split the cell data: gmsh:physical and gmsh:geometrical are tags, the\n        # rest is actual cell data.\n        tag_data = {}\n        other_data = {}\n        for cell_type, a in mesh.cell_data.items():\n            tag_data[cell_type] = {}\n            other_data[cell_type] = {}\n            for key, data in a.items():\n                if key in [\"gmsh:physical\", \"gmsh:geometrical\"]:\n                    tag_data[cell_type][key] = data.astype(c_int)\n                else:\n                    other_data[cell_type][key] = data\n\n        _write_nodes(fh, mesh.points, write_binary)\n        _write_elements(fh, cells, tag_data, write_binary)\n        if mesh.gmsh_periodic is not None:\n            _write_periodic(fh, mesh.gmsh_periodic)\n        for name, dat in mesh.point_data.items():\n            _write_data(fh, \"NodeData\", name, dat, write_binary)\n        cell_data_raw = raw_from_cell_data(other_data)\n        for name, dat in cell_data_raw.items():\n            _write_data(fh, \"ElementData\", name, dat, write_binary)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread an unstructured mesh with added data.", "response": "def read(filename, file_format=None):\n    \"\"\"Reads an unstructured mesh with added data.\n\n    :param filenames: The files to read from.\n    :type filenames: str\n\n    :returns mesh{2,3}d: The mesh data.\n    \"\"\"\n    # https://stackoverflow.com/q/4843173/353337\n    assert isinstance(filename, str)\n\n    if not file_format:\n        # deduce file format from extension\n        file_format = _filetype_from_filename(filename)\n\n    format_to_reader = {\n        \"ansys\": ansys_io,\n        \"ansys-ascii\": ansys_io,\n        \"ansys-binary\": ansys_io,\n        #\n        \"gmsh\": msh_io,\n        \"gmsh-ascii\": msh_io,\n        \"gmsh-binary\": msh_io,\n        \"gmsh2\": msh_io,\n        \"gmsh2-ascii\": msh_io,\n        \"gmsh2-binary\": msh_io,\n        \"gmsh4\": msh_io,\n        \"gmsh4-ascii\": msh_io,\n        \"gmsh4-binary\": msh_io,\n        #\n        \"med\": med_io,\n        \"medit\": medit_io,\n        \"dolfin-xml\": dolfin_io,\n        \"permas\": permas_io,\n        \"moab\": h5m_io,\n        \"off\": off_io,\n        #\n        \"stl\": stl_io,\n        \"stl-ascii\": stl_io,\n        \"stl-binary\": stl_io,\n        #\n        \"vtu-ascii\": vtu_io,\n        \"vtu-binary\": vtu_io,\n        #\n        \"vtk-ascii\": vtk_io,\n        \"vtk-binary\": vtk_io,\n        #\n        \"xdmf\": xdmf_io,\n        \"exodus\": exodus_io,\n        #\n        \"abaqus\": abaqus_io,\n        #\n        \"mdpa\": mdpa_io,\n    }\n\n    assert file_format in format_to_reader, \"Unknown file format '{}' of '{}'.\".format(\n        file_format, filename\n    )\n\n    return format_to_reader[file_format].read(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a mesh together with data to a file.", "response": "def write(filename, mesh, file_format=None, **kwargs):\n    \"\"\"Writes mesh together with data to a file.\n\n    :params filename: File to write to.\n    :type filename: str\n\n    :params point_data: Named additional point data to write to the file.\n    :type point_data: dict\n    \"\"\"\n    if not file_format:\n        # deduce file format from extension\n        file_format = _filetype_from_filename(filename)\n\n    # check cells for sanity\n    for key, value in mesh.cells.items():\n        if key[:7] == \"polygon\":\n            assert value.shape[1] == int(key[7:])\n        else:\n            assert value.shape[1] == num_nodes_per_cell[key]\n\n    try:\n        interface, args, default_kwargs = _writer_map[file_format]\n    except KeyError:\n        raise KeyError(\n            \"Unknown format '{}'. Pick one of {}\".format(\n                file_format, sorted(list(_writer_map.keys()))\n            )\n        )\n\n    # Build kwargs\n    _kwargs = default_kwargs.copy()\n    _kwargs.update(kwargs)\n\n    # Write\n    return interface.write(filename, mesh, *args, **_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next_items(self, n):\n        items = []\n        while len(items) < n:\n            if self._line_ptr >= len(self._line):\n                # Load the next line.\n                line = next(self._file).strip()\n                # Skip all comment and empty lines.\n                while not line or line[0] == \"#\":\n                    line = next(self._file).strip()\n                self._line = self._re_delimiter.split(line)\n                self._line_ptr = 0\n            n_read = min(n - len(items), len(self._line) - self._line_ptr)\n            items.extend(self._line[self._line_ptr : self._line_ptr + n_read])\n            self._line_ptr += n_read\n        return items", "response": "Returns the next n items. Throws StopIteration when there is not enough data to return n items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_param_map(word, required_keys=None):\n    if required_keys is None:\n        required_keys = []\n    words = word.split(\",\")\n    param_map = {}\n    for wordi in words:\n        if \"=\" not in wordi:\n            key = wordi.strip()\n            value = None\n        else:\n            sword = wordi.split(\"=\")\n            assert len(sword) == 2, sword\n            key = sword[0].strip()\n            value = sword[1].strip()\n        param_map[key] = value\n\n    msg = \"\"\n    for key in required_keys:\n        if key not in param_map:\n            msg += \"%r not found in %r\\n\" % (key, word)\n    if msg:\n        raise RuntimeError(msg)\n    return param_map", "response": "get the optional arguments on a line"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if user is signed in.", "response": "def check_login(func):\n    \"\"\"\u68c0\u67e5\u7528\u6237\u767b\u5f55\u72b6\u6001\n    :param func: \u9700\u8981\u88ab\u68c0\u67e5\u7684\u51fd\u6570\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        if type(ret) == requests.Response:\n            # \u68c0\u6d4b\u7ed3\u679c\u662f\u5426\u4e3aJSON\n            if ret.content[0]!=b'{' and ret.content[0]!=b'[':\n                return ret\n            try:\n                foo = json.loads(ret.content.decode('utf-8'))\n                if 'errno' in foo and foo['errno'] == -6:\n                    logging.debug(\n                            'Offline, deleting cookies file then relogin.')\n                    path = '.{0}.cookies'.format(args[0].username)\n                    if os.path.exists(path):\n                        os.remove(path)\n                    args[0]._initiate()\n            except:\n                raise LoginFailed('User unsigned in.')\n        return ret\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __err_handler(self, act, errno, callback=None, args=(), kwargs=None):\n        kwargs = kwargs or {}\n        errno = int(errno)\n\n        def err_handler_download():\n            if errno == 112:\n                # \u9875\u9762\u5931\u6548, \u91cd\u65b0\u5237\u65b0\u9875\u9762\n                url = 'http://pan.baidu.com/disk/home'\n                self.session.get(url)\n\n            return\n\n        def err_handler_upload():\n            # \u5b9e\u9645\u51fa\u95ee\u9898\u4e86\u518d\u5199\n            return\n\n        def err_handler_generic():\n            return\n\n        _act = {'download': err_handler_download,\n                'upload': err_handler_upload,\n                'generic': err_handler_generic\n                }\n\n        if act not in _act:\n            raise Exception('\u884c\u4e3a\u672a\u5b9a\u4e49, \u65e0\u6cd5\u5904\u7406\u8be5\u884c\u4e3a\u7684\u9519\u8bef')\n\n        if callback:\n            return callback(*args, **kwargs)\n        return None", "response": "Error handler for download and upload"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading a file to a directory.", "response": "def upload(self, dest_dir, file_handler, filename, callback=None, **kwargs):\n        \"\"\"\u4e0a\u4f20\u5355\u4e2a\u6587\u4ef6\uff08<2G\uff09.\n\n        | \u767e\u5ea6PCS\u670d\u52a1\u76ee\u524d\u652f\u6301\u6700\u59272G\u7684\u5355\u4e2a\u6587\u4ef6\u4e0a\u4f20\u3002\n        | \u5982\u9700\u652f\u6301\u8d85\u5927\u6587\u4ef6\uff08>2G\uff09\u7684\u65ad\u70b9\u7eed\u4f20\uff0c\u8bf7\u53c2\u8003\u4e0b\u9762\u7684\u201c\u5206\u7247\u6587\u4ef6\u4e0a\u4f20\u201d\u65b9\u6cd5\u3002\n\n        :param dest_dir: \u7f51\u76d8\u4e2d\u6587\u4ef6\u7684\u4fdd\u5b58\u8def\u5f84\uff08\u4e0d\u5305\u542b\u6587\u4ef6\u540d\uff09\u3002\n                            \u5fc5\u987b\u4ee5 / \u5f00\u5934\u3002\n\n                            .. warning::\n                                * \u6ce8\u610f\u672c\u63a5\u53e3\u7684 dest_dir \u53c2\u6570\u4e0d\u5305\u542b\u6587\u4ef6\u540d\uff0c\u53ea\u5305\u542b\u8def\u5f84\n                                * \u8def\u5f84\u957f\u5ea6\u9650\u5236\u4e3a1000\uff1b\n                                * \u5f84\u4e2d\u4e0d\u80fd\u5305\u542b\u4ee5\u4e0b\u5b57\u7b26\uff1a``\\\\\\\\ ? | \" > < : *``\uff1b\n                                * \u6587\u4ef6\u540d\u6216\u8def\u5f84\u540d\u5f00\u5934\u7ed3\u5c3e\u4e0d\u80fd\u662f ``.``\n                                  \u6216\u7a7a\u767d\u5b57\u7b26\uff0c\u7a7a\u767d\u5b57\u7b26\u5305\u62ec\uff1a\n                                  ``\\\\r, \\\\n, \\\\t, \u7a7a\u683c, \\\\0, \\\\x0B`` \u3002\n        :param file_handler: \u4e0a\u4f20\u6587\u4ef6\u5bf9\u8c61 \u3002(e.g. ``open('foobar', 'rb')`` )\n\n                            .. warning::\n                                \u6ce8\u610f\u4e0d\u8981\u4f7f\u7528 .read() \u65b9\u6cd5.\n        :type file_handler: file\n        :param callback: \u4e0a\u4f20\u8fdb\u5ea6\u56de\u8c03\u51fd\u6570\n            \u9700\u8981\u5305\u542b size \u548c progress \u540d\u5b57\u7684\u53c2\u6570\n\n        :param filename:\n\n        :return: requests.Response \u5bf9\u8c61\n\n            .. note::\n                \u8fd4\u56de\u6b63\u786e\u65f6\u8fd4\u56de\u7684 Reponse \u5bf9\u8c61 content \u4e2d\u7684\u6570\u636e\u7ed3\u6784\n\n                {\"path\":\"\u670d\u52a1\u5668\u6587\u4ef6\u8def\u5f84\",\"size\":\u6587\u4ef6\u5927\u5c0f,\"ctime\":\u521b\u5efa\u65f6\u95f4,\"mtime\":\u4fee\u6539\u65f6\u95f4,\"md5\":\"\u6587\u4ef6md5\u503c\",\"fs_id\":\u670d\u52a1\u5668\u6587\u4ef6\u8bc6\u522b\u53f7,\"isdir\":\u662f\u5426\u4e3a\u76ee\u5f55,\"request_id\":\u8bf7\u6c42\u8bc6\u522b\u53f7}\n\n        \"\"\"\n\n        params = {\n            'path':str(dest_dir)+\"/\"+str(filename)\n        }\n\n        tmp_filename = ''.join(random.sample(string.ascii_letters, 10))\n        files = {'file': (tmp_filename, file_handler)}\n\n        url = 'https://{0}/rest/2.0/pcs/file'.format(BAIDUPCS_SERVER)\n        return self._request('file', 'upload', url=url, extra_params=params,\n                             files=files, callback=callback, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload_tmpfile(self, file_handler, callback=None, **kwargs):\n\n        params = {\n            'type': 'tmpfile'\n        }\n        files = {'file': (str(int(time.time())), file_handler)}\n        url = 'https://{0}/rest/2.0/pcs/file'.format(BAIDUPCS_SERVER)\n        return self._request('file', 'upload', url=url, extra_params=params, callback=callback,\n                             files=files, **kwargs)", "response": "Upload a file to a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_url(self, remote_path, **kwargs):\n\n        def get_url(dlink):\n            return self.session.get(dlink,\n                                    headers=BAIDUPAN_HEADERS,\n                                    stream=True).url\n\n        if not hasattr(self, 'dsign'):\n            self.get_sign()\n\n        if isinstance(remote_path, str) or isinstance(remote_path, str):\n            remote_path = [remote_path]\n\n        file_list = []\n        jdata = json.loads(self.meta(remote_path).content)\n        if jdata['errno'] != 0:\n            jdata = self.__err_handler('generic', jdata['errno'],\n                                       self.meta,\n                                       args=(remote_path,)\n                                       )\n        logging.debug('[*]' + str(jdata))\n        for i, entry in enumerate(jdata['info']):\n            url = entry['dlink']\n            foo = get_url(url)\n            if 'wenxintishi' in foo:\n                file_list.append(self._yunguanjia_format(remote_path[i]))\n            else:\n                file_list.append(get_url(entry['dlink']))\n\n        return file_list", "response": "downloads a file from Yunguanjia"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a file from M3U8", "response": "def get_streaming(self, path, stype=\"M3U8_AUTO_480\", **kwargs):\n        \"\"\"\u83b7\u5f97\u89c6\u9891\u7684m3u8\u5217\u8868\n\n        :param path: \u89c6\u9891\u6587\u4ef6\u8def\u5f84\n        :param stype: \u8fd4\u56destream\u7c7b\u578b, \u5df2\u77e5\u6709``M3U8_AUTO_240``/``M3U8_AUTO_480``/``M3U8_AUTO_720``\n\n            .. warning::\n                M3U8_AUTO_240\u4f1a\u6709\u95ee\u9898, \u76ee\u524d480P\u662f\u6700\u7a33\u5b9a\u7684, \u4e5f\u662f\u767e\u5ea6\u7f51\u76d8\u9ed8\u8ba4\u7684\n        :return: str \u64ad\u653e(\u5217\u8868)\u9700\u8981\u7684\u4fe1\u606f\n        \"\"\"\n\n        params = {\n            'path': path,\n            'type': stype\n        }\n        url = 'https://{0}/rest/2.0/pcs/file'.format(BAIDUPCS_SERVER)\n        while True:\n            ret = self._request('file', 'streaming', url=url, extra_params=params, **kwargs)\n            if not ret.ok:\n                logging.debug('get_streaming ret_status_code %s' % ret.status_code)\n                jdata = json.loads(ret.content)\n                if jdata['error_code'] == 31345:\n                    # \u518d\u8bd5\u4e00\u6b21\n                    continue\n                elif jdata['error_code'] == 31066:\n                    # \u6587\u4ef6\u4e0d\u5b58\u5728\n                    return 31066\n                elif jdata['error_code'] == 31304:\n                    # \u6587\u4ef6\u7c7b\u578b\u4e0d\u652f\u6301\n                    return 31304\n                elif jdata['error_code'] == 31023:\n                    # params error\n                    return 31023\n            return ret.content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_files(self, remote_path, by=\"name\", order=\"desc\",\n                   limit=None, extra_params=None, is_share=False, **kwargs):\n        \"\"\"\u83b7\u53d6\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\u5217\u8868.\n\n        :param remote_path: \u7f51\u76d8\u4e2d\u76ee\u5f55\u7684\u8def\u5f84\uff0c\u5fc5\u987b\u4ee5 / \u5f00\u5934\u3002\n\n                            .. warning::\n                                * \u8def\u5f84\u957f\u5ea6\u9650\u5236\u4e3a1000\uff1b\n                                * \u5f84\u4e2d\u4e0d\u80fd\u5305\u542b\u4ee5\u4e0b\u5b57\u7b26\uff1a``\\\\\\\\ ? | \" > < : *``\uff1b\n                                * \u6587\u4ef6\u540d\u6216\u8def\u5f84\u540d\u5f00\u5934\u7ed3\u5c3e\u4e0d\u80fd\u662f ``.``\n                                  \u6216\u7a7a\u767d\u5b57\u7b26\uff0c\u7a7a\u767d\u5b57\u7b26\u5305\u62ec\uff1a\n                                  ``\\\\r, \\\\n, \\\\t, \u7a7a\u683c, \\\\0, \\\\x0B`` \u3002\n        :param by: \u6392\u5e8f\u5b57\u6bb5\uff0c\u7f3a\u7701\u6839\u636e\u6587\u4ef6\u7c7b\u578b\u6392\u5e8f\uff1a\n\n                   * time\uff08\u4fee\u6539\u65f6\u95f4\uff09\n                   * name\uff08\u6587\u4ef6\u540d\uff09\n                   * size\uff08\u5927\u5c0f\uff0c\u6ce8\u610f\u76ee\u5f55\u65e0\u5927\u5c0f\uff09\n        :param order: \u201casc\u201d\u6216\u201cdesc\u201d\uff0c\u7f3a\u7701\u91c7\u7528\u964d\u5e8f\u6392\u5e8f\u3002\n\n                      * asc\uff08\u5347\u5e8f\uff09\n                      * desc\uff08\u964d\u5e8f\uff09\n        :param limit: \u8fd4\u56de\u6761\u76ee\u63a7\u5236\uff0c\u53c2\u6570\u683c\u5f0f\u4e3a\uff1an1-n2\u3002\n\n                      \u8fd4\u56de\u7ed3\u679c\u96c6\u7684[n1, n2)\u4e4b\u95f4\u7684\u6761\u76ee\uff0c\u7f3a\u7701\u8fd4\u56de\u6240\u6709\u6761\u76ee\uff1b\n                      n1\u4ece0\u5f00\u59cb\u3002\n\n        :param is_share: \u662f\u5426\u662f\u5206\u4eab\u7684\u6587\u4ef6\u5939(\u5927\u6982)\n\n        :return: requests.Response \u5bf9\u8c61\n\n            .. note::\n                \u8fd4\u56de\u6b63\u786e\u65f6\u8fd4\u56de\u7684 Reponse \u5bf9\u8c61 content \u4e2d\u7684\u6570\u636e\u7ed3\u6784\n\n                {\n                    \"errno\":0,\n                    \"list\":[\n                        {\"fs_id\":\u670d\u52a1\u5668\u6587\u4ef6\u8bc6\u522b\u53f7\"path\":\"\u8def\u5f84\",\"server_filename\":\"\u670d\u52a1\u5668\u6587\u4ef6\u540d\uff08\u4e0d\u6c57\u542b\u8def\u5f84\uff09\",\"size\":\u6587\u4ef6\u5927\u5c0f,\"server_mtime\":\u670d\u52a1\u5668\u4fee\u6539\u65f6\u95f4,\"server_ctime\":\u670d\u52a1\u5668\u521b\u5efa\u65f6\u95f4,\"local_mtime\":\u672c\u5730\u4fee\u6539\u65f6\u95f4,\"local_ctime\":\u672c\u5730\u521b\u5efa\u65f6\u95f4,\"isdir\":\u662f\u5426\u662f\u76ee\u5f55,\"category\":\u7c7b\u578b,\"md5\":\"md5\u503c\"}\u2026\u2026\u7b49\u7b49\n                           ],\n                    \"request_id\":\u8bf7\u6c42\u8bc6\u522b\u53f7\n                }\n\n        \"\"\"\n        if order == \"desc\":\n            desc = \"1\"\n        else:\n            desc = \"0\"\n        params = dict()\n        if extra_params:\n            params.update(extra_params)\n        params['dir'] = remote_path\n        params['order'] = by\n        params['desc'] = desc\n        if is_share:\n            return self._request('/share/list', None, extra_params=params, url=\"https://pan.baidu.com/share/list\",\n                                 **kwargs)\n        return self._request('list', 'list', extra_params=params, **kwargs)", "response": "List all files in a file system."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves a file or folder to another location.", "response": "def move(self, path_list, dest, **kwargs):\n        \"\"\"\n        \u79fb\u52a8\u6587\u4ef6\u6216\u6587\u4ef6\u5939\n\n        :param path_list: \u5728\u767e\u5ea6\u76d8\u4e0a\u8981\u79fb\u52a8\u7684\u6e90\u6587\u4ef6path\n        :type path_list: list\n\n        :param dest: \u8981\u79fb\u52a8\u5230\u7684\u76ee\u5f55\n        :type dest: str\n\n        \"\"\"\n\n        def __path(path):\n            if path.endswith('/'):\n                return path.split('/')[-2]\n            else:\n                return os.path.basename(path)\n\n        params = {\n            'opera': 'move'\n        }\n        data = {\n            'filelist': json.dumps([{\n                                        \"path\": path,\n                                        \"dest\": dest,\n                                        \"newname\": __path(path)} for path in path_list]),\n        }\n        url = 'http://{0}/api/filemanager'.format(BAIDUPAN_SERVER)\n        return self._request('filemanager', 'move', url=url, data=data, extra_params=params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename(self, rename_pair_list, **kwargs):\n        foo = []\n        for path, newname in rename_pair_list:\n            foo.append({'path': path,\n                        'newname': newname\n                        })\n\n        data = {'filelist': json.dumps(foo)}\n        params = {\n            'opera': 'rename'\n        }\n\n        url = 'http://{0}/api/filemanager'.format(BAIDUPAN_SERVER)\n        logging.debug('rename ' + str(data) + 'URL:' + url)\n        return self._request('filemanager', 'rename', url=url, data=data, extra_params=params, **kwargs)", "response": "rename a set of files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a list of files", "response": "def delete(self, path_list, **kwargs):\n        \"\"\"\n        \u5220\u9664\u6587\u4ef6\u6216\u6587\u4ef6\u5939\n\n        :param path_list: \u5f85\u5220\u9664\u7684\u6587\u4ef6\u6216\u6587\u4ef6\u5939\u5217\u8868,\u6bcf\u4e00\u9879\u4e3a\u670d\u52a1\u5668\u8def\u5f84\n        :type path_list: list\n\n\n        \"\"\"\n        data = {\n            'filelist': json.dumps([path for path in path_list])\n        }\n        url = 'http://{0}/api/filemanager?opera=delete'.format(BAIDUPAN_SERVER)\n        return self._request('filemanager', 'delete', url=url, data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef share(self, file_ids, pwd=None, **kwargs):\n        if pwd:\n            data = {\n                'fid_list': json.dumps([int(fid) for fid in file_ids]),\n                'pwd': pwd,\n                'schannel': 4,\n                'channel_list': json.dumps([])\n            }\n        else:\n            data = {\n                'fid_list': json.dumps([int(fid) for fid in file_ids]),\n                'schannel': 0,\n                'channel_list': json.dumps([])\n            }\n        url = 'http://pan.baidu.com/share/set'\n        return self._request('share/set', '', url=url, data=data, **kwargs)", "response": "This method is used to set the file_ids of a user in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a download task to the current instance of the class.", "response": "def add_download_task(self, source_url, remote_path, selected_idx=(), **kwargs):\n        \"\"\"\n        :param source_url: \u79bb\u7ebf\u4e0b\u8f7d\u76ee\u6807\u7684URL\n        :param remote_path: \u6b32\u4fdd\u5b58\u5230\u767e\u5ea6\u7f51\u76d8\u7684\u76ee\u5f55, \u6ce8\u610f\u4ee5 / \u6253\u5934\n        :param selected_idx: \u5728 BT \u6216\u8005\u78c1\u529b\u94fe\u7684\u4e0b\u8f7d\u7c7b\u578b\u4e2d, \u9009\u62e9\u54ea\u4e9bidx\u4e0b\u8f7d, \u4e0d\u586b\u5199\u4e3a\u5168\u90e8\n        \u6dfb\u52a0\u79bb\u7ebf\u4efb\u52a1\uff0c\u652f\u6301\u6240\u6709\u767e\u5ea6\u7f51\u76d8\u652f\u6301\u7684\u7c7b\u578b\n        \"\"\"\n        if source_url.startswith('magnet:?'):\n            print('Magnet: \"%s\"' % source_url)\n            return self.add_magnet_task(source_url, remote_path, selected_idx, **kwargs)\n        elif source_url.endswith('.torrent'):\n            print('BitTorrent: \"%s\"' % source_url)\n            return self.add_torrent_task(source_url, remote_path, selected_idx, **kwargs)\n        else:\n            print('Others: \"%s\"' % source_url)\n            data = {\n                'method': 'add_task',\n                'source_url': source_url,\n                'save_path': remote_path,\n            }\n            url = 'http://{0}/rest/2.0/services/cloud_dl'.format(BAIDUPAN_SERVER)\n            return self._request('services/cloud_dl', 'add_task', url=url,\n                                 data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_torrent_task(self, torrent_path, save_path='/', selected_idx=(), **kwargs):\n\n        # \u4e0a\u4f20\u79cd\u5b50\u6587\u4ef6\n        torrent_handler = open(torrent_path, 'rb')\n        basename = os.path.basename(torrent_path)\n\n        # \u6e05\u7406\u540c\u540d\u6587\u4ef6\n        self.delete(['/' + basename])\n\n        response = self.upload('/', torrent_handler, basename).json()\n        remote_path = response['path']\n        logging.debug('REMOTE PATH:' + remote_path)\n\n        # \u83b7\u53d6\u79cd\u5b50\u4fe1\u606f\n        response = self._get_torrent_info(remote_path).json()\n        if response.get('error_code'):\n            print(response.get('error_code'))\n            return\n        if not response['torrent_info']['file_info']:\n            return\n\n        # \u8981\u4e0b\u8f7d\u7684\u6587\u4ef6\u5e8f\u53f7\uff1a\u96c6\u5408\u4e3a\u7a7a\u4e0b\u8f7d\u6240\u6709\uff0c\u975e\u7a7a\u96c6\u5408\u6307\u5b9a\u5e8f\u53f7\u96c6\u5408\uff0c\u7a7a\u4e32\u4e0b\u8f7d\u9ed8\u8ba4\n        if isinstance(selected_idx, (tuple, list, set)):\n            if len(selected_idx) > 0:\n                selected_idx = ','.join(map(str, selected_idx))\n            else:\n                selected_idx = ','.join(map(str, range(1, len(response['torrent_info']['file_info']) + 1)))\n        else:\n            selected_idx = ''\n\n        # \u5f00\u59cb\u4e0b\u8f7d\n        data = {\n            'file_sha1': response['torrent_info']['sha1'],\n            'save_path': save_path,\n            'selected_idx': selected_idx,\n            'task_from': '1',\n            'source_path': remote_path,\n            'type': '2'  # 2 is torrent file\n        }\n        url = 'http://{0}/rest/2.0/services/cloud_dl'.format(BAIDUPAN_SERVER)\n        return self._request('create', 'add_task', url=url, data=data, **kwargs)", "response": "add_torrent_task - Add a new torrent to the cache"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_download_tasks(self, task_ids, operate_type=1, **kwargs):\n\n        params = {\n            'task_ids': ','.join(map(str, task_ids)),\n            'op_type': operate_type,\n        }\n        url = 'http://{0}/rest/2.0/services/cloud_dl'.format(BAIDUPAN_SERVER)\n        return self._request('services/cloud_dl', 'query_task', url=url,\n                             extra_params=params, **kwargs)", "response": "\u6839\u636e\u4efb\u52a1ID\u53f7\uff0c\u67e5\u8be2\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1\u4fe1\u606f\u53ca\u8fdb\u5ea6\u4fe1\u606f\u3002\n\n        :param task_ids: \u8981\u67e5\u8be2\u7684\u4efb\u52a1 ID\u5b57\u7b26\u4e32 \u5217\u8868\n        :type task_ids: list or tuple\n        :param operate_type:\n                            * 0\uff1a\u67e5\u4efb\u52a1\u4fe1\u606f\n                            * 1\uff1a\u67e5\u8fdb\u5ea6\u4fe1\u606f\uff0c\u9ed8\u8ba4\u4e3a1\n\n        :return: requests.Response\n\n            .. note::\n                \u8fd4\u56de\u6b63\u786e\u65f6\u8fd4\u56de\u7684 Reponse \u5bf9\u8c61 content \u4e2d\u7684\u6570\u636e\u7ed3\u6784\n\n                \u7ed9\u51fa\u4e00\u4e2a\u8303\u4f8b\n\n                {\n                    \"task_info\":\n                        {\"70970481\":{\n                                \"status\":\"0\",\n\n                                \"file_size\":\"122328178\",\n\n                                \"finished_size\":\"122328178\",\n\n                                \"create_time\":\"1391620757\",\n\n                                \"start_time\":\"1391620757\",\n\n                                \"finish_time\":\"1391620757\",\n\n                                \"save_path\":\"\\/\",\n\n                                \"source_url\":\"\\/saki-nation04gbcn.torrent\",\n\n                                \"task_name\":\"[KTXP][Saki-National][04][GB_CN][720p]\",\n\n                                \"od_type\":\"2\",\n\n                                \"file_list\":[\n                                    {\n                                        \"file_name\":\"[KTXP][Saki-National][04][GB_CN][720p].mp4\",\n\n                                        \"file_size\":\"122328178\"\n                                    }\n                                ],\n\n                                \"result\":0\n\n                                }\n                        },\n\n                        \"request_id\":861570268\n\n                }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_tasks_number(self):\n        ret = self.list_download_tasks().content\n        foo = json.loads(ret)\n        return foo['total']", "response": "get total number of download tasks"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all the tasks in a specific resource.", "response": "def list_download_tasks(self, need_task_info=\"1\", asc=\"0\", start=0, create_time=None, limit=1000, status=\"255\",\n                            source_url=None, remote_path=None, **kwargs):\n        \"\"\"\u67e5\u8be2\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1ID\u5217\u8868\u53ca\u4efb\u52a1\u4fe1\u606f.\n\n        :param need_task_info: \u662f\u5426\u9700\u8981\u8fd4\u56de\u4efb\u52a1\u4fe1\u606f:\n                               * 0\uff1a\u4e0d\u9700\u8981\n                               * 1\uff1a\u9700\u8981\uff0c\u9ed8\u8ba4\u4e3a1\n        :param start: \u67e5\u8be2\u4efb\u52a1\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4\u4e3a0\u3002\n        :param limit: \u8bbe\u5b9a\u8fd4\u56de\u4efb\u52a1\u6570\u91cf\uff0c\u9ed8\u8ba4\u4e3a10\u3002\n        :param asc:\n                   * 0\uff1a\u964d\u5e8f\uff0c\u9ed8\u8ba4\u503c\n                   * 1\uff1a\u5347\u5e8f\n        :param create_time: \u4efb\u52a1\u521b\u5efa\u65f6\u95f4\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u3002\n        :type create_time: int\n        :param status: \u4efb\u52a1\u72b6\u6001\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u3002\n\n            .. note::\n                \u4efb\u52a1\u72b6\u6001\u6709\n                       0:\u4e0b\u8f7d\u6210\u529f\n\n                       1:\u4e0b\u8f7d\u8fdb\u884c\u4e2d\n\n                       2:\u7cfb\u7edf\u9519\u8bef\n\n                       3:\u8d44\u6e90\u4e0d\u5b58\u5728\n\n                       4:\u4e0b\u8f7d\u8d85\u65f6\n\n                       5:\u8d44\u6e90\u5b58\u5728\u4f46\u4e0b\u8f7d\u5931\u8d25\n\n                       6:\u5b58\u50a8\u7a7a\u95f4\u4e0d\u8db3\n\n                       7:\u76ee\u6807\u5730\u5740\u6570\u636e\u5df2\u5b58\u5728, 8:\u4efb\u52a1\u53d6\u6d88.\n        :type status: int\n        :param source_url: \u6e90\u5730\u5740URL\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u3002\n        :param remote_path: \u6587\u4ef6\u4fdd\u5b58\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u3002\n\n                            .. warning::\n                                * \u8def\u5f84\u957f\u5ea6\u9650\u5236\u4e3a1000\uff1b\n                                * \u5f84\u4e2d\u4e0d\u80fd\u5305\u542b\u4ee5\u4e0b\u5b57\u7b26\uff1a``\\\\\\\\ ? | \" > < : *``\uff1b\n                                * \u6587\u4ef6\u540d\u6216\u8def\u5f84\u540d\u5f00\u5934\u7ed3\u5c3e\u4e0d\u80fd\u662f ``.``\n                                  \u6216\u7a7a\u767d\u5b57\u7b26\uff0c\u7a7a\u767d\u5b57\u7b26\u5305\u62ec\uff1a\n                                  ``\\\\r, \\\\n, \\\\t, \u7a7a\u683c, \\\\0, \\\\x0B`` \u3002\n        :return: Response \u5bf9\u8c61\n\n             .. note::\n                \u8fd4\u56de\u6b63\u786e\u65f6\u8fd4\u56de\u7684 Reponse \u5bf9\u8c61 content \u4e2d\u7684\u6570\u636e\u7ed3\u6784\n\n                    {\n                        \"task_info\": [\n\n                            {\n\n                                \"task_id\": \"\u4efb\u52a1\u8bc6\u522b\u53f7\",\n\n                                \"od_type\": \"2\",\n\n                                \"source_url\": \"\u539f\u5730\u5740\uff0cbt\u4efb\u52a1\u4e3a\u79cd\u5b50\u5728\u670d\u52a1\u5668\u4e0a\u7684\u8def\u5f84\uff0c\u5426\u5219\u4e3a\u539f\u59cbURL\",\n\n                                \"save_path\": \"\u4fdd\u5b58\u8def\u5f84\",\n\n                                \"rate_limit\": \"\u901f\u5ea6\u9650\u5236\uff0c0\u4e3a\u4e0d\u9650\",\n\n                                \"timeout\": \"0\",\n\n                                \"callback\": \"\",\n\n                                \"status\": \"\u4efb\u52a1\u72b6\u6001\",\n\n                                \"create_time\": \"\u521b\u5efa\u65f6\u95f4\",\n\n                                \"task_name\": \"\u4efb\u52a1\u540d\"\n\n                            },\u2026\u2026\u7b49\u7b49\n\n                        ],\n\n                        \"total\": \u603b\u6570,\n\n                        \"request_id\": \u8bf7\u6c42\u8bc6\u522b\u53f7\n\n                    }\n        \"\"\"\n\n        params = {\n            'start': start,\n            'limit': limit,\n            'status': status,\n            'need_task_info': need_task_info,\n            'asc': asc,\n            'source_url': source_url,\n            'remote_path': remote_path,\n            'create_time': create_time\n\n        }\n        url = 'http://{0}/rest/2.0/services/cloud_dl'.format(BAIDUPAN_SERVER)\n        return self._request('services/cloud_dl', 'list_task', url=url, extra_params=params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancel_download_task(self, task_id, expires=None, **kwargs):\n\n        data = {\n            'expires': expires,\n            'task_id': task_id,\n        }\n        url = 'http://{0}/rest/2.0/services/cloud_dl'.format(BAIDUPAN_SERVER)\n        return self._request('services/cloud_dl', 'cancel_task', url=url,\n                             data=data, **kwargs)", "response": "\u53d6\u6d88\u79bb\u7ebf\u4e0b\u8f7d\u4efb\u52a1.\n\n        :param task_id: \u8981\u53d6\u6d88\u7684\u4efb\u52a1ID\u53f7\u3002\n        :type task_id: str\n        :param expires: \u8bf7\u6c42\u5931\u6548\u65f6\u95f4\uff0c\u5982\u679c\u6709\uff0c\u5219\u4f1a\u6821\u9a8c\u3002\n        :type expires: int\n        :return: requests.Response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_download_task(self, task_id, **kwargs):\n\n        data = {\n            'task_id': task_id,\n        }\n        url = 'http://{0}/rest/2.0/services/cloud_dl'.format(BAIDUPAN_SERVER)\n        return self._request('services/cloud_dl', 'delete_task', url=url,\n                             data=data, **kwargs)", "response": "Delete a task from the Cloud DL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_recycle_bin(self, order=\"time\", desc=\"1\", start=0, limit=1000, page=1, **kwargs):\n        # Done\n        \"\"\"\u83b7\u53d6\u56de\u6536\u7ad9\u4e2d\u7684\u6587\u4ef6\u53ca\u76ee\u5f55\u5217\u8868.\n        :param order: \u8fd4\u56de\u6761\u76ee\u6392\u5e8f\u5217\n        :param desc: \u8fd4\u56de\u6761\u76ee\u6392\u5e8f\u964d\u5e8f\u63a7\u5236\n        :param page: \u8fd4\u56de\u6761\u76ee\u7684\u5206\u9875\u63a7\u5236, \u5f53\u524d\u9875\u7801\n        :param start: \u8fd4\u56de\u6761\u76ee\u7684\u8d77\u59cb\u503c\uff0c\u7f3a\u7701\u503c\u4e3a0\n        :param limit: \u8fd4\u56de\u6761\u76ee\u7684\u957f\u5ea6\uff0c\u7f3a\u7701\u503c\u4e3a1000\n        :return: requests.Response\n\n            \u683c\u5f0f\u540c list_files\n        \"\"\"\n\n        params = {\n            'start': start,\n            'num': limit,\n            'dir': '/',\n            'order': order,\n            'desc': desc,\n            'page': page\n        }\n        url = 'http://{0}/api/recycle/list'.format(BAIDUPAN_SERVER)\n        return self._request('recycle', 'list', url=url, extra_params=params, **kwargs)", "response": "\u83b7\u53d6\u56de\u6536\u7ad9\u4e2d\u7684\u6587\u4ef6\u53ca\u76ee\u5f55\u5217\u8868.\n        :param order: \u8fd4\u56de\u6761\u76ee\u6392\u5e8f\u5217\n        :param desc: \u8fd4\u56de\u6761\u76ee\u6392\u5e8f\u964d\u5e8f\u63a7\u5236\n        :param page: \u8fd4\u56de\u6761\u76ee\u7684\u5206\u9875\u63a7\u5236, \u5f53\u524d\u9875\u7801\n        :param start: \u8fd4\u56de\u6761\u76ee\u7684\u8d77\u59cb\u503c\uff0c\u7f3a\u7701\u503c\u4e3a0\n        :param limit: \u8fd4\u56de\u6761\u76ee\u7684\u957f\u5ea6\uff0c\u7f3a\u7701\u503c\u4e3a1000\n        :return: requests.Response\n\n            \u683c\u5f0f\u540c list_files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclear the current set of keys from the Baidu server.", "response": "def clean_recycle_bin(self, **kwargs):\n        \"\"\"\u6e05\u7a7a\u56de\u6536\u7ad9.\n\n        :return: requests.Response\n        \"\"\"\n\n        url = 'http://{0}/api/recycle/clear'.format(BAIDUPAN_SERVER)\n        return self._request('recycle', 'clear', url=url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rapidupload(self, file_handler, path, **kwargs):\n        file_handler.seek(0, 2)\n        _BLOCK_SIZE = 2 ** 20\n        content_length = file_handler.tell()\n        file_handler.seek(0)\n\n        # \u6821\u9a8c\u6bb5\u4e3a\u524d 256kb\n        first_256bytes = file_handler.read(256 * 1024)\n        slice_md5 = md5(first_256bytes).hexdigest()\n\n        content_crc32 = crc32(first_256bytes).conjugate()\n        content_md5 = md5(first_256bytes)\n\n        while True:\n            block = file_handler.read(_BLOCK_SIZE)\n            if not block:\n                break\n            # \u66f4\u65b0crc32\u548cmd5\u6821\u9a8c\u503c\n            content_crc32 = crc32(block, content_crc32).conjugate()\n            content_md5.update(block)\n\n        data = {'path': path,\n                'content-length': content_length,\n                'content-md5': content_md5.hexdigest(),\n                'slice-md5': slice_md5,\n                'content-crc32': '%d' % (content_crc32.conjugate() & 0xFFFFFFFF)}\n        logging.debug('RAPIDUPLOAD DATA ' + str(data))\n        # url = 'http://pan.baidu.com/api/rapidupload'\n        return self._request('rapidupload', 'rapidupload', data=data, **kwargs)", "response": "upload a file to rapid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for a set of files in a directory.", "response": "def search(self, path, keyword, page=1, recursion=1, limit=1000, **kwargs):\n        \"\"\"\u641c\u7d22\u6587\u4ef6\n\n        :param path: \u641c\u7d22\u76ee\u5f55\n        :param keyword: \u5173\u952e\u8bcd\n        :param page: \u8fd4\u56de\u7b2c\u51e0\u9875\u7684\u6570\u636e\n        :param recursion: \u662f\u5426\u9012\u5f52\u641c\u7d22\uff0c\u9ed8\u8ba4\u4e3a1 \uff08\u4f3c\u4e4e0\u548c1\u90fd\u6ca1\u5f71\u54cd\uff0c\u90fd\u662f\u9012\u5f52\u641c\u7d22\u7684\uff09\n        :param limit: \u6bcf\u9875\u6761\u76ee\n\n        :return: requests.Repsonse\n        \u8fd4\u56de\u7ed3\u679c\u548clist_files\u4e00\u6837\u7ed3\u6784\n        \"\"\"\n        params = {'dir': path,\n                  'recursion': recursion,\n                  'key': keyword,\n                  'page': page,\n                  'num': limit}\n\n        # url = 'http://pan.baidu.com/api/search'\n\n        return self._request('search', 'search', extra_params=params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a thumbnail of a file.", "response": "def thumbnail(self, path, height, width, quality=100, **kwargs):\n        \"\"\"\u83b7\u53d6\u6587\u4ef6\u7f29\u7565\u56fe\n\n        :param path: \u8fdc\u7a0b\u6587\u4ef6\u8def\u5f84\n        :param height: \u7f29\u7565\u56fe\u9ad8\n        :param width: \u7f29\u7565\u56fe\u5bbd\n        :param quality: \u7f29\u7565\u56fe\u8d28\u91cf\uff0c\u9ed8\u8ba4100\n\n        :return: requests.Response\n\n            .. note::\n                \u5982\u679c\u8fd4\u56de HTTP 404 \u8bf4\u660e\u8be5\u6587\u4ef6\u4e0d\u5b58\u5728\u7f29\u7565\u56fe\u5f62\u5f0f\n        \"\"\"\n        params = {'ec': 1,\n                  'path': path,\n                  'quality': quality,\n                  'width': width,\n                  'height': height}\n\n        url = 'http://{0}/rest/2.0/pcs/thumbnail'.format(BAIDUPCS_SERVER)\n        return self._request('thumbnail', 'generate', url=url, extra_params=params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef meta(self, file_list, **kwargs):\n        if not isinstance(file_list, list):\n            file_list = [file_list]\n        data = {'target': json.dumps(file_list)}\n\n        return self._request('filemetas?blocks=0&dlink=1', 'filemetas', data=data, **kwargs)", "response": "This method returns a list of all the metainfo that are available on the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_remote_download_task(self, link):\n\n        ruokuai_captcha_handler = functools.partial(\n            self.ruokuai_captcha_handler,\n            self.captcha_params\n        )\n\n        # \u521d\u59cb\u5316pcs\uff0c\u8fd9\u91cc\u8bbe\u7f6e\u4e86\u9a8c\u8bc1\u7801\u5904\u7406\u51fd\u6570\u548c\u9a8c\u8bc1\u7801\u5904\u7406\u53c2\u6570\n        pcs = PCS(\n            self.baidu_username,\n            self.baidu_password,\n            ruokuai_captcha_handler,\n            None,\n        )\n\n        # \u83b7\u53d6\u4e0b\u8f7d\u8def\u5f84\u4e2d\u7684\u6587\u4ef6\uff0c\u9632\u6b62\u6587\u4ef6\u91cd\u590d\u6dfb\u52a0\n        rsp = pcs.list_files(BASE_PATH)\n        result = rsp.json()\n        exist_list = []\n        if result['errno'] == 0:\n            exist_list = result['list']\n        else:\n            print(json.dumps(result))\n\n        exist_names = [exist['server_filename'] for exist in exist_list]\n\n        if link not in exist_names:\n            # \u7f51\u76d8\u4e2d\u4e0d\u5b58\u5728\u7684\u624d\u6dfb\u52a0\n            pcs.add_download_task(\n                link,\n                BASE_PATH\n            )\n        else:\n            print(link + ' \u5df2\u7ecf\u5b58\u5728\u4e8e\u7f51\u76d8\u4e2d')", "response": "add remote download task"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements the get - parameters - by - path - API in the backend.", "response": "def get_parameters_by_path(self, path, with_decryption, recursive, filters=None):\n        \"\"\"Implement the get-parameters-by-path-API in the backend.\"\"\"\n        result = []\n        # path could be with or without a trailing /. we handle this\n        # difference here.\n        path = path.rstrip('/') + '/'\n        for param in self._parameters:\n            if path != '/' and not param.startswith(path):\n                continue\n            if '/' in param[len(path) + 1:] and not recursive:\n                continue\n            if not self._match_filters(self._parameters[param], filters):\n                continue\n            result.append(self._parameters[param])\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the given parameter matches all the filters", "response": "def _match_filters(parameter, filters=None):\n        \"\"\"Return True if the given parameter matches all the filters\"\"\"\n        for filter_obj in (filters or []):\n            key = filter_obj['Key']\n            option = filter_obj.get('Option', 'Equals')\n            values = filter_obj.get('Values', [])\n\n            what = None\n            if key == 'Type':\n                what = parameter.type\n            elif key == 'KeyId':\n                what = parameter.keyid\n\n            if option == 'Equals'\\\n                    and not any(what == value for value in values):\n                return False\n            elif option == 'BeginsWith'\\\n                    and not any(what.startswith(value) for value in values):\n                return False\n        # True if no false match (or no filters at all)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_commands(self, **kwargs):\n        commands = self._commands\n\n        command_id = kwargs.get('CommandId', None)\n        if command_id:\n            commands = [self.get_command_by_id(command_id)]\n        instance_id = kwargs.get('InstanceId', None)\n        if instance_id:\n            commands = self.get_commands_by_instance_id(instance_id)\n\n        return {\n            'Commands': [command.response_object() for command in commands]\n        }", "response": "List all available commands."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_command_invocation(self, **kwargs):\n\n        command_id = kwargs.get('CommandId')\n        instance_id = kwargs.get('InstanceId')\n        plugin_name = kwargs.get('PluginName', None)\n\n        command = self.get_command_by_id(command_id)\n        return command.get_invocation(instance_id, plugin_name)", "response": "Get the command invocation object for the specified command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize_service(service, operation, api_protocol):\n    lib_dir = get_lib_dir(service)\n    test_dir = get_test_dir(service)\n\n    print_progress('Initializing service', service, 'green')\n\n    client = boto3.client(service)\n    service_class = client.__class__.__name__\n    endpoint_prefix = client._service_model.endpoint_prefix\n\n    tmpl_context = {\n        'service': service,\n        'service_class': service_class,\n        'endpoint_prefix': endpoint_prefix,\n        'api_protocol': api_protocol,\n        'escaped_service': get_escaped_service(service)\n    }\n\n    # initialize service directory\n    if os.path.exists(lib_dir):\n        print_progress('skip creating', lib_dir, 'yellow')\n    else:\n        print_progress('creating', lib_dir, 'green')\n        os.makedirs(lib_dir)\n\n    tmpl_dir = os.path.join(TEMPLATE_DIR, 'lib')\n    for tmpl_filename in os.listdir(tmpl_dir):\n        render_template(\n            tmpl_dir, tmpl_filename, tmpl_context, service\n        )\n\n    # initialize test directory\n    if os.path.exists(test_dir):\n        print_progress('skip creating', test_dir, 'yellow')\n    else:\n        print_progress('creating', test_dir, 'green')\n        os.makedirs(test_dir)\n    tmpl_dir = os.path.join(TEMPLATE_DIR, 'test')\n    for tmpl_filename in os.listdir(tmpl_dir):\n        alt_filename = 'test_{}.py'.format(get_escaped_service(service)) if tmpl_filename == 'test_service.py.j2' else None\n        render_template(\n            tmpl_dir, tmpl_filename, tmpl_context, service, alt_filename\n        )\n\n    # append mock to init files\n    append_mock_to_init_py(service)\n    append_mock_import_to_backends_py(service)\n    append_mock_dict_to_backends_py(service)", "response": "create lib and test dirs if not exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_function_in_responses(service, operation, protocol):\n    client = boto3.client(service)\n\n    aws_operation_name = to_upper_camel_case(operation)\n    op_model = client._service_model.operation_model(aws_operation_name)\n    if not hasattr(op_model.output_shape, 'members'):\n        outputs = {}\n    else:\n        outputs = op_model.output_shape.members\n    inputs = op_model.input_shape.members\n    input_names = [to_snake_case(_) for _ in inputs.keys() if _ not in INPUT_IGNORED_IN_BACKEND]\n    output_names = [to_snake_case(_) for _ in outputs.keys() if _ not in OUTPUT_IGNORED_IN_BACKEND]\n    body = '\\ndef {}(self):\\n'.format(operation)\n\n    for input_name, input_type in inputs.items():\n        type_name = input_type.type_name\n        if type_name == 'integer':\n            arg_line_tmpl = '    {} = self._get_int_param(\"{}\")\\n'\n        elif type_name == 'list':\n            arg_line_tmpl = '    {} = self._get_list_prefix(\"{}.member\")\\n'\n        else:\n            arg_line_tmpl = '    {} = self._get_param(\"{}\")\\n'\n        body += arg_line_tmpl.format(to_snake_case(input_name), input_name)\n    if output_names:\n        body += '    {} = self.{}_backend.{}(\\n'.format(', '.join(output_names), get_escaped_service(service), operation)\n    else:\n        body += '    self.{}_backend.{}(\\n'.format(get_escaped_service(service), operation)\n    for input_name in input_names:\n        body += '        {}={},\\n'.format(input_name, input_name)\n\n    body += '    )\\n'\n    if protocol == 'query':\n        body += '    template = self.response_template({}_TEMPLATE)\\n'.format(operation.upper())\n        body += '    return template.render({})\\n'.format(\n            ', '.join(['{}={}'.format(_, _) for _ in output_names])\n        )\n    elif protocol in ['json', 'rest-json']:\n        body += '    # TODO: adjust response\\n'\n        body += '    return json.dumps(dict({}))\\n'.format(', '.join(['{}={}'.format(to_lower_camel_case(_), _) for _ in output_names]))\n    return body", "response": "returns function in response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_function_in_models(service, operation):\n    client = boto3.client(service)\n    aws_operation_name = to_upper_camel_case(operation)\n    op_model = client._service_model.operation_model(aws_operation_name)\n    inputs = op_model.input_shape.members\n    if not hasattr(op_model.output_shape, 'members'):\n        outputs = {}\n    else:\n        outputs = op_model.output_shape.members\n    input_names = [to_snake_case(_) for _ in inputs.keys() if _ not in INPUT_IGNORED_IN_BACKEND]\n    output_names = [to_snake_case(_) for _ in outputs.keys() if _ not in OUTPUT_IGNORED_IN_BACKEND]\n    if input_names:\n        body = 'def {}(self, {}):\\n'.format(operation, ', '.join(input_names))\n    else:\n        body = 'def {}(self)\\n'\n    body += '    # implement here\\n'\n    body += '    return {}\\n\\n'.format(', '.join(output_names))\n\n    return body", "response": "returns function in models"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning xml tree that contains all the response for the given operation", "response": "def get_response_query_template(service, operation):\n    \"\"\"refers to definition of API in botocore, and autogenerates template\n    Assume that response format is xml when protocol is query\n\n    You can see example of elbv2 from link below.\n      https://github.com/boto/botocore/blob/develop/botocore/data/elbv2/2015-12-01/service-2.json\n    \"\"\"\n    client = boto3.client(service)\n    aws_operation_name = to_upper_camel_case(operation)\n    op_model = client._service_model.operation_model(aws_operation_name)\n    result_wrapper = op_model.output_shape.serialization['resultWrapper']\n    response_wrapper = result_wrapper.replace('Result', 'Response')\n    metadata = op_model.metadata\n    xml_namespace = metadata['xmlNamespace']\n\n    # build xml tree\n    t_root = etree.Element(response_wrapper,  xmlns=xml_namespace)\n\n    # build metadata\n    t_metadata = etree.Element('ResponseMetadata')\n    t_request_id = etree.Element('RequestId')\n    t_request_id.text = '1549581b-12b7-11e3-895e-1334aEXAMPLE'\n    t_metadata.append(t_request_id)\n    t_root.append(t_metadata)\n\n    # build result\n    t_result = etree.Element(result_wrapper)\n    outputs = op_model.output_shape.members\n    replace_list = []\n    for output_name, output_shape in outputs.items():\n        t_result.append(_get_subtree(output_name, output_shape, replace_list))\n    t_root.append(t_result)\n    xml_body = etree.tostring(t_root, pretty_print=True).decode('utf-8')\n    xml_body_lines = xml_body.splitlines()\n    for replace in replace_list:\n        name = replace[0]\n        prefix = replace[1]\n        singular_name = singularize(name)\n\n        start_tag = '<%s>' % name\n        iter_name = '{}.{}'.format(prefix[-1], name.lower())if prefix else name.lower()\n        loop_start = '{%% for %s in %s %%}' % (singular_name.lower(), iter_name)\n        end_tag = '</%s>' % name\n        loop_end = '{{ endfor }}'\n\n        start_tag_indexes = [i for i, l in enumerate(xml_body_lines) if start_tag in l]\n        if len(start_tag_indexes) != 1:\n            raise Exception('tag %s not found in response body' % start_tag)\n        start_tag_index = start_tag_indexes[0]\n        xml_body_lines.insert(start_tag_index + 1, loop_start)\n\n        end_tag_indexes = [i for i, l in enumerate(xml_body_lines) if end_tag in l]\n        if len(end_tag_indexes) != 1:\n            raise Exception('tag %s not found in response body' % end_tag)\n        end_tag_index = end_tag_indexes[0]\n        xml_body_lines.insert(end_tag_index, loop_end)\n    xml_body = '\\n'.join(xml_body_lines)\n    body = '\\n{}_TEMPLATE = \"\"\"{}\"\"\"'.format(operation.upper(), xml_body)\n    return body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_function(self, fn):\n        if fn.function_name in self._functions:\n            self._functions[fn.function_name]['latest'] = fn\n        else:\n            self._functions[fn.function_name] = {\n                'latest': fn,\n                'versions': [],\n                'alias': weakref.WeakValueDictionary()\n            }\n\n        self._arns[fn.function_arn] = fn", "response": "Adds a function to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef camelcase_to_underscores(argument):\n    ''' Converts a camelcase param like theNewAttribute to the equivalent\n    python underscore variable like the_new_attribute'''\n    result = ''\n    prev_char_title = True\n    if not argument:\n        return argument\n    for index, char in enumerate(argument):\n        try:\n            next_char_title = argument[index + 1].istitle()\n        except IndexError:\n            next_char_title = True\n\n        upper_to_lower = char.istitle() and not next_char_title\n        lower_to_upper = char.istitle() and not prev_char_title\n\n        if index and (upper_to_lower or lower_to_upper):\n            # Only add underscore if char is capital, not first letter, and next\n            # char is not capital\n            result += \"_\"\n        prev_char_title = char.istitle()\n        if not char.isspace():  # Only add non-whitespace\n            result += char.lower()\n    return result", "response": "Converts a camelcase param like theNewAttribute to the equivalent\n    python underscore variable like the_new_attribute"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a camelcase param like the_new_attribute to the equivalent camelcase version like theNewAttribute.", "response": "def underscores_to_camelcase(argument):\n    ''' Converts a camelcase param like the_new_attribute to the equivalent\n    camelcase version like theNewAttribute. Note that the first letter is\n    NOT capitalized by this function '''\n    result = ''\n    previous_was_underscore = False\n    for char in argument:\n        if char != '_':\n            if previous_was_underscore:\n                result += char.upper()\n            else:\n                result += char\n        previous_was_underscore = char == '_'\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a regex matching url to one that can be used with flask", "response": "def convert_regex_to_flask_path(url_path):\n    \"\"\"\n    Converts a regex matching url to one that can be used with flask\n    \"\"\"\n    for token in [\"$\"]:\n        url_path = url_path.replace(token, \"\")\n\n    def caller(reg):\n        match_name, match_pattern = reg.groups()\n        return '<regex(\"{0}\"):{1}>'.format(match_pattern, match_name)\n\n    url_path = re.sub(\"\\(\\?P<(.*?)>(.*?)\\)\", caller, url_path)\n\n    if url_path.endswith(\"/?\"):\n        # Flask does own handling of trailing slashes\n        url_path = url_path.rstrip(\"/?\")\n    return url_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_boto3_response(operation):\n    def _boto3_request(method):\n        @wraps(method)\n        def f(self, *args, **kwargs):\n            rendered = method(self, *args, **kwargs)\n            if 'json' in self.headers.get('Content-Type', []):\n                self.response_headers.update(\n                    {'x-amzn-requestid': '2690d7eb-ed86-11dd-9877-6fad448a8419',\n                     'date': datetime.now(pytz.utc).strftime('%a, %d %b %Y %H:%M:%S %Z'),\n                     'content-type': 'application/x-amz-json-1.1'})\n                resp = xml_to_json_response(\n                    self.aws_service_spec, operation, rendered)\n                return '' if resp is None else json.dumps(resp)\n            return rendered\n        return f\n    return _boto3_request", "response": "The decorator to convert an XML response to JSON if the request is is\n    determined to be from boto3. Pass the API action as a parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all the available repositories for this registry.", "response": "def describe_repositories(self, registry_id=None, repository_names=None):\n        \"\"\"\n        maxResults and nextToken not implemented\n        \"\"\"\n        if repository_names:\n            for repository_name in repository_names:\n                if repository_name not in self.repositories:\n                    raise RepositoryNotFoundException(repository_name, registry_id or DEFAULT_REGISTRY_ID)\n\n        repositories = []\n        for repository in self.repositories.values():\n            # If a registry_id was supplied, ensure this repository matches\n            if registry_id:\n                if repository.registry_id != registry_id:\n                    continue\n            # If a list of repository names was supplied, esure this repository\n            # is in that list\n            if repository_names:\n                if repository.name not in repository_names:\n                    continue\n            repositories.append(repository.response_object)\n        return repositories"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_images(self, repository_name, registry_id=None):\n        repository = None\n        found = False\n        if repository_name in self.repositories:\n            repository = self.repositories[repository_name]\n            if registry_id:\n                if repository.registry_id == registry_id:\n                    found = True\n            else:\n                found = True\n\n        if not found:\n            raise RepositoryNotFoundException(repository_name, registry_id or DEFAULT_REGISTRY_ID)\n\n        images = []\n        for image in repository.images:\n            images.append(image)\n        return images", "response": "List all images in a repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the lhs attribute of the item", "response": "def _lhs(self, item):\n        \"\"\"\n        :type item: moto.dynamodb2.models.Item\n        \"\"\"\n        lhs = self.lhs\n        if isinstance(self.lhs, (Op, Func)):\n            lhs = self.lhs.expr(item)\n        elif isinstance(self.lhs, six.string_types):\n            try:\n                lhs = item.attrs[self.lhs].cast_value\n            except Exception:\n                pass\n\n        return lhs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_next_entry(self, method, info, request):\n\n        if method not in self.current_entries:\n            self.current_entries[method] = 0\n\n        # restrict selection to entries that match the requested method\n        entries_for_method = [e for e in self.entries if e.method == method]\n\n        if self.current_entries[method] >= len(entries_for_method):\n            self.current_entries[method] = -1\n\n        if not self.entries or not entries_for_method:\n            raise ValueError('I have no entries for method %s: %s'\n                             % (method, self))\n\n        entry = entries_for_method[self.current_entries[method]]\n        if self.current_entries[method] != -1:\n            self.current_entries[method] += 1\n\n        # Attach more info to the entry\n        # So the callback can be more clever about what to do\n        # This does also fix the case where the callback\n        # would be handed a compiled regex as uri instead of the\n        # real uri\n        entry.info = info\n        entry.request = request\n        return entry", "response": "Cycle through available responses but only once."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the BlockDeviceMapping HTTP request.", "response": "def _block_device_mapping_handler(self):\n        \"\"\"\n        Handles requests which are generated by code similar to:\n\n            instance.modify_attribute(\n                BlockDeviceMappings=[{\n                    'DeviceName': '/dev/sda1',\n                    'Ebs': {'DeleteOnTermination': True}\n                }]\n            )\n\n        The querystring contains information similar to:\n\n            BlockDeviceMapping.1.Ebs.DeleteOnTermination : ['true']\n            BlockDeviceMapping.1.DeviceName : ['/dev/sda1']\n\n        For now we only support the \"BlockDeviceMapping.1.Ebs.DeleteOnTermination\"\n        configuration, but it should be trivial to add anything else.\n        \"\"\"\n        mapping_counter = 1\n        mapping_device_name_fmt = 'BlockDeviceMapping.%s.DeviceName'\n        mapping_del_on_term_fmt = 'BlockDeviceMapping.%s.Ebs.DeleteOnTermination'\n        while True:\n            mapping_device_name = mapping_device_name_fmt % mapping_counter\n            if mapping_device_name not in self.querystring.keys():\n                break\n\n            mapping_del_on_term = mapping_del_on_term_fmt % mapping_counter\n            del_on_term_value_str = self.querystring[mapping_del_on_term][0]\n            del_on_term_value = True if 'true' == del_on_term_value_str else False\n            device_name_value = self.querystring[mapping_device_name][0]\n\n            instance_id = self._get_param('InstanceId')\n            instance = self.ec2_backend.get_instance(instance_id)\n\n            if self.is_not_dryrun('ModifyInstanceAttribute'):\n                block_device_type = instance.block_device_mapping[\n                    device_name_value]\n                block_device_type.delete_on_termination = del_on_term_value\n\n            # +1 for the next device\n            mapping_counter += 1\n\n        if mapping_counter > 1:\n            return EC2_MODIFY_INSTANCE_ATTRIBUTE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmocking the X-Ray sdk by pwning its evil singleton with our methods The X-Ray SDK has normally been imported and `patched()` called long before we start mocking. This means the Context() will be very unhappy if an env var isnt present, so we set that, save the old context, then supply our new context. We also patch the Emitter by subclassing the UDPEmitter class replacing its methods and pushing that itno the recorder instance.", "response": "def mock_xray_client(f):\n    \"\"\"\n    Mocks the X-Ray sdk by pwning its evil singleton with our methods\n\n    The X-Ray SDK has normally been imported and `patched()` called long before we start mocking.\n    This means the Context() will be very unhappy if an env var isnt present, so we set that, save\n    the old context, then supply our new context.\n    We also patch the Emitter by subclassing the UDPEmitter class replacing its methods and pushing\n    that itno the recorder instance.\n    \"\"\"\n    @wraps(f)\n    def _wrapped(*args, **kwargs):\n        print(\"Starting X-Ray Patch\")\n\n        old_xray_context_var = os.environ.get('AWS_XRAY_CONTEXT_MISSING')\n        os.environ['AWS_XRAY_CONTEXT_MISSING'] = 'LOG_ERROR'\n        old_xray_context = aws_xray_sdk.core.xray_recorder._context\n        old_xray_emitter = aws_xray_sdk.core.xray_recorder._emitter\n        aws_xray_sdk.core.xray_recorder._context = AWSContext()\n        aws_xray_sdk.core.xray_recorder._emitter = MockEmitter()\n\n        try:\n            return f(*args, **kwargs)\n        finally:\n\n            if old_xray_context_var is None:\n                del os.environ['AWS_XRAY_CONTEXT_MISSING']\n            else:\n                os.environ['AWS_XRAY_CONTEXT_MISSING'] = old_xray_context_var\n\n            aws_xray_sdk.core.xray_recorder._emitter = old_xray_emitter\n            aws_xray_sdk.core.xray_recorder._context = old_xray_context\n\n    return _wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_description_dict(self):\n        return {\n            'certificateArn': self.arn,\n            'certificateId': self.certificate_id,\n            'status': self.status,\n            'certificatePem': self.certificate_pem,\n            'ownedBy': self.owner,\n            'creationDate': self.creation_date,\n            'lastModifiedDate': self.last_modified_date,\n            'transferData': self.transfer_data\n        }", "response": "Returns a dictionary that represents the certificate as described in RFC 2246 section 3. 1. 1."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_principal(self, principal_arn):\n        if ':cert/' in principal_arn:\n            certs = [_ for _ in self.certificates.values() if _.arn == principal_arn]\n            if len(certs) == 0:\n                raise ResourceNotFoundException()\n            principal = certs[0]\n            return principal\n        else:\n            # TODO: search for cognito_ids\n            pass\n        raise ResourceNotFoundException()", "response": "Get the principal from the cognito_ids\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an ec2 reservation if one doesn t already exist and call start_instance.", "response": "def start(self):\n        \"\"\"\n        create an ec2 reservation if one doesn't already exist and call\n        start_instance. Update instance attributes to the newly created instance\n        attributes\n        \"\"\"\n        if self.instance is None:\n            reservation = self.ec2_backend.add_instances(\n                image_id=self.ami_id,\n                count=1,\n                user_data=\"\",\n                security_group_names=[],\n                security_group_ids=self.security_group_ids,\n                instance_type=self.instance_type,\n                key_name=self.ssh_keyname,\n                ebs_optimized=self.ebs_optimized,\n                subnet_id=self.subnet_id,\n                associate_public_ip=self.associate_public_ip,\n            )\n            self.instance = reservation.instances[0]\n            self.reported_os = {\n                'Family': 'rhel (fixed)',\n                'Name': 'amazon (fixed)',\n                'Version': '2016.03 (fixed)'\n            }\n            self.platform = self.instance.platform\n            self.security_group_ids = self.instance.security_groups\n            self.architecture = self.instance.architecture\n            self.virtualization_type = self.instance.virtualization_type\n            self.subnet_id = self.instance.subnet_id\n            self.root_device_type = self.instance.root_device_type\n\n        self.ec2_backend.start_instances([self.instance.id])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten_json_request_body(prefix, dict_body, spec):\n    if len(spec) == 1 and 'type' in spec:\n        return {prefix: to_str(dict_body, spec)}\n\n    flat = {}\n    for key, value in dict_body.items():\n        node_type = spec[key]['type']\n        if node_type == 'list':\n            for idx, v in enumerate(value, 1):\n                pref = key + '.member.' + str(idx)\n                flat.update(flatten_json_request_body(\n                    pref, v, spec[key]['member']))\n        elif node_type == 'map':\n            for idx, (k, v) in enumerate(value.items(), 1):\n                pref = key + '.entry.' + str(idx)\n                flat.update(flatten_json_request_body(\n                    pref + '.key', k, spec[key]['key']))\n                flat.update(flatten_json_request_body(\n                    pref + '.value', v, spec[key]['value']))\n        else:\n            flat.update(flatten_json_request_body(key, value, spec[key]))\n\n    if prefix:\n        prefix = prefix + '.'\n    return dict((prefix + k, v) for k, v in flat.items())", "response": "Convert a JSON request body into query params."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xml_to_json_response(service_spec, operation, xml, result_node=None):\n\n    def transform(value, spec):\n        \"\"\"Apply transformations to make the output JSON comply with the\n        expected form. This function applies:\n\n          (1) Type cast to nodes with \"type\" property (e.g., 'true' to\n              True). XML field values are all in text so this step is\n              necessary to convert it to valid JSON objects.\n\n          (2) Squashes \"member\" nodes to lists.\n\n        \"\"\"\n        if len(spec) == 1:\n            return from_str(value, spec)\n\n        od = OrderedDict()\n        for k, v in value.items():\n            if k.startswith('@'):\n                continue\n\n            if k not in spec:\n                # this can happen when with an older version of\n                # botocore for which the node in XML template is not\n                # defined in service spec.\n                log.warning(\n                    'Field %s is not defined by the botocore version in use', k)\n                continue\n\n            if spec[k]['type'] == 'list':\n                if v is None:\n                    od[k] = []\n                elif len(spec[k]['member']) == 1:\n                    if isinstance(v['member'], list):\n                        od[k] = transform(v['member'], spec[k]['member'])\n                    else:\n                        od[k] = [transform(v['member'], spec[k]['member'])]\n                elif isinstance(v['member'], list):\n                    od[k] = [transform(o, spec[k]['member'])\n                             for o in v['member']]\n                elif isinstance(v['member'], OrderedDict):\n                    od[k] = [transform(v['member'], spec[k]['member'])]\n                else:\n                    raise ValueError('Malformatted input')\n            elif spec[k]['type'] == 'map':\n                if v is None:\n                    od[k] = {}\n                else:\n                    items = ([v['entry']] if not isinstance(v['entry'], list) else\n                             v['entry'])\n                    for item in items:\n                        key = from_str(item['key'], spec[k]['key'])\n                        val = from_str(item['value'], spec[k]['value'])\n                        if k not in od:\n                            od[k] = {}\n                        od[k][key] = val\n            else:\n                if v is None:\n                    od[k] = None\n                else:\n                    od[k] = transform(v, spec[k])\n        return od\n\n    dic = xmltodict.parse(xml)\n    output_spec = service_spec.output_spec(operation)\n    try:\n        for k in (result_node or (operation + 'Response', operation + 'Result')):\n            dic = dic[k]\n    except KeyError:\n        return None\n    else:\n        return transform(dic, output_spec)\n    return None", "response": "Convert rendered XML response to JSON for use with boto3."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_current_user(self):\n        if 'Authorization' in self.headers:\n            match = self.access_key_regex.search(self.headers['Authorization'])\n            if match:\n                return match.group(1)\n\n        if self.querystring.get('AWSAccessKeyId'):\n            return self.querystring.get('AWSAccessKeyId')\n        else:\n            # Should we raise an unauthorized exception instead?\n            return '111122223333'", "response": "Returns the current user id used in this request as the current user id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting uri w / placeholder to regexp", "response": "def uri_to_regexp(self, uri):\n        \"\"\"converts uri w/ placeholder to regexp\n          '/cars/{carName}/drivers/{DriverName}'\n        -> '^/cars/.*/drivers/[^/]*$'\n\n          '/cars/{carName}/drivers/{DriverName}/drive'\n        -> '^/cars/.*/drivers/.*/drive$'\n\n        \"\"\"\n        def _convert(elem, is_last):\n            if not re.match('^{.*}$', elem):\n                return elem\n            name = elem.replace('{', '').replace('}', '')\n            if is_last:\n                return '(?P<%s>[^/]*)' % name\n            return '(?P<%s>.*)' % name\n\n        elems = uri.split('/')\n        num_elems = len(elems)\n        regexp = '^{}$'.format('/'.join([_convert(elem, (i == num_elems - 1)) for i, elem in enumerate(elems)]))\n        return regexp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_action_from_method_and_request_uri(self, method, request_uri):\n\n        # service response class should have 'SERVICE_NAME' class member,\n        # if you want to get action from method and url\n        if not hasattr(self, 'SERVICE_NAME'):\n            return None\n        service = self.SERVICE_NAME\n        conn = boto3.client(service, region_name=self.region)\n\n        # make cache if it does not exist yet\n        if not hasattr(self, 'method_urls'):\n            self.method_urls = defaultdict(lambda: defaultdict(str))\n            op_names = conn._service_model.operation_names\n            for op_name in op_names:\n                op_model = conn._service_model.operation_model(op_name)\n                _method = op_model.http['method']\n                uri_regexp = self.uri_to_regexp(op_model.http['requestUri'])\n                self.method_urls[_method][uri_regexp] = op_model.name\n        regexp_and_names = self.method_urls[method]\n        for regexp, name in regexp_and_names.items():\n            match = re.match(regexp, request_uri)\n            self.uri_match = match\n            if match:\n                return name\n        return None", "response": "returns the action name for the given method and request uri"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_multi_param(self, param_prefix):\n        if param_prefix.endswith(\".\"):\n            prefix = param_prefix\n        else:\n            prefix = param_prefix + \".\"\n        values = []\n        index = 1\n        while True:\n            value_dict = self._get_multi_param_helper(prefix + str(index))\n            if not value_dict:\n                break\n\n            values.append(value_dict)\n            index += 1\n\n        return values", "response": "This method returns a list of dictionaries that are passed to the _get_multi_param_helper method."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary of the parameter keys that start with param_prefix", "response": "def _get_dict_param(self, param_prefix):\n        \"\"\"\n        Given a parameter dict of\n        {\n            'Instances.SlaveInstanceType': ['m1.small'],\n            'Instances.InstanceCount': ['1']\n        }\n\n        returns\n        {\n            \"SlaveInstanceType\": \"m1.small\",\n            \"InstanceCount\": \"1\",\n        }\n        \"\"\"\n        params = {}\n        for key, value in self.querystring.items():\n            if key.startswith(param_prefix):\n                params[camelcase_to_underscores(\n                    key.replace(param_prefix, \"\"))] = value[0]\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_list_prefix(self, param_prefix):\n        results = []\n        param_index = 1\n        while True:\n            index_prefix = \"{0}.{1}.\".format(param_prefix, param_index)\n            new_items = {}\n            for key, value in self.querystring.items():\n                if key.startswith(index_prefix):\n                    new_items[camelcase_to_underscores(\n                        key.replace(index_prefix, \"\"))] = value[0]\n            if not new_items:\n                break\n            results.append(new_items)\n            param_index += 1\n        return results", "response": "Returns a list of all the keys that start with the given prefix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_object_map(self, prefix, name='Name', value='Value'):\n        object_map = {}\n        index = 1\n        while True:\n            # Loop through looking for keys representing object name\n            name_key = '{0}.{1}.{2}'.format(prefix, index, name)\n            obj_name = self.querystring.get(name_key)\n            if not obj_name:\n                # Found all keys\n                break\n\n            obj = {}\n            value_key_prefix = '{0}.{1}.{2}.'.format(\n                prefix, index, value)\n            for k, v in self.querystring.items():\n                if k.startswith(value_key_prefix):\n                    _, value_key = k.split(value_key_prefix, 1)\n                    obj[value_key] = v[0]\n\n            object_map[obj_name[0]] = obj\n\n            index += 1\n\n        return object_map", "response": "Returns a dictionary that maps object names to objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the RecursiveDictRef object to keep reference to the dict at the key.", "response": "def set_reference(self, key, dic):\n        \"\"\"Set the RecursiveDictRef object to keep reference to dict object\n        (dic) at the key.\n\n        \"\"\"\n        self.key = key\n        self.dic = dic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare this type against a range of objects", "response": "def compare(self, range_comparison, range_objs):\n        \"\"\"\n        Compares this type against comparison filters\n        \"\"\"\n        range_values = [obj.value for obj in range_objs]\n        comparison_func = get_comparison_func(range_comparison)\n        return comparison_func(self.value, *range_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paginate(limit, start_arg=\"next_token\", limit_arg=\"max_results\"):\n    default_start = 0\n\n    def outer_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            start = int(default_start if kwargs.get(start_arg) is None else kwargs[start_arg])\n            lim = int(limit if kwargs.get(limit_arg) is None else kwargs[limit_arg])\n            stop = start + lim\n            result = func(*args, **kwargs)\n            limited_results = list(itertools.islice(result, start, stop))\n            next_token = stop if stop < len(result) else None\n            return limited_results, next_token\n        return wrapper\n    return outer_wrapper", "response": "Returns a decorated function that returns a limited result list and an offset into list of remaining items\nAppsite objects"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _url_matches(self, url, other, match_querystring=False):\n        '''\n        Need to override this so we can fix querystrings breaking regex matching\n        '''\n        if not match_querystring:\n            other = other.split('?', 1)[0]\n\n        if responses._is_string(url):\n            if responses._has_unicode(url):\n                url = responses._clean_unicode(url)\n                if not isinstance(other, six.text_type):\n                    other = other.encode('ascii').decode('utf8')\n            return self._url_matches_strict(url, other)\n        elif isinstance(url, responses.Pattern) and url.match(other):\n            return True\n        else:\n            return False", "response": "Returns True if the url matches the other."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef urls(self):\n        url_bases = self._url_module.url_bases\n        unformatted_paths = self._url_module.url_paths\n\n        urls = {}\n        for url_base in url_bases:\n            for url_path, handler in unformatted_paths.items():\n                url = url_path.format(url_base)\n                urls[url] = handler\n\n        return urls", "response": "A dictionary of the urls that should be mocked with this service and the handlers that should be called in their place\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url_paths(self):\n        unformatted_paths = self._url_module.url_paths\n\n        paths = {}\n        for unformatted_path, handler in unformatted_paths.items():\n            path = unformatted_path.format(\"\")\n            paths[path] = handler\n\n        return paths", "response": "A dictionary of the paths of the urls that should be mocked with this service and the handlers that should be called in their place."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flask_paths(self):\n        paths = {}\n        for url_path, handler in self.url_paths.items():\n            url_path = convert_regex_to_flask_path(url_path)\n            paths[url_path] = handler\n\n        return paths", "response": "The url paths that will be used for the flask server"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert shorthand intrinsic function to full name", "response": "def yaml_tag_constructor(loader, tag, node):\n    \"\"\"convert shorthand intrinsic function to full name\n    \"\"\"\n    def _f(loader, tag, node):\n        if tag == '!GetAtt':\n            return node.value.split('.')\n        elif type(node) == yaml.SequenceNode:\n            return loader.construct_sequence(node)\n        else:\n            return node.value\n\n    if tag == '!Ref':\n        key = 'Ref'\n    else:\n        key = 'Fn::{}'.format(tag[1:])\n\n    return {key: _f(loader, tag, node)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse request headers and extracts part od the X - Amz - Target that corresponds to a method of DynamoHandler ie. ListTables.", "response": "def get_endpoint_name(self, headers):\n        \"\"\"Parses request headers and extracts part od the X-Amz-Target\n        that corresponds to a method of DynamoHandler\n\n        ie: X-Amz-Target: DynamoDB_20111205.ListTables -> ListTables\n        \"\"\"\n        # Headers are case-insensitive. Probably a better way to do this.\n        match = headers.get('x-amz-target') or headers.get('X-Amz-Target')\n        if match:\n            return match.split(\".\")[1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning {u'AWSAccessKeyId': [u'the_key'], u'Action': [u'CreateDhcpOptions'], u'DhcpConfiguration.1.Key': [u'domain-name'], u'DhcpConfiguration.1.Value.1': [u'example.com'], u'DhcpConfiguration.2.Key': [u'domain-name-servers'], u'DhcpConfiguration.2.Value.1': [u'10.0.0.6'], u'DhcpConfiguration.2.Value.2': [u'10.0.0.7'], u'Signature': [u'uUMHYOoLM6r+sT4fhYjdNT6MHw22Wj1mafUpe0P0bY4='], u'SignatureMethod': [u'HmacSHA256'], u'SignatureVersion': [u'2'], u'Timestamp': [u'2014-03-18T21:54:01Z'], u'Version': [u'2013-10-15']} into: {u'domain-name': [u'example.com'], u'domain-name-servers': [u'10.0.0.6', u'10.0.0.7']}", "response": "def dhcp_configuration_from_querystring(querystring, option=u'DhcpConfiguration'):\n    \"\"\"\n    turn:\n        {u'AWSAccessKeyId': [u'the_key'],\n         u'Action': [u'CreateDhcpOptions'],\n         u'DhcpConfiguration.1.Key': [u'domain-name'],\n         u'DhcpConfiguration.1.Value.1': [u'example.com'],\n         u'DhcpConfiguration.2.Key': [u'domain-name-servers'],\n         u'DhcpConfiguration.2.Value.1': [u'10.0.0.6'],\n         u'DhcpConfiguration.2.Value.2': [u'10.0.0.7'],\n         u'Signature': [u'uUMHYOoLM6r+sT4fhYjdNT6MHw22Wj1mafUpe0P0bY4='],\n         u'SignatureMethod': [u'HmacSHA256'],\n         u'SignatureVersion': [u'2'],\n         u'Timestamp': [u'2014-03-18T21:54:01Z'],\n         u'Version': [u'2013-10-15']}\n    into:\n        {u'domain-name': [u'example.com'], u'domain-name-servers': [u'10.0.0.6', u'10.0.0.7']}\n    \"\"\"\n\n    key_needle = re.compile(u'{0}.[0-9]+.Key'.format(option), re.UNICODE)\n    response_values = {}\n\n    for key, value in querystring.items():\n        if key_needle.match(key):\n            values = []\n            key_index = key.split(\".\")[1]\n            value_index = 1\n            while True:\n                value_key = u'{0}.{1}.Value.{2}'.format(\n                    option, key_index, value_index)\n                if value_key in querystring:\n                    values.extend(querystring[value_key])\n                else:\n                    break\n                value_index += 1\n            response_values[value[0]] = values\n    return response_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_instance_identity_document(instance):\n\n    document = {\n        'devPayProductCodes': None,\n        'availabilityZone': instance.placement['AvailabilityZone'],\n        'privateIp': instance.private_ip_address,\n        'version': '2010-8-31',\n        'region': instance.placement['AvailabilityZone'][:-1],\n        'instanceId': instance.id,\n        'billingProducts': None,\n        'instanceType': instance.instance_type,\n        'accountId': '012345678910',\n        'pendingTime': '2015-11-19T16:32:11Z',\n        'imageId': instance.image_id,\n        'kernelId': instance.kernel_id,\n        'ramdiskId': instance.ramdisk_id,\n        'architecture': instance.architecture,\n    }\n\n    return document", "response": "Generate a JSON file that describes an instance. Usually retrieved by URL :\n    http://169. 254. 161. 254 / latest / dynamic / instance - identity - documents. html\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_multi_instances_by_id(self, instance_ids):\n        result = []\n\n        for reservation in self.all_reservations():\n            for instance in reservation.instances:\n                if instance.id in instance_ids:\n                    result.append(instance)\n\n        # TODO: Trim error message down to specific invalid id.\n        if instance_ids and len(instance_ids) > len(result):\n            raise InvalidInstanceIdError(instance_ids)\n\n        return result", "response": "Returns a list of all instances with the same id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_reservations_by_instance_ids(self, instance_ids, filters=None):\n        reservations = []\n        for reservation in self.all_reservations():\n            reservation_instance_ids = [\n                instance.id for instance in reservation.instances]\n            matching_reservation = any(\n                instance_id in reservation_instance_ids for instance_id in instance_ids)\n            if matching_reservation:\n                reservation.instances = [\n                    instance for instance in reservation.instances if instance.id in instance_ids]\n                reservations.append(reservation)\n        found_instance_ids = [\n            instance.id for reservation in reservations for instance in reservation.instances]\n        if len(found_instance_ids) != len(instance_ids):\n            invalid_id = list(set(instance_ids).difference(\n                set(found_instance_ids)))[0]\n            raise InvalidInstanceIdError(invalid_id)\n        if filters is not None:\n            reservations = filter_reservations(reservations, filters)\n        return reservations", "response": "Return all of the reservations associated with the given instance_ids."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_filter_value(self, filter_name):\n        if filter_name in ('cidr', 'cidrBlock', 'cidr-block'):\n            return self.cidr_block\n        elif filter_name in ('vpc-id', 'vpcId'):\n            return self.vpc_id\n        elif filter_name == 'subnet-id':\n            return self.id\n        elif filter_name in ('availabilityZone', 'availability-zone'):\n            return self.availability_zone\n        elif filter_name in ('defaultForAz', 'default-for-az'):\n            return self.default_for_az\n        else:\n            return super(Subnet, self).get_filter_value(\n                filter_name, 'DescribeSubnets')", "response": "Get the value of a specific filter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_filter_value(self, filter_name):\n        if filter_name == 'dhcp-options-id':\n            return self.id\n        elif filter_name == 'key':\n            return list(self._options.keys())\n        elif filter_name == 'value':\n            values = [item for item in list(self._options.values()) if item]\n            return itertools.chain(*values)\n        else:\n            return super(DHCPOptionsSet, self).get_filter_value(\n                filter_name, 'DescribeDhcpOptions')", "response": "Returns the value of a filter specified by filter_name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_task_definitions(self):\n        task_arns = []\n        for task_definition_list in self.task_definitions.values():\n            task_arns.extend(\n                [task_definition.arn for task_definition in task_definition_list])\n        return task_arns", "response": "Returns a list of all the task definitions in the current session."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new shadow object from a previous version of the shadow.", "response": "def create_from_previous_version(cls, previous_shadow, payload):\n        \"\"\"\n        set None to payload when you want to delete shadow\n        \"\"\"\n        version, previous_payload = (previous_shadow.version + 1, previous_shadow.to_dict(include_delta=False)) if previous_shadow else (1, {})\n\n        if payload is None:\n            # if given payload is None, delete existing payload\n            # this means the request was delete_thing_shadow\n            shadow = FakeShadow(None, None, None, version, deleted=True)\n            return shadow\n\n        # we can make sure that payload has 'state' key\n        desired = payload['state'].get(\n            'desired',\n            previous_payload.get('state', {}).get('desired', None)\n        )\n        reported = payload['state'].get(\n            'reported',\n            previous_payload.get('state', {}).get('reported', None)\n        )\n        shadow = FakeShadow(desired, reported, payload, version)\n        return shadow"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates metadata dict from state and timestamp", "response": "def _create_metadata_from_state(self, state, ts):\n        \"\"\"\n        state must be disired or reported stype dict object\n        replces primitive type with {\"timestamp\": ts} in dict\n        \"\"\"\n        if state is None:\n            return None\n\n        def _f(elem, ts):\n            if isinstance(elem, dict):\n                return {_: _f(elem[_], ts) for _ in elem.keys()}\n            if isinstance(elem, list):\n                return [_f(_, ts) for _ in elem]\n            return {\"timestamp\": ts}\n        return _f(state, ts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict representation of the object", "response": "def to_dict(self, include_delta=True):\n        \"\"\"returning nothing except for just top-level keys for now.\n        \"\"\"\n        if self.deleted:\n            return {\n                'timestamp': self.timestamp,\n                'version': self.version\n            }\n        delta = self.parse_payload(self.desired, self.reported)\n        payload = {}\n        if self.desired is not None:\n            payload['desired'] = self.desired\n        if self.reported is not None:\n            payload['reported'] = self.reported\n        if include_delta and (delta is not None and len(delta.keys()) != 0):\n            payload['delta'] = delta\n\n        metadata = {}\n        if self.metadata_desired is not None:\n            metadata['desired'] = self.metadata_desired\n        if self.metadata_reported is not None:\n            metadata['reported'] = self.metadata_reported\n\n        return {\n            'state': payload,\n            'metadata': metadata,\n            'timestamp': self.timestamp,\n            'version': self.version\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the thing shadow with the given payload.", "response": "def update_thing_shadow(self, thing_name, payload):\n        \"\"\"\n        spec of payload:\n          - need node `state`\n          - state node must be an Object\n          - State contains an invalid node: 'foo'\n        \"\"\"\n        thing = iot_backends[self.region_name].describe_thing(thing_name)\n\n        # validate\n        try:\n            payload = json.loads(payload)\n        except ValueError:\n            raise InvalidRequestException('invalid json')\n        if 'state' not in payload:\n            raise InvalidRequestException('need node `state`')\n        if not isinstance(payload['state'], dict):\n            raise InvalidRequestException('state node must be an Object')\n        if any(_ for _ in payload['state'].keys() if _ not in ['desired', 'reported']):\n            raise InvalidRequestException('State contains an invalid node')\n\n        new_shadow = FakeShadow.create_from_previous_version(thing.thing_shadow, payload)\n        thing.thing_shadow = new_shadow\n        return thing.thing_shadow"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_thing_shadow(self, thing_name):\n        thing = iot_backends[self.region_name].describe_thing(thing_name)\n        if thing.thing_shadow is None:\n            raise ResourceNotFoundException()\n        payload = None\n        new_shadow = FakeShadow.create_from_previous_version(thing.thing_shadow, payload)\n        thing.thing_shadow = new_shadow\n        return thing.thing_shadow", "response": "delete a shadow from the named thing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclean a resource dict.", "response": "def clean_json(resource_json, resources_map):\n    \"\"\"\n    Cleanup the a resource dict. For now, this just means replacing any Ref node\n    with the corresponding physical_resource_id.\n\n    Eventually, this is where we would add things like function parsing (fn::)\n    \"\"\"\n    if isinstance(resource_json, dict):\n        if 'Ref' in resource_json:\n            # Parse resource reference\n            resource = resources_map[resource_json['Ref']]\n            if hasattr(resource, 'physical_resource_id'):\n                return resource.physical_resource_id\n            else:\n                return resource\n\n        if \"Fn::FindInMap\" in resource_json:\n            map_name = resource_json[\"Fn::FindInMap\"][0]\n            map_path = resource_json[\"Fn::FindInMap\"][1:]\n            result = resources_map[map_name]\n            for path in map_path:\n                result = result[clean_json(path, resources_map)]\n            return result\n\n        if 'Fn::GetAtt' in resource_json:\n            resource = resources_map.get(resource_json['Fn::GetAtt'][0])\n            if resource is None:\n                return resource_json\n            try:\n                return resource.get_cfn_attribute(resource_json['Fn::GetAtt'][1])\n            except NotImplementedError as n:\n                logger.warning(str(n).format(\n                    resource_json['Fn::GetAtt'][0]))\n            except UnformattedGetAttTemplateException:\n                raise ValidationError(\n                    'Bad Request',\n                    UnformattedGetAttTemplateException.description.format(\n                        resource_json['Fn::GetAtt'][0], resource_json['Fn::GetAtt'][1]))\n\n        if 'Fn::If' in resource_json:\n            condition_name, true_value, false_value = resource_json['Fn::If']\n            if resources_map.lazy_condition_map[condition_name]:\n                return clean_json(true_value, resources_map)\n            else:\n                return clean_json(false_value, resources_map)\n\n        if 'Fn::Join' in resource_json:\n            join_list = clean_json(resource_json['Fn::Join'][1], resources_map)\n            return resource_json['Fn::Join'][0].join([str(x) for x in join_list])\n\n        if 'Fn::Split' in resource_json:\n            to_split = clean_json(resource_json['Fn::Split'][1], resources_map)\n            return to_split.split(resource_json['Fn::Split'][0])\n\n        if 'Fn::Select' in resource_json:\n            select_index = int(resource_json['Fn::Select'][0])\n            select_list = clean_json(resource_json['Fn::Select'][1], resources_map)\n            return select_list[select_index]\n\n        if 'Fn::Sub' in resource_json:\n            if isinstance(resource_json['Fn::Sub'], list):\n                warnings.warn(\n                    \"Tried to parse Fn::Sub with variable mapping but it's not supported by moto's CloudFormation implementation\")\n            else:\n                fn_sub_value = clean_json(resource_json['Fn::Sub'], resources_map)\n                to_sub = re.findall('(?=\\${)[^!^\"]*?}', fn_sub_value)\n                literals = re.findall('(?=\\${!)[^\"]*?}', fn_sub_value)\n                for sub in to_sub:\n                    if '.' in sub:\n                        cleaned_ref = clean_json({'Fn::GetAtt': re.findall('(?<=\\${)[^\"]*?(?=})', sub)[0].split('.')}, resources_map)\n                    else:\n                        cleaned_ref = clean_json({'Ref': re.findall('(?<=\\${)[^\"]*?(?=})', sub)[0]}, resources_map)\n                    fn_sub_value = fn_sub_value.replace(sub, cleaned_ref)\n                for literal in literals:\n                    fn_sub_value = fn_sub_value.replace(literal, literal.replace('!', ''))\n                return fn_sub_value\n            pass\n\n        if 'Fn::ImportValue' in resource_json:\n            cleaned_val = clean_json(resource_json['Fn::ImportValue'], resources_map)\n            values = [x.value for x in resources_map.cross_stack_resources.values() if x.name == cleaned_val]\n            if any(values):\n                return values[0]\n            else:\n                raise ExportNotFound(cleaned_val)\n\n        if 'Fn::GetAZs' in resource_json:\n            region = resource_json.get('Fn::GetAZs') or DEFAULT_REGION\n            result = []\n            # TODO: make this configurable, to reflect the real AWS AZs\n            for az in ('a', 'b', 'c', 'd'):\n                result.append('%s%s' % (region, az))\n            return result\n\n        cleaned_json = {}\n        for key, value in resource_json.items():\n            cleaned_val = clean_json(value, resources_map)\n            if cleaned_val is None:\n                # If we didn't find anything, don't add this attribute\n                continue\n            cleaned_json[key] = cleaned_val\n        return cleaned_json\n    elif isinstance(resource_json, list):\n        return [clean_json(val, resources_map) for val in resource_json]\n    else:\n        return resource_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_arn_from_idempotency_token(self, token):\n        now = datetime.datetime.now()\n        if token in self._idempotency_tokens:\n            if self._idempotency_tokens[token]['expires'] < now:\n                # Token has expired, new request\n                del self._idempotency_tokens[token]\n                return None\n            else:\n                return self._idempotency_tokens[token]['arn']\n\n        return None", "response": "Get the ARN from an idempotent token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_tag_key(self, tag_key, exception_param='tags.X.member.key'):\n        # Validate that the key length is correct:\n        if len(tag_key) > 128:\n            raise TagKeyTooBig(tag_key, param=exception_param)\n\n        # Validate that the tag key fits the proper Regex:\n        # [\\w\\s_.:/=+\\-@]+ SHOULD be the same as the Java regex on the AWS documentation: [\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]+\n        match = re.findall(r'[\\w\\s_.:/=+\\-@]+', tag_key)\n        # Kudos if you can come up with a better way of doing a global search :)\n        if not len(match) or len(match[0]) < len(tag_key):\n            raise InvalidTagCharacters(tag_key, param=exception_param)", "response": "Validates the tag key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enable_mfa_device(self,\n                          user_name,\n                          serial_number,\n                          authentication_code_1,\n                          authentication_code_2):\n        \"\"\"Enable MFA Device for user.\"\"\"\n        user = self.get_user(user_name)\n        if serial_number in user.mfa_devices:\n            raise IAMConflictException(\n                \"EntityAlreadyExists\",\n                \"Device {0} already exists\".format(serial_number)\n            )\n\n        user.enable_mfa_device(\n            serial_number,\n            authentication_code_1,\n            authentication_code_2\n        )", "response": "Enable MFA Device for user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deactivate_mfa_device(self, user_name, serial_number):\n        user = self.get_user(user_name)\n        if serial_number not in user.mfa_devices:\n            raise IAMNotFoundException(\n                \"Device {0} not found\".format(serial_number)\n            )\n\n        user.deactivate_mfa_device(serial_number)", "response": "Deactivate and detach MFA Device from user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encrypt(self):\n        value = self.parameters.get(\"Plaintext\")\n        if isinstance(value, six.text_type):\n            value = value.encode('utf-8')\n        return json.dumps({\"CiphertextBlob\": base64.b64encode(value).decode(\"utf-8\"), 'KeyId': 'key_id'})", "response": "This method encodes the value of the KeyID parameter and then decodes it in decrypt."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, *args, **kwargs):\n        ''' Not exposed as part of the Route 53 API - used for CloudFormation. args are ignored '''\n        hosted_zone = route53_backend.get_hosted_zone_by_name(\n            self.hosted_zone_name)\n        if not hosted_zone:\n            hosted_zone = route53_backend.get_hosted_zone(self.hosted_zone_id)\n        hosted_zone.delete_rrset_by_name(self.name)", "response": "Delete the resource from the Route 53 API."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a RequestLine and return the HTTP - Method and HTTP - Version.", "response": "def parse_requestline(s):\n    \"\"\"\n    http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5\n\n    >>> parse_requestline('GET / HTTP/1.0')\n    ('GET', '/', '1.0')\n    >>> parse_requestline('post /testurl htTP/1.1')\n    ('POST', '/testurl', '1.1')\n    >>> parse_requestline('Im not a RequestLine')\n    Traceback (most recent call last):\n        ...\n    ValueError: Not a Request-Line\n    \"\"\"\n    methods = '|'.join(HttpBaseClass.METHODS)\n    m = re.match(r'(' + methods + ')\\s+(.*)\\s+HTTP/(1.[0|1])', s, re.I)\n    if m:\n        return m.group(1).upper(), m.group(2), m.group(3)\n    else:\n        raise ValueError('Not a Request-Line')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the last line in sent_data that can be parsed with parse_requestline", "response": "def last_requestline(sent_data):\n    \"\"\"\n    Find the last line in sent_data that can be parsed with parse_requestline\n    \"\"\"\n    for line in reversed(sent_data):\n        try:\n            parse_requestline(decode_utf8(line))\n        except ValueError:\n            pass\n        else:\n            return line"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attribute_md5(self):\n        def utf8(str):\n            if isinstance(str, six.string_types):\n                return str.encode('utf-8')\n            return str\n        md5 = hashlib.md5()\n        struct_format = \"!I\".encode('ascii')  # ensure it's a bytestring\n        for name in sorted(self.message_attributes.keys()):\n            attr = self.message_attributes[name]\n            data_type = attr['data_type']\n\n            encoded = utf8('')\n            # Each part of each attribute is encoded right after it's\n            # own length is packed into a 4-byte integer\n            # 'timestamp' -> b'\\x00\\x00\\x00\\t'\n            encoded += struct.pack(struct_format, len(utf8(name))) + utf8(name)\n            # The datatype is additionally given a final byte\n            # representing which type it is\n            encoded += struct.pack(struct_format, len(data_type)) + utf8(data_type)\n            encoded += TRANSPORT_TYPE_ENCODINGS[data_type]\n\n            if data_type == 'String' or data_type == 'Number':\n                value = attr['string_value']\n            elif data_type == 'Binary':\n                print(data_type, attr['binary_value'], type(attr['binary_value']))\n                value = base64.b64decode(attr['binary_value'])\n            else:\n                print(\"Moto hasn't implemented MD5 hashing for {} attributes\".format(data_type))\n                # The following should be enough of a clue to users that\n                # they are not, in fact, looking at a correct MD5 while\n                # also following the character and length constraints of\n                # MD5 so as not to break client softwre\n                return('deadbeefdeadbeefdeadbeefdeadbeef')\n\n            encoded += struct.pack(struct_format, len(utf8(value))) + utf8(value)\n\n            md5.update(encoded)\n        return md5.hexdigest()", "response": "Calculates the MD5 of all attributes of the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmarking the message as received.", "response": "def mark_received(self, visibility_timeout=None):\n        \"\"\"\n        When a message is received we will set the first receive timestamp,\n        tap the ``approximate_receive_count`` and the ``visible_at`` time.\n        \"\"\"\n        if visibility_timeout:\n            visibility_timeout = int(visibility_timeout)\n        else:\n            visibility_timeout = 0\n\n        if not self.approximate_first_receive_timestamp:\n            self.approximate_first_receive_timestamp = int(unix_time_millis())\n\n        self.approximate_receive_count += 1\n\n        # Make message visible again in the future unless its\n        # destroyed.\n        if visibility_timeout:\n            self.change_visibility(visibility_timeout)\n\n        self.receipt_handle = generate_receipt_handle()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattempt to retrieve visible messages from a queue. If a message was read by client and not deleted it is considered to be \"inflight\" and cannot be read. We make attempts to obtain ``count`` messages but we may return less if messages are in-flight or there are simple not enough messages in the queue. :param string queue_name: The name of the queue to read from. :param int count: The maximum amount of messages to retrieve. :param int visibility_timeout: The number of seconds the message should remain invisible to other queue readers. :param int wait_seconds_timeout: The duration (in seconds) for which the call waits for a message to arrive in the queue before returning. If a message is available, the call returns sooner than WaitTimeSeconds", "response": "def receive_messages(self, queue_name, count, wait_seconds_timeout, visibility_timeout):\n        \"\"\"\n        Attempt to retrieve visible messages from a queue.\n\n        If a message was read by client and not deleted it is considered to be\n        \"inflight\" and cannot be read. We make attempts to obtain ``count``\n        messages but we may return less if messages are in-flight or there\n        are simple not enough messages in the queue.\n\n        :param string queue_name: The name of the queue to read from.\n        :param int count: The maximum amount of messages to retrieve.\n        :param int visibility_timeout: The number of seconds the message should remain invisible to other queue readers.\n        :param int wait_seconds_timeout:  The duration (in seconds) for which the call waits for a message to arrive in\n         the queue before returning. If a message is available, the call returns sooner than WaitTimeSeconds\n        \"\"\"\n        queue = self.get_queue(queue_name)\n        result = []\n        previous_result_count = len(result)\n\n        polling_end = unix_time() + wait_seconds_timeout\n\n        # queue.messages only contains visible messages\n        while True:\n\n            if result or (wait_seconds_timeout and unix_time() > polling_end):\n                break\n\n            messages_to_dlq = []\n\n            for message in queue.messages:\n                if not message.visible:\n                    continue\n\n                if message in queue.pending_messages:\n                    # The message is pending but is visible again, so the\n                    # consumer must have timed out.\n                    queue.pending_messages.remove(message)\n\n                if message.group_id and queue.fifo_queue:\n                    if message.group_id in queue.pending_message_groups:\n                        # There is already one active message with the same\n                        # group, so we cannot deliver this one.\n                        continue\n\n                queue.pending_messages.add(message)\n\n                if queue.dead_letter_queue is not None and message.approximate_receive_count >= queue.redrive_policy['maxReceiveCount']:\n                    messages_to_dlq.append(message)\n                    continue\n\n                message.mark_received(\n                    visibility_timeout=visibility_timeout\n                )\n                result.append(message)\n                if len(result) >= count:\n                    break\n\n            for message in messages_to_dlq:\n                queue._messages.remove(message)\n                queue.dead_letter_queue.add_message(message)\n\n            if previous_result_count == len(result):\n                if wait_seconds_timeout == 0:\n                    # There is timeout and we have added no additional results,\n                    # so break to avoid an infinite loop.\n                    break\n\n                import time\n                time.sleep(0.01)\n                continue\n\n            previous_result_count = len(result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_validated_visibility_timeout(self, timeout=None):\n        if timeout is not None:\n            visibility_timeout = int(timeout)\n        else:\n            visibility_timeout = int(self.querystring.get(\"VisibilityTimeout\")[0])\n\n        if visibility_timeout > MAXIMUM_VISIBILTY_TIMEOUT:\n            raise ValueError\n\n        return visibility_timeout", "response": "Returns the visibility timeout for the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message batch to the queue.", "response": "def send_message_batch(self):\n        \"\"\"\n        The querystring comes like this\n\n        'SendMessageBatchRequestEntry.1.DelaySeconds': ['0'],\n        'SendMessageBatchRequestEntry.1.MessageBody': ['test message 1'],\n        'SendMessageBatchRequestEntry.1.Id': ['6d0f122d-4b13-da2c-378f-e74244d8ad11']\n        'SendMessageBatchRequestEntry.2.Id': ['ff8cbf59-70a2-c1cb-44c7-b7469f1ba390'],\n        'SendMessageBatchRequestEntry.2.MessageBody': ['test message 2'],\n        'SendMessageBatchRequestEntry.2.DelaySeconds': ['0'],\n        \"\"\"\n\n        queue_name = self._get_queue_name()\n\n        messages = []\n        for index in range(1, 11):\n            # Loop through looking for messages\n            message_key = 'SendMessageBatchRequestEntry.{0}.MessageBody'.format(\n                index)\n            message_body = self.querystring.get(message_key)\n            if not message_body:\n                # Found all messages\n                break\n\n            message_user_id_key = 'SendMessageBatchRequestEntry.{0}.Id'.format(\n                index)\n            message_user_id = self.querystring.get(message_user_id_key)[0]\n            delay_key = 'SendMessageBatchRequestEntry.{0}.DelaySeconds'.format(\n                index)\n            delay_seconds = self.querystring.get(delay_key, [None])[0]\n            message = self.sqs_backend.send_message(\n                queue_name, message_body[0], delay_seconds=delay_seconds)\n            message.user_id = message_user_id\n\n            message_attributes = parse_message_attributes(\n                self.querystring, base='SendMessageBatchRequestEntry.{0}.'.format(index))\n            if type(message_attributes) == tuple:\n                return message_attributes[0], message_attributes[1]\n            message.message_attributes = message_attributes\n\n            messages.append(message)\n\n        template = self.response_template(SEND_MESSAGE_BATCH_RESPONSE)\n        return template.render(messages=messages)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a batch of messages from the queue.", "response": "def delete_message_batch(self):\n        \"\"\"\n        The querystring comes like this\n\n        'DeleteMessageBatchRequestEntry.1.Id': ['message_1'],\n        'DeleteMessageBatchRequestEntry.1.ReceiptHandle': ['asdfsfs...'],\n        'DeleteMessageBatchRequestEntry.2.Id': ['message_2'],\n        'DeleteMessageBatchRequestEntry.2.ReceiptHandle': ['zxcvfda...'],\n        ...\n        \"\"\"\n        queue_name = self._get_queue_name()\n\n        message_ids = []\n        for index in range(1, 11):\n            # Loop through looking for messages\n            receipt_key = 'DeleteMessageBatchRequestEntry.{0}.ReceiptHandle'.format(\n                index)\n            receipt_handle = self.querystring.get(receipt_key)\n            if not receipt_handle:\n                # Found all messages\n                break\n\n            self.sqs_backend.delete_message(queue_name, receipt_handle[0])\n\n            message_user_id_key = 'DeleteMessageBatchRequestEntry.{0}.Id'.format(\n                index)\n            message_user_id = self.querystring.get(message_user_id_key)[0]\n            message_ids.append(message_user_id)\n\n        template = self.response_template(DELETE_MESSAGE_BATCH_RESPONSE)\n        return template.render(message_ids=message_ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_table_keys_name(self, table_name, keys):\n        table = self.tables.get(table_name)\n        if not table:\n            return None, None\n        else:\n            if len(keys) == 1:\n                for key in keys:\n                    if key in table.hash_key_names:\n                        return key, None\n            # for potential_hash, potential_range in zip(table.hash_key_names, table.range_key_names):\n            #     if set([potential_hash, potential_range]) == set(keys):\n            #         return potential_hash, potential_range\n            potential_hash, potential_range = None, None\n            for key in set(keys):\n                if key in table.hash_key_names:\n                    potential_hash = key\n                elif key in table.range_key_names:\n                    potential_range = key\n            return potential_hash, potential_range", "response": "Given a set of keys extracts the key and range of the key and range of the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the attribute with the specified name.", "response": "def get_cfn_attribute(self, attribute_name):\n        '''\n        Implemented attributes:\n        * DNSName\n        * LoadBalancerName\n\n        Not implemented:\n        * CanonicalHostedZoneID\n        * LoadBalancerFullName\n        * SecurityGroups\n\n        This method is similar to models.py:FakeLoadBalancer.get_cfn_attribute()\n        '''\n        from moto.cloudformation.exceptions import UnformattedGetAttTemplateException\n        not_implemented_yet = [\n            'CanonicalHostedZoneID',\n            'LoadBalancerFullName',\n            'SecurityGroups',\n        ]\n        if attribute_name == 'DNSName':\n            return self.dns_name\n        elif attribute_name == 'LoadBalancerName':\n            return self.name\n        elif attribute_name in not_implemented_yet:\n            raise NotImplementedError('\"Fn::GetAtt\" : [ \"{0}\" , \"%s\" ]\"' % attribute_name)\n        else:\n            raise UnformattedGetAttTemplateException()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef metadata_response(self, request, full_url, headers):\n\n        parsed_url = urlparse(full_url)\n        tomorrow = datetime.datetime.utcnow() + datetime.timedelta(days=1)\n        credentials = dict(\n            AccessKeyId=\"test-key\",\n            SecretAccessKey=\"test-secret-key\",\n            Token=\"test-session-token\",\n            Expiration=tomorrow.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        )\n\n        path = parsed_url.path\n\n        meta_data_prefix = \"/latest/meta-data/\"\n        # Strip prefix if it is there\n        if path.startswith(meta_data_prefix):\n            path = path[len(meta_data_prefix):]\n\n        if path == '':\n            result = 'iam'\n        elif path == 'iam':\n            result = json.dumps({\n                'security-credentials': {\n                    'default-role': credentials\n                }\n            })\n        elif path == 'iam/security-credentials/':\n            result = 'default-role'\n        elif path == 'iam/security-credentials/default-role':\n            result = json.dumps(credentials)\n        else:\n            raise NotImplementedError(\n                \"The {0} metadata path has not been implemented\".format(path))\n        return 200, headers, result", "response": "Mock response for localhost metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimporting a certificate into a new instance of ACS.", "response": "def import_certificate(self):\n        \"\"\"\n        Returns errors on:\n        Certificate, PrivateKey or Chain not being properly formatted\n        Arn not existing if its provided\n        PrivateKey size > 2048\n        Certificate expired or is not yet in effect\n\n        Does not return errors on:\n        Checking Certificate is legit, or a selfsigned chain is provided\n\n        :return: str(JSON) for response\n        \"\"\"\n        certificate = self._get_param('Certificate')\n        private_key = self._get_param('PrivateKey')\n        chain = self._get_param('CertificateChain')  # Optional\n        current_arn = self._get_param('CertificateArn')  # Optional\n\n        # Simple parameter decoding. Rather do it here as its a data transport decision not part of the\n        # actual data\n        try:\n            certificate = base64.standard_b64decode(certificate)\n        except Exception:\n            return AWSValidationException('The certificate is not PEM-encoded or is not valid.').response()\n        try:\n            private_key = base64.standard_b64decode(private_key)\n        except Exception:\n            return AWSValidationException('The private key is not PEM-encoded or is not valid.').response()\n        if chain is not None:\n            try:\n                chain = base64.standard_b64decode(chain)\n            except Exception:\n                return AWSValidationException('The certificate chain is not PEM-encoded or is not valid.').response()\n\n        try:\n            arn = self.acm_backend.import_cert(certificate, private_key, chain=chain, arn=current_arn)\n        except AWSError as err:\n            return err.response()\n\n        return json.dumps({'CertificateArn': arn})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_timeouts(self):\n        timeout_candidates = []\n\n        # workflow execution timeout\n        timeout_candidates.append(self.first_timeout())\n\n        # decision tasks timeouts\n        for task in self.decision_tasks:\n            timeout_candidates.append(task.first_timeout())\n\n        # activity tasks timeouts\n        for task in self.activity_tasks:\n            timeout_candidates.append(task.first_timeout())\n\n        # remove blank values (foo.first_timeout() is a Timeout or None)\n        timeout_candidates = list(filter(None, timeout_candidates))\n\n        # now find the first timeout to process\n        first_timeout = None\n        if timeout_candidates:\n            first_timeout = min(\n                timeout_candidates,\n                key=lambda t: t.timestamp\n            )\n\n        if first_timeout:\n            should_schedule_decision_next = False\n            if isinstance(first_timeout.obj, WorkflowExecution):\n                self.timeout(first_timeout)\n            elif isinstance(first_timeout.obj, DecisionTask):\n                self.timeout_decision_task(first_timeout)\n                should_schedule_decision_next = True\n            elif isinstance(first_timeout.obj, ActivityTask):\n                self.timeout_activity_task(first_timeout)\n                should_schedule_decision_next = True\n            else:\n                raise NotImplementedError(\"Unhandled timeout object\")\n\n            # schedule decision task if needed\n            if should_schedule_decision_next:\n                self.schedule_decision_task()\n\n            # the workflow execution progressed, let's see if another\n            # timeout should be processed\n            self._process_timeouts()", "response": "Process all timeout objects in the SWF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the list of decisions and returns a list of validation problems.", "response": "def validate_decisions(self, decisions):\n        \"\"\"\n        Performs some basic validations on decisions. The real SWF service\n        seems to break early and *not* process any decision if there's a\n        validation problem, such as a malformed decision for instance. I didn't\n        find an explicit documentation for that though, so criticisms welcome.\n        \"\"\"\n        problems = []\n\n        # check close decision is last\n        # (the real SWF service also works that way if you provide 2 close decision tasks)\n        for dcs in decisions[:-1]:\n            close_decision_types = [\n                \"CompleteWorkflowExecution\",\n                \"FailWorkflowExecution\",\n                \"CancelWorkflowExecution\",\n            ]\n            if dcs[\"decisionType\"] in close_decision_types:\n                raise SWFValidationException(\n                    \"Close must be last decision in list\"\n                )\n\n        decision_number = 0\n        for dcs in decisions:\n            decision_number += 1\n            # check decision types mandatory attributes\n            # NB: the real SWF service seems to check attributes even for attributes list\n            # that are not in line with the decisionType, so we do the same\n            attrs_to_check = [\n                d for d in dcs.keys() if d.endswith(\"DecisionAttributes\")]\n            if dcs[\"decisionType\"] in self.KNOWN_DECISION_TYPES:\n                decision_type = dcs[\"decisionType\"]\n                decision_attr = \"{0}DecisionAttributes\".format(\n                    decapitalize(decision_type))\n                attrs_to_check.append(decision_attr)\n            for attr in attrs_to_check:\n                problems += self._check_decision_attributes(\n                    attr, dcs.get(attr, {}), decision_number)\n            # check decision type is correct\n            if dcs[\"decisionType\"] not in self.KNOWN_DECISION_TYPES:\n                problems.append({\n                    \"type\": \"bad_decision_type\",\n                    \"value\": dcs[\"decisionType\"],\n                    \"where\": \"decisions.{0}.member.decisionType\".format(decision_number),\n                    \"possible_values\": \", \".join(self.KNOWN_DECISION_TYPES),\n                })\n\n        # raise if any problem\n        if any(problems):\n            raise SWFDecisionValidationException(problems)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_decisions(self, event_id, decisions):\n        # handle each decision separately, in order\n        for decision in decisions:\n            decision_type = decision[\"decisionType\"]\n            attributes_key = \"{0}DecisionAttributes\".format(\n                decapitalize(decision_type))\n            attributes = decision.get(attributes_key, {})\n            if decision_type == \"CompleteWorkflowExecution\":\n                self.complete(event_id, attributes.get(\"result\"))\n            elif decision_type == \"FailWorkflowExecution\":\n                self.fail(event_id, attributes.get(\n                    \"details\"), attributes.get(\"reason\"))\n            elif decision_type == \"ScheduleActivityTask\":\n                self.schedule_activity_task(event_id, attributes)\n            else:\n                # TODO: implement Decision type: CancelTimer\n                # TODO: implement Decision type: CancelWorkflowExecution\n                # TODO: implement Decision type: ContinueAsNewWorkflowExecution\n                # TODO: implement Decision type: RecordMarker\n                # TODO: implement Decision type: RequestCancelActivityTask\n                # TODO: implement Decision type: RequestCancelExternalWorkflowExecution\n                # TODO: implement Decision type: ScheduleLambdaFunction\n                # TODO: implement Decision type: SignalExternalWorkflowExecution\n                # TODO: implement Decision type: StartChildWorkflowExecution\n                # TODO: implement Decision type: StartTimer\n                raise NotImplementedError(\n                    \"Cannot handle decision: {0}\".format(decision_type))\n\n        # finally decrement counter if and only if everything went well\n        self.open_counts[\"openDecisionTasks\"] -= 1", "response": "Handles a list of decisions in order."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef itemize(data):\n    if isinstance(data, dict):\n        ret = {}\n        for key in data:\n            ret[key] = itemize(data[key])\n        return ret\n    elif isinstance(data, list):\n        return {'item': [itemize(value) for value in data]}\n    else:\n        return data", "response": "This function takes a dictionary and returns a dictionary with the shape of the input dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntruing of any of the list elements starts with needle", "response": "def _list_element_starts_with(items, needle):\n        \"\"\"True of any of the list elements starts with needle\"\"\"\n        for item in items:\n            if item.startswith(needle):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a condition expression to a dictionary of attributes.", "response": "def condition_expression_to_expected(condition_expression, expression_attribute_names, expression_attribute_values):\n    \"\"\"\n    Limited condition expression syntax parsing.\n    Supports Global Negation ex: NOT(inner expressions).\n    Supports simple AND conditions ex: cond_a AND cond_b and cond_c.\n    Atomic expressions supported are attribute_exists(key), attribute_not_exists(key) and #key = :value.\n    \"\"\"\n    expected = {}\n    if condition_expression and 'OR' not in condition_expression:\n        reverse_re = re.compile('^NOT\\s*\\((.*)\\)$')\n        reverse_m = reverse_re.match(condition_expression.strip())\n\n        reverse = False\n        if reverse_m:\n            reverse = True\n            condition_expression = reverse_m.group(1)\n\n        cond_items = [c.strip() for c in condition_expression.split('AND')]\n        if cond_items:\n            exists_re = re.compile('^attribute_exists\\s*\\((.*)\\)$')\n            not_exists_re = re.compile(\n                '^attribute_not_exists\\s*\\((.*)\\)$')\n            equals_re = re.compile('^(#?\\w+)\\s*=\\s*(\\:?\\w+)')\n\n        for cond in cond_items:\n            exists_m = exists_re.match(cond)\n            not_exists_m = not_exists_re.match(cond)\n            equals_m = equals_re.match(cond)\n\n            if exists_m:\n                attribute_name = expression_attribute_names_lookup(exists_m.group(1), expression_attribute_names)\n                expected[attribute_name] = {'Exists': True if not reverse else False}\n            elif not_exists_m:\n                attribute_name = expression_attribute_names_lookup(not_exists_m.group(1), expression_attribute_names)\n                expected[attribute_name] = {'Exists': False if not reverse else True}\n            elif equals_m:\n                attribute_name = expression_attribute_names_lookup(equals_m.group(1), expression_attribute_names)\n                attribute_value = expression_attribute_values_lookup(equals_m.group(2), expression_attribute_values)\n                expected[attribute_name] = {\n                    'AttributeValueList': [attribute_value],\n                    'ComparisonOperator': 'EQ' if not reverse else 'NEQ'}\n\n    return expected"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the container and return the cloudwatch object", "response": "def run(self):\n        \"\"\"\n        Run the container.\n\n        Logic is as follows:\n        Generate container info (eventually from task definition)\n        Start container\n        Loop whilst not asked to stop and the container is running.\n          Get all logs from container between the last time I checked and now.\n        Convert logs into cloudwatch format\n        Put logs into cloudwatch\n\n        :return:\n        \"\"\"\n        try:\n            self.job_state = 'PENDING'\n            time.sleep(1)\n\n            image = 'alpine:latest'\n            cmd = '/bin/sh -c \"for a in `seq 1 10`; do echo Hello World; sleep 1; done\"'\n            name = '{0}-{1}'.format(self.job_name, self.job_id)\n\n            self.job_state = 'RUNNABLE'\n            # TODO setup ecs container instance\n            time.sleep(1)\n\n            self.job_state = 'STARTING'\n            container = self.docker_client.containers.run(\n                image, cmd,\n                detach=True,\n                name=name\n            )\n            self.job_state = 'RUNNING'\n            self.job_started_at = datetime.datetime.now()\n            try:\n                # Log collection\n                logs_stdout = []\n                logs_stderr = []\n                container.reload()\n\n                # Dodgy hack, we can only check docker logs once a second, but we want to loop more\n                # so we can stop if asked to in a quick manner, should all go away if we go async\n                # There also be some dodgyness when sending an integer to docker logs and some\n                # events seem to be duplicated.\n                now = datetime.datetime.now()\n                i = 1\n                while container.status == 'running' and not self.stop:\n                    time.sleep(0.15)\n                    if i % 10 == 0:\n                        logs_stderr.extend(container.logs(stdout=False, stderr=True, timestamps=True, since=datetime2int(now)).decode().split('\\n'))\n                        logs_stdout.extend(container.logs(stdout=True, stderr=False, timestamps=True, since=datetime2int(now)).decode().split('\\n'))\n                        now = datetime.datetime.now()\n                        container.reload()\n                    i += 1\n\n                # Container should be stopped by this point... unless asked to stop\n                if container.status == 'running':\n                    container.kill()\n\n                self.job_stopped_at = datetime.datetime.now()\n                # Get final logs\n                logs_stderr.extend(container.logs(stdout=False, stderr=True, timestamps=True, since=datetime2int(now)).decode().split('\\n'))\n                logs_stdout.extend(container.logs(stdout=True, stderr=False, timestamps=True, since=datetime2int(now)).decode().split('\\n'))\n\n                self.job_state = 'SUCCEEDED' if not self.stop else 'FAILED'\n\n                # Process logs\n                logs_stdout = [x for x in logs_stdout if len(x) > 0]\n                logs_stderr = [x for x in logs_stderr if len(x) > 0]\n                logs = []\n                for line in logs_stdout + logs_stderr:\n                    date, line = line.split(' ', 1)\n                    date = dateutil.parser.parse(date)\n                    date = int(date.timestamp())\n                    logs.append({'timestamp': date, 'message': line.strip()})\n\n                # Send to cloudwatch\n                log_group = '/aws/batch/job'\n                stream_name = '{0}/default/{1}'.format(self.job_definition.name, self.job_id)\n                self.log_stream_name = stream_name\n                self._log_backend.ensure_log_group(log_group, None)\n                self._log_backend.create_log_stream(log_group, stream_name)\n                self._log_backend.put_log_events(log_group, stream_name, logs, None)\n\n            except Exception as err:\n                logger.error('Failed to run AWS Batch container {0}. Error {1}'.format(self.name, err))\n                self.job_state = 'FAILED'\n                container.kill()\n            finally:\n                container.remove()\n        except Exception as err:\n            logger.error('Failed to run AWS Batch container {0}. Error {1}'.format(self.name, err))\n            self.job_state = 'FAILED'\n\n        self.job_stopped = True\n        self.job_stopped_at = datetime.datetime.now()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_compute_environment(self, identifier):\n        env = self.get_compute_environment_by_arn(identifier)\n        if env is None:\n            env = self.get_compute_environment_by_name(identifier)\n        return env", "response": "Get the compute environment by name or ARN."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_job_queue(self, identifier):\n        env = self.get_job_queue_by_arn(identifier)\n        if env is None:\n            env = self.get_job_queue_by_name(identifier)\n        return env", "response": "Get the job queue by name or ARN."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_job_definition(self, identifier):\n        env = self.get_job_definition_by_arn(identifier)\n        if env is None:\n            env = self.get_job_definition_by_name(identifier)\n        return env", "response": "Get job definition by name or ARN."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_job_definitions(self, identifier):\n        result = []\n        env = self.get_job_definition_by_arn(identifier)\n        if env is not None:\n            result.append(env)\n        else:\n            for value in self._job_definitions.values():\n                if value.name == identifier:\n                    result.append(value)\n\n        return result", "response": "Get job definition by name or ARN."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_compute_resources(self, cr):\n        for param in ('instanceRole', 'maxvCpus', 'minvCpus', 'instanceTypes', 'securityGroupIds', 'subnets', 'type'):\n            if param not in cr:\n                raise InvalidParameterValueException('computeResources must contain {0}'.format(param))\n\n        if self.iam_backend.get_role_by_arn(cr['instanceRole']) is None:\n            raise InvalidParameterValueException('could not find instanceRole {0}'.format(cr['instanceRole']))\n\n        if cr['maxvCpus'] < 0:\n            raise InvalidParameterValueException('maxVCpus must be positive')\n        if cr['minvCpus'] < 0:\n            raise InvalidParameterValueException('minVCpus must be positive')\n        if cr['maxvCpus'] < cr['minvCpus']:\n            raise InvalidParameterValueException('maxVCpus must be greater than minvCpus')\n\n        if len(cr['instanceTypes']) == 0:\n            raise InvalidParameterValueException('At least 1 instance type must be provided')\n        for instance_type in cr['instanceTypes']:\n            if instance_type == 'optimal':\n                pass  # Optimal should pick from latest of current gen\n            elif instance_type not in EC2_INSTANCE_TYPES:\n                raise InvalidParameterValueException('Instance type {0} does not exist'.format(instance_type))\n\n        for sec_id in cr['securityGroupIds']:\n            if self.ec2_backend.get_security_group_from_id(sec_id) is None:\n                raise InvalidParameterValueException('security group {0} does not exist'.format(sec_id))\n        if len(cr['securityGroupIds']) == 0:\n            raise InvalidParameterValueException('At least 1 security group must be provided')\n\n        for subnet_id in cr['subnets']:\n            try:\n                self.ec2_backend.get_subnet(subnet_id)\n            except InvalidSubnetIdError:\n                raise InvalidParameterValueException('subnet {0} does not exist'.format(subnet_id))\n        if len(cr['subnets']) == 0:\n            raise InvalidParameterValueException('At least 1 subnet must be provided')\n\n        if cr['type'] not in ('EC2', 'SPOT'):\n            raise InvalidParameterValueException('computeResources.type must be either EC2 | SPOT')\n\n        if cr['type'] == 'SPOT':\n            raise InternalFailure('SPOT NOT SUPPORTED YET')", "response": "Checks contents of computeResources for managed clusters and raises InvalidParameterValueException if they are not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_min_instances_to_meet_vcpus(instance_types, target):\n        # vcpus = [ (vcpus, instance_type), (vcpus, instance_type), ... ]\n        instance_vcpus = []\n        instances = []\n\n        for instance_type in instance_types:\n            if instance_type == 'optimal':\n                instance_type = 'm4.4xlarge'\n\n            instance_vcpus.append(\n                (EC2_INSTANCE_TYPES[instance_type]['vcpus'], instance_type)\n            )\n\n        instance_vcpus = sorted(instance_vcpus, key=lambda item: item[0], reverse=True)\n        # Loop through,\n        #   if biggest instance type smaller than target, and len(instance_types)> 1, then use biggest type\n        #   if biggest instance type bigger than target, and len(instance_types)> 1, then remove it and move on\n\n        #   if biggest instance type bigger than target and len(instan_types) == 1 then add instance and finish\n        #   if biggest instance type smaller than target and len(instan_types) == 1 then loop adding instances until target == 0\n        #   ^^ boils down to keep adding last till target vcpus is negative\n        #   #Algorithm ;-) ... Could probably be done better with some quality lambdas\n        while target > 0:\n            current_vcpu, current_instance = instance_vcpus[0]\n\n            if len(instance_vcpus) > 1:\n                if current_vcpu <= target:\n                    target -= current_vcpu\n                    instances.append(current_instance)\n                else:\n                    # try next biggest instance\n                    instance_vcpus.pop(0)\n            else:\n                # Were on the last instance\n                target -= current_vcpu\n                instances.append(current_instance)\n\n        return instances", "response": "Finds the minimum needed instances to meed a vcpu target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new job queue with the given parameters.", "response": "def create_job_queue(self, queue_name, priority, state, compute_env_order):\n        \"\"\"\n        Create a job queue\n\n        :param queue_name: Queue name\n        :type queue_name: str\n        :param priority: Queue priority\n        :type priority: int\n        :param state: Queue state\n        :type state: string\n        :param compute_env_order: Compute environment list\n        :type compute_env_order: list of dict\n        :return: Tuple of Name, ARN\n        :rtype: tuple of str\n        \"\"\"\n        for variable, var_name in ((queue_name, 'jobQueueName'), (priority, 'priority'), (state, 'state'), (compute_env_order, 'computeEnvironmentOrder')):\n            if variable is None:\n                raise ClientException('{0} must be provided'.format(var_name))\n\n        if state not in ('ENABLED', 'DISABLED'):\n            raise ClientException('state {0} must be one of ENABLED | DISABLED'.format(state))\n        if self.get_job_queue_by_name(queue_name) is not None:\n            raise ClientException('Job queue {0} already exists'.format(queue_name))\n\n        if len(compute_env_order) == 0:\n            raise ClientException('At least 1 compute environment must be provided')\n        try:\n            # orders and extracts computeEnvironment names\n            ordered_compute_environments = [item['computeEnvironment'] for item in sorted(compute_env_order, key=lambda x: x['order'])]\n            env_objects = []\n            # Check each ARN exists, then make a list of compute env's\n            for arn in ordered_compute_environments:\n                env = self.get_compute_environment_by_arn(arn)\n                if env is None:\n                    raise ClientException('Compute environment {0} does not exist'.format(arn))\n                env_objects.append(env)\n        except Exception:\n            raise ClientException('computeEnvironmentOrder is malformed')\n\n        # Create new Job Queue\n        queue = JobQueue(queue_name, priority, state, env_objects, compute_env_order, self.region_name)\n        self._job_queues[queue.arn] = queue\n\n        return queue_name, queue.arn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating a job queue with the given parameters.", "response": "def update_job_queue(self, queue_name, priority, state, compute_env_order):\n        \"\"\"\n        Update a job queue\n\n        :param queue_name: Queue name\n        :type queue_name: str\n        :param priority: Queue priority\n        :type priority: int\n        :param state: Queue state\n        :type state: string\n        :param compute_env_order: Compute environment list\n        :type compute_env_order: list of dict\n        :return: Tuple of Name, ARN\n        :rtype: tuple of str\n        \"\"\"\n        if queue_name is None:\n            raise ClientException('jobQueueName must be provided')\n\n        job_queue = self.get_job_queue(queue_name)\n        if job_queue is None:\n            raise ClientException('Job queue {0} does not exist'.format(queue_name))\n\n        if state is not None:\n            if state not in ('ENABLED', 'DISABLED'):\n                raise ClientException('state {0} must be one of ENABLED | DISABLED'.format(state))\n\n            job_queue.state = state\n\n        if compute_env_order is not None:\n            if len(compute_env_order) == 0:\n                raise ClientException('At least 1 compute environment must be provided')\n            try:\n                # orders and extracts computeEnvironment names\n                ordered_compute_environments = [item['computeEnvironment'] for item in sorted(compute_env_order, key=lambda x: x['order'])]\n                env_objects = []\n                # Check each ARN exists, then make a list of compute env's\n                for arn in ordered_compute_environments:\n                    env = self.get_compute_environment_by_arn(arn)\n                    if env is None:\n                        raise ClientException('Compute environment {0} does not exist'.format(arn))\n                    env_objects.append(env)\n            except Exception:\n                raise ClientException('computeEnvironmentOrder is malformed')\n\n            job_queue.env_order_json = compute_env_order\n            job_queue.environments = env_objects\n\n        if priority is not None:\n            job_queue.priority = priority\n\n        return queue_name, job_queue.arn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_file(filename, serverEndpoint=ServerEndpoint, xmlContent=False, headers=None, config_path=None):\n    '''\n    Parses a file for metadata and content\n    :param filename: path to file which needs to be parsed\n    :param serverEndpoint: Server endpoint url\n    :param xmlContent: Whether or not XML content be requested.\n                    Default is 'False', which results in text content.\n    :param headers: Request headers to be sent to the tika reset server, should\n                    be a dictionary. This is optional\n    :return: dictionary having 'metadata' and 'content' keys.\n            'content' has a str value and metadata has a dict type value.\n    '''\n    if not xmlContent:\n        jsonOutput = parse1('all', filename, serverEndpoint, headers=headers, config_path=config_path)\n    else:\n        jsonOutput = parse1('all', filename, serverEndpoint, services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/xml'},\n                            headers=headers, config_path=config_path)\n    return _parse(jsonOutput)", "response": "Parses a file for metadata and content and returns a dictionary of metadata and content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_buffer(string, serverEndpoint=ServerEndpoint, xmlContent=False, headers=None, config_path=None):\n    '''\n    Parses the content from buffer\n    :param string: Buffer value\n    :param serverEndpoint: Server endpoint. This is optional\n    :param xmlContent: Whether or not XML content be requested.\n                    Default is 'False', which results in text content.\n    :param headers: Request headers to be sent to the tika reset server, should\n                    be a dictionary. This is optional\n    :return:\n    '''\n    headers = headers or {}\n    headers.update({'Accept': 'application/json'})\n\n    if not xmlContent:\n        status, response = callServer('put', serverEndpoint, '/rmeta/text', string, headers, False, config_path=config_path)\n    else:\n        status, response = callServer('put', serverEndpoint, '/rmeta/xml', string, headers, False, config_path=config_path)\n\n    return _parse((status,response))", "response": "Parses the content of a buffer into a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse(jsonOutput):\n    '''\n    Parses JSON response from Tika REST API server\n    :param jsonOutput: JSON output from Tika Server\n    :return: a dictionary having 'metadata' and 'content' values\n    '''\n    parsed={}\n    if not jsonOutput:\n        return parsed\n    \n    parsed[\"status\"] = jsonOutput[0]\n    if jsonOutput[1] == None or jsonOutput[1] == \"\":\n        return parsed\n    realJson = json.loads(jsonOutput[1])\n\n    content = \"\"\n    for js in realJson:\n        if \"X-TIKA:content\" in js:\n            content += js[\"X-TIKA:content\"]\n\n    if content == \"\":\n        content = None\n\n    parsed[\"content\"] = content\n    parsed[\"metadata\"] = {}\n\n    for js in realJson:\n        for n in js:\n            if n != \"X-TIKA:content\":\n                if n in parsed[\"metadata\"]:\n                    if not isinstance(parsed[\"metadata\"][n], list):\n                        parsed[\"metadata\"][n] = [parsed[\"metadata\"][n]]\n                    parsed[\"metadata\"][n].append(js[n])\n                else:\n                    parsed[\"metadata\"][n] = js[n]\n\n    return parsed", "response": "Parses the JSON output from Tika REST API server\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the Tika command by calling the Tika server and return the results in JSON format or plain text.", "response": "def runCommand(cmd, option, urlOrPaths, port, outDir=None,\n               serverHost=ServerHost, tikaServerJar=TikaServerJar,\n               verbose=Verbose, encode=EncodeUtf8):\n    '''\n    Run the Tika command by calling the Tika server and return results in JSON format (or plain text).\n    :param cmd: a command from set ``{'parse', 'detect', 'language', 'translate', 'config'}``\n    :param option:\n    :param urlOrPaths:\n    :param port:\n    :param outDir:\n    :param serverHost:\n    :param tikaServerJar:\n    :param verbose:\n    :param encode:\n    :return: response for the command, usually a ``dict``\n    '''\n    # import pdb; pdb.set_trace()\n    if (cmd in 'parse' or cmd in 'detect') and (urlOrPaths == [] or urlOrPaths == None):\n        log.exception('No URLs/paths specified.')\n        raise TikaException('No URLs/paths specified.')\n    serverEndpoint = 'http://' + serverHost + ':' + port\n    if cmd == 'parse':\n        return parseAndSave(option, urlOrPaths, outDir, serverEndpoint, verbose, tikaServerJar)\n    elif cmd == \"detect\":\n        return detectType(option, urlOrPaths, serverEndpoint, verbose, tikaServerJar)\n    elif cmd == \"language\":\n        return detectLang(option, urlOrPaths, serverEndpoint, verbose, tikaServerJar)\n    elif cmd == \"translate\":\n        return doTranslate(option, urlOrPaths, serverEndpoint, verbose, tikaServerJar)        \n    elif cmd == \"config\":\n        status, resp = getConfig(option, serverEndpoint, verbose, tikaServerJar)\n        return resp\n    else:\n        log.exception('Bad args')\n        raise TikaException('Bad args')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPaths(urlOrPaths):\n    '''\n    Determines if the given URL in urlOrPaths is a URL or a file or directory. If it's\n    a directory, it walks the directory and then finds all file paths in it, and ads them\n    too. If it's a file, it adds it to the paths. If it's a URL it just adds it to the path.\n    :param urlOrPaths: the url or path to be scanned\n    :return: ``list`` of paths\n    '''\n    if isinstance(urlOrPaths, basestring):\n        #FIXME: basestring is undefined\n        urlOrPaths = [urlOrPaths]  # do not recursively walk over letters of a single path which can include \"/\"\n    paths = []\n    for eachUrlOrPaths in urlOrPaths:\n        if os.path.isdir(eachUrlOrPaths):\n            for root, directories, filenames in walk(eachUrlOrPaths):\n                for filename in filenames:\n                    paths.append(os.path.join(root,filename))\n        else:\n            paths.append(eachUrlOrPaths)\n    return paths", "response": "Determines if the given URL in urlOrPaths is a file or directory and adds it to the list of paths."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parseAndSave(option, urlOrPaths, outDir=None, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar,\n                 responseMimeType='application/json', metaExtension='_meta.json',\n                 services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta'}):\n    '''\n    Parse the objects and write extracted metadata and/or text in JSON format to matching\n    filename with an extension of '_meta.json'.\n    :param option:\n    :param urlOrPaths:\n    :param outDir:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param metaExtension:\n    :param services:\n    :return:\n    '''\n    metaPaths = []\n    paths = getPaths(urlOrPaths)\n    for path in paths:\n        if outDir is None:\n            metaPath = path + metaExtension\n        else:\n            metaPath = os.path.join(outDir, os.path.split(path)[1] + metaExtension)\n            log.info('Writing %s' % metaPath)\n            with open(metaPath, 'w', 'utf-8') as f:\n                f.write(parse1(option, path, serverEndpoint, verbose, tikaServerJar, \\\n                                    responseMimeType, services)[1] + u\"\\n\")\n        metaPaths.append(metaPath)\n    return metaPaths", "response": "Parse the objects and write them to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(option, urlOrPaths, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar, \n          responseMimeType='application/json',\n          services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta'}, rawResponse=False):\n    '''\n    Parse the objects and return extracted metadata and/or text in JSON format.\n    :param option:\n    :param urlOrPaths:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :return:\n    '''\n    return [parse1(option, path, serverEndpoint, verbose, tikaServerJar, responseMimeType, services)\n             for path in urlOrPaths]", "response": "Parse the objects and return the metadata and or text in JSON format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse1(option, urlOrPath, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar, \n          responseMimeType='application/json',\n          services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/text'}, rawResponse=False, headers=None, config_path=None):\n    '''\n    Parse the object and return extracted metadata and/or text in JSON format.\n    :param option:\n    :param urlOrPath:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :param rawResponse:\n    :param headers:\n    :return:\n    '''\n    headers = headers or {}\n\n    path, file_type = getRemoteFile(urlOrPath, TikaFilesPath)\n    headers.update({'Accept': responseMimeType, 'Content-Disposition': make_content_disposition_header(path)})\n\n    if option not in services:\n        log.warning('config option must be one of meta, text, or all; using all.')\n    service = services.get(option, services['all'])\n    if service == '/tika': responseMimeType = 'text/plain'\n    headers.update({'Accept': responseMimeType, 'Content-Disposition': make_content_disposition_header(path)})\n    status, response = callServer('put', serverEndpoint, service, open(path, 'rb'),\n                                  headers, verbose, tikaServerJar, config_path=config_path, rawResponse=rawResponse)\n\n    if file_type == 'remote': os.unlink(path)\n    return (status, response)", "response": "Parse the object and return the extracted metadata and or text in JSON format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetect the language of the provided file and return its 2 character code as text / plain.", "response": "def detectLang(option, urlOrPaths, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar,\n                responseMimeType='text/plain',\n                services={'file' : '/language/stream'}):\n    '''\n    Detect the language of the provided stream and return its 2 character code as text/plain.\n    :param option:\n    :param urlOrPaths:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :return:\n    '''\n    paths = getPaths(urlOrPaths)\n    return [detectLang1(option, path, serverEndpoint, verbose, tikaServerJar, responseMimeType, services)\n            for path in paths]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate the file from source language to destination language.", "response": "def doTranslate(option, urlOrPaths, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar, \n                responseMimeType='text/plain',\n                services={'all': '/translate/all'}):\n    '''\n    Translate the file from source language to destination language.\n    :param option:\n    :param urlOrPaths:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :return:\n    '''\n    paths = getPaths(urlOrPaths)\n    return [doTranslate1(option, path, serverEndpoint, verbose, tikaServerJar, responseMimeType, services)\n            for path in paths]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects the MIME type of the stream and return it in text / plain.", "response": "def detectType(option, urlOrPaths, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar, \n               responseMimeType='text/plain',\n               services={'type': '/detect/stream'}):\n    '''\n    Detect the MIME/media type of the stream and return it in text/plain.\n    :param option:\n    :param urlOrPaths:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :return:\n    '''\n    paths = getPaths(urlOrPaths)\n    return [detectType1(option, path, serverEndpoint, verbose, tikaServerJar, responseMimeType, services)\n             for path in paths]"}
